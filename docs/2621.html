<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How and Why to Use LLMs for Chunk-Based Information Retrieval</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How and Why to Use LLMs for Chunk-Based Information Retrieval</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-and-why-to-use-llms-for-chunk-based-information-retrieval-5242f0133b55?source=collection_archive---------4-----------------------#2024-10-28">https://towardsdatascience.com/how-and-why-to-use-llms-for-chunk-based-information-retrieval-5242f0133b55?source=collection_archive---------4-----------------------#2024-10-28</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@peronc79?source=post_page---byline--5242f0133b55--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Carlo Peron" class="l ep by dd de cx" src="../Images/e6db9521113aa6a2dd43b0b2aa6687b5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*uDXGuejWzRpj54GywWxenw.jpeg"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5242f0133b55--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://medium.com/@peronc79?source=post_page---byline--5242f0133b55--------------------------------" rel="noopener follow">Carlo Peron</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5242f0133b55--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 28, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx ko kp ab q ee kq kr" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="ks"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="kt k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ku an ao ap hr kv kw kx" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ky cn"><div class="l ae"><div class="ab cb"><div class="kz la lb lc ld le ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ku an ao ap hr lf lg kr lh li lj lk ll s lm ln lo lp lq lr ls u lt lu lv"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ku an ao ap hr lf lg kr lh li lj lk ll s lm ln lo lp lq lr ls u lt lu lv"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ku an ao ap hr lf lg kr lh li lj lk ll s lm ln lo lp lq lr ls u lt lu lv"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="lz ma mb mc md me lw lx paragraph-image"><div role="button" tabindex="0" class="mf mg ed mh bh mi"><div class="lw lx ly"><img src="../Images/ea9b1bb4b44a9f0b8ada220d93ecda54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y8grKHsaHyrtPSvVvyeYVQ.png"/></div></div><figcaption class="mk ml mm lw lx mn mo bf b bg z dx">Retrieve pipeline — Image by the author</figcaption></figure><p id="4af8" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">In this article, I aim to explain how and why it’s beneficial to use a Large Language Model (LLM) for chunk-based information retrieval.</p><p id="c88d" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">I use OpenAI’s GPT-4 model as an example, but this approach can be applied with any other LLM, such as those from Hugging Face, Claude, and others.</p><p id="7a45" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Everyone can access this <a class="af nn" href="https://medium.com/@peronc79/5242f0133b55?sk=aafe7dca2cb777410b6e426321c0b53e" rel="noopener">article</a> for free.</p><p id="8a93" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">Considerations on standard information retrieval</strong></p><p id="bf0b" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The primary concept involves having a list of documents (<strong class="mr fr">chunks of text</strong>) stored in a database, which could be retrieve based on some filter and conditions.</p><p id="5997" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Typically, a tool is used to enable hybrid search (such as Azure AI Search, LlamaIndex, etc.), which allows:</p><ul class=""><li id="ebca" class="mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm no np nq bk">performing a text-based search using term frequency algorithms like TF-IDF (e.g., BM25);</li><li id="5905" class="mp mq fq mr b ms nr mu mv mw ns my mz na nt nc nd ne nu ng nh ni nv nk nl nm no np nq bk">conducting a vector-based search, which identifies similar concepts even when different terms are used, by calculating vector distances (typically cosine similarity);</li><li id="ab3d" class="mp mq fq mr b ms nr mu mv mw ns my mz na nt nc nd ne nu ng nh ni nv nk nl nm no np nq bk">combining elements from steps 1 and 2, weighting them to highlight the most relevant results.</li></ul><figure class="nx ny nz oa ob me lw lx paragraph-image"><div role="button" tabindex="0" class="mf mg ed mh bh mi"><div class="lw lx nw"><img src="../Images/6f4f4ba0a88c11c7dc09513356aa58d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RXqYL8FaBldp4S2h0C6VgA.png"/></div></div><figcaption class="mk ml mm lw lx mn mo bf b bg z dx">Figure 1- Default hybrid search pipeline — Image by the author</figcaption></figure><p id="7cc9" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Figure 1 shows the classic retrieval pipeline:</p><ul class=""><li id="7283" class="mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm no np nq bk">the user asks the system a question: “I would like to talk about Paris”;</li><li id="15be" class="mp mq fq mr b ms nr mu mv mw ns my mz na nt nc nd ne nu ng nh ni nv nk nl nm no np nq bk">the system receives the question, converts it into an embedding vector (using the same model applied in the ingestion phase), and finds the chunks with the smallest distances;</li><li id="1521" class="mp mq fq mr b ms nr mu mv mw ns my mz na nt nc nd ne nu ng nh ni nv nk nl nm no np nq bk">the system also performs a text-based search based on frequency;</li><li id="6c4e" class="mp mq fq mr b ms nr mu mv mw ns my mz na nt nc nd ne nu ng nh ni nv nk nl nm no np nq bk">the chunks returned from both processes undergo further evaluation and are reordered based on a ranking formula.</li></ul><p id="78ec" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">This solution achieves good results but has some limitations:</p><ul class=""><li id="f6fd" class="mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm no np nq bk">not all relevant chunks are always retrieved;</li><li id="5ff3" class="mp mq fq mr b ms nr mu mv mw ns my mz na nt nc nd ne nu ng nh ni nv nk nl nm no np nq bk">sometime some chunks contain anomalies that affect the final response.</li></ul><p id="cb32" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">An example of a typical retrieval issue</strong></p><p id="9296" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Let’s consider the “documents” array, which represents an example of a knowledge base that could lead to incorrect chunk selection.</p><pre class="nx ny nz oa ob oc od oe bp of bb bk"><span id="57e0" class="og oh fq od b bg oi oj l ok ol">documents = [<br/>    "Chunk 1: This document contains information about topic A.",<br/>    "Chunk 2: Insights related to topic B can be found here.",<br/>    "Chunk 3: This chunk discusses topic C in detail.",<br/>    "Chunk 4: Further insights on topic D are covered here.",<br/>    "Chunk 5: Another chunk with more data on topic E.",<br/>    "Chunk 6: Extensive research on topic F is presented.",<br/>    "Chunk 7: Information on topic G is explained here.",<br/>    "Chunk 8: This document expands on topic H. It also talk about topic B",<br/>    "Chunk 9: Nothing about topic B are given.",<br/>    "Chunk 10: Finally, a discussion of topic J. This document doesn't contain information about topic B"<br/>]</span></pre><p id="3454" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Let’s assume we have a RAG system, consisting of a vector database with hybrid search capabilities and an LLM-based prompt, to which the user poses the following question: “I need to know something about topic B.”</p><p id="76f0" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">As shown in Figure 2, the search also returns an incorrect chunk that, while semantically relevant, is not suitable for answering the question and, in some cases, could even confuse the LLM tasked with providing a response.</p><figure class="nx ny nz oa ob me lw lx paragraph-image"><div role="button" tabindex="0" class="mf mg ed mh bh mi"><div class="lw lx om"><img src="../Images/356ad8ff68ceb28acca4d8a23ec584c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fj-FDEfNKMAlUxahv_DyuQ.png"/></div></div><figcaption class="mk ml mm lw lx mn mo bf b bg z dx">Figure 2 — Example of information retrieval that can lead to errors — Image by the author</figcaption></figure><p id="8d96" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">In this example, the user requests information about “<em class="on">topic B</em>,” and the search returns chunks that include “<em class="on">This document expands on topic H. It also talks about topic B</em>” and “<em class="on">Insights related to topic B can be found here.</em>” as well as the chunk stating, “<em class="on">Nothing about topic B are given</em>”.</p><p id="05f0" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">While this is the expected behavior of hybrid search (as chunks reference “<em class="on">topic B</em>”), it is not the desired outcome, as the third chunk is returned without recognizing that it isn’t helpful for answering the question.</p><p id="5c30" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The retrieval didn’t produce the intended result, not only because the BM25 search found the term “<em class="on">topic B</em>” in the third Chunk but also because the vector search yielded a high cosine similarity.</p><p id="c4c7" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">To understand this, refer to Figure 3, which shows the cosine similarity values of the chunks relative to the question, using OpenAI’s text-embedding-ada-002 model for embeddings.</p><figure class="nx ny nz oa ob me lw lx paragraph-image"><div role="button" tabindex="0" class="mf mg ed mh bh mi"><div class="lw lx oo"><img src="../Images/c42a17cff143c55984745f79381a2c9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*171_XVzg9DahP-Kg-fFhNw.png"/></div></div><figcaption class="mk ml mm lw lx mn mo bf b bg z dx">Figure 3 — Cosine similarity with text-embedding-ada-002- Image by the author</figcaption></figure><p id="8ef4" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">It is evident that the cosine similarity value for “Chunk 9” is among the highest, and that between this chunk and chunk 10, which references “<em class="on">topic B</em>,” there is also chunk 1, which does not mention “<em class="on">topic B</em>”.</p><p id="b385" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">This situation remains unchanged even when measuring distance using a different method, as seen in the case of Minkowski distance.</p><p id="c91e" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">Utilizing LLMs for Information Retrieval: An Example</strong></p><p id="843f" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The solution I will describe is inspired by what has been published in my GitHub repository <a class="af nn" href="https://github.com/peronc/LLMRetriever/" rel="noopener ugc nofollow" target="_blank">https://github.com/peronc/LLMRetriever/</a>.</p><p id="7c49" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The idea is to have the LLM analyze which chunks are useful for answering the user’s question, not by ranking the returned chunks (as in the case of RankGPT) but by directly evaluating all the available chunks.</p><figure class="nx ny nz oa ob me lw lx paragraph-image"><div role="button" tabindex="0" class="mf mg ed mh bh mi"><div class="lw lx ly"><img src="../Images/ea9b1bb4b44a9f0b8ada220d93ecda54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y8grKHsaHyrtPSvVvyeYVQ.png"/></div></div><figcaption class="mk ml mm lw lx mn mo bf b bg z dx">Figure 4- LLM Retrieve pipeline — Image by the author</figcaption></figure><p id="d541" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">In summary, as shown in Figure 4, the system receives a list of documents to analyze, which can come from any data source, such as file storage, relational databases, or vector databases.</p><p id="3a9c" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The chunks are divided into groups and processed in parallel by a number of threads proportional to the total amount of chunks.</p><p id="7bf8" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The logic for each thread includes a loop that iterates through the input chunks, calling an OpenAI prompt for each one to check its relevance to the user’s question.</p><p id="174a" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The prompt returns the chunk along with a boolean value: <em class="on">true</em> if it is relevant and <em class="on">false</em> if it is not.</p><p id="23b9" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">Lets’go coding 😊</strong></p><p id="c0da" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">To explain the code, I will simplify by using the chunks present in the <em class="on">documents</em> array (I will reference a real case in the conclusions).</p><p id="44f4" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">First of all, I import the necessary standard libraries, including os, langchain, and dotenv.</p><pre class="nx ny nz oa ob oc od oe bp of bb bk"><span id="d76d" class="og oh fq od b bg oi oj l ok ol">import os<br/>from langchain_openai.chat_models.azure import AzureChatOpenAI<br/>from dotenv import load_dotenv</span></pre><p id="05e7" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Next, I import my LLMRetrieverLib/llm_retrieve.py class, which provides several static methods essential for performing the analysis.</p><pre class="nx ny nz oa ob oc od oe bp of bb bk"><span id="088a" class="og oh fq od b bg oi oj l ok ol">from LLMRetrieverLib.retriever import llm_retriever</span></pre><p id="3668" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Following that, I need to import the necessary variables required for utilizing Azure OpenAI GPT-4o model.</p><pre class="nx ny nz oa ob oc od oe bp of bb bk"><span id="f41b" class="og oh fq od b bg oi oj l ok ol">load_dotenv()<br/>azure_deployment = os.getenv("AZURE_DEPLOYMENT")<br/>temperature = float(os.getenv("TEMPERATURE"))<br/>api_key  = os.getenv("AZURE_OPENAI_API_KEY")<br/>endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")<br/>api_version = os.getenv("API_VERSION")</span></pre><p id="e670" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Next, I proceed with the initialization of the LLM.</p><pre class="nx ny nz oa ob oc od oe bp of bb bk"><span id="c735" class="og oh fq od b bg oi oj l ok ol"># Initialize the LLM<br/>llm = AzureChatOpenAI(api_key=api_key, azure_endpoint=endpoint, azure_deployment=azure_deployment, api_version=api_version,temperature=temperature)</span></pre><p id="2185" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">We are ready to begin: the user asks a question to gather additional information about <em class="on">Topic B</em>.</p><pre class="nx ny nz oa ob oc od oe bp of bb bk"><span id="799b" class="og oh fq od b bg oi oj l ok ol">question = "I need to know something about topic B"</span></pre><p id="6431" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">At this point, the search for relevant chunks begins, and to do this, I use the function <code class="cx op oq or od b">llm_retrieve.process_chunks_in_parallel</code> from the <code class="cx op oq or od b">LLMRetrieverLib/retriever.py</code> library, which is also found in the same repository.</p><pre class="nx ny nz oa ob oc od oe bp of bb bk"><span id="8173" class="og oh fq od b bg oi oj l ok ol">relevant_chunks = LLMRetrieverLib.retriever.llm_retriever.process_chunks_in_parallel(llm, question, documents, 3)</span></pre><p id="867b" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">To optimize performance, the function <code class="cx op oq or od b">llm_retrieve.process_chunks_in_parallel</code> employs multi-threading to distribute chunk analysis across multiple threads.</p><p id="b542" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The main idea is to assign each thread a subset of chunks extracted from the database and have each thread analyze the relevance of those chunks based on the user’s question.</p><p id="a06a" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">At the end of the processing, the returned chunks are exactly as expected:</p><pre class="nx ny nz oa ob oc od oe bp of bb bk"><span id="d7d8" class="og oh fq od b bg oi oj l ok ol">['Chunk 2: Insights related to topic B can be found here.',<br/>'Chunk 8: This document expands on topic H. It also talk about topic B']</span></pre><p id="060f" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Finally, I ask the LLM to provide an answer to the user’s question:</p><pre class="nx ny nz oa ob oc od oe bp of bb bk"><span id="1318" class="og oh fq od b bg oi oj l ok ol">final_answer = LLMRetrieverLib.retriever.llm_retriever.generate_final_answer_with_llm(llm, relevant_chunks, question)<br/>print("Final answer:")<br/>print(final_answer)<br/></span></pre><p id="3137" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Below is the LLM’s response, which is trivial since the content of the chunks, while relevant, is not exhaustive on the topic of Topic B:</p><pre class="nx ny nz oa ob oc od oe bp of bb bk"><span id="c842" class="og oh fq od b bg oi oj l ok ol">Topic B is covered in both Chunk 2 and Chunk 8. <br/>Chunk 2 provides insights specifically related to topic B, offering detailed information and analysis. <br/>Chunk 8 expands on topic H but also includes discussions on topic B, potentially providing additional context or perspectives.</span></pre><p id="2fbc" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">Scoring Scenario</strong></p><p id="83d4" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Now let’s try asking the same question but using an approach based on scoring.</p><p id="7169" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">I ask the LLM to assign a score from 1 to 10 to evaluate the relevance between each chunk and the question, considering only those with a relevance higher than 5.</p><p id="5a19" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">To do this, I call the function <code class="cx op oq or od b">llm_retriever.process_chunks_in_parallel</code>, passing three additional parameters that indicate, respectively, that scoring will be applied, that the threshold for being considered valid must be greater than or equal to 5, and that I want a printout of the chunks with their respective scores.</p><pre class="nx ny nz oa ob oc od oe bp of bb bk"><span id="b585" class="og oh fq od b bg oi oj l ok ol">relevant_chunks = llm_retriever.process_chunks_in_parallel(llm, question, documents, 3, True, 5, True)</span></pre><p id="064e" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The retrieval phase with scoring produces the following result:</p><pre class="nx ny nz oa ob oc od oe bp of bb bk"><span id="6417" class="og oh fq od b bg oi oj l ok ol">score: 1 - Chunk 1: This document contains information about topic A.<br/>score: 1 - Chunk 7: Information on topic G is explained here.<br/>score: 1 - Chunk 4: Further insights on topic D are covered here.<br/>score: 9 - Chunk 2: Insights related to topic B can be found here.<br/>score: 7 - Chunk 8: This document expands on topic H. It also talk about topic B<br/>score: 1 - Chunk 5: Another chunk with more data on topic E.<br/>score: 1 - Chunk 9: Nothing about topic B are given.<br/>score: 1 - Chunk 3: This chunk discusses topic C in detail.<br/>score: 1 - Chunk 6: Extensive research on topic F is presented.<br/>score: 1 - Chunk 10: Finally, a discussion of topic J. This document doesn't contain information about topic B</span></pre><p id="1a0a" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">It’s the same as before, but with an interesting score 😊.</p><p id="bb27" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Finally, I once again ask the LLM to provide an answer to the user’s question, and the result is similar to the previous one:</p><pre class="nx ny nz oa ob oc od oe bp of bb bk"><span id="9d22" class="og oh fq od b bg oi oj l ok ol">Chunk 2 provides insights related to topic B, offering foundational information and key points.<br/>Chunk 8 expands on topic B further, possibly providing additional context or details, as it also discusses topic H.<br/>Together, these chunks should give you a well-rounded understanding of topic B. If you need more specific details, let me know!</span></pre><p id="dce5" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">Considerations</strong></p><p id="1f00" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">This retrieval approach has emerged as a necessity following some previous experiences.</p><p id="831a" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">I have noticed that pure vector-based searches produce useful results but are often insufficient when the embedding is performed in a language other than English.</p><p id="c369" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Using OpenAI with sentences in Italian makes it clear that the tokenization of terms is often incorrect; for example, the term “<em class="on">canzone</em>,” which means “<em class="on">song</em>” in Italian, gets tokenized into two distinct words: “<em class="on">can</em>” and “<em class="on">zone</em>”.</p><p id="23a5" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">This leads to the construction of an embedding array that is far from what was intended.</p><p id="7024" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">In cases like this, hybrid search, which also incorporates term frequency counting, leads to improved results, but they are not always as expected.</p><p id="2e6e" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">So, this retrieval methodology can be utilized in the following ways:</p><ul class=""><li id="48c7" class="mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm no np nq bk"><strong class="mr fr">as the primary search method:</strong> where the database is queried for all chunks or a subset based on a filter (e.g., a metadata filter);</li><li id="d72d" class="mp mq fq mr b ms nr mu mv mw ns my mz na nt nc nd ne nu ng nh ni nv nk nl nm no np nq bk"><strong class="mr fr">as a refinement in the case of hybrid search:</strong> (this is the same approach used by RankGPT) in this way, the hybrid search can extract a large number of chunks, and the system can filter them so that only the relevant ones reach the LLM while also adhering to the input token limit;</li><li id="757c" class="mp mq fq mr b ms nr mu mv mw ns my mz na nt nc nd ne nu ng nh ni nv nk nl nm no np nq bk"><strong class="mr fr">as a fallback:</strong> in situations where a hybrid search does not yield the desired results, all chunks can be analyzed.</li></ul><p id="530a" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">Let’s discuss costs and performance</strong></p><p id="1a78" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Of course, all that glitters is not gold, as one must consider response times and costs.</p><p id="9fef" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">In a real use case, I retrieved the chunks from a relational database consisting of 95 text segments semantically split using my <code class="cx op oq or od b">LLMChunkizerLib/chunkizer.py</code> library from two Microsoft Word documents, totaling 33 pages.</p><p id="e5a6" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The analysis of the relevance of the 95 chunks to the question was conducted by calling OpenAI's APIs from a local PC with non-guaranteed bandwidth, averaging around 10Mb, resulting in response times that varied from 7 to 20 seconds.</p><p id="60f8" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Naturally, on a cloud system or by using local LLMs on GPUs, these times can be significantly reduced.</p><p id="9017" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">I believe that considerations regarding response times are highly subjective: in some cases, it is acceptable to take longer to provide a correct answer, while in others, it is essential not to keep users waiting too long.</p><p id="2a77" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Similarly, considerations about costs are also quite subjective, as one must take a broader perspective to evaluate whether it is more important to provide as accurate answers as possible or if some errors are acceptable.</p><p id="9924" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">In certain fields, the damage to one’s reputation caused by incorrect or missing answers can outweigh the expense of tokens.</p><p id="3665" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Furthermore, even though the costs of OpenAI and other providers have been steadily decreasing in recent years, those who already have a GPU-based infrastructure, perhaps due to the need to handle sensitive or confidential data, will likely prefer to use a local LLM.</p><p id="4e13" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">Conclusions</strong></p><p id="133e" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">In conclusion, I hope to have provided my perspective on how retrieval can be approached.</p><p id="a0ca" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">If nothing else, I aim to be helpful and perhaps inspire others to explore new methods in their own work.</p><p id="bd90" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Remember, the world of information retrieval is vast, and with a little creativity and the right tools, we can uncover knowledge in ways we never imagined!</p></div></div></div><div class="ab cb os ot ou ov" role="separator"><span class="ow by bm ox oy oz"/><span class="ow by bm ox oy oz"/><span class="ow by bm ox oy"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="720e" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">If you’d like to discuss this further, feel free to connect with me on <a class="af nn" href="https://www.linkedin.com/in/carlo-peron" rel="noopener ugc nofollow" target="_blank">LinkedIn</a></p><p id="ff50" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">GitHub repositories can be found here:<br/>• <a class="af nn" href="https://github.com/peronc/LLMRetriever/" rel="noopener ugc nofollow" target="_blank">https://github.com/peronc/LLMRetriever/</a><br/>• <a class="af nn" href="https://github.com/peronc/LLMChunkizer/" rel="noopener ugc nofollow" target="_blank">https://github.com/peronc/LLMChunkizer/</a></p></div></div></div></div>    
</body>
</html>