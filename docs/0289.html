<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Implement a Custom Training Solution Based on Amazon EC2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Implement a Custom Training Solution Based on Amazon EC2</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-implement-a-custom-training-solution-based-on-amazon-ec2-c91fcc2b145a?source=collection_archive---------15-----------------------#2024-01-30">https://towardsdatascience.com/how-to-implement-a-custom-training-solution-based-on-amazon-ec2-c91fcc2b145a?source=collection_archive---------15-----------------------#2024-01-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="5ed0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A Simple Solution for Managing Cloud-Based ML-Training — Part 2</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--c91fcc2b145a--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--c91fcc2b145a--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--c91fcc2b145a--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--c91fcc2b145a--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/811d45664ed39ded00ae5b3892017d18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xuAegdD5doqq-u96"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@hiking_corgi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Vlad D</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="c03e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is a sequel to a <a class="af nc" rel="noopener" target="_blank" href="/a-simple-solution-for-managing-cloud-based-ml-training-c80a69c6939a">recent post</a> on the topic of building custom, cloud-based solutions for machine learning (ML) model development using low-level instance provisioning services. Our focus in this post will be on <a class="af nc" href="https://aws.amazon.com/ec2/" rel="noopener ugc nofollow" target="_blank">Amazon EC2</a>.</p><p id="ae54" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Cloud service providers (CSPs) typically offer fully managed solutions for training ML models in the cloud. <a class="af nc" href="https://aws.amazon.com/sagemaker/" rel="noopener ugc nofollow" target="_blank">Amazon SageMaker</a>, for example, Amazon’s managed service offering for ML development, simplifies the process of training significantly. Not only does SageMaker automate the end-to-end training execution — from auto-provisioning the requested instance types, to setting up the training environment, to running your training workload, to saving the training artifacts and shutting everything down — but it also offers a number of auxiliary services that support ML development, such as <a class="af nc" href="https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html" rel="noopener ugc nofollow" target="_blank">automatic model tuning</a>, <a class="af nc" href="https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html" rel="noopener ugc nofollow" target="_blank">platform optimized distributed training libraries</a>, and more. However, as is often the case with high-level solutions, the increased ease-of-use of SageMaker training is coupled with a certain level of loss of control over the underlying flow.</p><p id="2ac0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In our <a class="af nc" rel="noopener" target="_blank" href="/a-simple-solution-for-managing-cloud-based-ml-training-c80a69c6939a">previous post</a> we noted some of the limitations sometimes imposed by managed training services such as SageMaker, including reduced user privileges, inaccessibility of some instance types, reduced control over multi-node device placement, and more. Some scenarios require a higher level of autonomy over the environment specification and training flow. In this post, we illustrate one approach to addressing these cases by creating a custom training solution built on top of Amazon EC2.</p><p id="e5a7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Many thanks to <a class="af nc" href="https://www.linkedin.com/in/maxrabin/" rel="noopener ugc nofollow" target="_blank">Max Rabin</a> for his contributions to this post.</p><h1 id="3050" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Poor Man’s Managed Training on Amazon EC2</h1><p id="5ff0" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">In our previous post we listed a minimal set of features that we would require from an automated training solution and proceeded to demonstrate, in a step-by-step manner, one way of implementing these in <a class="af nc" href="https://cloud.google.com/" rel="noopener ugc nofollow" target="_blank">Google Cloud Platform (GCP)</a>. And although the same sequence of steps would apply to any other cloud platform, the details can be quite different due to the unique nuances of each one. Our intention in this post will be to propose an implementation based on Amazon EC2 using the <a class="af nc" href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2/service-resource/create_instances.html" rel="noopener ugc nofollow" target="_blank">create_instances</a> command of the <a class="af nc" href="https://boto3.amazonaws.com/v1/documentation/api/latest/index.html" rel="noopener ugc nofollow" target="_blank">AWS Python SDK</a> (version 1.34.23). As in our previous post, we will begin with a simple EC2 instance creation command and gradually supplement it with additional components that will incorporate our desired management features. The <a class="af nc" href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2/service-resource/create_instances.html" rel="noopener ugc nofollow" target="_blank">create_instances</a> command supports many controls. For the purposes of our demonstration, we will focus only on the ones that are relevant to our solution. We will assume the existence of a <a class="af nc" href="https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html" rel="noopener ugc nofollow" target="_blank">default VPC</a> and an <a class="af nc" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html" rel="noopener ugc nofollow" target="_blank">IAM instance profile</a> with appropriate permissions (including access to Amazon EC2, S3, and CloudWatch services).</p><p id="dbb0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note that there are multiple ways of using Amazon EC2 to fulfill the minimal set of features that we defined. We have chosen to demonstrate one possible implementation. Please do not interpret our choice of AWS, EC2, or any details of the specific implementation we have chosen as an endorsement. The best ML training solution for you will greatly depend on the specific needs and details of your project.</p><h1 id="91a2" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">1. Create an EC2 Instance</h1><p id="d177" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">We begin with a minimal example of a single EC2 instance request. We have chosen a GPU accelerated <a class="af nc" href="https://aws.amazon.com/ec2/instance-types/g5/" rel="noopener ugc nofollow" target="_blank">g5.xlarge</a> instance type and a recent <a class="af nc" href="https://docs.aws.amazon.com/dlami/" rel="noopener ugc nofollow" target="_blank">Deep Learning AMI</a> (with an Ubuntu 20.4 operating system).</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="73be" class="pe oa fq pb b bg pf pg l ph pi">import boto3<br/><br/>region = 'us-east-1'<br/>job_id = 'my-experiment' # replace with unique id<br/>num_instances = 1<br/>image_id = 'ami-0240b7264c1c9e6a9' # replace with image of choice<br/>instance_type = 'g5.xlarge' # replace with instance of choice<br/><br/>ec2 = boto3.resource('ec2', region_name=region)<br/><br/>instances = ec2.create_instances(<br/>    MaxCount=num_instances,<br/>    MinCount=num_instances,<br/>    ImageId=image_id,<br/>    InstanceType=instance_type,<br/>)</span></pre><h1 id="af29" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">2. Auto-start Training</h1><p id="75d4" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The first enhancement we would like to apply is for our training workload to automatically start as soon as our instance is up and running, without any need for manual intervention. Towards this goal, we will utilize the <em class="pj">UserData </em>argument of the <a class="af nc" href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2/service-resource/create_instances.html" rel="noopener ugc nofollow" target="_blank">create_instances</a> API that enables you to <a class="af nc" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html" rel="noopener ugc nofollow" target="_blank">specify what to run at launch</a>. In the code block below, we propose a sequence of commands that sets up the training environment (i.e., updates the <em class="pj">PATH </em>environment variable to point to the prebuilt PyTorch environment included in our image), downloads our training code from <a class="af nc" href="https://aws.amazon.com/s3/" rel="noopener ugc nofollow" target="_blank">Amazon S3</a>, installs the project dependencies, runs the training script, and syncs the output artifacts to persistent S3 storage. The demonstration assumes that the training code has already been created and uploaded to the cloud and that it contains two files: a requirements file (<em class="pj">requirements.txt</em>) and a stand-alone training script (<em class="pj">train.py</em>). In practice, the precise contents of the startup sequence will depend on the project. We include a pointer to our predefined IAM instance profile which is required for accessing S3.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="4bff" class="pe oa fq pb b bg pf pg l ph pi">import boto3<br/><br/>region = 'us-east-1'<br/>job_id = 'my-experiment' # replace with unique id<br/>num_instances = 1<br/>image_id = 'ami-0240b7264c1c9e6a9' # replace with image of choice<br/>instance_type = 'g5.xlarge' # replace with instance of choice<br/>instance_profile_arn = 'instance-profile-arn' # replace with profile arn<br/><br/>ec2 = boto3.resource('ec2', region_name=region)<br/><br/>script = """#!/bin/bash<br/>         # environment setup<br/>         export PATH=/opt/conda/envs/pytorch/bin/python:$PATH<br/><br/>         # download and unpack code<br/>         aws s3 cp s3://my-s3-path/my-code.tar .<br/>         tar -xvf my-code.tar<br/><br/>         # install dependencies<br/>         python3 -m pip install -r requirements.txt<br/> <br/>         # run training workload<br/>         python3 train.py<br/><br/>         # sync output artifacts<br/>         aws s3 sync artifacts s3://my-s3-path/artifacts<br/>         """<br/><br/>instances = ec2.create_instances(<br/>    MaxCount=num_instances,<br/>    MinCount=num_instances,<br/>    ImageId=image_id,<br/>    InstanceType=instance_type,<br/>    IamInstanceProfile={'Arn':instance_profile_arn},<br/>    UserData=script<br/> )</span></pre><p id="0e89" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note that the script above syncs the training artifacts only at the end of training. A more fault-tolerant solution would sync intermediate model checkpoints throughout the training job.</p><h1 id="fbe6" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">3. Self-destruct on Completion</h1><p id="0993" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">When you train using a managed service, your instances are automatically shut down as soon as your script completes to ensure that you only pay for what you need. In the code block below, we append a self-destruction command to the end of our <em class="pj">UserData </em>script. We do this using the AWS CLI <a class="af nc" href="https://docs.aws.amazon.com/cli/latest/reference/ec2/terminate-instances.html" rel="noopener ugc nofollow" target="_blank">terminate-instances</a> command. The command requires that we know the <em class="pj">instance-id </em>and the hosting <em class="pj">region</em> of our instance which we extract from the <a class="af nc" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html" rel="noopener ugc nofollow" target="_blank">instance metadata</a>. Our updated script assumes that our IAM instance profile has appropriate instance-termination authorization.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="933b" class="pe oa fq pb b bg pf pg l ph pi">script = """#!/bin/bash<br/>         # environment setup<br/>         TOKEN=$(curl -s -X PUT "http://169.254.169.254/latest/api/token" -H \<br/>                              "X-aws-ec2-metadata-token-ttl-seconds: 21600")<br/>         INST_MD=http://169.254.169.254/latest/meta-data<br/>         CURL_FLAGS="-H \"X-aws-ec2-metadata-token: ${TOKEN}\" -s"<br/>         INSTANCE_ID=$(curl $CURL_FLAGS $INST_MD/instance-id)<br/>         REGION=$(curl $CURL_FLAGS $INST_MD/placement/region)<br/>         export PATH=/opt/conda/envs/pytorch/bin/python:$PATH<br/>         <br/>         # download and unpack code<br/>         aws s3 cp s3://my-s3-path/my-code.tar .<br/>         tar -xvf my-code.tar<br/>         <br/>         # install dependencies<br/>         python3 -m pip install -r requirements.txt<br/>     <br/>         # run training workload<br/>         python3 train.py<br/>         <br/>         # sync output artifacts<br/>         aws s3 sync artifacts s3://my-s3-path/artifacts<br/><br/>         # self-destruct<br/>         aws ec2 terminate-instances --instance-ids $INSTANCE_ID \<br/>                                     --region $REGION<br/>         """</span></pre><p id="30e7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We highly recommend introducing additional mechanisms for verifying appropriate instance deletion to avoid the possibility of having unused (“orphan”) instances in the system racking up unnecessary costs. In a <a class="af nc" href="https://medium.com/towards-data-science/using-server-less-functions-to-govern-and-monitor-cloud-based-training-experiments-755c43fba26b" rel="noopener">recent post</a> we showed how serverless functions can be used to address this kind of problem.</p><h1 id="ca84" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">4. Apply Custom Tags to EC2 Instances</h1><p id="11a3" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Amazon EC2 enables you to apply custom metadata to your instance <a class="af nc" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html" rel="noopener ugc nofollow" target="_blank">using EC2 instance tags</a>. This enables you to pass information to the instance regarding the training workload and/or the training environment. Here, we use the <em class="pj">TagSpecifications</em> setting to pass in an instance name and a unique training job id. We use the unique id to define a dedicated S3 path for our job artifacts. Note that we need to explicitly enable the instance to access the metadata tags via the <em class="pj">MetadataOptions</em> setting.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="8bf2" class="pe oa fq pb b bg pf pg l ph pi">import boto3<br/><br/>region = 'us-east-1'<br/>job_id = 'my-experiment' # replace with unique id<br/>num_instances = 1<br/>image_id = 'ami-0240b7264c1c9e6a9' # replace with image of choice<br/>instance_type = 'g5.xlarge' # replace with instance of choice<br/>instance_profile_arn = 'instance-profile-arn' # replace with profile arn<br/><br/>ec2 = boto3.resource('ec2', region_name=region)<br/><br/>script = """#!/bin/bash<br/>         # environment setup<br/>         TOKEN=$(curl -s -X PUT "http://169.254.169.254/latest/api/token" -H \<br/>                              "X-aws-ec2-metadata-token-ttl-seconds: 21600")<br/>         INST_MD=http://169.254.169.254/latest/meta-data<br/>         CURL_FLAGS="-H \"X-aws-ec2-metadata-token: ${TOKEN}\" -s"<br/>         INSTANCE_ID=$(curl $CURL_FLAGS $INST_MD/instance-id)<br/>         REGION=$(curl $CURL_FLAGS $INST_MD/placement/region)<br/>         JOB_ID=$(curl $CURL_FLAGS $INST_MD/tags/instance/JOB_ID)<br/>         export PATH=/opt/conda/envs/pytorch/bin/python:$PATH<br/><br/>         # download and unpack code<br/>         aws s3 cp s3://my-s3-path/$JOB_ID/my-code.tar .<br/>         tar -xvf my-code.tar<br/>         <br/>         # install dependencies<br/>         python3 -m pip install -r requirements.txt<br/>     <br/>         # run training workload<br/>         python3 train.py<br/>         <br/>         # sync output artifacts<br/>         aws s3 sync artifacts s3://my-s3-path/$JOB_ID/artifacts<br/><br/>         # self-destruct<br/>         aws ec2 terminate-instances --instance-ids $INSTANCE_ID \<br/>                                     --region $REGION<br/>         """<br/><br/>instances = ec2.create_instances(<br/>    MaxCount=num_instances,<br/>    MinCount=num_instances,<br/>    ImageId=image_id,<br/>    InstanceType=instance_type,<br/>    IamInstanceProfile={'Arn':instance_profile_arn},<br/>    UserData=script,<br/>    MetadataOptions={"InstanceMetadataTags":"enabled"},<br/>    TagSpecifications=[{<br/>        'ResourceType': 'instance',<br/>        'Tags': [<br/>            {'Key': 'NAME', 'Value': 'test-vm'},<br/>            {'Key': 'JOB_ID', 'Value': f'{job_id}'}<br/>        ]<br/>    }],<br/>)</span></pre><p id="0581" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Using metadata tags to pass information to our instances will be particularly useful in the next sections.</p><h1 id="6234" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">5. Write Application Logs to Persistent Storage</h1><p id="ec22" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Naturally, we require the ability to analyze our application’s output logs both during and after training. This requires that they be periodically synced to persistent logging. In this post we implement this using <a class="af nc" href="https://docs.aws.amazon.com/cloudwatch/" rel="noopener ugc nofollow" target="_blank">Amazon CloudWatch</a>. Below we define a minimum JSON configuration file for enabling CloudWatch log collection which we add to our source code tar-ball as <em class="pj">cw_config.json</em>. Please see the official documentation for details on <a class="af nc" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-Configuration-File-Details.html" rel="noopener ugc nofollow" target="_blank">CloudWatch setup and configuration</a>.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="3945" class="pe oa fq pb b bg pf pg l ph pi">{<br/>    "logs": {<br/>        "logs_collected": {<br/>            "files": {<br/>                "collect_list": [<br/>                    {<br/>                        "file_path": "/output.log",<br/>                        "log_group_name": "/aws/ec2/training-jobs",<br/>                        "log_stream_name": "job-id"<br/>                    }<br/>                ]<br/>            }<br/>        }<br/>    }<br/>}</span></pre><p id="4df9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In practice, we would like the <em class="pj">log_stream_name </em>to uniquely identify the training job. Towards that end, we use the <a class="af nc" href="https://www.gnu.org/software/sed/manual/sed.html" rel="noopener ugc nofollow" target="_blank">sed</a> command to replace the generic “job-id” string with the job id metadata tag from the previous section. Our enhanced script also includes the CloudWatch start up command and modifications for piping the standard output to the designated <em class="pj">output.log</em> defined in the CloudWatch config file.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="1aa7" class="pe oa fq pb b bg pf pg l ph pi">script = """#!/bin/bash<br/>         # environment setup<br/>         TOKEN=$(curl -s -X PUT "http://169.254.169.254/latest/api/token" -H \<br/>                              "X-aws-ec2-metadata-token-ttl-seconds: 21600")<br/>         INST_MD=http://169.254.169.254/latest/meta-data<br/>         CURL_FLAGS="-H \"X-aws-ec2-metadata-token: ${TOKEN}\" -s"<br/>         INSTANCE_ID=$(curl $CURL_FLAGS $INST_MD/instance-id)<br/>         REGION=$(curl $CURL_FLAGS $INST_MD/placement/region)<br/>         JOB_ID=$(curl $CURL_FLAGS $INST_MD/tags/instance/JOB_ID)<br/>         export PATH=/opt/conda/envs/pytorch/bin/python:$PATH<br/><br/>         # download and unpack code<br/>         aws s3 cp s3://my-s3-path/$JOB_ID/my-code.tar .<br/>         tar -xvf my-code.tar<br/><br/>         # configure cloudwatch<br/>         sed -i "s/job-id/${JOB_ID}/g" cw_config.json<br/>         /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl \<br/>                  -a fetch-config -m ec2 -c file:cw_config.json -s<br/>         <br/>         # install dependencies<br/>         python3 -m pip install -r requirements.txt 2&gt;&amp;1 | tee -a output.log<br/>     <br/>         # run training workload<br/>         python3 train.py 2&gt;&amp;1 | tee -a output.log<br/>         <br/>         # sync output artifacts<br/>         aws s3 sync artifacts s3://my-s3-path/$JOB_ID/artifacts<br/><br/>         # self-destruct<br/>         aws ec2 terminate-instances --instance-ids $INSTANCE_ID \<br/>                                     --region $REGION<br/>         """</span></pre><h1 id="2b3d" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">6. Support Multi-node Training</h1><p id="3302" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Nowadays, it is quite common for training jobs to run on multiple nodes in parallel. Modifying our instance request code to support multiple nodes is a simple matter of modifying the <em class="pj">num_instances</em> setting. The challenge is how to configure the environment in a manner that supports distributed training, i.e., a manner that enables — and optimizes — the transfer of data between the instances.</p><p id="1cc3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To minimize the network latency between the instances and maximize throughput, we add a pointer to a predefined <a class="af nc" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster" rel="noopener ugc nofollow" target="_blank">cluster placement group</a> in the <em class="pj">Placement </em>field of our ec2 instance request. The following command line demonstrates the creation of a cluster placement group.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="f1f2" class="pe oa fq pb b bg pf pg l ph pi">aws ec2 create-placement-group --group-name cluster-placement-group \<br/>    --strategy cluster</span></pre><p id="3bfc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For our instances to communicate with one another, they need to be aware of each other’s presence. In this post we will demonstrate a minimal environment configuration required for running <a class="af nc" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" rel="noopener ugc nofollow" target="_blank">data parallel training</a> in <a class="af nc" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">PyTorch</a>. For PyTorch <a class="af nc" href="https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel" rel="noopener ugc nofollow" target="_blank">DistributedDataParallel</a> (DDP), each instance needs to know the IP of the master node, the master port, the total number of instances, and its serial <em class="pj">rank</em> amongst all of the nodes. The script below demonstrates the configuration of a <a class="af nc" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" rel="noopener ugc nofollow" target="_blank">data parallel training</a> job using the environment variables <em class="pj">MASTER_ADDR</em>, <em class="pj">MASTER_PORT</em>, <em class="pj">NUM_NODES</em>, and <em class="pj">NODE_RANK</em>.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="3d43" class="pe oa fq pb b bg pf pg l ph pi">import os, ast, socket<br/>import torch<br/>import torch.distributed as dist<br/>import torch.multiprocessing as mp<br/><br/>def mp_fn(local_rank, *args):<br/>    # discover topology settings<br/>    num_nodes = int(os.environ.get('NUM_NODES',1))<br/>    node_rank = int(os.environ.get('NODE_RANK',0))<br/>    gpus_per_node = torch.cuda.device_count()<br/>    world_size = num_nodes * gpus_per_node<br/>    node_rank = nodes.index(socket.gethostname())<br/>    global_rank = (node_rank * gpus_per_node) + local_rank<br/>    print(f'local rank {local_rank} '<br/>          f'global rank {global_rank} '<br/>          f'world size {world_size}')<br/>    # init_process_group assumes the existence of MASTER_ADDR<br/>    # and MASTER_PORT environment variables<br/>    dist.init_process_group(backend='nccl',<br/>                            rank=global_rank, <br/>                            world_size=world_size)<br/>    torch.cuda.set_device(local_rank)<br/>    # Add training logic<br/><br/>if __name__ == '__main__':<br/>    mp.spawn(mp_fn,<br/>             args=(),<br/>             nprocs=torch.cuda.device_count())</span></pre><p id="0e47" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The node rank can be retrieved from the <a class="af nc" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMI-launch-index-examples.html" rel="noopener ugc nofollow" target="_blank">ami-launch-index</a>. The number of nodes and the master port are known at the time of <a class="af nc" href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2/service-resource/create_instances.html" rel="noopener ugc nofollow" target="_blank">create_instances</a> invocation and can be passed in as EC2 instance tags. However, the IP address of the master node is only determined once the master instance is created and can only be communicated to the instances following the <a class="af nc" href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2/service-resource/create_instances.html" rel="noopener ugc nofollow" target="_blank">create_instances</a> call. In the code block below, we chose to pass the master address to each of the instances using a dedicated call to the <a class="af nc" href="https://boto3.amazonaws.com/v1/documentation/api/latest/index.html" rel="noopener ugc nofollow" target="_blank">AWS Python SDK</a> <a class="af nc" href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2/client/create_tags.html" rel="noopener ugc nofollow" target="_blank">create_tags</a> API. We use the same call to update the name tag of each instance according to its launch-index value.</p><p id="9945" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The full solution for multi-node training appears below:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="514c" class="pe oa fq pb b bg pf pg l ph pi">import boto3<br/><br/>region = 'us-east-1'<br/>job_id = 'my-multinode-experiment' # replace with unique id<br/>num_instances = 4<br/>image_id = 'ami-0240b7264c1c9e6a9' # replace with image of choice<br/>instance_type = 'g5.xlarge' # replace with instance of choice<br/>instance_profile_arn = 'instance-profile-arn' # replace with profile arn<br/>placement_group = 'cluster-placement-group' # replace with placement group<br/><br/>ec2 = boto3.resource('ec2', region_name=region)<br/><br/>script = """#!/bin/bash<br/>         # environment setup<br/>         TOKEN=$(curl -s -X PUT "http://169.254.169.254/latest/api/token" -H \<br/>                              "X-aws-ec2-metadata-token-ttl-seconds: 21600")<br/>         INST_MD=http://169.254.169.254/latest/meta-data<br/>         CURL_FLAGS="-H \"X-aws-ec2-metadata-token: ${TOKEN}\" -s"<br/>         INSTANCE_ID=$(curl $CURL_FLAGS $INST_MD/instance-id)<br/>         REGION=$(curl $CURL_FLAGS $INST_MD/placement/region)<br/>         JOB_ID=$(curl $CURL_FLAGS $INST_MD/tags/instance/JOB_ID)<br/>         export NODE_RANK=$(curl $CURL_FLAGS $INST_MD/ami-launch-index)<br/>         export NUM_NODES=$(curl $CURL_FLAGS $INST_MD/NUM_NODES)<br/>         export MASTER_PORT=$(curl $CURL_FLAGS $INST_MD/tags/instance/MASTER_PORT)<br/>         export PATH=/opt/conda/envs/pytorch/bin/python:$PATH         <br/>         <br/>         # download and unpack code<br/>         aws s3 cp s3://my-s3-path/$JOB_ID/my-code.tar .<br/>         tar -xvf my-code.tar<br/>         <br/>         # configure cloudwatch<br/>         sed -i "s/job-id/${JOB_ID}_${NODE_RANK}/g" cw_config.json<br/>         /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl \<br/>                  -a fetch-config -m ec2 -c file:cw_config.json -s<br/>         <br/>         # install dependencies<br/>         python3 -m pip install -r requirements.txt 2&gt;&amp;1 | tee -a output.log<br/>         <br/>         # retrieve master address<br/>         # should be available but just in case tag application is delayed...<br/>         while true; do<br/>           export MASTER_ADDR=$(curl $CURL_FLAGS $INST_MD/tags/instance/MASTER_ADDR)<br/>           if [[ $MASTER_ADDR == "&lt;?xml"* ]]; then<br/>             echo 'tags missing, sleep for 5 seconds' 2&gt;&amp;1 | tee -a output.log<br/>             sleep 5<br/>           else<br/>             break<br/>           fi<br/>         done<br/>         <br/>         # run training workload<br/>         python3 train.py 2&gt;&amp;1 | tee -a output.log<br/>         <br/>         # sync output artifacts<br/>         aws s3 sync artifacts s3://my-s3-path/$JOB_ID/artifacts<br/>         <br/>         # self-destruct<br/>         aws ec2 terminate-instances --instance-ids $INSTANCE_ID \<br/>                                     --region $REGION<br/>         """<br/><br/>instances = ec2.create_instances(<br/>    MaxCount=num_instances,<br/>    MinCount=num_instances,<br/>    ImageId=image_id,<br/>    InstanceType=instance_type,<br/>    IamInstanceProfile={'Arn':instance_profile_arn},<br/>    UserData=script,<br/>    MetadataOptions={"InstanceMetadataTags":"enabled"},<br/>    TagSpecifications=[{<br/>        'ResourceType': 'instance',<br/>        'Tags': [<br/>            {'Key': 'NAME', 'Value': 'test-vm'},<br/>            {'Key': 'JOB_ID', 'Value': f'{job_id}'},<br/>            {'Key': 'MASTER_PORT', 'Value': '7777'},<br/>            {'Key': 'NUM_NODES', 'Value': f'{num_instances}'}<br/>        ]<br/>    }],<br/>    Placement={'GroupName': placement_group}<br/>)<br/><br/><br/>if num_instances &gt; 1:<br/><br/>    # find master_addr<br/>    for inst in instances:<br/>        if inst.ami_launch_index == 0:<br/>            master_addr = inst.network_interfaces_attribute[0]['PrivateIpAddress']<br/>            break<br/><br/>    # update ec2 tags<br/>    for inst in instances:<br/>        res = ec2.create_tags(<br/>            Resources=[inst.id],<br/>            Tags=[<br/>                {'Key': 'NAME', 'Value': f'test-vm-{inst.ami_launch_index}'},<br/>                {'Key': 'MASTER_ADDR', 'Value': f'{master_addr}'}]<br/>        )</span></pre><h1 id="c08c" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">7. Support Spot Instance Usage</h1><p id="54bd" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">A popular way of reducing training costs is to use discounted <a class="af nc" href="https://aws.amazon.com/ec2/spot/?cards.sort-by=item.additionalFields.startDateTime&amp;cards.sort-order=asc" rel="noopener ugc nofollow" target="_blank">Amazon EC2 Spot Instances</a>. Utilizing Spot instances effectively requires that you implement a way of detecting interruptions (e.g., by listening for <a class="af nc" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-instance-termination-notices.html" rel="noopener ugc nofollow" target="_blank">termination notices</a>) and taking the appropriate action (e.g., resuming incomplete workloads). Below, we show how to modify our script to use Spot instances using the <em class="pj">InstanceMarketOptions</em> API setting.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="b14f" class="pe oa fq pb b bg pf pg l ph pi">import boto3<br/><br/>region = 'us-east-1'<br/>job_id = 'my-spot-experiment' # replace with unique id<br/>num_instances = 1<br/>image_id = 'ami-0240b7264c1c9e6a9' # replace with image of choice<br/>instance_type = 'g5.xlarge' # replace with instance of choice<br/>instance_profile_arn = 'instance-profile-arn' # replace with profile arn<br/>placement_group = 'cluster-placement-group' # replace with placement group<br/><br/>instances = ec2.create_instances(<br/>    MaxCount=num_instances,<br/>    MinCount=num_instances,<br/>    ImageId=image_id,<br/>    InstanceType=instance_type,<br/>    IamInstanceProfile={'Arn':instance_profile_arn},<br/>    UserData=script,<br/>    MetadataOptions={"InstanceMetadataTags":"enabled"},<br/>    TagSpecifications=[{<br/>        'ResourceType': 'instance',<br/>        'Tags': [<br/>            {'Key': 'NAME', 'Value': 'test-vm'},<br/>            {'Key': 'JOB_ID', 'Value': f'{job_id}'},<br/>        ]<br/>    }],<br/>    InstanceMarketOptions = {<br/>        'MarketType': 'spot',<br/>        'SpotOptions': {<br/>            "SpotInstanceType": "one-time",<br/>            "InstanceInterruptionBehavior": "terminate"<br/>        }<br/>    }<br/>)</span></pre><p id="00a7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Please see our previous posts (e.g., <a class="af nc" rel="noopener" target="_blank" href="/a-simple-solution-for-managing-cloud-based-ml-training-c80a69c6939a">here</a> and <a class="af nc" rel="noopener" target="_blank" href="/using-server-less-functions-to-govern-and-monitor-cloud-based-training-experiments-755c43fba26b">here</a>) for some ideas for how to implement a solution for Spot instance life-cycle management.</p><h1 id="a1da" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Summary</h1><p id="a182" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Managed cloud services for AI development can simplify model training and lower the entry bar for potential incumbents. However, there are some situations where greater control over the training process is required. In this post we have illustrated one approach to building a customized managed training environment on top of Amazon EC2. Of course, the precise details of the solution will greatly depend on the specific needs of the projects at hand.</p><p id="1c7a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As always, please feel free to respond to this post with comments, questions, or corrections.</p></div></div></div></div>    
</body>
</html>