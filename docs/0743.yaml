- en: Large Language Models Just Got A Whole Lot Smaller
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2?source=collection_archive---------1-----------------------#2024-03-20](https://towardsdatascience.com/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2?source=collection_archive---------1-----------------------#2024-03-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And how this might change the game for software startups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://arijoury.medium.com/?source=post_page---byline--f93425ee59a2--------------------------------)[![Ari
    Joury, PhD](../Images/5b9e49279fb3f26373b393f29a4daaf7.png)](https://arijoury.medium.com/?source=post_page---byline--f93425ee59a2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f93425ee59a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f93425ee59a2--------------------------------)
    [Ari Joury, PhD](https://arijoury.medium.com/?source=post_page---byline--f93425ee59a2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f93425ee59a2--------------------------------)
    ·15 min read·Mar 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/afd673415e30151b37e539a583a1e1f5.png)'
  prefs: []
  type: TYPE_IMG
- en: LLMs are getting smaller and more efficient! Image inspired by [Benjamin Marie](/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598)
  prefs: []
  type: TYPE_NORMAL
- en: '**This piece was co-written with** [**David Meiborg**](https://medium.com/u/8523370997f4?source=post_page---user_mention--f93425ee59a2--------------------------------)**.**'
  prefs: []
  type: TYPE_NORMAL
- en: '*TLDR: Large Language Models (LLMs for short) are currently huge, costly to
    run, and have a* [*significant carbon footprint*](https://arxiv.org/abs/2309.14393)*.
    Recent advancements in model compression and system-level optimization methods
    might, however, enhance LLM inference. In particular, an approach using parameters
    with ternary structure has the potential of circumventing much of the costly matrix
    multiplication that is standard today. This has exciting consequences for hardware
    startups making specialized chips, but also for software startups that use or
    custom-build their own LLMs. Startups that help their customers deploy LLMs might
    also have more business coming for them.*'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models today are big. Like, really big. If you want to load a
    LlaMa-2–70B model, you’d need 140 GB of VRAM (that’s 70 billion parameters multiplied
    by 2 bytes per parameter). For comparison, GPUs like the NVIDIA RTX 3090 or 4090,
    have just 24 GB of VRAM — a fraction of what one would need.
  prefs: []
  type: TYPE_NORMAL
- en: There are some [workarounds with quantization](/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598),
    but these tend to be cumbersome. Likely you will still have your GPU running hot
    for up to 15 hours until the model is loaded. Not to mention that you still…
  prefs: []
  type: TYPE_NORMAL
