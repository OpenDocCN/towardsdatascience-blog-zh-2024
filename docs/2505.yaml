- en: PyTorch Optimizers Aren’t Fast Enough. Try These Instead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pytorch-optimizers-arent-fast-enough-try-these-instead-61a1350e3eac?source=collection_archive---------1-----------------------#2024-10-14](https://towardsdatascience.com/pytorch-optimizers-arent-fast-enough-try-these-instead-61a1350e3eac?source=collection_archive---------1-----------------------#2024-10-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These 4 advanced optimizers will open your mind.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@benjybo7?source=post_page---byline--61a1350e3eac--------------------------------)[![Benjamin
    Bodner](../Images/f66d5ba5b80455f8a0cd198499e8f4c2.png)](https://medium.com/@benjybo7?source=post_page---byline--61a1350e3eac--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--61a1350e3eac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--61a1350e3eac--------------------------------)
    [Benjamin Bodner](https://medium.com/@benjybo7?source=post_page---byline--61a1350e3eac--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--61a1350e3eac--------------------------------)
    ·10 min read·Oct 14, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bdf3ef0a7f55b35562b754c50d2e50b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: image by author'
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve been working with deep learning for a while, you’re probably well-acquainted
    with the usual optimizers in PyTorch — `[SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)`,
    `[Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)`, maybe
    even `[AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)`.
    *These are some of the go-to tools in every ML engineer’s toolkit.*
  prefs: []
  type: TYPE_NORMAL
- en: But what if I told you that there are pleanty of powerful optimization algorithms
    out there, which aren’t part of the standard PyTorch package?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not just that, the algorithms can sometimes **outperform Adam** for certain
    tasks and help you crack tough optimization problems you’ve been struggling with!
  prefs: []
  type: TYPE_NORMAL
- en: If that got your attention, great!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this article, we’ll take a look at some ***advanced optimization techniques***
    that you may or may not have heard of and see **how we can apply them to deep
    learning.**
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, We’ll be talking about Sequential Least Squares Programming`[SLSQP](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html)`,
    Particle Swarm Optimization `[PSO](https://pyswarms.readthedocs.io/en/latest/)`,
    Covariant Matrix Adaptation Evolution Strategy`[CMA-ES](https://arxiv.org/pdf/1604.00772)`,
    and Simulated Annealing `[SA](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.dual_annealing.html#scipy.optimize.dual_annealing)`.
  prefs: []
  type: TYPE_NORMAL
- en: Why use these algorithms?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several key advantages:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
