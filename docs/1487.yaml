- en: Understanding Buffer of Thoughts (BoT) — Reasoning with Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-buffer-of-thoughts-bot-reasoning-with-large-language-models-391919d2f76f?source=collection_archive---------1-----------------------#2024-06-14](https://towardsdatascience.com/understanding-buffer-of-thoughts-bot-reasoning-with-large-language-models-391919d2f76f?source=collection_archive---------1-----------------------#2024-06-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A new prompting tool for complex reasoning, compared with Chain of thought (CoT)
    and Tree of Thought (ToT)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--391919d2f76f--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--391919d2f76f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--391919d2f76f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--391919d2f76f--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--391919d2f76f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--391919d2f76f--------------------------------)
    ·10 min read·Jun 14, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: If you’re not a member, [**read for free!**](https://hesamsheikh.substack.com/)
    **✨**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/1dd8cf331a7b47649dd087344982ff56.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Etienne Girardet](https://unsplash.com/@etiennegirardet?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Squeezing proficiency in complex reasoning tasks and avoiding hallucinations
    remains a major research topic in Large Language Models (LLMs). Despite the effort,
    LLMs need help with generalized reasoning capabilities. Traditional methods such
    as **Chain-of-Thought (CoT)** or **Tree-of-Thought (ToT)** often require multiple
    assumptions or numerous back-and-forth prompting which means intensive computation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/acb9be520c3976344727f77d1cbdfb81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Buffer of Thoughts (BoT) vs other prompting methods. (source: [Paper](https://arxiv.org/abs/2406.04271))'
  prefs: []
  type: TYPE_NORMAL
- en: 'The new proposed method in the paper, [**Buffer of Thoughts: Thought-Augmented
    Reasoning with Large Language Models**](https://arxiv.org/abs/2406.04271) **[1]**,
    combats these limitations with a dynamic, adaptive repository of high-level thought
    templatescalled **meta-buffer.** In BoT, once the user presents a new problem,
    it is first simplified and analyzed to extract key elements, which then guide
    the retrieval of a relevant thought template from a dynamic dataset. This allows
    adaptive and efficient problem-solving through modified and complex reasoning
    patterns. According to the original paper, this is so effective that *“****Llama3–8B+BoT***
    *has the potential to surpass* ***Llama3–70B model.”***'
  prefs: []
  type: TYPE_NORMAL
- en: '**BoT** achieves efficient reasoning across problems that are similar to its
    templates as it:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) leverages previous solutions on new challenges,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2) boosts efficiency by eliminating the need for multiple query iterations
    (as we see in Graph-of-Thoughts (GoT) or ToT), and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (3) dynamically updates its template repository to ensure it evolves as it encounters
    new tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this article, we will first go through the general outline of how BoT works,
    understand the function of each key part, and test the procedure with an example.
  prefs: []
  type: TYPE_NORMAL
- en: How does BoT work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The general thought-augmented reasoning process (as shown in the figure below)
    starts with **Problem Distillation**, which analyzes and condenses the incoming
    task into essential elements and constraints and then creates a simplified problem
    statement.
  prefs: []
  type: TYPE_NORMAL
- en: This distilled information is then used to query the **Meta-Buffer**, a dynamic
    repository that contains high-level thought templates. From the thought templates,
    one that is most similar to the distilled problem is retrieved. Then, during the
    **Instantiation Process**, it is instantiated with specific requirements and information
    about the distilled problem.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this process, the **Buffer Manager** actively keeps an eye on the
    **Meta-Buffer.** Once it detects a new insight not included in the meta-buffer,
    **Buffer Manager** updates it to ensure a continual evolution of the thought template
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32292bd0be51f32c6798de545021c314.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The BoT process. (source: [Paper](http://Paper))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through each of these key parts to gain a more detailed look:'
  prefs: []
  type: TYPE_NORMAL
- en: Problem Distiller
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem Distiller can be thought of as a **preprocess** on the input tasks in
    order to…
  prefs: []
  type: TYPE_NORMAL
- en: (1) extract essential information of the problem, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2) simplify complex tasks for a better search and retrieval of thought templates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Problem Distiller takes the burden off of LLM to identify and extract vital
    information and constraints of the problem. This is done by a meta prompt ϕ:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/944bc53eb01b77fdf82f8608b1a19b46.png)'
  prefs: []
  type: TYPE_IMG
- en: '(source: [Paper](http://Paper))'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt used by the authors to distill key information about a task is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Meta-Buffer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The meta-buffer is a central database that stores high-level thought templates.
    These templates are high-level abstractions representing various problem-solving
    processes. The idea is that LLM can leverage past problems and insights to solve
    current challenges. The best part is that the Meta-Buffer dynamically updates
    to ensure new unseen problems are also included. The Meta-Buffer doesn’t enforce
    thought templates to follow a specific instruction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Template Retrieval**: Once a task is distilled, BoT would go through the
    thought templates and grab the one most similar to the task. This is done by calculating
    the embedding similarity between the task and the thought templates.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3877ad60c789b210ee2cd5f42b642b33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The retriever would calculate the similarity between the embedding of the input
    task f(xd), and the embedding of templates f(DTi ). (source: [Paper](https://arxiv.org/abs/2406.04271))'
  prefs: []
  type: TYPE_NORMAL
- en: 'the retriever would calculate the similarity between the embedding of the input
    task **f(xd)**, and the embedding of templates **f(D*Ti* ).** This is only done
    if the similarity is above a certain threshold δ (0.5–0.7). If none of the thought
    templates have a similarity score with the task above the δ threshold, then the
    **xd** is identified as a new task. Depending on if the task is new or not, one
    of the two paths would be taken:'
  prefs: []
  type: TYPE_NORMAL
- en: If the task is similar to one of the thought templates, the template would be
    instantiated with the distilled information using an instantiation prompt (which
    you can check in the paper). This instantiation process can be denoted as
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/872661a6c9986487d31d5d362f937a78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instantiated Reasoning. (source: [Paper](https://arxiv.org/abs/2406.04271))'
  prefs: []
  type: TYPE_NORMAL
- en: If the task is new, a general thought template that is designed to address a
    broad set of problems is used. As the task is processed, the **Buffer Manager**
    observes and learns and potentially creates a new, more specific thought template
    and pushes it to the meta-buffer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Buffer Manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Buffer Manager serves as a crucial player in maintaining and enhancing the
    meta-buffer. Based on the new insights and outcomes from the tasks that are solved,
    it updates the thought templates. Also, whenever a new or drastically different
    problem is solved, the buffer manager assesses whether or not to create a new
    thought template. This is to ensure thought templates remain to the point and
    are not redundant.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64359637991b99ea155bceb95391d9d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'checking whether or not a newly generated template is similar to the existing
    ones. (source: Paper)'
  prefs: []
  type: TYPE_NORMAL
- en: By employing the above formulation, the Buffer Manager checks whether or not
    the meta-buffer already has the necessary knowledge to tackle a problem.
  prefs: []
  type: TYPE_NORMAL
- en: BoT vs. Single-Query vs. Multi-Query
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How does the BoT stand out compared to previous methods? The authors of the
    paper evaluate various methods on different datasets of various tasks, such as
    Data Understanding, Python Programming Puzzles, Multilingual Grade School Math(MGSM),
    etc. The results show a surprising advantage of **BoT** in almost all the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/498b49fc3dd24dbe76978a7fb2099b3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'BoT vs previous methods. The best results (marked in blue) are all achieved
    by BoT. (source: [Paper](https://arxiv.org/abs/2406.04271))'
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of BoT is its efficiency — requiring **only 12%**
    of the computational cost on average when compared to multi-query prompting methods.
    This high computational cost and latency of multi-query methods such as ToT often
    renders them impractical in real-world use cases.
  prefs: []
  type: TYPE_NORMAL
- en: BoT+Llama3–8B has the potential to surpass single Llama3–70B model
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/d343e715b3bfa85f8ddd6a75569f32f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'comparing the effect of BoT on Llama3–8B and 70B. Annotated. (source: [Paper](https://arxiv.org/abs/2406.04271))'
  prefs: []
  type: TYPE_NORMAL
- en: Buffer of Thoughts in Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The demo code for Buffer of Thoughts is published on [GitHub](https://github.com/YangLing0818/buffer-of-thought-llm)
    [2]. To test out the functionality in practice, I will use this method on a custom
    task: Word Reordering. In this task, the LLM must take a scrambled sentence, such
    as *“Sam name is my”*, and return a permutation of these words that is semantically
    meaningful, which in this example would be *“my name is Sam”,* (this is not a
    benchmark with baseline performance). Some examples of the scrambled sentences
    and the correct ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'I will also use the following user prompt to explain the task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Provided this task, the BoT framework first distills my task and extracts the
    goal, the restriction, and how I would like my output along with other key information.
    The full procedure of BoT in detail, is the following terminal output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]python'
  prefs: []
  type: TYPE_NORMAL
- en: 'def ReorderWords(input_sentence):'
  prefs: []
  type: TYPE_NORMAL
- en: Remove the start and end tags, and split the sentence into words
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: words = input_sentence.replace("<start>", "").replace("<end>", "").strip().split()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Tentative reordering to form a meaningful sentence
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: Based on the context provided in the task, the following reordering seems logical
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: reordered_words = ["the", "melting", "in", "solid", "metals", "is", "achieved",
    "by", "mixing", "gold", "leaf", "and", "desired", "other", "color", "gold"]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Join the words into a single sentence and add start and end tags
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: reordered_sentence = "<start> " + " ".join(reordered_words) + " <end>"
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return reordered_sentence
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Input sentence provided in the task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: input_sentence = "<start> the melting in solid to gold leaf metals is achieve
    made by desired gold and mixing color other <end>"
  prefs: []
  type: TYPE_NORMAL
- en: print(ReorderWords(input_sentence))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Some of the examples of the reordered sentences using BoT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note, that the BoT repository I used is a demo code and lacks some of the features
    mentioned in the original paper, such as a general thought template, the dynamic
    updating of the Meta-Buffer, or finding the closest template embedding to the
    user task. These are important aspects of the framework and without them, we cannot
    conclude the performance of Buffer of Thoughts in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Final Words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, BoT shows promising results in both accuracy and efficiency in
    various domains and tasks. It’s an interesting approach to breaking down a reasoning
    problem into its fundamental restrictions and key information and building on
    top of previous solutions and templates to better formulate the task for an LLM
    to understand.
  prefs: []
  type: TYPE_NORMAL
- en: By addressing some of the limitations in other prompting techniques, Buffer
    of Thoughts allows LLM to have more complex thinking patterns, potentially making
    smaller lightweight models perform at the level of larger ones.
  prefs: []
  type: TYPE_NORMAL
- en: Allowing small LLMs to achieve results close to large LLMs is a key topic addressed
    in many of the current research papers. The goal is to employ various prompting
    and fine-tuning techniques to extract accurate AI outputs with low computation
    and cost.
  prefs: []
  type: TYPE_NORMAL
- en: Buffer of Thoughts is a novel and promising prompting framework that leverages
    a domain of techniques to guide LLM step by step, in a reasoning process. A complete
    practical implementation of the Buffer of Thoughts technique is yet to come, but
    in the meanwhile, test out the provided benchmarks in the demo GitHub repository
    [2].
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have made it this far, consider this for a **further read**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/platonic-representation-hypothesis-c812813d7248?source=post_page-----391919d2f76f--------------------------------)
    [## Platonic Representation: Are AI Deep Network Models Converging?'
  prefs: []
  type: TYPE_NORMAL
- en: Are Artificial Intelligence models evolving towards a unified representation
    of reality? The Platonic Representation…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/platonic-representation-hypothesis-c812813d7248?source=post_page-----391919d2f76f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**🌟 Join +1000 people learning about** Python, ML / MLOps / AI, Data Science,
    and LLM. [**follow me**](https://medium.com/@itshesamsheikh/subscribe)and check
    out my [**X/Twitter**](https://twitter.com/itsHesamSheikh), where I keep you updated
    Daily**.**'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading,
  prefs: []
  type: TYPE_NORMAL
- en: — Hesam
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Yang, L., Yu, Z., Zhang, T., Cao, S., Xu, M., Zhang, W., Gonzalez, J. E.,
    & Cui, B. (2024). Buffer of Thoughts: Thought-Augmented Reasoning with Large Language
    Models. *arXiv*. Retrieved from [https://arxiv.org/abs/2406.04271](https://arxiv.org/abs/2406.04271)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] buffer-of-thought-llm, [https://github.com/YangLing0818/buffer-of-thought-llm](https://github.com/YangLing0818/buffer-of-thought-llm)'
  prefs: []
  type: TYPE_NORMAL
