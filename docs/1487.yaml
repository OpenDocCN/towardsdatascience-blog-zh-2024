- en: Understanding Buffer of Thoughts (BoT) â€” Reasoning with Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è§£æ€ç»´ç¼“å†²åŒºï¼ˆBoTï¼‰â€”â€”ä¸å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/understanding-buffer-of-thoughts-bot-reasoning-with-large-language-models-391919d2f76f?source=collection_archive---------1-----------------------#2024-06-14](https://towardsdatascience.com/understanding-buffer-of-thoughts-bot-reasoning-with-large-language-models-391919d2f76f?source=collection_archive---------1-----------------------#2024-06-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/understanding-buffer-of-thoughts-bot-reasoning-with-large-language-models-391919d2f76f?source=collection_archive---------1-----------------------#2024-06-14](https://towardsdatascience.com/understanding-buffer-of-thoughts-bot-reasoning-with-large-language-models-391919d2f76f?source=collection_archive---------1-----------------------#2024-06-14)
- en: A new prompting tool for complex reasoning, compared with Chain of thought (CoT)
    and Tree of Thought (ToT)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€ç§ç”¨äºå¤æ‚æ¨ç†çš„æ–°æç¤ºå·¥å…·ï¼Œä¸æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œæ€ç»´æ ‘ï¼ˆToTï¼‰è¿›è¡Œæ¯”è¾ƒ
- en: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--391919d2f76f--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--391919d2f76f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--391919d2f76f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--391919d2f76f--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--391919d2f76f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--391919d2f76f--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--391919d2f76f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--391919d2f76f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--391919d2f76f--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--391919d2f76f--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--391919d2f76f--------------------------------)
    Â·10 min readÂ·Jun 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--391919d2f76f--------------------------------)
    Â·é˜…è¯»æ—¶é—´ï¼š10åˆ†é’ŸÂ·2024å¹´6æœˆ14æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: If youâ€™re not a member, [**read for free!**](https://hesamsheikh.substack.com/)
    **âœ¨**
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä¸æ˜¯ä¼šå‘˜ï¼Œ[**å…è´¹é˜…è¯»ï¼**](https://hesamsheikh.substack.com/) **âœ¨**
- en: '![](../Images/1dd8cf331a7b47649dd087344982ff56.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dd8cf331a7b47649dd087344982ff56.png)'
- en: Photo by [Etienne Girardet](https://unsplash.com/@etiennegirardet?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Etienne Girardet](https://unsplash.com/@etiennegirardet?utm_source=medium&utm_medium=referral)æä¾›ï¼Œæ¥æºäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Squeezing proficiency in complex reasoning tasks and avoiding hallucinations
    remains a major research topic in Large Language Models (LLMs). Despite the effort,
    LLMs need help with generalized reasoning capabilities. Traditional methods such
    as **Chain-of-Thought (CoT)** or **Tree-of-Thought (ToT)** often require multiple
    assumptions or numerous back-and-forth prompting which means intensive computation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æå‡åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›å¹¶é¿å…å¹»è§‰ä¾ç„¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„ä¸€ä¸ªä¸»è¦ç ”ç©¶è¯¾é¢˜ã€‚å°½ç®¡è¿›è¡Œäº†å¤§é‡åŠªåŠ›ï¼ŒLLMs ä»ç„¶åœ¨å¹¿æ³›æ¨ç†èƒ½åŠ›ä¸Šéœ€è¦å¸®åŠ©ã€‚ä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚**æ€ç»´é“¾ï¼ˆCoTï¼‰**æˆ–**æ€ç»´æ ‘ï¼ˆToTï¼‰**ï¼Œé€šå¸¸éœ€è¦å¤šä¸ªå‡è®¾æˆ–å¤§é‡çš„åå¤æç¤ºï¼Œè¿™æ„å‘³ç€éœ€è¦å¼ºå¤§çš„è®¡ç®—èµ„æºã€‚
- en: '![](../Images/acb9be520c3976344727f77d1cbdfb81.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acb9be520c3976344727f77d1cbdfb81.png)'
- en: 'Buffer of Thoughts (BoT) vs other prompting methods. (source: [Paper](https://arxiv.org/abs/2406.04271))'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ€ç»´ç¼“å†²åŒºï¼ˆBoTï¼‰ä¸å…¶ä»–æç¤ºæ–¹æ³•çš„æ¯”è¾ƒã€‚ï¼ˆæ¥æºï¼š[è®ºæ–‡](https://arxiv.org/abs/2406.04271)ï¼‰
- en: 'The new proposed method in the paper, [**Buffer of Thoughts: Thought-Augmented
    Reasoning with Large Language Models**](https://arxiv.org/abs/2406.04271) **[1]**,
    combats these limitations with a dynamic, adaptive repository of high-level thought
    templatescalled **meta-buffer.** In BoT, once the user presents a new problem,
    it is first simplified and analyzed to extract key elements, which then guide
    the retrieval of a relevant thought template from a dynamic dataset. This allows
    adaptive and efficient problem-solving through modified and complex reasoning
    patterns. According to the original paper, this is so effective that *â€œ****Llama3â€“8B+BoT***
    *has the potential to surpass* ***Llama3â€“70B model.â€***'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ä¸­æå‡ºçš„æ–°çš„æ–¹æ³•ï¼Œ[**æ€ç»´ç¼“å†²åŒºï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ€ç»´å¢å¼ºæ¨ç†**](https://arxiv.org/abs/2406.04271) **[1]**ï¼Œé€šè¿‡ä¸€ä¸ªåŠ¨æ€ã€è‡ªé€‚åº”çš„é«˜å±‚æ¬¡æ€ç»´æ¨¡æ¿åº“â€”â€”**å…ƒç¼“å†²åŒºï¼ˆmeta-bufferï¼‰**ï¼Œæ¥åº”å¯¹è¿™äº›å±€é™ã€‚åœ¨BoTä¸­ï¼Œä¸€æ—¦ç”¨æˆ·æå‡ºä¸€ä¸ªæ–°é—®é¢˜ï¼Œé¦–å…ˆå¯¹é—®é¢˜è¿›è¡Œç®€åŒ–å’Œåˆ†æï¼Œä»¥æå–å…³é”®ä¿¡æ¯ï¼Œç„¶åä»åŠ¨æ€æ•°æ®é›†ä¸­æ£€ç´¢ä¸ä¹‹ç›¸å…³çš„æ€ç»´æ¨¡æ¿ã€‚è¿™ä½¿å¾—é€šè¿‡ä¿®æ”¹å’Œå¤æ‚çš„æ¨ç†æ¨¡å¼å®ç°è‡ªé€‚åº”å’Œé«˜æ•ˆçš„é—®é¢˜è§£å†³ã€‚æ ¹æ®åŸå§‹è®ºæ–‡ï¼Œæ•ˆæœå¦‚æ­¤æ˜¾è‘—ï¼Œ*â€œ****Llama3â€“8B+BoT***
    *æœ‰æ½œåŠ›è¶…è¶Š* ***Llama3â€“70Bæ¨¡å‹ã€‚â€***
- en: '**BoT** achieves efficient reasoning across problems that are similar to its
    templates as it:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**BoT**é€šè¿‡ä»¥ä¸‹æ–¹å¼ï¼Œåœ¨ä¸å…¶æ¨¡æ¿ç›¸ä¼¼çš„é—®é¢˜ä¸­å®ç°é«˜æ•ˆæ¨ç†ï¼š'
- en: (1) leverages previous solutions on new challenges,
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) åœ¨æ–°æŒ‘æˆ˜ä¸­å€Ÿç”¨ä»¥å‰çš„è§£å†³æ–¹æ¡ˆï¼Œ
- en: (2) boosts efficiency by eliminating the need for multiple query iterations
    (as we see in Graph-of-Thoughts (GoT) or ToT), and
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) æé«˜æ•ˆç‡ï¼Œé¿å…å¤šæ¬¡æŸ¥è¯¢è¿­ä»£ï¼ˆæ­£å¦‚æˆ‘ä»¬åœ¨æ€ç»´å›¾ï¼ˆGraph-of-Thoughts, GoTï¼‰æˆ–æ€ç»´é“¾ï¼ˆToTï¼‰ä¸­æ‰€çœ‹åˆ°çš„ï¼‰ï¼Œå¹¶
- en: (3) dynamically updates its template repository to ensure it evolves as it encounters
    new tasks.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) åŠ¨æ€æ›´æ–°å…¶æ¨¡æ¿åº“ï¼Œä»¥ç¡®ä¿åœ¨é‡åˆ°æ–°ä»»åŠ¡æ—¶èƒ½å¤Ÿä¸æ–­å‘å±•ã€‚
- en: In this article, we will first go through the general outline of how BoT works,
    understand the function of each key part, and test the procedure with an example.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é¦–å…ˆäº†è§£BoTå¦‚ä½•å·¥ä½œçš„ä¸€èˆ¬æ¦‚è¿°ï¼Œç†è§£æ¯ä¸ªå…³é”®éƒ¨åˆ†çš„åŠŸèƒ½ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªç¤ºä¾‹æµ‹è¯•è¯¥è¿‡ç¨‹ã€‚
- en: How does BoT work?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BoTæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ
- en: The general thought-augmented reasoning process (as shown in the figure below)
    starts with **Problem Distillation**, which analyzes and condenses the incoming
    task into essential elements and constraints and then creates a simplified problem
    statement.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬çš„æ€ç»´å¢å¼ºæ¨ç†è¿‡ç¨‹ï¼ˆå¦‚ä¸‹å›¾æ‰€ç¤ºï¼‰ä»**é—®é¢˜æç‚¼**å¼€å§‹ï¼Œåˆ†æå¹¶æµ“ç¼©ä¼ å…¥çš„ä»»åŠ¡ï¼Œæå–å‡ºå…³é”®å…ƒç´ å’Œçº¦æŸæ¡ä»¶ï¼Œè¿›è€Œåˆ›å»ºç®€åŒ–çš„é—®é¢˜é™ˆè¿°ã€‚
- en: This distilled information is then used to query the **Meta-Buffer**, a dynamic
    repository that contains high-level thought templates. From the thought templates,
    one that is most similar to the distilled problem is retrieved. Then, during the
    **Instantiation Process**, it is instantiated with specific requirements and information
    about the distilled problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æç‚¼çš„ä¿¡æ¯éšåè¢«ç”¨æ¥æŸ¥è¯¢**å…ƒç¼“å†²åŒºï¼ˆMeta-Bufferï¼‰**ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«é«˜å±‚æ¬¡æ€ç»´æ¨¡æ¿çš„åŠ¨æ€åº“ã€‚ä»æ€ç»´æ¨¡æ¿ä¸­ï¼Œæ£€ç´¢å‡ºä¸æç‚¼é—®é¢˜æœ€ç›¸ä¼¼çš„æ¨¡æ¿ã€‚æ¥ä¸‹æ¥ï¼Œåœ¨**å®ä¾‹åŒ–è¿‡ç¨‹ï¼ˆInstantiation
    Processï¼‰**ä¸­ï¼Œå®ƒä¼šæ ¹æ®æç‚¼é—®é¢˜çš„å…·ä½“è¦æ±‚å’Œä¿¡æ¯è¿›è¡Œå®ä¾‹åŒ–ã€‚
- en: Throughout this process, the **Buffer Manager** actively keeps an eye on the
    **Meta-Buffer.** Once it detects a new insight not included in the meta-buffer,
    **Buffer Manager** updates it to ensure a continual evolution of the thought template
    repository.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œ**ç¼“å†²åŒºç®¡ç†å™¨ï¼ˆBuffer Managerï¼‰**ä¼šä¸»åŠ¨ç›‘æ§**å…ƒç¼“å†²åŒºï¼ˆMeta-Bufferï¼‰**ã€‚ä¸€æ—¦æ£€æµ‹åˆ°å…ƒç¼“å†²åŒºä¸­æœªåŒ…å«çš„æ–°è§è§£ï¼Œ**ç¼“å†²åŒºç®¡ç†å™¨ï¼ˆBuffer
    Managerï¼‰**ä¼šæ›´æ–°å…ƒç¼“å†²åŒºï¼Œç¡®ä¿æ€ç»´æ¨¡æ¿åº“çš„æŒç»­è¿›åŒ–ã€‚
- en: '![](../Images/32292bd0be51f32c6798de545021c314.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32292bd0be51f32c6798de545021c314.png)'
- en: 'The BoT process. (source: [Paper](http://Paper))'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: BoTè¿‡ç¨‹ã€‚ï¼ˆæ¥æºï¼š[è®ºæ–‡](http://Paper)ï¼‰
- en: 'Letâ€™s go through each of these key parts to gain a more detailed look:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€ä¸€äº†è§£è¿™äº›å…³é”®éƒ¨åˆ†ï¼Œæ·±å…¥æ¢è®¨æ¯ä¸€éƒ¨åˆ†çš„ç»†èŠ‚ï¼š
- en: Problem Distiller
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é—®é¢˜æç‚¼å™¨
- en: Problem Distiller can be thought of as a **preprocess** on the input tasks in
    order toâ€¦
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜æç‚¼å™¨å¯ä»¥çœ‹ä½œæ˜¯å¯¹è¾“å…¥ä»»åŠ¡çš„**é¢„å¤„ç†**ï¼Œç›®çš„æ˜¯â€¦â€¦
- en: (1) extract essential information of the problem, and
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) æå–é—®é¢˜çš„å…³é”®ä¿¡æ¯ï¼Œå¹¶
- en: (2) simplify complex tasks for a better search and retrieval of thought templates.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) ç®€åŒ–å¤æ‚ä»»åŠ¡ï¼Œä»¥ä¾¿æ›´å¥½åœ°æœç´¢å’Œæ£€ç´¢æ€ç»´æ¨¡æ¿ã€‚
- en: 'Problem Distiller takes the burden off of LLM to identify and extract vital
    information and constraints of the problem. This is done by a meta prompt Ï•:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜æç‚¼å™¨ï¼ˆProblem Distillerï¼‰é€šè¿‡å…ƒæç¤ºÏ•å‡è½»äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯†åˆ«å’Œæå–é—®é¢˜çš„å…³é”®ä¿¡æ¯ä¸çº¦æŸçš„è´Ÿæ‹…ï¼š
- en: '![](../Images/944bc53eb01b77fdf82f8608b1a19b46.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/944bc53eb01b77fdf82f8608b1a19b46.png)'
- en: '(source: [Paper](http://Paper))'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆæ¥æºï¼š[è®ºæ–‡](http://Paper)ï¼‰
- en: 'The prompt used by the authors to distill key information about a task is as
    follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…ç”¨äºæç‚¼ä»»åŠ¡å…³é”®ä¿¡æ¯çš„æç¤ºå¦‚ä¸‹ï¼š
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Meta-Buffer
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…ƒç¼“å†²åŒºï¼ˆMeta-Bufferï¼‰
- en: The meta-buffer is a central database that stores high-level thought templates.
    These templates are high-level abstractions representing various problem-solving
    processes. The idea is that LLM can leverage past problems and insights to solve
    current challenges. The best part is that the Meta-Buffer dynamically updates
    to ensure new unseen problems are also included. The Meta-Buffer doesnâ€™t enforce
    thought templates to follow a specific instruction.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å…ƒç¼“å†²åŒºæ˜¯ä¸€ä¸ªä¸­å¤®æ•°æ®åº“ï¼Œå­˜å‚¨ç€é«˜çº§çš„æ€ç»´æ¨¡æ¿ã€‚è¿™äº›æ¨¡æ¿æ˜¯è¡¨ç¤ºå„ç§é—®é¢˜è§£å†³è¿‡ç¨‹çš„é«˜çº§æŠ½è±¡ã€‚å…¶ç†å¿µæ˜¯LLMå¯ä»¥åˆ©ç”¨è¿‡å»çš„é—®é¢˜å’Œè§è§£æ¥è§£å†³å½“å‰çš„æŒ‘æˆ˜ã€‚æœ€æ£’çš„æ˜¯ï¼Œå…ƒç¼“å†²åŒºä¼šåŠ¨æ€æ›´æ–°ï¼Œä»¥ç¡®ä¿æ–°çš„æœªè§è¿‡çš„é—®é¢˜ä¹Ÿèƒ½è¢«åŒ…æ‹¬åœ¨å†…ã€‚å…ƒç¼“å†²åŒºä¸ä¼šå¼ºåˆ¶æ€ç»´æ¨¡æ¿éµå¾ªç‰¹å®šçš„æŒ‡ä»¤ã€‚
- en: '**Template Retrieval**: Once a task is distilled, BoT would go through the
    thought templates and grab the one most similar to the task. This is done by calculating
    the embedding similarity between the task and the thought templates.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¨¡æ¿æ£€ç´¢**ï¼šä¸€æ—¦ä»»åŠ¡è¢«æç‚¼ï¼ŒBoTä¼šéå†æ€ç»´æ¨¡æ¿å¹¶æŠ“å–ä¸ä»»åŠ¡æœ€ç›¸ä¼¼çš„ä¸€ä¸ªã€‚é€šè¿‡è®¡ç®—ä»»åŠ¡ä¸æ€ç»´æ¨¡æ¿ä¹‹é—´çš„åµŒå…¥ç›¸ä¼¼åº¦æ¥å®Œæˆæ­¤æ“ä½œã€‚'
- en: '![](../Images/3877ad60c789b210ee2cd5f42b642b33.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3877ad60c789b210ee2cd5f42b642b33.png)'
- en: 'The retriever would calculate the similarity between the embedding of the input
    task f(xd), and the embedding of templates f(DTi ). (source: [Paper](https://arxiv.org/abs/2406.04271))'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€ç´¢å™¨ä¼šè®¡ç®—è¾“å…¥ä»»åŠ¡f(xd)çš„åµŒå…¥ä¸æ¨¡æ¿f(DTi)çš„åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚ï¼ˆæ¥æºï¼š[è®ºæ–‡](https://arxiv.org/abs/2406.04271)ï¼‰
- en: 'the retriever would calculate the similarity between the embedding of the input
    task **f(xd)**, and the embedding of templates **f(D*Ti* ).** This is only done
    if the similarity is above a certain threshold Î´ (0.5â€“0.7). If none of the thought
    templates have a similarity score with the task above the Î´ threshold, then the
    **xd** is identified as a new task. Depending on if the task is new or not, one
    of the two paths would be taken:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€ç´¢å™¨ä¼šè®¡ç®—è¾“å…¥ä»»åŠ¡**f(xd)**çš„åµŒå…¥ä¸æ¨¡æ¿**f(D*Ti*)**çš„åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚ä»…åœ¨ç›¸ä¼¼åº¦è¶…è¿‡æŸä¸ªé˜ˆå€¼Î´ï¼ˆ0.5â€“0.7ï¼‰æ—¶æ‰ä¼šè¿›è¡Œæ­¤æ“ä½œã€‚å¦‚æœæ²¡æœ‰æ€ç»´æ¨¡æ¿ä¸ä»»åŠ¡çš„ç›¸ä¼¼åº¦è¶…è¿‡Î´é˜ˆå€¼ï¼Œåˆ™**xd**ä¼šè¢«è¯†åˆ«ä¸ºæ–°ä»»åŠ¡ã€‚æ ¹æ®ä»»åŠ¡æ˜¯å¦æ˜¯æ–°ä»»åŠ¡ï¼Œå°†é€‰æ‹©ä»¥ä¸‹ä¸¤æ¡è·¯å¾„ä¹‹ä¸€ï¼š
- en: If the task is similar to one of the thought templates, the template would be
    instantiated with the distilled information using an instantiation prompt (which
    you can check in the paper). This instantiation process can be denoted as
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä»»åŠ¡ä¸æŸä¸ªæ€ç»´æ¨¡æ¿ç›¸ä¼¼ï¼Œåˆ™ä¼šä½¿ç”¨å®ä¾‹åŒ–æç¤ºï¼ˆå¯ä»¥åœ¨è®ºæ–‡ä¸­æŸ¥çœ‹ï¼‰ç”¨æç‚¼è¿‡çš„ä¿¡æ¯å®ä¾‹åŒ–è¯¥æ¨¡æ¿ã€‚è¿™ä¸ªå®ä¾‹åŒ–è¿‡ç¨‹å¯ä»¥è¡¨ç¤ºä¸º
- en: '![](../Images/872661a6c9986487d31d5d362f937a78.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/872661a6c9986487d31d5d362f937a78.png)'
- en: 'Instantiated Reasoning. (source: [Paper](https://arxiv.org/abs/2406.04271))'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å®ä¾‹åŒ–æ¨ç†ã€‚ï¼ˆæ¥æºï¼š[è®ºæ–‡](https://arxiv.org/abs/2406.04271)ï¼‰
- en: If the task is new, a general thought template that is designed to address a
    broad set of problems is used. As the task is processed, the **Buffer Manager**
    observes and learns and potentially creates a new, more specific thought template
    and pushes it to the meta-buffer.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä»»åŠ¡æ˜¯æ–°çš„ï¼Œåˆ™ä½¿ç”¨ä¸€ä¸ªæ—¨åœ¨è§£å†³å¹¿æ³›é—®é¢˜çš„é€šç”¨æ€ç»´æ¨¡æ¿ã€‚éšç€ä»»åŠ¡çš„å¤„ç†ï¼Œ**ç¼“å†²åŒºç®¡ç†å™¨**ä¼šè§‚å¯Ÿå¹¶å­¦ä¹ ï¼Œå¹¶å¯èƒ½åˆ›å»ºä¸€ä¸ªæ–°çš„ã€æ›´å…·ä½“çš„æ€ç»´æ¨¡æ¿å¹¶å°†å…¶æ¨é€åˆ°å…ƒç¼“å†²åŒºã€‚
- en: Buffer Manager
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¼“å†²åŒºç®¡ç†å™¨
- en: The Buffer Manager serves as a crucial player in maintaining and enhancing the
    meta-buffer. Based on the new insights and outcomes from the tasks that are solved,
    it updates the thought templates. Also, whenever a new or drastically different
    problem is solved, the buffer manager assesses whether or not to create a new
    thought template. This is to ensure thought templates remain to the point and
    are not redundant.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼“å†²åŒºç®¡ç†å™¨åœ¨ç»´æŠ¤å’Œå¢å¼ºå…ƒç¼“å†²åŒºä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ ¹æ®ä»è§£å†³ä»»åŠ¡ä¸­è·å¾—çš„æ–°è§è§£å’Œç»“æœï¼Œå®ƒæ›´æ–°æ€ç»´æ¨¡æ¿ã€‚åŒæ—¶ï¼Œæ¯å½“è§£å†³ä¸€ä¸ªæ–°çš„æˆ–æˆªç„¶ä¸åŒçš„é—®é¢˜æ—¶ï¼Œç¼“å†²åŒºç®¡ç†å™¨ä¼šè¯„ä¼°æ˜¯å¦åˆ›å»ºä¸€ä¸ªæ–°çš„æ€ç»´æ¨¡æ¿ã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿æ€ç»´æ¨¡æ¿å§‹ç»ˆç´§æ‰£ä¸»é¢˜ï¼Œä¸å†—ä½™ã€‚
- en: '![](../Images/64359637991b99ea155bceb95391d9d6.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64359637991b99ea155bceb95391d9d6.png)'
- en: 'checking whether or not a newly generated template is similar to the existing
    ones. (source: Paper)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥æ–°ç”Ÿæˆçš„æ¨¡æ¿æ˜¯å¦ä¸ç°æœ‰æ¨¡æ¿ç›¸ä¼¼ã€‚ï¼ˆæ¥æºï¼šè®ºæ–‡ï¼‰
- en: By employing the above formulation, the Buffer Manager checks whether or not
    the meta-buffer already has the necessary knowledge to tackle a problem.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡é‡‡ç”¨ä¸Šè¿°å…¬å¼ï¼Œç¼“å†²åŒºç®¡ç†å™¨æ£€æŸ¥å…ƒç¼“å†²åŒºæ˜¯å¦å·²ç»æ‹¥æœ‰è§£å†³é—®é¢˜æ‰€éœ€çš„çŸ¥è¯†ã€‚
- en: BoT vs. Single-Query vs. Multi-Query
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BoT vs. å•æŸ¥è¯¢ vs. å¤šæŸ¥è¯¢
- en: How does the BoT stand out compared to previous methods? The authors of the
    paper evaluate various methods on different datasets of various tasks, such as
    Data Understanding, Python Programming Puzzles, Multilingual Grade School Math(MGSM),
    etc. The results show a surprising advantage of **BoT** in almost all the tasks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: BoT ç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•æœ‰ä»€ä¹ˆçªå‡ºä¹‹å¤„ï¼Ÿè®ºæ–‡çš„ä½œè€…åœ¨å¤šä¸ªæ•°æ®é›†å’Œä¸åŒä»»åŠ¡ä¸Šè¯„ä¼°äº†å„ç§æ–¹æ³•ï¼Œå¦‚æ•°æ®ç†è§£ã€Python ç¼–ç¨‹éš¾é¢˜ã€è¯­è¨€å­¦å¹´çº§æ•°å­¦ï¼ˆMGSMï¼‰ç­‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œ**BoT**
    åœ¨å‡ ä¹æ‰€æœ‰ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºæ„æƒ³ä¸åˆ°çš„ä¼˜åŠ¿ã€‚
- en: '![](../Images/498b49fc3dd24dbe76978a7fb2099b3f.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/498b49fc3dd24dbe76978a7fb2099b3f.png)'
- en: 'BoT vs previous methods. The best results (marked in blue) are all achieved
    by BoT. (source: [Paper](https://arxiv.org/abs/2406.04271))'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: BoT ä¸ä¹‹å‰çš„æ–¹æ³•æ¯”è¾ƒã€‚æœ€ä½³ç»“æœï¼ˆæ ‡è®°ä¸ºè“è‰²ï¼‰å…¨éƒ¨ç”± BoT å®ç°ã€‚ï¼ˆæ¥æºï¼š[Paper](https://arxiv.org/abs/2406.04271)ï¼‰
- en: One of the key advantages of BoT is its efficiency â€” requiring **only 12%**
    of the computational cost on average when compared to multi-query prompting methods.
    This high computational cost and latency of multi-query methods such as ToT often
    renders them impractical in real-world use cases.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: BoT çš„å…³é”®ä¼˜åŠ¿ä¹‹ä¸€æ˜¯å…¶æ•ˆç‡â€”â€”ä¸å¤šæŸ¥è¯¢æç¤ºæ–¹æ³•ç›¸æ¯”ï¼Œå¹³å‡ä»…éœ€ **12%** çš„è®¡ç®—æˆæœ¬ã€‚è¿™ç§å¤šæŸ¥è¯¢æ–¹æ³•ï¼ˆå¦‚ ToTï¼‰é€šå¸¸å› å…¶é«˜è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿï¼Œåœ¨å®é™…åº”ç”¨ä¸­å˜å¾—ä¸åˆ‡å®é™…ã€‚
- en: BoT+Llama3â€“8B has the potential to surpass single Llama3â€“70B model
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: BoT+Llama3â€“8B æœ‰æ½œåŠ›è¶…è¶Šå•ä¸€çš„ Llama3â€“70B æ¨¡å‹
- en: '![](../Images/d343e715b3bfa85f8ddd6a75569f32f8.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d343e715b3bfa85f8ddd6a75569f32f8.png)'
- en: 'comparing the effect of BoT on Llama3â€“8B and 70B. Annotated. (source: [Paper](https://arxiv.org/abs/2406.04271))'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒ BoT åœ¨ Llama3â€“8B å’Œ 70B ä¸Šçš„æ•ˆæœã€‚å·²æ ‡æ³¨ã€‚ï¼ˆæ¥æºï¼š[Paper](https://arxiv.org/abs/2406.04271)ï¼‰
- en: Buffer of Thoughts in Practice
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®è·µä¸­çš„ Buffer of Thoughts
- en: 'The demo code for Buffer of Thoughts is published on [GitHub](https://github.com/YangLing0818/buffer-of-thought-llm)
    [2]. To test out the functionality in practice, I will use this method on a custom
    task: Word Reordering. In this task, the LLM must take a scrambled sentence, such
    as *â€œSam name is myâ€*, and return a permutation of these words that is semantically
    meaningful, which in this example would be *â€œmy name is Samâ€,* (this is not a
    benchmark with baseline performance). Some examples of the scrambled sentences
    and the correct ones are as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Buffer of Thoughts çš„æ¼”ç¤ºä»£ç å·²å‘å¸ƒåœ¨ [GitHub](https://github.com/YangLing0818/buffer-of-thought-llm)
    [2] ä¸Šã€‚ä¸ºäº†åœ¨å®è·µä¸­æµ‹è¯•å…¶åŠŸèƒ½ï¼Œæˆ‘å°†åœ¨ä¸€ä¸ªè‡ªå®šä¹‰ä»»åŠ¡ä¸­ä½¿ç”¨è¯¥æ–¹æ³•ï¼šè¯è¯­é‡æ’ã€‚åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼ŒLLM å¿…é¡»æ¥å—ä¸€ä¸ªä¹±åºçš„å¥å­ï¼Œä¾‹å¦‚ *â€œSam name is
    myâ€*ï¼Œå¹¶è¿”å›ä¸€ä¸ªè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å•è¯æ’åˆ—ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­æ˜¯ *â€œmy name is Samâ€*ï¼Œï¼ˆè¿™ä¸æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œä¸”æ²¡æœ‰åŸºå‡†æ€§èƒ½ï¼‰ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ä¹±åºå¥å­åŠå…¶æ­£ç¡®ç‰ˆæœ¬çš„ç¤ºä¾‹ï¼š
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'I will also use the following user prompt to explain the task:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜å°†ä½¿ç”¨ä»¥ä¸‹ç”¨æˆ·æç¤ºæ¥è§£é‡Šä»»åŠ¡ï¼š
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Provided this task, the BoT framework first distills my task and extracts the
    goal, the restriction, and how I would like my output along with other key information.
    The full procedure of BoT in detail, is the following terminal output:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æä¾›è¿™ä¸ªä»»åŠ¡æ—¶ï¼ŒBoT æ¡†æ¶é¦–å…ˆæç‚¼æˆ‘çš„ä»»åŠ¡å¹¶æå–ç›®æ ‡ã€é™åˆ¶æ¡ä»¶ä»¥åŠæˆ‘å¸Œæœ›çš„è¾“å‡ºæ–¹å¼ï¼Œå’Œå…¶ä»–å…³é”®ä¿¡æ¯ã€‚BoT çš„å®Œæ•´æµç¨‹å¦‚ä¸‹ç»ˆç«¯è¾“å‡ºï¼š
- en: '[PRE3]python'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3]python'
- en: 'def ReorderWords(input_sentence):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'def ReorderWords(input_sentence):'
- en: Remove the start and end tags, and split the sentence into words
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ é™¤å¼€å§‹å’Œç»“æŸæ ‡ç­¾ï¼Œå¹¶å°†å¥å­æ‹†åˆ†æˆå•è¯
- en: words = input_sentence.replace("<start>", "").replace("<end>", "").strip().split()
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: words = input_sentence.replace("<start>", "").replace("<end>", "").strip().split()
- en: Tentative reordering to form a meaningful sentence
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆæ­¥é‡æ’ä»¥å½¢æˆæœ‰æ„ä¹‰çš„å¥å­
- en: Based on the context provided in the task, the following reordering seems logical
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ ¹æ®ä»»åŠ¡ä¸­æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œä»¥ä¸‹é‡æ’çœ‹èµ·æ¥æ˜¯åˆä¹é€»è¾‘çš„
- en: reordered_words = ["the", "melting", "in", "solid", "metals", "is", "achieved",
    "by", "mixing", "gold", "leaf", "and", "desired", "other", "color", "gold"]
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: reordered_words = ["the", "melting", "in", "solid", "metals", "is", "achieved",
    "by", "mixing", "gold", "leaf", "and", "desired", "other", "color", "gold"]
- en: Join the words into a single sentence and add start and end tags
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: å°†å•è¯åˆå¹¶æˆä¸€ä¸ªå¥å­ï¼Œå¹¶æ·»åŠ å¼€å§‹å’Œç»“æŸæ ‡ç­¾
- en: reordered_sentence = "<start> " + " ".join(reordered_words) + " <end>"
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: reordered_sentence = "<start> " + " ".join(reordered_words) + " <end>"
- en: return reordered_sentence
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return reordered_sentence
- en: Input sentence provided in the task
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»»åŠ¡ä¸­æä¾›çš„è¾“å…¥å¥å­
- en: input_sentence = "<start> the melting in solid to gold leaf metals is achieve
    made by desired gold and mixing color other <end>"
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: input_sentence = "<start> the melting in solid to gold leaf metals is achieve
    made by desired gold and mixing color other <end>"
- en: print(ReorderWords(input_sentence))
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: print(ReorderWords(input_sentence))
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Some of the examples of the reordered sentences using BoT:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ BoT é‡æ’çš„å¥å­çš„ä¸€äº›ç¤ºä¾‹ï¼š
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note, that the BoT repository I used is a demo code and lacks some of the features
    mentioned in the original paper, such as a general thought template, the dynamic
    updating of the Meta-Buffer, or finding the closest template embedding to the
    user task. These are important aspects of the framework and without them, we cannot
    conclude the performance of Buffer of Thoughts in practice.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä½¿ç”¨çš„BoTä»“åº“æ˜¯ä¸€ä¸ªæ¼”ç¤ºä»£ç ï¼Œç¼ºå°‘åŸè®ºæ–‡ä¸­æåˆ°çš„ä¸€äº›åŠŸèƒ½ï¼Œæ¯”å¦‚é€šç”¨æ€ç»´æ¨¡æ¿ã€Meta-Bufferçš„åŠ¨æ€æ›´æ–°ï¼Œæˆ–è€…æ‰¾åˆ°ä¸ç”¨æˆ·ä»»åŠ¡æœ€æ¥è¿‘çš„æ¨¡æ¿åµŒå…¥ã€‚è¿™äº›æ˜¯æ¡†æ¶çš„é‡è¦æ–¹é¢ï¼Œæ²¡æœ‰å®ƒä»¬ï¼Œæˆ‘ä»¬æ— æ³•å¾—å‡ºæ€æƒ³ç¼“å†²åŒºåœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚
- en: Final Words
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€åçš„è¯
- en: In conclusion, BoT shows promising results in both accuracy and efficiency in
    various domains and tasks. Itâ€™s an interesting approach to breaking down a reasoning
    problem into its fundamental restrictions and key information and building on
    top of previous solutions and templates to better formulate the task for an LLM
    to understand.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼ŒBoTåœ¨å„ä¸ªé¢†åŸŸå’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ—¢å‡†ç¡®åˆé«˜æ•ˆã€‚è¿™æ˜¯ä¸€ç§æœ‰è¶£çš„æ–¹æ³•ï¼Œé€šè¿‡å°†æ¨ç†é—®é¢˜åˆ†è§£ä¸ºåŸºæœ¬é™åˆ¶å’Œå…³é”®ä¿¡æ¯ï¼Œå¹¶åœ¨ä¹‹å‰çš„è§£å†³æ–¹æ¡ˆå’Œæ¨¡æ¿åŸºç¡€ä¸Šæ„å»ºï¼Œä»¥æ›´å¥½åœ°æ„é€ ä»»åŠ¡ï¼Œä½¿LLMèƒ½å¤Ÿç†è§£ã€‚
- en: By addressing some of the limitations in other prompting techniques, Buffer
    of Thoughts allows LLM to have more complex thinking patterns, potentially making
    smaller lightweight models perform at the level of larger ones.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è§£å†³å…¶ä»–æç¤ºæŠ€æœ¯çš„ä¸€äº›å±€é™æ€§ï¼Œæ€æƒ³ç¼“å†²åŒºä½¿LLMèƒ½å¤Ÿå…·å¤‡æ›´å¤æ‚çš„æ€ç»´æ¨¡å¼ï¼Œå¯èƒ½ä½¿è¾ƒå°çš„è½»é‡çº§æ¨¡å‹è¾¾åˆ°æ›´å¤§æ¨¡å‹çš„æ€§èƒ½æ°´å¹³ã€‚
- en: Allowing small LLMs to achieve results close to large LLMs is a key topic addressed
    in many of the current research papers. The goal is to employ various prompting
    and fine-tuning techniques to extract accurate AI outputs with low computation
    and cost.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å…è®¸å°å‹LLMå®ç°æ¥è¿‘å¤§å‹LLMçš„ç»“æœæ˜¯å½“å‰è®¸å¤šç ”ç©¶è®ºæ–‡ä¸­è®¨è®ºçš„ä¸€ä¸ªå…³é”®ä¸»é¢˜ã€‚ç›®æ ‡æ˜¯é‡‡ç”¨å„ç§æç¤ºå’Œå¾®è°ƒæŠ€æœ¯ï¼Œä»¥ä½è®¡ç®—é‡å’Œæˆæœ¬æå–å‡†ç¡®çš„AIè¾“å‡ºã€‚
- en: Buffer of Thoughts is a novel and promising prompting framework that leverages
    a domain of techniques to guide LLM step by step, in a reasoning process. A complete
    practical implementation of the Buffer of Thoughts technique is yet to come, but
    in the meanwhile, test out the provided benchmarks in the demo GitHub repository
    [2].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ€æƒ³ç¼“å†²åŒºæ˜¯ä¸€ä¸ªæ–°é¢–ä¸”æœ‰å‰æ™¯çš„æç¤ºæ¡†æ¶ï¼Œåˆ©ç”¨ä¸€ç³»åˆ—æŠ€æœ¯å¼•å¯¼LLMé€æ­¥è¿›è¡Œæ¨ç†è¿‡ç¨‹ã€‚æ€æƒ³ç¼“å†²åŒºæŠ€æœ¯çš„å®Œæ•´å®è·µå®ç°å°šæœªåˆ°æ¥ï¼Œä½†åœ¨æ­¤æœŸé—´ï¼Œå¯ä»¥åœ¨æä¾›çš„æ¼”ç¤ºGitHubä»“åº“[2]ä¸­æµ‹è¯•åŸºå‡†ã€‚
- en: 'If you have made it this far, consider this for a **further read**:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å·²ç»è¯»åˆ°è¿™é‡Œï¼Œè€ƒè™‘ç»§ç»­**é˜…è¯»æ›´å¤š**ï¼š
- en: '[](/platonic-representation-hypothesis-c812813d7248?source=post_page-----391919d2f76f--------------------------------)
    [## Platonic Representation: Are AI Deep Network Models Converging?'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/platonic-representation-hypothesis-c812813d7248?source=post_page-----391919d2f76f--------------------------------)
    [## æŸæ‹‰å›¾å¼è¡¨å¾ï¼šäººå·¥æ™ºèƒ½æ·±åº¦ç½‘ç»œæ¨¡å‹æ˜¯å¦è¶‹åŒï¼Ÿ'
- en: Are Artificial Intelligence models evolving towards a unified representation
    of reality? The Platonic Representationâ€¦
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: äººå·¥æ™ºèƒ½æ¨¡å‹æ˜¯å¦æ­£åœ¨æœç€ç°å®çš„ç»Ÿä¸€è¡¨å¾å‘å±•ï¼ŸæŸæ‹‰å›¾å¼è¡¨å¾â€¦â€¦
- en: towardsdatascience.com](/platonic-representation-hypothesis-c812813d7248?source=post_page-----391919d2f76f--------------------------------)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/platonic-representation-hypothesis-c812813d7248?source=post_page-----391919d2f76f--------------------------------)
- en: '**ğŸŒŸ Join +1000 people learning about** Python, ML / MLOps / AI, Data Science,
    and LLM. [**follow me**](https://medium.com/@itshesamsheikh/subscribe)and check
    out my [**X/Twitter**](https://twitter.com/itsHesamSheikh), where I keep you updated
    Daily**.**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸŒŸ åŠ å…¥+1000äººä¸€èµ·å­¦ä¹ ** Pythonã€æœºå™¨å­¦ä¹  / MLOps / äººå·¥æ™ºèƒ½ã€æ•°æ®ç§‘å­¦å’ŒLLMã€‚ [**å…³æ³¨æˆ‘**](https://medium.com/@itshesamsheikh/subscribe)å¹¶æŸ¥çœ‹æˆ‘çš„[**X/Twitter**](https://twitter.com/itsHesamSheikh)ï¼Œæˆ‘æ¯å¤©ä¸ºä½ æ›´æ–°**ã€‚**'
- en: Thanks for reading,
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼Œ
- en: â€” Hesam
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: â€” Hesam
- en: '[1] Yang, L., Yu, Z., Zhang, T., Cao, S., Xu, M., Zhang, W., Gonzalez, J. E.,
    & Cui, B. (2024). Buffer of Thoughts: Thought-Augmented Reasoning with Large Language
    Models. *arXiv*. Retrieved from [https://arxiv.org/abs/2406.04271](https://arxiv.org/abs/2406.04271)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] æ¨å‡Œã€ä½™æ³½ã€å¼ æ¶›ã€æ›¹èƒœã€è®¸æ¢¦ã€å¼ ä¼Ÿã€æˆˆæ©è¨é›·æ–¯ã€å´”åšï¼ˆ2024ï¼‰ã€‚ã€Šæ€æƒ³ç¼“å†²åŒºï¼šé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºçš„æ€ç»´æ¨ç†ã€‹ã€‚*arXiv*ã€‚å–è‡ª[https://arxiv.org/abs/2406.04271](https://arxiv.org/abs/2406.04271)'
- en: '[2] buffer-of-thought-llm, [https://github.com/YangLing0818/buffer-of-thought-llm](https://github.com/YangLing0818/buffer-of-thought-llm)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] buffer-of-thought-llm, [https://github.com/YangLing0818/buffer-of-thought-llm](https://github.com/YangLing0818/buffer-of-thought-llm)'
