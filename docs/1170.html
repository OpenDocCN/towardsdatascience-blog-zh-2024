<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Feature Selection with Optuna</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Feature Selection with Optuna</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-with-optuna-0ddf3e0f7d8c?source=collection_archive---------3-----------------------#2024-05-09">https://towardsdatascience.com/feature-selection-with-optuna-0ddf3e0f7d8c?source=collection_archive---------3-----------------------#2024-05-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="eed2" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A versatile and promising approach for the feature selection task</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@nicolupi.2?source=post_page---byline--0ddf3e0f7d8c--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Nicolas Lupi" class="l ep by dd de cx" src="../Images/7f0735890a77b9ef601dc6cd54a9a861.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*9JY1rQbu5HTc7A9LtJQtJw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--0ddf3e0f7d8c--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@nicolupi.2?source=post_page---byline--0ddf3e0f7d8c--------------------------------" rel="noopener follow">Nicolas Lupi</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--0ddf3e0f7d8c--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/dd461c48cbc1fb8dfe671c1403a9486e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Dl3tysLbsQGIsX8K"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@edgr?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Edu Grande</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="631e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Feature selection is a critical step in many machine learning pipelines. In practice, we generally have a wide range of variables available as predictors for our models, but only a few of them are related to our target. Feature selection consists of finding a reduced set of these features, mainly for:</p><ul class=""><li id="69a9" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk"><strong class="nf fr">Improved generalization </strong>— using a reduced number of features minimizes the risk of overfitting.</li><li id="e254" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk"><strong class="nf fr">Better inference</strong> — by removing redundant features (for example, two features very correlated with each other), we can retain only one of them and better capture its effect.</li><li id="515a" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk"><strong class="nf fr">Efficient training</strong> — having less features means shorter training times.</li><li id="d386" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk"><strong class="nf fr">Better interpretation</strong> — reducing the number of features produces more parsimonious models which are easier to understand.</li></ul><p id="c09c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There are many techniques available to perform feature selection, each with varying complexity. In this article, I want to share a way of using a powerful open source optimization tool, Optuna, to perform the feature selection task in an innovative way. The main idea is to have a flexible tool that can handle feature selection for a wide range of tasks, by efficiently testing different feature combinations (e.g., not trying them all one by one). Below, we’ll go through a hands-on example implementing this approach, and also comparing it to other common feature selection strategies. To experiment with the feature selection techniques discussed, you can follow along with this <a class="af nc" href="https://colab.research.google.com/drive/193Jwb0xXWh_UkvwIiFgufKEYer-86RNA?usp=sharing" rel="noopener ugc nofollow" target="_blank">Colab Notebook</a>.</p><p id="8703" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this example, we’ll focus on a classification task based on the <a class="af nc" href="https://www.kaggle.com/datasets/iabhishekofficial/mobile-price-classification" rel="noopener ugc nofollow" target="_blank">Mobile Price Classification</a> dataset from Kaggle. We have 20 features, including ‘<em class="oh">battery_power’</em>, ‘<em class="oh">clock_speed’</em> and ‘<em class="oh">ram’</em>, to predict the ‘<em class="oh">price_range’</em> feature, which can belong to four different bands: 0, 1, 2 and 3.</p><p id="8a4c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We first split our dataset into train and test sets, and we also prepare a 5-fold validation split within the train set — this will be useful later on.</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="1348" class="om on fq oj b bg oo op l oq or">import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.model_selection import StratifiedKFold<br/><br/>SEED = 32<br/><br/># Load data<br/>filename = "train.csv" # train.csv from https://www.kaggle.com/datasets/iabhishekofficial/mobile-price-classification<br/><br/>df = pd.read_csv(filename)<br/><br/># Train - test split<br/>df_train, df_test = train_test_split(df, test_size=0.2, stratify=df.iloc[:,-1], random_state=SEED)<br/>df_train = df_train.reset_index(drop=True)<br/>df_test = df_test.reset_index(drop=True)<br/><br/># The last column is the target variable<br/>X_train = df_train.iloc[:,0:20]<br/>y_train = df_train.iloc[:,-1]<br/>X_test = df_test.iloc[:,0:20]<br/>y_test = df_test.iloc[:,-1]<br/><br/># Stratified kfold over the train set for cross validation<br/>skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)<br/>splits = list(skf.split(X_train, y_train))</span></pre><p id="585a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The model we’ll use throughout the example is the <a class="af nc" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">Random Forest Classifier</a>, using the scikit-learn implementation and default parameters. We first train the model using all features to set our benchmark. The metric we’ll measure is the F1 score weighted for all four price ranges. After fitting the model over the train set, we evaluate it on the test set, obtaining an F1 score of around 0.87.</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="e0d7" class="om on fq oj b bg oo op l oq or">from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.metrics import f1_score, classification_report<br/><br/>model = RandomForestClassifier(random_state=SEED)<br/>model.fit(X_train,y_train)<br/>preds = model.predict(X_test)<br/><br/>print(classification_report(y_test, preds))<br/>print(f"Global F1: {f1_score(y_test, preds, average='weighted')}")</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk os"><img src="../Images/06ba109e36469ca2e01826c3e7150bc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*hQGXPF6QSmu9oFANC3zy7Q.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure><p id="8161" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The goal now is to improve these metrics by selecting a reduced feature set. We will first outline how our Optuna-based approach works, and then test and compare it with other common feature selection strategies.</p><h1 id="2901" class="ot on fq bf ou ov ow gq ox oy oz gt pa pb pc pd pe pf pg ph pi pj pk pl pm pn bk">Optuna</h1><p id="ef7e" class="pw-post-body-paragraph nd ne fq nf b go po nh ni gr pp nk nl nm pq no np nq pr ns nt nu ps nw nx ny fj bk"><a class="af nc" href="https://optuna.org/" rel="noopener ugc nofollow" target="_blank">Optuna</a> is an optimization framework mainly used for hyperparameter tuning. One of the key features of the framework is its use of Bayesian optimization techniques to search the parameter space. The main idea is that Optuna tries different combinations of parameters and evaluates how the objective function changes with each configuration. From these trials, it builds a probabilistic model used to estimate which parameter values are likely to yield better outcomes.</p><p id="19ea" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This strategy is much more efficient compared to grid or random search. For example, if we had <em class="oh">n</em> features, and attempted to try each possible feature subset, we would have to perform 2^<em class="oh">n</em> trials. With 20 features these would be more than a million trials. Instead, with Optuna, we can explore the search space with much fewer trials.</p><p id="cf84" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Optuna offers various samplers to try. For our case, we’ll use the default one, the <em class="oh">TPESampler</em>, based on the Tree-structured Parzen Estimator algorithm (TPE). This sampler is the most commonly used, and it’s recommended for searching categorical parameters, which is our case as we’ll see below. According to the documentation, this algorithm “fits one Gaussian Mixture Model (GMM) <em class="oh">l(x)</em> to the set of parameter values associated with the best objective values, and another GMM <em class="oh">g(x)</em> to the remaining parameter values. It chooses the parameter value x that maximizes the ratio<em class="oh"> l(x)/g(x)</em>.”</p><p id="bd1b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As mentioned earlier, Optuna is typically used for hyperparameter tuning. This is usually done by training the model repeatedly on the same data using a fixed set of features, and in each trial testing a new set of hyperparameters determined by the sampler. The parameter set that minimizes the given objective function is then returned as the best trial.</p><p id="a563" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In our case, however, we’ll use a fixed model with predetermined parameters, and in each trial, we’ll allow Optuna to select which features to try. The process aims to find the set of features that minimizes the loss function. In our case, we’ll guide the algorithm to maximize the F1 score (or minimize the negative of the F1). Additionally, we’ll add a small penalty for each feature used, to encourage smaller feature sets (if two feature sets yield similar results, we’ll prefer the one with fewer features).</p><p id="197a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The data we’ll use is the train dataset, split into five folds. In each trial, we’ll fit the classifier five times using four of the five folds for training and the remaining fold for validation. We’ll then average the validation metrics and add the penalty term to calculate the trial’s loss.</p><p id="3e55" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Below is the implemented class to perform the feature selection search:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="6920" class="om on fq oj b bg oo op l oq or">import optuna<br/><br/>class FeatureSelectionOptuna:<br/>    """<br/>    This class implements feature selection using Optuna optimization framework.<br/><br/>    Parameters:<br/><br/>    - model (object): The predictive model to evaluate; this should be any object that implements fit() and predict() methods.<br/>    - loss_fn (function): The loss function to use for evaluating the model performance. This function should take the true labels and the<br/>                          predictions as inputs and return a loss value.<br/>    - features (list of str): A list containing the names of all possible features that can be selected for the model.<br/>    - X (DataFrame): The complete set of feature data (pandas DataFrame) from which subsets will be selected for training the model.<br/>    - y (Series): The target variable associated with the X data (pandas Series).<br/>    - splits (list of tuples): A list of tuples where each tuple contains two elements, the train indices and the validation indices.<br/>    - penalty (float, optional): A factor used to penalize the objective function based on the number of features used.<br/>    """<br/><br/>    def __init__(self,<br/>                 model,<br/>                 loss_fn,<br/>                 features,<br/>                 X,<br/>                 y,<br/>                 splits,<br/>                 penalty=0):<br/><br/>        self.model = model<br/>        self.loss_fn = loss_fn<br/>        self.features = features<br/>        self.X = X<br/>        self.y = y<br/>        self.splits = splits<br/>        self.penalty = penalty<br/><br/>    def __call__(self,<br/>                 trial: optuna.trial.Trial):<br/><br/>        # Select True / False for each feature<br/>        selected_features = [trial.suggest_categorical(name, [True, False]) for name in self.features]<br/><br/>        # List with names of selected features<br/>        selected_feature_names = [name for name, selected in zip(self.features, selected_features) if selected]<br/><br/>        # Optional: adds a penalty for the amount of features used<br/>        n_used = len(selected_feature_names)<br/>        total_penalty = n_used * self.penalty<br/><br/>        loss = 0<br/><br/>        for split in self.splits:<br/>          train_idx = split[0]<br/>          valid_idx = split[1]<br/><br/>          X_train = self.X.iloc[train_idx].copy()<br/>          y_train = self.y.iloc[train_idx].copy()<br/>          X_valid = self.X.iloc[valid_idx].copy()<br/>          y_valid = self.y.iloc[valid_idx].copy()<br/><br/>          X_train_selected = X_train[selected_feature_names].copy()<br/>          X_valid_selected = X_valid[selected_feature_names].copy()<br/><br/>          # Train model, get predictions and accumulate loss<br/>          self.model.fit(X_train_selected, y_train)<br/>          pred = self.model.predict(X_valid_selected)<br/><br/>          loss += self.loss_fn(y_valid, pred)<br/><br/>        # Take the average loss across all splits<br/>        loss /= len(self.splits)<br/><br/>        # Add the penalty to the loss<br/>        loss += total_penalty<br/><br/>        return loss</span></pre><p id="da4b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The key part is where we define which features to use. We treat each feature as one parameter, which can take the values True or False. These values indicate whether the feature should be included in the model. We use the <em class="oh">suggest_categorical </em>method so that Optuna selects one of the two possible values for each feature.</p><p id="4d7b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We now initialize our Optuna study and perform the search for 100 trials. Notice that we enqueue a first trial using all features, as a starting point for the search, allowing Optuna to compare subsequent trials against a fully-featured model:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="b931" class="om on fq oj b bg oo op l oq or">from optuna.samplers import TPESampler<br/><br/>def loss_fn(y_true, y_pred):<br/>  """<br/>  Returns the negative F1 score, to be treated as a loss function.<br/>  """<br/>  res = -f1_score(y_true, y_pred, average='weighted')<br/>  return res<br/><br/>features = list(X_train.columns)<br/><br/>model = RandomForestClassifier(random_state=SEED)<br/><br/>sampler = TPESampler(seed = SEED)<br/>study = optuna.create_study(direction="minimize",sampler=sampler)<br/><br/># We first try the model using all features<br/>default_features = {ft: True for ft in features}<br/>study.enqueue_trial(default_features)<br/><br/>study.optimize(FeatureSelectionOptuna(<br/>                         model=model,<br/>                         loss_fn=loss_fn,<br/>                         features=features,<br/>                         X=X_train,<br/>                         y=y_train,<br/>                         splits=splits,<br/>                         penalty = 1e-4,<br/>                         ), n_trials=100)</span></pre><p id="1b1a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After completing the 100 trials, we retrieve the best one from the study and the features used in it. These are the following:</p><p id="5cda" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[<em class="oh">‘battery_power’, ‘blue’, ‘dual_sim’, ‘fc’, ‘mobile_wt’, ‘px_height’, ‘px_width’, ‘ram’, ‘sc_w’</em>]</p><p id="d954" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Notice that from the original 20 features, the search concluded with only 9 of them, which is a significant reduction. These features yielded a minimum validation loss of around -0.9117, which means they achieved an average F1 score of around 0.9108 across all folds (after adjusting for the penalty term).</p><p id="8866" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The next step is to train the model on the entire train set using these selected features and evaluate it on the test set. This results in an F1 score of around 0.882:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pt"><img src="../Images/3efce9ccee6e54966260ab9e0c568dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*yeFFt4ut6CaqUptdPTdmxw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure><p id="fc16" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">By selecting the right features, we were able to reduce our feature set by more than half, while still achieving a higher F1 score than with the full set. Below we will discuss some pros and cons of using Optuna for feature selection:</p><p id="8d44" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Pros</strong>:</p><ul class=""><li id="cc74" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Searches across feature sets efficiently, taking into account which feature combinations are most likely to produce good results.</li><li id="8c01" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Adaptable for many scenarios: As long as there is a model and a loss function, we can use it for any feature selection task.</li><li id="aa68" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Sees the whole picture: Unlike methods that evaluate features individually, Optuna takes into account which features tend to go well with each other, and which don’t.</li><li id="44f1" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Dynamically determines the number of features as part of the optimization process. This can be tuned with the penalty term.</li></ul><p id="8b06" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Cons</strong>:</p><ul class=""><li id="c8b5" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">It’s not as straightforward as simpler methods, and for smaller and simpler datasets it might not be worth it.</li><li id="9609" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Although it requires much fewer trials than other methods (like exhaustive search), it still typically requires around 100 to 1000 trials. Depending on the model and dataset, this can be time-consuming and computationally expensive.</li></ul><p id="b1da" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Next, we’ll compare our approach to other common feature selection strategies.</p><h1 id="b8b4" class="ot on fq bf ou ov ow gq ox oy oz gt pa pb pc pd pe pf pg ph pi pj pk pl pm pn bk">Other Methods</h1><h2 id="c7bf" class="pu on fq bf ou pv pw px ox py pz qa pa nm qb qc qd nq qe qf qg nu qh qi qj qk bk">Filter Methods — Chi-Squared</h2><p id="e153" class="pw-post-body-paragraph nd ne fq nf b go po nh ni gr pp nk nl nm pq no np nq pr ns nt nu ps nw nx ny fj bk">One of the simplest alternatives is to evaluate each feature individually using a statistial test and retain the top <em class="oh">k </em>features based on their scores. Notice that this approach doesn’t require any machine learning model. For example, for the classification task, we can choose the chi-squared test, which determines whether there is a statistically significant association between each feature and the target variable. We’ll use the <a class="af nc" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html" rel="noopener ugc nofollow" target="_blank">SelectKBest</a> class from scikit-learn, which applies the score function (chi-squared) to each feature and returns the top <em class="oh">k</em> scoring variables. Unlike the Optuna method, the number of features isn’t determined in the selection process, but must be set beforehand. In this case, we’ll set this number at ten. These methods fall within the filter methods class. They tend to be the easiest and fastest to compute since they don’t require any model behind.</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="6baf" class="om on fq oj b bg oo op l oq or">from sklearn.feature_selection import SelectKBest, chi2<br/><br/>skb = SelectKBest(score_func=chi2, k=10)<br/>skb.fit(X_train,y_train)<br/><br/>scores = pd.DataFrame(skb.scores_)<br/>cols = pd.DataFrame(X_train.columns)<br/>featureScores = pd.concat([cols,scores],axis=1)<br/>featureScores.columns = ['feature','score']<br/>featureScores.nlargest(10, 'score')</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ql"><img src="../Images/c5caab606183aa19ef91c6caf5dfd41b.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*D5GZp5Q1QNx7uSHs4SQLEA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure><p id="fcd3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In our case, <em class="oh">ram </em>scored the highest by far in the chi-squared test, followed by <em class="oh">px_height </em>and <em class="oh">battery_power</em>. Notice that these features were also selected by our Optuna method above, along with <em class="oh">px_width</em>, <em class="oh">mobile_wt </em>and <em class="oh">sc_w</em>. However, there are some new additions like <em class="oh">int_memory </em>and <em class="oh">talk_time</em> — these weren’t picked by the Optuna study. After training the random forest with these 10 features and evaluating it on the test set, we achieved an F1 score slightly higher than our previous best, at approximately 0.888:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qm"><img src="../Images/6d68dc4ce15b284ad414bbc2ead91a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*Jr4Jx8NgWifxqGfmgMoU-w.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure><p id="c578" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Pros</strong>:</p><ul class=""><li id="58a4" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Model agnostic: doesn’t require a machine learning model.</li><li id="ede6" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Easy and fast to implement and run.</li></ul><p id="4bae" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Cons</strong>:</p><ul class=""><li id="231d" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">It has to be adapted for each task. For instance, some score functions are only applicable for classification tasks, and others only for regression tasks.</li><li id="a09c" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Greedy: depending on the alternative used, it usually looks at features one by one, without taking into account which are already included in the set.</li><li id="b295" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Requires the number of features to select to be set beforehand.</li></ul><h2 id="d782" class="pu on fq bf ou pv pw px ox py pz qa pa nm qb qc qd nq qe qf qg nu qh qi qj qk bk">Wrapper Methods — Forward Search</h2><p id="6a34" class="pw-post-body-paragraph nd ne fq nf b go po nh ni gr pp nk nl nm pq no np nq pr ns nt nu ps nw nx ny fj bk">Wrapper methods are another class of feature selection strategies. These are iterative methods; they involve training the model with a set of features, evaluating its performance, and then deciding whether to add or remove features. Our Optuna strategy falls within these methods. However, most common examples include forward selection or backward selection. With forward selection, we begin with no features and, at each step, we greedily add the feature that provides the highest performance gain, until a stop criterion is met (number of features or performance decline). Conversely, backward selection starts with all features and iteratively removes the least significant ones at each step.</p><p id="18fd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Below, we try the <a class="af nc" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html" rel="noopener ugc nofollow" target="_blank">SequentialFeatureSelector</a> class from scikit-learn, performing a forward selection until we find the top 10 features. This method will also make use of the 5-fold split we performed above, averaging performance across the validation splits at each step.</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="a0f2" class="om on fq oj b bg oo op l oq or">from sklearn.feature_selection import SequentialFeatureSelector<br/><br/>model = RandomForestClassifier(random_state=SEED)<br/>sfs = SequentialFeatureSelector(model, n_features_to_select=10, cv=splits)<br/>sfs.fit(X_train, y_train);<br/><br/>selected_features = list(X_train.columns[sfs.get_support()])<br/>print(selected_features)</span></pre><p id="adde" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This method ends up selecting the following features:</p><p id="da68" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[<em class="oh">‘battery_power’, ‘blue’, ‘fc’, ‘mobile_wt’, ‘px_height’, ‘px_width’, ‘ram’, ‘talk_time’, ‘three_g’, ‘touch_screen’</em>]</p><p id="610b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Again, some are common to the previous methods, and some are new (e.g., <em class="oh">three_g </em>and <em class="oh">touch_screen</em>. Using these features, the Random Forest achieves a lower test F1 score, slightly below 0.88.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qn"><img src="../Images/e201b5306398514db1b2d9345d072136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*SX6J0jkm1HdDG4dpmptsIg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure><p id="5e63" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Pros</strong></p><ul class=""><li id="36de" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Easy to implement in just a few lines of code.</li><li id="8869" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">It can also be used to determine the number of features to use (using the tolerance parameter).</li></ul><p id="57ea" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Cons</strong></p><ul class=""><li id="94db" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Time consuming: Starting with zero features, it trains the model each time using a different variable, and retains the best one. For the next step, it again tries out all features (now including the previous one), and again selects the best one. This is repeated until the desired number of features is reached.</li><li id="5c20" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Greedy: Once a feature is included, it stays. This may lead to suboptimal results, as the feature providing the highest individual gain in early rounds might not be the best choice in the context of other feature interactions.</li></ul><h2 id="b514" class="pu on fq bf ou pv pw px ox py pz qa pa nm qb qc qd nq qe qf qg nu qh qi qj qk bk">Feature Importance</h2><p id="72a4" class="pw-post-body-paragraph nd ne fq nf b go po nh ni gr pp nk nl nm pq no np nq pr ns nt nu ps nw nx ny fj bk">Finally, we’ll explore another straightforward selection strategy, which involves using the feature importances the model learns (if available). Certain models, like Random Forests, provide a measure of which features are most important for prediction. We can use these rankings to filter out those features that, according to the model, have the least importance. In this case, we train the model on the entire train dataset, and retain the 10 most important features:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="973b" class="om on fq oj b bg oo op l oq or">model = RandomForestClassifier(random_state=SEED)<br/>model.fit(X_train,y_train)<br/><br/>importance = pd.DataFrame({'feature':X_train.columns, 'importance':model.feature_importances_})<br/>importance.nlargest(10, 'importance')</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qo"><img src="../Images/331f3e642958ee301da7619b9775d6e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*qa6Bdlo6huptex1g7YcDmA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure><p id="f5e2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Notice how, once again, <em class="oh">ram </em>is ranked highest, far above the second most important feature. Training with these 10 features, we obtain a test F1 score of almost 0.883, similar to the ones we’ve been seeing. Also, note how the features selected through feature importance are the same as those selected using the chi-squared test, although they are ranked differently. This difference in ranking results in a slightly different outcome.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qp"><img src="../Images/723f66b75c094da32fcf5cc1c88cef56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*M4-qz8AOa6GRb_hN8F5dVg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure><p id="5d42" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Pros</strong>:</p><ul class=""><li id="4ac0" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Easy and fast to implement: it requires a single training of the model and directly uses the derived feature importances.</li><li id="814f" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">It can be adapted into a recursive version, in which at each step the least important feature is removed and the model is then trained again (see <a class="af nc" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank">Recursive Feature Elimination</a>).</li><li id="ca6d" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Contained within the model: If the model we are using provides feature importances, we already have a feature selection alternative available at no additional cost.</li></ul><p id="127a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Cons</strong>:</p><ul class=""><li id="a43c" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Feature importance might not be aligned with our end goal. For instance, a feature might appear unimportant on its own but could be critical due to its interaction with other features. Also, an important feature might be counterproductive overall, by affecting the performance of other useful predictors.</li><li id="f480" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Not all models offer feature importance estimation.</li><li id="3241" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Requires the number of features to select to be predefined.</li></ul><h1 id="03dd" class="ot on fq bf ou ov ow gq ox oy oz gt pa pb pc pd pe pf pg ph pi pj pk pl pm pn bk">Closing Remarks</h1><p id="0ab6" class="pw-post-body-paragraph nd ne fq nf b go po nh ni gr pp nk nl nm pq no np nq pr ns nt nu ps nw nx ny fj bk">To conclude, we’ve seen how to use Optuna, a powerful optimization tool, for the feature selection task. By efficiently navigating the search space, it is able to find good feature subsets with relatively few trials. Not only that, but it is also flexible and can be adapted to many scenarios as long as we have a model and a loss function defined.</p><p id="3f1e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Throughout our examples, we observed that all techniques yielded similar feature sets and results. This is mainly because the dataset we used is rather simple. In these cases, simpler methods already produce a good feature selection, so it wouldn’t make much sense to go with the Optuna approach. However, for more complex datasets, with more features and intricate relationships between them, using Optuna might be a good idea. So, all in all, given its relative ease of implementation and ability to deliver good results, using Optuna for feature selection is a worthwhile addition to the data scientist’s toolkit.</p><p id="3bd1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Thanks for reading!</p></div></div></div><div class="ab cb qq qr qs qt" role="separator"><span class="qu by bm qv qw qx"/><span class="qu by bm qv qw qx"/><span class="qu by bm qv qw"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="17cf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Colab Notebook: <a class="af nc" href="https://colab.research.google.com/drive/193Jwb0xXWh_UkvwIiFgufKEYer-86RNA?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://colab.research.google.com/drive/193Jwb0xXWh_UkvwIiFgufKEYer-86RNA?usp=sharing</a></p></div></div></div></div>    
</body>
</html>