["```py\ndune_meta = {\n    'Dune': {'url': 'https://dune.fandom.com/wiki/Dune_(novel)'},\n    'Dune Messiah': {'url': 'https://dune.fandom.com/wiki/Dune_Messiah'},\n    'Children of Dune': {'url': 'https://dune.fandom.com/wiki/Children_of_Dune_(novel)'}\n}\n\nfor book, url in dune_meta.items():\n    sauce = urlopen(url['url']).read()\n    soup  = bs.BeautifulSoup(sauce,'lxml')\n    dune_meta[book]['chars'] = soup.find_all('li')\n```", "```py\ndune_meta['Dune']['char_start'] = 'Abulurd'\ndune_meta['Dune']['char_end'] = 'Arrakis'\ndune_meta['Dune Messiah']['char_start'] = 'Abumojandis'\ndune_meta['Dune Messiah']['char_end'] = 'Arrakis'\ndune_meta['Children of Dune']['char_start'] = '2018 Edition'\ndune_meta['Children of Dune']['char_end'] = 'Categories'\n```", "```py\nfor k, v in dune_meta.items():\n    names_urls = {}\n    keep_row = False\n    print(f'----- {k} -----')\n    for char in v['chars']:\n        if v['char_start'] in char.text.strip():\n            keep_row = True\n        if v['char_end'] in char.text.strip():\n            keep_row = False\n        if keep_row and 'Video' not in char.text:\n            try:\n                url = 'https://dune.fandom.com' + str(char).split('href=\"')[1].split('\" title')[0]\n                name = char.text.strip()\n                if 'wiki' in url and 'XD' not in name and 'DE' not in name and '(Mentioned only)' not in name:\n                    names_urls[name] = url\n                    print(name)\n            except:\n                pass\n    dune_meta[k]['names_urls'] = names_urls\n```", "```py\ndune_names_urls = {}\nfor k, v in dune_meta.items():\n    dune_names_urls.update(dune_meta[k]['names_urls'])\n\nnames_ids  = {n : u.split('/')[-1] for n, u in dune_names_urls.items()}\n\nprint(len(dune_names_urls))\n```", "```py\n# output folder for the profile htmls\nfolderout = 'fandom_profiles'\nif not os.path.exists(folderout):\n    os.makedirs(folderout)\n\n# crawl and save the profile htmls\nfor ind, (name, url) in enumerate(dune_names_urls.items()):\n    if not os.path.exists(folderout + '/' + name + '.html'):\n        try:\n            fout = open(folderout + '/' + name + '.html', \"w\")\n            fout.write(str(urlopen(url).read()))\n        except:\n            pass\n```", "```py\n# extract the name mentions from the html sources\n# and build the list of edges in a dictionary\nedges = {}\n\nfor fn in [fn for fn in os.listdir(folderout) if '.html' in fn]:\n\n    name = fn.split('.html')[0]\n\n    with open(folderout + '/' + fn) as myfile:\n        text = myfile.read()\n        soup  = bs.BeautifulSoup(text,'lxml')\n        text = ' '.join([str(a) for a in soup.find_all('p')[2:]])\n        soup = bs.BeautifulSoup(text,'lxml')\n\n        for n, i in names_ids.items():\n\n            w = text.split('Image Gallery')[0].count('/' + i) \n            if w>0:\n                edge = '\\t'.join(sorted([name, n]))\n                if edge not in edges:\n                    edges[edge] = w\n                else:\n                    edges[edge] += w\n\nlen(edges)\n```", "```py\n#  create the networkx graph from the dict of edges\nimport networkx as nx\nG = nx.Graph()\nfor e, w in edges.items():\n    if w>0:\n        e1, e2 = e.split('\\t')\n        G.add_edge(e1, e2, weight=w)\n\nG.remove_edges_from(nx.selfloop_edges(G))\n\nprint('Number of nodes: ', G.number_of_nodes())\nprint('Number of edges: ', G.number_of_edges())\n```", "```py\n# take a very brief look at the network\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(1,1,figsize=(15,15))\nnx.draw(G, ax=ax, with_labels=True)\n```", "```py\nnx.write_gexf(G, 'dune_network.gexf')\n```"]