# LLM中的认知提示

> 原文：[https://towardsdatascience.com/cognitive-prompting-in-llms-6234f6ab9a4b?source=collection_archive---------3-----------------------#2024-10-19](https://towardsdatascience.com/cognitive-prompting-in-llms-6234f6ab9a4b?source=collection_archive---------3-----------------------#2024-10-19)

## 我们能否教会机器像人类一样思考？

[](https://medium.com/@Oliver_Kramer?source=post_page---byline--6234f6ab9a4b--------------------------------)[![Oliver Kramer](../Images/1687be9e91f7e308df737b4d2c020116.png)](https://medium.com/@Oliver_Kramer?source=post_page---byline--6234f6ab9a4b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6234f6ab9a4b--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6234f6ab9a4b--------------------------------) [Oliver Kramer](https://medium.com/@Oliver_Kramer?source=post_page---byline--6234f6ab9a4b--------------------------------)

·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page---byline--6234f6ab9a4b--------------------------------) ·8分钟阅读·2024年10月19日

--

![](../Images/3e0141f321233bbc647675321cf0a43d.png)

图片由GPT-4o生成

## 介绍

当我开始学习人工智能时，最吸引我注意的一个想法是机器像人类一样思考。但是，当我仔细研究人工智能和机器学习方法到底在做什么时，我感到惊讶的是，关于人类思维（即人类认知）与机器思维之间实际上存在巨大的差距。对于我来说，这些差距的例子包括：感知机是如何工作的，它常被称为“受其生物学对偶体启发”，以及真实神经元是如何工作的。又如模糊逻辑如何试图建模人类的信息和推理概念，而人类推理实际上又是如何运作的。或者，人与人是如何通过观察并在黑板上围绕点云画圈来聚类一团点云的，而像DBSCAN和k-means这样的算法是如何完成这项任务的。

但现在，像ChatGPT、Claude和LLaMA这样的LLM已经成为焦点。它们基于数十亿甚至万亿的人工神经元和机制，而这些机制在认知中也起着重要作用：注意力（显然，这就是你所需要的一切）。我们已经走了很长一段路，同时诺贝尔奖也颁发给了该领域早期的伟大人物。LLM在总结文章、生成代码，甚至回答复杂问题和进行创造性思维方面取得了巨大的成功。关键点是——毫无疑问——正确的提示。你越明确地指定你希望模型做什么，结果就越好。提示工程已经成为一个不断发展的领域，甚至成为了一项专门的工作（尽管我个人怀疑这个角色的长期前景）。已经提出了许多提示策略：著名的有思维链（CoT）[2]或思维树（ToT）[3]，这些策略通过逐步提供成功问题解决的示例，来引导语言模型进行推理。但这些步骤通常是具体的示例，并且需要明确设计一个解决方案链。

其他方法试图优化提示，例如使用进化算法（EAs）如PromptBreeder。就我个人而言，我认为进化算法始终是一个好主意。最近，苹果的一支研究团队表明，LLM（大语言模型）很容易因为不同的提示而从解决问题的过程中分心[4]。由于关于CoT（思维链）和提示设计的许多优秀文章，尤其是在TDS上的文章（比如[这里](/create-your-own-prompt-enhancer-from-scratch-b870963a1ca0)的最新文章），我觉得没有必要在这里详细回顾。

## **什么是认知提示？**

仍然有一些东西缺失，因为显然与认知科学之间存在差距。这一切让我开始思考：我们能否帮助这些模型“更像人类一样思考”，如果可以，应该如何做？如果它们能够通过认知科学所称的认知操作来进行引导会怎样？例如，通过逐步拆解问题，过滤掉不必要的信息，并识别出在可用信息中存在的模式。这听起来有点像我们在解决难题时所做的事情。

这就是**认知提示**的作用所在。想象一下，人工智能不仅能够回答你的问题，还能通过“思考”结构化的步骤，指引自己——以及在你阅读它的输出时——通过复杂的解决问题的过程。

想象你正在解决一个数学应用题。你做的第一件事可能是明确你的目标：我究竟需要弄清楚什么，我们期望的结果是什么？接着，你将问题分解成更小的步骤，一种有前景的方法是识别相关信息，或许注意到一些有助于引导你思考的模式，帮助你更接近期望的解决方案。在这个示例中，我们将这些步骤称为目标明确、分解、过滤和模式识别。它们都是我们本能地执行的**认知操作**（COPs）的例子（或者在最好的情况下，我们被教师教导要遵循这些步骤）。

## **但这到底是如何工作的呢？**

下面是该过程的展开方式。我们定义了一个认知操作（COP）序列，并要求LLM按照这个序列进行操作。图1展示了提示的一个示例。以下是一些证明重要的COP示例：

+   **目标明确**：模型首先需要清晰地重述问题——它究竟在解决什么问题，期望的结果是什么？

+   **分解**：接下来，将问题分解成可管理的小部分。模型不应该被所有可用的信息压倒，而应该集中精力解决小部分——一步一步来。

+   **过滤**：要求模型过滤掉不必要的细节，专注于真正重要的内容。这通常是必要的，可以帮助模型将注意力集中在真正重要的信息上。

+   **模式识别**：识别模式，以有效地解决问题。例如，如果一个问题涉及重复的步骤，可以让模型识别出一个模式并应用它。

+   **整合**：最终，结合前面步骤中的所有见解，特别是基于最后几个COP的见解，并将其整合成最终答案的解决方案是有意义的。

这些结构化的步骤模仿了人类解决问题的方式——逻辑地、一步步地进行。还有许多进一步的认知操作，以及在选择操作时，如何选择、选择的顺序和如何为提示指定它们。显然，这为进一步改进留下了空间。

我们已经在以下方式上扩展了这一方法。我们不再遵循**确定性**的认知操作顺序，而是允许模型根据提供的列表自由选择认知操作的顺序——这被称为**自适应**认知提示。事实证明，这种方法效果相当不错。在下一段中，我们将比较两种变体在基准问题集上的表现。

![](../Images/2be503ad4f150dc29b7282ab83caafe5.png)

图1：认知提示：左侧是指导LLM推理的认知操作（COPs）的一般列表，右侧是专门适用于算术推理的版本。

另外，证明可以提高性能的是将COP描述适应于特定的问题领域。图1右侧展示了一个将一般COPs适应于数学问题的示例。这些COP“展开”成类似“清晰定义每个变量”或“逐步解决方程式”的提示。

在实际操作中，建议模型将最终答案以 JSON 字符串的形式输出是有意义的。一些大型语言模型（LLM）并不直接给出解决方案，而是提供用于解决问题的 Python 代码。在我们的实验分析中，我们秉持公正的原则，运行代码并将 Python 代码返回正确结果时的答案视为正确。

## 示例

让我们通过一个简短的示例，要求 LLaMA3.1 70B 解决来自 GSM8K [5] 的 8.5k 算术问题之一。图 2 显示了这个请求。

![](../Images/03d4b674bbf5dee7aa31b46322fb372e.png)

图 2：这是一个使用确定性认知提示进行算术推理的示例。

图 3 显示了模型的输出，得出了正确答案。事实证明，模型系统地遵循了 COP 的序列——甚至为人类提供了一个很好的问题解决解释。

![](../Images/faa60de9df990a31eeceea2dc90ee9c5.png)

图 3：LLaMA3.1 70B 对图 3 所示的基于认知提示的问题解决请求的输出。

## **认知提示的表现如何——从科学角度看？**

现在，让我们通过在一个典型的基准测试上测试认知提示，变得更加系统化。我们在 GSM8K [5] 数据集上的一组数学问题上进行了测试——这基本上是你在小学时会遇到的一些数学题。再次使用 Meta 的 LLaMA 模型，看看认知提示是否能提升它们的解题能力，我们应用了具有 80 亿参数的 LLaMA 和具有 700 亿参数的更大版本。

图 4 显示了一些结果。较小的模型在确定性认知提示下略有改进。也许它的规模还不足以应对结构化思维的复杂性。当它选择自己的 COP 序列时，性能显著提升。

![](../Images/faf494200350f635af5db4e5a8998bbd.png)

图 4：左侧为认知提示在 GSM8k 基准上的结果，右侧为所选 COP 序列的直方图（目标澄清 (GC)、分解 (DC)、模式识别 (PR)、归纳 (GN) 和重组 (RE)）。

没有认知提示时，较大的模型在数学问题上的得分大约为 87%。当我们加入了**确定性认知提示**（即模型遵循固定的认知步骤序列）时，得分跃升至 89%。但当我们允许模型动态调整并选择认知操作（自适应提示）时，得分飙升至 91%。对于一台获得了相当一般的推理建议的机器——没有额外的示例，表现不错吧？

## **这为什么重要？**

认知提示是一种将类似人类的认知操作组织成结构化过程的方法，并利用这些操作帮助 LLMs 解决复杂问题。从本质上讲，它就像是给模型提供了一个结构化的“思维策略”供其遵循。尽管像 CoT 这样的早期方法已经提供了帮助，认知提示通过结合各种认知操作，提供了更深层次的推理。

这对数学问题之外的领域也有令人兴奋的意义！想想决策、逻辑推理，甚至创造力等领域——这些任务不仅仅是复述事实或预测句子中的下一个词。通过教会AI像我们一样思考，我们为模型能够以更接近人类认知的方式推理问题打开了大门。

## **我们从这里开始往哪里去？**

结果很有前景，但这只是一个开始。认知提示肯定可以适应其他领域，也可以与AI的其他思路结合。随着我们探索更先进的认知提示版本，下一个重大挑战将是如何优化它以应对不同类型的问题。谁知道呢？也许有一天，我们会拥有能够解决从数学问题到道德困境的AI，同时它的思维方式既合乎逻辑又充满创造力。自己尝试认知提示，享受其中的乐趣吧！

## 参考文献

[1] O. Kramer, J. Baumann. [通过认知提示解锁语言模型中的结构化思维](https://arxiv.org/abs/2410.02953)（arXiv）

[2] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, 和 D. Zhou. 链式思维提示引发大型语言模型中的推理。在 S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho 和 A. Oh 编辑的《神经信息处理系统（NeurIPS）》研讨会，卷 35，第 24824–24837 页，2022年

[3] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, 和 K. Narasimhan. 思维树：使用大型语言模型进行深思熟虑的解决问题。在《神经信息处理系统（NeurIPS）》中，卷 36，第 11809–11822 页，2023年

[4] I. Mirzadeh, K. Alizadeh, H. Shahrokhi, O. Tuzel, S. Bengio, 和 M. Farajtabar. [GSM-Symbolic: 理解大型语言模型中数学推理的局限性。](https://arxiv.org/pdf/2410.05229) 2024年。

[5] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, 和 J. Schulman. 训练验证器解决数学文字题。arXiv预印本 arXiv:2110.14168, 2021年。
