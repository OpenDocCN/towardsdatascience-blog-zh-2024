["```py\nimport os\n\n# choose backend\nbackend = 'jax' # 'torch'\nos.environ[\"KERAS_BACKEND\"] = backend\n\nimport keras\nfrom keras import layers\nfrom keras import ops\n\n# set mixed precision policy\nkeras.mixed_precision.set_global_policy('mixed_bfloat16')\n\n# use ViT Base settings\nnum_classes = 1000\nimage_size = 224\ninput_shape = (image_size, image_size, 3)\npatch_size = 16  # Size of the patches to be extract from the input images\nnum_patches = (image_size // patch_size) ** 2\nprojection_dim = 768\nnum_heads = 12\ntransformer_units = [\n    projection_dim * 4,\n    projection_dim,\n]  # Size of the transformer layers\ntransformer_layers = 12\n\n# set training hyperparams\nbatch_size = 128\nmulti_worker = False # toggle to use multiple data loader workers\npreproc_workers = 0 if 'jax' else 16\n\n# ViT model components:\n# ---------------------\n\ndef mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x\n\nclass Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super().__init__()\n        self.patch_size = patch_size\n\n    def call(self, images):\n        input_shape = ops.shape(images)\n        batch_size = input_shape[0]\n        height = input_shape[1]\n        width = input_shape[2]\n        channels = input_shape[3]\n        num_patches_h = height // self.patch_size\n        num_patches_w = width // self.patch_size\n        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n        patches = ops.reshape(\n            patches,\n            (\n                batch_size,\n                num_patches_h * num_patches_w,\n                self.patch_size * self.patch_size * channels,\n            ),\n        )\n        return patches\n\nclass PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim):\n        super().__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(units=projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n\n    def call(self, patch):\n        positions = ops.expand_dims(\n            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n        )\n        projected_patches = self.projection(patch)\n        encoded = projected_patches + self.position_embedding(positions)\n        return encoded\n```", "```py\n# the attention layer we will use in our ViT classifier\nattention_layer = layers.MultiHeadAttention\n\ndef create_vit_classifier():\n    inputs = keras.Input(shape=input_shape)\n    # Create patches.\n    patches = Patches(patch_size)(inputs)\n    # Encode patches.\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n\n    # Create multiple layers of the Transformer block.\n    for _ in range(transformer_layers):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        # Create a multi-head attention layer.\n        attention_output = attention_layer(\n            num_heads=num_heads, key_dim=projection_dim//num_heads, dropout=0.1\n        )(x1, x1)\n        # Skip connection 1.\n        x2 = layers.Add()([attention_output, encoded_patches])\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        # MLP.\n        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n        # Skip connection 2.\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Create a [batch_size, projection_dim] tensor.\n    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    representation = layers.GlobalAveragePooling1D()(representation)\n    representation = layers.Dropout(0.5)(representation)\n\n    # Classify outputs.\n    logits = layers.Dense(num_classes)(representation)\n\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=logits)\n    return model\n\n# create the ViT model\nmodel = create_vit_classifier()\nmodel.summary()\n```", "```py\nmodel.compile(\n    optimizer=keras.optimizers.SGD(),\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    )\n\ndef get_data_loader(batch_size):\n    import torch\n    from torch.utils.data import Dataset, DataLoader\n\n    # create dataset of random image and label data\n    class FakeDataset(Dataset):\n        def __len__(self):\n            return 1000000\n\n        def __getitem__(self, index):\n            rand_image = torch.randn([224, 224, 3], dtype=torch.float32)\n            label = torch.tensor(data=[index % 1000], dtype=torch.int64)\n            return rand_image, label\n\n    ds = FakeDataset()\n    dl = DataLoader(\n        ds,\n        batch_size=batch_size,\n        num_workers=preproc_workers if multi_worker else 0,\n        pin_memory=True\n    )\n    return dl\n\ndl = get_data_loader(batch_size)\n```", "```py\nmodel.fit(\n    dl,\n    batch_size=batch_size,\n    epochs=1\n)\n```", "```py\n formatted += f\" {time_per_unit:.3f}s/{unit_name}\"\n```", "```py\nclass MyAttention(layers.MultiHeadAttention):\n    def _compute_attention(\n            self, query, key, value, attention_mask=None, training=None\n    ):\n        from torch.nn.functional import scaled_dot_product_attention\n        query = ops.multiply(\n            query, ops.cast(self._inverse_sqrt_key_dim, query.dtype))\n        return scaled_dot_product_attention(\n            query.transpose(1,2),\n            key.transpose(1,2),\n            value.transpose(1,2),\n            dropout_p=self._dropout if training else 0.\n            ).transpose(1,2), None\n\nattention_layer = MyAttention\n```", "```py\nimport os\nbackend = 'jax' #'torch'\nos.environ[\"KERAS_BACKEND\"] = backend\n\nnum_batches = 1000\nbatch_size = 4 if backend == 'jax' else 2\n\n# Avoid memory fragmentation on JAX backend.\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"\nos.environ[\"KAGGLE_USERNAME\"]=\"chaimrand\"\nos.environ[\"KAGGLE_KEY\"]=\"29abebb28f899a81ca48bec1fb97faf1\"\nimport keras\nimport keras_nlp\nkeras.mixed_precision.set_global_policy('mixed_bfloat16')\n\nimport json\ndata = []\nwith open(\"databricks-dolly-15k.jsonl\") as file:\n    for line in file:\n        features = json.loads(line)\n        # Filter out examples with context, to keep it simple.\n        if features[\"context\"]:\n            continue\n        # Format the entire example as a single string.\n        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n        data.append(template.format(**features))\n\n# Only use 1000 training batches, to keep it fast.\ndata = data[:num_batches*batch_size]\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n# Enable LoRA for the model and set the LoRA rank to 4.\ngemma_lm.backbone.enable_lora(rank=4)\n\ngemma_lm.summary()\n# Limit the input sequence length to 512 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = 512\n\ngemma_lm.compile(\n   loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n   optimizer=keras.optimizers.SGD(learning_rate=5e-5),\n   weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\ngemma_lm.fit(data, epochs=1, batch_size=batch_size)\n```", "```py\nimport torch\ntorch.set_float32_matmul_precision('high')\n```"]