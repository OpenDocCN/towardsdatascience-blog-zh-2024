- en: AI Model Optimization on AWS Inferentia and Trainium
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在AWS Inferentia和Trainium上优化AI模型
- en: 原文：[https://towardsdatascience.com/ai-model-optimization-on-aws-inferentia-and-trainium-cfd48e85d5ac?source=collection_archive---------3-----------------------#2024-10-20](https://towardsdatascience.com/ai-model-optimization-on-aws-inferentia-and-trainium-cfd48e85d5ac?source=collection_archive---------3-----------------------#2024-10-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ai-model-optimization-on-aws-inferentia-and-trainium-cfd48e85d5ac?source=collection_archive---------3-----------------------#2024-10-20](https://towardsdatascience.com/ai-model-optimization-on-aws-inferentia-and-trainium-cfd48e85d5ac?source=collection_archive---------3-----------------------#2024-10-20)
- en: Tips for accelerating ML with AWS Neuron SDK
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升ML性能的AWS Neuron SDK技巧
- en: '[](https://chaimrand.medium.com/?source=post_page---byline--cfd48e85d5ac--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--cfd48e85d5ac--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cfd48e85d5ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cfd48e85d5ac--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--cfd48e85d5ac--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page---byline--cfd48e85d5ac--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--cfd48e85d5ac--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cfd48e85d5ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cfd48e85d5ac--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--cfd48e85d5ac--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cfd48e85d5ac--------------------------------)
    ·9 min read·Oct 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cfd48e85d5ac--------------------------------)
    ·阅读时间9分钟·2024年10月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6cf33bb6d494f40d4338317fda1b9fb6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6cf33bb6d494f40d4338317fda1b9fb6.png)'
- en: Photo by [julien Tromeur](https://unsplash.com/@julientromeur?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[julien Tromeur](https://unsplash.com/@julientromeur?utm_source=medium&utm_medium=referral)
    via [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: We are in a golden age of AI, with cutting-edge models disrupting industries
    and poised to transform life as we know it. Powering these advancements are increasingly
    powerful AI accelerators, such as [NVIDIA H100 GPUs](https://www.nvidia.com/en-eu/data-center/h100/),
    [Google Cloud TPUs](https://cloud.google.com/tpu), [AWS’s Trainium](https://aws.amazon.com/machine-learning/trainium/)
    and [Inferentia](https://aws.amazon.com/machine-learning/inferentia/) chips, and
    more. With the growing number of options comes the challenge of [selecting the
    most optimal platform](/instance-selection-for-deep-learning-7463d774cff0) for
    our machine learning (ML) workloads — a crucial decision considering the high
    costs associated with AI computation. Importantly, a comprehensive assessment
    of each option necessitates ensuring that we are maximizing its utilization to
    fully leverage its capabilities.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正处于人工智能的黄金时代，前沿模型正在颠覆各行各业，并有望改变我们所知的生活方式。这些进步的推动力是日益强大的AI加速器，如[NVIDIA H100
    GPUs](https://www.nvidia.com/en-eu/data-center/h100/)、[Google Cloud TPUs](https://cloud.google.com/tpu)、[AWS的Trainium](https://aws.amazon.com/machine-learning/trainium/)和[Inferentia](https://aws.amazon.com/machine-learning/inferentia/)芯片等。随着选项数量的增加，面临的挑战是[选择最优平台](/instance-selection-for-deep-learning-7463d774cff0)来处理我们的机器学习（ML）工作负载——这是一个至关重要的决策，因为与AI计算相关的成本非常高。重要的是，全面评估每个选项需要确保我们正在最大化其利用率，以充分发挥其能力。
- en: In this post, we will review several techniques for optimizing an ML workload
    on AWS’s custom-built AI chips using the [AWS Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html).
    This continues our ongoing series of posts focused on ML model performance analysis
    and optimization across various platforms and environments (e.g., see [here](/pytorch-model-performance-analysis-and-optimization-10c3c5822869)
    and [here](/training-ai-models-on-cpu-3903adc9f388)). While our primary focus
    will be on an ML training workload and AWS Inferentia2, the techniques discussed
    are also applicable to AWS Trainium. (Recall that although AWS Inferentia is primarily
    designed as an AI inference chip, we have [previously demonstrated](/dl-training-on-aws-inferentia-53e103597a03)
    its effectiveness in training tasks as well.)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将回顾几种优化AWS定制AI芯片上的ML工作负载的技术，使用的是[AWS Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)。这继续了我们关于跨各种平台和环境的ML模型性能分析和优化的系列文章（例如，参见[此处](/pytorch-model-performance-analysis-and-optimization-10c3c5822869)和[此处](/training-ai-models-on-cpu-3903adc9f388)）。虽然我们的主要关注点将是ML训练工作负载和AWS
    Inferentia2，但所讨论的技术同样适用于AWS Trainium。（回想一下，尽管AWS Inferentia主要设计为AI推理芯片，但我们[之前展示过](/dl-training-on-aws-inferentia-53e103597a03)它在训练任务中的有效性。）
- en: Generally speaking, performance optimization is an iterative process that includes
    a performance analysis step to appropriately identify performance bottlenecks
    and resource under-utilization (e.g., see [here](/cloud-ml-performance-checklist-caa51e798002)).
    However, since the techniques we will discuss are general purpose (i.e., they
    are potentially applicable to any model, regardless of their performance profile),
    we defer the discussion on [performance analysis with the Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/torch-neuronx-profiling-dev-guide.html)
    to a future post.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，性能优化是一个迭代过程，其中包括性能分析步骤，用于适当识别性能瓶颈和资源利用不足（例如，参见[此处](/cloud-ml-performance-checklist-caa51e798002)）。然而，由于我们将讨论的技术是通用的（即，它们可能适用于任何模型，无论其性能特征如何），我们将[Neuron
    SDK性能分析](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/torch-neuronx-profiling-dev-guide.html)的讨论推迟到未来的文章中。
- en: Disclaimers
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 免责声明
- en: The code we will share is intended for demonstrative purposes only — we make
    no claims regarding its accuracy, optimality, or robustness. Please do not view
    this post as a substitute for the official [Neuron SDK documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html).
    Please do not interpret our mention of any platforms, libraries, or optimization
    techniques as an endorsement for their use. The best options for you will depend
    greatly on the specifics of your use-case and will require your own in-depth investigation
    and analysis.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分享的代码仅用于演示目的——我们不对其准确性、最优性或健壮性做任何声明。请不要将本文视为官方[Neuron SDK文档](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)的替代品。请不要将我们提到的任何平台、库或优化技术解读为对其使用的支持。最适合您的选项将极大地依赖于您的具体用例，并且需要您自己深入的调查和分析。
- en: The experiments described below were run on an [Amazon EC2 inf2.xlarge](https://aws.amazon.com/ec2/instance-types/inf2/)
    instance (containing two [Neuron cores](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html#neuroncores-v2-arch)
    and four vCPUs). We used the most recent version of the [Deep Learning AMI for
    Neuron](https://aws.amazon.com/releasenotes/aws-deep-learning-ami-neuron-ubuntu-22-04/)
    available at the time of this writing, “Deep Learning AMI Neuron (Ubuntu 22.04)
    20240927”, with [AWS Neuron 2.20](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html)
    and [PyTorch 2.1](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/torch-neuronx/introducing-pytorch-2-1.html).
    See the [SDK documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/neuron-setup/multiframework/multi-framework-ubuntu22-neuron-dlami.html#setup-ubuntu22-multi-framework-dlami)
    for more details on setup and installation. Keep in mind that the Neuron SDK is
    under active development and that the APIs we refer to, as well as the runtime
    measurements we report, may become outdated by the time you read this. Please
    be sure to stay up-to-date with the latest SDK and documentation available.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下实验是在一个[Amazon EC2 inf2.xlarge](https://aws.amazon.com/ec2/instance-types/inf2/)实例上运行的（该实例包含两个[Neuron核心](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html#neuroncores-v2-arch)和四个vCPU）。我们使用了当时可用的最新版本[Deep
    Learning AMI for Neuron](https://aws.amazon.com/releasenotes/aws-deep-learning-ami-neuron-ubuntu-22-04/)，即“Deep
    Learning AMI Neuron（Ubuntu 22.04）20240927”，并配备了[AWS Neuron 2.20](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html)和[PyTorch
    2.1](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/torch-neuronx/introducing-pytorch-2-1.html)。有关设置和安装的更多详细信息，请参见[SDK文档](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/neuron-setup/multiframework/multi-framework-ubuntu22-neuron-dlami.html#setup-ubuntu22-multi-framework-dlami)。请记住，Neuron
    SDK正在积极开发中，我们提到的API和我们报告的运行时测量结果在您阅读本文时可能已经过时。请确保与最新的SDK和文档保持同步。
- en: Toy Model
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩具模型
- en: 'To facilitate our discussion, we introduce the following simple [Vision Transformer](https://en.wikipedia.org/wiki/Vision_transformer)
    (ViT)-backed classification model (based on [timm](https://pypi.org/project/timm/)
    version 1.0.10):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便讨论，我们引入以下简单的[Vision Transformer](https://en.wikipedia.org/wiki/Vision_transformer)（ViT）分类模型（基于[timm](https://pypi.org/project/timm/)版本1.0.10）：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Running our baseline model on the two cores of our AWS Inferentia instance,
    results in a training speed of 251.98 samples per second.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的AWS Inferentia实例的两个核心上运行基准模型，结果是每秒训练251.98个样本。
- en: 'In the next sections, we will iteratively apply a number of potential optimization
    techniques and assess their impact on step time performance. While we won’t go
    into the full details of each method, we will provide references for further reading
    (e.g., [here](/pytorch-model-performance-analysis-and-optimization-10c3c5822869)).
    Importantly, the list we will present is not all-inclusive — there are many techniques
    beyond what we will cover. We will organize the methods into three categories:
    PyTorch optimizations, OpenXLA optimizations, and Neuron-specific optimizations.
    However, the order of presentation is not binding. In fact, some of the techniques
    are interdependent — for example, applying the mixed precision optimization may
    free up enough device memory to enable increasing the batch size.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将迭代应用多种潜在的优化技术，并评估它们对步长时间性能的影响。虽然我们不会详细介绍每种方法，但会提供进一步阅读的参考（例如，[这里](/pytorch-model-performance-analysis-and-optimization-10c3c5822869)）。重要的是，我们将呈现的列表并非全面涵盖——还有许多技术超出了我们的讨论范围。我们将这些方法分为三类：PyTorch优化、OpenXLA优化和特定于Neuron的优化。然而，展示的顺序并不固定。实际上，某些技术是相互依赖的——例如，应用混合精度优化可能会释放出足够的设备内存，从而允许增加批量大小。
- en: PyTorch Performance Optimizations
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch性能优化
- en: In previous posts (e.g., [here](/pytorch-model-performance-analysis-and-optimization-10c3c5822869))
    we have covered the topic of PyTorch model performance analysis and optimization
    on GPU, extensively. Many of the techniques we discussed are relevant to other
    AI accelerators. In this section we will revisit few of these techniques and apply
    them to AWS Inferentia.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的文章中（例如，[这里](/pytorch-model-performance-analysis-and-optimization-10c3c5822869)），我们已经广泛讨论了在GPU上进行PyTorch模型性能分析和优化的话题。我们讨论的许多技术也适用于其他AI加速器。在本节中，我们将重新审视其中的一些技术，并将其应用于AWS
    Inferentia。
- en: Multi-process Data Loading
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多进程数据加载
- en: 'In [multi process data loading](https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading)
    the input data is prepared in one or more dedicated CPU processes rather than
    in the same process that runs the training step. This allows for overlapping the
    data loading and training which can increase system utilization and lead to a
    significant speed-up. The number of processes is controlled by the *num_workers*
    parameter of the [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).
    In the following block we run our script with *num_workers* set to one:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在[多进程数据加载](https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading)中，输入数据是在一个或多个专用的CPU进程中准备的，而不是在运行训练步骤的相同进程中进行处理。这允许数据加载和训练过程重叠，从而提高系统利用率并显著加速。进程数由[PyTorch
    DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)的*num_workers*参数控制。在下面的代码块中，我们将*num_workers*设置为1来运行脚本：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This change results in a training speed of 253.56 samples per second for a boost
    of less than 1%.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种变化导致训练速度为每秒253.56个样本，提升不到1%。
- en: Batch Size Optimization
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量大小优化
- en: Another important hyperparameter that can influence training speed is the training
    batch size. Often, we have found that increasing the batch size improves system
    utilization and results in better performance. However, the effects can vary based
    on the model and platform. In the case of our toy model on AWS Inferentia, we
    find that running with a batch size of 8 samples per neuron core results in a
    speed of 265.68 samples per second — roughly 5% faster than a batch size of 16
    samples per core.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能影响训练速度的重要超参数是训练批量大小。我们发现，增加批量大小通常可以提高系统利用率并带来更好的性能。然而，效果会根据模型和平台的不同而有所变化。在我们的AWS
    Inferentia玩具模型中，我们发现每个神经元核心使用8个样本的批量大小训练时，速度为每秒265.68个样本，比每核心16个样本的批量大小快大约5%。
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: PyTorch Automatic Mixed Precision
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch自动混合精度
- en: Another common method for boosting performance is to use lower precision floats
    such as the 16-bit BFloat16\. Importantly, some model components might not be
    compatible with reduced precision floats. PyTorch’s [Automatic Mixed Precision
    (AMP)](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html) mode attempts
    to match the most appropriate floating point type to each model operation automatically.
    Although, the Neuron compiler offers different options for employing [mixed precision](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#mixed-precision-and-performance-accuracy-tuning-neuronx-cc),
    it also [supports the option of using PyTorch AMP](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/training/pytorch-neuron-programming-guide.html#automatic-mixed-precision).
    In the code block below we include the modifications required to use PyTorch AMP.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 提升性能的另一个常用方法是使用较低精度的浮点数，例如16位的BFloat16。值得注意的是，某些模型组件可能与降低精度的浮点数不兼容。PyTorch的[自动混合精度（AMP）](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)模式会尝试自动将最合适的浮点类型匹配到每个模型操作上。尽管Neuron编译器提供了不同的[混合精度](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#mixed-precision-and-performance-accuracy-tuning-neuronx-cc)选项，它也[支持使用PyTorch
    AMP](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/training/pytorch-neuron-programming-guide.html#automatic-mixed-precision)的选项。在下面的代码块中，我们包括了使用PyTorch
    AMP所需的修改。
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The resultant training speed is 196.64 samples per second, about 26% lower than
    the [default mixed precision](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#neuronx-cc-training-mixed-precision)
    setting of the Neuron compiler. It’s important to note that while this post focuses
    on performance, in real-world scenarios, we would also need to evaluate the effect
    of the mixed precision policy we choose on [model accuracy](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#performance-accuracy-tradeoffs).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 结果训练速度为每秒196.64个样本，比Neuron编译器的[默认混合精度](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#neuronx-cc-training-mixed-precision)设置低约26%。需要注意的是，虽然本文重点讨论性能，但在实际应用场景中，我们还需要评估所选择的混合精度策略对[模型准确性](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#performance-accuracy-tradeoffs)的影响。
- en: OpenXLA Optimizations
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenXLA优化
- en: As discussed in a [previous post](/a-first-look-at-aws-trainium-1e0605071970),
    Neuron Cores are treated as [XLA devices](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/training/pytorch-neuron-programming-guide.html#neuron-xla-device)
    and the [torch-neuronx](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/quick-start/torch-neuron.html)
    Python package implements the [PyTorch/XLA](https://github.com/pytorch/xla/) API.
    Consequently, any optimization opportunities provided by the OpenXLA framework,
    and specifically those offered by the PyTorch/XLA API, can be leveraged on AWS
    Inferentia and Trainium. In this section we consider a few of these opportunities.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[之前的文章](/a-first-look-at-aws-trainium-1e0605071970)中讨论的那样，Neuron Cores被视为[XLA设备](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/training/pytorch-neuron-programming-guide.html#neuron-xla-device)，并且[torch-neuronx](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/quick-start/torch-neuron.html)
    Python包实现了[PyTorch/XLA](https://github.com/pytorch/xla/) API。因此，OpenXLA框架提供的任何优化机会，特别是PyTorch/XLA
    API所提供的优化，都可以在AWS Inferentia和Trainium上利用。在本节中，我们将考虑其中的一些优化机会。
- en: BFloat16 Precision
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BFloat16精度
- en: 'OpenXLA supports the option of [casting all floats to BFloat16](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/training/pytorch-neuron-programming-guide.html#automatic-casting-of-float-tensors-to-bfloat16)
    via the XLA_USE_BF16 environment variable, as shown in the code block below:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: OpenXLA支持通过XLA_USE_BF16环境变量将所有浮动数值转换为BFloat16，如下代码块所示：[将所有浮动数值转换为BFloat16](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/training/pytorch-neuron-programming-guide.html#automatic-casting-of-float-tensors-to-bfloat16)：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The resultant training speed is 394.51 samples per second, nearly 50% faster
    than the speed of the [default mixed precision](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#neuronx-cc-training-mixed-precision)
    option.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 结果训练速度为每秒394.51个样本，比[默认混合精度](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#neuronx-cc-training-mixed-precision)选项的速度快近50%。
- en: Multi-process Device Loading
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多进程设备加载
- en: 'The PyTorch/XLA [MpDeviceLoader](https://pytorch.org/xla/master/_modules/torch_xla/distributed/parallel_loader.html#MpDeviceLoader)
    and its internal [ParallelLoader](https://pytorch.org/xla/master/_modules/torch_xla/distributed/parallel_loader.html),
    which are responsible for loading input data on to the accelerator, include a
    number of parameters for controlling the transfer of data from the host to the
    device. In the code block below we tune [*batches_per_execution*](https://github.com/pytorch/xla/blob/v2.1.0/torch_xla/distributed/parallel_loader.py#L86)
    setting which determines the number of batches copied to the device for each execution
    cycle of the [ParallelLoader](https://pytorch.org/xla/master/_modules/torch_xla/distributed/parallel_loader.html).
    By increasing this setting, we aim to reduce the overhead of the host-to-device
    communication:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch/XLA的[MpDeviceLoader](https://pytorch.org/xla/master/_modules/torch_xla/distributed/parallel_loader.html#MpDeviceLoader)及其内部的[ParallelLoader](https://pytorch.org/xla/master/_modules/torch_xla/distributed/parallel_loader.html)，负责将输入数据加载到加速器上，包括多个参数，用于控制数据从主机到设备的传输。在下面的代码块中，我们调节[*batches_per_execution*](https://github.com/pytorch/xla/blob/v2.1.0/torch_xla/distributed/parallel_loader.py#L86)设置，该设置决定了每次[ParallelLoader](https://pytorch.org/xla/master/_modules/torch_xla/distributed/parallel_loader.html)执行周期中，复制到设备的批次数。通过增加该设置，我们旨在减少主机与设备之间通信的开销：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As a result of this optimization, the training speed increased to 1,027.39 samples
    per second, representing an additional 260% speed-up.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种优化，训练速度提高到每秒1,027.39个样本，代表着额外的260%的加速。
- en: Torch Compilation with OpenXLA Backend
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用OpenXLA后端进行Torch编译
- en: 'In previous posts (e.g., [here](/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d)),
    we have demonstrated the potential performance gains from using [PyTorch’s graph
    compilation](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    offering. Although [OpenXLA](https://openxla.org/xla) includes its own graph creation
    and Just-In-Time (JIT) compilation mechanisms, [torch.compile](https://pytorch.org/xla/master/torch_compile.html)
    can provide additional acceleration by eliminating the need for tracing the model
    operations at every step. The following code snippet demonstrates the use of the
    dedicated *openxla* backend for compiling the model:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的文章中（例如，[这里](/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d)），我们演示了使用[PyTorch的图编译](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)所带来的潜在性能提升。尽管[OpenXLA](https://openxla.org/xla)包含了自己的图创建和即时编译机制，[torch.compile](https://pytorch.org/xla/master/torch_compile.html)可以通过消除每一步追踪模型操作的需求，提供额外的加速。以下代码片段演示了如何使用专用的*openxla*后端来编译模型：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Although torch.compile is currently [not yet supported](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/torch/torch-neuronx/index.html#known-limitations)
    by the Neuron SDK, we include its mention in anticipation of its future release.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管torch.compile目前[尚不支持](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/torch/torch-neuronx/index.html#known-limitations)Neuron
    SDK，但我们提前提及它，以期望其未来发布。
- en: Neuron SDK Optimizations
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Neuron SDK优化
- en: In this section we consider some of the optimization opportunities offered by
    the AWS Neuron SDK and, more specifically, by the Neuron compiler.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将考虑AWS Neuron SDK提供的一些优化机会，特别是Neuron编译器所提供的优化。
- en: Mixed Precision
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合精度
- en: The Neuron SDK supports a variety of [mixed precision](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#mixed-precision-and-performance-accuracy-tuning-neuronx-cc)
    settings. In the code block below we program the compiler to cast all floats to
    BFloat16 via the *NEURON_CC_FLAGS* environment variable.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Neuron SDK支持多种[混合精度](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#mixed-precision-and-performance-accuracy-tuning-neuronx-cc)设置。在下面的代码块中，我们通过*NEURON_CC_FLAGS*环境变量编程，指示编译器将所有浮点数转换为BFloat16。
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This results (unsurprisingly) in a similar training speed to the OpenXLA BFloat16
    experiment described above.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致（不足为奇）与上文提到的OpenXLA BFloat16实验相似的训练速度。
- en: FP8
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FP8
- en: 'One of the unique features of NeuronCoreV2 is its support of the eight-bit
    floating point type, fp8_e4m3\. The code block below demonstrates how to configure
    the Neuron compiler to automatically cast all floating-point operations to FP8:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: NeuronCoreV2的一个独特特点是它支持八位浮点类型fp8_e4m3。下面的代码块演示了如何配置Neuron编译器，以自动将所有浮点操作转换为FP8：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: While FP8 can accelerate training in some cases, maintaining stable convergence
    can be more challenging than when using BFloat16 due its reduced precision and
    dynamic range. Please see our [previous post](/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7)
    for more on the potential benefits and challenges of FP8 training.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然FP8在某些情况下可以加速训练，但由于其精度和动态范围的降低，保持稳定的收敛性比使用BFloat16时更具挑战性。请参阅我们的[上一篇文章](/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7)，了解有关FP8训练的潜在好处和挑战。
- en: In the case of our model, using FP8 actually harms runtime performance compared
    to BFloat16, reducing the training speed to 940.36 samples per second.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型中，使用FP8实际上会比BFloat16更影响运行时性能，将训练速度降低到每秒940.36个样本。
- en: Compiler Optimizations
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编译器优化
- en: The [Neuron compiler](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/compiler/neuronx-cc/api-reference-guide/neuron-compiler-cli-reference-guide.html)
    includes a number of controls for optimizing the runtime performance of the compiled
    graph. Two key settings are *model-type* and *opt-level*. The *model-type* setting
    applies optimizations tailored to specific model architectures, such as transformers,
    while the *opt-level* setting allows for balancing compilation time against runtime
    performance. In the code block below, we program the *model-type* setting to *tranformer*
    and the *opt-level* setting to the highest performance option. We further specify
    the *target* runtime device, *inf2*, to ensure that the model is optimized for
    the target device.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[Neuron 编译器](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/compiler/neuronx-cc/api-reference-guide/neuron-compiler-cli-reference-guide.html)包含多个控制项，用于优化编译后的图的运行时性能。两个关键设置是*model-type*和*opt-level*。*model-type*设置应用于特定模型架构的优化，如变换器（transformers），而*opt-level*设置则允许在编译时间和运行时性能之间找到平衡。在下面的代码块中，我们将*model-type*设置为*tranformer*，将*opt-level*设置为最高性能选项。我们还进一步指定*target*运行设备为*inf2*，以确保模型针对目标设备进行了优化。'
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The above configuration resulted in a training speed of 1093.25 samples per
    second, amounting to a modest 6% improvement.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述配置实现了每秒1093.25个样本的训练速度，带来了约6%的改善。
- en: Results
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: We summarize the results of our experiments in the table below. Keep in mind
    that the effect of each of the optimization methods we discussed will depend greatly
    on the model and the runtime environment.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下表中总结了实验的结果。请记住，我们讨论的每种优化方法的效果将极大地依赖于模型和运行时环境。
- en: '![](../Images/fd373857d4f25e1c26fe23f03a8ce4d0.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd373857d4f25e1c26fe23f03a8ce4d0.png)'
- en: Experiment Results (by Author)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果（作者）
- en: The techniques we employed resulted in a 435% performance boost compared to
    our baseline experiment. It is likely that additional acceleration could be achieved
    by revisiting and fine-tuning some of the methods we discussed, or by applying
    other optimization techniques not covered in this post.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用的技术使性能相比基准实验提高了435%。通过重新审视和微调我们讨论的一些方法，或者应用本文未涉及的其他优化技术，可能还会实现进一步的加速。
- en: Our goal has been to demonstrate some of the available optimization strategies
    and demonstrate their potential impact on runtime performance. However, in a real-world
    scenario, we would need to assess the manner in which each of these optimizations
    impact our model convergence. In some cases, adjustments to the model configuration
    may be necessary to ensure optimal performance without sacrificing accuracy. Additionally,
    using a performance profiler to identify bottlenecks and measure system resource
    utilization is essential for guiding and informing our optimization activities.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是展示一些可用的优化策略，并展示它们对运行时性能的潜在影响。然而，在实际场景中，我们需要评估这些优化如何影响模型的收敛性。在某些情况下，可能需要调整模型配置，以确保在不牺牲准确性的情况下实现最佳性能。此外，使用性能分析工具来识别瓶颈并衡量系统资源利用率，对于指导和促进我们的优化活动至关重要。
- en: Summary
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Nowadays, we are fortunate to have a wide variety of systems on which to run
    our ML workloads. No matter which platform we choose, our goal is to maximize
    its capabilities. In this post, we focused on AWS Inferentia and reviewed several
    techniques for accelerating ML workloads running on it. Be sure to check out our
    [other posts](https://chaimrand.medium.com/) for more optimization strategies
    across various AI accelerators.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，我们有幸能够在各种各样的系统上运行我们的机器学习工作负载。无论我们选择哪个平台，我们的目标都是最大化其能力。在本文中，我们重点介绍了AWS Inferentia，并回顾了几种加速在其上运行的机器学习工作负载的技术。一定要查看我们的[其他文章](https://chaimrand.medium.com/)，了解更多关于各类AI加速器的优化策略。
