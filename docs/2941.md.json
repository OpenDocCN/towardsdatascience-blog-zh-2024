["```py\nmodel_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\nmodel = MllamaForConditionalGeneration.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\")\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n```", "```py\nmessages = [\n    {\"role\": \"user\",      \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt1}]},\n    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": generated_texts1}]},\n    {\"role\": \"user\",      \"content\": [{\"type\": \"text\", \"text\": prompt2}]},\n    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": generated_texts2}]},\n    {\"role\": \"user\",      \"content\": [{\"type\": \"text\", \"text\": prompt3}]},\n    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": generated_texts3}]}\n]\n```", "```py\ndef chat_with_mllm (model, processor, prompt, images_path=[],do_sample=False, temperature=0.1, show_image=False, max_new_tokens=512, messages=[], images=[]):\n\n    # Ensure list:\n    if not isinstance(images_path, list):\n        images_path =  [images_path]\n\n    # Load images \n    if len (images)==0 and len (images_path)>0:\n            for image_path in tqdm (images_path):\n                image = load_image(image_path)\n                images.append (image)\n                if show_image:\n                    display ( image )\n\n    # If starting a new conversation about an image\n    if len (messages)==0:\n        messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]}]\n\n    # If continuing conversation on the image\n    else:\n        messages.append ({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]})\n\n    # process input data\n    text = processor.apply_chat_template(messages, add_generation_prompt=True)\n    inputs = processor(images=images, text=text, return_tensors=\"pt\", ).to(model.device)\n\n    # Generate response\n    generation_args = {\"max_new_tokens\": max_new_tokens, \"do_sample\": True}\n    if do_sample:\n        generation_args[\"temperature\"] = temperature\n    generate_ids = model.generate(**inputs,**generation_args)\n    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:-1]\n    generated_texts = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)\n\n    # Append the model's response to the conversation history\n    messages.append ({\"role\": \"assistant\", \"content\": [  {\"type\": \"text\", \"text\": generated_texts}]})\n\n    return generated_texts, messages, images\n```", "```py\ninstructions = \"Respond concisely in one sentence.\"\nprompt = instructions + \"Describe the image.\"\n\nresponse, messages,images= chat_with_mllm ( model, processor, prompt,\n                                             images_path=[img_path],\n                                             do_sample=True,\n                                             temperature=0.2,\n                                             show_image=True,\n                                             messages=[],\n                                             images=[])\n\n# Output:  \"The image depicts a butterfly emerging from its chrysalis, \n#           with a row of chrysalises hanging from a branch above it.\"\n```", "```py\nprompt = instructions + \"What would happen to the chrysalis in the near future?\"\nresponse, messages, images= chat_with_mllm ( model, processor, prompt,\n                                             images_path=[img_path,],\n                                             do_sample=True,\n                                             temperature=0.2,\n                                             show_image=False,\n                                             messages=messages,\n                                             images=images)\n\n# Output: \"The chrysalis will eventually hatch into a butterfly.\"\n```", "```py\ninstructions = \"You are a computer vision engineer with sense of humor.\"\nprompt = instructions + \"Can you explain this meme to me?\"\n\nresponse, messages,images= chat_with_mllm ( model, processor, prompt,\n                                             images_path=[img_path,],\n                                             do_sample=True,\n                                             temperature=0.5,\n                                             show_image=True,\n                                             messages=[],\n                                             images=[])\n```"]