<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Python Water Quality — Baseline Classification Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Python Water Quality — Baseline Classification Model</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/python-water-quality-baseline-classification-model-bb5584226a82?source=collection_archive---------10-----------------------#2024-01-08">https://towardsdatascience.com/python-water-quality-baseline-classification-model-bb5584226a82?source=collection_archive---------10-----------------------#2024-01-08</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="0c5a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Assess feature importance when estimating water quality using a reference baseline model</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://jamesmcneill06.medium.com/?source=post_page---byline--bb5584226a82--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="James McNeill" class="l ep by dd de cx" src="../Images/5f71dfac8d1d37a1232d88c4ac04bf84.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Ju54IRNVqGtNJxMCiwsJLQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--bb5584226a82--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://jamesmcneill06.medium.com/?source=post_page---byline--bb5584226a82--------------------------------" rel="noopener follow">James McNeill</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--bb5584226a82--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/14dc02e38c52e177eb13286d0101fc97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qqVgxj2uDy9YUS6f"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@craftedbygc?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unseen Studio</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="afb5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk ny"><span class="l nz oa ob bo oc od oe of og ed">U</span>nderstanding what can be used to classify the water quality can be a challenge. Having expert knowledge of different regions can provide local insights into what helps to see how water flows best. Without the time to fully review these details, it reduces the possibility of learning from mistakes to benefit others. A quantifiable approach can be taken by collecting datasets of the features that impact water. With quantification it allows the user to apply computer science techniques to gain data-driven insights. For this article, we are aiming to apply a baseline Machine Learning classification model to help highlight key features. Model predictions will be produced with unseen data, commonly referred to as test data to validate model performance.</p><p id="ea1a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For prior details on the initial exploratory data analysis performed on the input dataset the article “Python water quality EDA and Potability analysis” is shared at the end.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3ca8" class="op oq fq bf or os ot gq ou ov ow gt ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">Dataset</h1><p id="3d4a" class="pw-post-body-paragraph nc nd fq ne b go pl ng nh gr pm nj nk nl pn nn no np po nr ns nt pp nv nw nx fj bk">For this piece of analysis, the Water Quality dataset has been taken from Kaggle¹.</p><div class="pq pr ps pt pu pv"><a href="https://www.kaggle.com/datasets/adityakadiwal/water-potability?source=post_page-----bb5584226a82--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pw ab ig"><div class="px ab co cb py pz"><h2 class="bf fr hw z io qa iq ir qb it iv fp bk">Water Quality</h2><div class="qc l"><h3 class="bf b hw z io qa iq ir qb it iv dx">Drinking water potability</h3></div><div class="qd l"><p class="bf b dy z io qa iq ir qb it iv dx">www.kaggle.com</p></div></div><div class="qe l"><div class="qf l qg qh qi qe qj lq pv"/></div></div></a></div><p id="507e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A jupyter notebook instance written with Python code was used for processing.</p><pre class="ml mm mn mo mp qk ql qm bp qn bb bk"><span id="b49a" class="qo oq fq ql b bg qp qq l qr qs">import sys<br/>print(sys.version) # displays the version of python installed</span></pre><p id="c230" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After running the script above an output would show that version 3.7.10 of Python was used. To be able to replicate the results that follow, users should ensure that their working environment has Python 3.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><pre class="ml mm mn mo mp qk ql qm bp qn bb bk"><span id="64ae" class="qo oq fq ql b bg qp qq l qr qs"># import libraries<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>import os</span></pre><p id="99c6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To begin the process steps several Python libraries are needed. Each library shown above contains a range of methods, functions, and outputs that have been developed to aid data analysis. For user knowledge both sets of libraries have been built on top of each other i.e., one is taken as the base and used to produce additional outputs. Pandas is built on top of NumPy to produce data analysis. With Seaborn built on top of matplotlib to aid data visualization.</p><p id="d3b2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A common initial step to begin working with Pandas for data analysis is to import a CSV file. The code shown below references the folder containing the file for review.</p><pre class="ml mm mn mo mp qk ql qm bp qn bb bk"><span id="3e70" class="qo oq fq ql b bg qp qq l qr qs"># Import the dataset for review as a DataFrame<br/>df = pd.read_csv("../input/water-potability/water_potability.csv")</span></pre></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="0eac" class="qt oq fq bf or qu qv qw ou qx qy qz ox nl ra rb rc np rd re rf nt rg rh ri rj bk">Pre-processing data for modeling</h2><p id="41e5" class="pw-post-body-paragraph nc nd fq ne b go pl ng nh gr pm nj nk nl pn nn no np po nr ns nt pp nv nw nx fj bk">Before beginning to produce classification models we need to understand and pre-process the data. As mentioned earlier the EDA (Exploratory Data Analysis) was assessed in a previous article. Using the knowledge gained a pre-processing data pipeline can be produced. Creating a data pipeline has a range of benefits. Firstly, the steps that have been tested on sample data are used to automate processing for future iterations. Secondly, it allows other users to quickly begin working with a cleaner version of the dataset and avoid having to review the same initial steps. Lastly, the data pipeline can be copied or forked by other users and new additions can be added without impacting the initial pipeline.</p><p id="8780" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With ML modelling the main aim of analysis is to have a robust data pipeline that contains data pre-processing to allow other users to test different ML model algorithms. A great book called Effective Pandas by Matt Harrison shows how a chaining method can aid code legibility. We will show two methods that aim to process similar tasks. Readers are welcome to follow either approach.</p><p id="544c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A common data pre-processing step is to review missing data values. How these ultimately impact the model is unknown until the testing phase. It is advised to retain the raw data variable and develop a new variable to allow for later comparison. For the steps below we will leave this step out and aim to update the raw data variable.</p><p id="d91a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Firstly we need to review the volume of missing values associated with each variable.</p><pre class="ml mm mn mo mp qk ql qm bp qn bb bk"><span id="52c5" class="qo oq fq ql b bg qp qq l qr qs"># Understand missing values per variable within DataFrame<br/>(<br/>    df<br/>    .isnull().sum()<br/>)</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rk"><img src="../Images/60e295c97e554da4fcd4b57fb16f2ded.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*FEZv4dq4eE0W9vqVH0_xZg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Output 1.1 Missing value per column within DataFrame</figcaption></figure><p id="acfc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The output highlights three variables with missing values. However, there is a wide variation in the total missing. Sulfate shows the highest proportion with lower values for the other variables. When larger proportions of missing values are present, we must be careful when applying adjustments. By applying methods that remove the underlying characteristics of the missing values, final results could provide estimates that do not align with expectations. Having expert knowledge within the dataset domain can help to understand different options.</p><p id="6da9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Method #1</p><pre class="ml mm mn mo mp qk ql qm bp qn bb bk"><span id="5ad5" class="qo oq fq ql b bg qp qq l qr qs"># Apply mean value to the missing values<br/>df['ph'].fillna(df['ph'].mean(), inplace=True)<br/>df['Sulfate'].fillna(df['Sulfate'].mean(), inplace=True)<br/>df['Trihalomethanes'].fillna(df['Trihalomethanes'].mean(), inplace=True)<br/>df.isnull().sum()</span></pre><p id="08ee" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Application of a mean value from all non-missing values provides a good first approximation. Multiple variables required updating and Key Word (KW) parameters within the method fillna cater for in-line updates. Including the KW inplace will apply the method to the input dataframe df without requiring a copy.</p><p id="3102" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Method #2</p><pre class="ml mm mn mo mp qk ql qm bp qn bb bk"><span id="018f" class="qo oq fq ql b bg qp qq l qr qs"># Make updates with chaining method, allows for use of comments to update the columns.<br/># A new dataframe variable (df_1) can be assigned this output<br/>df1 = (<br/>    df<br/>#     .isnull().sum()<br/>    .assign(ph=lambda df_:df_.ph.fillna(df_.ph.mean()),<br/>            Sulfate=lambda df_:df_.Sulfate.fillna(df_.Sulfate.mean()),<br/>            Trihalomethanes=lambda df_:df_.Trihalomethanes.fillna(df_.Trihalomethanes.mean())<br/>           )<br/>)<br/><br/># Confirm that the columns have been updated<br/>df1.isnull().sum()</span></pre><p id="09d9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The second method seeks to use the chaining method to perform variable updates. A mean value adjustment is still being applied. Using the assign method with a single-line lambda function allows for greater readability. Another important aspect is that previous lines can be (un)commented out. If a review of pre- and post-processing was required it would be a simple step to uncomment and comment lines within the code. The output below highlights that all missing values have been updated.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rl"><img src="../Images/7c579c1925c8494ee6bb3e0e5ff5207a.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*DY4W6HmhFW0zuhLIL-R5Sw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Output 1.2 Post-processing update made to resolve missing values shows zero null values</figcaption></figure><p id="1744" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With pre-processing complete we are now able to split the dataframe into dependent (target or y) and independent (X) variables.</p><pre class="ml mm mn mo mp qk ql qm bp qn bb bk"><span id="bcc8" class="qo oq fq ql b bg qp qq l qr qs"># Separate into X and y variables<br/>X, y = df1.drop(['Potability'], axis=1), df1['Potability'].values<br/><br/># Show that only independent variables have been retained<br/>X.head()</span></pre><p id="e944" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Python allows for multiple variables to be produced on the left-hand side of the formula within the same line of code. Adding a comma between the variables on each side of the formula, Python interprets that two new variables are being created.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rm"><img src="../Images/d74211e4240dcfc9b1ca8f31ea454698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kDJHjOVCCOO2v_jGH8F03Q.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Output 1.3 Top 5 rows from the DataFrame show only independent variables</figcaption></figure><p id="57bd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The top 5 rows have been shown with the head method. A numpy array variable contains the y binary values.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="64f0" class="qt oq fq bf or qu qv qw ou qx qy qz ox nl ra rb rc np rd re rf nt rg rh ri rj bk">Classification model — Baseline</h2><p id="895c" class="pw-post-body-paragraph nc nd fq ne b go pl ng nh gr pm nj nk nl pn nn no np po nr ns nt pp nv nw nx fj bk">A common Python library used to develop ML models is scikit-learn. Within the library repository a wide range of techniques aid with model development. Many years of development have resulted in a mature library that continues to progress.</p><p id="be76" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When building a classification model many users dive straight into development with the latest ML techniques. However, a better approach is to first develop a baseline model. It can act as a point of reference with any model estimates below this baseline showing less effective techniques. A good first approximation can be produced before attempting to make adjustments to the model hyper-parameters. With hyper-parameters being KW variables that can be adjusted to improve the ML model performance.</p><p id="5aa6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Scikit learn contains a dummy classifier algorithm that can provide a baseline model. With the model output, it can be compared against more complex classifiers.</p><pre class="ml mm mn mo mp qk ql qm bp qn bb bk"><span id="002f" class="qo oq fq ql b bg qp qq l qr qs"># Dummy classifier - create a baseline accuracy score<br/>from sklearn.dummy import DummyClassifier<br/><br/># Define the reference model<br/>dummy_clf = DummyClassifier(strategy='most_frequent')<br/><br/># Fit the model<br/>dummy_clf.fit(X, y)<br/><br/># Predict the model<br/>dummy_clf.predict(X)<br/><br/># Evaluate the model<br/>score = dummy_clf.score(X, y)<br/>print(score)<br/><br/># Print statement displayed value<br/>0.6098901098901099</span></pre><p id="2a9b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The model steps above create a model classifier that can then be fit to the input data. A prediction of the target (y) is produced using the predict method. Finally, scoring will show the accuracy of the model.</p><p id="07fb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As the dummy classifier applied the most frequent value we are effectively predicting that the target value is 0 for each. It should be noted that applying this method can provide good context for future predictions.</p><pre class="ml mm mn mo mp qk ql qm bp qn bb bk"><span id="5ed4" class="qo oq fq ql b bg qp qq l qr qs"># Review the dependent variable frequency and percentage<br/>(<br/>    df1<br/>    .Potability<br/>#     .value_counts()<br/>    .value_counts(normalize=True) # display frequencies as a percentage<br/>)</span></pre><p id="7f93" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To validate the output of the predicted value of 0.60989 we can perform a value_counts of the target variable. The output below displays that the same percentage is shown as the scored prediction.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rn"><img src="../Images/67f000f732fd5950b934d2272f8b03ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*6vVHQPA0G2s-DiWGw3rgow.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Output 1.4 Display the percentage portion of the binary target variable</figcaption></figure><p id="56ff" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Therefore, should any future classification model result in a lower estimated score we should discount this model as not producing better results.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="eba5" class="qt oq fq bf or qu qv qw ou qx qy qz ox nl ra rb rc np rd re rf nt rg rh ri rj bk">Classification model — Complex approach</h2><p id="7958" class="pw-post-body-paragraph nc nd fq ne b go pl ng nh gr pm nj nk nl pn nn no np po nr ns nt pp nv nw nx fj bk">Now let's attempt to produce a more complex model to understand the ML challenge. It is with Gradient Boosted Models (GBM) that we will look for improved performance. A GBM is a tree-based model that allows for the development of multiple trees. With each tree, the input data is assessed to understand how model features predict the target variable. For this exercise, we will use a light GBM classifier. Alternatives such as XGBoost, which stands for Extreme Gradient Boosting, can be used in future developments.</p><pre class="ml mm mn mo mp qk ql qm bp qn bb bk"><span id="fde4" class="qo oq fq ql b bg qp qq l qr qs"># Lets try a Light GBM<br/>from lightgbm import LGBMClassifier<br/><br/># ML Preprocessing<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.pipeline import Pipeline<br/><br/># Hyperparameter tuning<br/>from sklearn.model_selection import GridSearchCV<br/><br/># ML Performance metrics<br/>from sklearn.metrics import classification_report<br/>from sklearn.metrics import confusion_matrix</span></pre><p id="ea51" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For the ML model above common library imports are shown. Each section shows the relevant steps used during model build and testing. Pre-processing aims to ensure that a pipeline of steps can be constructed to aid future developments.</p><pre class="ml mm mn mo mp qk ql qm bp qn bb bk"><span id="07c9" class="qo oq fq ql b bg qp qq l qr qs"># Split into training and test set<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=2, stratify=y)<br/><br/># Instantiate the LGBM<br/>lgbm = LGBMClassifier()<br/><br/># Fit the classifier to the training data<br/>lgbm.fit(X_train, y_train)<br/><br/># Perform prediction<br/>y_pred = lgbm.predict(X_test)<br/><br/># Print the accuracy<br/>print(lgbm.score(X_test, y_test))<br/><br/>print(classification_report(y_test, y_pred))<br/>print(confusion_matrix(y_test, y_pred))</span></pre><p id="495f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Firstly, we need to split the input data into training and testing samples. Having this split reduces the chance of overfitting the data. The goal is to create a model with the best performance on unseen data i.e., how models are used in the wild on real-life data. Using the test data to review how the trained model performs with unseen values aims to showcase areas for improvement. Including the keyword parameter stratify ensures that the target variable distribution is aligned across train and test data. Applying this step aims to ensure that the characteristics of the underlying variable distribution are not lost. Model predictions should align with what is observed within the data.</p><p id="20de" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Applying the classifier to the variable lgbm allows the user to work with all of the methods (functions) and attributes (data) of the Python object. Standard steps to train the model and score on the test data are followed.</p><p id="52e5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The results displayed below highlight how the model performance has increased relative to the baseline model. Applying to the test data provides comfort that predictions are working well. Accuracy has been produced with the scoring method. It displays the total number of correct predictions over all possible outcomes.</p><p id="6812" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using the classification report highlights the classification metrics of most interest. Precision shows how well the True Positive values have been predicted relative to all positive predictions (True Positive and False Positive). Having too many False Positive values results in a Type 1 error i.e., misclassifying an instance as positive, such as medical screening produces misdiagnosis. Recall assesses the True Positive values relative to all positive actuals (True Positive and False Negative). With too many False Negative values it produces a Type II error i.e., misclassifying an instance as negative, such as Fraud detection can result in financial loss.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj ro"><img src="../Images/f378eb1978d792b5de19e3fec6e8bb7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*YGnQlsA6yfhc4rh0TAodOg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Output 1.5 Provides details on accuracy and classification metrics</figcaption></figure><p id="f116" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When working with ML algorithms a set of default key parameter values are included to produce baseline results. It is the optimization of these initial parameters that will produce better model predictions.</p><pre class="ml mm mn mo mp qk ql qm bp qn bb bk"><span id="2b17" class="qo oq fq ql b bg qp qq l qr qs"># Lets understand the baseline params<br/>lgbm.get_params()</span></pre><p id="d41e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using the method get_params for the lgbm variable will display the output shown below. For further details on what each variable means users can review the documentation online.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rp"><img src="../Images/0ae64e42c2e7bd519b725628f6e9439e.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*FX_c7h5agFliHrZ3iEbCvg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Output 1.6 Default keyword parameter values</figcaption></figure><p id="7afe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The next important step in ML model development is to review the hyperparameter space of potential values. By performing hyperparameter tuning across a relevant space of options it is possible to efficiently produce improved predictions.</p><pre class="ml mm mn mo mp qk ql qm bp qn bb bk"><span id="3d6c" class="qo oq fq ql b bg qp qq l qr qs"># Setup the pipeline<br/>steps = [('scaler', StandardScaler()),<br/>         ('lgbm', LGBMClassifier())]<br/><br/>pipeline = Pipeline(steps)<br/><br/># Specify the hyperparameter space<br/>parameters = {<br/>    'lgbm__learning_rate':[0.03, 0.05, 0.1],<br/>    'lgbm__objective':['binary'],<br/>    'lgbm__metric':['binary_logloss'],<br/>    'lgbm__max_depth':[10],<br/>    'lgbm__n_estimators':[100, 200, 300]<br/>}<br/><br/># Create train and test sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)<br/><br/># Instantiate the GridSearchCV object<br/>cv = GridSearchCV(pipeline, parameters, cv=3)<br/><br/># Fit to the training set<br/>cv.fit(X_train, y_train)<br/><br/># Predict the labels of the test set<br/>y_pred = cv.predict(X_test)</span></pre><p id="b838" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Introducing a pipeline that scales numeric variables to align to similar scales will reduce the potential for variables with larger numeric ranges to dominate. Two steps have been included within the pipeline to produce scaled independent variables and then apply the LGBM classifier. By producing code in this format it aids other users understanding of pre-processing steps. A pipeline can include many more steps depending on the complexity of the steps required.</p><p id="832b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A parameter dictionary has been produced to allow for a mixture of hyperparameter inputs to be tested. Including the variable referencing the lgbm model with a double underscore will let Python recognize that the lgbm variable is to be adjusted.</p><p id="0013" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The gridsearchCV method will review each of the input parameters in combination to produce models for all combinations. By including a CV (cross-validation) parameter, it will perform three cross-validation procedures. Each validation run will select a different sample to train the model. The aim is to ensure that a model does not overfit a unique aspect shown within only one sample of the input independent variables.</p><pre class="ml mm mn mo mp qk ql qm bp qn bb bk"><span id="07b9" class="qo oq fq ql b bg qp qq l qr qs"># Display best score and params<br/>print(f'Best score : {cv.best_score_}')<br/>print(f'Best params : {cv.best_params_}')<br/><br/># Compute and print metrics<br/>print("Accuracy: {}".format(cv.score(X_test, y_test)))<br/>print(classification_report(y_test, y_pred))</span></pre><p id="931e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once processing has been completed, the best score and hyperparameters from the lgbm can be reviewed. As we have only reviewed a small number of potential hyperparameters, users could have identified this best model using a more brute-force manual approach. However, the real benefit of the gridsearch would be the inclusion of a much larger hyperparameter input space.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rq"><img src="../Images/a86efd27e2fed31fc100b636a7a6163e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hu7bc3JgG_h4kSYzJzQuOg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Output 1.7 Results from the hyperparameter tuning of the LGBM classifier</figcaption></figure><p id="43e3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After selecting the best parameters for the lgbm we can see an improvement in the model accuracy. It is this parameter selection that could be applied when new data is available to make future predictions.</p><p id="976a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Further steps to improve model performance could include a review of the independent variables relationships via correlation analysis. Also assessing if variables require more refined pre-processing of missing values could be assessed.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="4b7d" class="qt oq fq bf or qu qv qw ou qx qy qz ox nl ra rb rc np rd re rf nt rg rh ri rj bk">Conclusion</h2><p id="dd06" class="pw-post-body-paragraph nc nd fq ne b go pl ng nh gr pm nj nk nl pn nn no np po nr ns nt pp nv nw nx fj bk">Within this article, we have reviewed how the inclusion of a baseline ML model can help when assessing how well models are making predictions. Using a model accuracy metric will determine if alternative approaches have provided improvements. Instead of making a blind assessment of the model performance, there is a data-driven approach in place. We also reviewed how pipeline steps and hyperparameter tuning can aid with ML model performance.</p><p id="b661" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Thanks very much for reading! If you have any comments I would appreciate these as well.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b69b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">You can reach out to me on </strong><a class="af nb" href="https://www.linkedin.com/in/james-mc-neill-180a9057/" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr">LinkedIn </strong></a><strong class="ne fr">for a friendly chat about all things data. Other stories that I have shared:</strong></p><div class="pq pr ps pt pu pv"><a rel="noopener follow" target="_blank" href="/python-water-quality-eda-and-potability-analysis-ebc1cf553081?source=post_page-----bb5584226a82--------------------------------"><div class="pw ab ig"><div class="px ab co cb py pz"><h2 class="bf fr hw z io qa iq ir qb it iv fp bk">Python water quality EDA and Potability analysis</h2><div class="qc l"><h3 class="bf b hw z io qa iq ir qb it iv dx">Understanding data analysis, visualization techniques</h3></div><div class="qd l"><p class="bf b dy z io qa iq ir qb it iv dx">towardsdatascience.com</p></div></div><div class="qe l"><div class="rr l qg qh qi qe qj lq pv"/></div></div></a></div><div class="pq pr ps pt pu pv"><a rel="noopener follow" target="_blank" href="/getting-started-with-nlp-in-python-6a14d0bf4cfe?source=post_page-----bb5584226a82--------------------------------"><div class="pw ab ig"><div class="px ab co cb py pz"><h2 class="bf fr hw z io qa iq ir qb it iv fp bk">Getting started with NLP in Python</h2><div class="qc l"><h3 class="bf b hw z io qa iq ir qb it iv dx">Beginning a journey into the Natural Language Processing space</h3></div><div class="qd l"><p class="bf b dy z io qa iq ir qb it iv dx">towardsdatascience.com</p></div></div><div class="qe l"><div class="rs l qg qh qi qe qj lq pv"/></div></div></a></div><div class="pq pr ps pt pu pv"><a rel="noopener follow" target="_blank" href="/deep-dive-into-sql-window-functions-bdcb29b05853?source=post_page-----bb5584226a82--------------------------------"><div class="pw ab ig"><div class="px ab co cb py pz"><h2 class="bf fr hw z io qa iq ir qb it iv fp bk">Deep dive into SQL window functions</h2><div class="qc l"><h3 class="bf b hw z io qa iq ir qb it iv dx">The SQL window function performs calculations across a set of table rows to streamline data analysis</h3></div><div class="qd l"><p class="bf b dy z io qa iq ir qb it iv dx">towardsdatascience.com</p></div></div><div class="qe l"><div class="rt l qg qh qi qe qj lq pv"/></div></div></a></div><p id="1275" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[1] : Kaggle dataset water quality from <a class="af nb" href="https://www.kaggle.com/datasets/adityakadiwal/water-potability" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/datasets/adityakadiwal/water-potability</a>, with a license agreement of <a class="af nb" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank">https://creativecommons.org/publicdomain/zero/1.0/</a></p></div></div></div></div>    
</body>
</html>