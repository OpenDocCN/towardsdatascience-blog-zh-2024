<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>BlazeFace: How to Run Real-time Object Detection in the Browser</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>BlazeFace: How to Run Real-time Object Detection in the Browser</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/blazeface-how-to-run-real-time-object-detection-in-the-browser-66c2ac9acd75?source=collection_archive---------2-----------------------#2024-07-17">https://towardsdatascience.com/blazeface-how-to-run-real-time-object-detection-in-the-browser-66c2ac9acd75?source=collection_archive---------2-----------------------#2024-07-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7138" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A step-by-step guide to training a BlazeFace model, from the Python training pipeline to the JavaScript demo through model conversion.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@vincent.vandenbussche?source=post_page---byline--66c2ac9acd75--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Vincent Vandenbussche" class="l ep by dd de cx" src="../Images/b2febfc63ca0efbda0af5501f6080ab7.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*pyfH31oWD78ckKpLyzMCEg.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--66c2ac9acd75--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@vincent.vandenbussche?source=post_page---byline--66c2ac9acd75--------------------------------" rel="noopener follow">Vincent Vandenbussche</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--66c2ac9acd75--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ld le">4</span></p></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="lj k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lk an ao ap ig ll lm ln" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lo cn"><div class="l ae"><div class="ab cb"><div class="lp lq lr ls lt lu ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn mo"><img src="../Images/05b105d691b1a727b04e20b7e15caa57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h920_4uTt-94Eh1NuxFHQQ.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Freely adapted from a photo by <a class="af nf" href="https://unsplash.com/@visuals?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">visuals</a> on <a class="af nf" href="https://unsplash.com/photos/man-in-black-suit-jacket-smiling-Y4qzW3AsvqI?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="7410" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Thanks to libraries such as <a class="af nf" href="https://docs.ultralytics.com/" rel="noopener ugc nofollow" target="_blank">YOLO by Ultralytics</a>, it is fairly easy today to make robust object detection models with as little as a few lines of code. Unfortunately, those solutions are not yet fast enough to work in a web browser on a real-time video stream at 30 frames per second (which is usually considered the real-time limit for video applications) on any device. More often than not, it will run at less than 10 fps on an average mobile device.</p><p id="f4ed" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The most famous real-time object detection solution on web browser is <a class="af nf" href="https://ai.google.dev/edge/mediapipe/solutions/vision/object_detector" rel="noopener ugc nofollow" target="_blank">Google’s MediaPipe</a>. This is a really convenient and versatile solution, as it can work on many devices and platforms easily. But what if you want to make your own solution?</p><p id="3391" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In this post, we propose to build our own lightweight, fast and robust object detection model, that runs at more than 30 fps on almost any devices, based on the BlazeFace model. All the code used for this is available on my <a class="af nf" href="https://github.com/vincent-vdb/medium_posts/tree/main/blazeface" rel="noopener ugc nofollow" target="_blank">GitHub</a>, in the <em class="oc">blazeface</em> folder.</p><p id="78d8" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The <a class="af nf" href="https://arxiv.org/abs/1907.05047" rel="noopener ugc nofollow" target="_blank">BlazeFace</a> model, proposed by Google and originally used in MediaPipe for face detection, is really small and fast, while being robust enough for easy object detection tasks such as face detection. Unfortunately, to my knowledge, no training pipeline of this model is available online on GitHub; all I could find is <a class="af nf" href="https://github.com/hollance/BlazeFace-PyTorch" rel="noopener ugc nofollow" target="_blank">this inference-only model architecture</a>. Through this post, we will train our own BlazeFace model with a fully working pipeline and use it on browser with a working JavaScript code.</p><p id="1bf6" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">More specifically, we will go through the following steps:</p><ul class=""><li id="a6e5" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob od oe of bk">Training the model using <a class="af nf" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">PyTorch</a></li><li id="6d2c" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Converting the PyTorch model into a TFLite model</li><li id="c9c9" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Running the object detection in the browser thanks to JavaScript and TensorFlow.js</li></ul><p id="b6ac" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Let’s get started with the model training.</p><h1 id="1115" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">Training the PyTorch Model</h1><p id="d5a8" class="pw-post-body-paragraph ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob fj bk">As usual when training a model, there are a few typical steps in a training pipeline:</p><ul class=""><li id="0057" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob od oe of bk">Preprocessing the data: we will use a freely available Kaggle dataset for simplicity, but any dataset with the right format of labels would work</li><li id="bf72" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Building the model: we will reuse the proposed architecture in the original paper and the inference-only GitHub code</li><li id="dce8" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Training and evaluating the model: we will use a simple Multibox loss as the cost function to minimize</li></ul><p id="9c61" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Let’s go through those steps together.</p><h2 id="4266" class="pm om fq bf on pn po pp oq pq pr ps ot np pt pu pv nt pw px py nx pz qa qb qc bk">Data Preprocessing</h2><p id="e4a1" class="pw-post-body-paragraph ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob fj bk">We are going to use a subset of the <a class="af nf" href="https://storage.googleapis.com/openimages/web/index.html" rel="noopener ugc nofollow" target="_blank">Open Images Dataset</a> V7, proposed by Google. This dataset is made of about 9 million images with many annotations (including bounding boxes, segmentation masks, and many others). The dataset itself is quite large and contains many types of images.</p><p id="c1a7" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">For our specific use case, I decided to select images in the validation set fulfilling two specific conditions:</p><ul class=""><li id="6d64" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob od oe of bk">Containing labels of human face bounding box</li><li id="f006" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Having a permissive license for such a use case, more specifically the <a class="af nf" href="https://creativecommons.org/licenses/by/2.0/" rel="noopener ugc nofollow" target="_blank">CC BY 2.0</a> license</li></ul><p id="1b1c" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The script to download and build the dataset under those strict conditions is provided in the GitHub, so that anyone can reproduce it. The downloaded dataset with this script contains labels in the YOLO format (meaning box center, width and height). In the end, the downloaded dataset is made of about 3k images and 8k faces, that I have separated into train and validation set with a 80%-20% split ratio.</p><p id="7668" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">From this dataset, typical preprocessing it required before being able to train a model. The data preprocessing code I used is the following:</p><figure class="mp mq mr ms mt mu"><div class="qd ir l ed"><div class="qe qf l"/></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Data preprocessing class for model training with PyTorch. Some code has been omitted for clarity: full code is available on GitHub.</figcaption></figure><p id="7f1c" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">As we can see, the preprocessing is made of the following steps:</p><ul class=""><li id="b2e1" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob od oe of bk">It loads images and labels</li><li id="2478" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">It converts labels from YOLO format (center position, width, height) to box corner format (top-left corner position, bottom-right corner position)</li><li id="9cd7" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">It resizes images to the target size (e.g. 128 pixels), and adds padding if necessary to keep the original image aspect ratio and avoid image deformation. Finally, it normalizes the images.</li></ul><p id="411c" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Optionally, this code allows for data augmentation using <a class="af nf" href="https://albumentations.ai/" rel="noopener ugc nofollow" target="_blank">Albumentations</a>. For the training, I used the following data augmentations:</p><ul class=""><li id="5235" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob od oe of bk">Horizontal flip</li><li id="5b7a" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Random brightness contrast</li><li id="f339" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Random crop from borders</li><li id="3eee" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Affine transformation</li></ul><p id="4a2b" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Those augmentations will allow us to have a more robust, regularized model. After all those transformations and augmentations, the input data may look like the following sample:</p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn qg"><img src="../Images/f180793b42b7b04702cec440680ad18f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dyZ9NR19UeTPxutyJBstdg.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Preprocessed images, with data augmentation, used to train the model. Image by author, made of images from the <a class="af nf" href="https://storage.googleapis.com/openimages/web/download_v7.html" rel="noopener ugc nofollow" target="_blank">Open Images Dataset</a>.</figcaption></figure><p id="96ec" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">As we can see, the preprocessed images have grey borders because of augmentation (with rotation or translation) or padding (because the original image did not have a square aspect ratio). They all contain faces, although the context might be really different depending on the image.</p><p id="6dee" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><strong class="ni fr"><em class="oc">Important Note:</em></strong></p><p id="fdff" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><em class="oc">Face detection is a highly sensitive task with significant ethical and safety considerations. Bias in the dataset, such as underrepresentation or overrepresentation of certain facial characteristics, can lead to false negatives or false positives, potentially causing harm or offense. See below a dedicated section about ethical considerations.</em></p><p id="d8d0" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Now that our data can be loaded and preprocessed, let’s go to the next step: building the model.</p><h2 id="c133" class="pm om fq bf on pn po pp oq pq pr ps ot np pt pu pv nt pw px py nx pz qa qb qc bk">Model Building</h2><p id="4859" class="pw-post-body-paragraph ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob fj bk">In this section, we will build the model architecture of the original BlazeFace model, based on the original article and adapted from the <a class="af nf" href="https://github.com/hollance/BlazeFace-PyTorch" rel="noopener ugc nofollow" target="_blank">BlazeFace repository</a> containing inference code only.</p><p id="70e4" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The whole BlazeFace architecture is rather simple and is mostly made of what the paper’s author call a BlazeBlock, with various parameters.</p><p id="6f79" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The BlazeBlock can be defined with PyTorch as follows:</p><figure class="mp mq mr ms mt mu"><div class="qd ir l ed"><div class="qe qf l"/></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Implementation of the BlazeBlock, of which the BlazeFace is made of. Full code available on GitHub.</figcaption></figure><p id="4fc7" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">As we can see from this code, a BlazeBlock is simply made of the following layers:</p><ul class=""><li id="a591" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob od oe of bk">One depthwise 2D convolution layer</li><li id="e785" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">One batch norm 2D layer</li><li id="50d4" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">One 2D convolution layer</li><li id="f9f8" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">One batch norm 2D layer</li></ul><p id="c22d" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><em class="oc">N.B.: You can read the PyTorch documentation for more about these layers: </em><a class="af nf" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html" rel="noopener ugc nofollow" target="_blank"><em class="oc">Conv2D layer</em></a><em class="oc"> and </em><a class="af nf" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html" rel="noopener ugc nofollow" target="_blank"><em class="oc">BatchNorm2D layer</em></a><em class="oc">.</em></p><p id="aac8" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">This block is repeated many times with different input parameters, to go from a 128-pixel image up to a typical object detection prediction using tensor reshaping in the final stages. Feel free to have a look at the full code in the GitHub repository for more about the implementation of this architecture.</p><p id="92ab" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Before moving to the next section about training the model, note that there are actually two architectures:</p><ul class=""><li id="6835" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob od oe of bk">A 128-pixel input image architecture</li><li id="409d" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">A 256-pixel input image architecture</li></ul><p id="2eed" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">As you can imagine, the 256-pixel architecture is slightly larger, but still lightweight and sometimes more robust. This architecture is also implemented in the provided code, so that you can use it if you want.</p><p id="339e" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><em class="oc">N.B.: The original BlazeFace model not only predicts a bounding box, but also six approximate face landmarks. Since I did not have such labels, I simplified the model architecture to predict only the bounding boxes.</em></p><p id="97d5" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Now that we can build a model, let’s move on to the next step: training the model.</p><h2 id="5a93" class="pm om fq bf on pn po pp oq pq pr ps ot np pt pu pv nt pw px py nx pz qa qb qc bk">Model Training</h2><p id="c104" class="pw-post-body-paragraph ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob fj bk">For anyone familiar with PyTorch, training models such as this one is usually quite simple and straightforward, as shown in this code:</p><figure class="mp mq mr ms mt mu"><div class="qd ir l ed"><div class="qe qf l"/></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Code used to train the BlazeFace model. Full code available on GitHub.</figcaption></figure><p id="22e4" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">As we can see, the idea is to loop over your data for a given number of epochs, one batch at a time, and do the following:</p><ul class=""><li id="3e6a" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob od oe of bk">Get the processed data and corresponding labels</li><li id="a22b" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Make the forward inference</li><li id="83ba" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Compute the loss of the inference against the label</li><li id="1b05" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Update the weights</li></ul><p id="4439" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">I am not getting into all the details for clarity in this post, but feel free to navigate through the code to get a better sense of the training part if needed.</p><p id="5539" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">After training on 100 epochs, I had the following results on the validation set:</p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn qh"><img src="../Images/47ff2c73041c624853d570fdc4a90a0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jNyxUujLKP8G4XN9X2lM0w.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Results of the model on the validation set after 50 epochs. Green boxes are ground truth labels, red boxes are model predictions. Image by author, made of images from the <a class="af nf" href="https://storage.googleapis.com/openimages/web/download_v7.html" rel="noopener ugc nofollow" target="_blank">Open Images Dataset</a>.</figcaption></figure><p id="83ef" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">As we can see on those results, even if the object detection is not perfect, it works pretty well for most cases (probably the IoU threshold was not optimal, leading sometimes to overlapping boxes). Keep in mind it’s a very light model; it can’t exhibit the same performances as a YOLOv8, for example.</p><p id="9fc4" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Before going to the next step about converting the model, let’s have a short discussion about ethical and safety considerations.</p><h2 id="8043" class="pm om fq bf on pn po pp oq pq pr ps ot np pt pu pv nt pw px py nx pz qa qb qc bk">Ethical and Safety Considerations</h2><p id="84de" class="pw-post-body-paragraph ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob fj bk">Let’s go over a few points about ethics and safety, since face detection can be a very sensitive topic:</p><ul class=""><li id="335c" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob od oe of bk"><strong class="ni fr">Dataset importance and selection:</strong> This dataset is used to demonstrate face detection techniques for educational purposes. It was chosen for its relevance to the topic, but it may not fully represent the diversity needed for unbiased results.</li><li id="c5bd" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk"><strong class="ni fr">Bias awareness:</strong> The dataset is not claimed to be bias-free, and potential biases have not been fully mitigated. Please be aware of potential biases that can affect the accuracy and fairness of face detection models.</li><li id="4afa" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk"><strong class="ni fr">Risks:</strong> The trained face detection model may reflect these biases, raising potential ethical concerns. Users should critically assess the outcomes and consider the broader implications.</li></ul><p id="6ca9" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">To address these concerns, anyone willing to build a product on such topic should focus on:</p><ul class=""><li id="f589" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob od oe of bk">Collecting diverse and representative images</li><li id="c306" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Verifying the data is bias-free and any category is equally represented</li><li id="8ae7" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Continuously evaluating the ethical implications of face detection technologies</li></ul><p id="cd2e" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><em class="oc">N.B.: A useful approach to address these concerns is to examine what Google did for their own </em><a class="af nf" href="https://storage.googleapis.com/mediapipe-assets/MediaPipe%20BlazeFace%20Model%20Card%20(Short%20Range).pdf" rel="noopener ugc nofollow" target="_blank"><em class="oc">face detection</em></a><em class="oc"> and </em><a class="af nf" href="https://storage.googleapis.com/mediapipe-assets/Model%20Card%20MediaPipe%20Face%20Mesh%20V2.pdf" rel="noopener ugc nofollow" target="_blank"><em class="oc">face landmarks</em></a><em class="oc"> models</em>.</p><p id="3189" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Again, the used dataset is intended solely for educational purposes. Anyone willing to use it should exercise caution and be mindful of its limitations when interpreting results. Let’s now move to the next step with the model conversion.</p><h1 id="9d1d" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">Converting the Model</h1><p id="fbe4" class="pw-post-body-paragraph ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob fj bk">Remember that our goal is to make our object detection model work in a web browser. Unfortunately, once we have a trained PyTorch model, we can not directly use it in a web browser. We first need to convert it.</p><p id="30b0" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Currently, to my knowledge, the most reliable way to run a deep learning model in a web browser is by using a <a class="af nf" href="https://www.tensorflow.org/lite" rel="noopener ugc nofollow" target="_blank">TFLite</a> model with T<a class="af nf" href="https://www.tensorflow.org/js" rel="noopener ugc nofollow" target="_blank">ensorFlow.js</a>. In other words, we need to convert our PyTorch model into a TFLite model.</p><p id="ea17" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><em class="oc">N.B.: Some alternative ways are emerging, such as </em><a class="af nf" href="https://pytorch.org/executorch-overview" rel="noopener ugc nofollow" target="_blank"><em class="oc">ExecuTorch</em></a><em class="oc">, but they do not seem to be mature enough yet for web use.</em></p><p id="8760" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">As far as I know, there is no robust, reliable way to do so directly. But there are side ways, by going through ONNX. <a class="af nf" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank">ONNX</a> (which stands for Open Neural Network Exchange) is a standard for storing and running (using <a class="af nf" href="https://onnxruntime.ai/" rel="noopener ugc nofollow" target="_blank">ONNX Runtime</a>) machine learning models. Conveniently, there are available libraries for conversion from torch to ONNX, as well as from ONNX to TensorFlow models.</p><p id="b92b" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">To summarize, the conversion workflow is made of the three following steps:</p><ul class=""><li id="74b5" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob od oe of bk">Convert from PyTorch to ONNX</li><li id="88ab" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Convert from ONNX to TensorFlow</li><li id="c295" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Convert from TensorFlow to TFLite</li></ul><p id="6a7f" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">This is exactly what the following code does:</p><figure class="mp mq mr ms mt mu"><div class="qd ir l ed"><div class="qe qf l"/></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Model conversion from PyTorch format to TFLite format, through ONNX. Full code available on GitHub.</figcaption></figure><p id="841f" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">This code can be slightly more cryptic than the previous ones, as there are some specific optimizations and parameters used to make it work properly. One can also try to go one step further and quantize the TFLite model to make it even smaller. If you are interested in doing so, you can have a look at the <a class="af nf" href="https://www.tensorflow.org/lite/performance/post_training_quantization" rel="noopener ugc nofollow" target="_blank">official documentation</a>.</p><p id="cb4e" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><em class="oc">N.B.: The conversion code is highly sensitive of the versions of the libraries. To ensure a smooth conversion, I would strongly recommend using the specified versions in the requirements.txt file on GitHub.</em></p><p id="ad89" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">On my side, after TFLite conversion, I finally have a TFLite model of only about 400kB, which is lightweight and quite acceptable for web usage. Next step is to actually test it out in a web browser, and to make sure it works as expected.</p><p id="93c7" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">On a side note, be aware that another solution is currently being developed by Google for PyTorch model conversion to TFLite format: <a class="af nf" href="https://github.com/google-ai-edge/ai-edge-torch" rel="noopener ugc nofollow" target="_blank">AI Edge Torch</a>. Unfortunately, this is quite new and I couldn’t make it work for my use case. However, any feedback about this library is very welcome.</p><h1 id="96c9" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">Running the Model</h1><p id="ac28" class="pw-post-body-paragraph ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob fj bk">Now that we finally have a TFLite model, we are able to run it in a web browser using TensorFlow.js. If you are not familiar with JavaScript (since this is not usually a language used by data scientists and machine learning engineers) do not worry; all the code is provided and is rather easy to understand.</p><p id="355c" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">I won’t comment all the code here, just the most relevant parts. If you look at the code on <a class="af nf" href="https://github.com/vincent-vdb/medium_posts" rel="noopener ugc nofollow" target="_blank">GitHub</a>, you will see the following in the <em class="oc">javascript</em> folder:</p><ul class=""><li id="c7f9" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob od oe of bk"><em class="oc">index.html</em>: contains the home page running the whole demo</li><li id="f6b5" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk"><em class="oc">assets</em>: the folder containing the TFLite model that we just converted</li><li id="937f" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk"><em class="oc">js</em>: the folder containing the JavaScript codes</li></ul><p id="c9ec" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">If we take a step back, all we need to do in the JavaScript code is to loop over the frames of the camera feed (either a webcam on a computer or the front-facing camera on a mobile phone) and do the following:</p><ul class=""><li id="e2b5" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob od oe of bk">Preprocess the image: resize it as a 128-pixel image, with padding and normalization</li><li id="cdf3" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Compute the inference on the preprocessed image</li><li id="663b" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Postprocess the model output: apply thresholding and non max suppression to the detections</li></ul><p id="4c54" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">We won’t comment the image preprocessing since this would be redundant with the Python preprocessing, but feel free to have a look at the code. When it comes to making an inference with a TFLite model in JavaScript, it’s fairly easy:</p><figure class="mp mq mr ms mt mu"><div class="qd ir l ed"><div class="qe qf l"/></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Simplistic example of code to instantiate a TFLite model and compute an inference, assuming an image of the right shape. Full working code on GitHub.</figcaption></figure><p id="9b5a" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The tricky part is actually the postprocessing. As you may know, the output of a SSD object detection model is not directly usable: this is not the bounding boxes locations. Here is the postprocessing code that I used:</p><figure class="mp mq mr ms mt mu"><div class="qd ir l ed"><div class="qe qf l"/></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Postprocessing the BlazeFace model output in JavaScript. Full code on GitHub.</figcaption></figure><p id="e82c" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In the code above, the model output is postprocessed with the following steps:</p><ul class=""><li id="11fd" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob od oe of bk">The boxes locations are corrected with the anchors</li><li id="eec3" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">The box format is converted to get the top-left and the bottom-right corners</li><li id="3aaf" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">Non-max suppression is applied to the boxes with the detection score, allowing the removal of all boxes below a given threshold and overlapping other already-existing boxes</li></ul><p id="6170" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">This is exactly what has been done in Python too to display the resulting bounding boxes, if it may help you get a better understanding of that part.</p><p id="ee33" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Finally, below is a screenshot of the resulting web browser demo:</p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div class="mm mn qi"><img src="../Images/7531f025b2456d12dcd9b3b675f298ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*M7KOEw4Ir5yp6UV3iHzQJA.png"/></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Screenshot of the running demo in the web browser, with picture-in-picture by <a class="af nf" href="https://unsplash.com/photos/a-man-sitting-at-a-table-in-front-of-a-laptop-wAUjHURVbDw" rel="noopener ugc nofollow" target="_blank">Vitaly Gariev</a> on Unsplash</figcaption></figure><p id="a707" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">As you can see, it properly detects the face in the image. I decided to use a <a class="af nf" href="https://unsplash.com/photos/a-man-sitting-at-a-table-in-front-of-a-laptop-wAUjHURVbDw" rel="noopener ugc nofollow" target="_blank">static image from Unsplash</a>, but the code on GitHub allows you to run it on your webcam, so feel free to test it yourself.</p><p id="e05f" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Before concluding, note that if you run this code on your own computer or smartphone, depending on your device you may not reach 30 fps (on my personal laptop having a rather old 2017 <a class="af nf" href="https://www.intel.fr/content/www/fr/fr/products/sku/124967/intel-core-i58250u-processor-6m-cache-up-to-3-40-ghz/specifications.html" rel="noopener ugc nofollow" target="_blank">Intel® Core™ i5–8250U</a>, it runs at 36fps). If that’s the case, a few tricks may help you get there. The easiest one is to run the model inference only once every N frames (N to be fine tuned depending on your application, of course). Indeed, in most cases, from one frame to the next, there are not many changes, and the boxes can remain almost unchanged.</p><h1 id="d33d" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">Conclusion</h1><p id="6672" class="pw-post-body-paragraph ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob fj bk">I hope you enjoyed reading this post and thanks if you got this far. Even though doing object detection is fairly easy nowadays, doing it with limited resources can be quite challenging. Learning about BlazeFace and converting models for web browser gives some insights into how MediaPipe was built, and opens the way to other interesting applications such as blurring backgrounds in video call (like Google Meets or Microsoft Teams) in real time in the browser.</p><h1 id="c647" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">References</h1><ul class=""><li id="d6f3" class="ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob od oe of bk">The <a class="af nf" href="https://arxiv.org/abs/1811.00982" rel="noopener ugc nofollow" target="_blank">Open Images Dataset publication</a></li><li id="4ef0" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">The <a class="af nf" href="https://github.com/vincent-vdb/medium_posts" rel="noopener ugc nofollow" target="_blank">GitHub repo</a> containing all the working code in the folder blazeface</li><li id="195b" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">The <a class="af nf" href="https://github.com/hollance/BlazeFace-PyTorch/blob/master/blazeface.py" rel="noopener ugc nofollow" target="_blank">original GitHub</a> containing inference code for BlazeFace</li><li id="9eb6" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk">The <a class="af nf" href="https://arxiv.org/abs/1907.05047" rel="noopener ugc nofollow" target="_blank">BlazeFace paper</a></li><li id="12d0" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk"><a class="af nf" href="https://ai.google.dev/edge/mediapipe/solutions/vision/face_detector" rel="noopener ugc nofollow" target="_blank">MediaPipe’s face detection</a> and <a class="af nf" href="https://storage.googleapis.com/mediapipe-assets/MediaPipe%20BlazeFace%20Model%20Card%20(Short%20Range).pdf" rel="noopener ugc nofollow" target="_blank">BlazeFace model card</a></li><li id="bda6" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk"><a class="af nf" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank">ONNX</a> and <a class="af nf" href="https://onnxruntime.ai/" rel="noopener ugc nofollow" target="_blank">ONNX Runtime</a></li><li id="b8e9" class="ng nh fq ni b go og nk nl gr oh nn no np oi nr ns nt oj nv nw nx ok nz oa ob od oe of bk"><a class="af nf" href="https://www.tensorflow.org/lite/performance/post_training_quantization" rel="noopener ugc nofollow" target="_blank">TFLite quantization</a></li></ul></div></div></div></div>    
</body>
</html>