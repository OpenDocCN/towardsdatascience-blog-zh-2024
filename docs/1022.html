<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Experimenting with MLFlow and Microsoft Fabric</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Experimenting with MLFlow and Microsoft Fabric</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/experimenting-with-mlflow-and-microsoft-fabric-68f43043ff34?source=collection_archive---------7-----------------------#2024-04-22">https://towardsdatascience.com/experimenting-with-mlflow-and-microsoft-fabric-68f43043ff34?source=collection_archive---------7-----------------------#2024-04-22</a></blockquote><div><div class="em ff fg fh fi fj"/><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><div/><div><h2 id="4044" class="pw-subtitle-paragraph go fq fr bf b gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cq dx">Fabric Madness part 4</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@roger_noble?source=post_page---byline--68f43043ff34--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Roger Noble" class="l ep by dd de cx" src="../Images/869b5b0f237f24b119ca6c41c2e31162.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*DSDhBVvFKAUKXJfpbO5beg.jpeg"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--68f43043ff34--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@roger_noble?source=post_page---byline--68f43043ff34--------------------------------" rel="noopener follow">Roger Noble</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--68f43043ff34--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Apr 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk lb lc ab q ee ld le" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lf"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap ie li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/e7c1d4c5510a3052bfb3ab90be98619c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8lQg6q8zAWPPu-CIzNwRIA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author and ChatGPT. ‚ÄúDesign an illustration, with imagery representing data experiments, focusing on basketball data‚Äù prompt. ChatGPT, 4, OpenAI, 15April. 2024. <a class="af nc" href="https://chat.openai.com./" rel="noopener ugc nofollow" target="_blank">https://chat.openai.com.</a></figcaption></figure><p id="61a2" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk"><em class="nz">A Huge thanks to </em><a class="af nc" href="https://medium.com/@mgrc99" rel="noopener"><em class="nz">Martim Chaves</em></a><em class="nz"> who co-authored this post and developed the example scripts.</em></p><p id="57ce" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">It‚Äôs no secret that Machine Learning (ML) systems require careful tuning to become truly useful, and it would be an extremely rare occurrence for a model to work perfectly the first time it‚Äôs run!</p><p id="0a69" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">When first starting out on your ML journey, an easy trap to fall into is to try lots of different things to improve performance, but not recording these configurations along the way. This then makes it difficult to know which configuration (or combination of configurations) had the best performance.</p><p id="5eb2" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">When developing models, there are lots of ‚Äúknobs‚Äù and ‚Äúlevers‚Äù that can be adjusted, and often the best way to improve is to try different configurations and see which one works best. These things include <a class="af nc" href="https://medium.com/@roger_noble/feature-engineering-with-microsoft-fabric-and-pyspark-16d458018744" rel="noopener">improving the features being used</a>, trying different model architectures, adjusting the model‚Äôs hyperparameters, and others. Experimentation needs to be systematic, and the results need to be logged. That‚Äôs why having a good setup to carry out these experiments is fundamental in the development of any practical ML System, in the same way that source control is fundamental for code.</p><p id="5211" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">This is where <em class="nz">experiments</em> come in to play. Experiments are a way to keep track of these different configurations, and the results that come from them.</p><p id="67bf" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">What‚Äôs great about experiments in Fabric is that they are actually a wrapper for <a class="af nc" href="https://mlflow.org/" rel="noopener ugc nofollow" target="_blank">MLFlow</a>, a hugely popular, open-source platform for managing the end-to-end machine learning lifecycle. This means that we can use all of the great features that MLFlow has to offer, but with the added benefit of not having to worry about setting up the infrastructure that a collaborative MLFlow environment would require. This allows us to focus on the fun stuff üòé!</p><p id="fead" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In this post, we‚Äôll be going over how to use experiments in Fabric, and how to log and analyse the results of these experiments. Specifically, we‚Äôll cover:</p><ul class=""><li id="7164" class="nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny oa ob oc bk">How does MLFlow work?</li><li id="a2ed" class="nd ne fr nf b gp od nh ni gs oe nk nl nm of no np nq og ns nt nu oh nw nx ny oa ob oc bk">Creating and Setting experiments</li><li id="d8d7" class="nd ne fr nf b gp od nh ni gs oe nk nl nm of no np nq og ns nt nu oh nw nx ny oa ob oc bk">Running experiments and Logging Results</li><li id="d559" class="nd ne fr nf b gp od nh ni gs oe nk nl nm of no np nq og ns nt nu oh nw nx ny oa ob oc bk">Analysing Results</li></ul><p id="0a96" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">At a high level, MLFlow is a platform that helps manage the end-to-end machine learning lifecycle. It‚Äôs a tool that helps with tracking experiments, packaging code into reproducible runs, and sharing and deploying models. It‚Äôs essentially a database that‚Äôs dedicated to keeping track of all the different configurations and results of the experiments that you run.</p><p id="46f6" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">There are two main organisational structures in MLFlow ‚Äî <strong class="nf fs">experiments</strong> and <strong class="nf fs">runs</strong>.</p><p id="efcd" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">An experiment is a group of runs, where a run is the execution of a block of code, a function or a script. This could be training a model, but it could also be used to track anything where things might change between runs. An experiment is then a way to group related runs.</p><p id="bd14" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">For each run, information can be logged and attached to it ‚Äî these could be metrics, hyperparameters, tags, artifacts (like plots, files or other useful outputs), and even models! By attaching models to runs, we can keep track of which model was used in which run, and how it performed. Think of it like source control for models, which is something we‚Äôll go into in the next post.</p><p id="f1cb" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Runs can be filtered and compared. This allows us to understand which runs were more successful, and select the best performing run and use its setup (for example, in deployment).</p><p id="ff96" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Now that we‚Äôve covered the basics of how MLFlow works, let‚Äôs get into how we can use it in Fabric!</p><h1 id="c38b" class="oi oj fr bf ok ol om gr on oo op gu oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Creating and setting experiments</h1><p id="1c12" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">Like everything in Fabric, creating items can be done in a few ways, either from the workspace <strong class="nf fs">+ New</strong> menu, using the Data Science experience or in code. In this case, we‚Äôll be using the Data Science experience.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pj"><img src="../Images/be643f7ab148a28cc673d70288df619f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X5QAER2QF388hN7baJcwEw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Fig. 1 ‚Äî Creating an Experiment using the UI. Image by author.</figcaption></figure><p id="45c5" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Once that is done, to use that experiment in a Notebook, we need to <code class="cx pk pl pm pn b">import mlflow</code> and set up the experiment name:</p><pre class="mm mn mo mp mq po pn pp bp pq bb bk"><span id="d623" class="pr oj fr pn b bg ps pt l pu pv">import mlflow<br/><br/>experiment_name = "[name of the experiment goes here]"<br/><br/># Set the experiment<br/>mlflow.set_experiment(experiment_name)</span></pre><p id="904d" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Alternatively, an experiment can be created from code, which requires one extra command:</p><pre class="mm mn mo mp mq po pn pp bp pq bb bk"><span id="84bf" class="pr oj fr pn b bg ps pt l pu pv">import mlflow<br/><br/>experiment_name = "[name of the experiment goes here]"<br/><br/># First create the experiment<br/>mlflow.create_experiment(name=experiment_name)<br/><br/># Then select it<br/>mlflow.set_experiment(experiment_name)</span></pre><p id="5016" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Note that, if an experiment with that name already exists, <code class="cx pk pl pm pn b">create_experiment</code> will throw an error. We can avoid this by first checking for the existence of an experiment, and only creating it if it doesn't exist:</p><pre class="mm mn mo mp mq po pn pp bp pq bb bk"><span id="3de5" class="pr oj fr pn b bg ps pt l pu pv"># Check if experiment exists<br/># if not, create it<br/>if not mlflow.get_experiment_by_name(experiment_name):<br/>    mlflow.create_experiment(name=experiment_name)</span></pre><p id="46b3" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Now that we have the experiment set in the current context, we can start running code that will be saved to that experiment.</p><h1 id="644a" class="oi oj fr bf ok ol om gr on oo op gu oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Running experiments and logging results</h1><p id="f80b" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">To start logging our results to an experiment, we need to start a run. This is done using the <code class="cx pk pl pm pn b">start_run()</code> function and returns a <code class="cx pk pl pm pn b">run</code> context manager. Here's an example of how to start a run:</p><pre class="mm mn mo mp mq po pn pp bp pq bb bk"><span id="3f4c" class="pr oj fr pn b bg ps pt l pu pv"><br/># Start the training job with `start_run()`<br/>with mlflow.start_run(run_name="example_run") as run:<br/>    # rest of the code goes here</span></pre><p id="9737" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Once the run is started, we can then begin logging metrics, parameters, and artifacts. Here‚Äôs an example of code that would do that using a simple model and dataset, where we log the model‚Äôs score and the hyperparameters used:</p><pre class="mm mn mo mp mq po pn pp bp pq bb bk"><span id="5274" class="pr oj fr pn b bg ps pt l pu pv"># Set the hyperparameters<br/>hyper_params = {"alpha": 0.5, "beta": 1.2}<br/><br/># Start the training job with `start_run()`<br/>with mlflow.start_run(run_name="simple_training") as run:<br/> # Create model and dataset<br/> model = create_model(hyper_params)<br/> X, y = create_dataset()<br/> <br/> # Train model<br/> model.fit(X, y)<br/><br/> # Calculate score<br/> score = lr.score(X, y)<br/><br/> # Log metrics and hyper-parameters<br/> print("Log metric.")<br/> mlflow.log_metric("score", score)<br/><br/> print("Log params.")<br/> mlflow.log_param("alpha", hyper_params["alpha"])<br/> mlflow.log_param("beta", hyper_params["beta"])</span></pre><p id="3b8f" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In our example above, a simple model is trained, and its score is calculated. Note how metrics can be logged by using <code class="cx pk pl pm pn b">mlflow.log_metric("metric_name", metric)</code> and hyperparameters can be logged using <code class="cx pk pl pm pn b">mlflow.log_param("param_name", param)</code>.</p><h2 id="ae7b" class="pw oj fr bf ok px py pz on qa qb qc oq nm qd qe qf nq qg qh qi nu qj qk ql qm bk">The Data</h2><p id="de27" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">Let‚Äôs now look at the code used for training our models, which are based on the outcome of basketball games. The data we are looking at is from the 2024 US college basketball tournaments, which was obtained from the March Machine Learning Mania 2024 Kaggle competition, the details of which can be found <a class="af nc" href="https://www.kaggle.com/competitions/march-machine-learning-mania-2024/overview" rel="noopener ugc nofollow" target="_blank">here</a>, and is licensed under CC BY 4.0</p><p id="c02b" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In out setup, we wanted to try three different models, that used an increasing number of parameters. For each model, we also wanted to try three different learning rates (a hyperparameter that controls how much we are adjusting the weights of our network for each iteration). The goal was to find the best model and learning rate combination that would give us the best <a class="af nc" href="https://en.wikipedia.org/wiki/Brier_score" rel="noopener ugc nofollow" target="_blank">Brier score</a> on the test set.</p><h2 id="0a74" class="pw oj fr bf ok px py pz on qa qb qc oq nm qd qe qf nq qg qh qi nu qj qk ql qm bk">The Models</h2><p id="0a0c" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">To define the model architecture, we used TensorFlow, creating three simple neural networks. Here are the functions that helped define the models.</p><pre class="mm mn mo mp mq po pn pp bp pq bb bk"><span id="63f6" class="pr oj fr pn b bg ps pt l pu pv">from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense<br/><br/>def create_model_small(input_shape):<br/>    model = Sequential([<br/>        Dense(64, activation='relu', input_shape=(input_shape,)),<br/>        Dense(1, activation='sigmoid')<br/>    ])<br/>    return model<br/><br/>def create_model_medium(input_shape):<br/>    model = Sequential([<br/>        Dense(64, activation='relu', input_shape=(input_shape,)),<br/>        Dense(64, activation='relu'),<br/>        Dense(1, activation='sigmoid')<br/>    ])<br/>    return model<br/><br/>def create_model_large(input_shape):<br/>    model = Sequential([<br/>        Dense(128, activation='relu', input_shape=(input_shape,)),<br/>        Dense(64, activation='relu'),<br/>        Dense(64, activation='relu'),<br/>        Dense(1, activation='sigmoid')<br/>    ])<br/>    return model</span></pre><p id="386d" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Creating our models in this way allows us to easily experiment with different architectures, and see how they perform. We can then use a dictionary to create a little <em class="nz">model factory</em>, that will allow us to easily create the models we want to experiment with.</p><p id="0ec9" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">We also defined the input shape, which was the number of features that were available. We decided to train the models for 100 epochs, which should be enough for convergence ü§û.</p><pre class="mm mn mo mp mq po pn pp bp pq bb bk"><span id="9d9e" class="pr oj fr pn b bg ps pt l pu pv">model_dict = {<br/>    'model_sma': create_model_small,   # small<br/>    'model_med': create_model_medium,  # medium<br/>    'model_lar': create_model_large    # large<br/>}<br/><br/>input_shape = X_train_scaled_df.shape[1]<br/>epochs = 100</span></pre><p id="3c05" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">After this initial setup, it was time to iterate over the models‚Äô dictionary. For each model, an experiment was created. Note how we‚Äôre using the code snippet from before, where we first check if the experiment exists, and only if it doesn‚Äôt do we create it. Otherwise, we just set it.</p><pre class="mm mn mo mp mq po pn pp bp pq bb bk"><span id="a54a" class="pr oj fr pn b bg ps pt l pu pv">import mlflow<br/><br/>for model_name in model_dict:<br/>    <br/>    # create mlflow experiment<br/>    experiment_name = "experiment_v2_" + model_name<br/><br/>    # Check if experiment exists<br/>    # if not, create it<br/>    if not mlflow.get_experiment_by_name(experiment_name):<br/>        mlflow.create_experiment(name=experiment_name)<br/><br/>    # Set experiment<br/>    mlflow.set_experiment(experiment_name)</span></pre><p id="aee8" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Having set the experiment, we then performed three runs for each model, trying out different learning rates <code class="cx pk pl pm pn b">[0.001, 0.01, 0.1]</code>.</p><pre class="mm mn mo mp mq po pn pp bp pq bb bk"><span id="062e" class="pr oj fr pn b bg ps pt l pu pv">for model_name in model_dict:<br/> <br/> # Set the experiment<br/> ...<br/><br/> learning_rate_list = [0.001, 0.01, 0.1]<br/><br/>    for lr in learning_rate_list:<br/>        <br/>        # Create run name for better identification<br/>        run_name = f"{model_name}_{lr}"<br/>        with mlflow.start_run(run_name=run_name) as run:<br/>   ...<br/>   # Train model<br/>   # Save metrics</span></pre><p id="1bd2" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Then, in each run, we initialised a model, compiled it, and trained it. The compilation and training were done in a separate function, which we‚Äôll go into next. As we wanted to set the learning rate, we had to manually initialise the Adam optimiser. As our metric we used the Mean Squared Error (MSE) loss function, saving the model with the best validation loss, and logged the training and validation loss to ensure that the model was converging.</p><pre class="mm mn mo mp mq po pn pp bp pq bb bk"><span id="4d68" class="pr oj fr pn b bg ps pt l pu pv">def compile_and_train(model, X_train, y_train, X_val, y_val, epochs=100, learning_rate=0.001):<br/>    # Instantiate the Adam optimiser with the desired learning rate<br/>    optimiser = Adam(learning_rate=learning_rate)<br/><br/>    model.compile(optimizer=optimiser, loss='mean_squared_error', metrics=['mean_squared_error'])<br/>    <br/>    # Checkpoint to save the best model according to validation loss<br/>    checkpoint_cb = ModelCheckpoint("best_model.h5", save_best_only=True, monitor='val_loss')<br/>    <br/>    history = model.fit(X_train, y_train, validation_data=(X_val, y_val),<br/>                        epochs=epochs, callbacks=[checkpoint_cb], verbose=1)<br/>    <br/>    # Load and return the best model saved during training<br/>    best_model = load_model("best_model.h5")<br/>    return history, best_model</span></pre><p id="1e5b" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Having initialised a model, compiled and trained it, the next step was logging the training and validation losses, calculating the brier score for the test set, then logging the score and the learning rate used. Typically we would also log the training and validation loss using the <code class="cx pk pl pm pn b">step</code> argument in <code class="cx pk pl pm pn b">log_metric</code>, like so:</p><pre class="mm mn mo mp mq po pn pp bp pq bb bk"><span id="49dc" class="pr oj fr pn b bg ps pt l pu pv"># Log training and validation losses<br/>for epoch in range(epochs):<br/> train_loss = history.history['loss'][epoch]<br/> val_loss = history.history['val_loss'][epoch]<br/> mlflow.log_metric("train_loss", train_loss, step=epoch)<br/> mlflow.log_metric("val_loss", val_loss, step=epoch)</span></pre><p id="5b2a" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">However, we opted to create the training and validation loss plot ourselves using <code class="cx pk pl pm pn b">matplotlib</code> and log that as an artifact.</p><p id="5926" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Here‚Äôs the plot function:</p><pre class="mm mn mo mp mq po pn pp bp pq bb bk"><span id="0852" class="pr oj fr pn b bg ps pt l pu pv">import matplotlib.pyplot as plt<br/><br/>def create_and_save_plot(train_loss, val_loss, model_name, lr):<br/>    epochs = range(1, len(train_loss) + 1)<br/><br/>    # Creating the plot<br/>    plt.figure(figsize=(10, 6))<br/>    plt.plot(epochs, train_loss, 'b', label='Training loss')<br/>    plt.plot(epochs, val_loss, 'r', label='Validation loss')<br/>    plt.title('Training and Validation Loss')<br/>    plt.xlabel('Epochs')<br/>    plt.ylabel('Loss')<br/>    plt.legend()<br/>    plt.grid(True)<br/><br/>    plt.title(f"Training and Validation Loss (M: {model_name}, LR: {lr})")<br/><br/>    # Save plot to a file<br/>    plot_path = f"{model_name}_{lr}_loss_plot.png"<br/>    plt.savefig(plot_path)<br/>    plt.close()<br/><br/>    return plot_path</span></pre><p id="df64" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Putting everything together, here‚Äôs what the code for that looks like:</p><pre class="mm mn mo mp mq po pn pp bp pq bb bk"><span id="3d1a" class="pr oj fr pn b bg ps pt l pu pv"><br/>with mlflow.start_run(run_name=run_name) as run:<br/> # Create model and dataset<br/> model = model_dict[model_name](input_shape)<br/><br/> # Train model<br/> history, best_model = compile_and_train(model,<br/>           X_train_scaled_df, y_train,<br/>           X_validation_scaled_df, y_validation,<br/>           epochs,<br/>           lr)<br/> <br/> # Log training and validation loss plot as an artifact<br/> train_loss = history.history['loss']<br/> val_loss = history.history['val_loss']<br/><br/> plot_path = create_and_save_plot(train_loss, val_loss, model_name, lr)<br/> mlflow.log_artifact(plot_path)<br/><br/> # Calculate score<br/> brier_score = evaluate_model(best_model, X_test_scaled_df, y_test)<br/><br/> # Log metrics and hyper-parameters<br/> mlflow.log_metric("brier", brier_score)<br/><br/> # Log hyper-param<br/> mlflow.log_param("lr", lr)<br/><br/> # Log model<br/> ...</span></pre><p id="f84b" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">For each run we also logged the model, which will be useful later on.</p><p id="7153" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The experiments were run, creating an experiment for each model, and three different runs for each experiment with each of the learning rates.</p><h1 id="bd4a" class="oi oj fr bf ok ol om gr on oo op gu oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Analysing results</h1><p id="63e5" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">Now that we‚Äôve run some experiments, it‚Äôs time to analyse the results! To do this, we can go back to the workspace, where we‚Äôll find our newly created experiments with several runs.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qn"><img src="../Images/716c608c377213fdf76833c2e0823194.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ntth5FHi8S7oq0o0_t-Khw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Fig. 2 ‚Äî List of experiments. Image by author.</figcaption></figure><p id="4b29" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Clicking on one experiment, here‚Äôs what we‚Äôll see:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qo"><img src="../Images/b9c60a7efb769462fc52aca550c959b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q1mGNP7yT4Ghzskwa2FKBg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Fig. 3 ‚Äî The Experiment UI. Image by author.</figcaption></figure><p id="2a83" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">On the left we‚Äôll find all of the runs related to that experiment. In this case, we‚Äôre looking at the small model experiment. For each run, there‚Äôs two artifacts, the validation loss plot and the trained model. There‚Äôs also information about the run‚Äôs properties ‚Äî its status and duration, as well as the metrics and hyper-parameters logged.</p><p id="6fb4" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">By clicking on the <strong class="nf fs">View run list</strong>, under the <strong class="nf fs">Compare runs</strong> section, we can compare the different runs.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qp"><img src="../Images/1eece2a8179dad8814505c5fdb9f7e7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_WWEACq_t50razvr1MOfsQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Fig. 4 ‚Äî Comparing runs. Image by author.</figcaption></figure><p id="f6a0" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Inside the run list view, we can select the runs that we wish to compare. In the <strong class="nf fs">metric comparison</strong> tab, we can find plots that show the Brier score against the learning rate. In our case, it looks like the lower the learning rate, the better the score. We could even go further and create more plots for the different metrics against other hyperparameters (if different metrics and hyperparameters had been logged).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qq"><img src="../Images/44a7a6f448f6cbd157dbfee2415b5bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EmT3DuMarGutoR4b_gNOMA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Fig. 5 ‚Äî Plot that shows Brier score against learning rate. Image by author.</figcaption></figure><p id="5d7e" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Perhaps we would like to filter the runs ‚Äî that can be done using <strong class="nf fs">Filters</strong>. For example we can select the runs that have a Brier score lower than 0.25. You can create filters based on logged metrics and parameters and the runs‚Äô properties.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qq"><img src="../Images/66491b7a477f7c74f04dd736a281a55b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6QYR85HmiMCqRpZN5iBN_Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Fig. 6 ‚Äî Filtering runs based on their Brier score. Image by author.</figcaption></figure><p id="f635" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">By doing this, we can visually compare the different runs and assess which configuration led to the best performance. This can also be done using code ‚Äî this is something that will be further explored in the next post.</p><p id="1870" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Using the experiment UI, we are then able to visually explore the different experiments and runs, comparing and filtering them as needed, to understand which configuration works best.</p><h1 id="95b2" class="oi oj fr bf ok ol om gr on oo op gu oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Conclusion</h1><p id="e6c9" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">And that wraps up our exploration of experiments in Fabric!</p><p id="417a" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Not only did we cover how to create and set up experiments, but we also went through how to run experiments and log the results. We also showed how to analyse the results, using the experiment UI to compare and filter runs.</p><p id="0a9f" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In the next post, we‚Äôll be looking at how to select the best model, and how to deploy it. Stay tuned!</p></div></div></div><div class="ab cb qr qs qt qu" role="separator"><span class="qv by bm qw qx qy"/><span class="qv by bm qw qx qy"/><span class="qv by bm qw qx"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><p id="46d6" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk"><em class="nz">Originally published at </em><a class="af nc" href="https://nobledynamic.com/posts/fabric-madness-4/" rel="noopener ugc nofollow" target="_blank"><em class="nz">https://nobledynamic.com</em></a><em class="nz"> on April 22, 2024.</em></p></div></div></div></div>    
</body>
</html>