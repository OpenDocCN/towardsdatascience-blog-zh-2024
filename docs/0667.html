<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Quantum Machine Learning with Python: Kernel Methods and Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Quantum Machine Learning with Python: Kernel Methods and Neural Networks</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quantum-machine-learning-with-python-kernel-methods-and-neural-networks-d60738aa99e1?source=collection_archive---------3-----------------------#2024-03-12">https://towardsdatascience.com/quantum-machine-learning-with-python-kernel-methods-and-neural-networks-d60738aa99e1?source=collection_archive---------3-----------------------#2024-03-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://xaviervasques.medium.com/?source=post_page---byline--d60738aa99e1--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Xavier Vasques" class="l ep by dd de cx" src="../Images/5517ccd82bb4744fc0bff5be9ba399e4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*YEVsGxLRPtepqi_jV6WbEw.jpeg"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d60738aa99e1--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://xaviervasques.medium.com/?source=post_page---byline--d60738aa99e1--------------------------------" rel="noopener follow">Xavier Vasques</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d60738aa99e1--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx kq kr ab q ee ks kt" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="kp"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ko kp">1</span></p></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="ku k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al kv an ao ap hr kw kx ky" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep kz cn"><div class="l ae"><div class="ab cb"><div class="la lb lc ld le lf ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ma mb mc md me mf lx ly paragraph-image"><div role="button" tabindex="0" class="mg mh ed mi bh mj"><div class="lx ly lz"><img src="../Images/6926b4f6dbcce2f545ddac86fa1016d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WfDyVAx9Px3lwKqwXqRW6w.jpeg"/></div></div><figcaption class="ml mm mn lx ly mo mp bf b bg z dx">Photo by Annamária Borsos (used with permission)</figcaption></figure><h1 id="d259" class="mq mr fq bf ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn bk">Introduction</h1><p id="5c6b" class="pw-post-body-paragraph no np fq nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Quantum Machine Learning (QML) represents a fascinating convergence of quantum computing and machine learning technologies. With quantum computing’s potential in mathematics and data processing with complex structure, QML could revolutionize areas like drug discovery, finance, and beyond. This blog delves into the innovative realms of quantum neural networks (QNNs) and quantum kernel techniques, showcasing their unique capabilities through practical Python examples. The blog will not detail the mathematical concepts. For more information do not hesitate to read my latest book <em class="om">Machine Learning Theory and Applications: Hands-on Use Cases with Python on Classical and Quantum Machines, </em>Wiley, 2024.</p><p id="beec" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">Quantum kernel methods, introduce a quantum-enhanced way of processing data. By mapping classical data into quantum feature space, these methods utilize the superposition and entanglement properties of quantum mechanics to perform classifications or regression tasks. The use of quantum kernel estimator and quantum variational classifier examples illustrates the practical application of these concepts. QNNs, leveraging quantum states for computation, offer a novel approach to neural network architecture. The Qiskit framework facilitates the implementation of both quantum kernel methods and QNNs, enabling the exploration of quantum algorithms’ efficiency in learning and pattern recognition.</p><p id="7c0a" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">Incorporating Python code examples, this blog aims to provide comprehensive code examples of QML for readers to explore its promising applications, and the challenges it faces. Through these examples, readers can start practicing and gain an appreciation for the transformative potential of quantum computing in machine learning and the exciting possibilities that lie ahead.</p><h1 id="eac6" class="mq mr fq bf ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn bk">First, install qiskit</h1><p id="379d" class="pw-post-body-paragraph no np fq nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">We will use the open-source SDK Qiskit (<a class="af os" href="https://qiskit.org/" rel="noopener ugc nofollow" target="_blank">https://qiskit.org</a>) which allows working with quantum computers. Qiskit supports Python version 3.6 or later.</p><p id="b0c5" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">In our environment, we can install Qiskit with pip:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="1823" class="pc mr fq oz b bg pd pe l pf pg">pip install qiskit</span></pre><p id="bad8" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">We can also install qiskit-machine-learning using pip:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="f1ff" class="pc mr fq oz b bg pd pe l pf pg">pip install qiskit-machine-learning</span></pre><p id="9ee9" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">Documentation can be found on GitHub: <a class="af os" href="https://github.com/Qiskit/qiskit-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://github.com/Qiskit/qiskit-machine-learning/</a>.</p><p id="73f7" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">To run our code, we can use either simulators or real hardware even if I strongly recommend the use of hardware or push the limits of simulators to improve research in this field. While studying the Qiskit documentation, you will encounter references to the Qiskit Runtime primitives, which serve as implementations of the Sampler and Estimator interfaces found in the qiskit.primitives module. These interfaces facilitate the seamless interchangeability of primitive implementations with minimal code modifications. The initial release of Qiskit Runtime comprises two essential primitives:</p><ul class=""><li id="b987" class="no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol ph pi pj bk">Sampler: This primitive generates quasi-probabilities based on input circuits.</li><li id="ba44" class="no np fq nq b nr pk nt nu nv pl nx ny nz pm ob oc od pn of og oh po oj ok ol ph pi pj bk">Estimator: This primitive calculates expectation values derived from input circuits and observables.</li></ul><p id="fedd" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">For more comprehensive insights, detailed information is available in the following resource: <a class="af os" href="https://qiskit.org/ecosystem/ibm-runtime/tutorials/how-to-getting-started-with-sampler.html." rel="noopener ugc nofollow" target="_blank">https://qiskit.org/ecosystem/ibm-runtime/tutorials/how-to-getting-started-with-sampler.html.</a></p><h1 id="f07b" class="mq mr fq bf ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn bk">Quantum Kernel Methods</h1><p id="1de0" class="pw-post-body-paragraph no np fq nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Venturing into quantum approaches for supervised machine learning poses a novel research direction. Classical machine learning extensively utilizes kernel methods, among which the support vector machine (SVM) for classification stands out for its widespread application.</p><p id="6b81" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">SVMs, known for their role in binary classification, have increasingly been applied to multiclass problems. The essence of binary SVM involves devising a hyperplane to linearly separate n-dimensional data points into two groups, aiming for an optimal margin that distinctively classifies the data into its respective categories. This hyperplane, effective in either the original feature space or a transformed higher-dimensional kernel space, is selected for its capacity to maximize the separation between classes, which involves an optimization problem to maximize the margin, defined as the distance from the nearest data point to the hyperplane on either side. This leads to the formulation of a maximum-margin classifier. The critical data points on the boundary are termed support vectors, and the margin represents a zone typically devoid of data points. An optimal hyperplane too proximate to the data points, indicating a slender margin, undermines the model’s predictive robustness and generalization capability.</p><p id="fdd6" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">To navigate multiclass SVM challenges, methods like the all-pair strategy, which conducts a binary classification for each pair of classes, have been introduced. Beyond straightforward linear classification, nonlinear classifications can be achieved through the kernel trick. This technique employs a kernel function to elevate inputs into a more expansive, higher-dimensional feature space, facilitating the separation of data that is not linearly separable in the input space. The kernel function essentially performs an inner product in a potentially vast Euclidian space, known as the feature space. The goal of nonlinear SVM is to achieve this separation by mapping data to a higher dimension using a suitable mapping. Selecting an appropriate feature map becomes crucial for data that cannot be addressed by linear methods alone. This is where quantum can jump into it. Quantum kernel methods, blending classical kernel strategies with quantum innovations, carve out new avenues in machine learning. Early quantum kernel approaches have focused on encoding data points into inner products or amplitudes in Hilbert space through quantum feature maps. The complexity of the quantum circuit implementing the feature map scales linearly or polylogarithmically with the dataset size.</p><h1 id="c988" class="mq mr fq bf ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn bk">Quantum Kernel with ZZFeatureMaps</h1><p id="7bc8" class="pw-post-body-paragraph no np fq nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">In this first example, we will use the ZZFeatureMap with linear entanglement, we will repeat the data encoding step two times, and we will use feature reduction with principal component analysis. You can of course use other feature reduction, data rescaling or feature selection techniques to improve the accuracy of your models. We will use the breast cancer dataset that you can find here: <a class="af os" href="https://github.com/xaviervasques/hephaistos/blob/main/data/datasets/breastcancer.csv" rel="noopener ugc nofollow" target="_blank">https://github.com/xaviervasques/hephaistos/blob/main/data/datasets/breastcancer.csv</a></p><p id="baf0" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">Let’s describe the steps of the Python script below. This Python script demonstrates an application of integrating quantum computing techniques with traditional machine learning to classify breast cancer data. It represents a hybrid approach, where quantum-enhanced features are used within a classical machine learning workflow. The goal is to predict breast cancer diagnosis (benign or malignant) based on a set of features extracted from the breast mass characteristics.</p><p id="e6ea" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">The way of doing quantum kernel machine learning is very similar to what we do classically as data scientists. We import the necessary libraries (Pandas, NumPy, scikit-learn) and Qiskit for quantum computing and kernel estimation, we load the data, preprocess the data and separate the data into features (X) and target labels (y). A specific step is the quantum feature mapping. The script sets up a quantum feature map using the ZZFeatureMap from Qiskit, configured with specified parameters for feature dimension, repetitions, and entanglement type. Quantum feature maps are critical for translating classical data into quantum states, enabling the application of quantum computing principles for data analysis. Then, the quantum kernel setup consists in configuring a quantum kernel with a fidelity-based approach. It serves as a new method to compute the similarity between data points in the feature space defined by quantum states and potentially capturing complex patterns. The last step comes back to a classic machine learning pipeline with data rescaling with standard scaler, dimension reduction using principal component analysis and the use of support vector classifier (SVC) which utilizes the quantum kernel for classification. We evaluate the model using 5-fold cross-validation.</p><p id="b458" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">Let’s code.</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="e7ef" class="pc mr fq oz b bg pd pe l pf pg"># Import necessary libraries for data manipulation, machine learning, and quantum computing<br/>import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelEncoder<br/><br/># Load the dataset using pandas, specifying the file location and delimiter<br/>breastcancer = './breastcancer.csv'<br/>df = pd.read_csv(breastcancer, delimiter=';')<br/><br/># Remove the 'id' column as it is not useful for prediction, to simplify the dataset<br/>df = df.drop(["id"], axis=1)<br/><br/># Separate the dataset into features (X) and target label (y)<br/>y = df['diagnosis']  # Target label: diagnosis<br/>X = df.drop('diagnosis', axis=1)  # Features: all other columns<br/><br/># Convert the diagnosis string labels into numeric values to be used by machine learning models<br/>label_encoder = LabelEncoder()<br/>y = label_encoder.fit_transform(y)<br/><br/># Quantum computing sections start here<br/># Set parameters for the quantum feature map<br/>feature_dimension = 2  # Number of features used in the quantum feature map<br/>reps = 2  # Number of repetitions of the feature map circuit<br/>entanglement = 'linear'  # Type of entanglement in the quantum circuit<br/><br/># Import quantum feature mapping utilities from Qiskit<br/>from qiskit.circuit.library import ZZFeatureMap<br/>qfm = ZZFeatureMap(feature_dimension=feature_dimension, reps=reps, entanglement=entanglement)<br/><br/># Set up a local simulator for quantum computation<br/>from qiskit.primitives import Sampler<br/>sampler = Sampler()<br/><br/># Configure quantum kernel using ZZFeatureMap and a fidelity-based quantum kernel<br/>from qiskit.algorithms.state_fidelities import ComputeUncompute<br/>from qiskit_machine_learning.kernels import FidelityQuantumKernel<br/>fidelity = ComputeUncompute(sampler=sampler)<br/>quantum_zz = FidelityQuantumKernel(fidelity=fidelity, feature_map=qfm)<br/><br/># Create a machine learning pipeline integrating standard scaler, PCA for dimensionality reduction,<br/># and a Support Vector Classifier using the quantum kernel<br/>from sklearn.pipeline import make_pipeline<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.decomposition import PCA<br/>from sklearn.svm import SVC<br/>pipeline = make_pipeline(StandardScaler(), PCA(n_components=2), SVC(kernel=quantum_zz.evaluate))<br/><br/># Evaluate the model using cross-validation to assess its performance<br/>from sklearn.model_selection import cross_val_score<br/>cv = cross_val_score(pipeline, X, y, cv=5, n_jobs=1)  # n_jobs=1 specifies that the computation will use 1 CPU<br/>mean_score = np.mean(cv)  # Calculate the mean of the cross-validation scores<br/><br/># Print the mean cross-validation score to evaluate the model's performance<br/>print(mean_score)</span></pre><p id="ebf0" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">We will obtain a mean score validation score of 0.63.</p><p id="4501" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">This code is executed with the local simulator. To run on real hardware, replace the following lines:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="4613" class="pc mr fq oz b bg pd pe l pf pg"># Set up a local simulator for quantum computation<br/>from qiskit.primitives import Sampler<br/>sampler = Sampler()</span></pre><p id="d33d" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">by</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="9424" class="pc mr fq oz b bg pd pe l pf pg"># Import necessary classes from qiskit_ibm_runtime for accessing IBM Quantum services<br/>from qiskit_ibm_runtime import QiskitRuntimeService, Sampler<br/><br/># Initialize the QiskitRuntimeService with your IBM Quantum credentials<br/># 'channel', 'token', and 'instance' are placeholders for your actual IBM Quantum account details<br/>service = QiskitRuntimeService(channel='YOUR CHANNEL', token='YOUR TOKEN FROM IBM QUANTUM', instance='YOUR INSTANCE')<br/><br/># Specify the backend you wish to use. This could be a simulator or an actual quantum computer available through IBM Quantum<br/># 'quantum_backend' should be replaced with the name of the quantum backend you wish to use<br/>backend = service.backend('quantum_backend')<br/><br/># Import the Options class to customize the execution of quantum programs<br/>from qiskit_ibm_runtime import Options<br/>options = Options()  # Create an instance of Options<br/><br/># Set the resilience level. Level 1 typically implies some level of error mitigation or resilience against errors<br/>options.resilience_level = 1<br/><br/># Set the number of shots, which is the number of times the quantum circuit will be executed to gather statistics<br/># More shots can lead to more accurate results but take longer to execute<br/>options.execution.shots = 1024<br/><br/># Set the optimization level for compiling the quantum circuit<br/># Higher optimization levels attempt to reduce the circuit's complexity, which can improve execution but may take longer to compile<br/>options.optimization_level = 3<br/><br/># Initialize the Sampler, which is used to run quantum circuits and obtain samples from their measurement outcomes<br/># The Sampler is configured with the specified backend and options<br/>sampler = Sampler(session=backend, options=options)</span></pre><h1 id="d387" class="mq mr fq bf ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn bk">Quantum Kernel Training</h1><p id="3cd4" class="pw-post-body-paragraph no np fq nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">This part will explore the method of Quantum Kernel Alignment (QKA) for the purpose of binary classification. QKA iteratively adjusts a quantum kernel that’s parameterized to fit a dataset, aiming for the largest possible margin in Support Vector Machines (SVM). For further details on QKA, reference is made to the preprint titled “Covariant quantum kernels for data with group structure.” The Python script below is a comprehensive example of integrating traditional machine learning techniques with quantum computing for the prediction accuracy in classifying breast cancer diagnosis. It employs a dataset of breast cancer characteristics to predict the diagnosis (benign or malignant).</p><p id="d65c" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">The machine learning pipeline is similar to the one used in the quantum kernel with ZZFeatureMaps section. The difference is that we will constructs a custom quantum circuit, integrating a rotational layer with a ZZFeatureMap, to prepare the quantum state representations of the data. The quantum kernel estimation step utilizes Qiskit primitives and algorithms for optimizing the quantum kernel’s parameters using a quantum kernel trained (QKT) and an optimizer.</p><p id="b4a3" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">Let’s code.</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="bd1d" class="pc mr fq oz b bg pd pe l pf pg"># Import necessary libraries for data manipulation, machine learning, and quantum computing<br/>import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelEncoder<br/><br/># Load the dataset using pandas, specifying the file location and delimiter<br/>breastcancer = './breastcancer.csv'<br/>df = pd.read_csv(breastcancer, delimiter=';')<br/><br/># Remove the 'id' column as it is not useful for prediction, to simplify the dataset<br/>df = df.drop(["id"], axis=1)<br/><br/># Reduce the dataframe size by sampling 1/3 of the data<br/>df = df.sample(frac=1/3, random_state=1)  # random_state for reproducibility<br/><br/># Separate the dataset into features (X) and target label (y)<br/>y = df['diagnosis']  # Target label: diagnosis<br/>X = df.drop('diagnosis', axis=1)  # Features: all other columns<br/><br/># Convert the diagnosis string labels into numeric values to be used by machine learning models<br/>label_encoder = LabelEncoder()<br/>y = label_encoder.fit_transform(y)<br/><br/># Quantum computing sections start here<br/># Set parameters for the quantum feature map<br/>feature_dimension = 2  # Number of features used in the quantum feature map<br/>reps = 2  # Number of repetitions of the feature map circuit<br/>entanglement = 'linear'  # Type of entanglement in the quantum circuit<br/><br/># Define a custom rotational layer for the quantum feature map<br/>from qiskit import QuantumCircuit<br/>from qiskit.circuit import ParameterVector<br/>training_params = ParameterVector("θ", 1)<br/>fm0 = QuantumCircuit(feature_dimension)<br/>for qubit in range(feature_dimension):<br/>    fm0.ry(training_params[0], qubit)<br/><br/># Use ZZFeatureMap to represent input data<br/>from qiskit.circuit.library import ZZFeatureMap<br/>fm1 = ZZFeatureMap(feature_dimension=feature_dimension, reps=reps, entanglement=entanglement)<br/><br/># Compose the custom rotational layer with the ZZFeatureMap to create the feature map<br/>fm = fm0.compose(fm1)<br/><br/># Initialize the Sampler, a Qiskit primitive for sampling from quantum circuits<br/>from qiskit.primitives import Sampler<br/>sampler = Sampler()<br/><br/># Set up the ComputeUncompute fidelity object for quantum kernel estimation<br/>from qiskit.algorithms.state_fidelities import ComputeUncompute<br/>from qiskit_machine_learning.kernels import TrainableFidelityQuantumKernel<br/>fidelity = ComputeUncompute(sampler=sampler)<br/><br/># Instantiate the quantum kernel with the feature map and training parameters<br/>quant_kernel = TrainableFidelityQuantumKernel(fidelity=fidelity, feature_map=fm, training_parameters=training_params)<br/><br/># Callback class for tracking optimization progress<br/>class QKTCallback:<br/>    # Callback wrapper class<br/>    def __init__(self):<br/>        self._data = [[] for i in range(5)]<br/><br/>    def callback(self, x0, x1=None, x2=None, x3=None, x4=None):<br/>        #Capture callback data for analysis<br/>        for i, x in enumerate([x0, x1, x2, x3, x4]):<br/>            self._data[i].append(x)<br/><br/>    def get_callback_data(self):<br/>        #Get captured callback data<br/>        return self._data<br/><br/>    def clear_callback_data(self):<br/>        #Clear captured callback data<br/>        self._data = [[] for i in range(5)]<br/><br/># Setup and instantiate the optimizer for the quantum kernel<br/>from qiskit.algorithms.optimizers import SPSA<br/>cb_qkt = QKTCallback()<br/>spsa_opt = SPSA(maxiter=10, callback=cb_qkt.callback, learning_rate=0.01, perturbation=0.05)<br/><br/># Quantum Kernel Trainer (QKT) for optimizing the kernel parameters<br/>from qiskit_machine_learning.kernels.algorithms import QuantumKernelTrainer<br/>qkt = QuantumKernelTrainer(<br/>    quantum_kernel=quant_kernel, loss="svc_loss", optimizer=spsa_opt, initial_point=[np.pi / 2]<br/>)<br/><br/># Reduce dimensionality of the data using PCA<br/>from sklearn.decomposition import PCA<br/>pca = PCA(n_components=2)<br/>X_ = pca.fit_transform(X)<br/><br/># Train the quantum kernel with the reduced dataset<br/>qka_results = qkt.fit(X_, y)<br/>optimized_kernel = qka_results.quantum_kernel<br/><br/># Use the quantum-enhanced kernel in a Quantum Support Vector Classifier (QSVC)<br/>from qiskit_machine_learning.algorithms import QSVC<br/>from sklearn.pipeline import make_pipeline<br/>from sklearn.preprocessing import StandardScaler<br/>qsvc = QSVC(quantum_kernel=optimized_kernel)<br/>pipeline = make_pipeline(StandardScaler(), PCA(n_components=2), qsvc)<br/><br/># Evaluate the performance of the model using cross-validation<br/>from sklearn.model_selection import cross_val_score<br/>cv = cross_val_score(pipeline, X, y, cv=5, n_jobs=1)<br/>mean_score = np.mean(cv)<br/><br/># Print the mean cross-validation score<br/>print(mean_score)</span></pre><p id="38fa" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">We will obtain the following output: 0.6526315789473685</p><p id="ccd5" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">As you certainly observed, there is time differences in execution between QKT and using a quantum kernel with a predefined feature map like ZZFeatureMap even if we reduced the dataframe size by sampling 1/3 of the data and setting the maximum iteration for SPSA to 10. QKT involves not only the use of a quantum kernel but also the optimization of parameters within the quantum feature map or the kernel itself to improve model performance. This optimization process requires iterative adjustments to the parameters, where each iteration involves running quantum computations to evaluate the performance of the current parameter set. This iterative nature significantly increases computational time. When using a predefined quantum kernel like the ZZFeatureMap, the feature mapping is fixed, and there’s no iterative optimization of quantum parameters involved. The quantum computations are performed to evaluate the kernel between data points, but without the added overhead of adjusting and optimizing quantum circuit parameters. This approach is more straightforward and requires fewer quantum computations, making it faster. Each step of the optimization process in QKT requires evaluating the model’s performance with the current quantum kernel, which depends on the quantum feature map parameters at that step. This means multiple evaluations of the kernel matrix, each of which requires a substantial number of quantum computations.</p><h1 id="dc70" class="mq mr fq bf ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn bk">Quantum Neural Networks</h1><p id="a581" class="pw-post-body-paragraph no np fq nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">This Python script below incorporates quantum neural networks (QNNs) into a machine learning pipeline. In the script, we need to configure the quantum feature map and ansatz (a quantum circuit structure), construct a quantum circuit by appending the feature map and ansatz to a base quantum circuit (this setup is crucial for creating quantum neural networks that process input data quantum mechanically) and create a QNN using the quantum circuit designed for binary classification. Before coming back to the classic machine learning pipeline with data rescaling, data reduction and model evaluation, we employ a quantum classifier which integrates the QNN with a classical optimization algorithm (COBYLA) for training. A callback function is defined to visualize the optimization process, tracking the objective function value across iterations.</p><p id="db4d" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">Let’s code.</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="29df" class="pc mr fq oz b bg pd pe l pf pg"># Importing essential libraries for handling data, machine learning, and integrating quantum computing<br/>import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelEncoder<br/>import matplotlib.pyplot as plt  # For data visualization<br/><br/># Load and prepare the dataset<br/>breastcancer = './breastcancer.csv'<br/>df = pd.read_csv(breastcancer, delimiter=';')  # Load dataset from CSV file<br/>df = df.drop(["id"], axis=1)  # Remove the 'id' column as it's not necessary for analysis<br/><br/># Splitting the data into features (X) and the target variable (y)<br/>y = df['diagnosis']  # Target variable: diagnosis result<br/>X = df.drop('diagnosis', axis=1)  # Feature matrix: all data except the diagnosis<br/><br/># Encoding string labels in 'y' into numerical form for machine learning models<br/>label_encoder = LabelEncoder()<br/>y = label_encoder.fit_transform(y)  # Transform labels to numeric<br/><br/># Quantum feature map and circuit configuration<br/>feature_dimension = 2  # Dimensionality for the feature map (matches PCA reduction later)<br/>reps = 2  # Number of repetitions of the ansatz circuit for depth<br/>entanglement = 'linear'  # Type of qubit entanglement in the circuit<br/><br/># Initialize an array to store evaluations of the objective function during optimization<br/>objective_func_vals = []<br/><br/># Define a callback function for visualization of the optimization process<br/>def callback_graph(weights, obj_func_eval):<br/>    """Updates and saves a plot of the objective function value after each iteration."""<br/>    objective_func_vals.append(obj_func_eval)<br/>    plt.title("Objective function value against iteration")<br/>    plt.xlabel("Iteration")<br/>    plt.ylabel("Objective function value")<br/>    plt.plot(range(len(objective_func_vals)), objective_func_vals)<br/>    plt.savefig('Objective_function_value_against_iteration.png')  # Save plot to file<br/><br/># Example function not directly used in the main workflow, demonstrating a utility function<br/>def parity(x):<br/>    """Example function to calculate parity of an integer."""<br/>    return "{:b}".format(x).count("1") % 2<br/><br/># Initializing the quantum sampler from Qiskit<br/>from qiskit.primitives import Sampler<br/>sampler = Sampler()  # Used for sampling from quantum circuits<br/><br/># Constructing the quantum feature map and ansatz for the quantum circuit<br/>from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes<br/>feature_map = ZZFeatureMap(feature_dimension)<br/>ansatz = RealAmplitudes(feature_dimension, reps=reps)  # Quantum circuit ansatz<br/><br/># Composing the quantum circuit with the feature map and ansatz<br/>from qiskit import QuantumCircuit<br/>qc = QuantumCircuit(feature_dimension)<br/>qc.append(feature_map, range(feature_dimension))  # Apply feature map to circuit<br/>qc.append(ansatz, range(feature_dimension))  # Apply ansatz to circuit<br/>qc.decompose().draw()  # Draw and decompose circuit for visualization<br/><br/># Creating a Quantum Neural Network (QNN) using the configured quantum circuit<br/>from qiskit_machine_learning.neural_networks import SamplerQNN<br/>sampler_qnn = SamplerQNN(<br/>    circuit=qc,<br/>    input_params=feature_map.parameters,<br/>    weight_params=ansatz.parameters,<br/>    output_shape=2,  # For binary classification<br/>    sampler=sampler<br/>)<br/><br/># Configuring the quantum classifier with the COBYLA optimizer<br/>from qiskit.algorithms.optimizers import COBYLA<br/>from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier<br/>sampler_classifier = NeuralNetworkClassifier(<br/>    neural_network=sampler_qnn, optimizer=COBYLA(maxiter=100), callback=callback_graph)<br/><br/># Setting up K-Fold Cross Validation to assess model performance<br/>from sklearn.model_selection import KFold<br/>k_fold = KFold(n_splits=5)  # 5-fold cross-validation<br/>score = np.zeros(5)  # Array to store scores for each fold<br/>i = 0  # Index counter for scores array<br/>for indices_train, indices_test in k_fold.split(X):<br/>    X_train, X_test = X.iloc[indices_train], X.iloc[indices_test]<br/>    y_train, y_test = y[indices_train], y[indices_test]<br/>    <br/>    # Applying PCA to reduce the dimensionality of the dataset to match the quantum feature map<br/>    from sklearn.decomposition import PCA<br/>    pca = PCA(n_components=2)  # Reduce to 2 dimensions for the quantum circuit<br/>    X_train = pca.fit_transform(X_train)  # Transform training set<br/>    X_test = pca.fit_transform(X_test)  # Transform test set<br/>    <br/>    # Training the quantum classifier with the training set<br/>    sampler_classifier.fit(X_train, y_train)<br/>    <br/>    # Evaluating the classifier's performance on the test set<br/>    score[i] = sampler_classifier.score(X_test, y_test)  # Store score for this fold<br/>    i += 1  # Increment index for next score<br/><br/># Calculating and displaying the results of cross-validation<br/>import math<br/>print("Cross-validation scores:", score)<br/>cross_mean = np.mean(score)  # Mean of cross-validation scores<br/>cross_var = np.var(score)  # Variance of scores<br/>cross_std = math.sqrt(cross_var)  # Standard deviation of scores<br/><br/>print("Mean cross-validation score:", cross_mean)<br/>print("Standard deviation of cross-validation scores:", cross_std)</span></pre><p id="3a67" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">We obtain the following results:</p><p id="8911" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">Cross-validation scores: [0.34210526 0.4122807 0.42982456 0.21929825 0.50442478]</p><p id="807b" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">Mean cross-validation score: 0.3815867101381773</p><p id="09b8" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">Standard deviation of cross-validation scores: 0.09618163326986424</p><figure class="ot ou ov ow ox mf lx ly paragraph-image"><div class="lx ly pp"><img src="../Images/194a9fd37a05cf66d9a3ee867de7e44a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*zljyXKGnpo1j2QVwK8sSwA.png"/></div></figure><p id="12f7" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">As we can see, in this specific dataset, QNN doesn’t provide a very good classification score.</p><h1 id="62b5" class="mq mr fq bf ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn bk">Conclusion</h1><p id="03d6" class="pw-post-body-paragraph no np fq nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">This idea of this blog is to make it easy to start using quantum machine learning. Quantum Machine Learning is an emerging field at the intersection of quantum computing and machine learning that holds the potential to revolutionize how we process and analyze vast datasets by leveraging the inherent advantages of quantum mechanics. As we showed in our paper <em class="om">Application of quantum machine learning using quantum kernel algorithms on multiclass neuron M-type classification</em> published in Nature Scientific Report, a crucial aspect of optimizing QML models, including Quantum Neural Networks (QNNs), involves pre-processing techniques such as feature rescaling, feature extraction, and feature selection.</p><p id="cfcc" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">These techniques are not only essential in classical machine learning but also present significant benefits when applied within the quantum computing framework, enhancing the performance and efficiency of quantum machine learning algorithms. In the quantum realm, feature extraction techniques like Principal Component Analysis (PCA) can be quantum-enhanced to reduce the dimensionality of the data while retaining most of its significant information. This reduction is vital for QML models due to the limited number of qubits available on current quantum hardware.</p><p id="cf63" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">Quantum feature extraction can efficiently map high-dimensional data into a lower-dimensional quantum space, enabling quantum models to process complex datasets with fewer resources. Selecting the most relevant features is also a way for optimizing quantum circuit complexity and resource allocation. In quantum machine learning, feature selection helps in identifying and utilizing the most informative features, reducing the need for extensive quantum resources.</p><p id="1eec" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">This process not only simplifies the quantum models but also enhances their performance by focusing the computational efforts on the features that contribute the most to the predictive accuracy of the model.</p><p id="517d" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk"><strong class="nq fr">Sources</strong></p><p id="3904" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk"><a class="af os" href="https://www.amazon.fr/Machine-Learning-Theory-Applications-Hands-ebook/dp/B0CS8MMSB4/ref=sr_1_2?__mk_fr_FR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&amp;crid=1SIBDYLGUX7XR&amp;dib=eyJ2IjoiMSJ9.IV7g17el89GNQznK4WE3TFa0RaOwsRf3n53jTAydpJXjqj4juibAmhXCmZv4JjLLjmbjY0nFBPuzH_p0TYDq_8QTq6SXsh_o3U9b6VT2U_FD5takpiI6ctH05JelH-XQ2mItrIX02LcvRu2jHDE6RDe20qR9DNLG5lY5gi93vbYlPY2ahKtCH5imnzLE4jgLSuU81s5qAaRPPD13MwhwPofgaJ9FqbYbtxrHFAKjPfE.VdY_00svNarbBN4C07OopzZlgOZEVb3AIKSlz_hDA2Y&amp;dib_tag=se&amp;keywords=xavier+vasques&amp;qid=1710253907&amp;sprefix=xavier+vasques%2Caps%2C104&amp;sr=8-2" rel="noopener ugc nofollow" target="_blank"><em class="om">Machine Learning Theory and Applications: Hands-on Use Cases with Python on Classical and Quantum Machines, </em>Wiley, 2024</a></p><p id="e578" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk"><em class="om">Vasques, X., Paik, H. &amp; Cif, L. Application of quantum machine learning using quantum kernel algorithms on multiclass neuron M-type classification. Sci Rep 13, 11541 (2023). </em><a class="af os" href="https://doi.org/10.1038/s41598-023-38558-z" rel="noopener ugc nofollow" target="_blank"><em class="om">https://doi.org/10.1038/s41598-023-38558-z</em></a></p><p id="9c0b" class="pw-post-body-paragraph no np fq nq b nr on nt nu nv oo nx ny nz op ob oc od oq of og oh or oj ok ol fj bk">This dataset used is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.</p></div></div></div></div>    
</body>
</html>