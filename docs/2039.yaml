- en: Creating a RAG Chatbot with Langflow and Astra DB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Langflow和Astra DB创建RAG聊天机器人
- en: 原文：[https://towardsdatascience.com/creating-a-rag-chatbot-with-langflow-and-astra-db-582ad588cf37?source=collection_archive---------6-----------------------#2024-08-21](https://towardsdatascience.com/creating-a-rag-chatbot-with-langflow-and-astra-db-582ad588cf37?source=collection_archive---------6-----------------------#2024-08-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/creating-a-rag-chatbot-with-langflow-and-astra-db-582ad588cf37?source=collection_archive---------6-----------------------#2024-08-21](https://towardsdatascience.com/creating-a-rag-chatbot-with-langflow-and-astra-db-582ad588cf37?source=collection_archive---------6-----------------------#2024-08-21)
- en: A walkthrough on how to create a RAG chatbot using Langflow’s intuitive interface,
    integrating LLMs with vector databases for context-driven responses.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过Langflow直观界面创建RAG聊天机器人的步骤，将LLMs与向量数据库集成，实现基于上下文的响应。
- en: '[](https://brunocaraffa.medium.com/?source=post_page---byline--582ad588cf37--------------------------------)[![Bruno
    Caraffa](../Images/414f88c6974dba79ec9851c051eff49f.png)](https://brunocaraffa.medium.com/?source=post_page---byline--582ad588cf37--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--582ad588cf37--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--582ad588cf37--------------------------------)
    [Bruno Caraffa](https://brunocaraffa.medium.com/?source=post_page---byline--582ad588cf37--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://brunocaraffa.medium.com/?source=post_page---byline--582ad588cf37--------------------------------)[![Bruno
    Caraffa](../Images/414f88c6974dba79ec9851c051eff49f.png)](https://brunocaraffa.medium.com/?source=post_page---byline--582ad588cf37--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--582ad588cf37--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--582ad588cf37--------------------------------)
    [Bruno Caraffa](https://brunocaraffa.medium.com/?source=post_page---byline--582ad588cf37--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--582ad588cf37--------------------------------)
    ·6 min read·Aug 21, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--582ad588cf37--------------------------------)·6分钟阅读·2024年8月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4c4fd5e21eece275bd63778538e7fb4b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c4fd5e21eece275bd63778538e7fb4b.png)'
- en: Photo by [Igor Omilaev](https://unsplash.com/@omilaev?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Igor Omilaev](https://unsplash.com/@omilaev?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: A Retrieval-Augmented Generation, or RAG, is a natural language process that
    involves combining traditional retrieval techniques with LLMs to generate a more
    accurate and relevant text by integrating the generation properties with the context
    provided by the retrievals. It has been used widely recently in the context of
    chatbots, providing the ability for companies to improve their automated communications
    with clients by using cutting-edge LLM models customized with their data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成，或RAG，是一种自然语言处理，涉及将传统的检索技术与LLMs结合，通过将生成属性与检索提供的上下文集成，生成更准确和相关的文本。最近在聊天机器人的背景下广泛使用，为公司提供了利用定制数据的尖端LLM模型改进与客户的自动通信的能力。
- en: Langflow is the graphical user interface of Langchain, a centralized development
    environment for LLMs. Back in October 2022, Langchain was released and by June
    2023 it had become one of the most used open-source projects on GitHub. It took
    the AI community by storm, specifically for the framework developed to create
    and customize multiple LLMs with functionalities like integrations with the most
    relevant text generation and embedding models, the possibility of chaining LLM
    calls, the ability to manage prompts, the option of equipping vector databases
    to speed up calculations, and delivering smoothly the outcomes to external APIs
    and task flows.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Langflow是Langchain的图形用户界面，Langchain是LLMs的集中式开发环境。2022年10月，Langchain发布，到2023年6月已成为GitHub上使用最广泛的开源项目之一。它席卷了人工智能社区，特别是为创建和定制多个LLMs开发的框架，具有与最相关的文本生成和嵌入模型集成的功能，链式LLM调用的可能性，管理提示的能力，装备向量数据库以加快计算速度的选项，并顺利将结果交付给外部API和任务流程。
- en: 'In this article, an end-to-end RAG Chatbot created with Langflow is going to
    be presented using the famous Titanic dataset. First, the sign-up needs to be
    made in the Langflow platform, [here](https://astra.datastax.com/langflow/). To
    begin a new project some useful pre-built flows can be quickly customizable based
    on the user needs. To create a RAG Chatbot the best option is to select the **Vector
    Store RAG** template. Image 1 exhibits the original flow:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将介绍一个使用Langflow创建的端到端RAG聊天机器人，并使用著名的Titanic数据集。首先，需在Langflow平台进行注册，[这里](https://astra.datastax.com/langflow/)。要开始一个新项目，可以根据用户需求快速定制一些有用的预构建流程。要创建RAG聊天机器人，最佳选择是选择**Vector
    Store RAG**模板。图片1展示了原始流程：
- en: '![](../Images/9326f9718c2b6f48fc1960551a3094d2.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9326f9718c2b6f48fc1960551a3094d2.png)'
- en: 'Image 1 — Langflow Vector Store RAG Template Flows. Source: The author.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图片1 — Langflow Vector Store RAG模板流程。来源：作者。
- en: The template has OpenAI preselected for the embeddings and text generations,
    and those are the ones used in this article, but other options like Ollama, NVIDIA,
    and Amazon Bedrock are available and easily integrable by just setting up the
    API key. Before using the integration with an LLM provider is important to check
    if the chosen integration is active on the configurations, just like in Image
    2 below. Also, global variables like API keys and model names can be defined to
    facilitate the input on the flow objects.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该模板已预设OpenAI用于嵌入和文本生成，并且这些选项在本文中使用，但其他选项如Ollama、NVIDIA和Amazon Bedrock也可以轻松集成，只需设置API密钥即可。在使用与LLM提供商的集成之前，重要的是检查所选择的集成是否在配置中处于激活状态，就像下面的图片2所示。另外，像API密钥和模型名称这样的全局变量可以定义，以便在流程对象中简化输入。
- en: '![](../Images/47ed3e1fc3ae4ac10f98b2cb274387e5.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47ed3e1fc3ae4ac10f98b2cb274387e5.png)'
- en: 'Image 2 — OpenAI Active Integrations and Overview. Source: The author.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图片2 — OpenAI 活跃集成与概览。来源：作者。
- en: There are two different flows on the Vector Store Rag template, the one below
    displays the retrieval part of the RAG where the context is provided by uploading
    a document, splitting, embedding, and then saving it into a Vector Database on
    Astra DB that can be created easily on the flow interface. Currently, by default,
    the Astra DB object retrieves the Astra DB application token so it is not even
    necessary to gather it. Finally, the collection that will store the embedded values
    in the vector DB needs to be created. The collection dimension needs to match
    the one from the embedding model, which is available in the documentation, for
    proper storing of the embedding results. So if the chosen embedding model is OpenAI’s
    text-embedding-3-small therefore the created collection dimension has to be 1536\.
    Image 3 below presents the complete retrieval flow.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在Vector Store RAG模板中有两种不同的流程，下面展示了RAG的检索部分，其中上下文是通过上传文档、分割、嵌入，然后保存到Astra DB的Vector数据库中完成的，用户可以在流程界面轻松创建该数据库。目前，默认情况下，Astra
    DB对象会检索Astra DB应用程序令牌，因此不需要手动收集它。最后，需要创建用于存储嵌入值的集合。该集合的维度需要与嵌入模型中的维度相匹配，这些信息可以在文档中找到，以确保嵌入结果的正确存储。因此，如果选择的嵌入模型是OpenAI的text-embedding-3-small，那么创建的集合维度必须是1536。下面的图片3展示了完整的检索流程。
- en: '![](../Images/9c7000c9daa4eb839ade62fb4a3f207a.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c7000c9daa4eb839ade62fb4a3f207a.png)'
- en: 'Image 3 — Retrieval Flow From the Titanic Dataset. Source: The author.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图片3 — 来自Titanic数据集的检索流程。来源：作者。
- en: The dataset used to enhance the chatbot context was the [Titanic dataset](https://www.kaggle.com/datasets/yasserh/titanic-dataset?resource=download)
    (CC0 License). By the end of the RAG process, the chatbot should be able to provide
    specific details and answer complex questions about the passengers. But first,
    we update the file on a generic file loader object and then split it using the
    global variable “separator;” since the original format was CSV. Also, the chunk
    overlap and chunk size were set to 0 since each chunk will be a passenger by using
    the separator. If the input file is in straight text format it is necessary to
    apply the chunk overlap and size setups to properly create the embeddings. To
    finish the flow the vectors are stored in the ***titanic_vector_db*** on the ***demo_assistente***
    database.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 用于增强聊天机器人上下文的数据集是[Titanic 数据集](https://www.kaggle.com/datasets/yasserh/titanic-dataset?resource=download)（CC0
    许可）。在 RAG 过程结束时，聊天机器人应该能够提供乘客的具体细节并回答关于乘客的复杂问题。但首先，我们在通用文件加载对象上更新文件，然后使用全局变量“separator;”对其进行分割，因为原始格式是
    CSV。此外，块重叠和块大小被设置为 0，因为每个块将是一个乘客，并使用分隔符。如果输入文件是纯文本格式，则需要应用块重叠和块大小设置，以正确创建嵌入。最后，向量被存储在
    ***titanic_vector_db*** 中，存储在 ***demo_assistente*** 数据库中。
- en: '![](../Images/ac404fab072ee2604e5c781c4f9c3397.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac404fab072ee2604e5c781c4f9c3397.png)'
- en: Image 4 — Full Generation Flow. The Author.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4 — 完整的生成流程。来源：作者。
- en: Moving to the generation flow of the RAG, displayed in Image 4, it is triggered
    with the user input on the chat which is then searched into the database to provide
    context for the prompt later on. So if the user asks something related to the
    name “Owen” on the input the search will run through the vector DB’s collection
    looking for “Owen” related vectors, retrieve and run them through the parser to
    convert them to text, and finally, the context necessary for the prompt later
    on is obtained. Image 5 shows the results of the search.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 移动到 RAG 的生成流程，如图 4 所示，它是通过用户在聊天中的输入触发的，然后该输入会在数据库中进行搜索，以提供后续提示所需的上下文。所以，如果用户输入与名称“欧文”（Owen）相关的内容，搜索将通过向量数据库的集合运行，寻找与“欧文”相关的向量，检索并通过解析器将其转换为文本，最终获得后续提示所需的上下文。图
    5 显示了搜索结果。
- en: '![](../Images/197b8efe0ed0fe3acd688a302f2e806e.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/197b8efe0ed0fe3acd688a302f2e806e.png)'
- en: 'Image 5 — Result of the search conducted in the Vector DB to obtain context.
    Source: The Author.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5 — 在向量数据库中进行的搜索结果，以获得上下文。来源：作者。
- en: Back to the beginning, it is also critical to connect again the embedding model
    to the vector DB using the same model in the retrieval flow to run a valid search,
    otherwise, it would always come empty since the embedding models used in the retrieval
    and generation flows then would be different. Furthermore, this step evidences
    the massive performance benefits of using vector DBs in a RAG, where the context
    needs to be retrieved and passed to the prompt quickly before forging any type
    of response to the user.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 回到开始，重新连接嵌入模型到向量数据库也至关重要，必须在检索流程中使用相同的模型来执行有效的搜索，否则搜索将始终为空，因为检索和生成流程中使用的嵌入模型将不一致。此外，这一步突显了在
    RAG 中使用向量数据库的巨大性能优势，在该过程中，需要迅速检索上下文并将其传递到提示中，然后再生成任何类型的响应给用户。
- en: In the prompt, shown in Image 6, the context comes from the parser already converted
    to text and the question comes from the original user input. The image below shows
    how the prompt can be structured to integrate the context with the question.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 6 所示的提示中，上下文来自已通过解析器转换为文本的内容，问题来自原始用户输入。下面的图像展示了如何构建提示，将上下文与问题集成。
- en: '![](../Images/4b33e6502020e5f408c1143376445d66.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b33e6502020e5f408c1143376445d66.png)'
- en: 'Image 6 — Prompt that will be passed to the AI model. Source: The Author.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6 — 将传递给 AI 模型的提示。来源：作者。
- en: With the prompt written it is time for the text generation model. In this flow,
    the GPT4 model was chosen with a temperature of 0.5, a recommended standard for
    chatbots. The temperature controls the randomness of predictions made by a LLM.
    A lower temperature will generate more deterministic and straightforward answers,
    leading to a more predictable text. A higher one will generate more creative outputs
    even though if it is too high the model can easily hallucinate and produce incoherent
    text. Finally, just set the API key using the global variable with OpenAI’s API
    key and it’s as easy as that. Then, it’s time to run the flows and check the results
    on the playground.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备好文本生成模型后，现在是时候进行文本生成了。在这个流程中，选择了GPT4模型，温度为0.5，这是聊天机器人的推荐标准。温度控制着由LLM进行的预测的随机性。较低的温度将生成更确定和直接的答案，导致更可预测的文本。较高的温度将生成更具创造性的输出，尽管如果太高，模型可能会轻易产生幻觉并产生不连贯的文本。最后，只需使用OpenAI的API密钥设置全局变量，就这么简单。然后，是时候运行流程并在游乐场上检查结果了。
- en: '![](../Images/830ae5f627aa329c8d434c247c532cfa.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/830ae5f627aa329c8d434c247c532cfa.png)'
- en: 'Image 7 — Playground showing the result of the RAG Chatbot. Source: The Author.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图片7 — 展示RAG聊天机器人结果的游乐场。来源：作者。
- en: The conversation in Image 7 clearly shows that the chatbot has correctly obtained
    the context and rightfully answered detailed questions about the passengers. And
    even though it might be disappointing to find out that there were not any Rose
    or Jack on the Titanic, unfortunately, that is true. And that’s it. The RAG chatbot
    is created, and of course, it can be enhanced to increase conversational performance
    and cover some possible misinterpretations, but this article demonstrates how
    easy Langflow makes it to adapt and customize LLMs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图片7中的对话清楚地显示，聊天机器人已经正确获取了上下文，并正确回答了关于乘客的详细问题。尽管发现泰坦尼克号上没有罗斯或杰克可能令人失望，但不幸的是，这是事实。就是这样。RAG聊天机器人已经创建，当然，可以增强其以提高对话性能并覆盖一些可能的误解，但本文展示了Langflow如何轻松地使LLMs适应和定制。
- en: Finally, to deploy the flow there are multiple possibilities. HuggingFace Spaces
    is an easy way to deploy the RAG chatbot with scalable hardware infrastructure
    and native Langflow that wouldn’t require any installations. Langflow can also
    be installed and used through a Kubernetes cluster, a Docker container, or directly
    in GCP by using a VM and Google Cloud Shell. For more information about deployment
    look at the [documentation](https://docs.langflow.org/deployment-hugging-face-spaces).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，部署流程有多种可能性。HuggingFace Spaces是一种轻松部署RAG聊天机器人的方式，具有可扩展的硬件基础设施和本地Langflow，无需任何安装。Langflow也可以通过Kubernetes集群、Docker容器或直接在GCP中使用VM和Google
    Cloud Shell进行安装和使用。有关部署的更多信息，请查看[文档](https://docs.langflow.org/deployment-hugging-face-spaces)。
- en: New times are coming and low-code solutions are starting to set the tone of
    how AI is going to be developed in the real world in the short future. This article
    presented how Langflow revolutionizes AI by centralizing multiple integrations
    with an intuitive UI and templates. Nowadays anyone with basic knowledge of AI
    can build a complex application that at the beginning of the decade would take
    a huge amount of code and deep learning frameworks expertise.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 新时代即将到来，低代码解决方案开始主导未来短期内AI在现实世界中的发展方式。本文介绍了Langflow如何通过集成多个直观UI和模板来革新AI。如今，任何具备基本AI知识的人都可以构建一个复杂的应用程序，而在本十年初，这将需要大量的代码和深度学习框架专业知识。
