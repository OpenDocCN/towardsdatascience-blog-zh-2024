["```py\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\nSEED = 32\n\n# Load data\nfilename = \"train.csv\" # train.csv from https://www.kaggle.com/datasets/iabhishekofficial/mobile-price-classification\n\ndf = pd.read_csv(filename)\n\n# Train - test split\ndf_train, df_test = train_test_split(df, test_size=0.2, stratify=df.iloc[:,-1], random_state=SEED)\ndf_train = df_train.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\n# The last column is the target variable\nX_train = df_train.iloc[:,0:20]\ny_train = df_train.iloc[:,-1]\nX_test = df_test.iloc[:,0:20]\ny_test = df_test.iloc[:,-1]\n\n# Stratified kfold over the train set for cross validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\nsplits = list(skf.split(X_train, y_train))\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, classification_report\n\nmodel = RandomForestClassifier(random_state=SEED)\nmodel.fit(X_train,y_train)\npreds = model.predict(X_test)\n\nprint(classification_report(y_test, preds))\nprint(f\"Global F1: {f1_score(y_test, preds, average='weighted')}\")\n```", "```py\nimport optuna\n\nclass FeatureSelectionOptuna:\n    \"\"\"\n    This class implements feature selection using Optuna optimization framework.\n\n    Parameters:\n\n    - model (object): The predictive model to evaluate; this should be any object that implements fit() and predict() methods.\n    - loss_fn (function): The loss function to use for evaluating the model performance. This function should take the true labels and the\n                          predictions as inputs and return a loss value.\n    - features (list of str): A list containing the names of all possible features that can be selected for the model.\n    - X (DataFrame): The complete set of feature data (pandas DataFrame) from which subsets will be selected for training the model.\n    - y (Series): The target variable associated with the X data (pandas Series).\n    - splits (list of tuples): A list of tuples where each tuple contains two elements, the train indices and the validation indices.\n    - penalty (float, optional): A factor used to penalize the objective function based on the number of features used.\n    \"\"\"\n\n    def __init__(self,\n                 model,\n                 loss_fn,\n                 features,\n                 X,\n                 y,\n                 splits,\n                 penalty=0):\n\n        self.model = model\n        self.loss_fn = loss_fn\n        self.features = features\n        self.X = X\n        self.y = y\n        self.splits = splits\n        self.penalty = penalty\n\n    def __call__(self,\n                 trial: optuna.trial.Trial):\n\n        # Select True / False for each feature\n        selected_features = [trial.suggest_categorical(name, [True, False]) for name in self.features]\n\n        # List with names of selected features\n        selected_feature_names = [name for name, selected in zip(self.features, selected_features) if selected]\n\n        # Optional: adds a penalty for the amount of features used\n        n_used = len(selected_feature_names)\n        total_penalty = n_used * self.penalty\n\n        loss = 0\n\n        for split in self.splits:\n          train_idx = split[0]\n          valid_idx = split[1]\n\n          X_train = self.X.iloc[train_idx].copy()\n          y_train = self.y.iloc[train_idx].copy()\n          X_valid = self.X.iloc[valid_idx].copy()\n          y_valid = self.y.iloc[valid_idx].copy()\n\n          X_train_selected = X_train[selected_feature_names].copy()\n          X_valid_selected = X_valid[selected_feature_names].copy()\n\n          # Train model, get predictions and accumulate loss\n          self.model.fit(X_train_selected, y_train)\n          pred = self.model.predict(X_valid_selected)\n\n          loss += self.loss_fn(y_valid, pred)\n\n        # Take the average loss across all splits\n        loss /= len(self.splits)\n\n        # Add the penalty to the loss\n        loss += total_penalty\n\n        return loss\n```", "```py\nfrom optuna.samplers import TPESampler\n\ndef loss_fn(y_true, y_pred):\n  \"\"\"\n  Returns the negative F1 score, to be treated as a loss function.\n  \"\"\"\n  res = -f1_score(y_true, y_pred, average='weighted')\n  return res\n\nfeatures = list(X_train.columns)\n\nmodel = RandomForestClassifier(random_state=SEED)\n\nsampler = TPESampler(seed = SEED)\nstudy = optuna.create_study(direction=\"minimize\",sampler=sampler)\n\n# We first try the model using all features\ndefault_features = {ft: True for ft in features}\nstudy.enqueue_trial(default_features)\n\nstudy.optimize(FeatureSelectionOptuna(\n                         model=model,\n                         loss_fn=loss_fn,\n                         features=features,\n                         X=X_train,\n                         y=y_train,\n                         splits=splits,\n                         penalty = 1e-4,\n                         ), n_trials=100)\n```", "```py\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nskb = SelectKBest(score_func=chi2, k=10)\nskb.fit(X_train,y_train)\n\nscores = pd.DataFrame(skb.scores_)\ncols = pd.DataFrame(X_train.columns)\nfeatureScores = pd.concat([cols,scores],axis=1)\nfeatureScores.columns = ['feature','score']\nfeatureScores.nlargest(10, 'score')\n```", "```py\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\nmodel = RandomForestClassifier(random_state=SEED)\nsfs = SequentialFeatureSelector(model, n_features_to_select=10, cv=splits)\nsfs.fit(X_train, y_train);\n\nselected_features = list(X_train.columns[sfs.get_support()])\nprint(selected_features)\n```", "```py\nmodel = RandomForestClassifier(random_state=SEED)\nmodel.fit(X_train,y_train)\n\nimportance = pd.DataFrame({'feature':X_train.columns, 'importance':model.feature_importances_})\nimportance.nlargest(10, 'importance')\n```"]