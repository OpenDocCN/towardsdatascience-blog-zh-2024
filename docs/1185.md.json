["```py\n{\n    \"MBR-2001\": \"Traditional sleigh bed crafted in rich walnut wood, featuring a curved headboard and footboard with intricate grain details. Queen size, includes a plush, supportive mattress. Produced by Heritage Bed Co. Dimensions: 65\\\"W x 85\\\"L x 50\\\"H.\",\n    \"MBR-2002\": \"Art Deco-inspired vanity table in a polished ebony finish, featuring a tri-fold mirror and five drawers with crystal knobs. Includes a matching stool upholstered in silver velvet. Made by Luxe Interiors. Vanity dimensions: 48\\\"W x 20\\\"D x 30\\\"H, Stool dimensions: 22\\\"W x 16\\\"D x 18\\\"H.\",\n    \"MBR-2003\": \"Set of sheer linen drapes in soft ivory, offering a delicate and airy touch to bedroom windows. Each panel measures 54\\\"W x 84\\\"L. Features hidden tabs for easy hanging. Manufactured by Tranquil Home Textiles.\",\n\n    \"LVR-3001\": \"Convertible sofa bed upholstered in navy blue linen fabric, easily transitions from sofa to full-size sleeper. Perfect for guests or small living spaces. Features a sturdy wooden frame. Produced by SofaBed Solutions. Dimensions: 70\\\"W x 38\\\"D x 35\\\"H.\",\n    \"LVR-3002\": \"Ornate Persian area rug in deep red and gold, hand-knotted from silk and wool. Adds a luxurious touch to any living room. Measures 8' x 10'. Manufactured by Ancient Weaves.\",\n    \"LVR-3003\": \"Contemporary TV stand in matte black with tempered glass doors and chrome legs. Features integrated cable management and adjustable shelves. Accommodates up to 65-inch TVs. Made by Streamline Tech. Dimensions: 60\\\"W x 20\\\"D x 24\\\"H.\",\n\n    \"OPT-4001\": \"Modular outdoor sofa set in espresso brown polyethylene wicker, includes three corner pieces and two armless chairs with water-resistant cushions in cream. Configurable to fit any patio space. Produced by Outdoor Living. Corner dimensions: 32\\\"W x 32\\\"D x 28\\\"H, Armless dimensions: 28\\\"W x 32\\\"D x 28\\\"H.\",\n    \"OPT-4002\": \"Cantilever umbrella in sunflower yellow, featuring a 10-foot canopy and adjustable tilt for optimal shade. Constructed with a sturdy aluminum pole and fade-resistant fabric. Manufactured by Shade Masters. Dimensions: 120\\\"W x 120\\\"D x 96\\\"H.\",\n    \"OPT-4003\": \"Rustic fire pit table made from faux stone, includes a natural gas hookup and a matching cover. Ideal for evening gatherings on the patio. Manufactured by Warmth Outdoor. Dimensions: 42\\\"W x 42\\\"D x 24\\\"H.\",\n\n    \"ENT-5001\": \"Digital jukebox with touchscreen interface and built-in speakers, capable of streaming music and playing CDs. Retro design with modern technology, includes customizable LED lighting. Produced by RetroSound. Dimensions: 24\\\"W x 15\\\"D x 48\\\"H.\",\n    \"ENT-5002\": \"Gaming console storage unit in sleek black, featuring designated compartments for systems, controllers, and games. Ventilated to prevent overheating. Manufactured by GameHub. Dimensions: 42\\\"W x 16\\\"D x 24\\\"H.\",\n    \"ENT-5003\": \"Virtual reality gaming set by VR Innovations, includes headset, two motion controllers, and a charging station. Offers a comprehensive library of immersive games and experiences.\",\n\n    \"KIT-6001\": \"Chef's rolling kitchen cart in stainless steel, features two shelves, a drawer, and towel bars. Portable and versatile, ideal for extra storage and workspace in the kitchen. Produced by KitchenAid. Dimensions: 30\\\"W x 18\\\"D x 36\\\"H.\",\n    \"KIT-6002\": \"Contemporary pendant light cluster with three frosted glass shades, suspended from a polished nickel ceiling plate. Provides elegant, diffuse lighting over kitchen islands. Manufactured by Luminary Designs. Adjustable drop length up to 60\\\".\",\n    \"KIT-6003\": \"Eight-piece ceramic dinnerware set in ocean blue, includes dinner plates, salad plates, bowls, and mugs. Dishwasher and microwave safe, adds a pop of color to any meal. Produced by Tabletop Trends.\",\n\n    \"GBR-7001\": \"Twin-size daybed with trundle in brushed silver metal, ideal for guest rooms or small spaces. Includes two comfortable twin mattresses. Manufactured by Guestroom Gadgets. Bed dimensions: 79\\\"L x 42\\\"W x 34\\\"H.\",\n    \"GBR-7002\": \"Wall art set featuring three abstract prints in blue and grey tones, framed in light wood. Each frame measures 24\\\"W x 36\\\"H. Adds a modern touch to guest bedrooms. Produced by Artistic Expressions.\",\n    \"GBR-7003\": \"Set of two bedside lamps in brushed nickel with white fabric shades. Offers a soft, ambient light suitable for reading or relaxing in bed. Dimensions per lamp: 12\\\"W x 24\\\"H. Manufactured by Bright Nights.\",\n\n    \"BMT-8001\": \"Industrial-style pool table with a slate top and black felt, includes cues, balls, and a rack. Perfect for entertaining and game nights in finished basements. Produced by Billiard Masters. Dimensions: 96\\\"L x 52\\\"W x 32\\\"H.\",\n    \"BMT-8002\": \"Leather home theater recliner set in black, includes four connected seats with individual cup holders and storage compartments. Offers a luxurious movie-watching experience. Made by CinemaComfort. Dimensions per seat: 22\\\"W x 40\\\"D x 40\\\"H.\",\n    \"BMT-8003\": \"Adjustable height pub table set with four stools, featuring a rustic wood finish and black metal frame. Ideal for casual dining or socializing in basements. Produced by Casual Home. Table dimensions: 36\\\"W x 36\\\"D x 42\\\"H, Stool dimensions: 15\\\"W x 15\\\"D x 30\\\"H.\"\n}\n```", "```py\nGiven different \"categories\" for furniture, I want you to generate a synthetic 'SKU' and product description.\n\nGenerate 3 for each category. Be extremely granular with your details and descriptions (colors, sizes, synthetic manufacturers, etc..).\n\nEvery response should follow this format and should be only JSON:\n{<SKU>:<description>}.\n\n- master bedroom\n- living room\n- outdoor patio\n- entertainment \n- kitchen\n- guest bedroom\n- finished basement\n```", "```py\nimport re\nimport os\nimport uuid\nfrom transformers import AutoTokenizer, AutoModel\n\ndef document_chunker(directory_path,\n                     model_name,\n                     paragraph_separator='\\n\\n',\n                     chunk_size=1024,\n                     separator=' ',\n                     secondary_chunking_regex=r'\\S+?[\\.,;!?]',\n                     chunk_overlap=0):\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load tokenizer for the specified model\n    documents = {}  # Initialize dictionary to store results\n\n    # Read each file in the specified directory\n    for filename in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, filename)\n        base = os.path.basename(file_path)\n        sku = os.path.splitext(base)[0]\n        if os.path.isfile(file_path):\n            with open(file_path, 'r', encoding='utf-8') as file:\n                text = file.read()\n\n            # Generate a unique identifier for the document\n            doc_id = str(uuid.uuid4())\n\n            # Process each file using the existing chunking logic\n            paragraphs = re.split(paragraph_separator, text)\n            all_chunks = {}\n            for paragraph in paragraphs:\n                words = paragraph.split(separator)\n                current_chunk = \"\"\n                chunks = []\n\n                for word in words:\n                    new_chunk = current_chunk + (separator if current_chunk else '') + word\n                    if len(tokenizer.tokenize(new_chunk)) <= chunk_size:\n                        current_chunk = new_chunk\n                    else:\n                        if current_chunk:\n                            chunks.append(current_chunk)\n                        current_chunk = word\n\n                if current_chunk:\n                    chunks.append(current_chunk)\n\n                refined_chunks = []\n                for chunk in chunks:\n                    if len(tokenizer.tokenize(chunk)) > chunk_size:\n                        sub_chunks = re.split(secondary_chunking_regex, chunk)\n                        sub_chunk_accum = \"\"\n                        for sub_chunk in sub_chunks:\n                            if sub_chunk_accum and len(tokenizer.tokenize(sub_chunk_accum + sub_chunk + ' ')) > chunk_size:\n                                refined_chunks.append(sub_chunk_accum.strip())\n                                sub_chunk_accum = sub_chunk\n                            else:\n                                sub_chunk_accum += (sub_chunk + ' ')\n                        if sub_chunk_accum:\n                            refined_chunks.append(sub_chunk_accum.strip())\n                    else:\n                        refined_chunks.append(chunk)\n\n                final_chunks = []\n                if chunk_overlap > 0 and len(refined_chunks) > 1:\n                    for i in range(len(refined_chunks) - 1):\n                        final_chunks.append(refined_chunks[i])\n                        overlap_start = max(0, len(refined_chunks[i]) - chunk_overlap)\n                        overlap_end = min(chunk_overlap, len(refined_chunks[i+1]))\n                        overlap_chunk = refined_chunks[i][overlap_start:] + ' ' + refined_chunks[i+1][:overlap_end]\n                        final_chunks.append(overlap_chunk)\n                    final_chunks.append(refined_chunks[-1])\n                else:\n                    final_chunks = refined_chunks\n\n                # Assign a UUID for each chunk and structure it with text and metadata\n                for chunk in final_chunks:\n                    chunk_id = str(uuid.uuid4())\n                    all_chunks[chunk_id] = {\"text\": chunk, \"metadata\": {\"file_name\":sku}}  # Initialize metadata as dict\n\n            # Map the document UUID to its chunk dictionary\n            documents[doc_id] = all_chunks\n\n    return documents\n```", "```py\ndocs = document_chunker(directory_path='/Users/joesasson/Desktop/articles/rag-from-scratch/text_data',\n                        model_name='BAAI/bge-small-en-v1.5',\n                        chunk_size=256)\n\nkeys = list(docs.keys())\nprint(len(docs))\nprint(docs[keys[0]])\n\nOut -->\n105\n{'61d6318e-644b-48cd-a635-9490a1d84711': {'text': 'Gaming console storage unit in sleek black, featuring designated compartments for systems, controllers, and games. Ventilated to prevent overheating. Manufactured by GameHub. Dimensions: 42\"W x 16\"D x 24\"H.', 'metadata': {'file_name': 'ENT-5002'}}}\n```", "```py\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel_name = \"BAAI/bge-small-en-v1.5\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\ntokenizer.save_pretrained(\"model/tokenizer\")\nmodel.save_pretrained(\"model/embedding\") \n```", "```py\ndef compute_embeddings(text):\n    tokenizer = AutoTokenizer.from_pretrained(\"/model/tokenizer\") \n    model = AutoModel.from_pretrained(\"/model/embedding\")\n\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True) \n\n    # Generate the embeddings \n    with torch.no_grad():    \n        embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze()\n\n    return embeddings.tolist()\n```", "```py\ndef create_vector_store(doc_store):\n    vector_store = {}\n    for doc_id, chunks in doc_store.items():\n        doc_vectors = {}\n        for chunk_id, chunk_dict in chunks.items():\n            # Generate an embedding for each chunk of text\n            doc_vectors[chunk_id] = compute_embeddings(chunk_dict.get(\"text\"))\n        # Store the document's chunk embeddings mapped by their chunk UUIDs\n        vector_store[doc_id] = doc_vectors\n    return vector_store\n```", "```py\ndef compute_matches(vector_store, query_str, top_k):\n    \"\"\"\n    This function takes in a vector store dictionary, a query string, and an int 'top_k'.\n    It computes embeddings for the query string and then calculates the cosine similarity against every chunk embedding in the dictionary.\n    The top_k matches are returned based on the highest similarity scores.\n    \"\"\"\n    # Get the embedding for the query string\n    query_str_embedding = np.array(compute_embeddings(query_str))\n    scores = {}\n\n    # Calculate the cosine similarity between the query embedding and each chunk's embedding\n    for doc_id, chunks in vector_store.items():\n        for chunk_id, chunk_embedding in chunks.items():\n            chunk_embedding_array = np.array(chunk_embedding)\n            # Normalize embeddings to unit vectors for cosine similarity calculation\n            norm_query = np.linalg.norm(query_str_embedding)\n            norm_chunk = np.linalg.norm(chunk_embedding_array)\n            if norm_query == 0 or norm_chunk == 0:\n                # Avoid division by zero\n                score = 0\n            else:\n                score = np.dot(chunk_embedding_array, query_str_embedding) / (norm_query * norm_chunk)\n\n            # Store the score along with a reference to both the document and the chunk\n            scores[(doc_id, chunk_id)] = score\n\n    # Sort scores and return the top_k results\n    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]\n    top_results = [(doc_id, chunk_id, score) for ((doc_id, chunk_id), score) in sorted_scores]\n\n    return top_results\n```", "```py\nmatches = compute_matches(vector_store=vec_store,\n                query_str=\"Wall-mounted electric fireplace with realistic LED flames\",\n                top_k=3)\n\n# matches\n[('d56bc8ca-9bbc-4edb-9f57-d1ea2b62362f',\n  '3086bed2-65e7-46cc-8266-f9099085e981',\n  0.8600385118142513),\n ('240c67ce-b469-4e0f-86f7-d41c630cead2',\n  '49335ccf-f4fb-404c-a67a-19af027a9fc2',\n  0.7067269230771228),\n ('53faba6d-cec8-46d2-8d7f-be68c3080091',\n  'b88e4295-5eb1-497c-8536-59afd84d2210',\n  0.6959163226146977)]\n\n# plug the top match document ID keys into doc_store to access the retrieved content\ndocs['d56bc8ca-9bbc-4edb-9f57-d1ea2b62362f']['3086bed2-65e7-46cc-8266-f9099085e981']\n\n# result\n{'text': 'Wall-mounted electric fireplace with realistic LED flames and heat settings. Features a black glass frame and remote control for easy operation. Ideal for adding warmth and ambiance. Manufactured by Hearth & Home. Dimensions: 50\"W x 6\"D x 21\"H.',\n 'metadata': {'file_name': 'ENT-4001'}}\n```", "```py\nfrom llama_cpp import Llama\nimport sys\n\ndef stream_and_buffer(base_prompt, llm, max_tokens=800, stop=[\"Q:\", \"\\n\"], echo=True, stream=True):\n\n    # Formatting the base prompt\n    formatted_prompt = f\"Q: {base_prompt} A: \"\n\n    # Streaming the response from llm\n    response = llm(formatted_prompt, max_tokens=max_tokens, stop=stop, echo=echo, stream=stream)\n\n    buffer = \"\"\n\n    for message in response:\n        chunk = message['choices'][0]['text']\n        buffer += chunk\n\n        # Split at the last space to get words\n        words = buffer.split(' ')\n        for word in words[:-1]:  # Process all words except the last one (which might be incomplete)\n            sys.stdout.write(word + ' ')  # Write the word followed by a space\n            sys.stdout.flush()  # Ensure it gets displayed immediately\n\n        # Keep the rest in the buffer\n        buffer = words[-1]\n\n    # Print any remaining content in the buffer\n    if buffer:\n        sys.stdout.write(buffer)\n        sys.stdout.flush()\n\ndef construct_prompt(system_prompt, retrieved_docs, user_query):\n    prompt = f\"\"\"{system_prompt}\n\n    Here is the retrieved context:\n    {retrieved_docs}\n\n    Here is the users query:\n    {user_query}\n    \"\"\"\n    return prompt\n\n# Usage\nsystem_prompt = \"\"\"\nYou are an intelligent search engine. You will be provided with some retrieved context, as well as the users query.\n\nYour job is to understand the request, and answer based on the retrieved context.\n\"\"\"\n\nretrieved_docs = \"\"\"\nWall-mounted electric fireplace with realistic LED flames and heat settings. Features a black glass frame and remote control for easy operation. Ideal for adding warmth and ambiance. Manufactured by Hearth & Home. Dimensions: 50\"W x 6\"D x 21\"H.\n\"\"\"\n\nprompt = construct_prompt(system_prompt=system_prompt,\n                          retrieved_docs=retrieved_docs,\n                          user_query=\"I am looking for a wall-mounted electric fireplace with realistic LED flames\")\n\nllm = Llama(model_path=\"/Users/joesasson/Downloads/mistral-7b-instruct-v0.2.Q3_K_L.gguf\", n_gpu_layers=1)\n\nstream_and_buffer(prompt, llm)\n```", "```py\nfrom flask import Flask, request, jsonify\nimport numpy as np\nimport json\nfrom typing import Dict, List, Any\nfrom llama_cpp import Llama\nimport torch\nimport logging\nfrom transformers import AutoModel, AutoTokenizer\n\napp = Flask(__name__)\n\n# Set the logger level for Flask's logger\napp.logger.setLevel(logging.INFO)\n\ndef compute_embeddings(text):\n    tokenizer = AutoTokenizer.from_pretrained(\"/app/model/tokenizer\") \n    model = AutoModel.from_pretrained(\"/app/model/embedding\")\n\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True) \n\n    # Generate the embeddings \n    with torch.no_grad():    \n        embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze()\n\n    return embeddings.tolist()\n\ndef compute_matches(vector_store, query_str, top_k):\n    \"\"\"\n    This function takes in a vector store dictionary, a query string, and an int 'top_k'.\n    It computes embeddings for the query string and then calculates the cosine similarity against every chunk embedding in the dictionary.\n    The top_k matches are returned based on the highest similarity scores.\n    \"\"\"\n    # Get the embedding for the query string\n    query_str_embedding = np.array(compute_embeddings(query_str))\n    scores = {}\n\n    # Calculate the cosine similarity between the query embedding and each chunk's embedding\n    for doc_id, chunks in vector_store.items():\n        for chunk_id, chunk_embedding in chunks.items():\n            chunk_embedding_array = np.array(chunk_embedding)\n            # Normalize embeddings to unit vectors for cosine similarity calculation\n            norm_query = np.linalg.norm(query_str_embedding)\n            norm_chunk = np.linalg.norm(chunk_embedding_array)\n            if norm_query == 0 or norm_chunk == 0:\n                # Avoid division by zero\n                score = 0\n            else:\n                score = np.dot(chunk_embedding_array, query_str_embedding) / (norm_query * norm_chunk)\n\n            # Store the score along with a reference to both the document and the chunk\n            scores[(doc_id, chunk_id)] = score\n\n    # Sort scores and return the top_k results\n    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]\n    top_results = [(doc_id, chunk_id, score) for ((doc_id, chunk_id), score) in sorted_scores]\n\n    return top_results\n\ndef open_json(path):\n    with open(path, 'r') as f:\n        data = json.load(f)\n    return data\n\ndef retrieve_docs(doc_store, matches):\n    top_match = matches[0]\n    doc_id = top_match[0]\n    chunk_id = top_match[1]\n    docs = doc_store[doc_id][chunk_id]\n    return docs\n\ndef construct_prompt(system_prompt, retrieved_docs, user_query):\n    prompt = f\"\"\"{system_prompt}\n\n    Here is the retrieved context:\n    {retrieved_docs}\n\n    Here is the users query:\n    {user_query}\n    \"\"\"\n    return prompt\n\n@app.route('/rag_endpoint', methods=['GET', 'POST'])\ndef main():\n    app.logger.info('Processing HTTP request')\n\n    # Process the request\n    query_str = request.args.get('query') or (request.get_json() or {}).get('query')\n    if not query_str:\n        return jsonify({\"error\":\"missing required parameter 'query'\"})\n\n    vec_store = open_json('/app/vector_store.json')\n    doc_store = open_json('/app/doc_store.json')\n\n    matches = compute_matches(vector_store=vec_store, query_str=query_str, top_k=3)\n    retrieved_docs = retrieve_docs(doc_store, matches)\n\n    system_prompt = \"\"\"\n    You are an intelligent search engine. You will be provided with some retrieved context, as well as the users query.\n\n    Your job is to understand the request, and answer based on the retrieved context.\n    \"\"\"\n\n    base_prompt = construct_prompt(system_prompt=system_prompt, retrieved_docs=retrieved_docs, user_query=query_str)\n\n    app.logger.info(f'constructed prompt: {base_prompt}')\n\n    # Formatting the base prompt\n    formatted_prompt = f\"Q: {base_prompt} A: \"\n\n    llm = Llama(model_path=\"/app/mistral-7b-instruct-v0.2.Q3_K_L.gguf\")\n    response = llm(formatted_prompt, max_tokens=800, stop=[\"Q:\", \"\\n\"], echo=False, stream=False)\n\n    return jsonify({\"response\": response})\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5001)\n```", "```py\n# Use an official Python runtime as a parent image\nFROM --platform=linux/arm64 python:3.11\n\n# Set the working directory in the container to /app\nWORKDIR /app\n\n# Copy the requirements file\nCOPY requirements.txt .\n\n# Update system packages, install gcc and Python dependencies\nRUN apt-get update && \\\n    apt-get install -y gcc g++ make libtool && \\\n    apt-get upgrade -y && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/* && \\\n    pip install --no-cache-dir -r requirements.txt\n\n# Copy the current directory contents into the container at /app\nCOPY . /app\n\n# Expose port 5001 to the outside world\nEXPOSE 5001\n\n# Run script when the container launches\nCMD [\"python\", \"app.py\"]\n```", "```py\nimport requests, json\n\ndef call_api(query):\n    URL = \"http://127.0.0.1:5001/rag_endpoint\"\n\n    # Headers for the request\n    headers = {\n        \"Content-Type\": \"application/json\"\n    }\n\n    # Body for the request.\n    body = {\"query\": query}\n\n    # Making the POST request\n    response = requests.post(URL, headers=headers, data=json.dumps(body))\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        return response.json()\n    else:\n        return f\"Error: {response.status_code}, Message: {response.text}\"\n\n# Test\nquery = \"Wall-mounted electric fireplace with realistic LED flames\"\n\nresult = call_api(query)\nprint(result)\n\n# result\n{'response': {'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'text': ' Based on the retrieved context, the wall-mounted electric fireplace mentioned includes features such as realistic LED flames. Therefore, the answer to the user\\'s query \"Wall-mounted electric fireplace with realistic LED flames\" is a match to the retrieved context. The specific model mentioned in the context is manufactured by Hearth & Home and comes with additional heat settings.'}], 'created': 1715307125, 'id': 'cmpl-dd6c41ee-7c89-440f-9b04-0c9da9662f26', 'model': '/app/mistral-7b-instruct-v0.2.Q3_K_L.gguf', 'object': 'text_completion', 'usage': {'completion_tokens': 78, 'prompt_tokens': 177, 'total_tokens': 255}}}\n```"]