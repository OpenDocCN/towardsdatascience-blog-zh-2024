- en: Erasing Clouds from Satellite Imagery Using GANs (Generative Adversarial Networks)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ GANï¼ˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼‰å»é™¤å«æ˜Ÿå›¾åƒä¸­çš„äº‘
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/erasing-clouds-from-satellite-imagery-using-gans-generative-adversarial-networks-2d7f8467ef2e?source=collection_archive---------2-----------------------#2024-06-15](https://towardsdatascience.com/erasing-clouds-from-satellite-imagery-using-gans-generative-adversarial-networks-2d7f8467ef2e?source=collection_archive---------2-----------------------#2024-06-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/erasing-clouds-from-satellite-imagery-using-gans-generative-adversarial-networks-2d7f8467ef2e?source=collection_archive---------2-----------------------#2024-06-15](https://towardsdatascience.com/erasing-clouds-from-satellite-imagery-using-gans-generative-adversarial-networks-2d7f8467ef2e?source=collection_archive---------2-----------------------#2024-06-15)
- en: '**Building GANs from scratch in python**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ä»é›¶å¼€å§‹åœ¨ Python ä¸­æ„å»º GAN**'
- en: '[](https://medium.com/@alexroz?source=post_page---byline--2d7f8467ef2e--------------------------------)[![Aleksei
    Rozanov](../Images/748b69bfaccf39c9aa568a9e6f41eec3.png)](https://medium.com/@alexroz?source=post_page---byline--2d7f8467ef2e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2d7f8467ef2e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2d7f8467ef2e--------------------------------)
    [Aleksei Rozanov](https://medium.com/@alexroz?source=post_page---byline--2d7f8467ef2e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alexroz?source=post_page---byline--2d7f8467ef2e--------------------------------)[![Aleksei
    Rozanov](../Images/748b69bfaccf39c9aa568a9e6f41eec3.png)](https://medium.com/@alexroz?source=post_page---byline--2d7f8467ef2e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2d7f8467ef2e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2d7f8467ef2e--------------------------------)
    [Aleksei Rozanov](https://medium.com/@alexroz?source=post_page---byline--2d7f8467ef2e--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2d7f8467ef2e--------------------------------)
    Â·12 min readÂ·Jun 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2d7f8467ef2e--------------------------------)
    Â·12 åˆ†é’Ÿé˜…è¯»Â·2024 å¹´ 6 æœˆ 15 æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a8ae5b6d4e446b5af4e38c5c19ed1129.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8ae5b6d4e446b5af4e38c5c19ed1129.png)'
- en: Photo by [Michael & Diane Weidner](https://unsplash.com/@michaelbweidner?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Michael & Diane Weidner](https://unsplash.com/@michaelbweidner?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The idea of Generative Adversarial Networks, or GANs, was introduced by Goodfellow
    and his colleagues [1] in 2014, and shortly after that became extremely popular
    in the field of computer vision and image generation. Despite the last 10 years
    of rapid development within the domain of AI and growth of the number of new algorithms,
    the simplicity and brilliance of this concept are still extremely impressive.
    So today I want to illustrate how powerful these networks can be by attempting
    to remove clouds from satellite RGB (Red, Green, Blue) images.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ¦‚å¿µç”± Goodfellow å’Œä»–çš„åŒäº‹ä»¬åœ¨ 2014 å¹´æå‡º [1]ï¼Œå¹¶ä¸”å¾ˆå¿«åœ¨è®¡ç®—æœºè§†è§‰å’Œå›¾åƒç”Ÿæˆé¢†åŸŸè·å¾—äº†æå¤§å…³æ³¨ã€‚å°½ç®¡åœ¨è¿‡å»çš„
    10 å¹´é‡Œï¼Œäººå·¥æ™ºèƒ½é¢†åŸŸç»å†äº†å¿«é€Ÿå‘å±•å¹¶å‡ºç°äº†è®¸å¤šæ–°ç®—æ³•ï¼Œä½†è¿™ä¸ªæ¦‚å¿µçš„ç®€å•æ€§å’Œ brillianceï¼ˆå·§å¦™ä¹‹å¤„ï¼‰ä¾ç„¶ä»¤äººå°è±¡æ·±åˆ»ã€‚æ‰€ä»¥ä»Šå¤©æˆ‘æƒ³é€šè¿‡å°è¯•ä»å«æ˜Ÿ
    RGBï¼ˆçº¢ã€ç»¿ã€è“ï¼‰å›¾åƒä¸­å»é™¤äº‘å±‚ï¼Œæ¥å±•ç¤ºè¿™äº›ç½‘ç»œçš„å¼ºå¤§èƒ½åŠ›ã€‚
- en: Preparation of a properly balanced, big enough and correctly pre-processed CV
    dataset takes a solid amount of time, so I decided to explore what Kaggle has
    to offer. The dataset I found the most appropriate for this task is EuroSat [2],
    which has an open license. It comprises **27000** labeled RGB images 64x64 pixels
    from [Sentinel-2](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2)
    and is built for solving the multiclass classification problem.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†å¤‡ä¸€ä¸ªé€‚å½“å¹³è¡¡ã€è¶³å¤Ÿå¤§ä¸”ç»è¿‡æ­£ç¡®é¢„å¤„ç†çš„è®¡ç®—æœºè§†è§‰æ•°æ®é›†éœ€è¦å¤§é‡æ—¶é—´ï¼Œå› æ­¤æˆ‘å†³å®šæ¢ç´¢ Kaggle æä¾›çš„èµ„æºã€‚æˆ‘å‘ç°æœ€é€‚åˆè¿™ä¸ªä»»åŠ¡çš„æ•°æ®é›†æ˜¯ EuroSat
    [2]ï¼Œå®ƒå…·æœ‰å¼€æ”¾è®¸å¯ã€‚è¯¥æ•°æ®é›†åŒ…å« **27000** å¼ æ ‡æ³¨çš„ RGB å›¾åƒï¼Œå°ºå¯¸ä¸º 64x64 åƒç´ ï¼Œæ¥è‡ª [Sentinel-2](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2)ï¼Œå¹¶ç”¨äºè§£å†³å¤šç±»åˆ†ç±»é—®é¢˜ã€‚
- en: '[](https://www.kaggle.com/datasets/apollo2506/eurosat-dataset/data?source=post_page-----2d7f8467ef2e--------------------------------)
    [## EuroSat Dataset'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.kaggle.com/datasets/apollo2506/eurosat-dataset/data?source=post_page-----2d7f8467ef2e--------------------------------)
    [## EuroSat æ•°æ®é›†'
- en: Dataset contains all the RGB and Bands images from Sentinel-2
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®é›†åŒ…å«æ¥è‡ª Sentinel-2 çš„æ‰€æœ‰ RGB å’Œæ³¢æ®µå›¾åƒ
- en: www.kaggle.com](https://www.kaggle.com/datasets/apollo2506/eurosat-dataset/data?source=post_page-----2d7f8467ef2e--------------------------------)
    ![](../Images/78b9510aa3c9c7e3a95911929f732e88.png)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.kaggle.com](https://www.kaggle.com/datasets/apollo2506/eurosat-dataset/data?source=post_page-----2d7f8467ef2e--------------------------------)
    ![](../Images/78b9510aa3c9c7e3a95911929f732e88.png)'
- en: EuroSat dataset imagery example. [License](https://github.com/phelber/eurosat).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: EuroSatæ•°æ®é›†çš„å›¾åƒç¤ºä¾‹ã€‚[è®¸å¯](https://github.com/phelber/eurosat)ã€‚
- en: We are not interested in classification itself, but one of the main features
    of the EuroSat dataset is that all its images have a clear sky. Thatâ€˜s exactly
    what we need. Adopting this approach from [3], we will use these Sentinel-2 shots
    as targets and create inputs by adding noise (clouds) to them.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹åˆ†ç±»æœ¬èº«å¹¶ä¸æ„Ÿå…´è¶£ï¼Œä½†EuroSatæ•°æ®é›†çš„ä¸€ä¸ªä¸»è¦ç‰¹ç‚¹æ˜¯ï¼Œæ‰€æœ‰å›¾åƒéƒ½æœ‰æ¸…æ™°çš„å¤©ç©ºã€‚è¿™æ­£æ˜¯æˆ‘ä»¬éœ€è¦çš„ã€‚å€Ÿç”¨[3]ä¸­çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å°†è¿™äº›Sentinel-2å½±åƒä½œä¸ºç›®æ ‡ï¼Œé€šè¿‡å‘å®ƒä»¬æ·»åŠ å™ªå£°ï¼ˆäº‘æœµï¼‰æ¥åˆ›å»ºè¾“å…¥ã€‚
- en: So letâ€™s prepare our data before actually talking about GANs. Firstly, we need
    to download the data and merge all the classes into one directory.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆåœ¨çœŸæ­£è®¨è®ºGANsä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆå‡†å¤‡ä¸€ä¸‹æ•°æ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½æ•°æ®ï¼Œå¹¶å°†æ‰€æœ‰ç±»åˆ«åˆå¹¶åˆ°ä¸€ä¸ªç›®å½•ä¸­ã€‚
- en: '**ğŸThe full python code:** [**GitHub**](https://github.com/alexxxroz/Medium/blob/main/GANs%26Clouds.ipynb)**.**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸå®Œæ•´çš„Pythonä»£ç ï¼š** [**GitHub**](https://github.com/alexxxroz/Medium/blob/main/GANs%26Clouds.ipynb)**.**'
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The second important step is generating noise. Whereas you can use different
    approaches, e.g. randomly masking out some pixels, adding some Gaussian noise,
    in this article I want to try a new thing for me â€” Perlin noise. It was invented
    in the 80s by Ken Perlin [4] when developing cinematic smoke effects. This kind
    of noise has a more organic appearance compared to regular random noise. Just
    let me prove it.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªé‡è¦æ­¥éª¤æ˜¯ç”Ÿæˆå™ªå£°ã€‚è™½ç„¶ä½ å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ–¹æ³•ï¼Œä¾‹å¦‚éšæœºé®ç½©ä¸€äº›åƒç´ ã€æ·»åŠ ä¸€äº›é«˜æ–¯å™ªå£°ï¼Œä½†åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æƒ³å°è¯•ä¸€ä¸ªå¯¹æˆ‘æ¥è¯´æ–°é¢–çš„ä¸œè¥¿â€”â€”Perlinå™ªå£°ã€‚å®ƒæ˜¯ç”±Ken
    Perlinåœ¨80å¹´ä»£å‘æ˜çš„[4]ï¼Œç”¨äºå¼€å‘ç”µå½±ä¸­çš„çƒŸé›¾æ•ˆæœã€‚è¿™ç§å™ªå£°ä¸æ™®é€šçš„éšæœºå™ªå£°ç›¸æ¯”ï¼Œå…·æœ‰æ›´è‡ªç„¶çš„å¤–è§‚ã€‚è®©æˆ‘æ¥è¯æ˜ä¸€ä¸‹ã€‚
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/663c3610117fdf1d41c60edb0e5a06e6.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/663c3610117fdf1d41c60edb0e5a06e6.png)'
- en: Image by [author](https://medium.com/@alexroz).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[ä½œè€…](https://medium.com/@alexroz)ã€‚
- en: As you can see above, the clouds on the images are very realistic, they have
    different â€œdensityâ€ and texture resembling the real ones.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šæ‰€ç¤ºï¼Œå›¾åƒä¸­çš„äº‘æœµéå¸¸é€¼çœŸï¼Œå®ƒä»¬å…·æœ‰ä¸åŒçš„â€œå¯†åº¦â€å’Œç±»ä¼¼çœŸå®äº‘æœµçš„çº¹ç†ã€‚
- en: 'If you are intrigued by Perlin noise as I was, here is a really cool video
    on how this noise can be applied in the GameDev industry:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åƒæˆ‘ä¸€æ ·å¯¹Perlinå™ªå£°æ„Ÿå…´è¶£ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªéå¸¸é…·çš„è§†é¢‘ï¼Œå±•ç¤ºäº†è¿™ç§å™ªå£°å¦‚ä½•åº”ç”¨äºæ¸¸æˆå¼€å‘è¡Œä¸šï¼š
- en: Since now we have a ready-to-use dataset, letâ€™s talk about GANs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¢ç„¶æˆ‘ä»¬ç°åœ¨æœ‰äº†ä¸€ä¸ªç°æˆå¯ç”¨çš„æ•°æ®é›†ï¼Œé‚£ä¹ˆè®©æˆ‘ä»¬æ¥è°ˆè°ˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚
- en: Generative Adversarial Network
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰
- en: 'To better illustrate this idea, letâ€™s imagine that youâ€™re traveling around
    South-East Asia and find yourself in an urgent need of a hoodie, since itâ€™s too
    cold outside. Coming to the closest street market, you find a small shop with
    some branded clothes. The seller brings you a nice hoodie to try on saying that
    itâ€™s the famous brand ExpensiveButNotWorthIt. You take a closer look and conclude
    that itâ€™s obviously a fake. The seller says: â€˜Wait a sec, I have the REAL one.
    He returns with another hoodie, which looks more like the branded one, but still
    a fake. After several iterations like this, the seller brings an indistinguishable
    copy of the legendary ExpensiveButNotWorthIt and you readily buy it. Thatâ€™s basically
    how the GANs work!'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°è¯´æ˜è¿™ä¸ªæ¦‚å¿µï¼Œå‡è®¾ä½ æ­£åœ¨ä¸œå—äºšæ—…è¡Œï¼Œçªç„¶éœ€è¦ä¸€ä»¶è¿å¸½è¡«ï¼Œå› ä¸ºå¤–é¢å¤ªå†·äº†ã€‚ä½ æ¥åˆ°æœ€è¿‘çš„è¡—å¤´å¸‚åœºï¼Œå‘ç°ä¸€å®¶å°åº—æœ‰ä¸€äº›å“ç‰Œæœè£…ã€‚å–å®¶æ‹¿æ¥ä¸€ä»¶ä¸é”™çš„è¿å¸½è¡«è®©ä½ è¯•ç©¿ï¼Œå¹¶è¯´å®ƒæ˜¯è‘—åå“ç‰ŒExpensiveButNotWorthItã€‚ä½ ä»”ç»†ä¸€çœ‹ï¼Œå¾—å‡ºç»“è®ºï¼Œè¿™æ˜¾ç„¶æ˜¯å‡çš„ã€‚å–å®¶è¯´ï¼šâ€œç­‰ä¸€ä¸‹ï¼Œæˆ‘æœ‰çœŸçš„ã€‚â€ç„¶åä»–å¸¦ç€å¦ä¸€ä»¶è¿å¸½è¡«å›æ¥ï¼Œçœ‹èµ·æ¥æ›´åƒæ˜¯å“ç‰Œçš„ï¼Œä½†ä¾æ—§æ˜¯å‡è´§ã€‚ç»è¿‡å‡ è½®è¿™æ ·çš„å°è¯•åï¼Œå–å®¶å¸¦æ¥äº†ä¸€ä»¶æ— æ³•åˆ†è¾¨çš„ä¼ å¥‡å“ç‰ŒExpensiveButNotWorthItçš„å¤åˆ¶å“ï¼Œä½ ä¾¿é«˜å…´åœ°ä¹°ä¸‹äº†å®ƒã€‚è¿™åŸºæœ¬ä¸Šå°±æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„å·¥ä½œåŸç†ï¼
- en: In the case of GANs, you are called a discriminator (D). The goal of a discriminator
    is to distinguish between a true object and a fake one, or to solve the binary
    classification task. The seller is called a generator (G), since heâ€™s trying to
    generate a high-quality fake. The discriminator and generator are trained independently
    to outperform each other. Hence, in the end we get a high-quality fake.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨GANsçš„æƒ…å†µä¸‹ï¼Œä½ è¢«ç§°ä¸ºåˆ¤åˆ«å™¨ï¼ˆDï¼‰ã€‚åˆ¤åˆ«å™¨çš„ç›®æ ‡æ˜¯åŒºåˆ†çœŸå®ç‰©ä½“å’Œè™šå‡ç‰©ä½“ï¼Œæˆ–è€…è§£å†³äºŒåˆ†ç±»ä»»åŠ¡ã€‚å–å®¶è¢«ç§°ä¸ºç”Ÿæˆå™¨ï¼ˆGï¼‰ï¼Œå› ä¸ºä»–åœ¨å°è¯•ç”Ÿæˆé«˜è´¨é‡çš„å‡è´§ã€‚åˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨æ˜¯ç‹¬ç«‹è®­ç»ƒçš„ï¼Œç›®çš„æ˜¯äº’ç›¸è¶…è¶Šã€‚å› æ­¤ï¼Œæœ€ç»ˆæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªé«˜è´¨é‡çš„å‡è´§ã€‚
- en: '![](../Images/dd222e2e675a330b6217b13ea4acc8d4.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd222e2e675a330b6217b13ea4acc8d4.png)'
- en: GANs architecture. [License](https://paperswithcode.com/method/gan).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GANsæ¶æ„ã€‚[è®¸å¯](https://paperswithcode.com/method/gan)ã€‚
- en: 'The training process originally looks like this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¿‡ç¨‹æœ€åˆçœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ï¼š
- en: Sample input noise (in our case images with clouds).
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡‡æ ·è¾“å…¥å™ªå£°ï¼ˆåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­æ˜¯æœ‰äº‘çš„å›¾åƒï¼‰ã€‚
- en: Feed the noise to G and collect the prediction.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†å™ªå£°è¾“å…¥ G å¹¶æ”¶é›†é¢„æµ‹ç»“æœã€‚
- en: Calculate the D loss by getting 2 predictions one for Gâ€™s output and another
    for the real data.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡è·å–ä¸¤ä¸ªé¢„æµ‹å€¼æ¥è®¡ç®— D çš„æŸå¤±ï¼Œä¸€ä¸ªæ˜¯ G çš„è¾“å‡ºï¼Œå¦ä¸€ä¸ªæ˜¯çœŸå®æ•°æ®çš„é¢„æµ‹å€¼ã€‚
- en: Update Dâ€™s weights.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ›´æ–° D çš„æƒé‡ã€‚
- en: Sample input noise again.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å†æ¬¡é‡‡æ ·è¾“å…¥å™ªå£°ã€‚
- en: Feed the noise to G and collect the prediction.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†å™ªå£°è¾“å…¥ G å¹¶æ”¶é›†é¢„æµ‹ç»“æœã€‚
- en: Calculate the G loss by feeding its prediction to D.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡å°† G çš„é¢„æµ‹è¾“å…¥åˆ° D ä¸­æ¥è®¡ç®— G çš„æŸå¤±ã€‚
- en: Update Gâ€™s weights.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ›´æ–° G çš„æƒé‡ã€‚
- en: '![](../Images/788502677c2e805ddd67f00aa1010601.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/788502677c2e805ddd67f00aa1010601.png)'
- en: 'GANs training loop. Source: [1].'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GANs è®­ç»ƒå¾ªç¯ã€‚æ¥æºï¼š[1]ã€‚
- en: 'In other words we can define a value function V(G,D):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªå€¼å‡½æ•° V(G,D)ï¼š
- en: '![](../Images/ba0147e691508a1cee54eda81aec27ca.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba0147e691508a1cee54eda81aec27ca.png)'
- en: 'Source: [1].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[1]ã€‚
- en: where we want to minimize the term **log(1-D(G(z)))** to train G and maximize
    **log D(x)** to train D (in this notation x â€” real data sample and z â€” noise).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¸Œæœ›æœ€å°åŒ–é¡¹ **log(1-D(G(z)))** æ¥è®­ç»ƒ Gï¼Œå¹¶æœ€å¤§åŒ– **log D(x)** æ¥è®­ç»ƒ Dï¼ˆåœ¨æ­¤ç¬¦å·ä¸­ï¼Œx è¡¨ç¤ºçœŸå®æ•°æ®æ ·æœ¬ï¼Œz
    è¡¨ç¤ºå™ªå£°ï¼‰ã€‚
- en: Now letâ€™s try to implement it in pytorch!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å°è¯•åœ¨ pytorch ä¸­å®ç°å®ƒï¼
- en: In the original paper authors talk about using Multilayer Perceptron (MLP);
    itâ€™s also often referred simply as ANN, but I want to try a little bit more complicated
    approach â€” I want to use the UNet [5] architecture as a Generator and ResNet [6]
    as a Discriminator. These are both well-known CNN architectures, so I wonâ€™t be
    explaining them here (let me know if I should write a separate article in the
    comments).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸå§‹è®ºæ–‡ä¸­ï¼Œä½œè€…æåˆ°ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ï¼›å®ƒé€šå¸¸ä¹Ÿç®€ç§°ä¸º ANNï¼Œä½†æˆ‘æƒ³å°è¯•ä¸€ä¸ªæ›´å¤æ‚çš„æ–¹æ³•â€”â€”æˆ‘æƒ³ä½¿ç”¨ UNet [5] æ¶æ„ä½œä¸ºç”Ÿæˆå™¨ï¼ŒResNet
    [6] ä½œä¸ºåˆ¤åˆ«å™¨ã€‚è¿™ä¸¤è€…éƒ½æ˜¯è‘—åçš„ CNN æ¶æ„ï¼Œæ‰€ä»¥æˆ‘ä¸ä¼šåœ¨è¿™é‡Œè§£é‡Šå®ƒä»¬ï¼ˆå¦‚æœéœ€è¦æˆ‘å†™ä¸€ç¯‡å•ç‹¬çš„æ–‡ç« ï¼Œè¯·åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘ï¼‰ã€‚
- en: 'Letâ€™s build them. Discriminator:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥æ„å»ºå®ƒä»¬ã€‚åˆ¤åˆ«å™¨ï¼š
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Generator:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå™¨ï¼š
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we need to split our data into train/test and wrap them into a torch dataset:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶å°†å®ƒä»¬å°è£…ä¸º torch æ•°æ®é›†ï¼š
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Perfect. Itâ€™s time to write the training loop. Before doing so, letâ€™s define
    our loss functions and optimizer:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œç¾ã€‚æ˜¯æ—¶å€™ç¼–å†™è®­ç»ƒå¾ªç¯äº†ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼š
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As you can see, these losses are different from the picture with the GAN algorithm.
    In particular, I added L1Loss. The idea is that we are not simply generating a
    random image from noise, we want to keep most of the information from the input
    and just remove noise. So G loss will be:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œè¿™äº›æŸå¤±ä¸ GAN ç®—æ³•ä¸­çš„å›¾ç‰‡æœ‰æ‰€ä¸åŒã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘åŠ å…¥äº† L1Lossã€‚å…¶æƒ³æ³•æ˜¯ï¼Œæˆ‘ä»¬ä¸ä»…ä»…æ˜¯ä»å™ªå£°ä¸­ç”Ÿæˆä¸€å¼ éšæœºå›¾ç‰‡ï¼Œæˆ‘ä»¬è¿˜å¸Œæœ›ä¿ç•™è¾“å…¥ä¸­çš„å¤§éƒ¨åˆ†ä¿¡æ¯ï¼Œåªå»é™¤å™ªå£°ã€‚æ‰€ä»¥
    G çš„æŸå¤±å°†æ˜¯ï¼š
- en: '**G_loss = log(1 âˆ’ D(G(z))) + ğ€ |G(z)-y|**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**G_loss = log(1 âˆ’ D(G(z))) + ğ€ |G(z)-y|**'
- en: instead of just
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸æ˜¯ä»…ä»…
- en: '**G_loss = log(1 âˆ’ D(G(z)))**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**G_loss = log(1 âˆ’ D(G(z)))**'
- en: ğ€ is an arbitrary coefficient, which balances two components of the losses.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ğ€ æ˜¯ä¸€ä¸ªä»»æ„ç³»æ•°ï¼Œç”¨äºå¹³è¡¡æŸå¤±å‡½æ•°çš„ä¸¤ä¸ªéƒ¨åˆ†ã€‚
- en: 'Finally, letâ€™s split the data to start the training process:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè®©æˆ‘ä»¬åˆ’åˆ†æ•°æ®å¹¶å¼€å§‹è®­ç»ƒè¿‡ç¨‹ï¼š
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we can run our training loop:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿è¡Œæˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ï¼š
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After the code is finished we can plot the losses. This code was partly adopted
    from [this cool website](https://python-graph-gallery.com/web-small-multiple-with-highlights/):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç å®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶æŸå¤±å›¾ã€‚æ­¤ä»£ç éƒ¨åˆ†æ¥æºäº[è¿™ä¸ªé…·ç½‘ç«™](https://python-graph-gallery.com/web-small-multiple-with-highlights/)ï¼š
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/413ed7942d93aa8dfd0134cb587b28e8.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/413ed7942d93aa8dfd0134cb587b28e8.png)'
- en: Image by [author](https://medium.com/@alexroz).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[ä½œè€…](https://medium.com/@alexroz)ã€‚
- en: 'And also visualize a random sample from the test dataset:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ—¶ä¹Ÿå¯ä»¥å¯è§†åŒ–æ¥è‡ªæµ‹è¯•æ•°æ®é›†çš„éšæœºæ ·æœ¬ï¼š
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/a16b1e301ecc94fe2928a33cc3216350.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a16b1e301ecc94fe2928a33cc3216350.png)'
- en: Image by [author](https://medium.com/@alexroz).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[ä½œè€…](https://medium.com/@alexroz)ã€‚
- en: As you can see, the results are not perfect and depend a lot on the land cover
    type. Nevertheless, the built model certainly removes the clouds from images and
    its performance can be improved by increasing G and D depth. Another promising
    strategy to test is training separate models for different land cover types. For
    instance, crop fields and water basins are definitely have quite distinct spatial
    features, so it might effect modelâ€™s ability to generalize.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œç»“æœå¹¶ä¸å®Œç¾ï¼Œå¹¶ä¸”å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºåœ°è²Œç±»å‹ã€‚ç„¶è€Œï¼Œæ„å»ºçš„æ¨¡å‹è‚¯å®šèƒ½å¤Ÿå»é™¤å›¾åƒä¸­çš„äº‘å±‚ï¼Œå¹¶ä¸”é€šè¿‡å¢åŠ  G å’Œ D çš„æ·±åº¦å¯ä»¥æé«˜å…¶æ€§èƒ½ã€‚å¦ä¸€ä¸ªæœ‰å‰æ™¯çš„ç­–ç•¥æ˜¯ä¸ºä¸åŒçš„åœ°è²Œç±»å‹è®­ç»ƒç‹¬ç«‹çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå†œç”°å’Œæ°´åŸŸçš„ç©ºé—´ç‰¹å¾å·®å¼‚è¾ƒå¤§ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
- en: I hope this article provided you with a fresh perspective on applying Deep Learning
    algorithms in the geospatial domain. In my opinion, GANs are among the most powerful
    tools a data scientist can utilize, and I hope they become an essential part of
    your toolkit as well!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½ä¸ºä½ æä¾›ä¸€ç§åœ¨åœ°ç†ç©ºé—´é¢†åŸŸåº”ç”¨æ·±åº¦å­¦ä¹ ç®—æ³•çš„æ–°è§†è§’ã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æ˜¯æ•°æ®ç§‘å­¦å®¶å¯ä»¥åˆ©ç”¨çš„æœ€å¼ºå¤§å·¥å…·ä¹‹ä¸€ï¼Œæˆ‘å¸Œæœ›å®ƒä»¬ä¹Ÿèƒ½æˆä¸ºä½ å·¥å…·ç®±ä¸­çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼
- en: ===========================================
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ===========================================
- en: '***References:***'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '***å‚è€ƒæ–‡çŒ®ï¼š***'
- en: 1\. Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio. â€œGenerative adversarial nets.â€
    *Advances in neural information processing systems* 27 (2014). [https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville å’Œ Yoshua Bengioã€‚â€œç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚â€ *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•* 27ï¼ˆ2014å¹´ï¼‰ã€‚[https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)
- en: '2\. Helber, Patrick, Benjamin Bischke, Andreas Dengel, and Damian Borth. â€œEurosat:
    A novel dataset and deep learning benchmark for land use and land cover classification.â€
    *IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing*
    12, no. 7 (2019): 2217â€“2226\. [https://arxiv.org/pdf/1709.00029](https://arxiv.org/pdf/1709.00029)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. Helber, Patrick, Benjamin Bischke, Andreas Dengel å’Œ Damian Borthã€‚â€œEurosatï¼šä¸€ä¸ªç”¨äºåœŸåœ°åˆ©ç”¨å’ŒåœŸåœ°è¦†ç›–åˆ†ç±»çš„å…¨æ–°æ•°æ®é›†å’Œæ·±åº¦å­¦ä¹ åŸºå‡†ã€‚â€
    *IEEEåº”ç”¨åœ°çƒè§‚æµ‹ä¸é¥æ„Ÿç²¾é€‰ä¸»é¢˜æœŸåˆŠ* 12å·ï¼Œç¬¬7æœŸï¼ˆ2019å¹´ï¼‰ï¼š2217â€“2226ã€‚[https://arxiv.org/pdf/1709.00029](https://arxiv.org/pdf/1709.00029)
- en: '3\. Wen, Xue, Zongxu Pan, Yuxin Hu, and Jiayin Liu. â€œGenerative adversarial
    learning in YUV color space for thin cloud removal on satellite imagery.â€ *Remote
    Sensing* 13, no. 6 (2021): 1079\. [https://www.mdpi.com/2072-4292/13/6/1079](https://www.mdpi.com/2072-4292/13/6/1079)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. Wen, Xue, Zongxu Pan, Yuxin Hu å’Œ Jiayin Liuã€‚â€œåŸºäºYUVé¢œè‰²ç©ºé—´çš„ç”Ÿæˆå¯¹æŠ—å­¦ä¹ ç”¨äºå«æ˜Ÿå›¾åƒä¸­çš„è–„äº‘å»é™¤ã€‚â€
    *é¥æ„Ÿ* 13å·ï¼Œç¬¬6æœŸï¼ˆ2021å¹´ï¼‰ï¼š1079ã€‚[https://www.mdpi.com/2072-4292/13/6/1079](https://www.mdpi.com/2072-4292/13/6/1079)
- en: '4\. Perlin, Ken. â€œAn image synthesizer.â€ *ACM Siggraph Computer Graphics* 19,
    no. 3 (1985): 287â€“296\. [https://dl.acm.org/doi/pdf/10.1145/325165.325247](https://dl.acm.org/doi/pdf/10.1145/325165.325247)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. Perlin, Kenã€‚â€œå›¾åƒåˆæˆå™¨ã€‚â€ *ACM Siggraphè®¡ç®—æœºå›¾å½¢å­¦* 19å·ï¼Œç¬¬3æœŸï¼ˆ1985å¹´ï¼‰ï¼š287â€“296ã€‚[https://dl.acm.org/doi/pdf/10.1145/325165.325247](https://dl.acm.org/doi/pdf/10.1145/325165.325247)
- en: '5\. Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. â€œU-net: Convolutional
    networks for biomedical image segmentation.â€ In *Medical image computing and computer-assisted
    interventionâ€“MICCAI 2015: 18th international conference, Munich, Germany, October
    5â€“9, 2015, proceedings, part III 18*, pp. 234â€“241\. Springer International Publishing,
    2015\. [https://arxiv.org/pdf/1505.04597](https://arxiv.org/pdf/1505.04597)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. Ronneberger, Olaf, Philipp Fischer å’Œ Thomas Broxã€‚â€œU-netï¼šç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²çš„å·ç§¯ç½‘ç»œã€‚â€
    è§ *åŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„â€“MICCAI 2015ï¼šç¬¬18å±Šå›½é™…ä¼šè®®ï¼Œå¾·å›½æ…•å°¼é»‘ï¼Œ2015å¹´10æœˆ5æ—¥è‡³9æ—¥ï¼Œä¼šè®®å½•ï¼Œç¬¬ä¸‰éƒ¨åˆ† 18*ï¼Œç¬¬234â€“241é¡µã€‚æ–½æ™®æ—æ ¼å›½é™…å‡ºç‰ˆå…¬å¸ï¼Œ2015å¹´ã€‚[https://arxiv.org/pdf/1505.04597](https://arxiv.org/pdf/1505.04597)
- en: 6\. He, Kaiming, et al. â€œDeep residual learning for image recognition.â€ *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 2016.[https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. He, Kaiming ç­‰äººã€‚â€œæ·±åº¦æ®‹å·®å­¦ä¹ ç”¨äºå›¾åƒè¯†åˆ«ã€‚â€ *IEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†*ã€‚2016ã€‚[https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)
- en: ===========================================
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ===========================================
- en: '***All my publications on Medium are free and open-access, thatâ€™s why Iâ€™d really
    appreciate if you followed me here!***'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '***æˆ‘åœ¨Mediumä¸Šçš„æ‰€æœ‰å‡ºç‰ˆç‰©éƒ½æ˜¯å…è´¹çš„å¹¶ä¸”å¼€æ”¾è®¿é—®çš„ï¼Œå› æ­¤å¦‚æœä½ åœ¨è¿™é‡Œå…³æ³¨æˆ‘ï¼Œæˆ‘å°†éå¸¸æ„Ÿæ¿€ï¼***'
- en: P.s. Iâ€™m extremely passionate about (Geo)Data Science, ML/AI and Climate Change.
    So if you want to work together on some project pls contact me in [LinkedIn](https://www.linkedin.com/in/alexxxroz/).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: P.s. æˆ‘å¯¹ï¼ˆåœ°ç†ï¼‰æ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ /äººå·¥æ™ºèƒ½å’Œæ°”å€™å˜åŒ–å……æ»¡çƒ­æƒ…ã€‚å¦‚æœä½ æƒ³åˆä½œè¿›è¡ŒæŸäº›é¡¹ç›®ï¼Œè¯·åœ¨[LinkedIn](https://www.linkedin.com/in/alexxxroz/)ä¸Šè”ç³»æˆ‘ã€‚
- en: ğŸ›°ï¸Follow for moreğŸ›°ï¸
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ›°ï¸å…³æ³¨ä»¥è·å–æ›´å¤šä¿¡æ¯ğŸ›°ï¸
