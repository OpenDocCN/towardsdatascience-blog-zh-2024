- en: Erasing Clouds from Satellite Imagery Using GANs (Generative Adversarial Networks)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/erasing-clouds-from-satellite-imagery-using-gans-generative-adversarial-networks-2d7f8467ef2e?source=collection_archive---------2-----------------------#2024-06-15](https://towardsdatascience.com/erasing-clouds-from-satellite-imagery-using-gans-generative-adversarial-networks-2d7f8467ef2e?source=collection_archive---------2-----------------------#2024-06-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Building GANs from scratch in python**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alexroz?source=post_page---byline--2d7f8467ef2e--------------------------------)[![Aleksei
    Rozanov](../Images/748b69bfaccf39c9aa568a9e6f41eec3.png)](https://medium.com/@alexroz?source=post_page---byline--2d7f8467ef2e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2d7f8467ef2e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2d7f8467ef2e--------------------------------)
    [Aleksei Rozanov](https://medium.com/@alexroz?source=post_page---byline--2d7f8467ef2e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2d7f8467ef2e--------------------------------)
    ¬∑12 min read¬∑Jun 15, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8ae5b6d4e446b5af4e38c5c19ed1129.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Michael & Diane Weidner](https://unsplash.com/@michaelbweidner?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The idea of Generative Adversarial Networks, or GANs, was introduced by Goodfellow
    and his colleagues [1] in 2014, and shortly after that became extremely popular
    in the field of computer vision and image generation. Despite the last 10 years
    of rapid development within the domain of AI and growth of the number of new algorithms,
    the simplicity and brilliance of this concept are still extremely impressive.
    So today I want to illustrate how powerful these networks can be by attempting
    to remove clouds from satellite RGB (Red, Green, Blue) images.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation of a properly balanced, big enough and correctly pre-processed CV
    dataset takes a solid amount of time, so I decided to explore what Kaggle has
    to offer. The dataset I found the most appropriate for this task is EuroSat [2],
    which has an open license. It comprises **27000** labeled RGB images 64x64 pixels
    from [Sentinel-2](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2)
    and is built for solving the multiclass classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.kaggle.com/datasets/apollo2506/eurosat-dataset/data?source=post_page-----2d7f8467ef2e--------------------------------)
    [## EuroSat Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset contains all the RGB and Bands images from Sentinel-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.kaggle.com](https://www.kaggle.com/datasets/apollo2506/eurosat-dataset/data?source=post_page-----2d7f8467ef2e--------------------------------)
    ![](../Images/78b9510aa3c9c7e3a95911929f732e88.png)
  prefs: []
  type: TYPE_NORMAL
- en: EuroSat dataset imagery example. [License](https://github.com/phelber/eurosat).
  prefs: []
  type: TYPE_NORMAL
- en: We are not interested in classification itself, but one of the main features
    of the EuroSat dataset is that all its images have a clear sky. That‚Äòs exactly
    what we need. Adopting this approach from [3], we will use these Sentinel-2 shots
    as targets and create inputs by adding noise (clouds) to them.
  prefs: []
  type: TYPE_NORMAL
- en: So let‚Äôs prepare our data before actually talking about GANs. Firstly, we need
    to download the data and merge all the classes into one directory.
  prefs: []
  type: TYPE_NORMAL
- en: '**üêçThe full python code:** [**GitHub**](https://github.com/alexxxroz/Medium/blob/main/GANs%26Clouds.ipynb)**.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The second important step is generating noise. Whereas you can use different
    approaches, e.g. randomly masking out some pixels, adding some Gaussian noise,
    in this article I want to try a new thing for me ‚Äî Perlin noise. It was invented
    in the 80s by Ken Perlin [4] when developing cinematic smoke effects. This kind
    of noise has a more organic appearance compared to regular random noise. Just
    let me prove it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/663c3610117fdf1d41c60edb0e5a06e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [author](https://medium.com/@alexroz).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see above, the clouds on the images are very realistic, they have
    different ‚Äúdensity‚Äù and texture resembling the real ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are intrigued by Perlin noise as I was, here is a really cool video
    on how this noise can be applied in the GameDev industry:'
  prefs: []
  type: TYPE_NORMAL
- en: Since now we have a ready-to-use dataset, let‚Äôs talk about GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To better illustrate this idea, let‚Äôs imagine that you‚Äôre traveling around
    South-East Asia and find yourself in an urgent need of a hoodie, since it‚Äôs too
    cold outside. Coming to the closest street market, you find a small shop with
    some branded clothes. The seller brings you a nice hoodie to try on saying that
    it‚Äôs the famous brand ExpensiveButNotWorthIt. You take a closer look and conclude
    that it‚Äôs obviously a fake. The seller says: ‚ÄòWait a sec, I have the REAL one.
    He returns with another hoodie, which looks more like the branded one, but still
    a fake. After several iterations like this, the seller brings an indistinguishable
    copy of the legendary ExpensiveButNotWorthIt and you readily buy it. That‚Äôs basically
    how the GANs work!'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of GANs, you are called a discriminator (D). The goal of a discriminator
    is to distinguish between a true object and a fake one, or to solve the binary
    classification task. The seller is called a generator (G), since he‚Äôs trying to
    generate a high-quality fake. The discriminator and generator are trained independently
    to outperform each other. Hence, in the end we get a high-quality fake.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd222e2e675a330b6217b13ea4acc8d4.png)'
  prefs: []
  type: TYPE_IMG
- en: GANs architecture. [License](https://paperswithcode.com/method/gan).
  prefs: []
  type: TYPE_NORMAL
- en: 'The training process originally looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Sample input noise (in our case images with clouds).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the noise to G and collect the prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the D loss by getting 2 predictions one for G‚Äôs output and another
    for the real data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update D‚Äôs weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample input noise again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the noise to G and collect the prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the G loss by feeding its prediction to D.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update G‚Äôs weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/788502677c2e805ddd67f00aa1010601.png)'
  prefs: []
  type: TYPE_IMG
- en: 'GANs training loop. Source: [1].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words we can define a value function V(G,D):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba0147e691508a1cee54eda81aec27ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [1].'
  prefs: []
  type: TYPE_NORMAL
- en: where we want to minimize the term **log(1-D(G(z)))** to train G and maximize
    **log D(x)** to train D (in this notation x ‚Äî real data sample and z ‚Äî noise).
  prefs: []
  type: TYPE_NORMAL
- en: Now let‚Äôs try to implement it in pytorch!
  prefs: []
  type: TYPE_NORMAL
- en: In the original paper authors talk about using Multilayer Perceptron (MLP);
    it‚Äôs also often referred simply as ANN, but I want to try a little bit more complicated
    approach ‚Äî I want to use the UNet [5] architecture as a Generator and ResNet [6]
    as a Discriminator. These are both well-known CNN architectures, so I won‚Äôt be
    explaining them here (let me know if I should write a separate article in the
    comments).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs build them. Discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to split our data into train/test and wrap them into a torch dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Perfect. It‚Äôs time to write the training loop. Before doing so, let‚Äôs define
    our loss functions and optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, these losses are different from the picture with the GAN algorithm.
    In particular, I added L1Loss. The idea is that we are not simply generating a
    random image from noise, we want to keep most of the information from the input
    and just remove noise. So G loss will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '**G_loss = log(1 ‚àí D(G(z))) + ùùÄ |G(z)-y|**'
  prefs: []
  type: TYPE_NORMAL
- en: instead of just
  prefs: []
  type: TYPE_NORMAL
- en: '**G_loss = log(1 ‚àí D(G(z)))**'
  prefs: []
  type: TYPE_NORMAL
- en: ùùÄ is an arbitrary coefficient, which balances two components of the losses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let‚Äôs split the data to start the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can run our training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After the code is finished we can plot the losses. This code was partly adopted
    from [this cool website](https://python-graph-gallery.com/web-small-multiple-with-highlights/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/413ed7942d93aa8dfd0134cb587b28e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [author](https://medium.com/@alexroz).
  prefs: []
  type: TYPE_NORMAL
- en: 'And also visualize a random sample from the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a16b1e301ecc94fe2928a33cc3216350.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [author](https://medium.com/@alexroz).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the results are not perfect and depend a lot on the land cover
    type. Nevertheless, the built model certainly removes the clouds from images and
    its performance can be improved by increasing G and D depth. Another promising
    strategy to test is training separate models for different land cover types. For
    instance, crop fields and water basins are definitely have quite distinct spatial
    features, so it might effect model‚Äôs ability to generalize.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this article provided you with a fresh perspective on applying Deep Learning
    algorithms in the geospatial domain. In my opinion, GANs are among the most powerful
    tools a data scientist can utilize, and I hope they become an essential part of
    your toolkit as well!
  prefs: []
  type: TYPE_NORMAL
- en: ===========================================
  prefs: []
  type: TYPE_NORMAL
- en: '***References:***'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio. ‚ÄúGenerative adversarial nets.‚Äù
    *Advances in neural information processing systems* 27 (2014). [https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Helber, Patrick, Benjamin Bischke, Andreas Dengel, and Damian Borth. ‚ÄúEurosat:
    A novel dataset and deep learning benchmark for land use and land cover classification.‚Äù
    *IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing*
    12, no. 7 (2019): 2217‚Äì2226\. [https://arxiv.org/pdf/1709.00029](https://arxiv.org/pdf/1709.00029)'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Wen, Xue, Zongxu Pan, Yuxin Hu, and Jiayin Liu. ‚ÄúGenerative adversarial
    learning in YUV color space for thin cloud removal on satellite imagery.‚Äù *Remote
    Sensing* 13, no. 6 (2021): 1079\. [https://www.mdpi.com/2072-4292/13/6/1079](https://www.mdpi.com/2072-4292/13/6/1079)'
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Perlin, Ken. ‚ÄúAn image synthesizer.‚Äù *ACM Siggraph Computer Graphics* 19,
    no. 3 (1985): 287‚Äì296\. [https://dl.acm.org/doi/pdf/10.1145/325165.325247](https://dl.acm.org/doi/pdf/10.1145/325165.325247)'
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. ‚ÄúU-net: Convolutional
    networks for biomedical image segmentation.‚Äù In *Medical image computing and computer-assisted
    intervention‚ÄìMICCAI 2015: 18th international conference, Munich, Germany, October
    5‚Äì9, 2015, proceedings, part III 18*, pp. 234‚Äì241\. Springer International Publishing,
    2015\. [https://arxiv.org/pdf/1505.04597](https://arxiv.org/pdf/1505.04597)'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. He, Kaiming, et al. ‚ÄúDeep residual learning for image recognition.‚Äù *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 2016.[https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: ===========================================
  prefs: []
  type: TYPE_NORMAL
- en: '***All my publications on Medium are free and open-access, that‚Äôs why I‚Äôd really
    appreciate if you followed me here!***'
  prefs: []
  type: TYPE_NORMAL
- en: P.s. I‚Äôm extremely passionate about (Geo)Data Science, ML/AI and Climate Change.
    So if you want to work together on some project pls contact me in [LinkedIn](https://www.linkedin.com/in/alexxxroz/).
  prefs: []
  type: TYPE_NORMAL
- en: üõ∞Ô∏èFollow for moreüõ∞Ô∏è
  prefs: []
  type: TYPE_NORMAL
