<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Train an Instance Segmentation Model with No Training Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Train an Instance Segmentation Model with No Training Data</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-train-an-instance-segmentation-model-with-no-training-data-190dc020bf73?source=collection_archive---------10-----------------------#2024-01-29">https://towardsdatascience.com/how-to-train-an-instance-segmentation-model-with-no-training-data-190dc020bf73?source=collection_archive---------10-----------------------#2024-01-29</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="695f" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">All you need is a bit of computing power</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@vincent.vandenbussche?source=post_page---byline--190dc020bf73--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Vincent Vandenbussche" class="l ep by dd de cx" src="../Images/b2febfc63ca0efbda0af5501f6080ab7.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*pyfH31oWD78ckKpLyzMCEg.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--190dc020bf73--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@vincent.vandenbussche?source=post_page---byline--190dc020bf73--------------------------------" rel="noopener follow">Vincent Vandenbussche</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--190dc020bf73--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 29, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km ld le ab q ee lf lg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lh"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="li k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lj an ao ap ig lk ll lm" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ln cn"><div class="l ae"><div class="ab cb"><div class="lo lp lq lr ls lt ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lj an ao ap ig lu lv lg lw lx ly lz ma s mb mc md me mf mg mh u mi mj mk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lj an ao ap ig lu lv lg lw lx ly lz ma s mb mc md me mf mg mh u mi mj mk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lj an ao ap ig lu lv lg lw lx ly lz ma s mb mc md me mf mg mh u mi mj mk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div role="button" tabindex="0" class="mu mv ed mw bh mx"><div class="ml mm mn"><img src="../Images/f3c91d0c17e4991f49a48ae9d09eb9f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4p-atWutg_ra9dde"/></div></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Photo by <a class="af ne" href="https://unsplash.com/@blue_jean?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Izzy Park</a> on <a class="af ne" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="03ba" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Did you know that for most common types of things, you don’t necessarily need data anymore to train object detection or even instance segmentation models?</p><p id="f3c5" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Let’s get real on a given example. Let’s assume you have been given the task to build an instance segmentation model for the following classes:</p><ul class=""><li id="e9a0" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Lion</li><li id="5f1e" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Horse</li><li id="afc5" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Zebra</li><li id="ba30" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Tiger</li></ul><p id="e03c" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Arguably, data would be easy to find for such classes: plenty of images of those animals are available on the internet. But if we need to build a commercially viable product for instance segmentation, we still need two things:</p><ul class=""><li id="3fe7" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Make sure we have collected images with commercial use license</li><li id="1e3b" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Label the data</li></ul><p id="3f1a" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Both of these tasks can be very time consuming and/or cost some significant amount of money.</p><p id="287f" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Let’s explore another path: the use of free, available models. To do so, we’ll use a 2-step process to generate both the data and the associated labels:</p><ul class=""><li id="b87d" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">First, we’ll generate images with <a class="af ne" href="https://en.wikipedia.org/wiki/Stable_Diffusion" rel="noopener ugc nofollow" target="_blank">Stable Diffusion</a>, a very powerful, Generative AI model</li><li id="06c2" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Then, we’ll generate and curate the labels with Meta’s <a class="af ne" href="https://segment-anything.com/" rel="noopener ugc nofollow" target="_blank">Segment Anything Model</a> (SAM)</li></ul><p id="0552" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><em class="oj">Note that, at the date of publication of this article, images generated with Stable Diffusion are kind of in a grey area, and can be used for commercial use. But the regulation may change in the future.</em></p><p id="f260" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">All the codes used in this post are available <a class="af ne" href="https://github.com/vincent-vdb/medium_posts" rel="noopener ugc nofollow" target="_blank">in this repository</a>.</p><h1 id="b542" class="ok ol fq bf om on oo gq op oq or gt os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">Generating data using Stable Diffusion</h1><p id="5e04" class="pw-post-body-paragraph nf ng fq nh b go pg nj nk gr ph nm nn no pi nq nr ns pj nu nv nw pk ny nz oa fj bk">I generated the data with Stable Diffusion. Before actually generating the data, let’s quickly give a few information about stable diffusion and how to use it.</p><h2 id="c063" class="pl ol fq bf om pm pn po op pp pq pr os no ps pt pu ns pv pw px nw py pz qa qb bk">How to use Stable Diffusion</h2><p id="66ef" class="pw-post-body-paragraph nf ng fq nh b go pg nj nk gr ph nm nn no pi nq nr ns pj nu nv nw pk ny nz oa fj bk">For that, I used the following repository: <a class="af ne" href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" rel="noopener ugc nofollow" target="_blank">https://github.com/AUTOMATIC1111/stable-diffusion-webui</a></p><p id="18e4" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">It is very complete and frequently updated, allowing to use a lot of tools and plugins. It is very easy to install, on any distribution, by following the instructions in the readme. You can also find some very useful tutorials on how to use effectively Stable Diffusion:</p><ul class=""><li id="a0e9" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">For beginners, I would suggest <a class="af ne" href="https://stable-diffusion-art.com/beginners-guide/" rel="noopener ugc nofollow" target="_blank">this tutorial</a></li><li id="c191" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">For more advanced usage, I would suggest <a class="af ne" href="https://stable-diffusion-art.com/prompt-guide/" rel="noopener ugc nofollow" target="_blank">this tutorial</a></li></ul><p id="df0e" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Without going into the details of how the stable diffusion model is trained and works (there are plenty of good resources for that), it’s good to know that actually there is more than one model.</p><p id="7075" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">There are several “official” versions of the model released by Stability AI, such as Stable Diffusion 1.5, 2.1 or XL. These official models can be easily downloaded on the <a class="af ne" href="https://huggingface.co/stabilityai" rel="noopener ugc nofollow" target="_blank">HuggingFace of Stability AI</a>.</p><p id="c713" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">But since Stable Diffusion is open source, anyone can train their own model. There is a huge number of available models on the website <a class="af ne" href="https://civitai.com/models/" rel="noopener ugc nofollow" target="_blank">Civitai</a>, sometimes trained for specific purposes, such as fantasy images, punk images or realistic images.</p><h2 id="ebd2" class="pl ol fq bf om pm pn po op pp pq pr os no ps pt pu ns pv pw px nw py pz qa qb bk">Generating the data</h2><p id="5df6" class="pw-post-body-paragraph nf ng fq nh b go pg nj nk gr ph nm nn no pi nq nr ns pj nu nv nw pk ny nz oa fj bk">For our need, I will use two models including one specifically trained for realistic image generation, since I want to generate realistic images of animals.</p><p id="af1c" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The used models and hyperparameters are the following:</p><ul class=""><li id="1ad5" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Models: <a class="af ne" href="https://civitai.com/models/133005/juggernaut-xl" rel="noopener ugc nofollow" target="_blank">JuggernautXL</a> and <a class="af ne" href="https://civitai.com/models/4201/realistic-vision-v60-b1" rel="noopener ugc nofollow" target="_blank">Realistic Vision V6.0 B1</a></li><li id="bc13" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Sampling: Euler a, 20 iterations</li><li id="f44e" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">CFG Scale: 2 (the lower the value, the more randomness in the produced output)</li><li id="b089" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Negative prompt: “<em class="oj">bad quality, bad anatomy, worst quality, low quality, low resolution, blur, blurry, ugly, wrong proportions, watermark, image artifacts, lowres, ugly, jpeg artifacts, deformed, noisy image, deformation, digital art, unrealistic, drawing, painting</em>”</li><li id="5dee" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Prompt: “<em class="oj">a realistic picture of a lion sitting on the grass</em>”</li></ul><p id="1510" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">To automate image generation with different settings, I used a specific feature script called X/Y/Z plot with prompt S/R for each axis.</p><p id="76ce" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The “prompt S/R” means search and replace, allowing to search for a string in the original prompt and replace it with other strings. Using X/Y/Z plot and prompt S/R on each axis, it allows to generate images for any combination of the possible given values (just like a hyperparameter grid search).</p><p id="8e01" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Here are the parameters I used on each axis:</p><ul class=""><li id="81ad" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od bk"><em class="oj">lion, zebra, tiger, horse</em></li><li id="2112" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk"><em class="oj">sitting, sleeping, standing, running, walking</em></li><li id="5508" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk"><em class="oj">on the grass, in the wild, in the city, in the jungle, from the back, from side view</em></li></ul><p id="131c" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Using this, I can easily generate in one go images of the following prompt “a realistic picture of a &lt;animal&gt; &lt;action&gt; &lt;location&gt;” with all the values proposed in the parameters.</p><p id="f203" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">All in all, it would generate images for 4 animals, 5 actions and 6 locations: so 120 possibilities. Adding to that, I used a batch count of 2 and 2 different models, increasing the generated images to 480 to create my dataset (120 for each animal class). Below are some examples of the generated images.</p><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div role="button" tabindex="0" class="mu mv ed mw bh mx"><div class="ml mm qc"><img src="../Images/54cbcd00b8d086177b3e4aaf3705e836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uow2-8NArUbqGBPPQ8Xxnw.png"/></div></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Samples of the generated images using Stable Diffusion. Image by author.</figcaption></figure><p id="628c" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">As we can see, most of the pictures are realistic enough. We will now get the instance masks, so that we can then train a segmentation model.</p><h1 id="650a" class="ok ol fq bf om on oo gq op oq or gt os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">Getting the labels</h1><p id="456e" class="pw-post-body-paragraph nf ng fq nh b go pg nj nk gr ph nm nn no pi nq nr ns pj nu nv nw pk ny nz oa fj bk">To get the labels, we will use SAM model to generate masks, and we will then manually filter out masks that are not good enough, as well as unrealistic images (often called hallucinations).</p><h2 id="ce92" class="pl ol fq bf om pm pn po op pp pq pr os no ps pt pu ns pv pw px nw py pz qa qb bk">Generating the raw masks</h2><p id="960c" class="pw-post-body-paragraph nf ng fq nh b go pg nj nk gr ph nm nn no pi nq nr ns pj nu nv nw pk ny nz oa fj bk">To generate the raw masks, let’s use SAM model. The SAM model requires input prompts (not a textual prompt): either a bounding box or a few point locations. This allows the model to generate the mask from this input prompt.</p><p id="dd73" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">In our case, we will do the most simple input prompt: the center point. Indeed, in most images generated by Stable Diffusion, the main object is centered, allowing us to efficiently use SAM with always the same input prompt and absolutely no labeling. To do so, we use the following function:</p><figure class="mo mp mq mr ms mt"><div class="qd ir l ed"><div class="qe qf l"/></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Function to generate the masks using SAM. Full code available in the repository.</figcaption></figure><p id="f76f" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This function will first instantiate a SAM predictor, given a model type and a checkpoint (to download <a class="af ne" href="https://github.com/facebookresearch/segment-anything#model-checkpoints" rel="noopener ugc nofollow" target="_blank">here</a>). It will then loop over the images in the input folder and do the following:</p><ul class=""><li id="bec4" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Load the image</li><li id="faa8" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Compute the mask thanks to SAM, with both the options <em class="oj">multimask_output</em> set to <em class="oj">True</em> and <em class="oj">False</em></li><li id="151c" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Apply closing to the mask before writing it as an image</li></ul><p id="8d3d" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">A few things to note:</p><ul class=""><li id="7291" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">We use both options <em class="oj">multimask_output </em>set to <em class="oj">True</em> and <em class="oj">False</em> because no option gives consistently superior results</li><li id="379c" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">We apply closing to the masks, because raw masks sometimes have a few holes</li></ul><p id="2a53" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Here are a few examples of images with their masks:</p><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div role="button" tabindex="0" class="mu mv ed mw bh mx"><div class="ml mm qg"><img src="../Images/4495d22c8066eb188e3a74e1a7fc8508.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FMIZJnnFNClW9T4nOTuZkw.png"/></div></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">A few images with the generated SAM masks displayed as a yellowish overlay. Image by author.</figcaption></figure><p id="ee40" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">As we can see, once selected, the masks are quite accurate and it took virtually no time to label.</p><h2 id="0f00" class="pl ol fq bf om pm pn po op pp pq pr os no ps pt pu ns pv pw px nw py pz qa qb bk">Selecting the masks</h2><p id="321d" class="pw-post-body-paragraph nf ng fq nh b go pg nj nk gr ph nm nn no pi nq nr ns pj nu nv nw pk ny nz oa fj bk">Not all the masks were correctly computed in the previous subsection. Indeed, sometimes the object was not centered, thus the mask prediction was off. Sometimes, for some reason, the mask is just wrong and would need more input prompts to make it work.</p><p id="039c" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">One quick workaround is to simply either select the best mask between the 2 computed ones, or simply remove the image from the dataset if no mask was good enough. Let’s do that with the following code:</p><figure class="mo mp mq mr ms mt"><div class="qd ir l ed"><div class="qe qf l"/></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Function allowing to select the best mask, or just reject the image. Full code available in the repository.</figcaption></figure><p id="3835" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This code loops over all the generated images with Stable Diffusion and does the following for each image:</p><ul class=""><li id="826a" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Load the two generated SAM masks</li><li id="9e38" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Display the image twice, one with each masks as an overlay, side by side</li><li id="6a70" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Waits for a keyboard event to make the selection</li></ul><p id="95a9" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The expected keyboard events are the following:</p><ul class=""><li id="466a" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Left arrow of the keyboard to select the left mask</li><li id="17b4" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Right arrow to select the left mask</li><li id="8972" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Down arrow to discard this image</li></ul><p id="ab8b" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Running this script may take some time, since you have to go through all the images. Assuming 1 second per image, it would take about 10 minutes for 600 images. This is still much faster than actually labeling images with masks, that usually takes at least 30 second per mask for high quality masks. Moreover, this allows to effectively filter out any unrealistic image.</p><p id="fddf" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Running this script on the generated 480 images took me less than 5 minutes. I selected the masks and filtered unrealistic images, so that I ended up with 412 masks. Next step is to train the model.</p><h1 id="9e41" class="ok ol fq bf om on oo gq op oq or gt os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">Training the model</h1><p id="2c1f" class="pw-post-body-paragraph nf ng fq nh b go pg nj nk gr ph nm nn no pi nq nr ns pj nu nv nw pk ny nz oa fj bk">Before training the YOLO segmentation model, we need to create the dataset properly. Let’s go through these steps.</p><h2 id="fe34" class="pl ol fq bf om pm pn po op pp pq pr os no ps pt pu ns pv pw px nw py pz qa qb bk">Creating the dataset</h2><figure class="mo mp mq mr ms mt"><div class="qd ir l ed"><div class="qe qf l"/></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Function to create the dataset. Full code available in the repository.</figcaption></figure><p id="23bc" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This code loops through all the image and does the following:</p><ul class=""><li id="ff25" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Randomly select the train or validation set</li><li id="4e83" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Convert the masks to polygons for YOLO expected input label</li><li id="d504" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">Copy the image and the label in the right folders</li></ul><p id="3b50" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">One tricky part in this code is in the mask to polygon conversion, done by the <em class="oj">mask2yolo</em> function. This makes use of <a class="af ne" href="https://pypi.org/project/shapely/" rel="noopener ugc nofollow" target="_blank">shapely</a> and <a class="af ne" href="https://pypi.org/project/rasterio/" rel="noopener ugc nofollow" target="_blank">rasterio</a> libraries to make this conversion efficiently. Of course, you can find the fully working in the repository.</p><p id="2cb8" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">In the end, you would end up with the following structure in your <em class="oj">datasets</em> folder:</p><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div role="button" tabindex="0" class="mu mv ed mw bh mx"><div class="ml mm qh"><img src="../Images/5cd3f4d34f280df4c7fe5b85b946013b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ie6l7i60MkMX0wNAwuGgcg.png"/></div></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Folder structure after creating the dataset. Image by author.</figcaption></figure><p id="b88f" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This is the expected structure to train a model using the YOLOv8 library: it’s finally time to train the model!</p><h2 id="f2ba" class="pl ol fq bf om pm pn po op pp pq pr os no ps pt pu ns pv pw px nw py pz qa qb bk">Training the model</h2><p id="6421" class="pw-post-body-paragraph nf ng fq nh b go pg nj nk gr ph nm nn no pi nq nr ns pj nu nv nw pk ny nz oa fj bk">We can now train the model. Let’s use a YOLOv8 nano segmentation model. Training a model is just two lines of code with the <a class="af ne" href="https://pypi.org/project/ultralytics/" rel="noopener ugc nofollow" target="_blank">Ultralytics library</a>, as we can see in the following gist:</p><figure class="mo mp mq mr ms mt"><div class="qd ir l ed"><div class="qe qf l"/></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Function to train a YOLO segmentation model. Full code available in the repository.</figcaption></figure><p id="f038" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">After 15 epochs of training on the previously prepared dataset, the results are the following:</p><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div role="button" tabindex="0" class="mu mv ed mw bh mx"><div class="ml mm qi"><img src="../Images/c1df48de7b65b8e0337edb9843d485dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fTbQ9wgREcuW0QXkk5sOCQ.png"/></div></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Results generated by YOLOv8 library after 15 epochs.</figcaption></figure><p id="969a" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">As we can see, the metrics are quite high with a mAP50–95 close to 1, suggesting good performances. Of course, the dataset diversity being quite limited, those good performances are mostly likely caused by overfitting in some extent.</p><p id="18e6" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">For a more realistic evaluation, next we’ll evaluate the model on a few real images.</p><h2 id="48b8" class="pl ol fq bf om pm pn po op pp pq pr os no ps pt pu ns pv pw px nw py pz qa qb bk">Evaluating the model on real data</h2><p id="21bc" class="pw-post-body-paragraph nf ng fq nh b go pg nj nk gr ph nm nn no pi nq nr ns pj nu nv nw pk ny nz oa fj bk">From <a class="af ne" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">Unsplash</a>, I got a few images from each class and tested the model on this data. The results are right below:</p><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div role="button" tabindex="0" class="mu mv ed mw bh mx"><div class="ml mm qj"><img src="../Images/fb175cb46b3fc0e02c2113040b890191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BeVmq_Jf_49Rm5o0jneWIw.png"/></div></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Segmentation and class prediction results on real images from Unsplash.</figcaption></figure><p id="6b97" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">On these 8 real images, the model performed quite well: the animal class is successfully predicted, and the mask seems quite accurate. Of course, to evaluate properly this model, we would need a proper labeled dataset images and segmentation masks of each class.</p><h1 id="c0ab" class="ok ol fq bf om on oo gq op oq or gt os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">Conclusion</h1><p id="bd32" class="pw-post-body-paragraph nf ng fq nh b go pg nj nk gr ph nm nn no pi nq nr ns pj nu nv nw pk ny nz oa fj bk">With absolutely no images and no labels, we could train a segmentation model for 4 classes: horse, lion, tiger and zebra. To do so, we leveraged three amazing tools:</p><ul class=""><li id="9703" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Stable diffusion to generate realistic images</li><li id="d14f" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">SAM to compute the accurate masks of the objects</li><li id="a473" class="nf ng fq nh b go oe nj nk gr of nm nn no og nq nr ns oh nu nv nw oi ny nz oa ob oc od bk">YOLOv8 to efficiently train an instance segmentation model</li></ul><p id="df91" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">While we couldn’t properly evaluate the trained model because we lack a labeled test dataset, it seems promising on a few images. Do not take this post as self-sufficient way to train any instance segmentation, but more as a method to speed up and boost the performances in your next projects. From my own experience, the use of synthetic data and tools like SAM can greatly improve your productivity in building production-grade computer vision models.</p><p id="3894" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Of course, all the code to do this on your own is fully available in <a class="af ne" href="https://github.com/vincent-vdb/medium_posts/tree/main" rel="noopener ugc nofollow" target="_blank">this repository</a>, and will hopefully help you in your next computer vision project!</p></div></div></div></div>    
</body>
</html>