- en: 'Building LLM Apps: A Clear Step-By-Step Guide'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd?source=collection_archive---------0-----------------------#2024-06-10](https://towardsdatascience.com/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd?source=collection_archive---------0-----------------------#2024-06-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Comprehensive Steps for Building LLM-Native Apps: From Initial Idea to Experimentation,
    Evaluation, and Productization'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@almogbaku?source=post_page---byline--1fe1e6ef60fd--------------------------------)[![Almog
    Baku](../Images/3ac36986f6ca0ba56c8edced6ec7dd07.png)](https://medium.com/@almogbaku?source=post_page---byline--1fe1e6ef60fd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1fe1e6ef60fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1fe1e6ef60fd--------------------------------)
    [Almog Baku](https://medium.com/@almogbaku?source=post_page---byline--1fe1e6ef60fd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1fe1e6ef60fd--------------------------------)
    ·12 min read·Jun 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) are swiftly becoming a cornerstone of modern AI.
    Yet, there are **no established best practices**, and often, pioneers are left
    with **no clear roadmap**, needing to reinvent the wheel or getting stuck.
  prefs: []
  type: TYPE_NORMAL
- en: Over the past two years, I’ve helped organizations leverage LLMs to build innovative
    applications. Through this experience, I developed a ***battle-tested method***
    for creating innovative solutions (shaped by insights from the [LLM.org.il](https://llm.org.il)
    community), which I’ll share in this article.
  prefs: []
  type: TYPE_NORMAL
- en: This guide provides a *clear roadmap* for navigating the complex landscape of
    LLM-native development. You’ll learn how to move from ideation to experimentation,
    evaluation, and productization, unlocking your potential to create groundbreaking
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36d91b22a5b41e77c6e555b9655839f9.png)'
  prefs: []
  type: TYPE_IMG
- en: (Created with Dall-E3)
  prefs: []
  type: TYPE_NORMAL
- en: Why a Standardized Process is Essential
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LLM space is so dynamic that sometimes, we hear about new groundbreaking
    innovations day after day. This is quite exhilarating but also very chaotic —
    you may find yourself lost in the process, wondering what to do or how to bring
    your novel idea to life.
  prefs: []
  type: TYPE_NORMAL
- en: Long story short, if you are an **AI Innovator** (a manager or a practitioner)
    who wants to build LLM-native apps effectively, **this is for you**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing a standardized process helps kick off new projects and offers
    several key benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standardize the process —** A standardized process helps align the team members
    and ensures a smooth new member onboarding process (especially in this chaos).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Defines clear milestones** — A straightforward way to track your work, measure
    it, and make sure you''re on the right path'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Identify decision points** — LLM-native development is full of unknowns and
    "small experimentation" [see below]. Clear decision points make it easy to mitigate
    our risk and always stay lean with our development effort.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Essential Skills of an LLM Engineer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unlike any other established role in Software R&D, LLM-native development absolutely
    requires a new role: the **LLM Engineer** or the **AI Engineer.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The LLM Engineer is a unique hybrid creature that involves skills from different
    (established) roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Software Engineering skills—**Like most SWEs, most of the work involves putting
    the Lego pieces together and gluing everything together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Research skills** —Properly understanding the LLM-native experimental nature
    is ***essential.*** While building "cool demo apps" is pretty accessible, the
    distance between a "cool demo" and a practical solution requires experimentation
    and agility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep business/product understanding —** Due to the fragility of the models,
    it''s essential to understand the business goals and procedures rather than sticking
    to the architecture we defined. The ability to model a manual process is a golden
    skill for LLM Engineers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While writing this, LLM Engineering is still brand new, and **hiring can be
    very challenging**. It can be a good idea to look for candidates with a background
    in backend/data engineering or data science.
  prefs: []
  type: TYPE_NORMAL
- en: '*Software Engineers* might expect a *smoother transition*, as the experimentation
    process is much more "engineer-y" and not that "scientific" (compared to traditional
    data science work). That being said, I''ve seen many *Data Scientists* do this
    transition as well. As long as you''re okay with the fact that you''ll have to
    embrace new soft skills, you''re on the right path!'
  prefs: []
  type: TYPE_NORMAL
- en: The Key Elements of LLM-Native Development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike classical backend apps (such as CRUD), there are no step-by-step recipes
    here. Like everything else in "AI," LLM-native apps require a **research and experimentation
    *mindset*.**
  prefs: []
  type: TYPE_NORMAL
- en: To tame the beast, you must divide and conquer by splitting your work into smaller
    experiments, trying some of them, and selecting the most promising experiment.
  prefs: []
  type: TYPE_NORMAL
- en: I can't emphasize enough the importance of the *research mindset*.That means
    you might invest the time to explore a research vector and find out that it's
    "not possible," "not good enough," or "not worth it." That's totally okay — it
    means you're on the right track.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d80576c3df986e4e82ba49558a9d3111.png)'
  prefs: []
  type: TYPE_IMG
- en: Experimenting with LLMs is the only way to build LLM-native apps (and avoid
    the snakes in the way) (Created with Dall-E3)
  prefs: []
  type: TYPE_NORMAL
- en: 'Embracing Experimentation: The Heart of the Process'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, your "experiment" will fail, then you slightly pivot your work, and
    this other experiment succeeded much better.
  prefs: []
  type: TYPE_NORMAL
- en: That's precisely why, *before* designing our endgame solution, we must start
    simple and hedge our risks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Define a "budget" or timeframe.** Let''s see what we can do in X weeks and
    then decide how or if to continue. Usually, 2–4 weeks to understand basic PoC
    will be sufficient. If it looks promising — continue investing resources to improve
    it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Experiment—**Whether you choose a bottom-up or top-down approach for your
    experimentation phase, your goal is to maximize the result succession rate. By
    the end of the first experimentation iteration, you should have some PoC (that
    stakeholders can play with) and a baseline you achieved.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Retrospective —** By the end of our research phase, we can understand the
    feasibility, limitations, and cost of building such an app. This helps us decide
    whether to productionize it and how to design the final product and its UX.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Productization —** Develop a production-ready version of your project and
    integrate it with the rest of your solution by following standard SWE best practices
    and implementing a feedback and data collection mechanism.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/3584722390c58fe3144fc2d6496b099d.png)'
  prefs: []
  type: TYPE_IMG
- en: LLM-Native app development lifecycle (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement the experiment-oriented process well, we must make an informed
    decision on approaching and constructing these experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting Lean: The Bottom-Up Approach'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While many early adopters quickly jump into" *State-Of-The-Art"* multichain
    agentic systems with full-fledged Langchain or something similar, I found "**The
    Bottom-Up approach**" often yields better results.
  prefs: []
  type: TYPE_NORMAL
- en: Start lean, **very lean**, embracing the *“one prompt to rule them all”* philosophy.
    Although this strategy might seem unconventional and will likely produce bad results
    at first, it establishes a *baseline* for your system.
  prefs: []
  type: TYPE_NORMAL
- en: From there, continuously iterate and refine your prompts, employing prompt engineering
    techniques to optimize outcomes. As you identify weaknesses in your lean solution,
    split the process by adding branches to address those shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: While designing each "leaf" of my LLM workflow graph, or LLM-native architecture,
    I follow the[*LLM Triangle Principles*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)³
    to determine where and when to cut the branches, split them, or thicken the roots
    (by using prompt engineering techniques) and squeeze more of the lemon.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5201b05f41b5d4fed69473f9d53293e3.png)'
  prefs: []
  type: TYPE_IMG
- en: An illustration for the Bottom-Up approach (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: For example, to implement "Native language SQL querying" with the bottom-up
    approach, we'll start by naively sending the schemas to the LLM and ask it to
    generate a query.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1cc8092436c0a284cf6ca8b5914b78ad.png)'
  prefs: []
  type: TYPE_IMG
- en: A Bottom-Up approach example (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Usually, this does not contradict the "top-down approach" but serves as another
    step before it. This allows us to show quick wins and attract more project investment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Big Picture Upfront: The Top-Down Strategy'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “We know that LLM workflow is not easy, and to achieve our goal, we'll probably
    end up with some workflow or LLM-native architecture.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Top-Down approach recognizes it and starts by designing the LLM-native architecture
    from day one and implementing its different steps/chains from the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: This way, you can test your workflow architecture as a whole and squeeze the
    whole lemon instead of refining each leaf separately.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c45de8272f121dbc6dc5740d4459892f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Top-down approach process: design your architecture once, implement, test &
    measure (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to implement "Native language SQL querying" with the top-down
    approach, we''ll start designing the architecture before even starting to code
    and then jump to the full implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59ca9496b40b497469678b254636bbcc.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of the Top-Down approach (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Finding the Right Balance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you start experimenting with LLMs, you'll probably start at one of the
    extremes (overcomplicated top-down or super simple one-shot). In reality, there's
    no such a winner.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally — you'll define a good SoP¹ and model an expert before coding and experimenting
    with the model. In reality, modeling is very hard; sometimes, you may not have
    access to such an expert.
  prefs: []
  type: TYPE_NORMAL
- en: I found it challenging to land on a good architecture/SoP¹ at the first shot,
    so it's worth experimenting lightly before jumping to the big guns. However, it
    doesn't mean that *everything* has to be *too lean.* If you already have a *prior
    understanding* that something *MUST* be broken into smaller pieces — do that.
  prefs: []
  type: TYPE_NORMAL
- en: You should leverage the [*LLM Triangle Principles*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)³
    and correctly model the manual process while designing your solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Optimizing Your Solution: Squeezing the Lemon'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'During the experimentation phase, we continuously squeeze the lemon and add
    more "layers of complexity":'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Prompt engineering techniques**](https://www.promptingguide.ai/) — Like
    Few Shots, Role assignment, or even Dynamic few-shot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expanding the Context Window** from simple variable information to complex
    RAG flows can help improve the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experimenting with different models** — Different models perform differently
    on different tasks. Also, the large LLMs are often not very cost-effective, and
    it''s worth trying more task-specific models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt dieting —** I learned that putting the SOP¹ (specifically, the prompt
    and the requested output) through a "diet" usually improves latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By reducing the prompt size and the steps the model needs to go through, we
    can reduce both the input and output the model needs to generate. You'll be surprised,
    but prompt dieting can sometimes even improve the quality!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Be aware that the diet might also cause quality degradation, so it's important
    to set up a sanity test before doing so.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Splitting the process** into smaller steps can also be very beneficial and
    make optimizing a subprocess of your SOP¹ easier and feasible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware that this might increase the solution's complexity or damage the performance
    (e.g., increase the number of tokens processed). To mitigate this, aim for concise
    prompts and smaller models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As a rule of thumb, it's usually a good idea to split when a dramatic change
    of the System Prompt yields much better results for this part of the SOP¹ flow.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/0ccb287ce6adee9c671ffc63e24be4ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Squeezing the AI Lemon (Created with Dall-E3)
  prefs: []
  type: TYPE_NORMAL
- en: The Anatomy of an LLM Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Personally, I prefer to start *lean* with a simple Jupyter Notebook using Python,
    Pydantic, and Jinja2:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use Pydantic** to define my outputs'' schema from the model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the **prompt template** with **Jinja2**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Define a structured output** format (in **YAML**²). This will ensure the
    model follows the "thinking steps" and is guided by my SOP.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Ensure this output** with your Pydantic validations; if needed — retry.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Stabilize your work** — structure your code into functional units with Python
    files and packages.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a broader scope, you can use different tools such as [openai-streaming](https://github.com/AlmogBaku/openai-streaming)
    to easily **utilize streaming (and tools)**, [LiteLLM](https://docs.litellm.ai/docs/)
    to have a **standardized LLM SDK** across different providers, or [vLLM](https://docs.vllm.ai/)
    to **serve open-source LLMs.**
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring Quality with Sanity Tests and Evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A sanity test evaluates the quality of your project and ensures that you're
    not degrading a certain success rate baseline you defined.
  prefs: []
  type: TYPE_NORMAL
- en: Think of your solution/prompts as a short blanket — if you stretch it too much,
    it might suddenly not cover some use cases it used to cover.
  prefs: []
  type: TYPE_NORMAL
- en: To do that, define a set of cases you have already covered successfully and
    ensure you keep it that way (or at least it's worth it). Thinking of it like a
    [table-driven test](https://lorenzopeppoloni.com/tabledriventestspy/) might help.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the success of a "generative" solution(e.g., writing text) is much
    more complex than using LLMs for other tasks (such as categorization, entity extraction,
    etc.). For these kinds of tasks, you might want to involve a smarter model (such
    as GPT4, Claude Opus, or LLAMA3–70B) to act as a "judge."
  prefs: []
  type: TYPE_NORMAL
- en: 'It might also be a good idea to try and make the output include "deterministic
    parts" before the "generative" output, as these kinds of output are easier to
    test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few cutting-edge,🤩🤩 promising solutions worth investigating. I
    found them especially relevant when evaluating RAG-based solutions: take a look
    at [DeepChecks](https://deepchecks.com/), [Ragas](https://github.com/explodinggradients/ragas),
    or [ArizeAI](https://arize.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Making Informed Decisions: The Importance of Retrospectives'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After each major/time-framed experiment or milestone, we should stop and make
    **an informed decision** on how and if to proceed with this approach.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, your experiment will have a clear success rate baseline, and
    you'll have an idea of what needs to be improved.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is also a good point to start discussing the productization implications
    of this solution and start with the "product work":'
  prefs: []
  type: TYPE_NORMAL
- en: What will this look like within the product?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the limitations/challenges? How would you mitigate them?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What’s your current latency? Is it good enough?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What should the UX be? Which UI hacks can you use? Can [streaming](https://github.com/AlmogBaku/openai-streaming)
    help?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the estimated spending on tokens? Can we use smaller models to reduce
    spending?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are priorities? Is any of the challenges a showstopper?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose the *baseline* we achieved is “good enough,” and we believe we can mitigate
    the problems we raised. In that case, we will continue investing in and improving
    the project while *ensuring it never degrades* and using the sanity tests.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bacac79b87978cd360d070bb0f44464a.png)'
  prefs: []
  type: TYPE_IMG
- en: (Created with Dall-E3)
  prefs: []
  type: TYPE_NORMAL
- en: 'From Experiment to Product: Bringing Your Solution to Life'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Last but not least, we have to productize our work. Like any other production-grade
    solution, we must implement production engineering concepts like logging, monitoring,
    dependency management, containerization, caching, etc.
  prefs: []
  type: TYPE_NORMAL
- en: This is a huge world, but luckily, we can borrow many mechanisms from classical
    production engineering and even adopt many of the existing tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'That being said, it''s important to take extra care of the nuances involving
    LLM-native apps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feedback loop** — How do we measure success? Is it simply a "thumb up/down"
    mechanism or something more sophisticated that considers the adoption of our solution?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also important to collect this data; down the road, this can help us redefine
    our sanity "baseline" or fine-tune our results with [dynamic-few shots](https://arxiv.org/abs/1804.09458)
    or fine-tune the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Caching** — Unlike traditional SWE, caching can be very challenging when
    we involve a generative aspect in our solution. To mitigate it, explore the option
    to cache similar results(e.g., using RAG) and/or reduce the generative output
    (by having a strict output schema)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost tracking** — Many companies find it very tempting to start with a "strong
    model" (such as GPT-4 or Opus) however - in production, the costs can quickly
    rise. Avoid being surprised on the final bill, and make sure to measure the input/output
    tokens and keep track of your workflow impact (without these practices — good
    luck profiling it later on)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Debuggability and tracing** — Ensure you have set up the right tools to track
    a "buggy" input and track it throughout the process. This usually involves retaining
    the user input for later investigation and setting up [a tracing system](https://github.com/traceloop/openllmetry?tab=readme-ov-file).
    Remember: "Unlike traditional software, AI fails silently!"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Closing Remarks: Your Role in Advancing LLM-Native Technology'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This might be the end of the article, but certainly not the end of our work.
    LLM-native development is an iterative process that covers more use cases, challenges,
    and features and continuously improves our LLM-native product.
  prefs: []
  type: TYPE_NORMAL
- en: As you continue your AI development journey, stay agile, experiment fearlessly,
    and keep the end-user in mind. Share your experiences and insights with the community,
    and together, we can push the boundaries of what's possible with LLM-native apps.
    Keep exploring, learning, and building — the possibilities are endless.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this guide has been a valuable companion on your LLM-native development
    journey! I'd love to hear your story — share your triumphs and challenges in the
    comments below. 💬
  prefs: []
  type: TYPE_NORMAL
- en: If you find this article helpful, please give it a few **claps** 👏 on Medium
    and **share** it with your fellow AI enthusiasts. Your support means the world
    to me! 🌍
  prefs: []
  type: TYPE_NORMAL
- en: Let's keep the conversation going — feel free to reach out via [email](mailto:almog.baku@gmail.com)
    or [connect on LinkedIn](https://www.linkedin.com/in/almogbaku/) 🤝
  prefs: []
  type: TYPE_NORMAL
- en: Special thanks to [Yonatan V. Levin](https://medium.com/u/8735065c2497?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    [Gal Peretz](https://medium.com/u/532f8dc01db8?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    [Philip Tannor](https://medium.com/u/5c5d2a69bcdb?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    [Ori Cohen](https://medium.com/u/4dde5994e6c1?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    [Nadav](https://medium.com/u/ed1905bd6262?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    [Ben Huberman](https://medium.com/u/e6ad8abedec9?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    [Carmel Barniv](https://medium.com/u/6374acbf3a05?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    [Omri Allouche](https://medium.com/u/bf14cec4d697?source=post_page---user_mention--1fe1e6ef60fd--------------------------------),
    and [Liron Izhaki Allerhand](https://medium.com/u/251cd1007ce8?source=post_page---user_mention--1fe1e6ef60fd--------------------------------)
    for insights, feedback, and editing notes.
  prefs: []
  type: TYPE_NORMAL
- en: '**¹SoP**- Standard operating procedure, a concept borrowed from [The *LLM Triangle
    Principles*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)³'
  prefs: []
  type: TYPE_NORMAL
- en: '**²YAML**- I found that using YAML to structure your output works much better
    with LLMs. Why? My theory is that it reduces the non-relevant tokens and behaves
    much like the native language. This article dives deep into this subject.'
  prefs: []
  type: TYPE_NORMAL
- en: '**³**[**The LLM Triangle Principles**](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)-
    Software design principles for designing and building LLM-native apps; Update-
    the whitepaper recently published, [you can read it here](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e).'
  prefs: []
  type: TYPE_NORMAL
