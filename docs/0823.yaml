- en: Optimizing Multi-task Learning Models in Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/optimizing-multi-task-learning-models-in-practice-bde4f18f0bd8?source=collection_archive---------10-----------------------#2024-03-29](https://towardsdatascience.com/optimizing-multi-task-learning-models-in-practice-bde4f18f0bd8?source=collection_archive---------10-----------------------#2024-03-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is multi-task learning models, and how to optimize them
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vuphuongthao9611?source=post_page---byline--bde4f18f0bd8--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page---byline--bde4f18f0bd8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bde4f18f0bd8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bde4f18f0bd8--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page---byline--bde4f18f0bd8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bde4f18f0bd8--------------------------------)
    ·6 min read·Mar 29, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2f8bdb9f8fc4a7c5f2b6a54ba16d130.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Laura Rivera](https://unsplash.com/@laurar1vera?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '**Why Multi-task learning**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-task learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multi-task learning (MTL) [1] is a field in machine learning in which we utilize
    a single model to learn multiple tasks simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b644a837fc43b44719463f50bde0b387.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-task learning model (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: In theory, the approach allows knowledge sharing between tasks and achieves
    better results than single-task training. Moreover, as the model tries to learn
    a representation to optimize multiple tasks, there is a lower chance of overfitting
    and, hence, better generalization.
  prefs: []
  type: TYPE_NORMAL
- en: Multitask Learning is an approach to inductive transfer that improves generalization
    by using the domain information contained in the training signals of related tasks
    as an inductive bias. It does this by learning tasks in parallel while using a
    shared representation; what is learned for each task can help other tasks be learned
    better. [2]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In practice, large recommendation and search systems often measure user satisfaction
    based on multiple metrics, such as stay time, click-through rate, and…
  prefs: []
  type: TYPE_NORMAL
