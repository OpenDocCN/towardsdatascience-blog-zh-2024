<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Monocular Depth Estimation with Depth Anything V2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Monocular Depth Estimation with Depth Anything V2</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/monocular-depth-estimation-with-depth-anything-v2-54b6775abc9f?source=collection_archive---------4-----------------------#2024-07-24">https://towardsdatascience.com/monocular-depth-estimation-with-depth-anything-v2-54b6775abc9f?source=collection_archive---------4-----------------------#2024-07-24</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="cd10" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How do neural networks learn to estimate depth from 2D images?</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@neural.avb?source=post_page---byline--54b6775abc9f--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Avishek Biswas" class="l ep by dd de cx" src="../Images/6feb591069f354aa096f6108f1a70ea7.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*7-2CKsevyqzgVs8m"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--54b6775abc9f--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@neural.avb?source=post_page---byline--54b6775abc9f--------------------------------" rel="noopener follow">Avishek Biswas</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--54b6775abc9f--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 24, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h2 id="7fbb" class="mi mj fq bf mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bk">What is Monocular Depth Estimation?</h2><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/53c47ae75f37e0bef19e2c77b3169255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2RTXWyPO-YwMB-adMIqtVg.jpeg"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">The Depth Anything V2 Algorithm (Illustration by Author)</figcaption></figure><p id="f050" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">Monocular Depth Estimation (MDE) is the task of training a neural network to determine depth information from a single image. This is an exciting and challenging area of Machine Learning and Computer Vision because predicting a depth map requires the neural network to form a 3-dimensional understanding from just a 2-dimensional image.</p><p id="1ac7" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">In this article, we will discuss a new model called <em class="os">Depth Anything V2</em> and its precursor, <em class="os">Depth Anything V1</em>. Depth Anything V2 has outperformed nearly all other models in Depth Estimation, showing impressive results on tricky images.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ot"><img src="../Images/26476d8feb1529cf50ab27a0b30729f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HqLsheXpCFJVW_i-zqYoTQ.gif"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Depth Anything V2 Demo (Source: Screen recording by the author from Depth Anything V2 <a class="af ou" href="https://huggingface.co/spaces/depth-anything/Depth-Anything-V2" rel="noopener ugc nofollow" target="_blank">DEMO page</a>)</figcaption></figure><p id="984b" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk"><strong class="ob fr">This article is based on a video I made on the same topic. </strong><a class="af ou" href="https://youtu.be/sz30TDttIBA" rel="noopener ugc nofollow" target="_blank"><strong class="ob fr">Here is a video link for learners who prefer a visual medium.</strong></a> For those who prefer reading, continue!</p><h2 id="52cb" class="mi mj fq bf mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bk">Why should we even care about MDE models?</h2><p id="4b32" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh mt ox oj ok mx oy om on nb oz op oq or fj bk">Good MDE models have many practical uses, such as <a class="af ou" href="https://natesimon.github.io/mononav/" rel="noopener ugc nofollow" target="_blank">aiding navigation and obstacle avoidance for robots</a>, drones, and autonomous vehicles. They can also be used in video and image editing, background replacement, object removal, and creating 3D effects. Additionally, they are <a class="af ou" href="https://arxiv.org/abs/2202.08010" rel="noopener ugc nofollow" target="_blank">useful for AR and VR headsets to create interactive 3D spaces around the user</a>.</p><h2 id="92d1" class="mi mj fq bf mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bk">There are two main approaches for doing MDE (this article only covers one)</h2><p id="d70f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh mt ox oj ok mx oy om on nb oz op oq or fj bk">Two main approaches have emerged for training MDE models — one, discriminative approaches where the network tries to predict depth as a supervised learning objective, and two, generative approaches like conditional diffusion where depth prediction is an iterative image generation task. <strong class="ob fr">Depth Anything belongs to the first category of discriminative approaches, and that’s what we will be discussing today.</strong> Welcome to Neural Breakdown, and let’s go deep with Depth Estimation[!</p><h1 id="4855" class="pa mj fq bf mk pb pc gq mo pd pe gt ms pf pg ph pi pj pk pl pm pn po pp pq pr bk"><strong class="al">Traditional Datasets and the MiDAS paper</strong></h1><p id="ea1e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh mt ox oj ok mx oy om on nb oz op oq or fj bk">To fully understand Depth Anything, let’s first revisit the MiDAS paper from 2019, which serves as a precursor to the Depth Anything algorithm.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ps"><img src="../Images/c35232fe74fc443615630d9de12c652e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VBJesbBsliWB2rP_GAQhng.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Source: Screenshot taken from the <a class="af ou" href="https://arxiv.org/abs/1907.01341" rel="noopener ugc nofollow" target="_blank">MIDAS</a> Paper (License: Free)</figcaption></figure><p id="8fe6" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">MiDAS trains an MDE model using a combination of different datasets containing labeled depth information. For instance, the <a class="af ou" href="https://www.cvlibs.net/datasets/kitti/" rel="noopener ugc nofollow" target="_blank">KITTI</a> dataset for autonomous driving provides outdoor images, while the <a class="af ou" href="https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html" rel="noopener ugc nofollow" target="_blank">NYU-Depth V2</a> dataset offers indoor scenes. Understanding how these datasets are collected is crucial because newer models like Depth Anything and Depth Anything V2 address several issues inherent in the data collection process.</p><h2 id="b35f" class="mi mj fq bf mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bk">How real-world depth datasets are collected</h2><p id="d310" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh mt ox oj ok mx oy om on nb oz op oq or fj bk">These datasets are typically collected using stereo cameras, where two or more cameras placed at fixed distances capture images simultaneously from slightly different perspectives, allowing for depth information extraction. The NYU-Depth V2 dataset uses RGB-D cameras that capture depth values along with pixel colors. Some datasets utilize LiDAR, projecting laser beams to capture 3D information about a scene.</p><p id="d55b" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk"><strong class="ob fr">However, these methods come with several problems.</strong> The amount of labeled data is limited due to the high operational costs of obtaining these datasets. Additionally, the annotations can be noisy and low-resolution. Stereo cameras struggle under various lighting conditions and can’t reliably identify transparent or highly reflective surfaces. LiDAR is expensive, and both LiDAR and RGB-D cameras have limited range and generate low-resolution, sparse depth maps.</p><h2 id="073d" class="mi mj fq bf mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bk">Can we use Unlabelled Images to learn Depth Estimation?</h2><p id="207b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh mt ox oj ok mx oy om on nb oz op oq or fj bk">It would be beneficial to use unlabeled images to train depth estimation models, given the abundance of such images available online. The major innovation proposed in the original Depth Anything paper from 2023 was the incorporation of these unlabeled datasets into the training pipeline. In the next section, we’ll explore how this was achieved.</p></div></div></div><div class="ab cb pt pu pv pw" role="separator"><span class="px by bm py pz qa"/><span class="px by bm py pz qa"/><span class="px by bm py pz"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="658d" class="pa mj fq bf mk pb qb gq mo pd qc gt ms pf qd ph pi pj qe pl pm pn qf pp pq pr bk"><strong class="al">Depth Anything Architecture</strong></h1><p id="e926" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh mt ox oj ok mx oy om on nb oz op oq or fj bk">The original <a class="af ou" href="https://depth-anything.github.io/" rel="noopener ugc nofollow" target="_blank">Depth Anything (V1) model from 2023</a> was trained in a three-step process. Let’s get a high-level overview of the algorithm before diving into each section.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh qg"><img src="../Images/b59c38e97f10cfa343bac2b3e428fc20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TVOjnQhJJQuwwvMimhpfgQ.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Depth Anything V1 Algorithm (Illustration made by the Author)</figcaption></figure><h2 id="e313" class="mi mj fq bf mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bk">Step 1: Teacher Training</h2><p id="2281" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh mt ox oj ok mx oy om on nb oz op oq or fj bk">First, a neural network called the TEACHER model is trained for supervised depth estimation using five different publicly available datasets.</p><p id="6dcb" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk"><strong class="ob fr"><em class="os">Converting from Depth to Disparity Space</em></strong></p><p id="ee2d" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">The TEACHER model is initialized with a <a class="af ou" href="https://arxiv.org/abs/2304.07193" rel="noopener ugc nofollow" target="_blank">pre-trained Dino-V2 encoder</a> and then trained on the combined labeled dataset. A major challenge with training on multiple datasets is the variability in absolute depths. To address this, the depths are inverted into disparity space <strong class="ob fr">(d = 1 / t)</strong> and normalized between 0 and 1 for each depth map — 1 for the nearest pixel and 0 for the farthest. This way, all datasets share the same output space, allowing the model to predict disparity.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/feb773be7a55da3e9a247bc018362171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KiPhEI3eN45r9bCBuQHjtg.jpeg"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Different Depth Estimation datasets provide depths at different scales. We need to align them to have the same output space. Disparity lets us normalize all depth values between 0 and 1 (Illustration by Author)</figcaption></figure><p id="f1f1" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">Two loss functions are used to train these models: <strong class="ob fr">a scale-shift invariant loss and a gradient-matching loss</strong>, both also utilized in the MiDAS paper from 2019.</p><ol class=""><li id="29d4" class="nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or qh qi qj bk"><strong class="ob fr"><em class="os">Scale-shift invariant loss</em></strong></li></ol><p id="a0ca" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">There is a problem with using a simple mean square error loss between the predicted and ground truth images. Let’s say the ground truth depth values of three pixels in an image are <em class="os">1, 0.5, and 0.1</em>, while our network predicts <em class="os">0.9, 0.6, and 0.3</em>. Although the predictions aren’t exact, the relationship between the predicted and ground truth depths is similar, differing only by a multiplicative and additive factor. <strong class="ob fr">We don’t want this scale and shift to affect our loss function — we need to align the two maps before applying the mean square error loss.</strong></p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/84703dc474e959b2a4de1549e58249df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sEljrOPD5JkEDIxHmlsppA.jpeg"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Scale and Shift Invariant Loss (Illustration by Author)</figcaption></figure><p id="185d" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">The MiDaS paper proposes normalizing the ground truth and predicted depths to have zero translation and unit scale. The median and deviation are calculated, and the depth maps are scaled and shifted accordingly. Once aligned, the mean square error loss is applied.</p></div></div><div class="no"><div class="ab cb"><div class="ll qk lm ql ln qm cf qn cg qo ci bh"><div class="nj nk nl nm nn ab ke"><figure class="le no qp qq qr qs qt paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><img src="../Images/8a6dd80e0cf700348801e9c879cb35a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*4L5F5UFRIAD4TDeBwoFRCA.png"/></div></figure><figure class="le no qu qq qr qs qt paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><img src="../Images/84a74de5ee9b690b8f57509e828181af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*MUqDun2PU5DHvMhj8uFdNQ.png"/></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx qv ed qw qx">The SSI Loss (Source: <a class="af ou" href="https://arxiv.org/abs/1907.01341" rel="noopener ugc nofollow" target="_blank">MiDAS Paper</a>) (License: Free)</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="de0b" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk"><strong class="ob fr"><em class="os">2. Gradient Matching Loss</em></strong></p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/5c5a5575969a4594c1abe8a4a352f364.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nBeLafL4tvYAopdlJ7kmjQ.jpeg"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Without Gradient Matching Loss depth maps may become too smudgy and less sharp (Illustration by Author)</figcaption></figure><p id="5fd3" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">Using only the SSI loss might result in smoothed depth maps that fail to capture sharp distinctions between adjacent pixels. Gradient Matching Loss helps preserve these details by aligning the gradients of the predicted depth map with those of the ground truth.</p><p id="c499" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">First, we calculate the gradients of the predicted and ground truth depth maps across the x and y axes, then apply the loss at the gradient level. MiDaS also uses a multi-scale gradient matching loss with four scale levels. The predicted and ground truth depth maps are downsampled four times, and the loss is applied at each resolution.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/b0ff56b585ba50ea0d90f590e51805e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pfwqjt5MQQeW-mkR7uy1uQ.jpeg"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Gradient Matching Loss. This loss is applied at multiple downscaled depth maps (not shown above) (Illustration by Author)</figcaption></figure><p id="3448" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">The final loss is the weighted sum of the scale-and-shift invariant loss and the multi-scale gradient matching loss. While the SSI loss encourages the model to learn general relative depth relationships, the gradient matching loss helps preserve sharp edges and fine-grained information in the scene.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/e2e92b97e1ef6e0478d89fd6f1aa09c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EXiMsk8-3I5U_gCvQ7cG7g.jpeg"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">The Loss functions used to train Depth Estimation models in MIDAS and Depth Anything V1 (Illustration by Author)</figcaption></figure></div></div></div><div class="ab cb pt pu pv pw" role="separator"><span class="px by bm py pz qa"/><span class="px by bm py pz qa"/><span class="px by bm py pz"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="c051" class="mi mj fq bf mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bk">Step 2 — Pseudo-Labelling Unlabelled Dataset</h2><p id="4bac" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh mt ox oj ok mx oy om on nb oz op oq or fj bk">With our trained TEACHER model, we can now annotate millions of unlabeled images to create a massive pseudo-depth label dataset. These labels are called pseudo because they are AI-generated and may not represent the actual ground truth depth. We now have a lot of (pseudo) labeled images to train a new network.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh qy"><img src="../Images/caede365e9a8d6d752dbe1cad7b4c9db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u3yOhZoCo2K9Aov3EL0bog.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Pseudo — Labelling images (Note that this screen is actually from the Depth Anything V2 paper and not V1) Source: <a class="af ou" href="https://arxiv.org/abs/2406.09414" rel="noopener ugc nofollow" target="_blank">Depth Anything V2 Paper</a> (License: Free)</figcaption></figure><h2 id="48cc" class="mi mj fq bf mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bk">Step 3 — Training Student Network</h2><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh qg"><img src="../Images/b59c38e97f10cfa343bac2b3e428fc20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TVOjnQhJJQuwwvMimhpfgQ.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Flashback to the Depth Anything V1 algorithm. We are in Step 3 now. (Illustration made by the author)</figcaption></figure><p id="86ae" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">We will be training a new neural network (the student network) on the combination of the labeled and pseudo-labeled datasets. <strong class="ob fr">However, simply training the network on the annotations provided by the Teacher Network won’t improve the model beyond the capabilities of the base Teacher model. </strong>To make the student network more capable, two strategies were employed: heavy perturbations with image augmentations and introducing an auxiliary semantic preservation loss.</p><p id="ddb9" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk"><strong class="ob fr"><em class="os">Heavy Perturbations</em></strong></p><p id="f783" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">One interesting perturbation used was the Cut Mix operation. This involves combining a random pair of unlabeled images using a binary mask, replacing a rectangular portion of image A with image B. The final loss is the combined SSI and Gradient Matching loss of the two sections from the two ground truth depth maps. These spatial distortions are also combined with color distortions to help the Student Network handle the diversity of open-world images.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/d248f0a505eebf438fa29e5c540cde53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aYGX78WBuCKduZhlLiLbEQ.jpeg"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">The Cut Mix Operation (Illustration by Author)</figcaption></figure><p id="bf12" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk"><strong class="ob fr"><em class="os">Auxiliary Semantic Preservation Loss</em></strong></p><p id="8d12" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">The network is also trained with an auxiliary task called Semantic Assisted Perception. A strong pre-trained computer vision model like Dino-V2, which has been trained on millions of images in a self-supervised manner, is used. Given an image, we aim to reduce the cosine distance between the embeddings produced by our new Student model and the pre-trained Dino-V2 encoder. This enables our Student model to capture some of the semantic perception capabilities of the larger and more general Dino-V2 model, which it uses to predict the depth map.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/3240ebbb1b601d8cf1aec0cd5932a074.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dMqA18heabuC_sXy_eRUQA.jpeg"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Semantic Assisted Perception (Illustration by author)</figcaption></figure><p id="04c6" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">By combining spatial distortions, semantic-assisted perception, and the power of both labeled and unlabeled datasets, the Student Network generalizes better and outperforms the original Teacher Network in-depth estimation! Here are some incredible results from the Depth Anything V1 model!</p></div></div></div><div class="ab cb pt pu pv pw" role="separator"><span class="px by bm py pz qa"/><span class="px by bm py pz qa"/><span class="px by bm py pz"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6915" class="pa mj fq bf mk pb qb gq mo pd qc gt ms pf qd ph pi pj qe pl pm pn qf pp pq pr bk"><strong class="al">Depth Anything V2</strong></h1><p id="b42c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh mt ox oj ok mx oy om on nb oz op oq or fj bk"><strong class="ob fr">As impressive as Depth Anything V1’s results are, it struggles with transparent objects and capturing fine-grained details.</strong> <strong class="ob fr">The authors of Depth Anything V2 suggest that the biggest bottleneck for model performance isn’t the architecture itself, but the quality of the data. </strong>Most labeled datasets captured with sensors can be quite noisy, ignore fine-grained details, generate low-resolution depth maps, and struggle with lighting conditions and reflective/transparent objects.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh qz"><img src="../Images/94bb158d8593a17797ed2214b00abd67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zaKTaOa3UhwwBQP3OZl0eQ.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Issues with real-world sensor datasets (Illustration by Author)</figcaption></figure><p id="871a" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">Depth Anything V2 discards labeled datasets from real-world sensors like stereo cameras, LiDAR, and RGB-D cameras, instead using only synthetic datasets. Synthetic datasets are generated through graphics engines, not captured with equipment. An example is the <a class="af ou" href="https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction" rel="noopener ugc nofollow" target="_blank">Virtual KITTI dataset</a>, which uses the Unity Game Engine to create rendered images and depth maps for automated driving. There are also indoor datasets like IRS and Hyper-sim. Depth Anything V2 uses five synthetic datasets containing close to 595K photorealistic images.</p><h2 id="7e95" class="mi mj fq bf mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bk"><strong class="al"><em class="ra">Synthetic Datasets vs Real World Sensor Datasets</em></strong></h2><p id="db62" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh mt ox oj ok mx oy om on nb oz op oq or fj bk">Synthetic images do have their pros and cons. They are super accurate, they have high-resolution outputs that capture the finest of the details, and the depth of transparent and reflective surfaces can be easily obtained. <strong class="ob fr">Synthetic datasets have direct access to all the 3D information needed since the graphics engine itself creates the scene.</strong></p><p id="0931" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">On the cons side, these images may not essentially capture the images that we will encounter in real-world scenarios. The scene coverage of these datasets isn’t particularly diverse enough too, and is a much smaller subset of real-world images. Depth Anything 2 combines the power of synthetic images with millions of unlabelled images to train an MDE model that outperforms pretty much everything else we have seen so far.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh rb"><img src="../Images/217dc74172e0dee404d98a287ddc4508.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K8IxT190rxpvSje-j8w7Ig.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">The pros and cons of Synthetic or Computer Generated Datasets (Illustration by Author)</figcaption></figure><p id="7b5e" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">Much like V1, the Teacher model in V2 is first trained on labeled datasets. However, in V2, it is exclusively trained on synthetic datasets. In Step 2, the Teacher model assigns pseudo-depth labels to all unlabeled images. Finally, in Step 3, the Student model is trained exclusively on pseudo-labeled images — no real labeled datasets and no synthetic datasets. The synthetic datasets are not used at this stage due to the distribution shift mentioned earlier. The Student network is trained on real-world images annotated by the Teacher model. Just like in V1, the auxiliary semantic preservation loss is used along with the Scale-and-Shift invariant and gradient matching loss.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/53c47ae75f37e0bef19e2c77b3169255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2RTXWyPO-YwMB-adMIqtVg.jpeg"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">The Depth Anything V2 architecture (Illustration by the Author)</figcaption></figure></div></div></div><div class="ab cb pt pu pv pw" role="separator"><span class="px by bm py pz qa"/><span class="px by bm py pz qa"/><span class="px by bm py pz"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="cdad" class="pa mj fq bf mk pb qb gq mo pd qc gt ms pf qd ph pi pj qe pl pm pn qf pp pq pr bk">Video link explaining the concepts here visually</h1><p id="8ab1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh mt ox oj ok mx oy om on nb oz op oq or fj bk">Here is a video that explains all the concepts discussed in this video in a step-by-step method.</p><figure class="nj nk nl nm nn no"><div class="rc io l ed"><div class="rd re l"/></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">You can also learn the topics covered in this article in video form</figcaption></figure></div></div></div><div class="ab cb pt pu pv pw" role="separator"><span class="px by bm py pz qa"/><span class="px by bm py pz qa"/><span class="px by bm py pz"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="81da" class="pa mj fq bf mk pb qb gq mo pd qc gt ms pf qd ph pi pj qe pl pm pn qf pp pq pr bk">Depth Anything V1 vs Depth Anything V2</h1><p id="a592" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh mt ox oj ok mx oy om on nb oz op oq or fj bk">The original Depth Anything emphasized the importance of using unlabeled images in the MDE training pipeline. It introduced the knowledge distillation pipeline with Teacher training, pseudo-labeling unlabeled images, and then training the Student network on a combination of labeled and unlabeled images. The use of strong spatial and color distortions, and a semantic-assisted perception loss, helped create more general and robust embeddings. This resulted in efficient and high-quality depth maps for complex scenes. However, Depth Anything V1 still struggled with reflective surfaces and fine details due to noisy and low-resolution depth labels from real-world sensors.</p><p id="d131" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">Depth Anything V2 improved performance by ignoring real-world sensor datasets and only using synthetic images generated with graphics engines to train the Teacher Network. The Teacher Network then annotates millions of unlabeled images, and the Student Network is trained solely on these pseudo-labeled datasets with real-world images. With these techniques, Depth Anything V2 can now predict fine-level depth maps and handle transparent and reflective surfaces more effectively.</p></div></div></div><div class="ab cb pt pu pv pw" role="separator"><span class="px by bm py pz qa"/><span class="px by bm py pz qa"/><span class="px by bm py pz"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9338" class="pa mj fq bf mk pb qb gq mo pd qc gt ms pf qd ph pi pj qe pl pm pn qf pp pq pr bk"><strong class="al">Relevant Links</strong></h1><p id="9a40" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh mt ox oj ok mx oy om on nb oz op oq or fj bk">MiDAS: <a class="af ou" href="https://arxiv.org/abs/1907.01341" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1907.01341</a><br/>Depth Anything: <a class="af ou" href="https://depth-anything.github.io/" rel="noopener ugc nofollow" target="_blank">https://depth-anything.github.io/</a><br/>Depth Anything V2: <a class="af ou" href="https://depth-anything-v2.github.io/" rel="noopener ugc nofollow" target="_blank">https://depth-anything-v2.github.io/</a></p><p id="2049" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">KITTI DATASET: <a class="af ou" href="https://www.cvlibs.net/datasets/kitti/" rel="noopener ugc nofollow" target="_blank">https://www.cvlibs.net/datasets/kitti/</a><br/>NYU V2: <a class="af ou" href="https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html" rel="noopener ugc nofollow" target="_blank">https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html</a><br/>VIRTUAL KITTI: <a class="af ou" href="https://datasetninja.com/virtual-kitti" rel="noopener ugc nofollow" target="_blank">https://datasetninja.com/virtual-kitti</a></p><p id="ecd1" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh mt oi oj ok mx ol om on nb oo op oq or fj bk">Youtube Video: <a class="af ou" href="https://youtu.be/sz30TDttIBA" rel="noopener ugc nofollow" target="_blank">https://youtu.be/sz30TDttIBA</a></p></div></div></div></div>    
</body>
</html>