["```py\nimport pandas as pd\n\ndf = pd.read_csv('IMDB Dataset.csv')\ndf.head()\n```", "```py\n# Remove the break tags (<br />)\ndf['review_cleaned'] = df['review'].apply(lambda x: x.replace('<br />', ''))\n\n# Remove unnecessary whitespace\ndf['review_cleaned'] = df['review_cleaned'].replace('\\s+', ' ', regex=True)\n\n# Compare 72 characters of the second review before and after cleaning\nprint('Before cleaning:')\nprint(df.iloc[1]['review'][0:72])\n\nprint('\\nAfter cleaning:')\nprint(df.iloc[1]['review_cleaned'][0:72])\n```", "```py\nBefore cleaning:\nA wonderful little production. <br /><br />The filming technique is very\n\nAfter cleaning:\nA wonderful little production. The filming technique is very unassuming-\n```", "```py\ndf['sentiment_encoded'] = df['sentiment'].\\\n    apply(lambda x: 0 if x == 'negative' else 1)\ndf.head()\n```", "```py\nfrom transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nprint(tokenizer)\n```", "```py\nBertTokenizer(\n  name_or_path='bert-base-uncased',\n  vocab_size=30522,\n  model_max_length=512,\n  is_fast=False,\n  padding_side='right',\n  truncation_side='right',\n  special_tokens={\n    'unk_token': '[UNK]',\n    'sep_token': '[SEP]',\n    'pad_token': '[PAD]',\n    'cls_token': '[CLS]',\n    'mask_token': '[MASK]'},\n  clean_up_tokenization_spaces=True),\n\nadded_tokens_decoder={\n  0: AddedToken(\n    \"[PAD]\",\n    rstrip=False,\n    lstrip=False,\n    single_word=False,\n    normalized=False,  \n    special=True),\n\n  100: AddedToken(\n    \"[UNK]\",\n    rstrip=False,\n    lstrip=False,\n    single_word=False,\n    normalized=False,\n    special=True),\n\n  101: AddedToken(\n    \"[CLS]\",\n    rstrip=False,\n    lstrip=False,\n    single_word=False,\n    normalized=False,\n    special=True),\n\n  102: AddedToken(\n    \"[SEP]\",\n    rstrip=False,\n    lstrip=False,\n    single_word=False,\n    normalized=False,\n    special=True),\n\n  103: AddedToken(\n    \"[MASK]\",\n    rstrip=False,\n    lstrip=False,\n    single_word=False,\n    normalized=False,\n    special=True),\n  }\n```", "```py\n# Encode a sample input sentence\nsample_sentence = 'I liked this movie'\ntoken_ids = tokenizer.encode(sample_sentence, return_tensors='np')[0]\nprint(f'Token IDs: {token_ids}')\n\n# Convert the token IDs back to tokens to reveal the special tokens added\ntokens = tokenizer.convert_ids_to_tokens(token_ids)\nprint(f'Tokens   : {tokens}')\n```", "```py\nToken IDs: [ 101 1045 4669 2023 3185  102]\nTokens   : ['[CLS]', 'i', 'liked', 'this', 'movie', '[SEP]']\n```", "```py\nreview = df['review_cleaned'].iloc[0]\n\ntoken_ids = tokenizer.encode(\n    review,\n    max_length = 512,\n    padding = 'max_length',\n    truncation = True,\n    return_tensors = 'pt')\n\nprint(token_ids)\n```", "```py\ntensor([[  101,  2028,  1997,  1996,  2060, 15814,  2038,  3855,  2008,  2044,\n          3666,  2074,  1015, 11472,  2792,  2017,  1005,  2222,  2022, 13322,\n\n                                       ...\n\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]])\n```", "```py\nreview = df['review_cleaned'].iloc[0]\n\nbatch_encoder = tokenizer.encode_plus(\n    review,\n    max_length = 512,\n    padding = 'max_length',\n    truncation = True,\n    return_tensors = 'pt')\n\nprint('Batch encoder keys:')\nprint(batch_encoder.keys())\n\nprint('\\nAttention mask:')\nprint(batch_encoder['attention_mask'])\n```", "```py\nBatch encoder keys:\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n\nAttention mask:\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n\n                                      ...\n\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n         0, 0, 0, 0, 0, 0, 0, 0]])\n```", "```py\nimport torch\n\ntoken_ids = []\nattention_masks = []\n\n# Encode each review\nfor review in df['review_cleaned']:\n    batch_encoder = tokenizer.encode_plus(\n        review,\n        max_length = 512,\n        padding = 'max_length',\n        truncation = True,\n        return_tensors = 'pt')\n\n    token_ids.append(batch_encoder['input_ids'])\n    attention_masks.append(batch_encoder['attention_mask'])\n\n# Convert token IDs and attention mask lists to PyTorch tensors\ntoken_ids = torch.cat(token_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\n\nval_size = 0.1\n\n# Split the token IDs\ntrain_ids, val_ids = train_test_split(\n                        token_ids,\n                        test_size=val_size,\n                        shuffle=False)\n\n# Split the attention masks\ntrain_masks, val_masks = train_test_split(\n                            attention_masks,\n                            test_size=val_size,\n                            shuffle=False)\n\n# Split the labels\nlabels = torch.tensor(df['sentiment_encoded'].values)\ntrain_labels, val_labels = train_test_split(\n                                labels,\n                                test_size=val_size,\n                                shuffle=False)\n\n# Create the DataLoaders\ntrain_data = TensorDataset(train_ids, train_masks, train_labels)\ntrain_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)\nval_data = TensorDataset(val_ids, val_masks, val_labels)\nval_dataloader = DataLoader(val_data, batch_size=16)\n```", "```py\nfrom transformers import BertForSequenceClassification\n\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=2)\n```", "```py\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom transformers import get_linear_schedule_with_warmup\n\nEPOCHS = 2\n\n# Optimizer\noptimizer = AdamW(model.parameters())\n\n# Loss function\nloss_function = nn.CrossEntropyLoss()\n\n# Scheduler\nnum_training_steps = EPOCHS * len(train_dataloader)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps)\n```", "```py\n# Check if GPU is available for faster training time\nif torch.cuda.is_available():\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')\n```", "```py\nfor epoch in range(0, EPOCHS):\n\n    model.train()\n    training_loss = 0\n\n    for batch in train_dataloader:\n\n        batch_token_ids = batch[0].to(device)\n        batch_attention_mask = batch[1].to(device)\n        batch_labels = batch[2].to(device)\n\n        model.zero_grad()\n\n        loss, logits = model(\n            batch_token_ids,\n            token_type_ids = None,\n            attention_mask=batch_attention_mask,\n            labels=batch_labels,\n            return_dict=False)\n\n        training_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n    average_train_loss = training_loss / len(train_dataloader)\n```", "```py\n model.eval()\n    val_loss = 0\n    val_accuracy = 0\n\n    for batch in val_dataloader:\n\n        batch_token_ids = batch[0].to(device)\n        batch_attention_mask = batch[1].to(device)\n        batch_labels = batch[2].to(device)\n\n        with torch.no_grad():\n            (loss, logits) = model(\n                batch_token_ids,\n                attention_mask = batch_attention_mask,\n                labels = batch_labels,\n                token_type_ids = None,\n                return_dict=False)\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = batch_labels.to('cpu').numpy()\n        val_loss += loss.item()\n        val_accuracy += calculate_accuracy(logits, label_ids)\n\n    average_val_accuracy = val_accuracy / len(val_dataloader)\n```", "```py\ndef calculate_accuracy(preds, labels):\n    \"\"\" Calculate the accuracy of model predictions against true labels.\n\n    Parameters:\n        preds (np.array): The predicted label from the model\n        labels (np.array): The true label\n\n    Returns:\n        accuracy (float): The accuracy as a percentage of the correct\n            predictions.\n    \"\"\"\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n    return accuracy\n```", "```py\ndef preprocess_dataset(path):\n    \"\"\" Remove unnecessary characters and encode the sentiment labels.\n\n    The type of preprocessing required changes based on the dataset. For the\n    IMDb dataset, the review texts contains HTML break tags (<br/>) leftover\n    from the scraping process, and some unnecessary whitespace, which are\n    removed. Finally, encode the sentiment labels as 0 for \"negative\" and 1 for\n    \"positive\". This method assumes the dataset file contains the headers\n    \"review\" and \"sentiment\".\n\n    Parameters:\n        path (str): A path to a dataset file containing the sentiment analysis\n            dataset. The structure of the file should be as follows: one column\n            called \"review\" containing the review text, and one column called\n            \"sentiment\" containing the ground truth label. The label options\n            should be \"negative\" and \"positive\".\n\n    Returns:\n        df_dataset (pd.DataFrame): A DataFrame containing the raw data\n            loaded from the self.dataset path. In addition to the expected\n            \"review\" and \"sentiment\" columns, are:\n\n            > review_cleaned - a copy of the \"review\" column with the HTML\n                break tags and unnecessary whitespace removed\n\n            > sentiment_encoded - a copy of the \"sentiment\" column with the\n                \"negative\" values mapped to 0 and \"positive\" values mapped\n                to 1\n    \"\"\"\n    df_dataset = pd.read_csv(path)\n\n    df_dataset['review_cleaned'] = df_dataset['review'].\\\n        apply(lambda x: x.replace('<br />', ''))\n\n    df_dataset['review_cleaned'] = df_dataset['review_cleaned'].\\\n        replace('\\s+', ' ', regex=True)\n\n    df_dataset['sentiment_encoded'] = df_dataset['sentiment'].\\\n        apply(lambda x: 0 if x == 'negative' else 1)\n\n    return df_dataset\n```", "```py\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom transformers import (\n    BertForSequenceClassification,\n    BertTokenizer,\n    get_linear_schedule_with_warmup)\n\nclass FineTuningPipeline:\n\n    def __init__(\n            self,\n            dataset,\n            tokenizer,\n            model,\n            optimizer,\n            loss_function = nn.CrossEntropyLoss(),\n            val_size = 0.1,\n            epochs = 4,\n            seed = 42):\n\n        self.df_dataset = dataset\n        self.tokenizer = tokenizer\n        self.model = model\n        self.optimizer = optimizer\n        self.loss_function = loss_function\n        self.val_size = val_size\n        self.epochs = epochs\n        self.seed = seed\n\n        # Check if GPU is available for faster training time\n        if torch.cuda.is_available():\n            self.device = torch.device('cuda:0')\n        else:\n            self.device = torch.device('cpu')\n\n        # Perform fine-tuning\n        self.model.to(self.device)\n        self.set_seeds()\n        self.token_ids, self.attention_masks = self.tokenize_dataset()\n        self.train_dataloader, self.val_dataloader = self.create_dataloaders()\n        self.scheduler = self.create_scheduler()\n        self.fine_tune()\n\n    def tokenize(self, text):\n        \"\"\" Tokenize input text and return the token IDs and attention mask.\n\n        Tokenize an input string, setting a maximum length of 512 tokens.\n        Sequences with more than 512 tokens will be truncated to this limit,\n        and sequences with less than 512 tokens will be supplemented with [PAD]\n        tokens to bring them up to this limit. The datatype of the returned\n        tensors will be the PyTorch tensor format. These return values are\n        tensors of size 1 x max_length where max_length is the maximum number\n        of tokens per input sequence (512 for BERT).\n\n        Parameters:\n            text (str): The text to be tokenized.\n\n        Returns:\n            token_ids (torch.Tensor): A tensor of token IDs for each token in\n                the input sequence.\n\n            attention_mask (torch.Tensor): A tensor of 1s and 0s where a 1\n                indicates a token can be attended to during the attention\n                process, and a 0 indicates a token should be ignored. This is\n                used to prevent BERT from attending to [PAD] tokens during its\n                training/inference.\n        \"\"\"\n        batch_encoder = self.tokenizer.encode_plus(\n            text,\n            max_length = 512,\n            padding = 'max_length',\n            truncation = True,\n            return_tensors = 'pt')\n\n        token_ids = batch_encoder['input_ids']\n        attention_mask = batch_encoder['attention_mask']\n\n        return token_ids, attention_mask\n\n    def tokenize_dataset(self):\n        \"\"\" Apply the self.tokenize method to the fine-tuning dataset.\n\n        Tokenize and return the input sequence for each row in the fine-tuning\n        dataset given by self.dataset. The return values are tensors of size\n        len_dataset x max_length where len_dataset is the number of rows in the\n        fine-tuning dataset and max_length is the maximum number of tokens per\n        input sequence (512 for BERT).\n\n        Parameters:\n            None.\n\n        Returns:\n            token_ids (torch.Tensor): A tensor of tensors containing token IDs\n            for each token in the input sequence.\n\n            attention_masks (torch.Tensor): A tensor of tensors containing the\n                attention masks for each sequence in the fine-tuning dataset.\n        \"\"\"\n        token_ids = []\n        attention_masks = []\n\n        for review in self.df_dataset['review_cleaned']:\n            tokens, masks = self.tokenize(review)\n            token_ids.append(tokens)\n            attention_masks.append(masks)\n\n        token_ids = torch.cat(token_ids, dim=0)\n        attention_masks = torch.cat(attention_masks, dim=0)\n\n        return token_ids, attention_masks\n\n    def create_dataloaders(self):\n        \"\"\" Create dataloaders for the train and validation set.\n\n        Split the tokenized dataset into train and validation sets according to\n        the self.val_size value. For example, if self.val_size is set to 0.1,\n        90% of the data will be used to form the train set, and 10% for the\n        validation set. Convert the \"sentiment_encoded\" column (labels for each\n        row) to PyTorch tensors to be used in the dataloaders.\n\n        Parameters:\n            None.\n\n        Returns:\n            train_dataloader (torch.utils.data.dataloader.DataLoader): A\n                dataloader of the train data, including the token IDs,\n                attention masks, and sentiment labels.\n\n            val_dataloader (torch.utils.data.dataloader.DataLoader): A\n                dataloader of the validation data, including the token IDs,\n                attention masks, and sentiment labels.\n\n        \"\"\"\n        train_ids, val_ids = train_test_split(\n                        self.token_ids,\n                        test_size=self.val_size,\n                        shuffle=False)\n\n        train_masks, val_masks = train_test_split(\n                                    self.attention_masks,\n                                    test_size=self.val_size,\n                                    shuffle=False)\n\n        labels = torch.tensor(self.df_dataset['sentiment_encoded'].values)\n        train_labels, val_labels = train_test_split(\n                                        labels,\n                                        test_size=self.val_size,\n                                        shuffle=False)\n\n        train_data = TensorDataset(train_ids, train_masks, train_labels)\n        train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)\n        val_data = TensorDataset(val_ids, val_masks, val_labels)\n        val_dataloader = DataLoader(val_data, batch_size=16)\n\n        return train_dataloader, val_dataloader\n\n    def create_scheduler(self):\n        \"\"\" Create a linear scheduler for the learning rate.\n\n        Create a scheduler with a learning rate that increases linearly from 0\n        to a maximum value (called the warmup period), then decreases linearly\n        to 0 again. num_warmup_steps is set to 0 here based on an example from\n        Hugging Face:\n\n        https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2\n        d008813037968a9e58/examples/run_glue.py#L308\n\n        Read more about schedulers here:\n\n        https://huggingface.co/docs/transformers/main_classes/optimizer_\n        schedules#transformers.get_linear_schedule_with_warmup\n        \"\"\"\n        num_training_steps = self.epochs * len(self.train_dataloader)\n        scheduler = get_linear_schedule_with_warmup(\n            self.optimizer,\n            num_warmup_steps=0,\n            num_training_steps=num_training_steps)\n\n        return scheduler\n\n    def set_seeds(self):\n        \"\"\" Set the random seeds so that results are reproduceable.\n\n        Parameters:\n            None.\n\n        Returns:\n            None.\n        \"\"\"\n        np.random.seed(self.seed)\n        torch.manual_seed(self.seed)\n        torch.cuda.manual_seed_all(self.seed)\n\n    def fine_tune(self):\n        \"\"\"Train the classification head on the BERT model.\n\n        Fine-tune the model by training the classification head (linear layer)\n        sitting on top of the BERT model. The model trained on the data in the\n        self.train_dataloader, and validated at the end of each epoch on the\n        data in the self.val_dataloader. The series of steps are described\n        below:\n\n        Training:\n\n        > Create a dictionary to store the average training loss and average\n          validation loss for each epoch.\n        > Store the time at the start of training, this is used to calculate\n          the time taken for the entire training process.\n        > Begin a loop to train the model for each epoch in self.epochs.\n\n        For each epoch:\n\n        > Switch the model to train mode. This will cause the model to behave\n          differently than when in evaluation mode (e.g. the batchnorm and\n          dropout layers are activated in train mode, but disabled in\n          evaluation mode).\n        > Set the training loss to 0 for the start of the epoch. This is used\n          to track the loss of the model on the training data over subsequent\n          epochs. The loss should decrease with each epoch if training is\n          successful.\n        > Store the time at the start of the epoch, this is used to calculate\n          the time taken for the epoch to be completed.\n        > As per the BERT authors' recommendations, the training data for each\n          epoch is split into batches. Loop through the training process for\n          each batch.\n\n        For each batch:\n\n        > Move the token IDs, attention masks, and labels to the GPU if\n          available for faster processing, otherwise these will be kept on the\n          CPU.\n        > Invoke the zero_grad method to reset the calculated gradients from\n          the previous iteration of this loop.\n        > Pass the batch to the model to calculate the logits (predictions\n          based on the current classifier weights and biases) as well as the\n          loss.\n        > Increment the total loss for the epoch. The loss is returned from the\n          model as a PyTorch tensor so extract the float value using the item\n          method.\n        > Perform a backward pass of the model and propagate the loss through\n          the classifier head. This will allow the model to determine what\n          adjustments to make to the weights and biases to improve its\n          performance on the batch.\n        > Clip the gradients to be no larger than 1.0 so the model does not\n          suffer from the exploding gradients problem.\n        > Call the optimizer to take a step in the direction of the error\n          surface as determined by the backward pass.\n\n        After training on each batch:\n\n        > Calculate the average loss and time taken for training on the epoch.\n\n        Validation step for the epoch:\n\n        > Switch the model to evaluation mode.\n        > Set the validation loss to 0\\. This is used to track the loss of the\n          model on the validation data over subsequent epochs. The loss should\n          decrease with each epoch if training was successful.\n        > Store the time at the start of the validation, this is used to\n          calculate the time taken for the validation for this epoch to be\n          completed.\n        > Split the validation data into batches.\n\n        For each batch:\n\n        > Move the token IDs, attention masks, and labels to the GPU if\n          available for faster processing, otherwise these will be kept on the\n          CPU.\n        > Invoke the no_grad method to instruct the model not to calculate the\n          gradients since we wil not be performing any optimization steps here,\n          only inference.\n        > Pass the batch to the model to calculate the logits (predictions\n          based on the current classifier weights and biases) as well as the\n          loss.\n        > Extract the logits and labels from the model and move them to the CPU\n          (if they are not already there).\n        > Increment the loss and calculate the accuracy based on the true\n          labels in the validation dataloader.\n        > Calculate the average loss and accuracy, and add these to the loss\n          dictionary.\n        \"\"\"\n\n        loss_dict = {\n            'epoch': [i+1 for i in range(self.epochs)],\n            'average training loss': [],\n            'average validation loss': []\n        }\n\n        t0_train = datetime.now()\n\n        for epoch in range(0, self.epochs):\n\n            # Train step\n            self.model.train()\n            training_loss = 0\n            t0_epoch = datetime.now()\n\n            print(f'{\"-\"*20} Epoch {epoch+1} {\"-\"*20}')\n            print('\\nTraining:\\n---------')\n            print(f'Start Time:       {t0_epoch}')\n\n            for batch in self.train_dataloader:\n\n                batch_token_ids = batch[0].to(self.device)\n                batch_attention_mask = batch[1].to(self.device)\n                batch_labels = batch[2].to(self.device)\n\n                self.model.zero_grad()\n\n                loss, logits = self.model(\n                    batch_token_ids,\n                    token_type_ids = None,\n                    attention_mask=batch_attention_mask,\n                    labels=batch_labels,\n                    return_dict=False)\n\n                training_loss += loss.item()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                self.optimizer.step()\n                self.scheduler.step()\n\n            average_train_loss = training_loss / len(self.train_dataloader)\n            time_epoch = datetime.now() - t0_epoch\n\n            print(f'Average Loss:     {average_train_loss}')\n            print(f'Time Taken:       {time_epoch}')\n\n            # Validation step\n            self.model.eval()\n            val_loss = 0\n            val_accuracy = 0\n            t0_val = datetime.now()\n\n            print('\\nValidation:\\n---------')\n            print(f'Start Time:       {t0_val}')\n\n            for batch in self.val_dataloader:\n\n                batch_token_ids = batch[0].to(self.device)\n                batch_attention_mask = batch[1].to(self.device)\n                batch_labels = batch[2].to(self.device)\n\n                with torch.no_grad():\n                    (loss, logits) = self.model(\n                        batch_token_ids,\n                        attention_mask = batch_attention_mask,\n                        labels = batch_labels,\n                        token_type_ids = None,\n                        return_dict=False)\n\n                logits = logits.detach().cpu().numpy()\n                label_ids = batch_labels.to('cpu').numpy()\n                val_loss += loss.item()\n                val_accuracy += self.calculate_accuracy(logits, label_ids)\n\n            average_val_accuracy = val_accuracy / len(self.val_dataloader)\n            average_val_loss = val_loss / len(self.val_dataloader)\n            time_val = datetime.now() - t0_val\n\n            print(f'Average Loss:     {average_val_loss}')\n            print(f'Average Accuracy: {average_val_accuracy}')\n            print(f'Time Taken:       {time_val}\\n')\n\n            loss_dict['average training loss'].append(average_train_loss)\n            loss_dict['average validation loss'].append(average_val_loss)\n\n        print(f'Total training time: {datetime.now()-t0_train}')\n\n    def calculate_accuracy(self, preds, labels):\n        \"\"\" Calculate the accuracy of model predictions against true labels.\n\n        Parameters:\n            preds (np.array): The predicted label from the model\n            labels (np.array): The true label\n\n        Returns:\n            accuracy (float): The accuracy as a percentage of the correct\n                predictions.\n        \"\"\"\n        pred_flat = np.argmax(preds, axis=1).flatten()\n        labels_flat = labels.flatten()\n        accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n        return accuracy\n\n    def predict(self, dataloader):\n        \"\"\"Return the predicted probabilities of each class for input text.\n\n        Parameters:\n            dataloader (torch.utils.data.DataLoader): A DataLoader containing\n                the token IDs and attention masks for the text to perform\n                inference on.\n\n        Returns:\n            probs (PyTorch.Tensor): A tensor containing the probability values\n                for each class as predicted by the model.\n\n        \"\"\"\n\n        self.model.eval()\n        all_logits = []\n\n        for batch in dataloader:\n\n            batch_token_ids, batch_attention_mask = tuple(t.to(self.device) \\\n                for t in batch)[:2]\n\n            with torch.no_grad():\n                logits = self.model(batch_token_ids, batch_attention_mask)\n\n            all_logits.append(logits)\n\n        all_logits = torch.cat(all_logits, dim=0)\n\n        probs = F.softmax(all_logits, dim=1).cpu().numpy()\n        return probs\n```", "```py\n# Initialise parameters\ndataset = preprocess_dataset('IMDB Dataset Very Small.csv')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=2)\noptimizer = AdamW(model.parameters())\n\n# Fine-tune model using class\nfine_tuned_model = FineTuningPipeline(\n    dataset = dataset,\n    tokenizer = tokenizer,\n    model = model,\n    optimizer = optimizer,\n    val_size = 0.1,\n    epochs = 2,\n    seed = 42\n    )\n\n# Make some predictions using the validation dataset\nmodel.predict(model.val_dataloader)\n```"]