<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>FormulaFeatures: A Tool to Generate Highly Predictive Features for Interpretable Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>FormulaFeatures: A Tool to Generate Highly Predictive Features for Interpretable Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/formulafeatures-a-tool-to-generate-highly-predictive-features-for-interpretable-models-e18aab45e96d?source=collection_archive---------0-----------------------#2024-10-06">https://towardsdatascience.com/formulafeatures-a-tool-to-generate-highly-predictive-features-for-interpretable-models-e18aab45e96d?source=collection_archive---------0-----------------------#2024-10-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f11a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Create more interpretable models by using concise, highly predictive features, automatically engineered based on arithmetic combinations of numeric features</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@wkennedy934?source=post_page---byline--e18aab45e96d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="W Brett Kennedy" class="l ep by dd de cx" src="../Images/b3ce55ffd028167326c117d47c64c467.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*v8pf2r3SPMLuHoSmF4IwlA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e18aab45e96d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@wkennedy934?source=post_page---byline--e18aab45e96d--------------------------------" rel="noopener follow">W Brett Kennedy</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e18aab45e96d--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">32 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ld le">3</span></p></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="lj k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lk an ao ap ig ll lm ln" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lo cn"><div class="l ae"><div class="ab cb"><div class="lp lq lr ls lt lu ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="77e2" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this article, we examine a tool called <a class="af ni" href="https://github.com/Brett-Kennedy/FormulaFeatures/tree/main" rel="noopener ugc nofollow" target="_blank">FormulaFeatures</a>. This is intended for use primarily with interpretable models, such as shallow decision trees, where having a small number of concise and highly predictive features can aid greatly with the interpretability and accuracy of the models.</p><h1 id="d712" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Interpretable Models in Machine Learning</h1><p id="e9d4" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">This article continues my series on interpretable machine learning, following articles on <a class="af ni" href="https://medium.com/towards-data-science/interpretable-knn-iknn-33d38402b8fc" rel="noopener">ikNN</a>, <a class="af ni" href="https://medium.com/towards-data-science/additive-decision-trees-85f2feda2223" rel="noopener">Additive Decision Trees</a>, <a class="af ni" href="https://medium.com/towards-data-science/create-stronger-decision-trees-with-bootstrapping-and-genetic-algorithms-1ae633a993c9" rel="noopener">Genetic Decision Trees</a>, and <a class="af ni" href="https://medium.com/towards-data-science/prism-rules-in-python-14d2cfd801a3" rel="noopener">PRISM rules</a>.</p><p id="c48a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As indicated in the previous articles (and covered there in more detail), there is often a strong incentive to use interpretable predictive models: each prediction can be well understood, and we can be confident the model will perform sensibly on future, unseen data.</p><p id="4168" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">There are a number of models available to provide interpretable ML, although, unfortunately, well less than we would likely wish. There are the models described in the articles linked above, as well as a small number of others, for example, decision trees, decision tables, rule sets and rule lists (created, for example by <a class="af ni" href="https://github.com/csinva/imodels" rel="noopener ugc nofollow" target="_blank">imodels</a>), <a class="af ni" href="https://arxiv.org/abs/1904.12847" rel="noopener ugc nofollow" target="_blank">Optimal Sparse Decision Trees</a>, GAMs (Generalized Additive Models, such as <a class="af ni" href="https://interpret.ml/docs/ebm.html" rel="noopener ugc nofollow" target="_blank">Explainable Boosted Machines</a>), as well as a few other options.</p><p id="76a9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In general, creating predictive machine learning models that are both accurate and interpretable is challenging. To improve the options available for interpretable ML, four of the main approaches are to:</p><ol class=""><li id="d25d" class="mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ok ol om bk">Develop additional model types</li><li id="d1ff" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh ok ol om bk">Improve the accuracy or interpretability of existing model types. For this, I’m referring to creating variations on existing model types, or the algorithms used to create the models, as opposed to completely novel models. For example, <a class="af ni" href="https://arxiv.org/abs/1904.12847" rel="noopener ugc nofollow" target="_blank">Optimal Sparse Decision Trees</a> and <a class="af ni" href="https://medium.com/towards-data-science/create-stronger-decision-trees-with-bootstrapping-and-genetic-algorithms-1ae633a993c9" rel="noopener">Genetic Decision Trees</a> seek to create stronger decision trees, but in the end, are still decision trees.</li><li id="7e2c" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh ok ol om bk">Provide visualizations of the data, model, and predictions made by the model. This is the approach taken, for example, by <a class="af ni" href="https://medium.com/towards-data-science/interpretable-knn-iknn-33d38402b8fc" rel="noopener">ikNN</a>, which works by creating an ensemble of 2D kNN models (that is, ensembles of kNN models that each use only a single pair of features). The 2D spaces may be visualized, which provides a high degree of visibility into how the model works and why it made each prediction as it did.</li><li id="26e2" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh ok ol om bk">Improve the quality of the features that are used by the models, in order that models can be either more accurate or more interpretable.</li></ol><p id="072b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">FormulaFeatures is used to support the last of these approaches. It was developed by myself to address a common issue in decision trees: they can often achieve a high level of accuracy, but only when grown to a large depth, which then precludes any interpretability. Creating new features that capture part of the function linking the original features to the target can allow for much more compact (and therefore interpretable) decision trees.</p><p id="40b8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The underlying idea is: for any labelled dataset, there is some true function, f(x) that maps the records to the target column. This function may take any number of forms, may be simple or complex, and may use any set of features in x. But regardless of the nature of f(x), by creating a model, we hope to approximate f(x) as well as we can given the data available. To create an interpretable model, we also need to do this clearly and concisely.</p><p id="7ffb" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">If the features themselves can capture a significant part of the function, this can be very helpful. For example, we may have a model that predicts client churn and we may have features for each client including: their number of purchases in the last year, and the average value of their purchases in the last year. The true f(x), though, may be based primarily on the product of these (the total value of their purchases in the last year, which is found by multiplying these two features).</p><p id="e2c9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In practice, we will generally never know the true f(x), but in this case, let’s assume that whether a client churns in the next year is related strongly to their total purchases in the prior year, and not strongly to their number of purchase or their average size.</p><p id="cf6a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can likely build an accurate model using just the two original features, but a model using just the product feature will be more clear and interpretable. And possibly more accurate.</p><h1 id="2b06" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Example using a decision tree</h1><p id="ff7a" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">If we have only two features, then we can view them in a 2d plot. In this case, we can look at just num_purc and avg_purc: the number of purchases in the last year per client, and their average dollar value. Assuming the true f(x) is based primarily on their product, the space may look like the plot below, where the light blue area represents client who will churn in the next year, and the dark blue those who will not.</p><p id="09cf" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">If using a decision tree to model this, we can create a model by dividing the data space recursively. The orange lines on the plot show a plausible set of splits a decision tree may use (for the first set of nodes) to try to predict churn. It may, as shown, first split on num_purc at a value of 250, then avg_purc at 24, and so on. It would then continue to make splits in order to fit the curved shape of the true function. The more it makes, the closer it can come to fitting the true function.</p><figure class="ov ow ox oy oz pa os ot paragraph-image"><div class="os ot ou"><img src="../Images/aa0c8bd9d474799c3cdb376ee63c188c.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*GM4Y7ICahOsmtWCJySUWHQ.png"/></div></figure><p id="6607" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Doing this will create a decision tree that looks something like the tree below, where the circles represent internal nodes, the rectangles represent the leaf nodes, and ellipses the sub-trees that would likely need to be grown several more levels deep to achieve decent accuracy. That is, this shows only a fraction of the full tree that would need to be grown to model this using these two features. We can see in the plot above as well: using axis-parallel splits, we will need a large number of splits to fit the boundary between the two classes well.</p><figure class="ov ow ox oy oz pa os ot paragraph-image"><div class="os ot pc"><img src="../Images/348a0f8cd2589ba1520923269e107d4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*OUOa03sxmrVlmRydk2g_RQ.png"/></div></figure><p id="ec1d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">If the tree is grown sufficiently, we can likely get a strong tree in terms of accuracy. But, the tree will be far from interpretable.</p><p id="9777" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It is possible to view the decision space, as in the plot above (and this does make the behaviour of the model clear), but this is only feasible here because the space is limited to two dimensions. Normally this is impossible, and our best means to interpret the decision tree is to examine the tree itself. But, where the tree has many dozens of nodes or more, it becomes impossible to see the patterns it is working to capture.</p><p id="ca59" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this case, if we engineered a feature for num_purc * avg_purc, we could have a very simple decision tree, with just a single internal node, with the split point: num_purc * avg_purc &gt; 25000.</p><p id="33e8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In practice, it’s never possible to produce features that are this close to the true function, and it’s never possible to create a fully accurate decision trees with very few nodes. But it is often quite possible to engineer features that are closer to the true f(x) than the original features.</p><p id="94b7" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Whenever there are interactions between features, if we can capture these with engineered features, this will allow for more compact models.</p><p id="476d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">So, with FormulaFeatures, we attempt to create features such as num_purchases * avg_value_of_purchases, and they can quite often be used in models such as decision trees to capture the true function reasonably well.</p><p id="dc58" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As well, simply knowing that num_purchases * avg_value_of_purchases is predictive of the target (and that higher values are associated with lower risk of churn) in itself is informative. But the new feature is most useful in the context of seeking to make interpretable models more accurate and more interpretable.</p><p id="85a7" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As we’ll describe below, FormulaFeatures also does this in a way that minimizing creating other features, so that only a small set of features, all relevant, are returned.</p><h1 id="1d5b" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Interpretable machine learning with decision trees</h1><p id="6fd7" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">With tabular data, the top-performing models for prediction problems are typically boosted tree-based ensembles, particularly LGBM, XGBoost, and CatBoost. It will vary from one prediction problem to another, but most of the time, these three models tend to do better than other models (and are considered, at least outside of AutoML approaches, the current state of the art). Other strong model types such as kNNs, neural networks, Bayesian Additive Regression Trees, SVMs, and others will also occasionally perform the best. All of these models types are, though, quite uninterpretable, and are effectively black-boxes.</p><p id="7728" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Unfortunately, interpretable models tend to be weaker than these with respect to accuracy. Sometimes, the drop in accuracy is fairly small (for example, in the 3rd decimal), and it’s worth sacrificing some accuracy for interpretability. In other cases, though, interpretable models may do substantially worse than the black-box alternatives. It’s difficult, for example for a single decision tree to compete with an ensemble of many decision trees.</p><p id="b616" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">So, it’s common to be able to create a strong black-box model, but at the same time for it to be challenging (or impossible) to create a strong interpretable model. This is the problem FormulaFeatures was designed to address. It seeks to capture some of logic that black-box models can represent, but in a simple, understandable way.</p><p id="4bcd" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Much of the research done in interpretable AI focusses on decision trees, and relates to making decision trees more accurate and more interpretable. This is fairly natural, as decision trees are a model type that’s inherently straight-forward to understand (when sufficiently small, they are arguably as interpretable as any other model) and often reasonably accurate (though this is very often not the case).</p><p id="0013" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Other interpretable models types (e.g. logistic regression, rules, GAMs, etc.) are used as well, but much of the research is focused on decision trees, and so this article works, for the most part, with decision trees. Nevertheless, FormulaFeatures is not specific to decision trees, and can be useful for other interpretable models. In fact, it’s fairly easy to see, once we explain FormulaFeatures below, how it may be applied as well to ikNN, Genetic Decision Trees, Additive Decision Trees, rules lists, rule sets, and so on.</p><p id="3936" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To be more precise with respect to decision trees, when using these for interpretable ML, we are looking specifically at <em class="pd">shallow </em>decision trees — trees that have relatively small depths, with the deepest nodes being restricted to perhaps 3, 4, or 5 levels. This ensures two things: that shallow decision trees can provide both what are called <em class="pd">local explanations </em>and what are called <em class="pd">global explanations</em>. These are the two main concerns with interpretable ML. I’ll explain these here.</p><p id="bc05" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">With local interpretability, we want to ensure that each individual prediction made by the model is understandable. Here, we can examine the decision path taken through the tree by each record for which we generate a decision. If a path includes the feature num_purc * avg_purc, and the path is very short, it can be reasonably clear. On the other hand, a path that includes: num_purc &gt; 250 AND avg_purc &gt; 24 AND num_purc &lt; 500 AND avg_purc_50, and so on (as in the tree generated above without the benefit of the num_purc * avg_pur feature) can become very difficult to interpret.</p><p id="e718" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">With global interpretability, we want to ensure that the model as a whole is understandable. This allows us to see the predictions that would be made under any circumstances. Again, using more compact trees, and where the features themselves are informative, can aid with this. It’s much simpler, in this case, to see the big picture of how the decision tree outputs predictions.</p><p id="ba55" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We should qualify this, though, by indicating that shallow decision trees are very difficult to create in a way that’s accurate for regression problems. Each leaf node can predict only a single value, and so a tree with n leaf nodes can only output, at most, n unique predictions. For regression problems, this usually results in high error rates: normally decision trees need to create a large number of leaf nodes in order to cover the full range of values that can be potentially predicted, with each node having reasonable precision.</p><p id="7fcc" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Consequently, shallow decision trees tend to be practical only for classification problems (if there are only a small number of classes that can be predicted, it is quite possible to create a decision tree with not too many leaf nodes to predict these accurately). FormulaFeatures can be useful for use with other interpretable regression models, but not typically with decision trees.</p><h1 id="c571" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Supervised and unsupervised feature engineering</h1><p id="c5b8" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">Now that we’ve seen some of the motivation behind FormulaFeatures, we’ll take a look at how it works.</p><p id="7d1a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">FormulaFeatures is a form of supervised feature engineering, which is to say that it considers the target column when producing features, and so can generate features specifically useful for predicting that target. FormulaFeatures supports both regression &amp; classification targets (though as indicated, when using decision trees, it may be that only classification targets are feasible).</p><p id="426e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Taking advantage of the target column allows it to generate only a small number of engineered features, each as simple or complex as necessary.</p><p id="92be" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Unsupervised methods, on the other hand, do not take the target feature into consideration, and simply generate all possible combinations of the original features using some system for generating features.</p><p id="d9e0" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">An example of this is scikit-learn’s <a class="af ni" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html" rel="noopener ugc nofollow" target="_blank">PolynomialFeatures</a>, which will generate all polynomial combinations of the features. If the original features are, say: [a, b, c], then PolynomialFeatures can create (depending on the parameters specified) a set of engineered features such as: [ab, ac, bc, a², b², c²] — that is, it will generate all combinations of pairs of features (using multiplication), as well as all original features raised to the 2nd degree.</p><p id="0a35" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Using unsupervised methods, there is very often an explosion in the number of features created. If we have 20 features to start with, returning just the features created by multiplying each pair of features would generate (20 * 19) / 2, or 190 features (that is, 20 choose 2). If allowed to create features based on multiplying sets of three features, there are 20 choose 3, or 1140 of these. Allowing features such as a²bc, a²bc², and so on results in even more massive numbers of features (though with a small set of useful features being, quite possibly, among these).</p><p id="80b2" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Supervised feature engineering methods would tend to return only a much smaller (and more relevant) subset of these.</p><p id="2606" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">However, even within the context of supervised feature engineering (depending on the specific approach used), an explosion in features may still occur to some extent, resulting in a time consuming feature engineering process, as well as producing more features than can be reasonably used by any downstream tasks, such as prediction, clustering, or outlier detection. FormulaFeatures is optimized to keep both the engineering time, and the number of features returned, tractable, and its algorithm is designed to limit the numbers of features generated.</p><h1 id="403f" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Algorithm</h1><p id="c910" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">The tool operates on the numeric features of a dataset. In the first iteration, it examines each pair of original numeric features. For each, it considers four potential new features based on the four basic arithmetic operations (+, -, *, and /). For the sake of performance, and interpretability, we limit the process to these four operations.</p><p id="eef9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">If any perform better than both parent features (in terms of their ability to predict the target — described soon), then the strongest of these is added to the set of features. For example, if A + B and A * B are both strong features (both stronger than either A or B), only the stronger of these will be included.</p><p id="5bf7" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Subsequent iterations then consider combining all features generated in the previous iteration will all other features, again taking the strongest of these, if any outperformed their two parent features. In this way, a practical number of new features are generated, all stronger than the previous features.</p><h1 id="c096" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Example stepping through the algorithm</h1><p id="3bc0" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">Assume we start with a dataset with features A, B, and C, that Y is the target, and that Y is numeric (this is a regression problem).</p><p id="7b5e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We start by determining how predictive of the target each feature is on its own. The currently-available version uses R2 for regression problems and F1 (macro) for classification problems. We create a simple model (a classification or regression decision tree) using only a single feature, determine how well it predicts the target column, and measure this with either R2 or F1 scores.</p><p id="1f40" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Using a decision tree allows us to capture reasonably well the relationships between the feature and target — even fairly complex, non-monotonic relationships — where they exist.</p><p id="e9e0" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Future versions will support more metrics. Using strictly R2 and F1, however, is not a significant limitation. While other metrics may be more relevant for your projects, using these metrics internally when engineering features will identify well the features that are strongly associated with the target, even if the strength of the association is not identical as it would be found using other metrics.</p><p id="7c41" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this example, we begin with calculating the R2 for each original feature, training a decision tree using only feature A, then another using only B, and then again using only C. This may give the following R2 scores:</p><pre class="ov ow ox oy oz pe pf pg bp ph bb bk"><span id="cf73" class="pi nk fq pf b bg pj pk l pl pm">A   0.43<br/>B   0.02<br/>C  -1.23</span></pre><p id="4752" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We then consider the combinations of pairs of these, which are: A &amp; B, A &amp; C, and B &amp; C. For each we try the four arithmetic operations: +, *, -, and /.</p><p id="760f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Where there are feature interactions in f(x), it will often be that a new feature incorporating the relevant original features can represent the interactions well, and so outperform either parent feature.</p><p id="bd43" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">When examining A &amp; B, assume we get the following R2 scores:</p><pre class="ov ow ox oy oz pe pf pg bp ph bb bk"><span id="58ba" class="pi nk fq pf b bg pj pk l pl pm">A + B  0.54<br/>A * B  0.44<br/>A - B  0.21<br/>A / B  -0.01</span></pre><p id="5426" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Here there are two operations that have a higher R2 score than either parent feature (A or B), which are + and *. We take the highest of these, A + B, and add this to the set of features. We do the same for A &amp; B and B &amp; C. In most cases, no feature will be added, but often one is.</p><p id="a441" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">After the first iteration we may have:</p><pre class="ov ow ox oy oz pe pf pg bp ph bb bk"><span id="f2cc" class="pi nk fq pf b bg pj pk l pl pm">A       0.43<br/>B       0.02<br/>C      -1.23<br/>A + B   0.54<br/>B / C   0.32</span></pre><p id="b200" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We then, in the next iteration, take the two features just added, and try combining them with all other features, including each other.</p><p id="5ba8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">After this we may have:</p><pre class="ov ow ox oy oz pe pf pg bp ph bb bk"><span id="d09c" class="pi nk fq pf b bg pj pk l pl pm">A                   0.43<br/>B                   0.02<br/>C                  -1.23<br/>A + B               0.54<br/>B / C               0.32<br/>(A + B) - C         0.56<br/>(A + B) * (B / C)   0.66</span></pre><p id="7e90" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This continues until there is no longer improvement, or a limit specified by a hyperparameter, max_iterations, is reached.</p><h1 id="fbe4" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Further pruning based on correlations</h1><p id="15a0" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">At the end of each iteration, further pruning of the features is performed, based on correlations. The correlation among the features created during the current iteration is examined, and where two or more features that are highly correlated were created, only the strongest is kept, removing the others. This limits creating near-redundant features, which can become possible, especially as the features become more complex.</p><p id="0b6a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example: (A + B + C) / E and (A + B + D) / E may both be strong, but quite similar, and if so, only the stronger of these will be kept.</p><p id="ea9e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">One allowance for correlated features is made, though. In general, as the algorithm proceeds, more complex features are created, and these features more accurately capture the true relationship between the features in x and the target. But, the new features created may also be correlated with the features they build upon, which are simpler, and FormulaFeatures also seeks to favour simpler features over more complex, everything else equal.</p><p id="b502" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, if (A + B + C) is correlated with (A + B), both would be kept even if (A + B + C) is stronger, in order that the simpler (A + B) may be combined with other features in subsequent iterations, possibly creating features that are stronger still.</p><h1 id="db1f" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">How FormulaFeatures limits the features created</h1><p id="ec64" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">In the example above, we have features A, B, and C, and see that part of the true f(x) can be approximated with (A + B) - C.</p><p id="f44d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We initially have only the original features. After the first iteration, we may generate (again, as in the example above) A + B and B / C, so now have five features.</p><p id="2d33" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In the next iteration, we may generate (A + B) — C.</p><p id="20fb" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This process is, in general, a combination of: 1) combining weak features to make them stronger (and more likely useful in a downstream task); as well as 2) combining strong features to make these even stronger, creating what are most likely the most predictive features.</p><p id="f6e8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">But, what’s important is that this combining is done only after it’s confirmed that A + B is a predictive feature in itself, more so than either A or B. That is, we do not create (A + B) — C until we confirm that A + B is predictive. This ensures that, for any complex features created, each component within them is useful.</p><p id="5070" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this way, each iteration creates a more powerful set of features than the previous, and does so in a way that’s reliable and stable. It minimizes the effects of simply trying many complex combinations of features, which can easily overfit.</p><p id="4a9d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">So, FormulaFeatures, executes in a principled, deliberate manner, creating only a small number of engineered features each step, and typically creates less features each iteration. As such, it, overall, favours creating features with low complexity. And, where complex features are generated, this can be shown to be justified.</p><p id="f772" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">With most datasets, in the end, the features engineered are combinations of just two or three original features. That is, it will usually create features more similar to A * B than to, say, (A * B) / (C * D).</p><p id="beed" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In fact, to generate a features such as (A * B) / (C * D), it would need to demonstrate that A * B is more predictive than either A or B, that C * D is more predictive that C or D, and that (A * B) / (C * D) is more predictive than either (A * B) or (C * D). As that’s a lot of conditions, relatively few features as complex as (A * B) / (C * D) will tend to be created, many more like A * B.</p><h1 id="74c6" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Using 1D decision trees internally to evaluate the features</h1><p id="e412" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">We’ll look here closer at using decision trees internally to evaluate each feature, both the original and the engineered features.</p><p id="dd0a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To evaluate the features, other methods are available, such as simple correlation tests. But creating simple, non-parametric models, and specifically decision trees, has a number of advantages:</p><ul class=""><li id="7611" class="mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh pn ol om bk">1D models are fast, both to train and to test, which allows the evaluation process to execute very quickly. We can quickly determine which engineered features are predictive of the target, and how predictive they are.</li><li id="2c0c" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh pn ol om bk">1D models are simple and so may reasonably be trained on small samples of the data, further improving efficiency.</li><li id="3eae" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh pn ol om bk">While 1D decision tree models are relatively simple, they can capture non-monotonic relationships between the features and the target, so can detect where features are predictive even where the relationships are complex enough to be missed by simpler tests, such as tests for correlation.</li><li id="a318" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh pn ol om bk">This ensures all features useful in themselves, so supports the features being a form of interpretability in themselves.</li></ul><p id="dac3" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">There are also some limitations of using 1D models to evaluate each feature, particularly: using single features precludes identifying effective combinations of features. This may result in missing some useful features (features that are not useful by themselves but are useful in combination with other features), but does allow the process to execute very quickly. It also ensures that all features produced are predictive on their own, which does aid in interpretability.</p><p id="9a77" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The goal is that: where features are useful only in combination with other features, a new feature is created to capture this.</p><p id="5f76" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Another limitation associated with this form of feature engineering is that almost all engineered features will have global significance, which is often desirable, but it does mean the tool can miss additionally generating features that are useful only in specific sub-spaces. However, given that the features will be used by interpretable models, such as shallow decision trees, the value of features that are predictive in only specific sub-spaces is much lower than where more complex models (such as large decision trees) are used.</p><h1 id="d17d" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Implications for the complexity of decision trees</h1><p id="fb46" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">FormulaFeatures does create features that are inherently more complex than the original features, which does lower the interpretability of the trees (assuming the engineered features are used by the trees one or more times).</p><p id="b52c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">At the same time, using these features can allow substantially smaller decision trees, resulting in a model that is, over all, more accurate and more interpretable. That is, even though the features used in a tree may be complex, the tree, may be substantially smaller (or substantially more accurate when keeping the size to a reasonable level), resulting in a net gain in interpretability.</p><p id="0460" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">When FormulaFeatures is used with shallow decision trees, the engineered features generated tend to be put at the top of the trees (as these are the most powerful features, best able to maximize information gain). No single feature can ever split the data perfectly at any step, which means further splits are almost always necessary. Other features are used lower in the tree, which tend to be simpler engineered features (based only only two, or sometimes three, original features), or the original features. On the whole, this can produce fairly interpretable decision trees, and tends to limit the use of the more complex engineered features to a useful level.</p><h1 id="a2c4" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">ArithmeticFeatures</h1><p id="a513" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">To explain better some of the context for FormulaFeatures, I’ll describe another tool, also developed by myself, called <a class="af ni" href="https://github.com/Brett-Kennedy/ArithmeticFeatures" rel="noopener ugc nofollow" target="_blank">ArithmeticFeatures</a>, which is similar but somewhat simpler. We’ll then look at some of the limitations associated with ArithmeticFeatures that FormulaFeatures was designed to address.</p><p id="37d4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">ArithmeticFeatures is a simple tool, but one I’ve found useful in a number of projects. I initially created it, as it was a recurring theme that it was useful to generate a set of simple arithmetic combinations of the numeric features available for various projects I was working on. I then hosted it on <a class="af ni" href="https://github.com/Brett-Kennedy/ArithmeticFeatures" rel="noopener ugc nofollow" target="_blank">github</a>.</p><p id="cd22" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Its purpose, and its signature, are similar to scikit-learn’s PolynomialFeatures. It’s also an unsupervised feature engineering tool.</p><p id="3e60" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Given a set of numeric features in a dataset, it generates a collection of new features. For each pair of numeric features, it generates four new features: the result of the +, -, * and / operations.</p><p id="5e96" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This can generate a set of features that are useful, but also generates a very large set of features, and potentially redundant features, which means feature selection is necessary after using this.</p><p id="086a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Formula Features was designed to address the issue that, as indicated above, frequently occurs with unsupervised feature engineering tools including ArithmeticFeatures: an explosion in the numbers of features created. With no target to guide the process, they simply combine the numeric features in as many ways are are possible.</p><p id="b289" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To quickly list the differences:</p><ul class=""><li id="4c91" class="mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh pn ol om bk">FormulaFeatures will generate far fewer features, but each that it generates will be known to be useful. ArithmeticFeatures provides no check as to which features are useful. It will generate features for every combination of original features and arithmetic operation.</li><li id="2a9e" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh pn ol om bk">FormulaFeatures will only generate features that are more predictive than either parent feature.</li><li id="7fd8" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh pn ol om bk">For any given pair of features, FormulaFeatures will include at most one combination, which is the one that is most predictive of the target.</li><li id="7ce5" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh pn ol om bk">FormulaFeatures will continue looping for either a specified number of iterations, or so long as it is able to create more powerful features, and so can create more powerful features than ArithmeticFeatures, which is limited to features based on pairs of original features.</li></ul><p id="a20c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">ArithmeticFeatures, as it executes only one iteration (in order to manage the number of features produced), is often quite limited in what it can create.</p><p id="b29a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Imagine a case where the dataset describes houses and the target feature is the house price. This may be related to features such as num_bedrooms, num_bathrooms and num_common rooms. Likely it is strongly related to the total number of rooms, which, let’s say, is: num_bedrooms + num_bathrooms + num_common rooms. ArithmeticFeatures, however is only able to produce engineered features based on pairs of original features, so can produce:</p><ul class=""><li id="22cc" class="mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh pn ol om bk">num_bedrooms + num_bathrooms</li><li id="906c" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh pn ol om bk">num_bedrooms + num_common rooms</li><li id="9517" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh pn ol om bk">num_bathrooms + num_common rooms</li></ul><p id="59e5" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">These may be informative, but producing num_bedrooms + num_bathrooms + num_common rooms (as FormulaFeatures is able to do) is both more clear as a feature, and allows more concise trees (and other interpretable models) than using features based on only pairs of original features.</p><p id="ec84" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Another popular feature engineering tool based on arithmetic operations is <a class="af ni" href="https://github.com/cod3licious/autofeat" rel="noopener ugc nofollow" target="_blank">AutoFeat</a>, which works similarly to ArithmeticFeatures, and also executes in an unsupervised manner, so will create a very large number of features. AutoFeat is able it to execute for multiple iterations, creating progressively more complex features each iterations, but with increasing large numbers of them. As well, AutoFeat supports unary operations, such as square, square root, log and so on, which allows for features such as A²/log(B).</p><p id="daa4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">So, I’ve gone over the motivations to create, and to use, FormulaFeatures over unsupervised feature engineering, but should also say: unsupervised methods such as PolynomialFeatures, ArithmeticFeatures, and AutoFeat are also often useful, particularly where feature selection will be performed in any case.</p><p id="1942" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">FormulaFeatures focuses more on interpretability (and to some extent on memory efficiency, but the primary motivation was interpretability), and so has a different purpose.</p><h1 id="7b06" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Feature selection with feature engineering</h1><p id="4011" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">Using unsupervised feature engineering tools such as PolynomialFeatures, ArithmeticFeatures, and AutoFeat increases the need for feature selection, but feature selection is generally performed in any case.</p><p id="b2c9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">That is, even if using a supervised feature engineering method such as FormulaFeatures, it will generally be useful to perform some feature selection after the feature engineering process. In fact, even if the feature engineering process produces no new features, feature selection is likely still useful simply to reduce the number of the original features used in the model.</p><p id="069d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">While FormulaFeatures seeks to minimize the number of features created, it does not perform feature selection per se, so can generate more features than will be necessary for any given task. We assume the engineered features will be used, in most cases, for a prediction task, but the relevant features will still depend on the specific model used, hyperparameters, evaluation metrics, and so on, which FormulaFeatures cannot predict</p><p id="a45f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">What can be relevant is that, using FormulaFeatures, as compared to many other feature engineering processes, the feature selection work, if performed, can be a much simpler process, as there will be far few features to consider. Feature selection can become slow and difficult when working with many features. For example, wrapper methods to select features become intractable.</p><h1 id="7201" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">API Signature</h1><p id="9216" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">The tool uses the fit-transform pattern, the same as that used by scikit-learn’s PolynomialFeatures and many other feature engineering tools (including ArithmeticFeatures). As such, it’s easy to substitute this tool for others to determine which is the most useful for any given project.</p><h1 id="8e8a" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Simple code example</h1><p id="7b64" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">In this example, we load the iris data set (a toy dataset provided by scikit-learn), split the data into train and test sets, use FormulaFeatures to engineer a set of additional features, and fit a Decision Tree using these.</p><p id="66a6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This is fairly typical example. Using FormulaFeatures requires only creating a FormulaFeatures object, fitting it, and transforming the available data. This produces a new dataframe that can be used for any subsequent tasks, in this case to train a classification model.</p><pre class="ov ow ox oy oz pe pf pg bp ph bb bk"><span id="49d3" class="pi nk fq pf b bg pj pk l pl pm">import pandas as pd<br/>from sklearn.datasets import load_iris<br/>from formula_features import FormulaFeatures<br/><br/># Load the data<br/>iris = load_iris()<br/>x, y = iris.data, iris.target<br/>x = pd.DataFrame(x, columns=iris.feature_names)<br/><br/># Split the data into train and test<br/>x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)<br/><br/># Engineer new features<br/>ff = FormulaFeatures()<br/>ff.fit(x_train, y_train)<br/>x_train_extended = ff.transform(x_train)<br/>x_test_extended = ff.transform(x_test)<br/><br/># Train a decision tree and make predictions<br/>dt = DecisionTreeClassifier(max_depth=4, random_state=0)<br/>dt.fit(x_train_extended, y_train)<br/>y_pred = dt.predict(x_test_extended)</span></pre><p id="8471" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Setting the tool to execute with verbose=1 or verbose=2 allows viewing the process in greater detail.</p><p id="9821" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The github page also provides a file called demo.py, which provides some examples using FormulaFeatures, though the signature is quite simple.</p><h1 id="2e53" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Example getting feature scores</h1><p id="a65c" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">Getting the feature scores, which we show in this example, may be useful for understanding the features generated and for feature selection.</p><p id="2faa" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this example, we use the gas-drift dataset from openml (<a class="af ni" href="https://www.openml.org/search?type=data&amp;sort=runs&amp;id=1476&amp;status=active" rel="noopener ugc nofollow" target="_blank">https://www.openml.org/search?type=data&amp;sort=runs&amp;id=1476&amp;status=active</a>, licensed under Creative Commons).</p><p id="8c5e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It largely works the same as the previous example, but also makes a call to the display_features() API, which provides information about the features engineered.</p><pre class="ov ow ox oy oz pe pf pg bp ph bb bk"><span id="7700" class="pi nk fq pf b bg pj pk l pl pm">data = fetch_openml('gas-drift')<br/>x = pd.DataFrame(data.data, columns=data.feature_names)<br/>y = data.target<br/>  <br/># Drop all non-numeric columns. This is not necessary, but is done here <br/># for simplicity.<br/>x = x.select_dtypes(include=np.number)<br/>  <br/># Divide the data into train and test splits. For a more reliable measure <br/># of accuracy, cross validation may also be used. This is done here for <br/># simplicity.<br/>x_train, x_test, y_train, y_test = train_test_split(<br/>  x, y, test_size=0.33, random_state=42)<br/>  <br/>ff = FormulaFeatures(<br/>      max_iterations=2,<br/>      max_original_features=10,<br/>      target_type='classification',<br/>      verbose=1)<br/>ff.fit(x_train, y_train)<br/>x_train_extended = ff.transform(x_train)<br/>x_test_extended = ff.transform(x_test)<br/>  <br/>display_df = x_test_extended.copy()<br/>display_df['Y'] = y_test.values<br/>print(display_df.head())<br/>  <br/># Test using the extended features<br/>extended_score = test_f1(x_train_extended, x_test_extended, y_train, y_test)<br/>print(f"F1 (macro) score on extended features: {extended_score}")<br/>  <br/># Get a summary of the features engineered and their scores based <br/># on 1D models<br/>ff.display_features()</span></pre><p id="5332" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This will produce the following report, listing each feature index, F1 macro score, and feature name:</p><pre class="ov ow ox oy oz pe pf pg bp ph bb bk"><span id="60b7" class="pi nk fq pf b bg pj pk l pl pm">0:    0.438, V9<br/>   1:    0.417, V65<br/>   2:    0.412, V67<br/>   3:    0.412, V68<br/>   4:    0.412, V69<br/>   5:    0.404, V70<br/>   6:    0.409, V73<br/>   7:    0.409, V75<br/>   8:    0.409, V76<br/>   9:    0.414, V78<br/>  10:    0.447, ('V65', 'divide', 'V9')<br/>  11:    0.465, ('V67', 'divide', 'V9')<br/>  12:    0.422, ('V67', 'subtract', 'V65')<br/>  13:    0.424, ('V68', 'multiply', 'V65')<br/>  14:    0.489, ('V70', 'divide', 'V9')<br/>  15:    0.477, ('V73', 'subtract', 'V65')<br/>  16:    0.456, ('V75', 'divide', 'V9')<br/>  17:     0.45, ('V75', 'divide', 'V67')<br/>  18:    0.487, ('V78', 'divide', 'V9')<br/>  19:    0.422, ('V78', 'divide', 'V65')<br/>  20:    0.512, (('V67', 'divide', 'V9'), 'multiply', ('V65', 'divide', 'V9'))<br/>  21:    0.449, (('V67', 'subtract', 'V65'), 'divide', 'V9')<br/>  22:     0.45, (('V68', 'multiply', 'V65'), 'subtract', 'V9')<br/>  23:    0.435, (('V68', 'multiply', 'V65'), 'multiply', ('V67', 'subtract', 'V65'))<br/>  24:    0.535, (('V73', 'subtract', 'V65'), 'multiply', 'V9')<br/>  25:    0.545, (('V73', 'subtract', 'V65'), 'multiply', 'V78')<br/>  26:    0.466, (('V75', 'divide', 'V9'), 'subtract', ('V67', 'divide', 'V9'))<br/>  27:    0.525, (('V75', 'divide', 'V67'), 'divide', ('V73', 'subtract', 'V65'))<br/>  28:    0.519, (('V78', 'divide', 'V9'), 'multiply', ('V65', 'divide', 'V9'))<br/>  29:    0.518, (('V78', 'divide', 'V9'), 'divide', ('V75', 'divide', 'V67'))<br/>  30:    0.495, (('V78', 'divide', 'V65'), 'subtract', ('V70', 'divide', 'V9'))<br/>  31:    0.463, (('V78', 'divide', 'V65'), 'add', ('V75', 'divide', 'V9'))</span></pre><p id="0e92" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This includes the original features (features 0 through 9) for context. In this example, there is a steady increase in the predictive power of the features engineered.</p><p id="afe6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Plotting is also provided. In the case of regression targets, the tool presents a scatter plot mapping each feature to the target. In the case of classification targets, the tool presents a boxplot, giving the distribution of a feature broken down by class label. It is often the case that the original features show little difference in distributions per class, while engineered features can show a distinct difference. For example, one feature generated, (V99 / V47) — (V81 / V5) shows a strong separation:</p><figure class="ov ow ox oy oz pa os ot paragraph-image"><div role="button" tabindex="0" class="pp pq ed pr bh ps"><div class="os ot po"><img src="../Images/d42772d16536ce9f26733d3b4de86ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gt60RsHDQlwZ7dsr6vUcUQ.png"/></div></div></figure><p id="90f5" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The separation isn’t perfect, but is cleaner than with any of the original features.</p><p id="6e46" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This is typical of the features engineered; while each has an imperfect separation, each is strong, often much more so than for the original features.</p><h1 id="9326" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Test Results</h1><p id="d876" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">Testing was performed on synthetic and real data. The tool performed very well on the synthetic data, though this provides more debugging and testing than meaningful evaluation. For real data, a set of 80 random classification datasets from OpenML were selected, though only those having at least two numeric features could be included, leaving 69 files. Testing consisted of performing a single train-test split on the data, then training and evaluating a model on the numeric feature both before and after engineering additional features.</p><p id="fca3" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Macro F1 was used as the evaluation metric, evaluating a scikit-learn DecisionTreeClassifer with and without the engineered features, setting setting max_leaf_nodes = 10 (corresponding to 10 induced rules) to ensure an interpretable model.</p><p id="4ab5" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In many cases, the tool provided no improvement, or only slight improvements, in the accuracy of the shallow decision trees, as is expected. No feature engineering technique will work in all cases. More important is that the tool led to significant increases inaccuracy an impressive number of times. This is without tuning or feature selection, which can further improve the utility of the tool.</p><p id="5028" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Using other interpretable models will give different results, possibly stronger or weaker than was found with shallow decision trees, which did have show quite strong results.</p><p id="4b33" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In these tests we found better results limiting max_iterations to 2 compared to 3. This is a hyperparameter, and must be tuned for different datasets. For most datasets, using 2 or 3 works well, while with others, setting higher, even much higher (setting it to None allows the process to continue so long as it can produce more effective features), can work well.</p><p id="8541" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In most cases, the time engineering the new features was just seconds, and in all cases was under two minutes, even with many of the test files having hundreds of columns and many thousands of rows.</p><p id="2d35" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The results were:</p><pre class="ov ow ox oy oz pe pf pg bp ph bb bk"><span id="293d" class="pi nk fq pf b bg pj pk l pl pm">Dataset  Score    Score<br/>                                    Original Extended Improvement<br/>                             isolet  0.248   0.256     0.0074<br/>                        bioresponse  0.750   0.752     0.0013<br/>                         micro-mass  0.750   0.775     0.0250<br/>                     mfeat-karhunen  0.665   0.765     0.0991<br/>                            abalone  0.127   0.122    -0.0059<br/>                             cnae-9  0.718   0.746     0.0276<br/>                            semeion  0.517   0.554     0.0368<br/>                            vehicle  0.674   0.726     0.0526<br/>                           satimage  0.754   0.699    -0.0546<br/>             analcatdata_authorship  0.906   0.896    -0.0103<br/>                          breast-w   0.946   0.939    -0.0063<br/>                       SpeedDating   0.601   0.608     0.0070<br/>                        eucalyptus   0.525   0.560     0.0349<br/>                             vowel   0.431   0.461     0.0296<br/>             wall-robot-navigation   0.975   0.975     0.0000<br/>                   credit-approval   0.748   0.710    -0.0377<br/>             artificial-characters   0.289   0.322     0.0328<br/>                               har   0.870   0.870    -0.0000<br/>                               cmc   0.492   0.402    -0.0897<br/>                           segment   0.917   0.934     0.0174<br/>                    JapaneseVowels   0.573   0.686     0.1128<br/>                               jm1   0.534   0.544     0.0103<br/>                         gas-drift   0.741   0.833     0.0918<br/>                             irish   0.659   0.610    -0.0486<br/>                             profb   0.558   0.544    -0.0140<br/>                             adult   0.588   0.588     0.0000<br/>                            anneal   0.609   0.619     0.0104<br/>                          credit-g   0.528   0.488    -0.0396<br/>  blood-transfusion-service-center   0.639   0.621    -0.0177<br/>                       qsar-biodeg   0.778   0.804     0.0259<br/>                              wdbc   0.936   0.947     0.0116<br/>                           phoneme   0.756   0.743    -0.0134<br/>                          diabetes   0.716   0.661    -0.0552<br/>                   ozone-level-8hr   0.575   0.591     0.0159<br/>                       hill-valley   0.527   0.743     0.2160<br/>                               kc2   0.683   0.683     0.0000<br/>                     eeg-eye-state   0.664   0.713     0.0484<br/>  climate-model-simulation-crashes   0.470   0.643     0.1731<br/>                          spambase   0.891   0.912     0.0217<br/>                              ilpd   0.566   0.607     0.0414<br/>         one-hundred-plants-margin   0.058   0.055    -0.0026<br/>           banknote-authentication   0.952   0.995     0.0430<br/>                          mozilla4   0.925   0.924    -0.0009<br/>                       electricity   0.778   0.787     0.0087<br/>                           madelon   0.712   0.760     0.0480<br/>                             scene   0.669   0.710     0.0411<br/>                              musk   0.810   0.842     0.0326<br/>                             nomao   0.905   0.911     0.0062<br/>                    bank-marketing   0.658   0.645    -0.0134<br/>                    MagicTelescope   0.780   0.807     0.0261<br/>            Click_prediction_small   0.494   0.494    -0.0001<br/>                       page-blocks   0.669   0.816     0.1469<br/>                       hypothyroid   0.924   0.907    -0.0161<br/>                             yeast   0.445   0.487     0.0419<br/>                  CreditCardSubset   0.785   0.803     0.0184<br/>                           shuttle   0.651   0.514    -0.1368<br/>                         Satellite   0.886   0.902     0.0168<br/>                          baseball   0.627   0.701     0.0738<br/>                               mc1   0.705   0.665    -0.0404<br/>                               pc1   0.473   0.550     0.0770<br/>                  cardiotocography   1.000   0.991    -0.0084<br/>                           kr-vs-k   0.097   0.116     0.0187<br/>                      volcanoes-a1   0.366   0.327    -0.0385<br/>                wine-quality-white   0.252   0.251    -0.0011<br/>                             allbp   0.555   0.553    -0.0028<br/>                            allrep   0.279   0.288     0.0087<br/>                               dis   0.696   0.563    -0.1330<br/>                steel-plates-fault   1.000   1.000     0.0000</span></pre><p id="c6d5" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The model performed better with, than without, Formula Features feature engineering 49 out of 69 cases. Some noteworthy examples are:</p><ul class=""><li id="9349" class="mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh pn ol om bk">Japanese Vowels improved from .57 to .68</li><li id="30d4" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh pn ol om bk">gas-drift improved from .74 to .83</li><li id="f3ac" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh pn ol om bk">hill-valley improved from .52 to .74</li><li id="5d0a" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh pn ol om bk">climate-model-simulation-crashes improved from .47 to .64</li><li id="2993" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh pn ol om bk">banknote-authentication improved from .95 to .99</li><li id="bc33" class="mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh pn ol om bk">page-blocks improved from .66 to .81</li></ul><h1 id="51b2" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Using Engineered Features with strong predictive models</h1><p id="3a19" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">We’ve looked so far primarily at shallow decision trees in this article, and have indicated that FormulaFeatures can also generate features useful for other interpretable models. But, this leaves the question of their utility with more powerful predictive models. On the whole, FormulaFeatures is not useful in combination with these tools.</p><p id="ef2f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For the most part, strong predictive models such as boosted tree models (e.g., CatBoost, LGBM, XGBoost), will be able to infer the patterns that FormulaFeatures captures in any case. Though they will capture these patterns in the form of large numbers of decision trees, combined in an ensemble, as opposed to single features, the effect will be the same, and may often be stronger, as the trees are not limited to simple, interpretable operators (+, -, *, and /).</p><p id="7f12" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">So, there may not be an appreciable gain in accuracy using engineered features with strong models, even where they match the true f(x) closely. It can be worth trying FormulaFeatures in this case, and I’ve found it helpful with some projects, but most often the gain is minimal.</p><p id="f98f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It’s really with smaller (interpretable) models where tools such as FormulaFeatures become most useful.</p><h1 id="d322" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Working with very large numbers of original features</h1><p id="c677" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">One limitation of feature engineering based on arithmetic operations is that it can be slow where there are a very large number of original features, and it’s relatively common in data science to encounter tables with hundreds of features, or more. This affects unsupervised feature engineering methods much more severely, but supervised methods can also be significantly slowed down.</p><p id="1ff6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In these cases, creating even pairwise engineered features can also invite overfitting, as an enormous number of features can be produced, with some performing very well simply by chance.</p><p id="8a97" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To address this, FormulaFeatures limits the number of original columns considered when the input data has many columns. So, where datasets have large numbers of columns, only the most predictive are considered after the first iteration. The subsequent iterations perform as normal; there is simply some pruning of the original features used during this first iteration.</p><h1 id="a21c" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Unary Functions</h1><p id="b585" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">By default, Formula Features does not incorporate unary functions, such as square, square root, or log (though it can do so if the relevant parameters are specified). As indicated above, some tools, such as AutoFeat also optionally support these operations, and they can be valuable at times.</p><p id="09f1" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In some cases, it may be that a feature such as A² / B predicts the target better than the equivalent form without the square operator: A / B. However, including unary operators can lead to misleading features if not substantially correct, and may not significantly increase the accuracy of any models using them.</p><p id="09dc" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">When working with decision trees, so long as there is a monotonic relationship between the features with and without the unary functions, there will not be any change in the final accuracy of the model. And, most unary functions maintain a rank order of values (with exceptions such as sin and cos, which may reasonably be used where cyclical patterns are strongly suspected). For example, the values in A will have the same rank values as A² (assuming all values in A are positive), so squaring will not add any predictive power — decision trees will treat the features equivalently.</p><p id="7ba8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As well, in terms of explanatory power, simpler functions can often capture nearly as much of the pattern as can more complex functions: simpler function such as A / B are generally more comprehensible than formulas such as A² / B, but still convey the same idea, that it’s the ratio of the two features that’s relevant.</p><p id="25f7" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Limiting the set of operators used by default also allows the process to execute faster and in a more regularized manner.</p><h1 id="3027" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Coefficients</h1><p id="0395" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">A similar argument may be made for including coefficients in engineered features. A feature such as 5.3A + 1.4B may capture the relationship A and B have with Y better than the simpler A + B, but the coefficients are often unnecessary, prone to be calculated incorrectly, and inscrutable even where approximately correct.</p><p id="c964" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">And, in the case of multiplication and division operations, the coefficients are most likely irrelevant (at least when used with decision trees). For example, 5.3A * 1.4B will be functionally equivalent to A * B for most purposes, as the difference is a constant which can be divided out. Again, there is a monotonic relationship with and without the coefficients, and thus the features are equivalent when used with models, such as decision trees, that are concerned only with the ordering of feature values, not their specific values.</p><h1 id="7f9a" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Scaling</h1><p id="ca73" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">Scaling the features generated by FormulaFeatures is not necessary if used with decision trees (or similar model types such as Additive Decision Trees, rules, or decision tables). But, for some model types, such as SVM, kNN, ikNN, logistic regression, and others (including any that work based on distance calculations between points), the features engineered by Formula Features may be on quite different scales than the original features, and will need to be scaled. This is straightforward to do, and is simply a point to remember.</p><h1 id="b440" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Explainable Machine Learning</h1><p id="de5c" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">In this article, we looked at interpretable models, but should indicate, at least quickly, FormulaFeatures can also be useful for what are called <em class="pd">explainable models</em> and it may be that this is actually a more important application.</p><p id="89a4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To explain the idea of explainability: where it is difficult or impossible to create interpretable models with sufficient accuracy, we often instead develop black-box models (e.g. boosted models or neural networks), and then create post-hoc explanations of the model. Doing this is referred to as <em class="pd">explainable AI</em> (or XAI). These explanations try to make the black-boxes more understandable. Technique for this include: feature importances, ALE plots, proxy models, and counterfactuals.</p><p id="ca87" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">These can be important tools in many contexts, but they are limited, in that they can provide only an approximate understanding of the model. As well, they may not be permissible in all environments: in some situations (for example, for safety, or for regulatory compliance), it can be necessary to strictly use interpretable models: that is, to use models where there are no questions about how the model behaves.</p><p id="4d1d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">And, even where not strictly required, it’s quite often preferable to use an interpretable model where possible: it’s often very useful to have a good understanding of the model and of the predictions made by the model.</p><p id="211f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Having said that, using black-box models and post-hoc explanations is very often the most suitable choice for prediction problems. As FormulaFeatures produces valuable features, it can support XAI, potentially making feature importances, plots, proxy models, or counter-factuals more interpretable.</p><p id="0aee" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, it may not be feasible to use a shallow decision tree as the actual model, but it may be used as a proxy model: a simple, interpretable model that approximates the actual model. In these cases, as much as with interpretable models, having a good set of engineered features can make the proxy models more interpretable and more able to capture the behaviour of the actual model.</p><h1 id="ea95" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Installation</h1><p id="bf35" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">The tool uses a <a class="af ni" href="https://github.com/Brett-Kennedy/FormulaFeatures/blob/main/formula_features.py" rel="noopener ugc nofollow" target="_blank">single .py</a> file, which may be simply downloaded and used. It has no dependencies other than numpy, pandas, matplotlib, and seaborn (used to plot the features generated).</p><h1 id="7b01" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Conclusions</h1><p id="fd3f" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">FormulaFeatures is a tool to engineer features based on arithmetic relationships between numeric features. The features can be informative in themselves, but are particularly useful when used with interpretable ML models.</p><p id="a5bf" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">While this tends to not improve the accuracy for all models, it does quite often improve the accuracy of interpretable models such as shallow decision trees.</p><p id="1900" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Consequently, it can be a useful tool to make it more feasible to use interpretable models for prediction problems — it may allow the use of interpretable models for problems that would otherwise be limited to black box models. And where interpretable models are used, it may allow these to be more accurate or interpretable. For example, with a classification decision tree, we may be able to achieve similar accuracy using fewer nodes, or may be able to achieve higher accuracy using the same number of nodes.</p><p id="4cfd" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">FormulaFeatures can very often support interpretable ML well, but there are some limitations. It does not work with categorical or other non-numeric features. And, even with numeric features, some interactions may be difficult to capture using arithmetic functions. Where there is a more complex relationship between pairs of features and the target column, it may be more appropriate to use <a class="af ni" href="https://github.com/Brett-Kennedy/ikNN" rel="noopener ugc nofollow" target="_blank">ikNN</a>. This works based on nearest neighbors, so can capture relationships of arbitrary complexity between features and the target.</p><p id="6136" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We focused on standard decision trees in this article, but for the most effective interpretable ML, it can be useful to try other interpretable models. It’s straightforward to see, for example, how the ideas here will apply directly to <a class="af ni" href="https://github.com/Brett-Kennedy/GeneticDecisionTree" rel="noopener ugc nofollow" target="_blank">Genetic Decision Trees</a>, which are similar to standard decision trees, simply created using bootstrapping and a genetic algorithm. Similarly for most other interpretable models.</p><p id="3b77" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">All images are by the author</p></div></div></div></div>    
</body>
</html>