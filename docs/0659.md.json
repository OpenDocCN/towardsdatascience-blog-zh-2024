["```py\n# Install the required Python packages \npip install mediapipe\npip install numpy\npip install opencv-python<4.6\npip install requests>=2.31,<3\npip install rerun-sdk\n\n# or just use the requirements file\npip install -r examples/python/human_pose_tracking/requirements.txt\n```", "```py\nimport mediapipe as mp\nimport numpy as np\nfrom typing import Any\nimport numpy.typing as npt\nimport cv2\n\n\"\"\"\n    Read 2D landmark positions from Mediapipe Pose results.\n\n    Args:\n        results (Any): Mediapipe Pose results.\n        image_width (int): Width of the input image.\n        image_height (int): Height of the input image.\n\n    Returns:\n        np.array | None: Array of 2D landmark positions or None if no landmarks are detected.\n\"\"\"\ndef read_landmark_positions_2d(\n        results: Any,\n        image_width: int,\n        image_height: int,\n) -> npt.NDArray[np.float32] | None:\n    if results.pose_landmarks is None:\n        return None\n    else:\n        # Extract normalized landmark positions and scale them to image dimensions\n        normalized_landmarks = [results.pose_landmarks.landmark[lm] for lm in mp.solutions.pose.PoseLandmark]\n        return np.array([(image_width * lm.x, image_height * lm.y) for lm in normalized_landmarks])\n\n\"\"\"\n    Read 3D landmark positions from Mediapipe Pose results.\n\n    Args:\n        results (Any): Mediapipe Pose results.\n\n    Returns:\n        np.array | None: Array of 3D landmark positions or None if no landmarks are detected.\n\"\"\"\ndef read_landmark_positions_3d(\n        results: Any,\n) -> npt.NDArray[np.float32] | None:\n    if results.pose_landmarks is None:\n        return None\n    else:\n        # Extract 3D landmark positions\n        landmarks = [results.pose_world_landmarks.landmark[lm] for lm in mp.solutions.pose.PoseLandmark]\n        return np.array([(lm.x, lm.y, lm.z) for lm in landmarks])\n\n\"\"\"\n    Track and analyze pose from an input image.\n\n    Args:\n        image_path (str): Path to the input image.\n\"\"\"\ndef track_pose(image_path: str) -> None:\n    # Read the image, convert color to RGB\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Create a Pose model instance\n    pose_detector = mp.solutions.pose.Pose(static_image_mode=True)\n\n    # Process the image to obtain pose landmarks\n    results = pose_detector.process(image)\n    h, w, _ = image.shape\n\n    # Read 2D and 3D landmark positions\n    landmark_positions_2d = read_landmark_positions_2d(results, w, h)\n    landmark_positions_3d = read_landmark_positions_3d(results)\n```", "```py\nrr.log(\n        \"/\",\n        rr.AnnotationContext(\n            rr.ClassDescription(\n                info=rr.AnnotationInfo(id=0, label=\"Person\"),\n                keypoint_annotations=[rr.AnnotationInfo(id=lm.value, label=lm.name) for lm in mp_pose.PoseLandmark],\n                keypoint_connections=mp_pose.POSE_CONNECTIONS,\n            )\n        ),\n        timeless=True,\n    )\n```", "```py\ndef track_pose_2d(video_path: str) -> None:\n    mp_pose = mp.solutions.pose  \n\n    with closing(VideoSource(video_path)) as video_source, mp_pose.Pose() as pose:\n        for idx, bgr_frame in enumerate(video_source.stream_bgr()):\n            if max_frame_count is not None and idx >= max_frame_count:\n                break\n\n            rgb = cv2.cvtColor(bgr_frame.data, cv2.COLOR_BGR2RGB)\n\n            # Associate frame with the data\n            rr.set_time_seconds(\"time\", bgr_frame.time)\n            rr.set_time_sequence(\"frame_idx\", bgr_frame.idx)\n\n            # Present the video\n            rr.log(\"video/rgb\", rr.Image(rgb).compress(jpeg_quality=75))\n\n            # Get the prediction results\n            results = pose.process(rgb)\n            h, w, _ = rgb.shape\n\n            # Log 2d points to 'video' entity\n            landmark_positions_2d = read_landmark_positions_2d(results, w, h)\n            if landmark_positions_2d is not None:\n                rr.log(\n                    \"video/pose/points\",\n                    rr.Points2D(landmark_positions_2d, class_ids=0, keypoint_ids=mp_pose.PoseLandmark),\n                )\n```", "```py\ndef track_pose_3d(video_path: str, *, segment: bool, max_frame_count: int | None) -> None:\n    mp_pose = mp.solutions.pose  \n\n    rr.log(\"person\", rr.ViewCoordinates.RIGHT_HAND_Y_DOWN, timeless=True)\n\n    with closing(VideoSource(video_path)) as video_source, mp_pose.Pose() as pose:\n        for idx, bgr_frame in enumerate(video_source.stream_bgr()):\n            if max_frame_count is not None and idx >= max_frame_count:\n                break\n\n            rgb = cv2.cvtColor(bgr_frame.data, cv2.COLOR_BGR2RGB)\n\n            # Associate frame with the data\n            rr.set_time_seconds(\"time\", bgr_frame.time)\n            rr.set_time_sequence(\"frame_idx\", bgr_frame.idx)\n\n            # Present the video\n            rr.log(\"video/rgb\", rr.Image(rgb).compress(jpeg_quality=75))\n\n            # Get the prediction results\n            results = pose.process(rgb)\n            h, w, _ = rgb.shape\n\n            # New entity \"Person\" for the 3D presentation\n            landmark_positions_3d = read_landmark_positions_3d(results)\n            if landmark_positions_3d is not None:\n                rr.log(\n                    \"person/pose/points\",\n                    rr.Points3D(landmark_positions_3d, class_ids=0, keypoint_ids=mp_pose.PoseLandmark),\n                )\n```", "```py\nrr.log(\n  \"video\", \n  rr.Image(img).compress(jpeg_quality=75)\n)\n```"]