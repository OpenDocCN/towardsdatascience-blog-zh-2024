["```py\nimport os\n\nimport pinecone\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Pinecone\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nfrom chatytt.embeddings.s3_json_document_loader import S3JsonFileLoader\n\n# Define the splitter with which to split the transcripts into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=pre_processing_conf[\"recursive_character_splitting\"][\"chunk_size\"],\n    chunk_overlap=pre_processing_conf[\"recursive_character_splitting\"][\n        \"chunk_overlap\"\n    ],\n)\n\n# Load and split the transcript\nloader = S3JsonFileLoader(\n    bucket=\"s3_bucket_name\",\n    key=\"transcript_file_name\",\n    text_splitter=text_splitter,\n)\ntranscript_chunks = loader.load(split_doc=True)\n\n# Connect to the relevant Pinecone vectorstore\npinecone.init(\n    api_key=os.environ.get(\"PINECONE_API_KEY\"), environment=\"gcp-starter\"\n)\npinecone_index = pinecone.Index(os.environ.get(\"INDEX_NAME\"), pool_threads=4)\n\n# Store the transcrpt chunks as embeddings in Pinecone\nvector_store = Pinecone(\n    index=pinecone_index,\n    embedding=OpenAIEmbeddings(),\n    text_key=\"text\",\n)\nvector_store.add_documents(documents=transcript_chunks)\n```", "```py\nimport pinecone\nfrom langchain.vectorstores import Pinecone\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Define the vector store with which to perform a semantic search against\n# the Pinecone vector database\npinecone.init(\n    api_key=os.environ.get(\"PINECONE_API_KEY\"), environment=\"gcp-starter\"\n)\npinecone_index = pinecone.Index(os.environ.get(\"INDEX_NAME\"), pool_threads=4)\nvector_store = Pinecone(\n    index=pinecone_index,\n    embedding=OpenAIEmbeddings(),\n    text_key=\"text\",\n)\n\n# Define the retrieval chain that will perform the steps in RAG\n# with conversation memory as outlined above.\nchain = ConversationalRetrievalChain.from_llm(\n    llm=ChatOpenAI(), retriever=vector_store.as_retriever()\n)\n\n# Call the chain passing in the current question and the chat history\nresponse = chain({\"question\": query, \"chat_history\": chat_history})[\"answer\"]\n```", "```py\nimport os\nimport time\nfrom typing import List, Any\n\nimport boto3\n\ntable = boto3.resource(\"dynamodb\").Table(os.environ.get(\"CHAT_HISTORY_TABLE_NAME\")\n\ndef fetch_chat_history(user_id: str) -> str:\n    response = table.get_item(Key={\"UserId\": user_id})\n    return response[\"Item\"]\n\ndef update_chat_history(user_id: str, chat_history: List[dict[str, Any]]):\n    chat_history_update_data = {\n        \"UpdatedTimestamp\": {\"Value\": int(time.time()), \"Action\": \"PUT\"},\n        \"ChatHistory\": {\"Value\": chat_history, \"Action\": \"PUT\"},\n    }\n    table.update_item(\n        Key={\"UserId\": user_id}, AttributeUpdates=chat_history_update_data\n    )\n\ndef is_new_user(user_id: str) -> bool:\n    response = table.get_item(Key={\"UserId\": user_id})\n    return response.get(\"Item\") is None\n\ndef create_chat_history(user_id: str, chat_history: List[dict[str, Any]]):\n    item = {\n        \"UserId\": user_id,\n        \"CreatedTimestamp\": int(time.time()),\n        \"UpdatedTimestamp\": None,\n        \"ChatHistory\": chat_history,\n    }\n    table.put_item(Item=item)\n```", "```py\nfrom dotenv import load_dotenv\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\n\nfrom chatytt.chains.standard import ConversationalQAChain\nfrom chatytt.vector_store.pinecone_db import PineconeDB\nfrom server.utils.chat import parse_chat_history\nfrom server.utils.dynamodb import (\n    is_new_user,\n    fetch_chat_history,\n    create_chat_history,\n    update_chat_history,\n)\n\nload_dotenv()\napp = Flask(__name__)\n\n# Enable Cross Origin Resource Sharing since the server and client\n# will be hosted seperately\nCORS(app)\n\npinecone_db = PineconeDB(index_name=\"youtube-transcripts\", embedding_source=\"open-ai\")\nchain = ConversationalQAChain(vector_store=pinecone_db.vector_store)\n\n@app.route(\"/get-query-response/\", methods=[\"POST\"])\ndef get_query_response():\n    data = request.get_json()\n    query = data[\"query\"]\n\n    raw_chat_history = data[\"chatHistory\"]\n    chat_history = parse_chat_history(raw_chat_history)\n    response = chain.get_response(query=query, chat_history=chat_history)\n\n    return jsonify({\"response\": response})\n\n@app.route(\"/get-chat-history/\", methods=[\"GET\"])\ndef get_chat_history():\n    user_id = request.args.get(\"userId\")\n\n    if is_new_user(user_id):\n        response = {\"chatHistory\": []}\n        return jsonify({\"response\": response})\n\n    response = {\"chatHistory\": fetch_chat_history(user_id=user_id)[\"ChatHistory\"]}\n\n    return jsonify({\"response\": response})\n\n@app.route(\"/save-chat-history/\", methods=[\"PUT\"])\ndef save_chat_history():\n    data = request.get_json()\n    user_id = data[\"userId\"]\n\n    if is_new_user(user_id):\n        create_chat_history(user_id=user_id, chat_history=data[\"chatHistory\"])\n    else:\n        update_chat_history(user_id=user_id, chat_history=data[\"chatHistory\"])\n\n    return jsonify({\"response\": \"chat saved\"})\n\nif __name__ == \"__main__\":\n    app.run(debug=True, port=8080)\n```", "```py\nimport React from \"react\";\nimport {chatItem} from \"./LiveChatFeed\";\n\ninterface Props {\n    setCurrentChat: React.SetStateAction<any>\n    userInput: string\n    currentChat: Array<chatItem>\n    setUserInput: React.SetStateAction<any>\n}\n\nfunction getCurrentChat({setCurrentChat, userInput, currentChat, setUserInput}: Props){\n    // The current chat is displayed in the live chat feed. Since we don't want\n    // to wait for the LLM response before displaying the users question in\n    // the chat feed, copy it to a seperate variable and pass it to the current\n    // chat before pining the API for the answer.\n    const userInputText = userInput\n    setUserInput(\"\")\n    setCurrentChat([\n        ...currentChat,\n        {\n            \"text\": userInputText,\n            isBot: false\n        }\n    ])\n\n    // Create API payload for the post request\n    const options = {\n        method: 'POST',\n        headers: {\n            \"Content-Type\": 'application/json',\n            'Accept': 'application/json'\n        },\n        body: JSON.stringify({\n            query: userInputText,\n            chatHistory: currentChat\n        })\n    }\n\n    // Ping the endpoint, wait for the response, and add it to the current chat\n    // so that it appears in the live chat feed.\n    fetch(`${import.meta.env.VITE_ENDPOINT}get-query-response/`, options).then(\n        (response) => response.json()\n    ).then(\n        (data) => {\n            setCurrentChat([\n                ...currentChat,\n                {\n                    \"text\": userInputText,\n                    \"isBot\": false\n                },\n                {\n                    \"text\": data.response,\n                    \"isBot\": true\n                }\n            ])\n        }\n    )\n}\n\nexport default getCurrentChat\n```", "```py\nimport React, {useState} from \"react\";\nimport {chatItem} from \"./LiveChatFeed\";\nimport saveIcon from \"../assets/saveicon.png\"\nimport tickIcon from \"../assets/tickicon.png\"\n\ninterface Props {\n    userId: String\n    previousChats: Array<Array<chatItem>>\n}\n\nfunction SaveChatHistoryButton({userId, previousChats}: Props){\n    // Define a state to determine if the current chat has been saved or not.\n    const [isChatSaved, setIsChatSaved] = useState(false)\n\n    // Construct the payload for the PUT request to save chat history.\n    const saveChatHistory = () => {\n        const options = {\n            method: 'PUT',\n            headers: {\n                \"Content-Type\": 'application/json',\n                'Accept': 'application/json'\n            },\n            body: JSON.stringify({\n                \"userId\": userId,\n                \"chatHistory\": previousChats\n            })\n        }\n\n        // Ping the API with the chat history, and set the state of\n        // isChatSaved to true if successful\n        fetch(`${import.meta.env.VITE_ENDPOINT}save-chat-history/`, options).then(\n            (response) => response.json()\n        ).then(\n            (data) => {\n                setIsChatSaved(true)\n            }\n        )\n    }\n\n    // Display text on the save chat button dynamically depending on the value\n    // of the isChatSaved state.\n    return (\n        <button\n            className=\"save-chat-history-button\"\n            onClick={() => {saveChatHistory()}}\n        > <img className={isChatSaved?\"tick-icon-img\":\"save-icon-img\"} src={isChatSaved?tickIcon:saveIcon}/>\n        {isChatSaved?\"Chats Saved\":\"Save Chat History\"}\n        </button>\n    )\n}\n\nexport default SaveChatHistoryButton\n```", "```py\nimport React from \"react\";\nimport {chatItem} from \"./LiveChatFeed\";\n\ninterface Props {\n    userId: String\n    previousChats: Array<Array<chatItem>>\n    setPreviousChats: React.SetStateAction<any>\n}\n\nfunction getUserChatHistory({userId, previousChats, setPreviousChats}: Props){\n    // Create the payload for the GET request\n    const options = {\n            method: 'GET',\n            headers: {\n                \"Content-Type\": 'application/json',\n                'Accept': 'application/json'\n            }\n        }\n\n        // Since this is a GET request pass in the user id as a query parameter\n        // Set the previousChats state to the chat history returned by the API.\n        fetch(`${import.meta.env.VITE_ENDPOINT}get-chat-history/?userId=${userId}`, options).then(\n            (response) => response.json()\n        ).then(\n            (data) => {\n            if (data.response.chatHistory.length > 0) {\n                setPreviousChats(\n                        [\n                            ...previousChats,\n                            ...data.response.chatHistory\n                        ]\n                    )\n                }\n            }\n        )\n}\n\nexport default getUserChatHistory\n```", "```py\nParameters:\n  YoutubeDataAPIKey:\n    Type: String\n    Default: '{{resolve:secretsmanager:youtube-data-api-key:SecretString:youtube-data-api-key}}'\n  PineconeAPIKey:\n    Type: String\n    Default: '{{resolve:secretsmanager:pinecone-api-key:SecretString:pinecone-api-key}}'\n  OpenaiAPIKey:\n    Type: String\n    Default: '{{resolve:secretsmanager:openai-api-key:SecretString:openai-api-key}}'\n```", "```py\nFetchLatestVideoIDsFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: ../code_uri/.\n      Handler: chatytt.youtube_data.lambda_handlers.fetch_latest_video_ids.lambda_handler\n      Policies:\n        - AmazonS3FullAccess\n      Environment:\n        Variables:\n          PLAYLIST_NAME:\n            Ref: PlaylistName\n          YOUTUBE_DATA_API_KEY:\n            Ref: YoutubeDataAPIKey\n```", "```py\nEmbeddingRetrieverStateMachine:\n  Type: AWS::Serverless::StateMachine\n  Properties:\n    DefinitionUri: statemachine/embedding_retriever.asl.json\n    DefinitionSubstitutions:\n      FetchLatestVideoIDsFunctionArn: !GetAtt FetchLatestVideoIDsFunction.Arn\n      FetchLatestVideoTranscriptsArn: !GetAtt FetchLatestVideoTranscripts.Arn\n      FetchLatestTranscriptEmbeddingsArn: !GetAtt FetchLatestTranscriptEmbeddings.Arn\n    Events:\n      WeeklySchedule:\n        Type: Schedule\n        Properties:\n          Description: Schedule to run the workflow once per week on a Monday.\n          Enabled: true\n          Schedule: cron(0 3 ? * 1 *)\n    Policies:\n    - LambdaInvokePolicy:\n        FunctionName: !Ref FetchLatestVideoIDsFunction\n    - LambdaInvokePolicy:\n        FunctionName: !Ref FetchLatestVideoTranscripts\n    - LambdaInvokePolicy:\n        FunctionName: !Ref FetchLatestTranscriptEmbeddings\n```", "```py\nChatYTTApi:\n    Type: AWS::Serverless::Api\n    Properties:\n      StageName: Prod\n      Cors:\n        AllowMethods: \"'*'\"\n        AllowHeaders: \"'*'\"\n        AllowOrigin: \"'*'\"\n```", "```py\nChatResponseFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      Runtime: python3.9\n      Timeout: 120\n      CodeUri: ../code_uri/.\n      Handler: server.lambda_handler.lambda_handler\n      Policies:\n        - AmazonDynamoDBFullAccess\n      MemorySize: 512\n      Architectures:\n        - x86_64\n      Environment:\n        Variables:\n          PINECONE_API_KEY:\n            Ref: PineconeAPIKey\n          OPENAI_API_KEY:\n            Ref: OpenaiAPIKey\n      Events:\n        GetQueryResponse:\n          Type: Api\n          Properties:\n            RestApiId: !Ref ChatYTTApi\n            Path: /get-query-response/\n            Method: post\n        GetChatHistory:\n          Type: Api\n          Properties:\n            RestApiId: !Ref ChatYTTApi\n            Path: /get-chat-history/\n            Method: get\n        UpdateChatHistory:\n          Type: Api\n          Properties:\n            RestApiId: !Ref ChatYTTApi\n            Path: /save-chat-history/\n            Method: put\n```", "```py\nAmplifyApp:\n    Type: AWS::Amplify::App\n    Properties:\n      Name: amplify-chatytt-client\n      Repository: <https://github.com/suresha97/ChatYTT>\n      AccessToken: '{{resolve:secretsmanager:github-token:SecretString:github-token}}'\n      IAMServiceRole: !GetAtt AmplifyRole.Arn\n      EnvironmentVariables:\n        - Name: ENDPOINT\n          Value: !ImportValue 'chatytt-api-ChatYTTAPIURL'\n```", "```py\nversion: 1\nfrontend:\n  phases:\n    preBuild:\n      commands:\n        - cd client\n        - npm ci\n    build:\n      commands:\n        - echo \"VITE_ENDPOINT=$ENDPOINT\" >> .env\n        - npm run build\n  artifacts:\n    baseDirectory: ./client/dist\n    files:\n      - \"**/*\"\n  cache:\n    paths:\n      - node_modules/**/*\n```", "```py\nAmplifyBranch:\n    Type: AWS::Amplify::Branch\n    Properties:\n      BranchName: main\n      AppId: !GetAtt AmplifyApp.AppId\n      EnableAutoBuild: true\n```"]