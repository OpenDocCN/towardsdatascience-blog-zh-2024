<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Superposition: What Makes it Difficult to Explain Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Superposition: What Makes it Difficult to Explain Neural Network</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/superposition-what-makes-it-difficult-to-explain-neural-network-565087243be4?source=collection_archive---------0-----------------------#2024-12-29">https://towardsdatascience.com/superposition-what-makes-it-difficult-to-explain-neural-network-565087243be4?source=collection_archive---------0-----------------------#2024-12-29</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="303c" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">When there are more features than model dimensions</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@vanillaxiangshuyang?source=post_page---byline--565087243be4--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Shuyang Xiang" class="l ep by dd de cx" src="../Images/36a5fd18fd9b7b88cb41094f09b83882.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Q-6F64L3h4jxYNYPiqHVaQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--565087243be4--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@vanillaxiangshuyang?source=post_page---byline--565087243be4--------------------------------" rel="noopener follow">Shuyang Xiang</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--565087243be4--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div>6 days ago</div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">6</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="0a00" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Introduction</h1><p id="3665" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">It would be ideal if the world of neural network represented a one-to-one relationship: each neuron activates on one and only one feature. In such a world, interpreting the model would be straightforward: this neuron fires for the dog ear feature, and that neuron fires for the wheel of cars. Unfortunately, that is not the case. In reality, a model with dimension <em class="ob">d</em> often needs to represent <em class="ob">m</em> features, where <em class="ob">d &lt; m</em>. This is when we observe the phenomenon of superposition.</p><p id="8d1f" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk"><em class="ob">One small remark concerning one comment: superposition might still occur when d&gt;m but with different intepretation and mechanism. The higher dimensionality in the hidden layer provides the network with more capacity without eliminating the need for superposition — it simply allows for a richer combination of shared and distinct features within the larger representational space.</em></p><p id="958e" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">In the context of machine learning, superposition refers to a specific phenomenon that one neuron in a model represents multiple overlapping features rather than a single, distinct one. For example, InceptionV1 contains one neuron that responds to cat faces, fronts of cars, and cat legs [1]. This leads to what we can superposition of different features activation in the same neuron or circuit.</p><p id="48c0" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">The existence of superposition makes model explainability challenging, especially in deep learning models, where neurons in hidden layers represent complex combinations of patterns rather than being associated with simple, direct features.</p><p id="ec2b" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">In this blog post, we will present a simple toy example of superposition, with detailed implementations by Python in this <a class="af oh" href="https://colab.research.google.com/drive/1WXHfWOjFBLN8T6E6QfkvsJ2v7ZoLY3qK#scrollTo=PAqPr42lwzVX" rel="noopener ugc nofollow" target="_blank">notebook</a>.</p><h1 id="8480" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">What makes Superposition Occur: Assumptions</h1><p id="c287" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">We begin this section by discussing the term “feature”.</p><p id="52ec" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">In tabular data, there is little ambiguity in defining what a feature is. For example, when predicting the quality of wine using a tabular dataset, features can be the percentage of alcohol, the year of production, etc.</p><p id="e78c" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">However, defining features can become complex when dealing with non-tabular data, such as images or textual data. In these cases, there is no universally agreed-upon definition of a feature. Broadly, a feature can be considered any property of the input that is recognizable to most humans. For instance, one feature in a large language model (LLM) might be whether a word is in French.</p><p id="6120" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">Superposition occurs when the number of features is more than the model dimensions. We claim that two necessary conditions must be met if superposition would occur:</p><ol class=""><li id="d1f2" class="nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa oi oj ok bk"><strong class="nh fr">Non-linearity</strong>: Neural networks typically include non-linear activation functions, such as sigmoid or ReLU, at the end of each hidden layer. These activation functions give the network possibilities to map inputs to outputs in a non-linear way, so that it can capture more complex relationships between features. We can imagine that without non-linearity, the model would behave as a simple linear transformation, where features remain linearly separable, without any possibility of compression of dimensions through superposition.</li><li id="1271" class="nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa oi oj ok bk"><strong class="nh fr">Feature Sparsity</strong>: Feature sparsity means the fact that only a small subset of features is non-zero. For example, in language models, many features are not present at the same time: e.g. one same word cannot be is_French and is_other_languages. If all features were dense, we can imagine an important interference due to overlapping representations, making it very difficult for the model to decode features.</li></ol><h1 id="6c94" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Toy Example: Linearity vs non-linearity with varying sparsity</h1><h2 id="8897" class="oq mk fq bf ml or os ot mo ou ov ow mr no ox oy oz ns pa pb pc nw pd pe pf pg bk">Synthetic Dataset</h2><p id="b711" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Let us consider a toy example of 40 features with linearly decreasing feature importance: the first feature has an importance of 1, the last feature has an importance of 0.1, and the importance of the remaining features is evenly spaced between these two values.</p><p id="d452" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">We then generate a synthetic dataset with the following code:</p><pre class="ph pi pj pk pl pm pn po bp pp bb bk"><span id="d046" class="pq mk fq pn b bg pr ps l pt pu">def generate_sythentic_dataset(dim_sample, num_sapmple, sparsity): <br/>  """Generate synthetic dataset according to sparsity"""<br/>  dataset=[]<br/>  for _ in range(num_sapmple): <br/>    x = np.random.uniform(0, 1, n)<br/>    mask = np.random.choice([0, 1], size=n, p=[sparsity, 1 - sparsity])<br/>    x = x * mask  # Apply sparsity<br/>    dataset.append(x)<br/>  return np.array(dataset)</span></pre><p id="537e" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">This function creates a synthetic dataset with the given number of dimensions, which is, 40 in our case. For each dimension, a random value is generated from a uniform distribution in [0, 1]. The sparsity parameter, varying between 0 and 1, controls the percentage of active features in each sample. For example, when the sparsity is 0.8, it the features in each sample has 80% chance to be zero. The function applies a mask matrix to realize the sparsity setting.</p><h2 id="7d26" class="oq mk fq bf ml or os ot mo ou ov ow mr no ox oy oz ns pa pb pc nw pd pe pf pg bk">Linear and Relu Models</h2><p id="a8ba" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">We would now like to explore how ReLU-based neural models lead to superposition formation and how sparsity values would change their behaviors.</p><p id="c123" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">We set our experiment in the following way: we compress the features with 40 dimensions into the 5 dimensional space, then reconstruct the vector by reversing the process. Observing the behavior of these transformations, we expect to see how superposition forms in each case.</p><p id="970a" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">To do so, we consider two very similar models:</p><ol class=""><li id="7162" class="nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa oi oj ok bk"><strong class="nh fr">Linear Model</strong>: A simple linear model with only 5 coefficients. Recall that we want to work with 40 features — far more than the model’s dimensions.</li><li id="9dc9" class="nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa oi oj ok bk"><strong class="nh fr">ReLU Model</strong>: A model almost the same to the linear one, but with an additional ReLU activation function at the end, introducing one level of non-linearity.</li></ol><p id="efde" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">Both models are built using PyTorch. For example, we build the ReLU model with the following code:</p><pre class="ph pi pj pk pl pm pn po bp pp bb bk"><span id="f14c" class="pq mk fq pn b bg pr ps l pt pu">class ReLUModel(nn.Module):<br/>    def __init__(self, n, m):<br/>        super().__init__()<br/>        self.W = nn.Parameter(torch.randn(m, n) * np.sqrt(1 / n))<br/>        self.b = nn.Parameter(torch.zeros(n))<br/>    <br/>    def forward(self, x):<br/>        h = torch.relu(torch.matmul(x, self.W.T))  # Add ReLU activation: x (batch, n) * W.T (n, m) -&gt; h (batch, m)<br/>        x_reconstructed = torch.relu(torch.matmul(h, self.W) + self.b)  # Reconstruction with ReLU<br/>        return x_reconstructed </span></pre><p id="bcaa" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">According to the code, the n-dimensional input vector x is projected into a lower-dimensional space by multiplying it with an m×n weight matrix. We then reconstruct the original vector by mapping it back to the original feature space through a ReLU transformation, adjusted by a bias vector. The Linear Model is given by the similar structure, with the only difference being that the reconstruction is done by using only the linear transformation instead of ReLU. We train the model by minimizing the mean squared error between the original feature samples and the reconstructed ones, weighted one the feature importance.</p><h1 id="c8d7" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Results Analysis</h1><p id="6f8e" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">We trained both models with different sparsity values: 0.1, 0.5, and 0.9, from less sparse to the most sparse. We have observed several important results.</p><p id="dc16" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">First, whatever the sparsity level, ReLU models “compress” features much better than linear models: While linear models mainly capture features with the highest feature importance, <strong class="nh fr">ReLU models could focus on less important features by formation of superposition</strong>— where a single model dimension represents multiple features. Let us have a vision of this phenomenon in the following visualizations: for linear models, the biases are smallest for the top five features, (in case you don’t remember: the feature importance is defined as a linearly decreasing function based on feature order). In contrast, the biases for the ReLU model do not show this order and are generally reduced more.</p><figure class="ph pi pj pk pl py pv pw paragraph-image"><div role="button" tabindex="0" class="pz qa ed qb bh qc"><div class="pv pw px"><img src="../Images/89aa16c3c4a92fafd8b191a5453c8e4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0HfEwi1yGNKcvAR9UH26dw.png"/></div></div><figcaption class="qe qf qg pv pw qh qi bf b bg z dx">Image by author: reconstructed bias</figcaption></figure><p id="284b" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">Another important and interesting result is that: superposition is much more likely to observe when sparsity level is high in the features. To get an impression of this phenomenon, we can visualize the matrix W^T@W, where W is the m×n weight matrix in the models. One might interpret the matrix W^T@W as a quantity of how the input features are projected onto the lower dimensional space:</p><p id="072e" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">In particular:</p><ol class=""><li id="835f" class="nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa oi oj ok bk">The diagonal of W^T@W represents the “self-similarity” of each feature inside the low dimensional transformed space.</li><li id="a2e9" class="nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa oi oj ok bk">The off-diagonal of the matrix represents how different features correlate to each other.</li></ol><p id="c404" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">We now visualize the values of W^T@W below for both the Linear and ReLU models we have constructed before with two different sparsity levels : 0.1 and 0.9. You can see that when the sparsity value is high as 0.9, the off-diagonal elements become much bigger compared to the case when sparsity is 0.1 (You actually don’t see much difference between the two models output). This observation indicates that correlations between different features are more easily to be learned when sparsity is high.</p><figure class="ph pi pj pk pl py pv pw paragraph-image"><div role="button" tabindex="0" class="pz qa ed qb bh qc"><div class="pv pw qj"><img src="../Images/43cb05a515e34ec6a6c5dce3c5040edb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*opt7iYLJHxSGK2mIpry5TQ.png"/></div></div><figcaption class="qe qf qg pv pw qh qi bf b bg z dx">Image by Author: matrix for sparsity 0.1</figcaption></figure><figure class="ph pi pj pk pl py pv pw paragraph-image"><div role="button" tabindex="0" class="pz qa ed qb bh qc"><div class="pv pw qk"><img src="../Images/657c391ae68846fcb240c7a2e1e79c9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b-w90n9Atm3gjoLdhYIFnw.png"/></div></div><figcaption class="qe qf qg pv pw qh qi bf b bg z dx">Image by author: matrix for sparsity 0.9</figcaption></figure><h1 id="f4e3" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Conclusion</h1><p id="a89e" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">In this blog post, I made a simple experiment to introduce the formation of superposition in neural networks by comparing Linear and ReLU models with fewer dimensions than features to represent. We observed that the non-linearity introduced by the ReLU activation, combined with a certain level of sparsity, can help the model form superposition.</p><p id="9617" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">In real-world applications, which are much more complex than my navie example, superposition is an important mechanism for representing complex relationships in neural models, especially in vision models or LLMs.</p><h1 id="b7c3" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">References</h1><p id="5ccc" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">[1] Zoom In: An Introduction to Circuits. <a class="af oh" href="https://distill.pub/2020/circuits/zoom-in/" rel="noopener ugc nofollow" target="_blank">https://distill.pub/2020/circuits/zoom-in/</a></p><p id="beaf" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">[2] Toy models with superposition. <a class="af oh" href="https://transformer-circuits.pub/2022/toy_model/index.html" rel="noopener ugc nofollow" target="_blank">https://transformer-circuits.pub/2022/toy_model/index.html</a></p></div></div></div></div>    
</body>
</html>