<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Automatic Labeling With GroundingDino</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Automatic Labeling With GroundingDino</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?source=collection_archive---------3-----------------------#2024-02-06">https://towardsdatascience.com/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?source=collection_archive---------3-----------------------#2024-02-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a680" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A practical guide to tag object detection datasets with the GroundingDino algorithm. Code included.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@lihigurarie?source=post_page---byline--b66c486656fe--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lihi Gur Arie, PhD" class="l ep by dd de cx" src="../Images/7a1eb30725a95159401c3672fa5f43ab.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M0YTyQAxsWWqI8MLBi2xrA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--b66c486656fe--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@lihigurarie?source=post_page---byline--b66c486656fe--------------------------------" rel="noopener follow">Lihi Gur Arie, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--b66c486656fe--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/18f9e79654ab0663260df8535cdebe7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C8-FnPBhZur_mA5-3UfcJQ.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Annotations by the author using GroundingDino with the ‘ripened tomato’ prompt. Image by <a class="af nc" href="https://www.pexels.com/photo/green-and-red-oval-fruits-965740/" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a>.</figcaption></figure><h1 id="bf08" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Introduction</h1><p id="b2af" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Until recently, object detection models performed a specific task, like detecting penguins in an image. However, recent advancements in deep learning have given rise to foundation models. These are large models trained on massive datasets in a general manner, making them adaptable for a wide range of tasks. Examples of such models include <a class="af nc" href="https://medium.com/towards-data-science/clip-creating-image-classifiers-without-data-b21c72b741fa" rel="noopener">CLIP</a> for image classification, SAM for segmentation, and GroundingDino for object detection. Foundation models are generally large and computationally demanding. When having no resources limitations, they can be used directly for zero-shot inference. Otherwise, they can be used to tag a datasets for training a smaller, more specific model in a process known as distillation.</p><p id="f0c1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In this guide, we’ll learn how to use GroundingDino model for zero-shot inference of a tomatoes image. We’ll explore the algorithm’s capabilities and use it to tag an entire tomato dataset. The resulted dataset can then be used to train a downstream target model such as YOLO.</p><blockquote class="pa pb pc"><p id="fbb2" class="nz oa pd ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">If you don’t have a paid Medium account, you can read for free <a class="af nc" rel="noopener" target="_blank" href="/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?sk=7c98df89b60ea49a6de9efd5278f645e">here</a>.</p></blockquote><h1 id="ed76" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">GroundingDino</h1><p id="356b" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><strong class="ob fr"><em class="pd">Background</em></strong></p><p id="eaee" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">GroundingDino is a state-of-the-art (SOTA) algorithm developed by IDEA-Research in 2023 [1]. It detects objects from images using text prompts. The name “GroundingDino” is a combination of “grounding” (a process that links vision and language understanding in AI systems) and the transformer-based detector “DINO” [2]. This algorithm is a zero-shot object detector, which means it can identify objects from categories it was not specifically trained on, without needing to see any examples (shots).</p><p id="1f36" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr"><em class="pd">Architecture</em></strong></p><ol class=""><li id="9b9b" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pe pf pg bk">The model takes pairs of image and text description as inputs.</li><li id="bd8f" class="nz oa fq ob b go ph od oe gr pi og oh oi pj ok ol om pk oo op oq pl os ot ou pe pf pg bk">Image features are extracted with an <strong class="ob fr">image backbone</strong> such as Swin Transformer, and text features with a <strong class="ob fr">text backbone</strong> like BERT.</li><li id="3711" class="nz oa fq ob b go ph od oe gr pi og oh oi pj ok ol om pk oo op oq pl os ot ou pe pf pg bk">The <strong class="ob fr">Feature Enhancer</strong> module combines the text and image features through multimodal refinment, using cross attention mechanism that foster interaction between these two modalities.</li><li id="41be" class="nz oa fq ob b go ph od oe gr pi og oh oi pj ok ol om pk oo op oq pl os ot ou pe pf pg bk">Next, the ‘<strong class="ob fr">Language-guided Query Selection</strong>’ module selects the features most relevant to the input text to use as decoder queries.</li><li id="a990" class="nz oa fq ob b go ph od oe gr pi og oh oi pj ok ol om pk oo op oq pl os ot ou pe pf pg bk">These queries are then fed into a <strong class="ob fr">decoder</strong> to refine the prediction of object detection boxes that best align with the text information. It outputs the final bounding boxes suggestions.</li><li id="3d4e" class="nz oa fq ob b go ph od oe gr pi og oh oi pj ok ol om pk oo op oq pl os ot ou pe pf pg bk">The model outputs 900 object bounding boxes and their similarity scores to the input words. The boxes with similarity scores above the <code class="cx pm pn po pp b">box_threshold</code> are chosen, and words whose similarities are higher than the <code class="cx pm pn po pp b">text_threshold</code> as predicted labels.</li></ol></div></div><div class="mr"><div class="ab cb"><div class="lm pq ln pr lo ps cf pt cg pu ci bh"><figure class="mm mn mo mp mq mr pw px paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pv"><img src="../Images/216264702fee5eb326e11e64b8d85eda.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*TWpPggNTp9du5pu1fSMhbQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Xiangyu et al., 2023 [3]</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="caef" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr"><em class="pd">Prompt Engineering</em></strong></p><p id="7b08" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The GroundingDino model encodes text prompts into a learned latent space. Altering the prompts can lead to different text features, which can affect the performance of the detector. To enhance prediction performance, it’s advisable to experiment with multiple prompts, choosing the one that delivers the best results. It’s important to note that while writing this article I had to try several prompts before finding the ideal one, sometimes encountering unexpected results.</p><h1 id="0e6e" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Code Implementation</h1><p id="7147" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><strong class="ob fr"><em class="pd">Getting Started</em></strong></p><p id="bfcb" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">To begin, we’ll clone the <a class="af nc" href="https://github.com/IDEA-Research/GroundingDINO" rel="noopener ugc nofollow" target="_blank">GroundingDino repository</a> from GitHub, set up the environment by installing the necessary dependencies, and download the pre-trained model weights.</p><pre class="mm mn mo mp mq py pp pz bp qa bb bk"><span id="0cf5" class="qb ne fq pp b bg qc qd l qe qf"># Clone:<br/>!git clone https://github.com/IDEA-Research/GroundingDINO.git<br/><br/># Install<br/>%cd GroundingDINO/<br/>!pip install -r requirements.txt<br/>!pip install -q -e .<br/><br/># Get weights<br/>!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth</span></pre><p id="a948" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr"><em class="pd">Inference on an image</em></strong></p><p id="d218" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We’ll start our exploration of the object detection algorithm by applying it to a single image of tomatoes. Our initial goal is to detect all the tomatoes in the image, so we’ll use the text prompt <code class="cx pm pn po pp b">tomato</code>. If you want to use different category names, you can separate them with a dot <code class="cx pm pn po pp b">.</code>. Note that the colors of the bounding boxes are random and have no particular meaning.</p><pre class="mm mn mo mp mq py pp pz bp qa bb bk"><span id="eef3" class="qb ne fq pp b bg qc qd l qe qf">python3 demo/inference_on_a_image.py \<br/>    --config_file 'groundingdino/config/GroundingDINO_SwinT_OGC.py' \<br/>    --checkpoint_path 'groundingdino_swint_ogc.pth' \<br/>    --image_path 'tomatoes_dataset/tomatoes1.jpg'  \<br/>    --text_prompt 'tomato' \<br/>    --box_threshold 0.35 \<br/>    --text_threshold 0.01 \<br/>    --output_dir 'outputs'</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/f8837e49d2c89df34eedfa619d8021a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y3ISbcdF2JEYZoEiHYcoLQ.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Annotations with the ‘<strong class="bf nf">tomato</strong>’ prompt. Image by <a class="af nc" href="https://www.pexels.com/photo/green-and-red-oval-fruits-965740/" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a>.</figcaption></figure><p id="4989" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">GroundingDino not only detects objects as categories, such as tomato, but also comprehends the input text, a task known as Referring Expression Comprehension (REC). Let’s change the text prompt from <code class="cx pm pn po pp b">tomato</code> to <code class="cx pm pn po pp b">ripened tomato</code>, and obtain the outcome:</p><pre class="mm mn mo mp mq py pp pz bp qa bb bk"><span id="6100" class="qb ne fq pp b bg qc qd l qe qf">python3 demo/inference_on_a_image.py \<br/>    --config_file 'groundingdino/config/GroundingDINO_SwinT_OGC.py' \<br/>    --checkpoint_path 'groundingdino_swint_ogc.pth' \<br/>    --image_path 'tomatoes_dataset/tomatoes1.jpg'  \<br/>    --text_prompt 'ripened tomato' \<br/>    --box_threshold 0.35 \<br/>    --text_threshold 0.01 \<br/>    --output_dir 'outputs'</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/ffd779c03397578fc361f8935242b524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tURxAdB8_EIeXjmW7ukQLg.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Annotations with the ‘<strong class="bf nf">ripened tomato</strong>’ prompt. Image by <a class="af nc" href="https://www.pexels.com/photo/green-and-red-oval-fruits-965740/" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a>.</figcaption></figure><p id="af1d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Remarkably, the model can ‘understand’ the text and differentiate between a ‘tomato’ and a ‘ripened tomato’. It even tags partially ripened tomatoes that aren’t fully red. If our task requires tagging only fully ripened red tomatoes, we can adjust the <code class="cx pm pn po pp b">box_threshold</code> from the default 0.35 to 0.5.</p><pre class="mm mn mo mp mq py pp pz bp qa bb bk"><span id="f49f" class="qb ne fq pp b bg qc qd l qe qf">python3 demo/inference_on_a_image.py \<br/>    --config_file 'groundingdino/config/GroundingDINO_SwinT_OGC.py' \<br/>    --checkpoint_path 'groundingdino_swint_ogc.pth' \<br/>    --image_path 'tomatoes_dataset/tomatoes1.jpg'  \<br/>    --text_prompt 'ripened tomato' \<br/>    --box_threshold 0.5 \<br/>    --text_threshold 0.01 \<br/>    --output_dir 'outputs'</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/18f9e79654ab0663260df8535cdebe7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C8-FnPBhZur_mA5-3UfcJQ.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Annotations with the ‘<strong class="bf nf">ripened tomato</strong>’ prompt, with <code class="cx pm pn po pp b"><strong class="bf nf">box_threshold = 0.5</strong></code>. Image by <a class="af nc" href="https://www.pexels.com/photo/green-and-red-oval-fruits-965740/" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a>.</figcaption></figure><p id="8a48" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr"><em class="pd">Generation of tagged dataset</em></strong></p><p id="6e62" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Even though GroundingDino has remarkable capabilities, it’s a large and slow model. If real-time object detection is needed, consider using a faster model like YOLO. Training YOLO and similar models require a lot of tagged data, which can be expensive and time-consuming to produce. However, if your data isn’t unique, you can use GroundingDino to tag it. To learn more about efficient YOLO training, refer to my previous article <a class="af nc" href="https://medium.com/towards-data-science/the-practical-guide-for-object-detection-with-yolov5-algorithm-74c04aac4843" rel="noopener">[4]</a>.</p><p id="eda9" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The GroundingDino repository includes a script to annotate image datasets in the <strong class="ob fr">COCO format</strong>, which is suitable for YOLOx, for instance.</p><pre class="mm mn mo mp mq py pp pz bp qa bb bk"><span id="b341" class="qb ne fq pp b bg qc qd l qe qf">from demo.create_coco_dataset import main<br/><br/>main(image_directory= 'tomatoes_dataset',<br/>    text_prompt= 'tomato',<br/>    box_threshold= 0.35,<br/>    text_threshold = 0.01,<br/>    export_dataset = True,<br/>    view_dataset = False,<br/>    export_annotated_images = True,<br/>    weights_path = 'groundingdino_swint_ogc.pth',<br/>    config_path = 'groundingdino/config/GroundingDINO_SwinT_OGC.py',<br/>    subsample = None<br/>)</span></pre><ul class=""><li id="a078" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou qg pf pg bk">export_dataset — If set to True, the COCO format annotations will be saved in a directory named ‘coco_dataset’.</li><li id="4f71" class="nz oa fq ob b go ph od oe gr pi og oh oi pj ok ol om pk oo op oq pl os ot ou qg pf pg bk">view_dataset — If set to True, the annotated dataset will be displayed for visualization in the FiftyOne app.</li><li id="a10e" class="nz oa fq ob b go ph od oe gr pi og oh oi pj ok ol om pk oo op oq pl os ot ou qg pf pg bk">export_annotated_images — If set to True, the annotated images will be stored in a directory named ‘images_with_bounding_boxes’.</li><li id="5c95" class="nz oa fq ob b go ph od oe gr pi og oh oi pj ok ol om pk oo op oq pl os ot ou qg pf pg bk">subsample (int) — If specified, only this number of images from the dataset will be annotated.</li></ul><p id="ef3d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Different YOLO algorithms require different annotation formats. If you’re planning to train YOLOv5 or YOLOv8, you’ll need to export your dataset in the <strong class="ob fr">YOLOv5 format</strong>. Although the export type is hard-coded in the main script, you can easily change it by adjusting the <code class="cx pm pn po pp b">dataset_type</code> argument in <code class="cx pm pn po pp b">create_coco_dataset.main</code>, from <code class="cx pm pn po pp b">fo.types.COCODetectionDataset</code> to <code class="cx pm pn po pp b">fo.types.YOLOv5Dataset</code>(line 72). To keep things organized, we’ll also change the output directory name from ‘coco_dataset’ to ‘yolov5_dataset’. After changing the script, run <code class="cx pm pn po pp b">create_coco_dataset.main</code> again.</p><pre class="mm mn mo mp mq py pp pz bp qa bb bk"><span id="7b77" class="qb ne fq pp b bg qc qd l qe qf">  if export_dataset:<br/>      dataset.export(<br/>          'yolov5_dataset',<br/>          dataset_type=fo.types.YOLOv5Dataset<br/>      )</span></pre><h1 id="37b3" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Concluding remarks</h1><p id="f880" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">GroundingDino offers a significant leap in object detection annotations by using text prompts. In this tutorial, we have explored how to use the model for automated labeling of an image or a whole dataset. It’s crucial, however, to manually review and verify these annotations before they are utilized in training subsequent models.</p><p id="7943" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">_________________________________________________________________</p><p id="7539" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><em class="pd">A user-friendly Jupyter notebook containing the complete code is included for your convenience:</em></p><figure class="mm mn mo mp mq mr"><div class="qh io l ed"><div class="qi qj l"/></div></figure><h1 id="8cbf" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Thank you for reading!</h1><p id="29d8" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><strong class="ob fr">Want to learn more?</strong></p><ul class=""><li id="4d4d" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou qg pf pg bk"><a class="af nc" href="https://medium.com/@lihigurarie" rel="noopener"><strong class="ob fr">Explore</strong></a> additional articles I’ve written</li><li id="9a0f" class="nz oa fq ob b go ph od oe gr pi og oh oi pj ok ol om pk oo op oq pl os ot ou qg pf pg bk"><a class="af nc" href="https://medium.com/@lihigurarie/subscribe" rel="noopener"><strong class="ob fr">Subscribe</strong></a><strong class="ob fr"> </strong>to get notified when I publish articles</li><li id="a38e" class="nz oa fq ob b go ph od oe gr pi og oh oi pj ok ol om pk oo op oq pl os ot ou qg pf pg bk">Follow me on <a class="af nc" href="https://www.linkedin.com/in/lihi-gur-arie/" rel="noopener ugc nofollow" target="_blank"><strong class="ob fr">Linkedin</strong></a></li></ul><h1 id="f995" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">References</h1><p id="074e" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">[0] Code on Colab Notebook: <a class="af nc" href="https://gist.github.com/Lihi-Gur-Arie/90e07b06a0cd756df9b5d29b96f7523a" rel="noopener ugc nofollow" target="_blank">link</a></p><p id="e3d8" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">[1] <a class="af nc" href="https://arxiv.org/pdf/2303.05499.pdf" rel="noopener ugc nofollow" target="_blank">Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</a>, 2023.</p><p id="7968" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">[2] <a class="af nc" href="https://arxiv.org/pdf/2203.03605.pdf" rel="noopener ugc nofollow" target="_blank">Dino: Detr with improved denoising anchor boxes for end-to-end object detection</a>, 2022.</p><p id="2072" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">[3] <a class="af nc" href="https://arxiv.org/pdf/2401.02361v2.pdf" rel="noopener ugc nofollow" target="_blank">An Open and Comprehensive Pipeline for Unified Object Grounding and Detection</a>, 2023.</p><p id="4b20" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">[4] <a class="af nc" href="https://medium.com/towards-data-science/the-practical-guide-for-object-detection-with-yolov5-algorithm-74c04aac4843" rel="noopener">The practical guide for Object Detection with YOLOv5 algorithm</a>, by Dr. Lihi Gur Arie.</p></div></div></div></div>    
</body>
</html>