- en: Understand REINFORCE, Actor-Critic, and PPO in One Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understand-reinforce-actor-critic-and-ppo-in-one-go-2569f520c066?source=collection_archive---------7-----------------------#2024-07-24](https://towardsdatascience.com/understand-reinforce-actor-critic-and-ppo-in-one-go-2569f520c066?source=collection_archive---------7-----------------------#2024-07-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Use the loss function of the Policy Gradient algorithm as key to understand
    various reinforcement learning algorithms: REINFORCE, Actor-Critic, and PPO, which
    are theoretical preparations to understand the Reinforcement Learning from Human
    Feedback (RLHF) algorithm used to build ChatGPT.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jasonweiyi.medium.com/?source=post_page---byline--2569f520c066--------------------------------)[![Wei
    Yi](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page---byline--2569f520c066--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2569f520c066--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2569f520c066--------------------------------)
    [Wei Yi](https://jasonweiyi.medium.com/?source=post_page---byline--2569f520c066--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2569f520c066--------------------------------)
    ·44 min read·Jul 24, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/314a362a79c9bafe7bd358fb0f1d3c57.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Unsplash](https://unsplash.com/photos/white-metal-trusses-F_zMkhov6G4)
  prefs: []
  type: TYPE_NORMAL
- en: Studying reinforcement learning can be frustrating because the field is cursed
    with confusing jargon and algorithms with subtle differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'I struggled, until one day my great colleague [Peter Vrancs](https://www.linkedin.com/in/petervrancx/)
    swiftly wrote down the derivation of the loss function for the Policy Gradient
    algorithm REINFORCE for me. Using this derivation, this article links the following
    algorithms together:'
  prefs: []
  type: TYPE_NORMAL
- en: REINFORCE
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The concept of advantage for variance reduction, and the Actor-Critic algorithm
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Proximal Policy Optimisation (PPO)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even if there are many articles covering these algorithms, this article provides
    a unique angle of studying them in one go to save you learning time!
  prefs: []
  type: TYPE_NORMAL
- en: In my opinion, understanding these three algorithms is the theoretical bare…
  prefs: []
  type: TYPE_NORMAL
