- en: Understand REINFORCE, Actor-Critic, and PPO in One Go
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一次性理解REINFORCE、Actor-Critic和PPO
- en: 原文：[https://towardsdatascience.com/understand-reinforce-actor-critic-and-ppo-in-one-go-2569f520c066?source=collection_archive---------7-----------------------#2024-07-24](https://towardsdatascience.com/understand-reinforce-actor-critic-and-ppo-in-one-go-2569f520c066?source=collection_archive---------7-----------------------#2024-07-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understand-reinforce-actor-critic-and-ppo-in-one-go-2569f520c066?source=collection_archive---------7-----------------------#2024-07-24](https://towardsdatascience.com/understand-reinforce-actor-critic-and-ppo-in-one-go-2569f520c066?source=collection_archive---------7-----------------------#2024-07-24)
- en: 'Use the loss function of the Policy Gradient algorithm as key to understand
    various reinforcement learning algorithms: REINFORCE, Actor-Critic, and PPO, which
    are theoretical preparations to understand the Reinforcement Learning from Human
    Feedback (RLHF) algorithm used to build ChatGPT.'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用策略梯度算法的损失函数作为理解各种强化学习算法的关键：REINFORCE、Actor-Critic和PPO，这些是理解用于构建ChatGPT的“人类反馈强化学习”（RLHF）算法的理论准备。
- en: '[](https://jasonweiyi.medium.com/?source=post_page---byline--2569f520c066--------------------------------)[![Wei
    Yi](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page---byline--2569f520c066--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2569f520c066--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2569f520c066--------------------------------)
    [Wei Yi](https://jasonweiyi.medium.com/?source=post_page---byline--2569f520c066--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jasonweiyi.medium.com/?source=post_page---byline--2569f520c066--------------------------------)[![魏一](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page---byline--2569f520c066--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2569f520c066--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2569f520c066--------------------------------)
    [魏一](https://jasonweiyi.medium.com/?source=post_page---byline--2569f520c066--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2569f520c066--------------------------------)
    ·44 min read·Jul 24, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2569f520c066--------------------------------)
    ·44分钟阅读·2024年7月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/314a362a79c9bafe7bd358fb0f1d3c57.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/314a362a79c9bafe7bd358fb0f1d3c57.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/white-metal-trusses-F_zMkhov6G4)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[Unsplash](https://unsplash.com/photos/white-metal-trusses-F_zMkhov6G4)
- en: Studying reinforcement learning can be frustrating because the field is cursed
    with confusing jargon and algorithms with subtle differences.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 学习强化学习可能令人沮丧，因为这个领域充满了令人困惑的术语和细微差别的算法。
- en: 'I struggled, until one day my great colleague [Peter Vrancs](https://www.linkedin.com/in/petervrancx/)
    swiftly wrote down the derivation of the loss function for the Policy Gradient
    algorithm REINFORCE for me. Using this derivation, this article links the following
    algorithms together:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾一度感到困惑，直到有一天，我的伟大同事[Peter Vrancs](https://www.linkedin.com/in/petervrancx/)迅速为我写下了策略梯度算法REINFORCE的损失函数推导。通过这个推导，本文将以下算法联系在一起：
- en: REINFORCE
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: REINFORCE
- en: The concept of advantage for variance reduction, and the Actor-Critic algorithm
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方差减少的优势概念，以及Actor-Critic算法
- en: Proximal Policy Optimisation (PPO)
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 近端策略优化（PPO）
- en: Even if there are many articles covering these algorithms, this article provides
    a unique angle of studying them in one go to save you learning time!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有很多文章涵盖这些算法，本文提供了一个独特的角度，将它们一次性学习，帮助你节省学习时间！
- en: In my opinion, understanding these three algorithms is the theoretical bare…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，理解这三种算法是理论上的基础……
