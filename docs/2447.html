<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Implementing Sequential Algorithms on TPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Implementing Sequential Algorithms on TPU</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-sequential-algorithms-on-tpu-41d75c6aaa95?source=collection_archive---------8-----------------------#2024-10-07">https://towardsdatascience.com/implementing-sequential-algorithms-on-tpu-41d75c6aaa95?source=collection_archive---------8-----------------------#2024-10-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c31e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Accelerating AI/ML Model Training with Custom Operators — Part 3.A</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--41d75c6aaa95--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--41d75c6aaa95--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--41d75c6aaa95--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--41d75c6aaa95--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/a2daa59de127264e6b8d93713b38f15d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZrLopni7PfVX3PWU"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@hdbernd?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Bernd Dittrich</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="3316" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is a direct sequel to a <a class="af nb" href="https://chaimrand.medium.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a" rel="noopener">previous post</a> on the topic of implementing custom TPU operations with <a class="af nb" href="https://jax.readthedocs.io/en/latest/pallas/index.html" rel="noopener ugc nofollow" target="_blank">Pallas</a>. Of particular interest are custom kernels that leverage the unique properties of the TPU architecture in a manner that optimizes runtime performance. In this post, we will attempt to demonstrate this opportunity by applying the power of Pallas to the challenge of running sequential algorithms that are interspersed within a predominantly parallelizable deep learning (DL) workload.</p><p id="f95b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We will focus on <a class="af nb" href="https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/" rel="noopener ugc nofollow" target="_blank">Non Maximum Suppression (NMS)</a> of bounding-box proposals as a representative algorithm, and explore ways to optimize its implementation. An important component of computer vision (CV) <a class="af nb" href="https://en.wikipedia.org/wiki/Object_detection" rel="noopener ugc nofollow" target="_blank">object detection</a> solutions (e.g., <a class="af nb" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank">Mask RCNN</a>), NMS is commonly used to filter out overlapping bounding boxes, keeping only the “best” ones. NMS receives a list of bounding box proposals, an associated list of scores, and an <a class="af nb" href="https://en.wikipedia.org/wiki/IOU" rel="noopener ugc nofollow" target="_blank">IOU</a> threshold, and proceeds to <em class="ny">greedily </em>and <em class="ny">iteratively </em>choose the remaining box with the highest score and disqualify all other boxes with which it has an IOU that exceeds the given threshold. The fact that the box chosen at the <em class="ny">n-th</em> iteration depends on the preceding <em class="ny">n-1 </em>steps of the algorithm dictates the sequential nature of its implementation. Please see <a class="af nb" href="https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/" rel="noopener ugc nofollow" target="_blank">here</a> and/or <a class="af nb" href="https://medium.com/analytics-vidhya/non-max-suppression-nms-6623e6572536" rel="noopener">here</a> for more on the rationale behind NMS and its implementation. Although we have chosen to focus on one specific algorithm, most of our discussion should carry over to other sequential algorithms.</p><h2 id="4f1b" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">Offloading Sequential Algorithms to CPU</h2><p id="3d65" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The presence of a sequential algorithm within a predominantly parallelizable ML model (e.g., Mask R-CNN) presents an interesting challenge. While GPUs, commonly used for such workloads, excel at executing parallel operations like matrix multiplication, they can significantly underperform compared to CPUs when handling sequential algorithms. This often leads to computation graphs that include crossovers between the GPU and CPU, where the GPU handles the parallel operations and the CPU handles the sequential ones. NMS is a prime example of a sequential algorithm that is commonly offloaded onto the CPU. In fact, a close analysis of <a class="af nb" href="https://pytorch.org/vision/stable/index.html" rel="noopener ugc nofollow" target="_blank">torchvision</a>’s “CUDA” implementation of <a class="af nb" href="https://github.com/pytorch/vision/blob/v0.19.1/torchvision/csrc/ops/cuda/nms_kernel.cu" rel="noopener ugc nofollow" target="_blank">NMS</a>, reveals that even it runs a significant portion of the algorithm on <a class="af nb" href="https://github.com/pytorch/vision/blob/v0.19.1/torchvision/csrc/ops/cuda/nms_kernel.cu#L136C33-L136C41" rel="noopener ugc nofollow" target="_blank">CPU</a>.</p><p id="4d11" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Although offloading sequential operations to the CPU may lead to improved runtime performance, there are several potential drawbacks to consider:</p><ol class=""><li id="d18b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oz pa pb bk">Cross-device execution between the CPU and GPU usually requires multiple points of synchronization between the devices which commonly results in idle time on the GPU while it waits for the CPU to complete its tasks. Given that the GPU is typically the most expensive component of the training platform our goal is to minimize such idle time.</li><li id="62c3" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oz pa pb bk">In standard ML workflows, the CPU is responsible for preparing and feeding data to the model, which resides on the GPU. If the data input pipeline involves compute-intensive processing, this can strain the CPU, leading to “input starvation” on the GPU. In such scenarios, offloading portions of the model’s computation to the CPU could further exacerbate this issue.</li></ol><p id="39b7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To avoid these drawbacks you could consider alternative approaches, such as replacing the sequential algorithm with a comparable alternative (e.g., the one suggested <a class="af nb" href="https://hertasecurity.com/wp-content/uploads/work-efficient-parallel-non-maximum-suppression.pdf" rel="noopener ugc nofollow" target="_blank">here</a>), settling for a slow/suboptimal GPU implementation of the sequential algorithm, or running the workload on CPU — each of which come with their own potential trade-offs.</p><h2 id="2dfd" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">Sequential Algorithms on TPU</h2><p id="17b9" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">This is where the unique architecture of the TPU could present an opportunity. Contrary to GPUs, TPUs are sequential processors. While their ability to run highly vectorized operations makes them competitive with GPUs when running parallelizable operations such as matrix multiplication, their sequential nature could make them uniquely suited for running ML workloads that include a mix of both sequential and parallel components. Armed with the <a class="af nb" href="https://jax.readthedocs.io/en/latest/pallas/index.html" rel="noopener ugc nofollow" target="_blank">Pallas extension</a> to JAX, our <a class="af nb" href="https://chaimrand.medium.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a" rel="noopener">newfound TPU kernel creation</a> tool, we will evaluate this opportunity by implementing and evaluating a custom implementation of NMS for TPU.</p><h2 id="3b21" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">Disclaimers</h2><p id="d5ed" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The NMS implementations we will share below are intended for demonstrative purposes only. We have not made any significant effort to optimize them or to verify their robustness, durability, or accuracy. Please keep in mind that, as of the time of this writing, Pallas is an <em class="ny">experimental</em> feature — still under active development. The code we share (based on <a class="af nb" href="https://pypi.org/project/jax/" rel="noopener ugc nofollow" target="_blank">JAX</a> version 0.4.32) may become outdated by the time you read this. Be sure to refer to the most up-to-date APIs and resources available for your Pallas development. Please do not view our mention of any algorithm, library, or API as an endorsement for their use.</p><h1 id="032d" class="ph oa fq bf ob pi pj gq of pk pl gt oj pm pn po pp pq pr ps pt pu pv pw px py bk">NMS on CPU</h1><p id="b2d7" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">We begin with a simple implementation of NMS in <a class="af nb" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank">numpy</a> that will serve as a baseline for performance comparison:</p><pre class="ml mm mn mo mp pz qa qb bp qc bb bk"><span id="e2dc" class="qd oa fq qa b bg qe qf l qg qh">import numpy as np<br/><br/>def nms_cpu(boxes, scores, max_output_size, threshold=0.1):<br/>    epsilon = 1e-5<br/><br/>    # Convert bounding boxes and scores to numpy<br/>    boxes = np.array(boxes)<br/>    scores = np.array(scores)<br/><br/>    # coordinates of bounding boxes<br/>    start_x = boxes[:, 0]<br/>    start_y = boxes[:, 1]<br/>    end_x = boxes[:, 2]<br/>    end_y = boxes[:, 3]<br/><br/>    # Compute areas of bounding boxes<br/>    areas = (end_x - start_x) * (end_y - start_y)<br/><br/>    # Sort by confidence score of bounding boxes<br/>    order = np.argsort(scores)<br/><br/>    # Picked bounding boxes<br/>    picked_boxes = []<br/><br/>    # Iterate over bounding boxes<br/>    while order.size &gt; 0 and len(picked_boxes) &lt; max_output_size:<br/>        <br/>        # The index of the remaining box with the highest score<br/>        index = order[-1]<br/><br/>        # Pick the bounding box with largest confidence score<br/>        picked_boxes.append(index.item())<br/><br/>        # Compute coordinates of intersection<br/>        x1 = np.maximum(start_x[index], start_x[order[:-1]])<br/>        x2 = np.minimum(end_x[index], end_x[order[:-1]])<br/>        y1 = np.maximum(start_y[index], start_y[order[:-1]])<br/>        y2 = np.minimum(end_y[index], end_y[order[:-1]])<br/><br/>        # Compute areas of intersection and union<br/>        w = np.maximum(x2 - x1, 0.0)<br/>        h = np.maximum(y2 - y1, 0.0)<br/><br/>        intersection = w * h<br/>        union = areas[index] + areas[order[:-1]] - intersection<br/><br/>        # Compute the ratio between intersection and union<br/>        ratio = intersection / np.clip(union, min=epsilon)<br/><br/>        # discard boxes above overlap threshold<br/>        keep = np.where(ratio &lt; threshold)<br/>        order = order[keep]<br/><br/>    return picked_boxes</span></pre><p id="dbcf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To evaluate the performance of our NMS function, we generate a batch of random boxes and scores (as JAX tensors) and run the script on a <a class="af nb" href="https://cloud.google.com/tpu/docs/v5e" rel="noopener ugc nofollow" target="_blank">Google Cloud TPU v5e</a> system using the same environment and same benchmarking utility as in our <a class="af nb" href="https://chaimrand.medium.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a" rel="noopener">previous post</a>. For this experiment, we specify the CPU as the <a class="af nb" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.default_device.html" rel="noopener ugc nofollow" target="_blank">JAX default device</a>:</p><pre class="ml mm mn mo mp pz qa qb bp qc bb bk"><span id="17f5" class="qd oa fq qa b bg qe qf l qg qh">import jax<br/>from jax import random<br/>import jax.numpy as jnp<br/><br/>def generate_random_boxes(run_on_cpu = False):<br/>    if run_on_cpu:<br/>        jax.config.update('jax_default_device', jax.devices('cpu')[0])<br/>    else:<br/>        jax.config.update('jax_default_device', jax.devices('tpu')[0])<br/><br/>    n_boxes = 1024<br/>    img_size = 1024<br/><br/>    k1, k2, k3 = random.split(random.key(0), 3)<br/><br/>    # Randomly generate box sizes and positions<br/>    box_sizes = random.randint(k1,<br/>                               shape=(n_boxes, 2),<br/>                               minval=1,<br/>                               maxval=img_size)<br/>    top_left = random.randint(k2,<br/>                              shape=(n_boxes, 2),<br/>                              minval=0,<br/>                              maxval=img_size - 1)<br/>    bottom_right = jnp.clip(top_left + box_sizes, 0, img_size - 1)<br/><br/>    # Concatenate top-left and bottom-right coordinates<br/>    rand_boxes = jnp.concatenate((top_left, bottom_right),<br/>                                 axis=1).astype(jnp.bfloat16)<br/>    rand_scores = jax.random.uniform(k3, <br/>                                     shape=(n_boxes,),<br/>                                     minval=0.0,<br/>                                     maxval=1.0)<br/><br/>    return rand_boxes, rand_scores<br/><br/>rand_boxes, rand_scores = generate_random_boxes(run_on_cpu=True)<br/><br/>time = benchmark(nms_cpu)(rand_boxes, rand_scores, max_output_size=128)<br/>print(f'nms_cpu: {time}')</span></pre><p id="b41e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The resultant average runtime is 2.99 milliseconds. Note the assumption that the input and output tensors reside on the CPU. If they are on the TPU, then the time to copy them between the devices should also be taken into consideration.</p><h1 id="ad6f" class="ph oa fq bf ob pi pj gq of pk pl gt oj pm pn po pp pq pr ps pt pu pv pw px py bk">NMS on TPU</h1><p id="c14f" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">If our NMS function is a component within a larger computation graph running on the TPU, we might prefer a TPU-compatible implementation to avoid the drawbacks of cross-device execution. The code block below contains a JAX implementation of NMS specifically designed to enable acceleration via JIT compilation. Denoting the number of boxes by <em class="ny">N</em>, we begin by calculating the IOU between each of the <em class="ny">N(N-1)</em> pairs of boxes and preparing an <em class="ny">N</em>x<em class="ny">N </em>boolean tensor (<em class="ny">mask_threshold</em>)<em class="ny"> </em>where the (<em class="ny">i,j</em>)-th entry indicates whether the IOU between boxes <em class="ny">i</em> and <em class="ny">j</em> exceed the predefined threshold.</p><p id="ceba" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To simplify the iterative selection of boxes, we create a copy of the mask tensor (<em class="ny">mask_threshold2</em>) where the diagonal elements are zeroed to prevent a box from suppressing itself. We further define two score-tracking tensors: <em class="ny">out_scores</em>, which retains the scores of the chosen boxes (and zeros the scores of the eliminated ones), and <em class="ny">remaining_scores</em>, which maintains the scores of the boxes still being considered. We then use the <a class="af nb" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.while_loop.html" rel="noopener ugc nofollow" target="_blank">jax.lax.while_loop</a> function to iteratively choose boxes while updating the <em class="ny">out_scores</em> and <em class="ny">remaining_scores </em>tensors. Note that the format of the output of this function differs from the previous function and may need to be adjusted to fit into subsequent steps of the computation graph.</p><pre class="ml mm mn mo mp pz qa qb bp qc bb bk"><span id="d4fa" class="qd oa fq qa b bg qe qf l qg qh">import functools<br/><br/># Given N boxes, calculates mask_threshold an NxN boolean mask<br/># where the (i,j) entry indicates whether the IOU of boxes i and j<br/># exceed the threshold. Returns mask_threshold, mask_threshold2<br/># which is equivalent to mask_threshold with zero diagonal and<br/># the scores modified so that all values are greater than 0<br/>def init_tensors(boxes, scores, threshold=0.1):<br/>    epsilon = 1e-5<br/><br/>    # Extract left, top, right, bottom coordinates<br/>    left = boxes[:, 0]<br/>    top = boxes[:, 1]<br/>    right = boxes[:, 2]<br/>    bottom = boxes[:, 3]<br/><br/>    # Compute areas of boxes<br/>    areas = (right - left) * (bottom - top)<br/><br/>    # Calculate intersection points<br/>    inter_l = jnp.maximum(left[None, :], left[:, None])<br/>    inter_t = jnp.maximum(top[None, :], top[:, None])<br/>    inter_r = jnp.minimum(right[None, :], right[:, None])<br/>    inter_b = jnp.minimum(bottom[None, :], bottom[:, None])<br/><br/>    # Width, height, and area of the intersection<br/>    inter_w = jnp.clip(inter_r - inter_l, 0)<br/>    inter_h = jnp.clip(inter_b - inter_t, 0)<br/>    inter_area = inter_w * inter_h<br/><br/>    # Union of the areas<br/>    union = areas[None, :] + areas[:, None] - inter_area<br/><br/>    # IoU calculation<br/>    iou = inter_area / jnp.clip(union, epsilon)<br/><br/>    # Shift scores to be greater than zero<br/>    out_scores = scores - jnp.min(scores) + epsilon<br/><br/>    # Create mask based on IoU threshold<br/>    mask_threshold = iou &gt; threshold<br/><br/>    # Create mask excluding diagonal (i.e., self IoU is ignored)<br/>    mask_threshold2 = mask_threshold * (1-jnp.eye(mask_threshold.shape[0],<br/>                                                  dtype=mask_threshold.dtype))<br/><br/>    return mask_threshold, mask_threshold2, out_scores<br/><br/>@functools.partial(jax.jit, static_argnames=['max_output_size', 'threshold'])<br/>def nms_jax(boxes, scores, max_output_size, threshold=0.1):<br/>    # initialize mask and score tensors<br/>    mask_threshold, mask_threshold2, out_scores = init_tensors(boxes,<br/>                                                           scores,<br/>                                                           threshold)<br/><br/>    # The out_scores tensor will retain the scores of the chosen boxes<br/>    # and zero the scores of the eliminated ones<br/>    # remaining_scores will maintain non-zero scores for boxes that<br/>    # have not been chosen or eliminated<br/>    remaining_scores = out_scores.copy()<br/><br/>    def choose_box(state):<br/>        i, remaining_scores, out_scores = state<br/>        # choose index of box with highest score from remaining scores<br/>        index = jnp.argmax(remaining_scores)<br/>        # check validity of chosen box<br/>        valid = remaining_scores[index] &gt; 0<br/>        # If valid, zero all scores with IOU greater than threshold<br/>        # (including the chosen index)<br/>        remaining_scores = jnp.where(mask_threshold[index] *valid,<br/>                                     0,<br/>                                     remaining_scores)<br/>        # zero the scores of the eliminated tensors (not including<br/>        # the chosen index)<br/>        out_scores = jnp.where(mask_threshold2[index]*valid,<br/>                               0,<br/>                               out_scores)<br/><br/>        i = i + 1<br/>        return i, remaining_scores, out_scores<br/><br/>    def cond_fun(state):<br/>        i, _, _ = state<br/>        return (i &lt; max_output_size)<br/><br/>    i = 0<br/>    state = (i, remaining_scores, out_scores)<br/><br/>    _, _, out_scores = jax.lax.while_loop(cond_fun, choose_box, state)<br/><br/>    # Output the resultant scores. To extract the chosen boxes,<br/>    # Take the max_output_size highest scores:<br/>    # min = jnp.minimum(jnp.count_nonzero(scores), max_output_size)<br/>    # indexes = jnp.argsort(out_scores, descending=True)[:min]<br/>    return out_scores<br/><br/># nms_jax can be run on either the CPU the TPU<br/>rand_boxes, rand_scores = generate_random_boxes(run_on_cpu=True)<br/><br/>time = benchmark(nms_jax)(rand_boxes, rand_scores, max_output_size=128)<br/>print(f'nms_jax on CPU: {time}')<br/><br/>rand_boxes, rand_scores = generate_random_boxes(run_on_cpu=False)<br/><br/>time = benchmark(nms_jax)(rand_boxes, rand_scores, max_output_size=128)<br/>print(f'nms_jax on TPU: {time}')</span></pre><p id="f98c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The runtimes of this implementation of NMS are 1.231 and 0.416 milliseconds on CPU and TPU, respectively.</p><h1 id="bec4" class="ph oa fq bf ob pi pj gq of pk pl gt oj pm pn po pp pq pr ps pt pu pv pw px py bk">Custom NMS Pallas Kernel</h1><p id="ea59" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">We now present a custom implementation of NMS in which we explicitly leverage the fact that on TPUs Pallas kernels are executed in a <a class="af nb" href="https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#grid-a-k-a-kernels-in-a-loop" rel="noopener ugc nofollow" target="_blank">sequential manner</a>. Our implementation uses two boolean matrix masks and two score-keeping tensors, similar to the approach in our previous function.</p><p id="94d2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We define a kernel function, <em class="ny">choose_box</em>, responsible for selecting the next box and updating the score-keeping tensors, which are maintained in scratch memory. We invoke the kernel across a one-dimensional grid where the number of steps (i.e., the grid-size) is determined by the <em class="ny">max_output_size </em>parameter.</p><p id="b1d3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that due to some <a class="af nb" href="https://jax.readthedocs.io/en/latest/pallas/tpu/details.html#supported-operations" rel="noopener ugc nofollow" target="_blank">limitations</a> (as of the time of this writing) on the operations supported by Pallas, some acrobatics are required to implement both the “argmax” function and the validity check for the selected boxes. For the sake of brevity, we omit the technical details and refer the interested reader to the comments in the code below.</p><pre class="ml mm mn mo mp pz qa qb bp qc bb bk"><span id="3e78" class="qd oa fq qa b bg qe qf l qg qh">from jax.experimental import pallas as pl<br/>from jax.experimental.pallas import tpu as pltpu<br/><br/># argmax helper function<br/>def pallas_argmax(scores, n_boxes):<br/>    # we assume that the index of each box is stored in the<br/>    # least significant bits of the score (see below)<br/>    idx = jnp.max(scores.astype(float)).astype(int) % n_boxes<br/>    return idx<br/><br/># Pallas kernel definition<br/>def choose_box(scores, thresh_mask1, thresh_mask2, ret_scores,<br/>               scores_scratch, remaining_scores_scratch, *, nsteps, n_boxes):<br/>    # initialize scratch memory on first step<br/>    @pl.when(pl.program_id(0) == 0)<br/>    def _():<br/>        scores_scratch[...] = scores[...]<br/>        remaining_scores_scratch[...] = scores[...]<br/><br/>    remaining_scores = remaining_scores_scratch[...]<br/><br/>    # choose box<br/>    idx = pallas_argmax(remaining_scores, n_boxes)<br/><br/>    # we use any to verfiy validity of the chosen box due<br/>    # to limitations on indexing in pallas<br/>    valid = (remaining_scores&gt;0).any()<br/><br/>    # updating score tensors<br/>    remaining_scores_scratch[...] = jnp.where(thresh_mask1[idx,...]*valid,<br/>                                              0,<br/>                                              remaining_scores)<br/>    scores_scratch[...] = jnp.where(thresh_mask2[idx,...]*valid,<br/>                                    0,<br/>                                    scores_scratch[...])<br/><br/>    # set return value on final step<br/>    @pl.when(pl.program_id(0) == nsteps - 1)<br/>    def _():<br/>        ret_scores[...] = scores_scratch[...]<br/><br/>@functools.partial(jax.jit, static_argnames=['max_output_size', 'threshold'])<br/>def nms_pallas(boxes, scores, max_output_size, threshold=0.1):<br/>    n_boxes = scores.size<br/>    mask_threshold, mask_threshold2, scores = init_tensors(boxes, <br/>                                                           scores,<br/>                                                           threshold)<br/><br/>    # In order to work around the Pallas argsort limitation<br/>    # we create a new scores tensor with the same ordering of<br/>    # the input scores tensor in which the index of each score<br/>    # in the ordering is encoded in the least significant bits<br/>    sorted = jnp.argsort(scores, descending=True)<br/><br/>    # descending integers: n_boxes-1, ..., 2, 1, 0<br/>    descending = jnp.flip(jnp.arange(n_boxes))<br/><br/>    # new scores in descending with the least significant<br/>    # bits carrying the argsort of the input scores<br/>    ordered_scores = n_boxes * descending + sorted<br/><br/>    # new scores with same ordering as input scores<br/>    scores = jnp.empty_like(ordered_scores<br/>                            ).at[sorted].set(ordered_scores)<br/><br/>    grid = (max_output_size,)<br/>    return pl.pallas_call(<br/>        functools.partial(choose_box, <br/>                          nsteps=max_output_size,<br/>                          n_boxes=n_boxes),<br/>        grid_spec=pltpu.PrefetchScalarGridSpec(<br/>            num_scalar_prefetch=0,<br/>            in_specs=[<br/>                pl.BlockSpec(block_shape=(n_boxes,)),<br/>                pl.BlockSpec(block_shape=(n_boxes, n_boxes)),<br/>                pl.BlockSpec(block_shape=(n_boxes, n_boxes)),<br/>            ],<br/>            out_specs=pl.BlockSpec(block_shape=(n_boxes,)),<br/>            scratch_shapes=[pltpu.VMEM((n_boxes,), scores.dtype),<br/>                            pltpu.VMEM((n_boxes,), scores.dtype)],<br/>            grid=grid,<br/>        ),<br/>        out_shape=jax.ShapeDtypeStruct((n_boxes,), scores.dtype),<br/>        compiler_params=dict(mosaic=dict(<br/>            dimension_semantics=("arbitrary",)))<br/>    )(scores, mask_threshold, mask_threshold2)<br/><br/>rand_boxes, rand_scores = generate_random_boxes(run_on_cpu=False)<br/><br/>time = benchmark(nms_pallas)(rand_boxes, rand_scores, max_output_size=128)<br/>print(f'nms_pallas: {time}')</span></pre><p id="b0be" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The average runtime of our custom NMS operator is 0.139 milliseconds, making it roughly three times faster than our JAX-native implementation. This result highlights the potential of tailoring the implementation of sequential algorithms to the unique properties of the TPU architecture.</p><p id="51bc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that in our Pallas kernel implementation, we load the full input tensors into <a class="af nb" href="https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.html#constraints-of-using-vmem-smem" rel="noopener ugc nofollow" target="_blank">TPU VMEM memory</a>. Given the limited capacity of VMEM, scaling up the input size (i.e., increase the number of bounding boxes) will likely lead to memory issues. Typically, such limitations can be addressed by <a class="af nb" href="https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#blockspec-a-k-a-how-to-chunk-up-inputs" rel="noopener ugc nofollow" target="_blank">chunking the inputs</a> with BlockSpecs. Unfortunately, applying this approach would break the current NMS implementation. Implementing NMS across input chunks would require a different design, which is beyond the scope of this post.</p><h1 id="0bd0" class="ph oa fq bf ob pi pj gq of pk pl gt oj pm pn po pp pq pr ps pt pu pv pw px py bk">Results</h1><p id="aa1e" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The results of our experiments are summarized in the table below:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qi"><img src="../Images/3f06e6fa7106deb64169f9bfcf4f9b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*TGJZzNrCGm0nhmD3WgGHew.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Results of NMS experiments (lower is better) — by Author</figcaption></figure><p id="1a1e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These results demonstrate the potential for running full ML computation graphs on TPU, even when they include sequential components. The performance improvement demonstrated by our Pallas NMS operator, in particular, highlights the opportunity of customizing kernels in a way that leverages the TPUs strengths.</p><h1 id="eba9" class="ph oa fq bf ob pi pj gq of pk pl gt oj pm pn po pp pq pr ps pt pu pv pw px py bk">Summary</h1><p id="77e4" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">In our <a class="af nb" href="https://chaimrand.medium.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a" rel="noopener">previous post</a> we learned of the opportunity for building custom TPU operators using the Pallas extension for JAX. Maximizing this opportunity requires tailoring the kernel implementations to the specific properties of the TPU architecture. In this post, we focused on the sequential nature of the TPU processor and its use in optimizing a custom NMS kernel. While scaling the solution to support an unrestricted number of bounding boxes would require further work, the core principles we have discussed remain applicable.</p><p id="deb5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Still in the experimental phase of its development, there remain some limitations in Pallas that may require creative workarounds. But the strength and potential are clearly evident and we anticipate that they will only increase as the framework matures.</p></div></div></div></div>    
</body>
</html>