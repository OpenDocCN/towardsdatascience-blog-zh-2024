- en: An Off-Beat Approach to Train-Test-Validation Split Your Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-off-beat-approach-to-train-test-validation-split-your-dataset-esp-small-ones-650492b735fb?source=collection_archive---------2-----------------------#2024-07-07](https://towardsdatascience.com/an-off-beat-approach-to-train-test-validation-split-your-dataset-esp-small-ones-650492b735fb?source=collection_archive---------2-----------------------#2024-07-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ensuring distributional integrity in splits of small datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@soniamarpreet17?source=post_page---byline--650492b735fb--------------------------------)[![Amarpreet
    Singh](../Images/eaa74303ea5583d1bfb44364883ef53f.png)](https://medium.com/@soniamarpreet17?source=post_page---byline--650492b735fb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--650492b735fb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--650492b735fb--------------------------------)
    [Amarpreet Singh](https://medium.com/@soniamarpreet17?source=post_page---byline--650492b735fb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--650492b735fb--------------------------------)
    ·8 min read·Jul 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa894c871e506a99304ada5e4808197d.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated with Microsoft Designer
  prefs: []
  type: TYPE_NORMAL
- en: We all require to sample our population to perform **statistical analysis**
    and gain insights. When we do so, the aim is to ensure that our sample’s distribution
    closely matches that of the population.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we have various methods: **simple random sampling** (where every
    member of the population has an equal chance of being selected), **stratified
    sampling** (which involves dividing the population into subgroups and sampling
    from each subgroup), **cluster sampling** (where the population is divided into
    clusters and entire clusters are randomly selected), **systematic sampling** (which
    involves selecting every nth member of the population), etc etc. Each method has
    its advantages and is chosen based on the specific needs and characteristics of
    the study.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we won’t be focusing on sampling methods themselves per se,
    but rather on using these concepts to split the dataset used for machine learning
    approaches into **Train-Test-Validation** sets. These approaches work for all
    kinds of **Tabular data**. We will be working in Python here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below are some approaches that you already might know:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Simple Train-Test-Val Split
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This approach uses **random-sampling** method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Stratified Train-Test-Val Split
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This approach ensures that the splits maintain the **same proportion of classes**
    as the original dataset (with random sampling again of course), which is useful
    for imbalanced datasets. This approach will work when your target variable is
    **not a continuous variable.**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 3\. K-Fold Cross-Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In K-Fold cross-validation, the dataset is split into `k` subsets (folds). The
    model is trained on `k-1` folds and tested on the remaining fold. This process
    is repeated `k` times.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Stratified K-Fold Cross-Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name suggests, this is a combination of Stratified sampling and K-fold
    cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Full example usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can use these methods to split your dataset but they have the following
    **limitations**:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Random Train-Test-Val Split****:* This method can’t guarantee **similar**
    **distributions** among the splits, especially if the dataset is not large enough
    or if there is an imbalance in the target variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Stratified Split****:* This method is useful only when you have a **non-continuous**
    target variable (y). Although there are workarounds for continuous target variables
    (such as converting the continuous variable into categorical through some conditions,
    e.g., if y ≥ quartile1 → 1, else 0), these approaches may still not always ensure
    similar distributions among the splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, suppose you have a **small** total number of observations in your dataset
    and it’s difficult to ensure similar distributions amongst your splits. In that
    case, you can combine **clustering** and **random sampling (or stratified sampling)**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is how I did it for **my problem** at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Clustering-based Train-Test-Validation split
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this method, first, we cluster our dataset and then use sampling methods
    on each cluster to obtain our data splits.
  prefs: []
  type: TYPE_NORMAL
- en: For example, using ***HDBSCAN:***
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use other clustering methods according to your problem for eg.
    ***K-Means clustering***:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can also add **levels of granularities** (any categorical variable)
    to your dataset to get more refined clusters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Once you have obtained **cluster labels** from any clustering method, you can
    use **random sampling or stratified sampling** to select samples from each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will select indices randomly and then use these indices to select our train-test-val
    sets as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As per **my use-case**, it was useful to sort my target variable y and then
    select every **1st, 2nd, and 3rd** indices for train, test, and validation set
    respectively (all mutually exclusive), a.k.a **systematic random sampling** as
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The above-discussed approaches of combining clustering with different sampling
    methods are very useful when you have a small number of observations in your dataset
    as they ensure to maintain similar distributions amongst the Train, Test and Validation
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading, and I hope you find this article helpful!
  prefs: []
  type: TYPE_NORMAL
