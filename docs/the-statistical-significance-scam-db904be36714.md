# 统计显著性骗局

> 原文：[https://towardsdatascience.com/the-statistical-significance-scam-db904be36714?source=collection_archive---------0-----------------------#2024-11-09](https://towardsdatascience.com/the-statistical-significance-scam-db904be36714?source=collection_archive---------0-----------------------#2024-11-09)

## 深入剖析科学最爱工具的缺陷

[](https://medium.com/@caiparryjones96?source=post_page---byline--db904be36714--------------------------------)[![Cai Parry-Jones](../Images/60b83f5167651f9621a3e73b8d72ccae.png)](https://medium.com/@caiparryjones96?source=post_page---byline--db904be36714--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--db904be36714--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--db904be36714--------------------------------) [Cai Parry-Jones](https://medium.com/@caiparryjones96?source=post_page---byline--db904be36714--------------------------------)

·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--db904be36714--------------------------------) ·13分钟阅读·2024年11月9日

--

![](../Images/6da0f69998c099f6c5f249b3cc38e731.png)

来源：[unsplash.com](https://unsplash.com/)

统计显著性就像是研究界的快餐车。你驶近研究，拿到你的“显著性套餐”，然后——你就有了一个可以和所有朋友分享的精彩结论。而这不仅方便读者，也让研究人员的工作变得更轻松。为什么要做复杂的推销，当你只需说两个字就能搞定呢？

但这里有一个陷阱。

那些我们方便忽略的复杂公式和细节？它们才是问题的核心。当研究人员和读者过于依赖某一种统计工具时，我们可能会犯下大错，正如那次几乎颠覆物理法则的错误。

2011年，著名的CERN实验室的物理学家宣布了一项震惊世界的发现：[中微子可能比光速还快](https://www.science.org/content/article/neutrinos-travel-faster-light-according-one-experiment)。这一发现威胁到推翻爱因斯坦的相对论理论——现代物理学的基石。研究人员对他们的结果充满信心，突破了物理学严格的统计显著性标准——99.9999998%。案子就这样结了，是吧？

其实不然。随着其他科学家对实验进行了审查，他们发现了方法学上的缺陷，最终[未能复制实验结果](https://www.nbcnews.com/id/wbna46760561)。尽管这一原始发现具有令人印象深刻的“统计显著性”，但结果证明是错误的。

在这篇文章中，我们将深入探讨为什么你不应该本能地信任一个统计显著的研究结果。更重要的是，为什么你不应该习惯性地忽视那些非统计显著的结果。

# TL;DR

统计学显著性的四个关键缺陷：

1.  **它是凭空捏造的**：统计学显著性/不显著性边界往往是凭空捏造的，或者懒散地取自95%置信度的一般标准。

1.  **它并不意味着（大多数）人们认为的那样**：统计学显著性并不意味着“有Y%的机会X是真的”。

1.  **这很容易被操纵（而且经常被操控）**：由于大规模实验，随机性经常被标记为具有统计学显著性。

1.  **这与结果的重要性无关**：统计学显著性与差异的意义无关。

# 缺陷1：这是凭空捏造的

统计学显著性只是人类创造的一条分界线，*没有*任何数学支持。想想看，这个通常被认为是客观度量的东西，本质上完全是主观的。

数学部分通过置信度的数值度量在决定显著性之前提供了一步。最常用的形式是[假设检验](https://www.scribbr.com/statistics/hypothesis-testing/)，它被称为[p值](https://www.scribbr.com/statistics/p-value/)。这提供了一个实际的数学概率，说明测试数据结果不仅仅是由随机性造成的。

例如，p值为0.05意味着有5%的机会看到这些数据点（或更极端的情况）是由于随机机会，或者说我们有95%的信心认为结果不是由于偶然性。例如，假设你认为一枚硬币是不公平的，偏向正面，即正面朝上的概率大于50%。你投掷硬币5次，每次都落在正面。假设硬币是公平的，那么它出现正面的概率为1/2 x 1/2 x 1/2 x 1/2 x 1/2 = 3.1%，这是纯粹由随机性引起的机会。

但这就足以说明它在统计学上显著吗？这取决于你问谁。

通常，决定在哪里划定显著性边界的人，比起底层数据本身，更能影响一个结果是否显著。

鉴于这一主观性的最终步骤，在我自己的分析中，我通常会向研究的读者提供置信度百分比，而不是二元的显著/不显著结果。最终步骤太过依赖个人观点。

**怀疑者**：“*但是有标准来决定统计学显著性。*”

我常常听到针对我上面论点的反驳（我谈论这个话题不少——这让我的学术研究女友很高兴）。对此，我的回应通常是：

**我**：“*当然，如果你必须遵守一个特定的标准，例如出于监管或学术期刊发布的原因，那么你别无选择，只能遵循这个标准。但如果不是这样，那就没有理由不遵守。*”

**怀疑者**：“*但是有一个普遍的标准。那就是95%的置信度。*”

在那个对话时刻，我尽量不翻白眼。仅仅因为 95% 是常规标准，就决定测试的统计显著性点为 95%，坦率地说，这是懒惰的做法。它没有考虑正在测试的内容的上下文。

在我的日常工作中，如果我看到有人在进行实验时使用 95% 的显著性阈值，却没有提供背景说明，我会觉得这是个警告信号。这表明这个人要么不理解选择的含义，要么不关心实验的具体业务需求。

一个例子可以最好地解释为什么这如此重要。

假设你在一家科技公司担任数据科学家，UI 团队想知道：“为了最大化点击率（CTR），我们应该选择红色还是蓝色作为‘订阅’按钮的颜色？”UI 团队对这两种颜色都没有偏好，但必须在本周结束前选择一个颜色。经过一些 A/B 测试和统计分析，我们得出了结果：

![](../Images/eaf109069c5d5b4801cd13b0e848d9c0.png)

图片由作者创作。

按标准做事的数据科学家可能会回来告诉 UI 团队，“*不幸的是，实验发现红色按钮和蓝色按钮的点击率没有统计显著差异。*”

这是一个糟糕的分析，完全是因为最后的主观判断。如果数据科学家主动去了解上下文，批判性地理解“UI 团队对颜色没有偏好，但必须在本周结束前选择一个”的背景，那么你应该把显著性点设置得非常高，理论上为 1.0，即统计分析不再重要，UI 团队可以选择那个具有最高点击率的颜色。

鉴于数据科学家等可能没有完整的背景信息来确定最佳显著性点，最好（也更简单）将这个责任交给那些掌握完整业务背景的人——在这个例子中，就是 UI 团队。换句话说，数据科学家应该告诉 UI 团队，“*实验结果显示蓝色按钮的点击率更高，且有 94% 的置信度证明这一结果不是由随机机会造成的。*” 确定显著性的最后步骤应该由 UI 团队做出。当然，这并不意味着数据科学家不应该教育团队什么是“94% 的置信度”，以及清楚地解释*为什么*统计显著性最好由他们来决定。

# 缺陷 2：它并不意味着（大多数）人们认为的那样

假设我们生活在一个稍微完美的世界里，第一点的问题不再存在。那个“沙线”图形永远是完美的，万岁！假设我们想要进行一个实验，统计显著性线设置为 99% 的置信度。几周后，最终我们得出了结果，统计分析发现它是统计上显著的，再次万岁！但这到底意味着什么呢？

在假设检验的情况下，常见的看法是假设正确的概率为99%。这完全错误。它的意思是，在**这个实验**中，随机性导致观察到如此极端的数据或更极端数据的概率为1%。

统计显著性并没有考虑实验本身是否准确。以下是统计显著性无法捕捉到的一些例子：

+   样本质量：所采样的总体可能存在偏差或不具代表性。

+   数据质量：测量错误、缺失数据或其他数据质量问题未得到解决。

+   假设有效性：统计测试的假设（如正态性、独立性）可能被违反。

+   研究设计质量：实验控制差，没有控制混杂变量，测试多个结果而没有调整显著性水平。

回到引言中提到的例子。经过未能独立复制初步发现的失败后，原始2011年实验的物理学家宣布，他们发现了测量设备主时钟中的一个漏洞，即数据质量问题，这导致他们撤回了最初的研究。

下次你听到一个与常识相悖的统计学显著性发现时，不要那么快相信它。

# 缺陷3：很容易被破解（且经常被破解）

鉴于统计显著性完全关注某事因随机性发生的可能性，一个更关心获得统计学显著结果而不是揭示真相的实验者很容易操控系统。

从两个骰子掷出两个一的概率是(1/6 × 1/6) = 1/36，或2.8%；这是一个非常罕见的结果，许多人会认为它在统计学上显著。但如果我掷出超过两个骰子呢？自然，至少掷出两个一的概率会增加：

+   3个骰子：≈ 7.4%

+   4个骰子：≈ 14.4%

+   5个骰子：≈ 23%

+   6个骰子：≈ 32.4%

+   7个骰子：≈ 42%

+   8个骰子：≈ 51%

+   12个骰子：≈ 80%*

> *至少掷出两个骰子得到**一**相当于：1（即100%，确定），减去掷出零个**一**的概率，减去掷出仅有一个**一**的概率。*
> 
> P(零个**一**) = (5/6)^n
> 
> P(恰好一个**一**) = n * (1/6) * (5/6)^(n-1)
> 
> n是骰子的数量
> 
> 所以完整的公式是：**1 — (5/6)^n — n*(1/6)*(5/6)^(n-1)**

假设我进行一个简单实验，最初的理论是**一**比其他数字更有可能被掷出。我掷了12个不同颜色和大小的骰子。以下是我的结果：

![](../Images/b1e420a68854f673406ba4c6ecf817e7.png)

图片由作者创建。

不幸的是，我（经过计算的）至少获得两个**1点**的希望已经破灭……其实，现在想想，我并不是真的想要两个1点。我更感兴趣的是大红色骰子的概率。我相信从它们中掷出六点的机会很大。啊！看起来我的理论是正确的，那两个大红色骰子竟然都掷出了六点！这种情况偶然发生的概率只有2.8%。非常有趣。接下来，我将根据我的发现写一篇论文，并计划将其发表在一个接受我结果为统计学上显著的学术期刊上。

这个故事听起来可能有些牵强，但现实与此并不如你想象的那样遥远，尤其是在备受尊敬的学术研究领域。事实上，这种事情发生得足够频繁，以至于形成了一个名声，[p-hacking](https://statisticsbyjim.com/hypothesis-testing/p-hacking/#:~:text=P%20hacking%20is%20the%20manipulation,the%20integrity%20of%20scientific%20research.)就是其中之一。

如果你感到惊讶，深入了解学术体系将能阐明为什么在科学领域中，许多看似与科学方法背道而驰的做法会如此频繁地发生。

在学术界，想要成功地发展事业是非常困难的。例如，在STEM学科中，只有[0.45%的博士生能成为教授](https://occamstypewriter.org/athenedonald/2014/02/08/thinking-about-the-pipeline/)。当然，一些博士生并不想从事学术职业，但大多数人是有这个愿望的（根据[这项](https://www.hepi.ac.uk/wp-content/uploads/2020/07/HEPI-Policy-Note-25_PhD-students-careers_FINAL.pdf)调查，比例为67%）。所以，粗略地说，如果你完成了博士学位并希望在学术界发展事业，你成为教授的机会大约是1%。考虑到这些几率，你需要认为自己是非常杰出的，或者更准确地说，你需要别人这么认为，因为你无法自己雇佣自己。那么，杰出是如何衡量的呢？

也许并不令人意外，学术成功的最重要衡量标准是他们的[研究影响力](https://www.lib.ncsu.edu/measuring-research-impact/your-impact#:~:text=Research%20impact%20is%20often%20measured,index%2C%20and%20journal%20impact%20factors.)。常见的作者影响力衡量标准包括h指数、g指数和i10指数。它们的共同点在于，它们高度关注引用次数，也就是他们的研究成果被其他已发表的工作提及了多少次。知道了这一点，如果我们希望在学术界做得好，我们就需要专注于发表那些可能会被引用的研究。

如果你将研究成果发表在高评级的学术期刊中，你[更有可能被引用](https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24810#:~:text=There%20is%20strong%20evidence%20from,tend%20to%20be%20more%20cited.)。而且，由于[88%的顶级期刊论文是统计显著的](https://pubmed.ncbi.nlm.nih.gov/19892276/)，如果你的研究具有统计显著性，你更有可能被这些期刊接受。这使得许多出于良好意图但又受到职业驱动的学者走上了一条滑坡之路。他们开始时会采用这样的科学方法来撰写研究论文：

![](../Images/95c193b98180ec75718cd37c9045573e.png)

使用科学方法进行实验研究发表的决策树。由作者使用[Mermaid](https://github.com/mermaid-js/mermaid)创建。

但他们最终扭曲了自己的方法论，看起来像是科学的表象——但实际上，他们已经抛弃了正统的科学方法：

![](../Images/119a202c44dcf7e2087c7b07dc3002f4.png)

最大化每个实验发表成功的决策树。由作者使用[Mermaid](https://github.com/mermaid-js/mermaid)创建。

由于决策图表让研究者在发现显著结果*之后*写论文，因此期刊审稿人无法批评实验中的p-hacking行为。

这理论上是如此。但现实中真的经常发生吗？

答案是响亮的“是”。事实上，[大多数科学研究无法被同行学者重复](https://www.bbc.co.uk/news/science-environment-39054778)。无法重复意味着一篇研究论文试图复制另一篇研究论文的实验，但结果却是统计上出乎意料的。通常，原始论文中的统计显著结果在重复实验中并不显著，或者在[某些情况下](https://journals.sagepub.com/doi/full/10.1177/1745691616652873)结果的统计显著性方向是相反的！

# 缺陷 4：它与结果的重要性无关

最后，统计显著性并不关心差异的规模。

从这个角度思考——统计显著性基本上只是告诉你“嘿，这个差异可能不是由于随机偶然”，但它并没有告诉你这个差异在现实世界中是否真的重要。

假设你测试一种新药，并发现它比安慰剂减少了0.0001%的头痛痛感。如果你在数百万人的样本中进行此测试，那么这个微小的差异可能是统计显著的，因为你的样本量非常大。但……谁在乎0.0001%的疼痛减少呢？在实际意义上这是毫无意义的！

另一方面，你可能会发现一种药物能减少5%的疼痛，但尚未进行大规模实验来证明其统计学意义。在医学领域，很可能存在许多这种情况，因为如果所讨论的药物价格便宜，制药公司就没有动力进行实验，因为大规模的医学测试非常昂贵。

这就是为什么在讨论影响效果（差异有多大）时，必须与统计学意义分开来看。在现实世界中，你需要两者——既要有不太可能是随机的差异，*又*要有足够大的差异，才能真正产生影响。

这一错误反复出现的一个例子是当发现某些致癌物（即导致癌症的物质）时。例如，2015年《卫报》的一篇[文章](https://www.theguardian.com/society/2015/oct/26/bacon-ham-sausages-processed-meats-cancer-risk-smoking-says-who)提到：

> “世界卫生组织表示，培根、火腿和香肠与香烟并列为癌症的主要原因，将腌制和加工肉类与石棉、酒精、砒霜和烟草放在同一类别。”

这完全是误导信息。实际上，培根、火腿和香肠与石棉、酒精、砒霜和烟草属于同一类。然而，这些类别并不表示致癌物的影响规模，而是世界卫生组织对这些物质是否为致癌物的信心程度，即统计学意义。

由加工肉类引起的癌症病例规模存在争议，因为尚未进行任何随机对照试验（RCT）。支持加工肉类导致癌症的最具破坏性的研究之一是在2020年英国进行的一项观察性[研究](https://academic.oup.com/ije/article/49/5/1540/5894731?login=false)。该研究发现，平均每天摄入超过79克红肉和加工肉类的人，患肠癌的风险比每天摄入不到11克的人高出32%。

然而，要了解真正的风险，我们需要了解有多少人处于患肠癌的风险中。在研究中，每10,000个每天摄入不到11克加工和红肉的人中，有45人被诊断为肠癌，而每天摄入79克加工和红肉的人中则有59人被诊断为肠癌。也就是说，每10,000人中额外增加了14例肠癌病例，或者0.14%。在英国，肠癌的生存率为[53%](https://www.cancerresearchuk.org/health-professional/cancer-statistics/statistics-by-cancer-type/bowel-cancer)，因此粗略估算，加工肉类中致癌物质导致你死亡的几率是0.07%。

将此与《卫报》提到的另一种物质——烟草进行比较。[癌症研究](https://www.cancerresearchuk.org/health-professional/cancer-statistics/risk/tobacco#heading-Zero)表示：

> “烟草是英国癌症和死亡的最大可预防原因，也是全球最大可预防的疾病和死亡原因之一。2021年，烟草导致英国约75,800人死亡——约占所有死因的十分之一（11%）。”

首先，哇。别抽烟。

其次，由烟草引起的癌症死亡率是11%/0.07% = **157**倍高于加工肉类！回到文章中的引用：“培根、火腿和香肠与香烟一起，成为癌症的主要原因。”简单来说，这是假新闻。

# 总结

总之，虽然统计学显著性在验证定量研究中有其作用，但理解其严重局限性至关重要。

作为读者，我们有责任以批判性的眼光看待“统计学显著性”的说法。下次你遇到宣扬“统计学显著”结果的研究或文章时，不妨花点时间问问自己：

1.  统计学显著性阈值是否适合该情境？

1.  这项研究设计和数据收集过程有多稳健？

1.  研究人员是否可能从事了p-hacking或其他可疑的做法？

1.  该效应大小的实际意义是什么？

通过提出这些问题并要求围绕统计学显著性进行更为细致的讨论，我们可以帮助推动更加负责任和准确地使用这一工具。

# 随时间推移的分析

我实际上认为，统计学显著性之所以获得如此过度的突出地位，主要原因在于这个名称。人们将“统计”与数学和客观联系在一起，将“显著性”与“重要性”联系在一起。我希望这篇文章能够说服你，这些关联实际上只是谬误。

如果科学界和更广泛的社区希望解决过度突出的问题，他们应该认真考虑重新命名“统计学显著性”。也许可以叫做“机会阈值检验”或“非随机置信度”。不过，这样做会失去其“大麦克”便利性。
