<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Adopting Spark Connect</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Adopting Spark Connect</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adopting-spark-connect-cdd6de69fa98?source=collection_archive---------7-----------------------#2024-11-07">https://towardsdatascience.com/adopting-spark-connect-cdd6de69fa98?source=collection_archive---------7-----------------------#2024-11-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="d56c" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How we use a shared Spark server to make our Spark infrastructure more efficient</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sergey.kotlov?source=post_page---byline--cdd6de69fa98--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sergey Kotlov" class="l ep by dd de cx" src="../Images/63dd13c266505832b4cd6242b75f4968.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*JHwKdBze75H0sfhxfCJH1w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--cdd6de69fa98--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@sergey.kotlov?source=post_page---byline--cdd6de69fa98--------------------------------" rel="noopener follow">Sergey Kotlov</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--cdd6de69fa98--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/80211dfce51f1158143441e6eba80f48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ie3ePGPJzUg15qi5GWJmjA.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by <a class="af nb" href="https://pixabay.com/users/kanenori-4749850/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5313115" rel="noopener ugc nofollow" target="_blank">Kanenori</a> from <a class="af nb" href="https://pixabay.com//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5313115" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><p id="d597" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af nb" href="https://spark.apache.org/docs/latest/spark-connect-overview.html" rel="noopener ugc nofollow" target="_blank">Spark Connect</a> is a relatively new component in the <a class="af nb" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">Spark ecosystem</a> that allows thin clients to run Spark applications on a remote Spark cluster. This technology can offer some benefits to Spark applications that use the DataFrame API. Spark has long allowed to run SQL queries on a remote Thrift JDBC server. However, this ability to remotely run client applications written in any supported language (Scala, Python) appeared only in Spark 3.4.</p><p id="c9c0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this article, I will share our experience using Spark Connect (version 3.5). I will talk about the benefits we gained, technical details related to running Spark client applications, and some tips on how to make your Spark Connect setup more efficient and stable.</p><h1 id="9c32" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Motivation for use</h1><p id="5f70" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Spark is one of the key components of the analytics platform at Joom. We have a large number of internal users and over 1000 custom Spark applications. These applications run at different times of day, have different complexity, and require very different amounts of computing resources (ranging from a few cores for a couple of minutes to over 250 cores for several days). Previously, all of them were always executed as separate Spark applications (with their own driver and executors), which, in the case of small and medium-sized applications (we historically have many such applications), led to noticeable overhead. With the introduction of Spark Connect, it is now possible to set up a shared Spark Connect server and run many Spark client applications on it. Technically, the Spark Connect server is a Spark application with an embedded Spark Connect endpoint.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj oz"><img src="../Images/0e9180f1354e8349d8b2b09848867414.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WN8fOowN7xpj5fb1qgDn9Q.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="a6b6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here are the benefits we were able to get from this:</p><ul class=""><li id="cc5a" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">Resource savings<br/>- When running via Spark Connect, client applications do not require their own Spark driver (which typically uses over 1.5 GB of memory). Instead, they use a thin client with a typical memory consumption of 200 MB.<br/>- Executor utilization improves since any executor can run the tasks of multiple client applications. For example, suppose some Spark application, at some point in its execution, starts using significantly fewer cores and memory than initially requested. There are many reasons why this can happen. Then, in the case of a separate Spark application, currently unused resources are often wasted since dynamic allocation often does not provide efficient scale-down. However, with the Spark Connect server, the freed-up cores and memory can immediately be used to run tasks of other client applications.</li><li id="aeb4" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Reduced startup wait time<br/>- For various reasons, we have to limit the number of simultaneously running separate Spark applications, and they may wait in the queue for quite a long time if all slots are currently occupied. It can negatively affect data readiness time and user experience. In the case of the Spark Connect server, we have so far been able to avoid such limitations, and all Spark Connect client applications start running immediately after launch.<br/>- For ad-hoc executions, it is desirable to minimize the time to get results as much as possible and avoid keeping people waiting. In the case of separate Spark applications, launching a client application often requires provisioning additional EC2 nodes for its driver and executors, as well as initializing the driver and executors. All of this together can take more than 4 minutes. In the case of the Spark Connect server, at least its driver is always up and ready to accept requests, so it is only a matter of waiting for additional executors, and often executors are already available. This may significantly reduce the wait time for ad-hoc applications to be ready.</li></ul><h2 id="23f4" class="pi nz fq bf oa pj pk pl od pm pn po og nl pp pq pr np ps pt pu nt pv pw px py bk">Our constraints</h2><p id="d493" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">At the moment, we do not run long-running heavy applications on Spark Connect for the following reasons:</p><ul class=""><li id="2cf8" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">They may cause failure or unstable behavior of the Spark Connect server (e.g., by overflowing disks on executor nodes). It can lead to large-scale problems for the entire platform.</li><li id="7941" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">They often require unique memory settings and use specific optimization techniques (e.g., custom extraStrategies).</li><li id="e7a3" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">We currently have a problem with giving the Spark Connect server a lot of executors to handle a very large simultaneous load (this is related to the behavior of Spark Task Scheduler and is beyond the scope of this article).</li></ul><p id="cbcf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Therefore, heavy applications still run as separate Spark applications.</p><h1 id="a45c" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Launching client applications</h1><p id="b375" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk"><em class="pz">We use </em><a class="af nb" href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" rel="noopener ugc nofollow" target="_blank"><em class="pz">Spark on Kubernetes/EKS</em></a><em class="pz"> and </em><a class="af nb" href="https://airflow.apache.org/" rel="noopener ugc nofollow" target="_blank"><em class="pz">Airflow</em></a><em class="pz">. Some code examples will be specific to this environment.</em></p><p id="597e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We have too many different, constantly changing Spark applications, and it would take too much time to manually determine for each one whether it should run on Spark Connect according to our criteria or not. Furthermore, the list of applications running on Spark Connect needs to be updated regularly. For example, suppose today, some application is light enough, so we have decided to run it on Spark Connect. But tomorrow, its developers may add several large joins, making it quite heavy. Then, it will be preferable to run it as a separate Spark application. The reverse situation is also possible.</p><p id="121a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Eventually, we created a service to automatically determine how to launch each specific client application. This service analyzes the history of previous runs for each application, evaluating such metrics as <code class="cx qa qb qc qd b">Total Task Time</code>, <code class="cx qa qb qc qd b">Shuffle Write</code>, <code class="cx qa qb qc qd b">Disk Spill</code>, and others (this data is collected using <a class="af nb" href="https://github.com/joomcode/spark-platform/blob/main/lib/src/main/scala/com/joom/spark/monitoring/StatsReportingSparkListener.scala" rel="noopener ugc nofollow" target="_blank">SparkListener</a>). Custom parameters set for the applications by developers (e.g., memory settings of drivers and executors) are also considered. Based on this data, the service automatically determines for each application whether it should be run this time on the Spark Connect server or as a separate Spark application. Thus, all our applications should be ready to run in either of the two ways.</p><p id="e1b2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In our environment, each client application is built independently of the others and has its own JAR file containing the application code, as well as specific dependencies (for example, ML applications often use third-party libraries like CatBoost and so on). The problem is that the SparkSession API for Spark Connect is somewhat different from the SparkSession API used for separate Spark applications (Spark Connect clients use the <code class="cx qa qb qc qd b">spark-connect-client-jvm</code> artifact). Therefore, we are supposed to know at the build time of each client application whether it will run via Spark Connect or not. But we do not know that. The following describes our approach to launching client applications, which eliminates the need to build and manage two versions of JAR artifact for the same application.</p><p id="7987" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For each Spark client application, we build only one JAR file containing the application code and specific dependencies. This JAR is used both when running on Spark Connect and when running as a separate Spark application. Therefore, these client JARs do not contain specific Spark dependencies. The appropriate Spark dependencies (<code class="cx qa qb qc qd b">spark-core</code>/<code class="cx qa qb qc qd b">spark-sql</code> or <code class="cx qa qb qc qd b">spark-connect-client-jvm</code>) will be provided later in the Java classpath, depending on the run mode. In any case, all client applications use the same Scala code to initialize SparkSession, which operates depending on the run mode. All client application JARs are built for the regular Spark API. So, in the part of the code intended for Spark Connect clients, the <code class="cx qa qb qc qd b">SparkSession</code> methods specific to the Spark Connect API (<code class="cx qa qb qc qd b">remote</code>, <code class="cx qa qb qc qd b">addArtifact</code>) are called via reflection:</p><pre class="ml mm mn mo mp qe qd qf bp qg bb bk"><span id="31b2" class="qh nz fq qd b bg qi qj l qk ql">val sparkConnectUri: Option[String] = Option(System.getenv("SPARK_CONNECT_URI"))<br/><br/>val isSparkConnectMode: Boolean = sparkConnectUri.isDefined<br/><br/>def createSparkSession(): SparkSession = {<br/>  if (isSparkConnectMode) {<br/>    createRemoteSparkSession()<br/>  } else {<br/>    SparkSession.builder<br/>      // Whatever you need to do to configure SparkSession for a separate <br/>      // Spark application.<br/>      .getOrCreate<br/>  }<br/>}<br/><br/>private def createRemoteSparkSession(): SparkSession = {<br/>  val uri = sparkConnectUri.getOrElse(throw new Exception(<br/>    "Required environment variable 'SPARK_CONNECT_URI' is not set."))<br/><br/>  val builder = SparkSession.builder<br/>  // Reflection is used here because the regular SparkSession API does not <br/>  // contain these methods. They are only available in the SparkSession API <br/>  // version for Spark Connect.<br/>  classOf[SparkSession.Builder]<br/>    .getDeclaredMethod("remote", classOf[String])<br/>    .invoke(builder, uri)<br/><br/>  // A set of identifiers for this application (to be used later).<br/>  val scAppId = s"spark-connect-${UUID.randomUUID()}"<br/>  val airflowTaskId = Option(System.getenv("AIRFLOW_TASK_ID"))<br/>    .getOrElse("unknown_airflow_task_id")<br/>  val session = builder<br/>    .config("spark.joom.scAppId", scAppId)<br/>    .config("spark.joom.airflowTaskId", airflowTaskId)<br/>    .getOrCreate()<br/><br/>  // If the client application uses your Scala code (e.g., custom UDFs), <br/>  // then you must add the jar artifact containing this code so that it <br/>  // can be used on the Spark Connect server side.<br/>  val addArtifact = Option(System.getenv("ADD_ARTIFACT_TO_SC_SESSION"))<br/>    .forall(_.toBoolean)<br/><br/>  if (addArtifact) {<br/>    val mainApplicationFilePath = <br/>      System.getenv("SPARK_CONNECT_MAIN_APPLICATION_FILE_PATH")<br/>    classOf[SparkSession]<br/>      .getDeclaredMethod("addArtifact", classOf[String])<br/>      .invoke(session, mainApplicationFilePath)<br/>  }<br/><br/>  Runtime.getRuntime.addShutdownHook(new Thread() {<br/>    override def run(): Unit = {<br/>      session.close()<br/>    }<br/>  })<br/><br/>  session<br/>}</span></pre><p id="5bba" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the case of Spark Connect mode, this client code can be run as a regular Java application anywhere. Since we use Kubernetes, this runs in a Docker container. All dependencies specific to Spark Connect are packed into a Docker image used to run client applications (a minimal example of this image can be found <a class="af nb" href="https://github.com/kotlovs/spark-connect-examples/tree/main/spark-connect-client-image" rel="noopener ugc nofollow" target="_blank">here</a>). The image contains not only the <code class="cx qa qb qc qd b">spark-connect-client-jvm</code> artifact but also other common dependencies used by almost all client applications (e.g., <code class="cx qa qb qc qd b">hadoop-aws</code> since we almost always have interaction with S3 storage on the client side).</p><pre class="ml mm mn mo mp qe qd qf bp qg bb bk"><span id="7795" class="qh nz fq qd b bg qi qj l qk ql">FROM openjdk:11-jre-slim<br/><br/>WORKDIR /app<br/><br/># Here, we copy the common artifacts required for any of our Spark Connect <br/># clients (primarily spark-connect-client-jvm, as well as spark-hive, <br/># hadoop-aws, scala-library, etc.).<br/>COPY build/libs/* /app/<br/><br/>COPY src/main/docker/entrypoint.sh /app/<br/>RUN chmod +x ./entrypoint.sh<br/>ENTRYPOINT ["./entrypoint.sh"]</span></pre><p id="d971" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This common Docker image is used to run all our client applications when it comes to running them via Spark Connect. At the same time, it does not contain client JARs with the code of particular applications and their dependencies because there are many such applications that are constantly updated and may depend on any third-party libraries. Instead, when a particular client application is launched, the location of its JAR file is passed using an environment variable, and that JAR is downloaded during initialization in <code class="cx qa qb qc qd b">entrypoint.sh</code>:</p><pre class="ml mm mn mo mp qe qd qf bp qg bb bk"><span id="ab0c" class="qh nz fq qd b bg qi qj l qk ql">#!/bin/bash<br/>set -eo pipefail<br/><br/># This variable will also be used in the SparkSession builder within <br/># the application code.<br/>export SPARK_CONNECT_MAIN_APPLICATION_FILE_PATH="/tmp/$(uuidgen).jar"<br/><br/># Download the JAR with the code and specific dependencies of the client <br/># application to be run. All such JAR files are stored in S3, and when <br/># creating a client Pod, the path to the required JAR is passed to it <br/># via environment variables.<br/>java -cp "/app/*" com.joom.analytics.sc.client.S3Downloader \ <br/>    ${MAIN_APPLICATION_FILE_S3_PATH} ${SPARK_CONNECT_MAIN_APPLICATION_FILE_PATH}<br/><br/># Launch the client application. Any MAIN_CLASS initializes a SparkSession <br/># at the beginning of its execution using the code provided above.<br/>java -cp ${SPARK_CONNECT_MAIN_APPLICATION_FILE_PATH}:"/app/*" ${MAIN_CLASS} "$@"</span></pre><p id="4c66" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Finally, when it comes time to launch the application, our custom SparkAirflowOperator automatically determines the execution mode (Spark Connect or separate) based on the statistics of previous runs of this application.</p><ul class=""><li id="a963" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">In the case of Spark Connect, we use <a class="af nb" href="https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/operators.html" rel="noopener ugc nofollow" target="_blank">KubernetesPodOperator</a> to launch the client Pod of the application. <code class="cx qa qb qc qd b">KubernetesPodOperator</code> takes as parameters the previously described Docker image, as well as the environment variables (<code class="cx qa qb qc qd b">MAIN_CLASS</code>, <code class="cx qa qb qc qd b">JAR_PATH</code> and others), which will be available for use within <code class="cx qa qb qc qd b">entrypoint.sh</code> and the application code. There is no need to allocate many resources to the client Pod (for example, its typical consumption in our environment: memory — 200 MB, vCPU — 0.15).</li><li id="53b8" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">In the case of a separate Spark application, we use our custom AirflowOperator, which runs Spark applications using <a class="af nb" href="https://github.com/kubeflow/spark-operator" rel="noopener ugc nofollow" target="_blank">spark-on-k8s-operator</a> and the official <a class="af nb" href="https://spark.apache.org/docs/latest/running-on-kubernetes.html#docker-images" rel="noopener ugc nofollow" target="_blank">Spark Docker image</a>. Let’s skip the details about our Spark AirflowOperator for now, as it is a large topic deserving a separate article.</li></ul><h1 id="0592" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Compatibility issues with regular Spark applications</h1><p id="7177" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Not all existing Spark applications can be successfully executed on Spark Connect since its SparkSession API is different from the SparkSession API used for separate Spark applications. For example, if your code uses <code class="cx qa qb qc qd b">sparkSession.sparkContext</code> or <code class="cx qa qb qc qd b">sparkSession.sessionState</code>, it will fail in the Spark Connect client because the Spark Connect version of SparkSession does not have these properties.</p><p id="1336" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In our case, the most common cause of problems was using <code class="cx qa qb qc qd b">sparkSession.sessionState.catalog</code> and <code class="cx qa qb qc qd b">sparkSession.sparkContext.hadoopConfiguration</code>. In some cases, <code class="cx qa qb qc qd b">sparkSession.sessionState.catalog</code> can be replaced with <code class="cx qa qb qc qd b">sparkSession.catalog</code>, but not always. <code class="cx qa qb qc qd b">sparkSession.sparkContext.hadoopConfiguration</code> may be needed if the code executed on the client side contains operations on your data storage, such as this:</p><pre class="ml mm mn mo mp qe qd qf bp qg bb bk"><span id="e5e3" class="qh nz fq qd b bg qi qj l qk ql">def delete(path: Path, recursive: Boolean = true)<br/>          (implicit hadoopConfig: Configuration): Boolean = {<br/>  val fs = path.getFileSystem(hadoopConfig)<br/>  fs.delete(path, recursive)<br/>}</span></pre><p id="33c3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Fortunately, it is possible to create a standalone <code class="cx qa qb qc qd b">SessionCatalog</code> for use within the Spark Connect client. In this case, the class path of the Spark Connect client must also include <code class="cx qa qb qc qd b">org.apache.spark:spark-hive_2.12</code>, as well as libraries for interacting with your storage (since we use S3, so in our case, it is <code class="cx qa qb qc qd b">org.apache.hadoop:hadoop-aws</code>).</p><pre class="ml mm mn mo mp qe qd qf bp qg bb bk"><span id="4cd0" class="qh nz fq qd b bg qi qj l qk ql">import org.apache.spark.SparkConf<br/>import org.apache.hadoop.conf.Configuration<br/>import org.apache.spark.sql.hive.StandaloneHiveExternalCatalog<br/>import org.apache.spark.sql.catalyst.catalog.{ExternalCatalogWithListener, SessionCatalog}<br/><br/>// This is just an example of what the required properties might look like. <br/>// All of them should already be set for existing Spark applications in one <br/>// way or another, and their complete list can be found in the UI of any<br/>// running separate Spark application on the Environment tab.<br/>val sessionCatalogConfig = Map(<br/>  "spark.hadoop.hive.metastore.uris" -&gt; "thrift://metastore.spark:9083",<br/>  "spark.sql.catalogImplementation" -&gt; "hive",<br/>  "spark.sql.catalog.spark_catalog" -&gt; "org.apache.spark.sql.delta.catalog.DeltaCatalog",<br/>)<br/><br/>val hadoopConfig = Map(<br/>  "hive.metastore.uris" -&gt; "thrift://metastore.spark:9083",<br/>  "fs.s3.impl" -&gt; "org.apache.hadoop.fs.s3a.S3AFileSystem",<br/>  "fs.s3a.aws.credentials.provider" -&gt; "com.amazonaws.auth.DefaultAWSCredentialsProviderChain",<br/>  "fs.s3a.endpoint" -&gt; "s3.amazonaws.com",<br/>  // and others...<br/>)<br/><br/>def createStandaloneSessionCatalog(): (SessionCatalog,  Configuration) = {<br/>  val sparkConf = new SparkConf().setAll(sessionCatalogConfig)<br/>  val hadoopConfiguration = new Configuration()<br/>  hadoopConfig.foreach { <br/>    case (key, value) =&gt; hadoopConfiguration.set(key, value) <br/>  }<br/><br/>  val externalCatalog = new StandaloneHiveExternalCatalog(<br/>    sparkConf, hadoopConfiguration)<br/>  val sessionCatalog = new SessionCatalog(<br/>    new ExternalCatalogWithListener(externalCatalog)<br/>  )<br/>  (sessionCatalog, hadoopConfiguration)<br/>}</span></pre><p id="40b4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You also need to create a wrapper for <code class="cx qa qb qc qd b">HiveExternalCatalog</code> accessible in your code (because the <code class="cx qa qb qc qd b">HiveExternalCatalog</code> class is private to the <code class="cx qa qb qc qd b">org.apache.spark</code> package):</p><pre class="ml mm mn mo mp qe qd qf bp qg bb bk"><span id="19f6" class="qh nz fq qd b bg qi qj l qk ql">package org.apache.spark.sql.hive<br/><br/>import org.apache.hadoop.conf.Configuration<br/>import org.apache.spark.SparkConf<br/><br/>class StandaloneHiveExternalCatalog(conf: SparkConf, hadoopConf: Configuration) <br/>  extends HiveExternalCatalog(conf, hadoopConf)</span></pre><p id="c96f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Additionally, it is often possible to replace code that does not work on Spark Connect with an alternative, for example:</p><ul class=""><li id="dbc5" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk"><code class="cx qa qb qc qd b">sparkSession.createDataFrame(sparkSession.sparkContext.parallelize(data), schema)</code> ==&gt; <code class="cx qa qb qc qd b">sparkSession.createDataFrame(data.toList.asJava, schema)</code></li><li id="a98b" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk"><code class="cx qa qb qc qd b">sparkSession.sparkContext.getConf.get(“some_property”)</code> ==&gt; <code class="cx qa qb qc qd b">sparkSession.conf.get(“some_property”)</code></li></ul><h2 id="f603" class="pi nz fq bf oa pj pk pl od pm pn po og nl pp pq pr np ps pt pu nt pv pw px py bk">Fallback to a separate Spark application</h2><p id="f3d5" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Unfortunately, it is not always easy to fix a particular Spark application to make it work as a Spark Connect client. For example, third-party Spark components used in the project pose a significant risk, as they are often written without considering compatibility with Spark Connect. Since, in our environment, any Spark application can be automatically launched on Spark Connect, we found it reasonable to implement a fallback to a separate Spark application in case of failure. Simplified, the logic is as follows:</p><ul class=""><li id="ddc0" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">If some application fails on Spark Connect, we immediately try to rerun it as a separate Spark application. At the same time, we increment the counter of failures that occurred during execution on Spark Connect (each client application has its own counter).</li><li id="1343" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">The next time this application is launched, we check the failure counter of this application:<br/>- If there are fewer than 3 failures, we assume that the last time, the application may have failed not because of incompatibility with Spark Connect but due to any other possible temporary reasons. So, we try to run it on Spark Connect again. If it completes successfully this time, the failure counter of this client application is reset to zero.<br/>- If there are already 3 failures, we assume that the application cannot work on Spark Connect and stop attempting to run it there for now. Further, it will be launched only as a separate Spark application.</li><li id="939c" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">If the application has 3 failures on Spark Connect, but the last one was more than 2 months ago, we try to run it on Spark Connect again (in case something has changed in it during that time, making it compatible with Spark Connect). If it succeeds this time, we reset the failure counter to zero again. If unsuccessful again, the next attempt will be in another 2 months.</li></ul><p id="5964" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This approach is somewhat simpler than maintaining code that identifies the reasons for failures from logs, and it works well in most cases. Attempts to run incompatible applications on Spark Connect usually do not have any significant negative impact because, in the vast majority of cases, if an application is incompatible with Spark Connect, it fails immediately after launch without wasting time and resources. However, it is important to mention that all our applications are idempotent.</p><h1 id="7ede" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Statistics gathering</h1><p id="ab40" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">As I already mentioned, we collect Spark statistics for each Spark application (most of our platform optimizations and alerts depend on it). This is easy when the application runs as a separate Spark application. In the case of Spark Connect, the stages and tasks of each client application need to be separated from the stages and tasks of all other client applications that run simultaneously within the shared Spark Connect server.</p><p id="86cd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can pass any identifiers to the Spark Connect server by setting custom properties for the client <code class="cx qa qb qc qd b">SparkSession</code>:</p><pre class="ml mm mn mo mp qe qd qf bp qg bb bk"><span id="4c8a" class="qh nz fq qd b bg qi qj l qk ql">val session = builder<br/>  .config("spark.joom.scAppId", scAppId)<br/>  .config("spark.joom.airflowTaskId", airflowTaskId)<br/>  .getOrCreate()</span></pre><p id="1245" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then, in the <code class="cx qa qb qc qd b">SparkListener</code> on the Spark Connect server side, you can retrieve all the passed information and associate each stage/task with the particular client application.</p><pre class="ml mm mn mo mp qe qd qf bp qg bb bk"><span id="21dc" class="qh nz fq qd b bg qi qj l qk ql">class StatsReportingSparkListener extends SparkListener {<br/><br/>  override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = {<br/>    val stageId = stageSubmitted.stageInfo.stageId<br/>    val stageAttemptNumber = stageSubmitted.stageInfo.attemptNumber()<br/>    val scAppId = stageSubmitted.properties.getProperty("spark.joom.scAppId")<br/>    // ...<br/>  }<br/>}</span></pre><p id="fdd7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af nb" href="https://github.com/joomcode/spark-platform/blob/main/lib/src/main/scala/com/joom/spark/monitoring/StatsReportingSparkListener.scala" rel="noopener ugc nofollow" target="_blank">Here, you can find the code</a> for the <code class="cx qa qb qc qd b">StatsReportingSparkListener</code> we use to collect statistics. You might also be interested in <a class="af nb" href="https://cloud.joom.ai/" rel="noopener ugc nofollow" target="_blank">this free tool</a> for finding performance issues in your Spark applications.</p><h1 id="841c" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Optimization and stability improvement</h1><p id="1a7b" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The Spark Connect server is a permanently running Spark application where a large number of clients can run their Jobs. Therefore, it can be worthwhile to customize its <a class="af nb" href="https://spark.apache.org/docs/latest/configuration.html" rel="noopener ugc nofollow" target="_blank">properties</a>, which can make it more reliable and prevent waste of resources. Here are some settings that turned out to be useful in our case:</p><pre class="ml mm mn mo mp qe qd qf bp qg bb bk"><span id="82a5" class="qh nz fq qd b bg qi qj l qk ql">// Using dynamicAllocation is important for the Spark Connect server <br/>// because the workload can be very unevenly distributed over time.<br/>spark.dynamicAllocation.enabled: true  // default: false<br/><br/>// This pair of parameters is responsible for the timely removal of idle <br/>// executors:<br/>spark.dynamicAllocation.cachedExecutorIdleTimeout: 5m  // default: infinity<br/>spark.dynamicAllocation.shuffleTracking.timeout: 5m  // default: infinity<br/><br/>// To create new executors only when the existing ones cannot handle <br/>// the received tasks for a significant amount of time. This allows you <br/>// to save resources when a small number of tasks arrive at some point <br/>// in time, which do not require many executors for timely processing. <br/>// With increased schedulerBacklogTimeout, unnecessary executors do not <br/>// have the opportunity to appear by the time all incoming tasks are <br/>// completed. The time to complete the tasks increases slightly with this, <br/>// but in most cases, this increase is not significant.<br/>spark.dynamicAllocation.schedulerBacklogTimeout: 30s  // default: 1s<br/><br/>// If, for some reason, you need to stop the execution of a client <br/>// application (and free up resources), you can forcibly terminate the client. <br/>// Currently, even explicitly closing the client SparkSession does not <br/>// immediately end the execution of its corresponding Jobs on the server. <br/>// They will continue to run for a duration equal to 'detachedTimeout'. <br/>// Therefore, it may be reasonable to reduce it.<br/>spark.connect.execute.manager.detachedTimeout: 2m  // default: 5m<br/><br/>// We have encountered a situation when killed tasks may hang for <br/>// an unpredictable amount of time, leading to bad consequences for their <br/>// executors. In this case, it is better to remove the executor on which <br/>// this problem occurred.<br/>spark.task.reaper.enabled: true // default: false<br/>spark.task.reaper.killTimeout: 300s  // default: -1<br/><br/>// The Spark Connect server can run for an extended period of time. During <br/>// this time, executors may fail, including for reasons beyond our control <br/>// (e.g., AWS Spot interruptions). This option is needed to prevent <br/>// the entire server from failing in such cases.<br/>spark.executor.maxNumFailures: 1000<br/><br/>// In our experience, BroadcastJoin can lead to very serious performance <br/>// issues in some cases. So, we decided to disable broadcasting. <br/>// Disabling this option usually does not result in a noticeable performance <br/>// degradation for our typical applications anyway.<br/>spark.sql.autoBroadcastJoinThreshold: -1 // default: 10MB<br/><br/>// For many of our client applications, we have to add an artifact to <br/>// the client session (method sparkSession.addArtifact()). <br/>// Using 'useFetchCache=true' results in double space consumption for <br/>// the application JAR files on executors' disks, as they are also duplicated <br/>// in a local cache folder. Sometimes, this even causes disk overflow with <br/>// subsequent problems for the executor.<br/>spark.files.useFetchCache: false   // default: true<br/><br/>// To ensure fair resource allocation when multiple applications are <br/>// running concurrently.<br/>spark.scheduler.mode: FAIR  // default: FIFO</span></pre><p id="7789" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For example, after we adjusted the <code class="cx qa qb qc qd b">idle timeout</code> properties, the resource utilization changed as follows:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qm"><img src="../Images/ea2bebad36ec8b519bfb5cfc004c4e11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GfggP_jczXDg5FAd5eFKCA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><h2 id="f137" class="pi nz fq bf oa pj pk pl od pm pn po og nl pp pq pr np ps pt pu nt pv pw px py bk">Preventive restart</h2><p id="0e17" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">In our environment, the Spark Connect server (version 3.5) may become unstable after a few days of continuous operation. Most often, we face randomly hanging client application jobs for an infinite amount of time, but there may be other problems as well. Also, over time, the probability of a random failure of the entire Spark Connect server increases dramatically, and this can happen at the wrong moment.</p><p id="07cb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As this component evolves, it will likely become more stable (or we will find out that we have done something wrong in our Spark Connect setup). But currently, the simplest solution has turned out to be a daily preventive restart of the Spark Connect server at a suitable moment (i.e., when no client applications are running on it). An example of what the restart code might look like <a class="af nb" href="https://gist.github.com/kotlovs/437809684d7ebe3b7a93a1af804def8c" rel="noopener ugc nofollow" target="_blank">can be found here</a>.</p><h1 id="e5b9" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Conclusion</h1><p id="130d" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">In this article, I described our experience using Spark Connect to run a large number of diverse Spark applications.</p><p id="dd58" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To summarize the above:</p><ul class=""><li id="9983" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">This component can help save resources and reduce the wait time for the execution of Spark client applications.</li><li id="6e8a" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">It is better to be careful about which applications should be run on the shared Spark Connect server, as resource-intensive applications may cause problems for the entire system.</li><li id="d2a6" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">You can create an infrastructure for launching client applications so that the decision on how to run any application (either as a separate Spark application or as a Spark Connect client) can be made automatically at the moment of launch.</li><li id="5ca4" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">It is important to note that not all applications will be able to run on Spark Connect, but the number of such cases can be significantly reduced. If there is a possibility of running applications that have not been tested for compatibility with the Spark Connect version of SparkSession API, it is worth implementing a fallback to separate Spark applications.</li><li id="4f47" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">It is worth paying attention to the Spark properties that can improve resource utilization and increase the overall stability of the Spark Connect server. It may also be reasonable to set up a periodic preventive restart of the Spark Connect server to reduce the probability of accidental failure and unwanted behavior.</li></ul><p id="85a3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Overall, we have had a positive experience using Spark Connect in our company. We will continue to watch the development of this technology with great interest, and there is a plan to expand its use.</p></div></div></div></div>    
</body>
</html>