- en: The Proof of Learning in Machine Learning/AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ /äººå·¥æ™ºèƒ½ä¸­çš„å­¦ä¹ è¯æ˜
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/the-proof-of-learning-in-machine-learning-ai-4faae3c85fe6?source=collection_archive---------0-----------------------#2024-05-17](https://towardsdatascience.com/the-proof-of-learning-in-machine-learning-ai-4faae3c85fe6?source=collection_archive---------0-----------------------#2024-05-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/the-proof-of-learning-in-machine-learning-ai-4faae3c85fe6?source=collection_archive---------0-----------------------#2024-05-17](https://towardsdatascience.com/the-proof-of-learning-in-machine-learning-ai-4faae3c85fe6?source=collection_archive---------0-----------------------#2024-05-17)
- en: Before any mathematical development, we must first understand the foundation
    of learning and how it is closely linked to the concept of error
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨è¿›è¡Œä»»ä½•æ•°å­¦æ¨å¯¼ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»é¦–å…ˆç†è§£å­¦ä¹ çš„åŸºç¡€ï¼Œå¹¶ä¸”äº†è§£å®ƒå¦‚ä½•ä¸è¯¯å·®çš„æ¦‚å¿µç´§å¯†ç›¸å…³ã€‚
- en: '[](https://medium.com/@romulo_pauliv?source=post_page---byline--4faae3c85fe6--------------------------------)[![RÃ´mulo
    Pauliv](../Images/2a2d648d3ac84cf300bc7113b839c4b5.png)](https://medium.com/@romulo_pauliv?source=post_page---byline--4faae3c85fe6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4faae3c85fe6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4faae3c85fe6--------------------------------)
    [RÃ´mulo Pauliv](https://medium.com/@romulo_pauliv?source=post_page---byline--4faae3c85fe6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@romulo_pauliv?source=post_page---byline--4faae3c85fe6--------------------------------)[![RÃ´mulo
    Pauliv](../Images/2a2d648d3ac84cf300bc7113b839c4b5.png)](https://medium.com/@romulo_pauliv?source=post_page---byline--4faae3c85fe6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4faae3c85fe6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4faae3c85fe6--------------------------------)
    [RÃ´mulo Pauliv](https://medium.com/@romulo_pauliv?source=post_page---byline--4faae3c85fe6--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4faae3c85fe6--------------------------------)
    Â·9 min readÂ·May 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4faae3c85fe6--------------------------------)
    Â·9åˆ†é’Ÿé˜…è¯»Â·2024å¹´5æœˆ17æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The Hypothetical Cook
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‡è®¾çš„å¨å¸ˆ
- en: Imagine that, on any given day, you decide to replicate a delicacy you ate at
    a renowned restaurant. You remember the taste of this delicacy perfectly. Based
    on this, you search for the recipe online and attempt to reproduce it at home.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸‹ï¼Œåœ¨æŸä¸€å¤©ï¼Œä½ å†³å®šå¤åˆ¶ä¸€ä»½ä½ åœ¨ä¸€å®¶è‘—åé¤é¦†åƒè¿‡çš„ç¾å‘³ã€‚ä½ å®Œç¾åœ°è®°å¾—è¿™é“èœçš„å‘³é“ã€‚åŸºäºæ­¤ï¼Œä½ åœ¨ç½‘ä¸Šæœç´¢é£Ÿè°±å¹¶å°è¯•åœ¨å®¶ä¸­é‡ç°å®ƒã€‚
- en: Letâ€™s denote the taste of the delicacy you ate at the restaurant as *T*, which
    will represent the expected taste, your *target*. Based on the recipe you found
    online, you hope to achieve this goal, i.e., the taste *T*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ åœ¨é¤é¦†å“å°çš„ç¾å‘³çš„å‘³é“è®°ä½œ *T*ï¼Œå®ƒä»£è¡¨äº†ä½ æœŸæœ›çš„å‘³é“ï¼Œå³ä½ çš„ *ç›®æ ‡*ã€‚æ ¹æ®ä½ åœ¨ç½‘ä¸Šæ‰¾åˆ°çš„é£Ÿè°±ï¼Œä½ å¸Œæœ›å®ç°è¿™ä¸ªç›®æ ‡ï¼Œä¹Ÿå°±æ˜¯å‘³é“ *T*ã€‚
- en: To reproduce this recipe, you follow all the indicated steps, use all the ingredients,
    the necessary temperature, the cooking time, etc. Letâ€™s denote all these methods
    and ingredients as *X*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¤åˆ¶è¿™ä¸ªé£Ÿè°±ï¼Œä½ æŒ‰ç…§æ‰€æœ‰æŒ‡ç¤ºçš„æ­¥éª¤ï¼Œä½¿ç”¨æ‰€æœ‰å¿…è¦çš„é£Ÿæã€æ¸©åº¦ã€çƒ¹é¥ªæ—¶é—´ç­‰ã€‚æˆ‘ä»¬å°†æ‰€æœ‰è¿™äº›æ–¹æ³•å’Œé£Ÿæè®°ä½œ *X*ã€‚
- en: After completing the entire process, you taste the dish. At this moment, you
    judge whether it is similar to the expected taste *T*. You notice that it is saltier
    or sweeter than expected. The taste of the delicacy you reproduced at home will
    be represented by *Y*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆæ•´ä¸ªè¿‡ç¨‹åï¼Œä½ å“å°è¿™é“èœã€‚æ­¤æ—¶ï¼Œä½ åˆ¤æ–­å®ƒæ˜¯å¦ä¸é¢„æœŸçš„å‘³é“ *T* ç›¸ä¼¼ã€‚ä½ æ³¨æ„åˆ°å®ƒæ¯”é¢„æœŸçš„å‘³é“æ›´å’¸æˆ–æ›´ç”œã€‚ä½ åœ¨å®¶ä¸­é‡ç°çš„ç¾å‘³å‘³é“è®°ä½œ *Y*ã€‚
- en: Therefore, upon realizing that the taste is different from the target *T*, you
    assign a quantitative measure of how different it is from the target taste based
    on taste *Y*. In other words, you could have added more salt or less salt, more
    seasoning or less seasoning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå½“ä½ æ„è¯†åˆ°å‘³é“ä¸ç›®æ ‡ *T* ä¸åŒæ—¶ï¼Œä½ ä¼šæ ¹æ®å‘³é“ *Y* èµ‹äºˆå®ƒä¸ç›®æ ‡å‘³é“çš„å·®å¼‚é‡åº¦ã€‚æ¢å¥è¯è¯´ï¼Œä½ å¯èƒ½æ·»åŠ äº†æ›´å¤šç›æˆ–æ›´å°‘ç›ï¼Œæ›´å¤šè°ƒæ–™æˆ–æ›´å°‘è°ƒæ–™ã€‚
- en: The difference between *T* and *Y* can be defined as the error *E*. The distinction
    between *T* and *Y* is made by the memory of your palate. Therefore, your palate
    performs a specific function at this moment, which we can define as *P(Y) = E*.
    In other words, when experiencing taste *Y*, the palate assigns the error *E*
    based on the target taste *T*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*T* å’Œ *Y* ä¹‹é—´çš„å·®å¼‚å¯ä»¥å®šä¹‰ä¸ºè¯¯å·® *E*ã€‚*T* å’Œ *Y* ä¹‹é—´çš„åŒºåˆ«ç”±ä½ å‘³è•¾çš„è®°å¿†äº§ç”Ÿã€‚å› æ­¤ï¼Œå‘³è•¾åœ¨æ­¤æ—¶æ‰§è¡Œç‰¹å®šåŠŸèƒ½ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶å®šä¹‰ä¸º
    *P(Y) = E*ã€‚æ¢å¥è¯è¯´ï¼Œå½“ä½ ä½“éªŒåˆ°å‘³é“ *Y* æ—¶ï¼Œå‘³è•¾æ ¹æ®ç›®æ ‡å‘³é“ *T* æ¥åˆ†é…è¯¯å·® *E*ã€‚'
- en: Having this quantitative measure of error *E*, we can reproduce this recipe
    every day so that with each passing day, the error *E* decreases. In other words,
    the distance between the target taste *T* and the taste *Y* decreases until *T
    = Y*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‹¥æœ‰è¿™ä¸ªå®šé‡çš„è¯¯å·®*E*è¡¡é‡æ ‡å‡†åï¼Œæˆ‘ä»¬å¯ä»¥æ¯å¤©å¤åˆ¶è¿™ä¸ªé£Ÿè°±ï¼Œè¿™æ ·éšç€æ—¶é—´çš„æ¨ç§»ï¼Œè¯¯å·®*E*ä¼šé€æ¸å‡å°ã€‚æ¢å¥è¯è¯´ï¼Œç›®æ ‡å‘³é“*T*ä¸å‘³é“*Y*ä¹‹é—´çš„è·ç¦»ä¼šå‡å°ï¼Œç›´åˆ°*T
    = Y*ã€‚
- en: Based on this hypothetical scenario, we can define error as the judgment that
    disagrees with the observed reality, where there is always a function that performs
    the action of judging. Therefore, in the above case, taste and memory created
    this judging function.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè¿™ä¸ªå‡è®¾åœºæ™¯ï¼Œæˆ‘ä»¬å¯ä»¥å°†é”™è¯¯å®šä¹‰ä¸ºä¸è§‚å¯Ÿåˆ°çš„ç°å®ä¸ä¸€è‡´çš„åˆ¤æ–­ï¼Œå…¶ä¸­æ€»æ˜¯å­˜åœ¨ä¸€ä¸ªæ‰§è¡Œåˆ¤æ–­çš„å‡½æ•°ã€‚å› æ­¤ï¼Œåœ¨ä¸Šè¿°æƒ…å†µä¸‹ï¼Œå‘³è§‰å’Œè®°å¿†åˆ›é€ äº†è¿™ä¸ªåˆ¤æ–­åŠŸèƒ½ã€‚
- en: The act of learning, in this specific case, is characterized by the ability
    to reduce error. In other words, it is the ability to interact in different ways
    with the reproduced object in order to decrease the output of the judging function.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç‰¹å®šçš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ çš„è¡Œä¸ºçš„ç‰¹ç‚¹æ˜¯å‡å°‘é”™è¯¯çš„èƒ½åŠ›ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒæ˜¯ä»¥ä¸åŒçš„æ–¹å¼ä¸å†ç°çš„å¯¹è±¡äº’åŠ¨ï¼Œä»¥å‡å°‘åˆ¤æ–­åŠŸèƒ½çš„è¾“å‡ºã€‚
- en: The Cookâ€™s Expertise
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¨å¸ˆçš„ä¸“ä¸šçŸ¥è¯†
- en: Returning to the hypothetical case, we have the ingredients and methods *X*
    as indicated by the recipe. All the ingredients and equipment are the same as
    those used by the restaurant; therefore, the outcome depends solely on your ability
    to manipulate them correctly to achieve the target taste *T*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å›åˆ°å‡è®¾çš„æƒ…å†µï¼Œæˆ‘ä»¬æœ‰é£Ÿæå’Œæ–¹æ³•*X*ï¼Œå¦‚é£Ÿè°±æ‰€ç¤ºã€‚æ‰€æœ‰çš„é£Ÿæå’Œè®¾å¤‡éƒ½ä¸é¤å…ä½¿ç”¨çš„ç›¸åŒï¼›å› æ­¤ï¼Œç»“æœå®Œå…¨å–å†³äºä½ æ­£ç¡®æ“æ§å®ƒä»¬çš„èƒ½åŠ›ï¼Œä»¥å®ç°ç›®æ ‡å‘³é“*T*ã€‚
- en: In other words, you manipulate *X* to obtain *Y*. Therefore, we can define that
    you are essentially a function that transforms *X* into *Y*, denoted as *f(X)
    = Y*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œä½ æ“æ§*X*æ¥è·å¾—*Y*ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä½ æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå°†*X*è½¬åŒ–ä¸º*Y*çš„å‡½æ•°ï¼Œè¡¨ç¤ºä¸º*f(X) = Y*ã€‚
- en: The function *f(X)*, which represents the act of manipulating the ingredients,
    also depends on how your brain functions. In other words, if you have had culinary
    experiences, you will find it easier to transform *X* into *Y*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºæ“æ§é£Ÿæè¡Œä¸ºçš„å‡½æ•°*f(X)*ï¼Œä¹Ÿä¾èµ–äºä½ çš„å¤§è„‘å¦‚ä½•è¿ä½œã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœä½ æœ‰è¿‡çƒ¹é¥ªç»éªŒï¼Œä½ ä¼šå‘ç°å°†*X*è½¬åŒ–ä¸º*Y*æ›´å®¹æ˜“ã€‚
- en: Letâ€™s define *W* as the weights of your neurons or your neural capacity to manipulate
    *X*. If *W* is already pre-adjusted based on culinary experiences, it will be
    easier to transform *X* into *Y*. Otherwise, we will need to adjust *W* until
    we can transform *X* into *Y*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å®šä¹‰*W*ä¸ºä½ ç¥ç»å…ƒçš„æƒé‡ï¼Œæˆ–è€…ä½ æ“æ§*X*çš„ç¥ç»èƒ½åŠ›ã€‚å¦‚æœ*W*å·²ç»åŸºäºçƒ¹é¥ªç»éªŒè¿›è¡Œäº†é¢„è°ƒæ•´ï¼Œé‚£ä¹ˆå°†*X*è½¬åŒ–ä¸º*Y*ä¼šæ›´å®¹æ˜“ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬å°†éœ€è¦è°ƒæ•´*W*ï¼Œç›´åˆ°èƒ½å¤Ÿå°†*X*è½¬åŒ–ä¸º*Y*ã€‚
- en: Therefore, we know that *f(X) = Y* also depends on *W*, i.e., we can represent
    it linearly where *f(X) = WX*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬çŸ¥é“*f(X) = Y*ä¹Ÿä¾èµ–äº*W*ï¼Œå³æˆ‘ä»¬å¯ä»¥å°†å…¶çº¿æ€§è¡¨ç¤ºä¸º*f(X) = WX*ã€‚
- en: Thus, our goal is to discover how we can modify *W* until the generated *Y*
    is very close or equal to *T*. In other words, how can we adjust *W* until the
    error *E* significantly decreases or becomes zero.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å‘ç°å¦‚ä½•ä¿®æ”¹*W*ï¼Œç›´åˆ°ç”Ÿæˆçš„*Y*éå¸¸æ¥è¿‘æˆ–ç­‰äº*T*ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¦‚ä½•è°ƒæ•´*W*ï¼Œç›´åˆ°è¯¯å·®*E*æ˜¾è‘—å‡å°‘æˆ–å˜ä¸ºé›¶ã€‚
- en: The Cost Function
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆæœ¬å‡½æ•°
- en: The function that evaluates the difference between the result and the expected
    outcome is the *cost function*. The function that converts the ingredients and
    culinary methods into the delicacy is our model, which can be an artificial neural
    network or other machine learning models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°ç»“æœä¸é¢„æœŸç»“æœä¹‹é—´å·®å¼‚çš„å‡½æ•°æ˜¯*æˆæœ¬å‡½æ•°*ã€‚å°†é£Ÿæå’Œçƒ¹é¥ªæ–¹æ³•è½¬åŒ–ä¸ºç¾é£Ÿçš„å‡½æ•°æ˜¯æˆ‘ä»¬çš„æ¨¡å‹ï¼Œè¿™å¯ä»¥æ˜¯äººå·¥ç¥ç»ç½‘ç»œæˆ–å…¶ä»–æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚
- en: '![](../Images/81c8069c15e37623b48e52f0d4d47639.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81c8069c15e37623b48e52f0d4d47639.png)'
- en: 'Source: The Author. Eq. (1)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ1ï¼‰
- en: In equation (1), the definition of the cost function *E*, which depends on the
    *n* weights *w*. In other words, it is a function that indicates the error based
    on the values of *w*. In a specific case where all *n* weights *w* are not adjusted,
    the value of the error *E* will be large. Conversely, in a case where the weights
    are properly adjusted, the value of the error *E* will be small or zero.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…¬å¼ï¼ˆ1ï¼‰ä¸­ï¼Œæˆæœ¬å‡½æ•°*E*çš„å®šä¹‰ä¾èµ–äº*n*ä¸ªæƒé‡*w*ã€‚æ¢å¥è¯è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº*w*çš„å€¼æŒ‡ç¤ºè¯¯å·®çš„å‡½æ•°ã€‚åœ¨ä¸€ä¸ªç‰¹å®šçš„æƒ…å†µä¸‹ï¼Œå½“æ‰€æœ‰*n*ä¸ªæƒé‡*w*éƒ½æ²¡æœ‰è°ƒæ•´æ—¶ï¼Œè¯¯å·®*E*çš„å€¼å°†å¾ˆå¤§ã€‚ç›¸åï¼Œåœ¨æƒé‡é€‚å½“è°ƒæ•´çš„æƒ…å†µä¸‹ï¼Œè¯¯å·®*E*çš„å€¼å°†å¾ˆå°æˆ–ä¸ºé›¶ã€‚
- en: '![](../Images/a9881fc9e2264c7e7090d4622f42596f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9881fc9e2264c7e7090d4622f42596f.png)'
- en: 'Source: The Author. Eq. (2)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ2ï¼‰
- en: Therefore, our objective is to find the values of the *n* weights *w* such that
    the condition above is true.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°*n*ä¸ªæƒé‡*w*çš„å€¼ï¼Œä½¿ä¸Šè¿°æ¡ä»¶æˆç«‹ã€‚
- en: The Gradient
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢¯åº¦
- en: 'To facilitate understanding of how we will do this, we will define the following
    function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¾¿äºç†è§£æˆ‘ä»¬å°†å¦‚ä½•è¿›è¡Œï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†å®šä¹‰ä»¥ä¸‹å‡½æ•°ï¼š
- en: '![](../Images/3f31937eb0c958d9a052d0c932592352.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f31937eb0c958d9a052d0c932592352.png)'
- en: 'Source: The Author. Img. (1)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å›¾åƒï¼ˆ1ï¼‰
- en: Therefore, we intuitively know that when *x = 0* and *y = 0*, *f(x, y) = 0*.
    However, we want an algorithm that, given random *x* and *y* values, adjusts the
    values of *x* and *y* until the function *f(x, y)* equals zero.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬ç›´è§‚åœ°çŸ¥é“ï¼Œå½“*x = 0*å’Œ*y = 0*æ—¶ï¼Œ*f(x, y) = 0*ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä¸€ç§ç®—æ³•ï¼Œåœ¨ç»™å®šéšæœºçš„*x*å’Œ*y*å€¼çš„æƒ…å†µä¸‹ï¼Œè°ƒæ•´*x*å’Œ*y*çš„å€¼ï¼Œç›´åˆ°å‡½æ•°*f(x,
    y)*ç­‰äºé›¶ã€‚
- en: To achieve this, we can use the gradient of the function. In vector calculus,
    the gradient is a vector that indicates the direction and magnitude in which,
    by displacement from the specified point, we obtain the greatest possible increase
    in the value of a quantity.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‡½æ•°çš„æ¢¯åº¦ã€‚åœ¨å‘é‡å¾®ç§¯åˆ†ä¸­ï¼Œæ¢¯åº¦æ˜¯ä¸€ä¸ªå‘é‡ï¼Œè¡¨ç¤ºé€šè¿‡ä»æŒ‡å®šç‚¹ä½ç§»ï¼Œå¯ä»¥è·å¾—æŸä¸€é‡å€¼çš„æœ€å¤§å¯èƒ½å¢é‡çš„æ–¹å‘å’Œå¤§å°ã€‚
- en: '![](../Images/6b0a4bc945f0bc3b6c5593ea6afea079.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b0a4bc945f0bc3b6c5593ea6afea079.png)'
- en: 'Source: The Author. Eq. (3)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ3ï¼‰
- en: That is, by applying the gradient to the function *f(x, y)*, we obtain a vector,
    as shown in the equation (3), which informs how to increment the values of *x*
    and *y* so that the value of *f(x, y)* grows. However, our goal is to find the
    values of *x* and *y* necessary for the function *f(x, y) = 0*. Therefore, we
    can use the negative gradient.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå°±æ˜¯è¯´ï¼Œé€šè¿‡å¯¹å‡½æ•°*f(x, y)*åº”ç”¨æ¢¯åº¦ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªå‘é‡ï¼Œå¦‚å…¬å¼ï¼ˆ3ï¼‰æ‰€ç¤ºï¼Œå®ƒæŒ‡ç¤ºäº†å¦‚ä½•å¢åŠ *x*å’Œ*y*çš„å€¼ï¼Œä»¥ä¾¿ä½¿*f(x, y)*çš„å€¼å¢é•¿ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä½¿*f(x,
    y) = 0*çš„*x*å’Œ*y*çš„å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è´Ÿæ¢¯åº¦ã€‚
- en: Below is a representation in two dimensions of the function *f(x, y)* where
    the coloring shows the value of *z*. Using the negative gradient, we see the vectors
    pointing to the minimum of the function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å‡½æ•°*f(x, y)*åœ¨äºŒç»´ç©ºé—´ä¸­çš„è¡¨ç¤ºï¼Œé¢œè‰²æ˜¾ç¤ºäº†*z*çš„å€¼ã€‚ä½¿ç”¨è´Ÿæ¢¯åº¦ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æŒ‡å‘å‡½æ•°æœ€å°å€¼çš„å‘é‡ã€‚
- en: '![](../Images/a0bb5dd854ce8c7cd8056e5bc0dcf2e6.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0bb5dd854ce8c7cd8056e5bc0dcf2e6.png)'
- en: 'Source: The Author. Img. (2)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å›¾åƒï¼ˆ2ï¼‰
- en: Based on this, we can develop a method to update *x* and *y* using the gradient
    field of the function *f(x, y)* to find the necessary values for *f(x, y) = 0*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å‘ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨å‡½æ•°*f(x, y)*çš„æ¢¯åº¦åœºæ¥æ›´æ–°*x*å’Œ*y*çš„å€¼ï¼Œä»¥æ‰¾åˆ°ä½¿*f(x, y) = 0*çš„å¿…è¦å€¼ã€‚
- en: The Proof of Learning
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å­¦ä¹ çš„è¯æ˜
- en: We will define a simple function *f(x)* for algorithm testing. Our intention
    is to find the minimum of this function. To do this, we can apply the gradient
    of *f(x)*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªç®€å•çš„å‡½æ•°*f(x)*è¿›è¡Œç®—æ³•æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°è¿™ä¸ªå‡½æ•°çš„æœ€å°å€¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨*f(x)*çš„æ¢¯åº¦ã€‚
- en: '![](../Images/2e5c69f07244b0fc525671c391f4667c.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e5c69f07244b0fc525671c391f4667c.png)'
- en: 'Source: The Author. Eq. (4)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ4ï¼‰
- en: Above, we have the gradient of the function *f(x)*. We will not delve into defining
    the concept of derivative in this article, but we recommend [reading about its
    definition](https://www.britannica.com/science/derivative-mathematics) and why
    we can represent it in this way.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢æ˜¯å‡½æ•°*f(x)*çš„æ¢¯åº¦ã€‚æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­ä¸ä¼šæ·±å…¥å®šä¹‰å¯¼æ•°çš„æ¦‚å¿µï¼Œä½†æˆ‘ä»¬å»ºè®®[é˜…è¯»å®ƒçš„å®šä¹‰](https://www.britannica.com/science/derivative-mathematics)ä»¥åŠä¸ºä»€ä¹ˆæˆ‘ä»¬å¯ä»¥è¿™æ ·è¡¨ç¤ºå®ƒã€‚
- en: 'Knowing that *h* tends to zero, we can represent the gradient of *f(x)* as
    follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ¥é“*h*è¶‹å‘äºé›¶ï¼Œæˆ‘ä»¬å¯ä»¥å¦‚ä¸‹è¡¨ç¤º*f(x)*çš„æ¢¯åº¦ï¼š
- en: '![](../Images/aacc5bc9b490877922185777eb5704ee.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aacc5bc9b490877922185777eb5704ee.png)'
- en: 'Source: The Author. Eq. (5)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ5ï¼‰
- en: 'Based on this, we can replace *h* with the following term:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä»¥ä¸‹é¡¹æ›¿ä»£*h*ï¼š
- en: '![](../Images/38547a0b1525f66f6c154596eff08895.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38547a0b1525f66f6c154596eff08895.png)'
- en: 'Source: The Author. Eq. (6)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ6ï¼‰
- en: 'We define the element *alpha* to maintain the necessity of the term *h*, where
    *alpha* must be strictly positive and always tend to zero, identical to the term
    *h*. Substituting the new relationship into the definition of derivative, we have:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰å…ƒç´ *alpha*æ¥ä¿æŒ*h*é¡¹çš„å¿…è¦æ€§ï¼Œå…¶ä¸­*alpha*å¿…é¡»ä¸¥æ ¼ä¸ºæ­£ï¼Œå¹¶ä¸”å§‹ç»ˆè¶‹å‘äºé›¶ï¼Œå’Œ*h*é¡¹ç›¸åŒã€‚å°†è¿™ä¸€æ–°çš„å…³ç³»ä»£å…¥å¯¼æ•°å®šä¹‰ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
- en: '![](../Images/d2855f453f451dc2a29c66fe68e26d12.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2855f453f451dc2a29c66fe68e26d12.png)'
- en: 'Source: The Author. Eq. (7)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ7ï¼‰
- en: Now we have a valuable relationship for our proof. We know that any element
    squared will be positive. From this concept comes the need to replace *h* with
    minus *alpha* times the *gradient of f(x)*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªå¯¹æˆ‘ä»¬è¯æ˜æœ‰ä»·å€¼çš„å…³ç³»ã€‚æˆ‘ä»¬çŸ¥é“ä»»ä½•å…ƒç´ çš„å¹³æ–¹éƒ½ä¼šæ˜¯æ­£æ•°ã€‚åŸºäºè¿™ä¸€æ¦‚å¿µï¼Œæˆ‘ä»¬éœ€è¦ç”¨è´Ÿ *alpha* å€çš„ *f(x)* çš„æ¢¯åº¦æ¥æ›¿æ¢
    *h*ã€‚
- en: 'So:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼š
- en: '![](../Images/eda81fa6ccc6d5f3476ae70aaf7b74fd.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eda81fa6ccc6d5f3476ae70aaf7b74fd.png)'
- en: 'Source: The Author. Eq. (8)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ8ï¼‰
- en: Therefore, we can judge that the condition in (8) is true as long as *alpha*
    is always a positive value.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ¤æ–­ï¼Œå½“ *alpha* å§‹ç»ˆä¸ºæ­£å€¼æ—¶ï¼Œæ¡ä»¶ï¼ˆ8ï¼‰æˆç«‹ã€‚
- en: '![](../Images/3252475d5891fd3c42050d356336ae5f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3252475d5891fd3c42050d356336ae5f.png)'
- en: 'Source: The Author. Eq. (9)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ9ï¼‰
- en: 'That is, the value of *f(x)* being subtracted by a strictly positive value
    will always be less than the original value of *f(x)*. Therefore, we can replace
    it with the following relationship using eq. (7) and (9):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå°±æ˜¯è¯´ï¼Œ*f(x)* çš„å€¼å‡å»ä¸€ä¸ªä¸¥æ ¼æ­£çš„å€¼å°†æ€»æ˜¯å°äº *f(x)* çš„åŸå§‹å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å…¬å¼ï¼ˆ7ï¼‰å’Œï¼ˆ9ï¼‰æ›¿æ¢ä¸ºä»¥ä¸‹å…³ç³»ï¼š
- en: '![](../Images/9196a08c8585b7dabac2f3681905d4cb.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9196a08c8585b7dabac2f3681905d4cb.png)'
- en: 'Source: The Author. Eq. (10)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ10ï¼‰
- en: Therefore, we have a proven relationship on how to update the values of *x*
    so that the function *f(x)* is at least smaller than its previous state.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å·²ç»è¯æ˜äº†å¦‚ä½•æ›´æ–° *x* çš„å€¼ï¼Œä»¥ç¡®ä¿å‡½æ•° *f(x)* è‡³å°‘å°äºå…¶å…ˆå‰çš„å€¼çš„å…³ç³»ã€‚
- en: '![](../Images/38dc51d4143b3e80cc9a4c707e7278e4.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38dc51d4143b3e80cc9a4c707e7278e4.png)'
- en: 'Source: The Author. Eq. (11)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ11ï¼‰
- en: 'So, we know how to decrease the current *x* to satisfy the inequality (11):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬çŸ¥é“å¦‚ä½•å‡å°‘å½“å‰çš„ *x* ä»¥æ»¡è¶³ä¸ç­‰å¼ï¼ˆ11ï¼‰ï¼š
- en: '![](../Images/f860f57ff92a6314dc8800177c2adb56.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f860f57ff92a6314dc8800177c2adb56.png)'
- en: 'Source: The Author. Eq. (12)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ12ï¼‰
- en: 'To confirm the validity of this relationship, we can apply this methodology
    to the function *f(x, y)* in img. (1) whose behavior we know. So:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¡®è®¤è¿™ä¸ªå…³ç³»çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ç§æ–¹æ³•åº”ç”¨äºå›¾åƒï¼ˆ1ï¼‰ä¸­çš„å‡½æ•° *f(x, y)*ï¼Œæˆ‘ä»¬å·²çŸ¥è¯¥å‡½æ•°çš„è¡Œä¸ºã€‚å› æ­¤ï¼š
- en: '![](../Images/d07ea17cab43755bdebf0265bf359b27.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d07ea17cab43755bdebf0265bf359b27.png)'
- en: 'Source: The Author. Eq. (13)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ13ï¼‰
- en: Applying this algorithm to the function *f(x, y)* numerous times, we expect
    to see the value of the function decrease until it reaches the minimum. To do
    this, we conducted a simulation where, in addition, we applied noise to the assignment
    of the updated *x* and *y* to visualize the decrease in the value of *f(x, y)*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤ç®—æ³•å¤šæ¬¡åº”ç”¨äºå‡½æ•° *f(x, y)*ï¼Œæˆ‘ä»¬é¢„æœŸçœ‹åˆ°å‡½æ•°å€¼ä¸‹é™ï¼Œç›´åˆ°è¾¾åˆ°æœ€å°å€¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ¨¡æ‹Ÿï¼Œå¹¶åœ¨å…¶ä¸­å¯¹æ›´æ–°åçš„ *x* å’Œ *y* èµ‹å€¼æ–½åŠ äº†å™ªå£°ï¼Œä»¥ä¾¿å¯è§†åŒ–
    *f(x, y)* çš„å€¼çš„ä¸‹é™ã€‚
- en: '![](../Images/54af915ff42ea099c2429f8cca2a7043.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54af915ff42ea099c2429f8cca2a7043.png)'
- en: 'Source: The Author. Img. (3)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å›¾åƒï¼ˆ3ï¼‰
- en: Notice that when the value of *alpha* tends to zero, we observe the values of
    *x* and *y* tending to the minimum of the function. When this is not true, for
    example, at *alpha = 0.6*, we observe a certain difficulty in finding the minimum
    of the function *f(x, y)*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå½“ *alpha* çš„å€¼è¶‹è¿‘äºé›¶æ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ° *x* å’Œ *y* çš„å€¼è¶‹å‘äºå‡½æ•°çš„æœ€å°å€¼ã€‚å½“è¿™ä¸€ç‚¹ä¸æˆç«‹æ—¶ï¼Œä¾‹å¦‚å½“ *alpha = 0.6*
    æ—¶ï¼Œæˆ‘ä»¬ä¼šå‘ç°æ‰¾åˆ° *f(x, y)* çš„æœ€å°å€¼æœ‰ä¸€å®šçš„éš¾åº¦ã€‚
- en: Gradient Descent
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™æ³•
- en: This algorithm is known as â€œGradient Descentâ€ or â€œMethod of Steepest Descent,â€
    being an optimization method to find the minimum of a function where each step
    is taken in the direction of the negative gradient. This method does not guarantee
    that the global minimum of the function will be found, but rather a local minimum.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç®—æ³•è¢«ç§°ä¸ºâ€œæ¢¯åº¦ä¸‹é™â€æˆ–â€œæœ€é€Ÿä¸‹é™æ³•â€ï¼Œå®ƒæ˜¯ä¸€ç§ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºå¯»æ‰¾å‡½æ•°çš„æœ€å°å€¼ï¼Œæ¯ä¸€æ­¥éƒ½æœç€è´Ÿæ¢¯åº¦çš„æ–¹å‘è¿›è¡Œã€‚è¯¥æ–¹æ³•å¹¶ä¸ä¿è¯æ‰¾åˆ°å‡½æ•°çš„å…¨å±€æœ€å°å€¼ï¼Œè€Œæ˜¯æ‰¾åˆ°å±€éƒ¨æœ€å°å€¼ã€‚
- en: Discussions about finding the global minimum could be developed in another article,
    but here, we have mathematically demonstrated how the gradient can be used for
    this purpose.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºæ‰¾åˆ°å…¨å±€æœ€å°å€¼çš„è®¨è®ºå¯ä»¥åœ¨å¦ä¸€ç¯‡æ–‡ç« ä¸­å±•å¼€ï¼Œä½†åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å·²ç»ä»æ•°å­¦ä¸Šè¯æ˜äº†å¦‚ä½•åˆ©ç”¨æ¢¯åº¦æ¥å®ç°è¿™ä¸€ç›®çš„ã€‚
- en: 'Now, applying it to the cost function *E* that depends on the *n* weights *w*,
    we have:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå°†å…¶åº”ç”¨äºä¾èµ–äº *n* ä¸ªæƒé‡ *w* çš„ä»£ä»·å‡½æ•° *E*ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
- en: '![](../Images/efddc76046b63efd822e311b8e49e13e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efddc76046b63efd822e311b8e49e13e.png)'
- en: 'Source: The Author. Eq. (14)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ14ï¼‰
- en: 'To update all elements of *W* based on gradient descent, we have:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åŸºäºæ¢¯åº¦ä¸‹é™æ³•æ›´æ–°æ‰€æœ‰å…ƒç´  *W*ï¼Œæˆ‘ä»¬æœ‰ï¼š
- en: '![](../Images/a714a82bbf8aa6a8fecdbcb25f225b76.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a714a82bbf8aa6a8fecdbcb25f225b76.png)'
- en: 'Source: The Author. Eq. (15)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ15ï¼‰
- en: 'And for any *n*th element ğ‘¤ of the vector *W*, we have:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå‘é‡ *W* çš„ä»»ä½• *n* ç¬¬nä¸ªå…ƒç´  ğ‘¤ï¼Œ æˆ‘ä»¬æœ‰ï¼š
- en: '![](../Images/3e9670c17c0ae010568107356bcca08d.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e9670c17c0ae010568107356bcca08d.png)'
- en: 'Source: The Author. Eq. (16)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…ã€‚å…¬å¼ï¼ˆ16ï¼‰
- en: Therefore, we have our *theoretical learning algorithm*. Logically, this is
    not applied to the hypothetical idea of the cook, but rather to numerous machine
    learning algorithms that we know today.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬æœ‰äº†æˆ‘ä»¬çš„*ç†è®ºå­¦ä¹ ç®—æ³•*ã€‚ä»é€»è¾‘ä¸Šè®²ï¼Œè¿™å¹¶ä¸æ˜¯åº”ç”¨äºå‡è®¾ä¸­çš„å¨å¸ˆçš„æ€æƒ³ï¼Œè€Œæ˜¯åº”ç”¨äºæˆ‘ä»¬ä»Šå¤©æ‰€çŸ¥é“çš„ä¼—å¤šæœºå™¨å­¦ä¹ ç®—æ³•ã€‚
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Based on what we have seen, we can conclude the demonstration and the mathematical
    proof of the theoretical learning algorithm. Such a structure is applied to numerous
    learning methods such as AdaGrad, Adam, and Stochastic Gradient Descent (SGD).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæˆ‘ä»¬æ‰€çœ‹åˆ°çš„å†…å®¹ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼Œç†è®ºå­¦ä¹ ç®—æ³•çš„æ¼”ç¤ºå’Œæ•°å­¦è¯æ˜ã€‚è¿™æ ·çš„ç»“æ„è¢«åº”ç”¨äºè®¸å¤šå­¦ä¹ æ–¹æ³•ï¼Œå¦‚AdaGradã€Adamå’Œéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ã€‚
- en: This method does not guarantee finding the *n*-weight values *w* where the *cost
    function* yields a result of zero or very close to it. However, it assures us
    that a local minimum of the cost function will be found.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•ä¸èƒ½ä¿è¯æ‰¾åˆ°ä½¿*æˆæœ¬å‡½æ•°*çš„ç»“æœä¸ºé›¶æˆ–æ¥è¿‘é›¶çš„*n*-æƒé‡å€¼*w*ï¼Œä½†å®ƒèƒ½ä¿è¯æ‰¾åˆ°æˆæœ¬å‡½æ•°çš„å±€éƒ¨æœ€å°å€¼ã€‚
- en: To address the issue of local minima, there are several more robust methods,
    such as SGD and Adam, which are commonly used in deep learning.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³å±€éƒ¨æœ€å°å€¼çš„é—®é¢˜ï¼Œæœ‰å‡ ç§æ›´ä¸ºç¨³å¥çš„æ–¹æ³•ï¼Œä¾‹å¦‚SGDå’ŒAdamï¼Œè¿™äº›æ–¹æ³•åœ¨æ·±åº¦å­¦ä¹ ä¸­å¸¸è¢«ä½¿ç”¨ã€‚
- en: Nevertheless, understanding the structure and the mathematical proof of the
    theoretical learning algorithm based on gradient descent will facilitate the comprehension
    of more complex algorithms.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œç†è§£åŸºäºæ¢¯åº¦ä¸‹é™çš„ç†è®ºå­¦ä¹ ç®—æ³•çš„ç»“æ„å’Œæ•°å­¦è¯æ˜ï¼Œå°†æœ‰åŠ©äºç†è§£æ›´å¤æ‚çš„ç®—æ³•ã€‚
- en: References
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'Carreira-Perpinan, M. A., & Hinton, G. E. (2005). On contrastive divergence
    learning. In R. G. Cowell & Z. Ghahramani (Eds.), Artificial Intelligence and
    Statistics, 2005\. (pp. 33â€“41). Fort Lauderdale, FL: Society for Artificial Intelligence
    and Statistics.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Carreira-Perpinan, M. A., & Hinton, G. E. (2005). å…³äºå¯¹æ¯”æ•£åº¦å­¦ä¹ ã€‚åœ¨R. G. Cowell & Z.
    Ghahramaniï¼ˆç¼–è¾‘ï¼‰ï¼Œäººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡å­¦ï¼Œ2005ã€‚ï¼ˆç¬¬33â€“41é¡µï¼‰ã€‚ä½›ç½—é‡Œè¾¾å·åŠ³å¾·ä»£å°”å ¡ï¼šäººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡å­¦å­¦ä¼šã€‚
- en: GarcÃ­a Cabello, J. Mathematical Neural Networks. Axioms 2022, 11, 80.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: GarcÃ­a Cabello, J. æ•°å­¦ç¥ç»ç½‘ç»œã€‚Axioms 2022, 11, 80ã€‚
- en: Geoffrey E. Hinton, Simon Osindero, Yee-Whye Teh. A Fast Learning Algorithm
    for Deep Belief Nets. Neural Computation 18, 1527â€“1554\. Massachusetts Institute
    of Technology
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Geoffrey E. Hinton, Simon Osindero, Yee-Whye Teh. A Fast Learning Algorithm
    for Deep Belief Nets. Neural Computation 18, 1527â€“1554. éº»çœç†å·¥å­¦é™¢
- en: LeCun, Y., Bottou, L., & Haffner, P. (1998). Gradient-based learning applied
    to document recognition. Proceedings of the IEEE, 86(11), 2278â€“2324.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: LeCun, Y., Bottou, L., & Haffner, P. (1998). åŸºäºæ¢¯åº¦çš„å­¦ä¹ åº”ç”¨äºæ–‡æ¡£è¯†åˆ«ã€‚IEEEå­¦æŠ¥ï¼Œ86(11)ï¼Œ2278â€“2324ã€‚
