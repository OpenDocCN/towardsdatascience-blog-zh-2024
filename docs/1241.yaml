- en: The Proof of Learning in Machine Learning/AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习/人工智能中的学习证明
- en: 原文：[https://towardsdatascience.com/the-proof-of-learning-in-machine-learning-ai-4faae3c85fe6?source=collection_archive---------0-----------------------#2024-05-17](https://towardsdatascience.com/the-proof-of-learning-in-machine-learning-ai-4faae3c85fe6?source=collection_archive---------0-----------------------#2024-05-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-proof-of-learning-in-machine-learning-ai-4faae3c85fe6?source=collection_archive---------0-----------------------#2024-05-17](https://towardsdatascience.com/the-proof-of-learning-in-machine-learning-ai-4faae3c85fe6?source=collection_archive---------0-----------------------#2024-05-17)
- en: Before any mathematical development, we must first understand the foundation
    of learning and how it is closely linked to the concept of error
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在进行任何数学推导之前，我们必须首先理解学习的基础，并且了解它如何与误差的概念紧密相关。
- en: '[](https://medium.com/@romulo_pauliv?source=post_page---byline--4faae3c85fe6--------------------------------)[![Rômulo
    Pauliv](../Images/2a2d648d3ac84cf300bc7113b839c4b5.png)](https://medium.com/@romulo_pauliv?source=post_page---byline--4faae3c85fe6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4faae3c85fe6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4faae3c85fe6--------------------------------)
    [Rômulo Pauliv](https://medium.com/@romulo_pauliv?source=post_page---byline--4faae3c85fe6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@romulo_pauliv?source=post_page---byline--4faae3c85fe6--------------------------------)[![Rômulo
    Pauliv](../Images/2a2d648d3ac84cf300bc7113b839c4b5.png)](https://medium.com/@romulo_pauliv?source=post_page---byline--4faae3c85fe6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4faae3c85fe6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4faae3c85fe6--------------------------------)
    [Rômulo Pauliv](https://medium.com/@romulo_pauliv?source=post_page---byline--4faae3c85fe6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4faae3c85fe6--------------------------------)
    ·9 min read·May 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4faae3c85fe6--------------------------------)
    ·9分钟阅读·2024年5月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The Hypothetical Cook
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 假设的厨师
- en: Imagine that, on any given day, you decide to replicate a delicacy you ate at
    a renowned restaurant. You remember the taste of this delicacy perfectly. Based
    on this, you search for the recipe online and attempt to reproduce it at home.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，在某一天，你决定复制一份你在一家著名餐馆吃过的美味。你完美地记得这道菜的味道。基于此，你在网上搜索食谱并尝试在家中重现它。
- en: Let’s denote the taste of the delicacy you ate at the restaurant as *T*, which
    will represent the expected taste, your *target*. Based on the recipe you found
    online, you hope to achieve this goal, i.e., the taste *T*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在餐馆品尝的美味的味道记作 *T*，它代表了你期望的味道，即你的 *目标*。根据你在网上找到的食谱，你希望实现这个目标，也就是味道 *T*。
- en: To reproduce this recipe, you follow all the indicated steps, use all the ingredients,
    the necessary temperature, the cooking time, etc. Let’s denote all these methods
    and ingredients as *X*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了复制这个食谱，你按照所有指示的步骤，使用所有必要的食材、温度、烹饪时间等。我们将所有这些方法和食材记作 *X*。
- en: After completing the entire process, you taste the dish. At this moment, you
    judge whether it is similar to the expected taste *T*. You notice that it is saltier
    or sweeter than expected. The taste of the delicacy you reproduced at home will
    be represented by *Y*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 完成整个过程后，你品尝这道菜。此时，你判断它是否与预期的味道 *T* 相似。你注意到它比预期的味道更咸或更甜。你在家中重现的美味味道记作 *Y*。
- en: Therefore, upon realizing that the taste is different from the target *T*, you
    assign a quantitative measure of how different it is from the target taste based
    on taste *Y*. In other words, you could have added more salt or less salt, more
    seasoning or less seasoning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当你意识到味道与目标 *T* 不同时，你会根据味道 *Y* 赋予它与目标味道的差异量度。换句话说，你可能添加了更多盐或更少盐，更多调料或更少调料。
- en: The difference between *T* and *Y* can be defined as the error *E*. The distinction
    between *T* and *Y* is made by the memory of your palate. Therefore, your palate
    performs a specific function at this moment, which we can define as *P(Y) = E*.
    In other words, when experiencing taste *Y*, the palate assigns the error *E*
    based on the target taste *T*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*T* 和 *Y* 之间的差异可以定义为误差 *E*。*T* 和 *Y* 之间的区别由你味蕾的记忆产生。因此，味蕾在此时执行特定功能，我们可以将其定义为
    *P(Y) = E*。换句话说，当你体验到味道 *Y* 时，味蕾根据目标味道 *T* 来分配误差 *E*。'
- en: Having this quantitative measure of error *E*, we can reproduce this recipe
    every day so that with each passing day, the error *E* decreases. In other words,
    the distance between the target taste *T* and the taste *Y* decreases until *T
    = Y*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这个定量的误差*E*衡量标准后，我们可以每天复制这个食谱，这样随着时间的推移，误差*E*会逐渐减小。换句话说，目标味道*T*与味道*Y*之间的距离会减小，直到*T
    = Y*。
- en: Based on this hypothetical scenario, we can define error as the judgment that
    disagrees with the observed reality, where there is always a function that performs
    the action of judging. Therefore, in the above case, taste and memory created
    this judging function.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个假设场景，我们可以将错误定义为与观察到的现实不一致的判断，其中总是存在一个执行判断的函数。因此，在上述情况下，味觉和记忆创造了这个判断功能。
- en: The act of learning, in this specific case, is characterized by the ability
    to reduce error. In other words, it is the ability to interact in different ways
    with the reproduced object in order to decrease the output of the judging function.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的情况下，学习的行为的特点是减少错误的能力。换句话说，它是以不同的方式与再现的对象互动，以减少判断功能的输出。
- en: The Cook’s Expertise
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 厨师的专业知识
- en: Returning to the hypothetical case, we have the ingredients and methods *X*
    as indicated by the recipe. All the ingredients and equipment are the same as
    those used by the restaurant; therefore, the outcome depends solely on your ability
    to manipulate them correctly to achieve the target taste *T*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 回到假设的情况，我们有食材和方法*X*，如食谱所示。所有的食材和设备都与餐厅使用的相同；因此，结果完全取决于你正确操控它们的能力，以实现目标味道*T*。
- en: In other words, you manipulate *X* to obtain *Y*. Therefore, we can define that
    you are essentially a function that transforms *X* into *Y*, denoted as *f(X)
    = Y*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，你操控*X*来获得*Y*。因此，我们可以定义你本质上是一个将*X*转化为*Y*的函数，表示为*f(X) = Y*。
- en: The function *f(X)*, which represents the act of manipulating the ingredients,
    also depends on how your brain functions. In other words, if you have had culinary
    experiences, you will find it easier to transform *X* into *Y*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表示操控食材行为的函数*f(X)*，也依赖于你的大脑如何运作。换句话说，如果你有过烹饪经验，你会发现将*X*转化为*Y*更容易。
- en: Let’s define *W* as the weights of your neurons or your neural capacity to manipulate
    *X*. If *W* is already pre-adjusted based on culinary experiences, it will be
    easier to transform *X* into *Y*. Otherwise, we will need to adjust *W* until
    we can transform *X* into *Y*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义*W*为你神经元的权重，或者你操控*X*的神经能力。如果*W*已经基于烹饪经验进行了预调整，那么将*X*转化为*Y*会更容易。否则，我们将需要调整*W*，直到能够将*X*转化为*Y*。
- en: Therefore, we know that *f(X) = Y* also depends on *W*, i.e., we can represent
    it linearly where *f(X) = WX*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们知道*f(X) = Y*也依赖于*W*，即我们可以将其线性表示为*f(X) = WX*。
- en: Thus, our goal is to discover how we can modify *W* until the generated *Y*
    is very close or equal to *T*. In other words, how can we adjust *W* until the
    error *E* significantly decreases or becomes zero.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的目标是发现如何修改*W*，直到生成的*Y*非常接近或等于*T*。换句话说，我们如何调整*W*，直到误差*E*显著减少或变为零。
- en: The Cost Function
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本函数
- en: The function that evaluates the difference between the result and the expected
    outcome is the *cost function*. The function that converts the ingredients and
    culinary methods into the delicacy is our model, which can be an artificial neural
    network or other machine learning models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 评估结果与预期结果之间差异的函数是*成本函数*。将食材和烹饪方法转化为美食的函数是我们的模型，这可以是人工神经网络或其他机器学习模型。
- en: '![](../Images/81c8069c15e37623b48e52f0d4d47639.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81c8069c15e37623b48e52f0d4d47639.png)'
- en: 'Source: The Author. Eq. (1)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（1）
- en: In equation (1), the definition of the cost function *E*, which depends on the
    *n* weights *w*. In other words, it is a function that indicates the error based
    on the values of *w*. In a specific case where all *n* weights *w* are not adjusted,
    the value of the error *E* will be large. Conversely, in a case where the weights
    are properly adjusted, the value of the error *E* will be small or zero.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式（1）中，成本函数*E*的定义依赖于*n*个权重*w*。换句话说，这是一个基于*w*的值指示误差的函数。在一个特定的情况下，当所有*n*个权重*w*都没有调整时，误差*E*的值将很大。相反，在权重适当调整的情况下，误差*E*的值将很小或为零。
- en: '![](../Images/a9881fc9e2264c7e7090d4622f42596f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9881fc9e2264c7e7090d4622f42596f.png)'
- en: 'Source: The Author. Eq. (2)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（2）
- en: Therefore, our objective is to find the values of the *n* weights *w* such that
    the condition above is true.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的目标是找到*n*个权重*w*的值，使上述条件成立。
- en: The Gradient
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度
- en: 'To facilitate understanding of how we will do this, we will define the following
    function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于理解我们将如何进行，接下来我们将定义以下函数：
- en: '![](../Images/3f31937eb0c958d9a052d0c932592352.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f31937eb0c958d9a052d0c932592352.png)'
- en: 'Source: The Author. Img. (1)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。图像（1）
- en: Therefore, we intuitively know that when *x = 0* and *y = 0*, *f(x, y) = 0*.
    However, we want an algorithm that, given random *x* and *y* values, adjusts the
    values of *x* and *y* until the function *f(x, y)* equals zero.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们直观地知道，当*x = 0*和*y = 0*时，*f(x, y) = 0*。然而，我们希望找到一种算法，在给定随机的*x*和*y*值的情况下，调整*x*和*y*的值，直到函数*f(x,
    y)*等于零。
- en: To achieve this, we can use the gradient of the function. In vector calculus,
    the gradient is a vector that indicates the direction and magnitude in which,
    by displacement from the specified point, we obtain the greatest possible increase
    in the value of a quantity.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们可以使用函数的梯度。在向量微积分中，梯度是一个向量，表示通过从指定点位移，可以获得某一量值的最大可能增量的方向和大小。
- en: '![](../Images/6b0a4bc945f0bc3b6c5593ea6afea079.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b0a4bc945f0bc3b6c5593ea6afea079.png)'
- en: 'Source: The Author. Eq. (3)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（3）
- en: That is, by applying the gradient to the function *f(x, y)*, we obtain a vector,
    as shown in the equation (3), which informs how to increment the values of *x*
    and *y* so that the value of *f(x, y)* grows. However, our goal is to find the
    values of *x* and *y* necessary for the function *f(x, y) = 0*. Therefore, we
    can use the negative gradient.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，通过对函数*f(x, y)*应用梯度，我们得到一个向量，如公式（3）所示，它指示了如何增加*x*和*y*的值，以便使*f(x, y)*的值增长。然而，我们的目标是找到使*f(x,
    y) = 0*的*x*和*y*的值。因此，我们可以使用负梯度。
- en: Below is a representation in two dimensions of the function *f(x, y)* where
    the coloring shows the value of *z*. Using the negative gradient, we see the vectors
    pointing to the minimum of the function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是函数*f(x, y)*在二维空间中的表示，颜色显示了*z*的值。使用负梯度，我们可以看到指向函数最小值的向量。
- en: '![](../Images/a0bb5dd854ce8c7cd8056e5bc0dcf2e6.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0bb5dd854ce8c7cd8056e5bc0dcf2e6.png)'
- en: 'Source: The Author. Img. (2)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。图像（2）
- en: Based on this, we can develop a method to update *x* and *y* using the gradient
    field of the function *f(x, y)* to find the necessary values for *f(x, y) = 0*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，我们可以开发一种方法，使用函数*f(x, y)*的梯度场来更新*x*和*y*的值，以找到使*f(x, y) = 0*的必要值。
- en: The Proof of Learning
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习的证明
- en: We will define a simple function *f(x)* for algorithm testing. Our intention
    is to find the minimum of this function. To do this, we can apply the gradient
    of *f(x)*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个简单的函数*f(x)*进行算法测试。我们的目标是找到这个函数的最小值。为此，我们可以应用*f(x)*的梯度。
- en: '![](../Images/2e5c69f07244b0fc525671c391f4667c.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e5c69f07244b0fc525671c391f4667c.png)'
- en: 'Source: The Author. Eq. (4)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（4）
- en: Above, we have the gradient of the function *f(x)*. We will not delve into defining
    the concept of derivative in this article, but we recommend [reading about its
    definition](https://www.britannica.com/science/derivative-mathematics) and why
    we can represent it in this way.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 上面是函数*f(x)*的梯度。我们在本文中不会深入定义导数的概念，但我们建议[阅读它的定义](https://www.britannica.com/science/derivative-mathematics)以及为什么我们可以这样表示它。
- en: 'Knowing that *h* tends to zero, we can represent the gradient of *f(x)* as
    follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 知道*h*趋向于零，我们可以如下表示*f(x)*的梯度：
- en: '![](../Images/aacc5bc9b490877922185777eb5704ee.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aacc5bc9b490877922185777eb5704ee.png)'
- en: 'Source: The Author. Eq. (5)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（5）
- en: 'Based on this, we can replace *h* with the following term:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，我们可以用以下项替代*h*：
- en: '![](../Images/38547a0b1525f66f6c154596eff08895.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38547a0b1525f66f6c154596eff08895.png)'
- en: 'Source: The Author. Eq. (6)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（6）
- en: 'We define the element *alpha* to maintain the necessity of the term *h*, where
    *alpha* must be strictly positive and always tend to zero, identical to the term
    *h*. Substituting the new relationship into the definition of derivative, we have:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义元素*alpha*来保持*h*项的必要性，其中*alpha*必须严格为正，并且始终趋向于零，和*h*项相同。将这一新的关系代入导数定义中，我们得到：
- en: '![](../Images/d2855f453f451dc2a29c66fe68e26d12.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2855f453f451dc2a29c66fe68e26d12.png)'
- en: 'Source: The Author. Eq. (7)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（7）
- en: Now we have a valuable relationship for our proof. We know that any element
    squared will be positive. From this concept comes the need to replace *h* with
    minus *alpha* times the *gradient of f(x)*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个对我们证明有价值的关系。我们知道任何元素的平方都会是正数。基于这一概念，我们需要用负 *alpha* 倍的 *f(x)* 的梯度来替换
    *h*。
- en: 'So:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所以：
- en: '![](../Images/eda81fa6ccc6d5f3476ae70aaf7b74fd.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eda81fa6ccc6d5f3476ae70aaf7b74fd.png)'
- en: 'Source: The Author. Eq. (8)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（8）
- en: Therefore, we can judge that the condition in (8) is true as long as *alpha*
    is always a positive value.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以判断，当 *alpha* 始终为正值时，条件（8）成立。
- en: '![](../Images/3252475d5891fd3c42050d356336ae5f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3252475d5891fd3c42050d356336ae5f.png)'
- en: 'Source: The Author. Eq. (9)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（9）
- en: 'That is, the value of *f(x)* being subtracted by a strictly positive value
    will always be less than the original value of *f(x)*. Therefore, we can replace
    it with the following relationship using eq. (7) and (9):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，*f(x)* 的值减去一个严格正的值将总是小于 *f(x)* 的原始值。因此，我们可以使用公式（7）和（9）替换为以下关系：
- en: '![](../Images/9196a08c8585b7dabac2f3681905d4cb.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9196a08c8585b7dabac2f3681905d4cb.png)'
- en: 'Source: The Author. Eq. (10)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（10）
- en: Therefore, we have a proven relationship on how to update the values of *x*
    so that the function *f(x)* is at least smaller than its previous state.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经证明了如何更新 *x* 的值，以确保函数 *f(x)* 至少小于其先前的值的关系。
- en: '![](../Images/38dc51d4143b3e80cc9a4c707e7278e4.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38dc51d4143b3e80cc9a4c707e7278e4.png)'
- en: 'Source: The Author. Eq. (11)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（11）
- en: 'So, we know how to decrease the current *x* to satisfy the inequality (11):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们知道如何减少当前的 *x* 以满足不等式（11）：
- en: '![](../Images/f860f57ff92a6314dc8800177c2adb56.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f860f57ff92a6314dc8800177c2adb56.png)'
- en: 'Source: The Author. Eq. (12)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（12）
- en: 'To confirm the validity of this relationship, we can apply this methodology
    to the function *f(x, y)* in img. (1) whose behavior we know. So:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认这个关系的有效性，我们可以将这种方法应用于图像（1）中的函数 *f(x, y)*，我们已知该函数的行为。因此：
- en: '![](../Images/d07ea17cab43755bdebf0265bf359b27.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d07ea17cab43755bdebf0265bf359b27.png)'
- en: 'Source: The Author. Eq. (13)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（13）
- en: Applying this algorithm to the function *f(x, y)* numerous times, we expect
    to see the value of the function decrease until it reaches the minimum. To do
    this, we conducted a simulation where, in addition, we applied noise to the assignment
    of the updated *x* and *y* to visualize the decrease in the value of *f(x, y)*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将此算法多次应用于函数 *f(x, y)*，我们预期看到函数值下降，直到达到最小值。为此，我们进行了模拟，并在其中对更新后的 *x* 和 *y* 赋值施加了噪声，以便可视化
    *f(x, y)* 的值的下降。
- en: '![](../Images/54af915ff42ea099c2429f8cca2a7043.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54af915ff42ea099c2429f8cca2a7043.png)'
- en: 'Source: The Author. Img. (3)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。图像（3）
- en: Notice that when the value of *alpha* tends to zero, we observe the values of
    *x* and *y* tending to the minimum of the function. When this is not true, for
    example, at *alpha = 0.6*, we observe a certain difficulty in finding the minimum
    of the function *f(x, y)*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当 *alpha* 的值趋近于零时，我们观察到 *x* 和 *y* 的值趋向于函数的最小值。当这一点不成立时，例如当 *alpha = 0.6*
    时，我们会发现找到 *f(x, y)* 的最小值有一定的难度。
- en: Gradient Descent
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降法
- en: This algorithm is known as “Gradient Descent” or “Method of Steepest Descent,”
    being an optimization method to find the minimum of a function where each step
    is taken in the direction of the negative gradient. This method does not guarantee
    that the global minimum of the function will be found, but rather a local minimum.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法被称为“梯度下降”或“最速下降法”，它是一种优化方法，用于寻找函数的最小值，每一步都朝着负梯度的方向进行。该方法并不保证找到函数的全局最小值，而是找到局部最小值。
- en: Discussions about finding the global minimum could be developed in another article,
    but here, we have mathematically demonstrated how the gradient can be used for
    this purpose.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 关于找到全局最小值的讨论可以在另一篇文章中展开，但在这里，我们已经从数学上证明了如何利用梯度来实现这一目的。
- en: 'Now, applying it to the cost function *E* that depends on the *n* weights *w*,
    we have:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将其应用于依赖于 *n* 个权重 *w* 的代价函数 *E*，我们得到：
- en: '![](../Images/efddc76046b63efd822e311b8e49e13e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efddc76046b63efd822e311b8e49e13e.png)'
- en: 'Source: The Author. Eq. (14)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（14）
- en: 'To update all elements of *W* based on gradient descent, we have:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要基于梯度下降法更新所有元素 *W*，我们有：
- en: '![](../Images/a714a82bbf8aa6a8fecdbcb25f225b76.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a714a82bbf8aa6a8fecdbcb25f225b76.png)'
- en: 'Source: The Author. Eq. (15)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（15）
- en: 'And for any *n*th element 𝑤 of the vector *W*, we have:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于向量 *W* 的任何 *n* 第n个元素 𝑤， 我们有：
- en: '![](../Images/3e9670c17c0ae010568107356bcca08d.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e9670c17c0ae010568107356bcca08d.png)'
- en: 'Source: The Author. Eq. (16)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者。公式（16）
- en: Therefore, we have our *theoretical learning algorithm*. Logically, this is
    not applied to the hypothetical idea of the cook, but rather to numerous machine
    learning algorithms that we know today.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有了我们的*理论学习算法*。从逻辑上讲，这并不是应用于假设中的厨师的思想，而是应用于我们今天所知道的众多机器学习算法。
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Based on what we have seen, we can conclude the demonstration and the mathematical
    proof of the theoretical learning algorithm. Such a structure is applied to numerous
    learning methods such as AdaGrad, Adam, and Stochastic Gradient Descent (SGD).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们所看到的内容，我们可以得出结论，理论学习算法的演示和数学证明。这样的结构被应用于许多学习方法，如AdaGrad、Adam和随机梯度下降（SGD）。
- en: This method does not guarantee finding the *n*-weight values *w* where the *cost
    function* yields a result of zero or very close to it. However, it assures us
    that a local minimum of the cost function will be found.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不能保证找到使*成本函数*的结果为零或接近零的*n*-权重值*w*，但它能保证找到成本函数的局部最小值。
- en: To address the issue of local minima, there are several more robust methods,
    such as SGD and Adam, which are commonly used in deep learning.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决局部最小值的问题，有几种更为稳健的方法，例如SGD和Adam，这些方法在深度学习中常被使用。
- en: Nevertheless, understanding the structure and the mathematical proof of the
    theoretical learning algorithm based on gradient descent will facilitate the comprehension
    of more complex algorithms.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，理解基于梯度下降的理论学习算法的结构和数学证明，将有助于理解更复杂的算法。
- en: References
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Carreira-Perpinan, M. A., & Hinton, G. E. (2005). On contrastive divergence
    learning. In R. G. Cowell & Z. Ghahramani (Eds.), Artificial Intelligence and
    Statistics, 2005\. (pp. 33–41). Fort Lauderdale, FL: Society for Artificial Intelligence
    and Statistics.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Carreira-Perpinan, M. A., & Hinton, G. E. (2005). 关于对比散度学习。在R. G. Cowell & Z.
    Ghahramani（编辑），人工智能与统计学，2005。（第33–41页）。佛罗里达州劳德代尔堡：人工智能与统计学学会。
- en: García Cabello, J. Mathematical Neural Networks. Axioms 2022, 11, 80.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: García Cabello, J. 数学神经网络。Axioms 2022, 11, 80。
- en: Geoffrey E. Hinton, Simon Osindero, Yee-Whye Teh. A Fast Learning Algorithm
    for Deep Belief Nets. Neural Computation 18, 1527–1554\. Massachusetts Institute
    of Technology
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Geoffrey E. Hinton, Simon Osindero, Yee-Whye Teh. A Fast Learning Algorithm
    for Deep Belief Nets. Neural Computation 18, 1527–1554. 麻省理工学院
- en: LeCun, Y., Bottou, L., & Haffner, P. (1998). Gradient-based learning applied
    to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: LeCun, Y., Bottou, L., & Haffner, P. (1998). 基于梯度的学习应用于文档识别。IEEE学报，86(11)，2278–2324。
