<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Aggregating Real-time Sensor Data with Python and Redpanda</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Aggregating Real-time Sensor Data with Python and Redpanda</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/aggregating-real-time-sensor-data-with-python-and-redpanda-30a139d59702?source=collection_archive---------0-----------------------#2024-05-20">https://towardsdatascience.com/aggregating-real-time-sensor-data-with-python-and-redpanda-30a139d59702?source=collection_archive---------0-----------------------#2024-05-20</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="d738" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Simple stream processing using Python and tumbling windows</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@tomasatquix?source=post_page---byline--30a139d59702--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Tomáš Neubauer" class="l ep by dd de cx" src="../Images/5eb14b73cfe100ef9a43148db6abd3a9.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*-sRvsiw7bufdDz12GQWOvQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--30a139d59702--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@tomasatquix?source=post_page---byline--30a139d59702--------------------------------" rel="noopener follow">Tomáš Neubauer</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--30a139d59702--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 20, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/d2873ac220cc6d905be3b31e86a270c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b1-iAik1A9FXQblosEVhyg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure><p id="f249" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this tutorial, I want to show you how to downsample a stream of sensor data using only Python (and Redpanda as a message broker). The goal is to show you how simple stream processing can be, and that you don’t need a heavy-duty stream processing framework to get started.</p><p id="201c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Until recently, stream processing was a complex task that usually required some Java expertise. But gradually, the Python stream processing ecosystem has matured and there are a few more options available to Python developers — such as <a class="af ny" href="https://faust-streaming.github.io/faust/introduction.html" rel="noopener ugc nofollow" target="_blank">Faust</a>, <a class="af ny" href="https://bytewax.io/" rel="noopener ugc nofollow" target="_blank">Bytewax</a> and <a class="af ny" href="https://quix.io/docs/quix-streams/quickstart.html" rel="noopener ugc nofollow" target="_blank">Quix</a>. Later, I’ll provide a bit more background on why these libraries have emerged to compete with the existing Java-centric options.</p><p id="d6b3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But first let’s get to the task at hand. We will use a Python libary called Quix Streams as our stream processor. Quix Streams is very similar to Faust, but it has been optimized to be more concise in its syntax and uses a Pandas like API called StreamingDataframes.</p><p id="4470" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can install the Quix Streams library with the following command:</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="e9fc" class="od oe fq oa b bg of og l oh oi">pip install quixstreams</span></pre><p id="1583" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">What you’ll build</strong></p><p id="826b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You’ll build a simple application that will calculate the rolling aggregations of temperature readings coming from various sensors. The temperature readings will come in at a relatively high frequency and this application will aggregate the readings and output them at a lower time resolution (every 10 seconds). You can think of this as a form of compression since we don’t want to work on data at an unnecessarily high resolution.</p><p id="209e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can access the complete code <a class="af ny" href="https://github.com/quixio/template-windowing-reduce.git" rel="noopener ugc nofollow" target="_blank">in this GitHub repository</a>.</p><p id="912f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This application includes code that generates synthetic sensor data, but in a real-world scenario this data could come from many kinds of sensors, such as sensors installed in a fleet of vehicles or a warehouse full of machines.</p><p id="86f3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here’s an illustration of the basic architecture:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk oj"><img src="../Images/c31694705376dcfea8141f0d9341bf62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DUlzlCjamnDY-OMqPj4GVA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Diagram by author</figcaption></figure><h1 id="456b" class="ok oe fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Components of a stream processing pipeline</h1><p id="54e9" class="pw-post-body-paragraph nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">The previous diagram reflects the main components of a stream processing pipeline: You have the sensors which are the <strong class="ne fr">data producers</strong>, Redpanda as the <strong class="ne fr">streaming data platform</strong>, and Quix as the <strong class="ne fr">stream processor</strong>.</p><p id="46ff" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Data producers</strong></p><p id="46f3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These are bits of code that are attached to systems that generate data such as firmware on ECUs (Engine Control Units), monitoring modules for cloud platforms, or web servers that log user activity. They take that raw data and send it to the streaming data platform in a format that that platform can understand.</p><p id="ac6b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Streaming data platform</strong></p><p id="4a21" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is where you put your streaming data. It plays more or less the same role as a database does for static data. But instead of tables, you use topics. Otherwise, it has similar features to a static database. You’ll want to manage who can consume and produce data, what schemas the data should adhere to. Unlike a database though, the data is constantly in flux, so it’s not designed to be queried. You’d usually use a stream processor to transform the data and put it somewhere else for data scientists to explore or sink the raw data into a queryable system optimized for streaming data such as RisingWave or Apache Pinot. However, for automated systems that are triggered by patterns in streaming data (such as recommendation engines), this isn’t an ideal solution. In this case, you definitely want to use a dedicated stream processor.</p><p id="d17b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Stream processors</strong></p><p id="65be" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These are engines that perform continuous operations on the data as it arrives. They could be compared to just regular old microservices that process data in any application back end, but there’s one big difference. For microservices, data arrives in drips like droplets of rain, and each “drip” is processed discreetly. Even if it “rains” heavily, it’s not too hard for the service to keep up with the “drops” without overflowing (think of a filtration system that filters out impurities in the water).</p><p id="93c2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For a stream processor, the data arrives as a continuous, wide gush of water. A filtration system would be quickly overwhelmed unless you change the design. I.e. break the stream up and route smaller streams to a battery of filtration systems. That’s kind of how stream processors work. They’re designed to be horizontally scaled and work in parallel as a battery. And they never stop, they process the data continuously, outputting the filtered data to the streaming data platform, which acts as a kind of reservoir for streaming data. To make things more complicated, stream processors often need to keep track of data that was received previously, such as in the windowing example you’ll try out here.</p><p id="e87b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that there are also “data consumers” and “data sinks” — systems that consume the processed data (such as front end applications and mobile apps) or store it for offline analysis (data warehouses like Snowflake or AWS Redshift). Since we won’t be covering those in this tutorial, I’ll skip over them for now.</p><h1 id="7585" class="ok oe fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Setting up a local streaming data cluster</h1><p id="863e" class="pw-post-body-paragraph nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">In this tutorial, I’ll show you how to use a local installation of Redpanda for managing your streaming data. I’ve chosen Redpanda because it’s very easy to run locally.</p><p id="8572" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You’ll use Docker compose to quickly spin up a cluster, including the Redpanda console, so make sure you have Docker installed first.</p><h1 id="76c5" class="ok oe fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Creating the streaming applications</h1><p id="5a44" class="pw-post-body-paragraph nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">First, you’ll create separate files to produce and process your streaming data. This makes it easier to manage the running processes independently. I.e. you can stop the producer without stopping the stream processor too. Here’s an overview of the two files that you’ll create:</p><ul class=""><li id="9955" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pk pl pm bk"><strong class="ne fr">The stream producer:</strong> <code class="cx pn po pp oa b">sensor_stream_producer.py<br/></code>Generates synthetic temperature data and produces (i.e. writes) that data to a “raw data” source topic in Redpanda. Just like the Faust example, it produces the data at a resolution of approximately 20 readings every 5 seconds, or around 4 readings a second.</li><li id="7b15" class="nc nd fq ne b go pq ng nh gr pr nj nk nl ps nn no np pt nr ns nt pu nv nw nx pk pl pm bk"><strong class="ne fr">The stream processor: </strong><code class="cx pn po pp oa b">sensor_stream_processor.py<br/></code>Consumes (reads) the raw temperature data from the “source” topic, performs a tumbling window calculation to decrease the resolution of the data. It calculates the average of the data received in 10-second windows so you get a reading for every 10 seconds. It then produces these aggregated readings to the <code class="cx pn po pp oa b">agg-temperatures</code> topic in Redpanda.</li></ul><p id="a94a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As you can see the stream processor does most of the heavy lifting and is the core of this tutorial. The stream producer is a stand-in for a proper data ingestion process. For example, in a production scenario, you might use something like this<a class="af ny" href="https://github.com/quixio/quix-samples/tree/main/python/sources/MQTT" rel="noopener ugc nofollow" target="_blank"> MQTT connector</a> to get data from your sensors and produce it to a topic.</p><ul class=""><li id="279e" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pk pl pm bk">For a tutorial, it’s simpler to simulate the data, so let’s get that set up first.</li></ul><h1 id="0a63" class="ok oe fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Creating the stream producer</h1><p id="e28d" class="pw-post-body-paragraph nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">You’ll start by creating a new file called <code class="cx pn po pp oa b">sensor_stream_producer.py</code> and define the main Quix application. (This example has been developed on Python 3.10, but different versions of Python 3 should work as well, as long as you are able to run pip install quixstreams.)</p><p id="1a0f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Create the file <code class="cx pn po pp oa b">sensor_stream_producer.py</code> and add all the required dependencies (including Quix Streams)</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="3528" class="od oe fq oa b bg of og l oh oi">from dataclasses import dataclass, asdict # used to define the data schema<br/>from datetime import datetime # used to manage timestamps<br/>from time import sleep # used to slow down the data generator<br/>import uuid # used for message id creation<br/>import json # used for serializing data<br/><br/>from quixstreams import Application</span></pre><p id="ac46" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then, define a Quix application and destination topic to send the data.</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="1700" class="od oe fq oa b bg of og l oh oi"><br/>app = Application(broker_address='localhost:19092')<br/><br/>destination_topic = app.topic(name='raw-temp-data', value_serializer="json")</span></pre><p id="93c8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <em class="pv">value_serializer</em> parameter defines the format of the expected source data (to be serialized into bytes). In this case, you’ll be sending JSON.</p><p id="c25e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s use the dataclass module to define a very basic schema for the temperature data and add a function to serialize it to JSON.</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="19af" class="od oe fq oa b bg of og l oh oi">@dataclass<br/>class Temperature:<br/>    ts: datetime<br/>    value: int<br/><br/>    def to_json(self):<br/>        # Convert the dataclass to a dictionary<br/>        data = asdict(self)<br/>        # Format the datetime object as a string<br/>        data['ts'] = self.ts.isoformat()<br/>        # Serialize the dictionary to a JSON string<br/>        return json.dumps(data)</span></pre><p id="afd1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, add the code that will be responsible for sending the mock temperature sensor data into our Redpanda source topic.</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="ef10" class="od oe fq oa b bg of og l oh oi">i = 0<br/>with app.get_producer() as producer:<br/>    while i &lt; 10000:<br/>        sensor_id = random.choice(["Sensor1", "Sensor2", "Sensor3", "Sensor4", "Sensor5"])<br/>       temperature = Temperature(datetime.now(), random.randint(0, 100))<br/>        value = temperature.to_json()<br/><br/>        print(f"Producing value {value}")<br/>        serialized = destination_topic.serialize(<br/>            key=sensor_id, value=value, headers={"uuid": str(uuid.uuid4())}<br/>        )<br/>        producer.produce(<br/>            topic=destination_topic.name,<br/>            headers=serialized.headers,<br/>            key=serialized.key,<br/>            value=serialized.value,<br/>        )<br/>        i += 1<br/>        sleep(random.randint(0, 1000) / 1000)</span></pre><p id="446f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This generates 1000 records separated by random time intervals between 0 and 1 second. It also randomly selects a sensor name from a list of 5 options.</p><p id="1f95" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now, try out the producer by running the following in the command line</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="238f" class="od oe fq oa b bg of og l oh oi">python sensor_stream_producer.py</span></pre><p id="fd9d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You should see data being logged to the console like this:</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="0c8e" class="od oe fq oa b bg of og l oh oi">[data produced]</span></pre><p id="a362" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once you’ve confirmed that it works, stop the process for now (you’ll run it alongside the stream processing process later).</p><h1 id="e154" class="ok oe fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Creating the stream processor</h1><p id="38fc" class="pw-post-body-paragraph nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">The stream processor performs three main tasks: 1) consume the raw temperature readings from the source topic, 2) continuously aggregate the data, and 3) produce the aggregated results to a sink topic.</p><p id="7219" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s add the code for each of these tasks. In your IDE, create a new file called <code class="cx pn po pp oa b">sensor_stream_processor.py</code>.</p><p id="33f8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">First, add the dependencies as before:</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="d892" class="od oe fq oa b bg of og l oh oi">import os<br/>import random<br/>import json<br/>from datetime import datetime, timedelta<br/>from dataclasses import dataclass<br/>import logging<br/>from quixstreams import Application<br/><br/>logging.basicConfig(level=logging.INFO)<br/>logger = logging.getLogger(__name__)</span></pre><p id="39d6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s also set some variables that our stream processing application needs:</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="6c2a" class="od oe fq oa b bg of og l oh oi">TOPIC = "raw-temperature" # defines the input topic<br/>SINK = "agg-temperature"  # defines the output topic<br/>WINDOW = 10  # defines the length of the time window in seconds<br/>WINDOW_EXPIRES = 1 # defines, in seconds, how late data can arrive before it is excluded from the window</span></pre><p id="e99c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We’ll go into more detail on what the window variables mean a bit later, but for now, let’s crack on with defining the main Quix application.</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="70de" class="od oe fq oa b bg of og l oh oi">app = Application(<br/>    broker_address='localhost:19092',<br/>    consumer_group="quix-stream-processor",<br/>    auto_offset_reset="earliest",<br/>)</span></pre><p id="3404" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that there are a few more application variables this time around, namely <code class="cx pn po pp oa b">consumer_group</code> and <code class="cx pn po pp oa b">auto_offset_reset</code>. To learn more about the interplay between these settings, check out the article “<a class="af ny" href="https://quix.io/blog/kafka-auto-offset-reset-use-cases-and-pitfalls" rel="noopener ugc nofollow" target="_blank">Understanding Kafka’s auto offset reset configuration: Use cases and pitfalls</a>“</p><p id="6a64" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, define the input and output topics on either side of the core stream processing function and add a function to put the incoming data into a DataFrame.</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="e8fe" class="od oe fq oa b bg of og l oh oi">input_topic = app.topic(TOPIC, value_deserializer="json")<br/>output_topic = app.topic(SINK, value_serializer="json")<br/><br/>sdf = app.dataframe(input_topic)<br/>sdf = sdf.update(lambda value: logger.info(f"Input value received: {value}"))</span></pre><p id="fc04" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We’ve also added a logging line to make sure the incoming data is intact.</p><p id="456d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, let’s add a custom timestamp extractor to use the timestamp from the message payload instead of Kafka timestamp. For your aggregations, this basically means that you want to use the time that the reading was generated rather than the time that it was received by Redpanda. Or in even simpler terms “Use the sensor’s definition of time rather than Redpanda’s”.</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="7440" class="od oe fq oa b bg of og l oh oi">def custom_ts_extractor(value):<br/> <br/>    # Extract the sensor's timestamp and convert to a datetime object<br/>    dt_obj = datetime.strptime(value["ts"], "%Y-%m-%dT%H:%M:%S.%f") # <br/><br/>    # Convert to milliseconds since the Unix epoch for efficent procesing with Quix<br/>    milliseconds = int(dt_obj.timestamp() * 1000)<br/>    value["timestamp"] = milliseconds<br/>    logger.info(f"Value of new timestamp is: {value['timestamp']}")<br/><br/>    return value["timestamp"]<br/><br/># Override the previously defined input_topic variable so that it uses the custom timestamp extractor <br/>input_topic = app.topic(TOPIC, timestamp_extractor=custom_ts_extractor, value_deserializer="json") </span></pre><p id="63b3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Why are we doing this? Well, we could get into a philosophical rabbit hole about which kind of time to use for processing, but that’s a subject for another article. With the custom timestamp, I just wanted to illustrate that there are many ways to interpret time in stream processing, and you don’t necessarily have to use the time of data arrival.</p><p id="d69f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, initialize the state for the aggregation when a new window starts. It will prime the aggregation when the first record arrives in the window.</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="273b" class="od oe fq oa b bg of og l oh oi">def initializer(value: dict) -&gt; dict:<br/><br/>    value_dict = json.loads(value)<br/>    return {<br/>        'count': 1,<br/>        'min': value_dict['value'],<br/>        'max': value_dict['value'],<br/>        'mean': value_dict['value'],<br/>    }</span></pre><p id="c927" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This sets the initial values for the window. In the case of min, max, and mean, they are all identical because you’re just taking the first sensor reading as the starting point.</p><p id="bc31" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now, let’s add the aggregation logic in the form of a “reducer” function.</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="ea5e" class="od oe fq oa b bg of og l oh oi">def reducer(aggregated: dict, value: dict) -&gt; dict:<br/>    aggcount = aggregated['count'] + 1<br/>    value_dict = json.loads(value)<br/>    return {<br/>        'count': aggcount,<br/>        'min': min(aggregated['min'], value_dict['value']),<br/>        'max': max(aggregated['max'], value_dict['value']),<br/>        'mean': (aggregated['mean'] * aggregated['count'] + value_dict['value']) / (aggregated['count'] + 1)<br/>    }</span></pre><p id="aa18" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This function is only necessary when you’re performing multiple aggregations on a window. In our case, we’re creating count, min, max, and mean values for each window, so we need to define these in advance.</p><p id="6880" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next up, the juicy part — adding the tumbling window functionality:</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="a887" class="od oe fq oa b bg of og l oh oi">### Define the window parameters such as type and length<br/>sdf = (<br/>    # Define a tumbling window of 10 seconds<br/>    sdf.tumbling_window(timedelta(seconds=WINDOW), grace_ms=timedelta(seconds=WINDOW_EXPIRES))<br/><br/>    # Create a "reduce" aggregation with "reducer" and "initializer" functions<br/>    .reduce(reducer=reducer, initializer=initializer)<br/><br/>    # Emit results only for closed 10 second windows<br/>    .final()<br/>)<br/><br/>### Apply the window to the Streaming DataFrame and define the data points to include in the output<br/>sdf = sdf.apply(<br/>    lambda value: {<br/>        "time": value["end"], # Use the window end time as the timestamp for message sent to the 'agg-temperature' topic<br/>        "temperature": value["value"], # Send a dictionary of {count, min, max, mean} values for the temperature parameter<br/>    }<br/>)</span></pre><p id="22eb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This defines the Streaming DataFrame as a set of aggregations based on a tumbling window — a set of aggregations performed on 10-second non-overlapping segments of time.</p><p id="1605" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Tip</strong>: If you need a refresher on the different types of windowed calculations, check out this article:<a class="af ny" href="https://quix.io/blog/windowing-stream-processing-guide" rel="noopener ugc nofollow" target="_blank"> “A guide to windowing in stream processing</a>”.</p><p id="95c8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Finally, produce the results to the downstream output topic:</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="650f" class="od oe fq oa b bg of og l oh oi">sdf = sdf.to_topic(output_topic)<br/>sdf = sdf.update(lambda value: logger.info(f"Produced value: {value}"))<br/><br/>if __name__ == "__main__":<br/>    logger.info("Starting application")<br/>    app.run(sdf)</span></pre><p id="736f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Note</strong>: You might wonder why the producer code looks very different to the producer code used to send the synthetic temperature data (the part that uses <code class="cx pn po pp oa b">with app.get_producer() as producer()</code>). This is because Quix uses a different producer function for transformation tasks (i.e. a task that sits between input and output topics).</p><p id="aed0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As you might notice when following along, we iteratively change the Streaming DataFrame (the <code class="cx pn po pp oa b">sdf</code> variable) until it is the final form that we want to send downstream. Thus, the <code class="cx pn po pp oa b">sdf.to_topic</code> function simply streams the final state of the Streaming DataFrame back to the output topic, row-by-row.</p><p id="d0a8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <code class="cx pn po pp oa b">producer</code> function on the other hand, is used to ingest data from an external source such as a CSV file, an MQTT broker, or in our case, a generator function.</p><h1 id="34d5" class="ok oe fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Run the streaming applications</h1><p id="6437" class="pw-post-body-paragraph nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">Finally, you get to run our streaming applications and see if all the moving parts work in harmony.</p><p id="27a1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">First, in a terminal window, start the producer again:</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="f53e" class="od oe fq oa b bg of og l oh oi">python sensor_stream_producer.py</span></pre><p id="25c2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then, in a second terminal window, start the stream processor:</p><pre class="mm mn mo mp mq nz oa ob bp oc bb bk"><span id="8789" class="od oe fq oa b bg of og l oh oi">python sensor_stream_processor.py</span></pre><p id="2b51" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Pay attention to the log output in each window, to make sure everything is running smoothly.</p><p id="669e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can also check the Redpanda console to make sure that the aggregated data is being streamed to the sink topic correctly (you’ll fine the topic browser at: <a class="af ny" href="http://localhost:8080/topics" rel="noopener ugc nofollow" target="_blank">http://localhost:8080/topics</a>).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/46c639a04de0e311f01229b2061fdb4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*et0P_adOkstvfAQL"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Screenshot by author</figcaption></figure><h1 id="5487" class="ok oe fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Wrapping up</h1><p id="67f4" class="pw-post-body-paragraph nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">What you’ve tried out here is just one way to do stream processing. Naturally, there are heavy duty tools such Apache Flink and Apache Spark Streaming which are have also been covered extensively online. But — those are predominantly Java-based tools. Sure, you can use their Python wrappers, but when things go wrong, you’ll still be debugging Java errors rather than Python errors. And Java skills aren’t exactly ubiquitous among data folks who are increasingly working alongside software engineers to tune stream processing algorithms.</p><p id="c2bd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this tutorial, we ran a simple aggregation as our stream processing algorithm, but in reality, these algorithms often employ machine learning models to transform that data — and the software ecosystem for machine learning is heavily dominated by Python.</p><p id="ce6a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">An oft overlooked fact is that Python is the lingua franca for data specialists, ML engineers, and software engineers to work together. It’s even better than SQL because you can use it to do non-data-related things like make API calls and trigger webhooks. That’s one of the reasons why libraries like Faust, Bytewax and Quix evolved — to bridge the so-called <a class="af ny" href="https://quix.io/blog/bridging-the-impedance-gap" rel="noopener ugc nofollow" target="_blank">impedance gap</a> between these different disciplines.</p><p id="99af" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Hopefully, I’ve managed to show you that Python is a viable language for stream processing, and that the Python ecosystem for stream processing is maturing at a steady rate and can hold its own against the older Java-based ecosystem.</p><ul class=""><li id="c074" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pk pl pm bk">As a reminder, all the code for this tutorial is available in <a class="af ny" href="https://github.com/quixio/template-windowing-reduce" rel="noopener ugc nofollow" target="_blank">this GitHub repository</a>.</li></ul></div></div></div></div>    
</body>
</html>