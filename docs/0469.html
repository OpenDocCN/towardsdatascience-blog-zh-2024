<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Advanced Retrieval-Augmented Generation: From Theory to LlamaIndex Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Advanced Retrieval-Augmented Generation: From Theory to LlamaIndex Implementation</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930?source=collection_archive---------0-----------------------#2024-02-19">https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930?source=collection_archive---------0-----------------------#2024-02-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1ccf" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How to address limitations of naive RAG pipelines by implementing targeted advanced RAG techniques in Python</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@iamleonie?source=post_page---byline--4de1464a9930--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Leonie Monigatti" class="l ep by dd de cx" src="../Images/4044b1685ada53a30160b03dc78f9626.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*TTIl4oynrJyfIkLbC6fumA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4de1464a9930--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@iamleonie?source=post_page---byline--4de1464a9930--------------------------------" rel="noopener follow">Leonie Monigatti</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4de1464a9930--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">13</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/de34a1aedcd7cbeadae031e9f75a3fc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8z-QRadKewNmos0J_4TNAQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Difference between Naive and Advanced RAG (Image by the author, inspired by [1])</figcaption></figure><p id="96fe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk ny"><span class="l nz oa ob bo oc od oe of og ed">A</span> recent survey on <a class="af oh" href="https://medium.com/towards-data-science/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2" rel="noopener">Retrieval-Augmented Generation (RAG)</a> [1] summarized three recently evolved paradigms:</p><ul class=""><li id="a4e6" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oi oj ok bk">Naive RAG,</li><li id="47de" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk">advanced RAG, and</li><li id="375d" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk">modular RAG.</li></ul><p id="f5cb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The advanced RAG paradigm comprises of a set of techniques targeted at addressing known limitations of naive RAG. This article first discusses these techniques, which can be categorized into <em class="oq">pre-retrieval, retrieval, and post-retrieval optimizations</em>.</p><p id="ec86" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the second half, you will learn how to implement a naive RAG pipeline using <a class="af oh" href="https://www.llamaindex.ai/" rel="noopener ugc nofollow" target="_blank">Llamaindex</a> in Python, which will then be enhanced to an advanced RAG pipeline with a selection of the following advanced RAG techniques:</p><ul class=""><li id="44f6" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oi oj ok bk"><a class="af oh" href="#c968" rel="noopener ugc nofollow">Pre-retrieval optimization: Sentence window retrieval</a></li><li id="f6f8" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk"><a class="af oh" href="#3275" rel="noopener ugc nofollow">Retrieval optimization: Hybrid search</a></li><li id="1483" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk"><a class="af oh" href="#c1e2" rel="noopener ugc nofollow">Post-retrieval optimization: Re-ranking</a></li></ul></div></div></div><div class="ab cb or os ot ou" role="separator"><span class="ov by bm ow ox oy"/><span class="ov by bm ow ox oy"/><span class="ov by bm ow ox"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="d4a8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This article focuses on the <strong class="ne fr">advanced RAG paradigm</strong> and its implementation. If you are unfamiliar with the fundamentals of RAG, you can catch up on it here:</p><div class="oz pa pb pc pd pe"><a rel="noopener follow" target="_blank" href="/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=post_page-----4de1464a9930--------------------------------"><div class="pf ab ig"><div class="pg ab co cb ph pi"><h2 class="bf fr hw z io pj iq ir pk it iv fp bk">Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation</h2><div class="pl l"><h3 class="bf b hw z io pj iq ir pk it iv dx">From the theory of the original academic paper to its Python implementation with OpenAI, Weaviate, and LangChain</h3></div><div class="pm l"><p class="bf b dy z io pj iq ir pk it iv dx">towardsdatascience.com</p></div></div><div class="pn l"><div class="po l pp pq pr pn ps lr pe"/></div></div></a></div><h1 id="4ab7" class="pt pu fq bf pv pw px gq py pz qa gt qb qc qd qe qf qg qh qi qj qk ql qm qn qo bk">What is Advanced RAG</h1><p id="5d77" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">With the recent advancements in the RAG domain, advanced RAG has evolved as a new paradigm with targeted enhancements to address some of the limitations of the naive RAG paradigm. As summarized in a recent survey [1], advanced RAG techniques can be categorized into pre-retrieval, retrieval, and post-retrieval optimizations.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/4448aad435386e145364d6e85076d755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6bTQeYn9814yAgLor2EFrQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Difference between Naive and Advanced RAG (Image by the author, inspired by [1])</figcaption></figure><h1 id="6c31" class="pt pu fq bf pv pw px gq py pz qa gt qb qc qd qe qf qg qh qi qj qk ql qm qn qo bk">Pre-retrieval optimization</h1><p id="ddee" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">Pre-retrieval optimizations focus on data indexing optimizations as well as query optimizations. Data indexing optimization techniques aim to store the data in a way that helps you improve retrieval efficiency, such as [1]:</p><ul class=""><li id="734d" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oi oj ok bk"><strong class="ne fr">Sliding window</strong> uses an overlap between chunks and is one of the simplest techniques.</li><li id="bb08" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk"><strong class="ne fr">Enhancing data granularity</strong> applies data cleaning techniques, such as removing irrelevant information, confirming factual accuracy, updating outdated information, etc.</li><li id="b55f" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk"><strong class="ne fr">Adding metadata</strong>, such as dates, purposes, or chapters, for filtering purposes.</li><li id="97e7" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk"><strong class="ne fr">Optimizing index structures</strong> involves different strategies to index data, such as adjusting the chunk sizes or using multi-indexing strategies. One technique we will implement in this article is sentence window retrieval, which embeds single sentences for retrieval and replaces them with a larger text window at inference time.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/42d3ba853f8b1794f5f0eadd32d9b55e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pbU5KBqYWhx0GOeNRMiPoA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Sentence window retrieval</figcaption></figure><p id="d8eb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Additionally, pre-retrieval techniques aren’t limited to data indexing and can cover <strong class="ne fr">techniques at inference time</strong>, such as query routing, query rewriting, and query expansion.</p><h1 id="d84c" class="pt pu fq bf pv pw px gq py pz qa gt qb qc qd qe qf qg qh qi qj qk ql qm qn qo bk">Retrieval optimization</h1><p id="d995" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">The retrieval stage aims to identify the most relevant context. Usually, the retrieval is based on vector search, which calculates the semantic similarity between the query and the indexed data. Thus, the majority of retrieval optimization techniques revolve around the embedding models [1]:</p><ul class=""><li id="4e2b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oi oj ok bk"><strong class="ne fr">Fine-tuning embedding models</strong> customizes embedding models to domain-specific contexts, especially for domains with evolving or rare terms. For example, <code class="cx qv qw qx qy b">BAAI/bge-small-en </code>is a high-performance embedding model that can be fine-tuned (see<a class="af oh" href="https://betterprogramming.pub/fine-tuning-your-embedding-model-to-maximize-relevance-retrieval-in-rag-pipeline-2ea3fa231149" rel="noopener ugc nofollow" target="_blank"> Fine-tuning guide</a>)</li><li id="f45a" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk"><strong class="ne fr">Dynamic Embedding</strong> adapts to the context in which words are used, unlike static embedding, which uses a single vector for each word. For example, OpenAI’s <code class="cx qv qw qx qy b">embeddings-ada-02</code> is a sophisticated dynamic embedding model that captures contextual understanding. [1]</li></ul><p id="6c45" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are also other retrieval techniques besides vector search, such as hybrid search, which often refers to the concept of combining vector search with keyword-based search. This retrieval technique is beneficial if your retrieval requires exact keyword matches.</p><div class="oz pa pb pc pd pe"><a rel="noopener follow" target="_blank" href="/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5?source=post_page-----4de1464a9930--------------------------------"><div class="pf ab ig"><div class="pg ab co cb ph pi"><h2 class="bf fr hw z io pj iq ir pk it iv fp bk">Improving Retrieval Performance in RAG Pipelines with Hybrid Search</h2><div class="pl l"><h3 class="bf b hw z io pj iq ir pk it iv dx">How to find more relevant search results by combining traditional keyword-based search with modern vector search</h3></div><div class="pm l"><p class="bf b dy z io pj iq ir pk it iv dx">towardsdatascience.com</p></div></div><div class="pn l"><div class="qz l pp pq pr pn ps lr pe"/></div></div></a></div><h1 id="53fb" class="pt pu fq bf pv pw px gq py pz qa gt qb qc qd qe qf qg qh qi qj qk ql qm qn qo bk">Post-retrieval optimization</h1><p id="abbe" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">Additional processing of the retrieved context can help address issues such as exceeding the context window limit or introducing noise, thus hindering the focus on crucial information. Post-retrieval optimization techniques summarized in the RAG survey [1] are:</p><ul class=""><li id="7043" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oi oj ok bk"><strong class="ne fr">Prompt compression</strong> reduces the overall prompt length by removing irrelevant and highlighting important context.</li><li id="cf63" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk"><strong class="ne fr">Re-ranking</strong> uses machine learning models to recalculate the relevance scores of the retrieved contexts.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/b53b6b4dead55ba22f8b77f052e09a30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*owudWLuoXhqeLjStCsnDiw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Re-ranking</figcaption></figure></div></div></div><div class="ab cb or os ot ou" role="separator"><span class="ov by bm ow ox oy"/><span class="ov by bm ow ox oy"/><span class="ov by bm ow ox"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="24fd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For additional ideas on how to improve the performance of your RAG pipeline to make it production-ready, continue reading here:</p><div class="oz pa pb pc pd pe"><a rel="noopener follow" target="_blank" href="/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439?source=post_page-----4de1464a9930--------------------------------"><div class="pf ab ig"><div class="pg ab co cb ph pi"><h2 class="bf fr hw z io pj iq ir pk it iv fp bk">A Guide on 12 Tuning Strategies for Production-Ready RAG Applications</h2><div class="pl l"><h3 class="bf b hw z io pj iq ir pk it iv dx">How to improve the performance of your Retrieval-Augmented Generation (RAG) pipeline with these “hyperparameters” and…</h3></div><div class="pm l"><p class="bf b dy z io pj iq ir pk it iv dx">towardsdatascience.com</p></div></div><div class="pn l"><div class="ra l pp pq pr pn ps lr pe"/></div></div></a></div><h1 id="22a9" class="pt pu fq bf pv pw px gq py pz qa gt qb qc qd qe qf qg qh qi qj qk ql qm qn qo bk">Prerequisites</h1><p id="3b0b" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">This section discusses the required packages and API keys to follow along in this article.</p><h2 id="01e6" class="rb pu fq bf pv rc rd re py rf rg rh qb nl ri rj rk np rl rm rn nt ro rp rq rr bk">Required Packages</h2><p id="9cd1" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">This article will guide you through implementing a naive and an advanced RAG pipeline using <a class="af oh" href="https://www.llamaindex.ai/" rel="noopener ugc nofollow" target="_blank">LlamaIndex</a> in Python.</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="cf38" class="rv pu fq qy b bg rw rx l ry rz">pip install llama-index</span></pre><p id="54c6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this article, we will be using <a class="af oh" href="https://blog.llamaindex.ai/llamaindex-v0-10-838e735948f8" rel="noopener ugc nofollow" target="_blank">LlamaIndex </a><code class="cx qv qw qx qy b"><a class="af oh" href="https://blog.llamaindex.ai/llamaindex-v0-10-838e735948f8" rel="noopener ugc nofollow" target="_blank">v0.10</a></code>. If you are upgrading from an older LlamaIndex version, you need to run the following commands to install and run LlamaIndex properly:</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="1b24" class="rv pu fq qy b bg rw rx l ry rz">pip uninstall llama-index<br/>pip install llama-index --upgrade --no-cache-dir --force-reinstall</span></pre><p id="0d6d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">LlamaIndex offers an option to store vector embeddings locally in JSON files for persistent storage, which is great for quickly prototyping an idea. However, we will use a vector database for persistent storage since advanced RAG techniques aim for production-ready applications.</p><p id="49a2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since we will need metadata storage and hybrid search capabilities in addition to storing the vector embeddings, we will use the open source vector database <a class="af oh" href="http://weaviate.io" rel="noopener ugc nofollow" target="_blank">Weaviate</a> (<code class="cx qv qw qx qy b">v3.26.2</code>), which supports these features.</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="2d59" class="rv pu fq qy b bg rw rx l ry rz">pip install weaviate-client llama-index-vector-stores-weaviate</span></pre><h2 id="5d3d" class="rb pu fq bf pv rc rd re py rf rg rh qb nl ri rj rk np rl rm rn nt ro rp rq rr bk">API Keys</h2><p id="b107" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">We will be using Weaviate embedded, which you can use for free without registering for an API key. However, this tutorial uses an embedding model and LLM from <a class="af oh" href="https://openai.com/" rel="noopener ugc nofollow" target="_blank">OpenAI</a>, for which you will need an OpenAI API key. To obtain one, you need an OpenAI account and then “Create new secret key” under <a class="af oh" href="https://platform.openai.com/account/api-keys" rel="noopener ugc nofollow" target="_blank">API keys</a>.</p><p id="6f7e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, create a local <code class="cx qv qw qx qy b">.env</code> file in your root directory and define your API keys in it:</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="28ad" class="rv pu fq qy b bg rw rx l ry rz">OPENAI_API_KEY="&lt;YOUR_OPENAI_API_KEY&gt;"</span></pre><p id="d107" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Afterwards, you can load your API keys with the following code:</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="e929" class="rv pu fq qy b bg rw rx l ry rz"># !pip install python-dotenv<br/>import os<br/>from dotenv import load_dotenv,find_dotenv<br/><br/>load_dotenv(find_dotenv())</span></pre><h1 id="3eb4" class="pt pu fq bf pv pw px gq py pz qa gt qb qc qd qe qf qg qh qi qj qk ql qm qn qo bk">Implementing Naive RAG with LlamaIndex</h1><p id="934a" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">This section discusses how to implement a naive RAG pipeline using LlamaIndex. You can find the entire naive RAG pipeline in this <a class="af oh" href="https://github.com/weaviate/recipes/blob/main/integrations/llamaindex/retrieval-augmented-generation/naive_rag.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter Notebook</a>. For the implementation using LangChain, you can continue in <a class="af oh" href="https://medium.com/towards-data-science/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2" rel="noopener">this article (naive RAG pipeline using LangChain</a>).</p><h2 id="31c5" class="rb pu fq bf pv rc rd re py rf rg rh qb nl ri rj rk np rl rm rn nt ro rp rq rr bk">Step 1: Define the embedding model and LLM</h2><p id="c2fb" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">First, you can define an embedding model and LLM in a global settings object. Doing this means you don’t have to specify the models explicitly in the code again.</p><ul class=""><li id="bd7a" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oi oj ok bk">Embedding model: used to generate vector embeddings for the document chunks and the query.</li><li id="b97b" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk">LLM: used to generate an answer based on the user query and the relevant context.</li></ul><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="f78c" class="rv pu fq qy b bg rw rx l ry rz">from llama_index.embeddings.openai import OpenAIEmbedding<br/>from llama_index.llms.openai import OpenAI<br/>from llama_index.core.settings import Settings<br/><br/>Settings.llm = OpenAI(model="gpt-3.5-turbo", temperature=0.1)<br/>Settings.embed_model = OpenAIEmbedding()</span></pre><h2 id="865e" class="rb pu fq bf pv rc rd re py rf rg rh qb nl ri rj rk np rl rm rn nt ro rp rq rr bk">Step 2: Load data</h2><p id="8579" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">Next, you will create a local directory named <code class="cx qv qw qx qy b">data</code> in your root directory and download some example data from the <a class="af oh" href="https://github.com/run-llama/llama_index" rel="noopener ugc nofollow" target="_blank">LlamaIndex GitHub repository</a> (MIT license).</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="5892" class="rv pu fq qy b bg rw rx l ry rz">!mkdir -p 'data'<br/>!wget '&lt;https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt&gt;' -O 'data/paul_graham_essay.txt'</span></pre><p id="4e52" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Afterward, you can load the data for further processing:</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="9d88" class="rv pu fq qy b bg rw rx l ry rz">from llama_index.core import SimpleDirectoryReader<br/><br/># Load data<br/>documents = SimpleDirectoryReader(<br/>        input_files=["./data/paul_graham_essay.txt"]<br/>).load_data()</span></pre><h2 id="9119" class="rb pu fq bf pv rc rd re py rf rg rh qb nl ri rj rk np rl rm rn nt ro rp rq rr bk">Step 3: Chunk documents into nodes</h2><p id="f837" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">As the entire document is too large to fit into the context window of the LLM, you will need to partition it into smaller text chunks, which are called <code class="cx qv qw qx qy b">Nodes</code> in LlamaIndex. You can parse the loaded documents into nodes using the <code class="cx qv qw qx qy b">SimpleNodeParser</code> with a defined chunk size of 1024.</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="84c8" class="rv pu fq qy b bg rw rx l ry rz">from llama_index.core.node_parser import SimpleNodeParser<br/><br/>node_parser = SimpleNodeParser.from_defaults(chunk_size=1024)<br/><br/># Extract nodes from documents<br/>nodes = node_parser.get_nodes_from_documents(documents)</span></pre><h2 id="aa07" class="rb pu fq bf pv rc rd re py rf rg rh qb nl ri rj rk np rl rm rn nt ro rp rq rr bk">Step 4: Build index</h2><p id="99eb" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">Next, you will build the index that stores all the external knowledge in <a class="af oh" href="https://weaviate.io/" rel="noopener ugc nofollow" target="_blank">Weaviate</a>, an open source vector database.</p><p id="f6be" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">First, you will need to connect to a Weaviate instance. In this case, we’re using <a class="af oh" href="https://weaviate.io/developers/weaviate/installation/embedded" rel="noopener ugc nofollow" target="_blank">Weaviate Embedded</a>, which allows you to experiment in Notebooks for free without an API key. For a production-ready solution, deploying Weaviate yourself, e.g., <a class="af oh" href="https://weaviate.io/developers/weaviate/installation/docker-compose" rel="noopener ugc nofollow" target="_blank">via Docker</a> or utilizing a <a class="af oh" href="https://weaviate.io/developers/weaviate/installation/weaviate-cloud-services" rel="noopener ugc nofollow" target="_blank">managed service</a>, is recommended.</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="d8b4" class="rv pu fq qy b bg rw rx l ry rz">import weaviate<br/><br/># Connect to your Weaviate instance<br/>client = weaviate.Client(<br/>    embedded_options=weaviate.embedded.EmbeddedOptions(), <br/>)</span></pre><p id="2d9d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, you will build a <code class="cx qv qw qx qy b">VectorStoreIndex</code> from the Weaviate client to store your data in and interact with.</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="bfe4" class="rv pu fq qy b bg rw rx l ry rz">from llama_index.core import VectorStoreIndex, StorageContext<br/>from llama_index.vector_stores.weaviate import WeaviateVectorStore<br/><br/>index_name = "MyExternalContext"<br/><br/># Construct vector store<br/>vector_store = WeaviateVectorStore(<br/>    weaviate_client = client, <br/>    index_name = index_name<br/>)<br/><br/># Set up the storage for the embeddings<br/>storage_context = StorageContext.from_defaults(vector_store=vector_store)<br/><br/># Setup the index<br/># build VectorStoreIndex that takes care of chunking documents<br/># and encoding chunks to embeddings for future retrieval<br/>index = VectorStoreIndex(<br/>    nodes,<br/>    storage_context = storage_context,<br/>)</span></pre><h2 id="73f0" class="rb pu fq bf pv rc rd re py rf rg rh qb nl ri rj rk np rl rm rn nt ro rp rq rr bk">Step 5: Setup query engine</h2><p id="2454" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">Lastly, you will set up the index as the query engine.</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="76cd" class="rv pu fq qy b bg rw rx l ry rz"># The QueryEngine class is equipped with the generator<br/># and facilitates the retrieval and generation steps<br/>query_engine = index.as_query_engine()</span></pre><h2 id="fe7d" class="rb pu fq bf pv rc rd re py rf rg rh qb nl ri rj rk np rl rm rn nt ro rp rq rr bk">Step 6: Run a naive RAG query on your data</h2><p id="4107" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">Now, you can run a naive RAG query on your data, as shown below:</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="ab02" class="rv pu fq qy b bg rw rx l ry rz"># Run your naive RAG query<br/>response = query_engine.query(<br/>    "What happened at Interleaf?"<br/>)</span></pre><h1 id="d495" class="pt pu fq bf pv pw px gq py pz qa gt qb qc qd qe qf qg qh qi qj qk ql qm qn qo bk">Implementing Advanced RAG with LlamaIndex</h1><p id="57e6" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">In this section, we will cover some simple adjustments you can make to turn the above naive RAG pipeline into an advanced one. This walkthrough will cover the following selection of advanced RAG techniques:</p><ul class=""><li id="a718" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oi oj ok bk"><a class="af oh" href="#c968" rel="noopener ugc nofollow">Pre-retrieval optimization: Sentence window retrieval</a></li><li id="e8c4" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk"><a class="af oh" href="#3275" rel="noopener ugc nofollow">Retrieval optimization: Hybrid search</a></li><li id="9721" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk"><a class="af oh" href="#c1e2" rel="noopener ugc nofollow">Post-retrieval optimization: Re-ranking</a></li></ul><p id="9021" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As we will only cover the modifications here, you can find the <a class="af oh" href="https://github.com/weaviate/recipes/blob/main/integrations/llamaindex/retrieval-augmented-generation/advanced_rag.ipynb" rel="noopener ugc nofollow" target="_blank">full end-to-end advanced RAG pipeline in this Jupyter Notebook</a>.</p><h1 id="c968" class="pt pu fq bf pv pw px gq py pz qa gt qb qc qd qe qf qg qh qi qj qk ql qm qn qo bk">Indexing optimization example: Sentence window retrieval</h1><p id="98f9" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">For the <a class="af oh" href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo.html" rel="noopener ugc nofollow" target="_blank">sentence window retrieval technique</a>, you need to make two adjustments: First, you must adjust how you store and post-process your data. Instead of the <code class="cx qv qw qx qy b">SimpleNodeParser</code>, we will use the <code class="cx qv qw qx qy b">SentenceWindowNodeParser</code>.</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="c58b" class="rv pu fq qy b bg rw rx l ry rz">from llama_index.core.node_parser import SentenceWindowNodeParser<br/><br/># create the sentence window node parser w/ default settings<br/>node_parser = SentenceWindowNodeParser.from_defaults(<br/>    window_size=3,<br/>    window_metadata_key="window",<br/>    original_text_metadata_key="original_text",<br/>)</span></pre><p id="3bc4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <code class="cx qv qw qx qy b">SentenceWindowNodeParser</code> does two things:</p><ol class=""><li id="dccf" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx sa oj ok bk">It separates the document into single sentences, which will be embedded.</li><li id="b9d1" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx sa oj ok bk">For each sentence, it creates a context window. If you specify a <code class="cx qv qw qx qy b">window_size = 3</code>, the resulting window will be three sentences long, starting at the previous sentence of the embedded sentence and spanning the sentence after. The window will be stored as metadata.</li></ol><p id="7935" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">During retrieval, the sentence that most closely matches the query is returned. After retrieval, you need to replace the sentence with the entire window from the metadata by defining a <code class="cx qv qw qx qy b">MetadataReplacementPostProcessor</code> and using it in the list of <code class="cx qv qw qx qy b">node_postprocessors</code>.</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="4fdb" class="rv pu fq qy b bg rw rx l ry rz">from llama_index.core.postprocessor import MetadataReplacementPostProcessor<br/><br/># The target key defaults to `window` to match the node_parser's default<br/>postproc = MetadataReplacementPostProcessor(<br/>    target_metadata_key="window"<br/>)<br/><br/>...<br/><br/>query_engine = index.as_query_engine( <br/>    node_postprocessors = [postproc],<br/>)</span></pre><h1 id="3275" class="pt pu fq bf pv pw px gq py pz qa gt qb qc qd qe qf qg qh qi qj qk ql qm qn qo bk">Retrieval optimization example: Hybrid search</h1><p id="dbe5" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">Implementing a hybrid search in LlamaIndex is as easy as two parameter changes to the <code class="cx qv qw qx qy b">query_engine</code> if the underlying vector database supports hybrid search queries. The <code class="cx qv qw qx qy b">alpha</code> parameter specifies the weighting between vector search and keyword-based search, where <code class="cx qv qw qx qy b">alpha=0</code> means keyword-based search and <code class="cx qv qw qx qy b">alpha=1</code> means pure vector search.</p><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="adaa" class="rv pu fq qy b bg rw rx l ry rz">query_engine = index.as_query_engine(<br/>    ...,<br/>    vector_store_query_mode="hybrid", <br/>    alpha=0.5,<br/>    ...<br/>)</span></pre><h1 id="c1e2" class="pt pu fq bf pv pw px gq py pz qa gt qb qc qd qe qf qg qh qi qj qk ql qm qn qo bk">Post-retrieval optimization example: Re-ranking</h1><p id="58ed" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">Adding a reranker to your advanced RAG pipeline only takes three simple steps:</p><ol class=""><li id="47f5" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx sa oj ok bk">First, define a reranker model. Here, we are using the <code class="cx qv qw qx qy b"><a class="af oh" href="https://huggingface.co/BAAI/bge-reranker-base" rel="noopener ugc nofollow" target="_blank">BAAI/bge-reranker-base</a></code><a class="af oh" href="https://huggingface.co/BAAI/bge-reranker-base" rel="noopener ugc nofollow" target="_blank">from Hugging Face</a>.</li><li id="c291" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx sa oj ok bk">In the query engine, add the reranker model to the list of <code class="cx qv qw qx qy b">node_postprocessors</code>.</li><li id="726c" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx sa oj ok bk">Increase the <code class="cx qv qw qx qy b">similarity_top_k</code> in the query engine to retrieve more context passages, which can be reduced to <code class="cx qv qw qx qy b">top_n</code> after reranking.</li></ol><pre class="mm mn mo mp mq rs qy rt bp ru bb bk"><span id="da08" class="rv pu fq qy b bg rw rx l ry rz"># !pip install torch sentence-transformers<br/>from llama_index.core.postprocessor import SentenceTransformerRerank<br/><br/># Define reranker model<br/>rerank = SentenceTransformerRerank(<br/>    top_n = 2, <br/>    model = "BAAI/bge-reranker-base"<br/>)<br/><br/>...<br/><br/># Add reranker to query engine<br/>query_engine = index.as_query_engine(<br/>		similarity_top_k = 6,<br/>		...,<br/>                node_postprocessors = [rerank],<br/>		...,<br/>)</span></pre></div></div></div><div class="ab cb or os ot ou" role="separator"><span class="ov by bm ow ox oy"/><span class="ov by bm ow ox oy"/><span class="ov by bm ow ox"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="17c1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are many more different techniques within the advanced RAG paradigm. If you are interested in further implementations, I recommend the following two resources:</p><div class="oz pa pb pc pd pe"><a href="https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?source=post_page-----4de1464a9930--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pf ab ig"><div class="pg ab co cb ph pi"><h2 class="bf fr hw z io pj iq ir pk it iv fp bk">Building and Evaluating Advanced RAG Applications</h2><div class="pl l"><h3 class="bf b hw z io pj iq ir pk it iv dx">Learn methods like sentence-window retrieval and auto-merging retrieval, improving your RAG pipeline's performance…</h3></div><div class="pm l"><p class="bf b dy z io pj iq ir pk it iv dx">www.deeplearning.ai</p></div></div><div class="pn l"><div class="sb l pp pq pr pn ps lr pe"/></div></div></a></div><div class="oz pa pb pc pd pe"><a rel="noopener follow" target="_blank" href="/advanced-rag-01-small-to-big-retrieval-172181b396d4?source=post_page-----4de1464a9930--------------------------------"><div class="pf ab ig"><div class="pg ab co cb ph pi"><h2 class="bf fr hw z io pj iq ir pk it iv fp bk">Advanced RAG 01: Small-to-Big Retrieval</h2><div class="pl l"><h3 class="bf b hw z io pj iq ir pk it iv dx">Child-Parent RecursiveRetriever and Sentence Window Retrieval with LlamaIndex</h3></div><div class="pm l"><p class="bf b dy z io pj iq ir pk it iv dx">towardsdatascience.com</p></div></div><div class="pn l"><div class="sc l pp pq pr pn ps lr pe"/></div></div></a></div><h1 id="e67e" class="pt pu fq bf pv pw px gq py pz qa gt qb qc qd qe qf qg qh qi qj qk ql qm qn qo bk">Summary</h1><p id="c384" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">This article covered the concept of advanced RAG, which covers a set of techniques to address the limitations of the naive RAG paradigm. After an overview of advanced RAG techniques, which can be categorized into pre-retrieval, retrieval, and post-retrieval techniques, this article implemented a naive and advanced RAG pipeline using LlamaIndex for orchestration.</p><p id="5559" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The RAG pipeline components were language models from <a class="af oh" href="https://openai.com/" rel="noopener ugc nofollow" target="_blank">OpenAI</a>, a reranker model from <a class="af oh" href="https://www.baai.ac.cn/english.html" rel="noopener ugc nofollow" target="_blank">BAAI</a> hosted on <a class="af oh" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">Hugging Face</a>, and a <a class="af oh" href="https://weaviate.io/" rel="noopener ugc nofollow" target="_blank">Weaviate</a> vector database.</p><p id="daef" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We implemented the following selection of techniques using LlamaIndex in Python:</p><ul class=""><li id="b189" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oi oj ok bk"><a class="af oh" href="#c968" rel="noopener ugc nofollow">Pre-retrieval optimization: Sentence window retrieval</a></li><li id="e432" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk"><a class="af oh" href="#3275" rel="noopener ugc nofollow">Retrieval optimization: Hybrid search</a></li><li id="3625" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk"><a class="af oh" href="#c1e2" rel="noopener ugc nofollow">Post-retrieval optimization: Re-ranking</a></li></ul><p id="2d43" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can find the Jupyter Notebooks containing the full end-to-end pipelines here:</p><ul class=""><li id="0cc2" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oi oj ok bk"><a class="af oh" href="https://github.com/weaviate/recipes/blob/main/integrations/llamaindex/retrieval-augmented-generation/naive_rag.ipynb" rel="noopener ugc nofollow" target="_blank">Naive RAG in LlamaIndex</a></li><li id="45cd" class="nc nd fq ne b go ol ng nh gr om nj nk nl on nn no np oo nr ns nt op nv nw nx oi oj ok bk"><a class="af oh" href="https://github.com/weaviate/recipes/blob/main/integrations/llamaindex/retrieval-augmented-generation/advanced_rag.ipynb" rel="noopener ugc nofollow" target="_blank">Advanced RAG in LlamaIndex</a></li></ul><h1 id="e189" class="pt pu fq bf pv pw px gq py pz qa gt qb qc qd qe qf qg qh qi qj qk ql qm qn qo bk">Enjoyed This Story?</h1><p id="05de" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk"><a class="af oh" href="https://medium.com/subscribe/@iamleonie" rel="noopener"><em class="oq">Subscribe for free</em></a><em class="oq"> to get notified when I publish a new story.</em></p><div class="oz pa pb pc pd pe"><a href="https://medium.com/@iamleonie/subscribe?source=post_page-----4de1464a9930--------------------------------" rel="noopener follow" target="_blank"><div class="pf ab ig"><div class="pg ab co cb ph pi"><h2 class="bf fr hw z io pj iq ir pk it iv fp bk">Get an email whenever Leonie Monigatti publishes.</h2><div class="pl l"><h3 class="bf b hw z io pj iq ir pk it iv dx">Get an email whenever Leonie Monigatti publishes. By signing up, you will create a Medium account if you don't already…</h3></div><div class="pm l"><p class="bf b dy z io pj iq ir pk it iv dx">medium.com</p></div></div><div class="pn l"><div class="sd l pp pq pr pn ps lr pe"/></div></div></a></div><p id="3b0f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oq">Find me on </em><a class="af oh" href="https://www.linkedin.com/in/804250ab/" rel="noopener ugc nofollow" target="_blank"><em class="oq">LinkedIn</em></a>,<em class="oq"> </em><a class="af oh" href="https://twitter.com/helloiamleonie" rel="noopener ugc nofollow" target="_blank"><em class="oq">Twitter</em></a><em class="oq">, and </em><a class="af oh" href="https://www.kaggle.com/iamleonie" rel="noopener ugc nofollow" target="_blank"><em class="oq">Kaggle</em></a><em class="oq">!</em></p><h1 id="ab5e" class="pt pu fq bf pv pw px gq py pz qa gt qb qc qd qe qf qg qh qi qj qk ql qm qn qo bk">Disclaimer</h1><p id="4f30" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">I am a Developer Advocate at Weaviate at the time of this writing.</p><h1 id="1f8b" class="pt pu fq bf pv pw px gq py pz qa gt qb qc qd qe qf qg qh qi qj qk ql qm qn qo bk">References</h1><h2 id="f252" class="rb pu fq bf pv rc rd re py rf rg rh qb nl ri rj rk np rl rm rn nt ro rp rq rr bk">Literature</h2><p id="2988" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">[1] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., … &amp; Wang, H. (2023). Retrieval-augmented generation for large language models: A survey. <a class="af oh" href="https://arxiv.org/pdf/2312.10997.pdf" rel="noopener ugc nofollow" target="_blank"><em class="oq">arXiv preprint arXiv:2312.10997</em></a>.</p><h2 id="fc66" class="rb pu fq bf pv rc rd re py rf rg rh qb nl ri rj rk np rl rm rn nt ro rp rq rr bk">Images</h2><p id="2459" class="pw-post-body-paragraph nc nd fq ne b go qp ng nh gr qq nj nk nl qr nn no np qs nr ns nt qt nv nw nx fj bk">If not otherwise stated, all images are created by the author.</p></div></div></div></div>    
</body>
</html>