- en: Create Synthetic Dataset Using Llama 3.1 to Fine-Tune Your LLM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Llama 3.1 åˆ›å»ºåˆæˆæ•°æ®é›†ï¼Œä»¥å¾®è°ƒä½ çš„ LLM
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/create-a-synthetic-dataset-using-llama-3-1-405b-for-instruction-fine-tuning-9afc22fb6eef?source=collection_archive---------2-----------------------#2024-08-07](https://towardsdatascience.com/create-a-synthetic-dataset-using-llama-3-1-405b-for-instruction-fine-tuning-9afc22fb6eef?source=collection_archive---------2-----------------------#2024-08-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/create-a-synthetic-dataset-using-llama-3-1-405b-for-instruction-fine-tuning-9afc22fb6eef?source=collection_archive---------2-----------------------#2024-08-07](https://towardsdatascience.com/create-a-synthetic-dataset-using-llama-3-1-405b-for-instruction-fine-tuning-9afc22fb6eef?source=collection_archive---------2-----------------------#2024-08-07)
- en: Using the giant Llama 3.1 405B and Nvidia Nemotron 4 reward model to create
    a synthetic dataset for instruction fine-tuning.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åºå¤§çš„ Llama 3.1 405B å’Œ Nvidia Nemotron 4 å¥–åŠ±æ¨¡å‹æ¥åˆ›å»ºç”¨äºæŒ‡ä»¤å¾®è°ƒçš„åˆæˆæ•°æ®é›†ã€‚
- en: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--9afc22fb6eef--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--9afc22fb6eef--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9afc22fb6eef--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9afc22fb6eef--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--9afc22fb6eef--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--9afc22fb6eef--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--9afc22fb6eef--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9afc22fb6eef--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9afc22fb6eef--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--9afc22fb6eef--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9afc22fb6eef--------------------------------)
    Â·9 min readÂ·Aug 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9afc22fb6eef--------------------------------)
    Â·9 åˆ†é’Ÿé˜…è¯»Â·2024å¹´8æœˆ7æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d59b24212f7745714c66c7591192d4a8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d59b24212f7745714c66c7591192d4a8.png)'
- en: Created by AI using Leonardo.AI
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç”± AI ä½¿ç”¨ Leonardo.AI åˆ›å»º
- en: Data is the heart of AI and while it is a valuable asset, we know how challenging
    and costly it is to develop high-quality datasets. A well-curated and filtered
    dataset can make up for a lack of complexity in a model. This is also the case
    with Large Language Models where smaller-sized models have shown to outperform
    bigger LLMs by leveraging good data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®æ˜¯ AI çš„æ ¸å¿ƒï¼Œå°½ç®¡å®ƒæ˜¯å®è´µçš„èµ„äº§ï¼Œä½†æˆ‘ä»¬çŸ¥é“å¼€å‘é«˜è´¨é‡æ•°æ®é›†æ˜¯å¤šä¹ˆå…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚ä¸€ä¸ªç²¾å¿ƒç­–åˆ’å’Œè¿‡æ»¤çš„æ•°æ®é›†å¯ä»¥å¼¥è¡¥æ¨¡å‹å¤æ‚åº¦çš„ä¸è¶³ã€‚å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œè¾ƒå°çš„æ¨¡å‹é€šè¿‡åˆ©ç”¨è‰¯å¥½çš„æ•°æ®å¾€å¾€èƒ½ä¼˜äºæ›´å¤§çš„
    LLMã€‚
- en: In this article, we will explore how to use **Llama 3.1 405B** to create a synthetic
    dataset of **git commands in natural language**. I will show how you can use this
    405B beast without running tens of GPUs in parallel. After having an initial dataset
    of instructions and responses, we will use **Nvidiaâ€™s Nemotron 4** as a reward
    model to filter out any bad prompt/response pairs. Finally, we will push this
    dataset to HuggingFace for later fine-tuning of our LLM.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•ä½¿ç”¨**Llama 3.1 405B**æ¥åˆ›å»ºä¸€ä¸ª**è‡ªç„¶è¯­è¨€ä¸­çš„ git å‘½ä»¤**åˆæˆæ•°æ®é›†ã€‚æˆ‘å°†å±•ç¤ºå¦‚ä½•åœ¨ä¸éœ€è¦å¹¶è¡Œè¿è¡Œæ•°åä¸ª
    GPU çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨è¿™æ¬¾405Bå·¨å…½ã€‚æ‹¥æœ‰åˆæ­¥çš„æŒ‡ä»¤å’Œå“åº”æ•°æ®é›†åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨**Nvidia çš„ Nemotron 4**ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œè¿‡æ»¤æ‰ä»»ä½•ä¸è‰¯çš„æç¤º/å“åº”å¯¹ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æŠŠè¿™ä¸ªæ•°æ®é›†æ¨é€åˆ°
    HuggingFaceï¼Œä»¥ä¾¿ç¨åå¯¹æˆ‘ä»¬çš„ LLM è¿›è¡Œå¾®è°ƒã€‚
- en: This will be fast, free, and will leave you much in control.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æ˜¯å¿«é€Ÿã€å…è´¹çš„ï¼Œå¹¶ä¸”è®©ä½ æŒæ¡æ›´å¤šçš„æ§åˆ¶æƒã€‚
- en: I will keep this post concise and knowledge-packed, so make sure to **read through
    the end** and familiarize yourself with this essential skill.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†ä¿æŒè¿™ç¯‡æ–‡ç« ç®€æ´ä¸”å……æ»¡å¹²è´§ï¼Œå› æ­¤è¯·ç¡®ä¿**é˜…è¯»åˆ°æœ€å**ï¼Œå¹¶ç†Ÿæ‚‰è¿™ä¸€é‡è¦æŠ€èƒ½ã€‚
- en: ğŸ¦™ Why Llama 3.1
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¦™ ä¸ºä»€ä¹ˆé€‰æ‹© Llama 3.1
- en: Meta has gained a firm foothold with the release of their latest family of LLMs,
    [Llama 3.1](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f).
    The new family includes an upgraded version of the previous 8B and 70B models
    with increased reasoning abilities and a giant 405B model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Meta åœ¨å‘å¸ƒæœ€æ–°ä¸€ä»£ LLM å®¶æ—åï¼Œç‰¢ç‰¢å æ®äº†å¸‚åœºçš„ä»½é¢ï¼Œ[Llama 3.1](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f)ã€‚è¿™ä¸ªæ–°å®¶æ—åŒ…æ‹¬äº†å‡çº§ç‰ˆçš„
    8B å’Œ 70B æ¨¡å‹ï¼Œå…·æœ‰æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ¨å‡ºäº†åºå¤§çš„ 405B æ¨¡å‹ã€‚
- en: '![](../Images/cbadf588e2561f45221657f93c764fa2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbadf588e2561f45221657f93c764fa2.png)'
- en: Llama 3.1 405 has been successful in reaching nearly the benchmark of the best
    closed-source models. (diagram by [Maxime Labonne](https://medium.com/u/dc89da634938?source=post_page---user_mention--9afc22fb6eef--------------------------------),
    with permission)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3.1 405 åœ¨æˆåŠŸæ¥è¿‘æœ€ä½³é—­æºæ¨¡å‹çš„åŸºå‡†æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆå°±ã€‚ï¼ˆå›¾è¡¨ç”± [Maxime Labonne](https://medium.com/u/dc89da634938?source=post_page---user_mention--9afc22fb6eef--------------------------------)
    æä¾›ï¼Œå·²è·è®¸å¯ï¼‰
- en: Llama 3.1 405B isnâ€™t just impressive in terms of the sheer scale, but also by
    closing the gap between closed-source and open-source models, more than ever before
    (above figure).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3.1 405B ä¸ä»…åœ¨è§„æ¨¡ä¸Šä»¤äººå°è±¡æ·±åˆ»ï¼Œè€Œä¸”é€šè¿‡ç¼©å°é—­æºå’Œå¼€æºæ¨¡å‹ä¹‹é—´çš„å·®è·ï¼Œæ¯”ä»¥å¾€ä»»ä½•æ—¶å€™éƒ½æ›´åŠ å‡ºè‰²ï¼ˆè§ä¸Šå›¾ï¼‰ã€‚
- en: This capability of the 405B model makes it ideal for some of the most important
    and nuanced workflows, such as Retrieval-Augmented Generation (RAG), supervised
    fine-tuning (SFT), and most importantly **synthetic data generation**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 405B æ¨¡å‹çš„è¿™ä¸€èƒ½åŠ›ä½¿å…¶éå¸¸é€‚åˆä¸€äº›æœ€é‡è¦ä¸”æœ€å¾®å¦™çš„å·¥ä½œæµï¼Œä¾‹å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä»¥åŠæœ€é‡è¦çš„**åˆæˆæ•°æ®ç”Ÿæˆ**ã€‚
- en: Why Synthetic Data?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆé€‰æ‹©åˆæˆæ•°æ®ï¼Ÿ
- en: Synthetic data is created using an artificial model by reproducing the characteristics
    and features of real-world data. At some point, you will need to work with it
    *when you need more data than you have*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åˆæˆæ•°æ®æ˜¯é€šè¿‡ä½¿ç”¨äººå·¥æ¨¡å‹å†ç°çœŸå®ä¸–ç•Œæ•°æ®çš„ç‰¹å¾å’Œç‰¹æ€§æ¥åˆ›å»ºçš„ã€‚åœ¨æŸäº›æ—¶å€™ï¼Œå½“ä½ éœ€è¦çš„æ•°æ®è¶…è¿‡ä½ ç°æœ‰çš„æ•°æ®æ—¶ï¼Œä½ å°†éœ€è¦ä½¿ç”¨å®ƒ*ã€‚
- en: Our example of a dataset of git commands in natural language can show this perfectly.
    If we want to create an application that takes as input, what the user needs and
    then suggests the right git command for it, then at the heart of this application
    we will need an expert LLM. We could use GPT-4o or Claude and will most likely
    get good results. But there is the problem of the cost. So the alternative would
    be to **fine-tune** a Small Language Model (SML) such as Llama 3.1 8B or Gemma
    2 2B (which I will get to in a later post).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è‡ªç„¶è¯­è¨€ä¸­ Git å‘½ä»¤æ•°æ®é›†çš„ç¤ºä¾‹å¯ä»¥å®Œç¾åœ°å±•ç¤ºè¿™ä¸€ç‚¹ã€‚å¦‚æœæˆ‘ä»¬æƒ³è¦åˆ›å»ºä¸€ä¸ªåº”ç”¨ç¨‹åºï¼Œå®ƒä»¥ç”¨æˆ·éœ€æ±‚ä¸ºè¾“å…¥ï¼Œç„¶åä¸ºå…¶å»ºè®®æ­£ç¡®çš„ git å‘½ä»¤ï¼Œé‚£ä¹ˆåœ¨è¿™ä¸ªåº”ç”¨ç¨‹åºçš„æ ¸å¿ƒï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¸“å®¶çº§çš„
    LLMã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ GPT-4o æˆ– Claudeï¼Œå¹¶ä¸”å¾ˆå¯èƒ½ä¼šè·å¾—ä¸é”™çš„ç»“æœã€‚ä½†é—®é¢˜åœ¨äºæˆæœ¬ã€‚å› æ­¤ï¼Œå¦ä¸€ç§é€‰æ‹©æ˜¯**å¾®è°ƒ**ä¸€ä¸ªå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSMLï¼‰ï¼Œä¾‹å¦‚
    Llama 3.1 8B æˆ– Gemma 2 2Bï¼ˆæˆ‘å°†åœ¨åç»­å¸–å­ä¸­è¯¦ç»†ä»‹ç»ï¼‰ã€‚
- en: And guess what we need for fine-tuningâ€¦ Data!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŒœæˆ‘ä»¬ä¸ºå¾®è°ƒéœ€è¦ä»€ä¹ˆå—â€¦â€¦æ•°æ®ï¼
- en: 'Since I didnâ€™t find the right dataset for this task, we are left with only
    one solution: to create our dataset **synthetically** using Llama 3.1 405B.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘æ²¡æœ‰æ‰¾åˆ°é€‚åˆæ­¤ä»»åŠ¡çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ Llama 3.1 405B åˆæˆåˆ›å»ºæˆ‘ä»¬çš„æ•°æ®é›†ã€‚
- en: ğŸ› ï¸Building the Dataset
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ› ï¸ æ„å»ºæ•°æ®é›†
- en: To build a synthetic dataset using AI, we will use the following outline. You
    can choose any other LLMs from what I have chosen.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿ç”¨ AI æ„å»ºä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å¤§çº²ã€‚ä½ å¯ä»¥é€‰æ‹©ä»»ä½•æˆ‘æ‰€é€‰æ‹©çš„å…¶ä»– LLMã€‚
- en: '![](../Images/07a1f36079f8e08fdc8b2bb76c584e37.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07a1f36079f8e08fdc8b2bb76c584e37.png)'
- en: Our outline of creating a synthetic dataset. (by author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ›å»ºåˆæˆæ•°æ®é›†çš„å¤§çº²ã€‚ï¼ˆä½œè€…æä¾›ï¼‰
- en: Setting Up the API Key
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½® API å¯†é’¥
- en: We will use the **Nvidia NIM API** to leverage these big LLMs without the hassle
    of running them locally. Running a model like Llama 3.1 405B on the device would
    normally require multiple H100 GPUs and unless you work in an organization with
    such resources, you need to use external APIs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨**Nvidia NIM API**æ¥åˆ©ç”¨è¿™äº›å¤§å‹ LLMï¼Œè€Œæ— éœ€åœ¨æœ¬åœ°è¿è¡Œå®ƒä»¬ã€‚åƒ Llama 3.1 405B è¿™æ ·çš„æ¨¡å‹åœ¨è®¾å¤‡ä¸Šè¿è¡Œé€šå¸¸éœ€è¦å¤šä¸ª
    H100 GPUï¼Œé™¤éä½ åœ¨æ‹¥æœ‰è¿™äº›èµ„æºçš„ç»„ç»‡ä¸­å·¥ä½œï¼Œå¦åˆ™ä½ éœ€è¦ä½¿ç”¨å¤–éƒ¨ APIã€‚
- en: To access your free Nvidia credits, go to [Llama 3.1 on Nvidia NIM](https://build.nvidia.com/explore/discover#llama-3_1-405b-instruct),
    and click on **Get API Key**. This is what we will use in our code or a `.env`
    file. Once we have the API, we can set up our connection to the Nvidia server
    to use the models remotely.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è®¿é—®ä½ çš„å…è´¹ Nvidia ç§¯åˆ†ï¼Œè¯·è®¿é—® [Llama 3.1 on Nvidia NIM](https://build.nvidia.com/explore/discover#llama-3_1-405b-instruct)ï¼Œç„¶åç‚¹å‡»**è·å–
    API å¯†é’¥**ã€‚è¿™å°†æ˜¯æˆ‘ä»¬åœ¨ä»£ç ä¸­æˆ– `.env` æ–‡ä»¶ä¸­ä½¿ç”¨çš„å†…å®¹ã€‚ä¸€æ—¦æˆ‘ä»¬è·å¾— API å¯†é’¥ï¼Œå°±å¯ä»¥è®¾ç½®ä¸ Nvidia æœåŠ¡å™¨çš„è¿æ¥ï¼Œä»¥è¿œç¨‹ä½¿ç”¨è¿™äº›æ¨¡å‹ã€‚
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Generate Subtopics
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå­ä¸»é¢˜
- en: Ideally, we like our dataset to cover various scenarios and situations as much
    as possible. One way to ensure this is to define **subtopics** and ask Llama 3.1
    to provide instructions/response pairs for each of the subtopics. We can choose
    these subtopics ourselves or leave it to the LLM to decide. I took the second
    approach in the following code snippet.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ•°æ®é›†èƒ½å¤Ÿå°½å¯èƒ½æ¶µç›–å„ç§åœºæ™¯å’Œæƒ…å†µã€‚ä¸€ç§ç¡®ä¿è¿™ä¸€ç‚¹çš„æ–¹æ³•æ˜¯å®šä¹‰**å­ä¸»é¢˜**ï¼Œå¹¶è¦æ±‚ Llama 3.1 ä¸ºæ¯ä¸ªå­ä¸»é¢˜æä¾›æŒ‡ä»¤/å“åº”å¯¹ã€‚æˆ‘ä»¬å¯ä»¥è‡ªå·±é€‰æ‹©è¿™äº›å­ä¸»é¢˜ï¼Œä¹Ÿå¯ä»¥è®©
    LLM æ¥å†³å®šã€‚æˆ‘åœ¨ä»¥ä¸‹ä»£ç ç‰‡æ®µä¸­é‡‡ç”¨äº†ç¬¬äºŒç§æ–¹æ³•ã€‚
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The LLM suggests five topics: Branching, Merging, Committing, Remote repositories,
    and Resolving conflicts. It seems like a fair selection of subjects to cover.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLMå»ºè®®äº†äº”ä¸ªä¸»é¢˜ï¼šåˆ†æ”¯ã€åˆå¹¶ã€æäº¤ã€è¿œç¨‹ä»“åº“å’Œè§£å†³å†²çªã€‚ä¼¼ä¹è¿™æ˜¯ä¸€ä¸ªåˆç†çš„ä¸»é¢˜é€‰æ‹©ã€‚
- en: Generate Instructions
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”ŸæˆæŒ‡ä»¤
- en: Having five subtopics of working with Git, we need Llama 3.1 to generate a set
    of instructions (or prompts) regarding each of the subtopics. I have asked for
    one hundred instructions per topic, so ideally, I should get 500 prompts.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ‰äº”ä¸ªå­ä¸»é¢˜å…³äºGitçš„å·¥ä½œæ—¶ï¼Œæˆ‘ä»¬éœ€è¦Llama 3.1ç”Ÿæˆä¸€ç»„å…³äºæ¯ä¸ªå­ä¸»é¢˜çš„æŒ‡ä»¤ï¼ˆæˆ–æç¤ºï¼‰ã€‚æˆ‘è¦æ±‚æ¯ä¸ªä¸»é¢˜ç”Ÿæˆä¸€ç™¾æ¡æŒ‡ä»¤ï¼Œå› æ­¤ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘åº”è¯¥å¾—åˆ°500æ¡æç¤ºã€‚
- en: 'One thing to keep in mind is that when asking for *N* number of instructions:
    it is rare that the model would return exactly as many as you want, even a big
    model like this.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦è®°ä½çš„ä¸€ç‚¹æ˜¯ï¼Œå½“è¦æ±‚ç”Ÿæˆ*N*æ¡æŒ‡ä»¤æ—¶ï¼šå³ä½¿æ˜¯è¿™æ ·ä¸€ä¸ªå¤§å‹æ¨¡å‹ï¼Œä¹Ÿå¾ˆå°‘ä¼šè¿”å›æ­£å¥½ç­‰äºä½ æƒ³è¦çš„æ•°é‡ã€‚
- en: Eventually, I got a total of 335 instructions for 5 subtopics, which is very
    different from 500\. There are methods to ensure this doesnâ€™t happen but for the
    sake of simplicity, we wonâ€™t dwell on this.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œæˆ‘ä¸ºäº”ä¸ªå­ä¸»é¢˜å¾—åˆ°äº†æ€»å…±335æ¡æŒ‡ä»¤ï¼Œè¿™ä¸500æ¡æœ‰å¾ˆå¤§å·®è·ã€‚è™½ç„¶æœ‰æ–¹æ³•å¯ä»¥ç¡®ä¿è¿™ç§æƒ…å†µä¸å‘ç”Ÿï¼Œä½†ä¸ºäº†ç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬å°±ä¸å†æ·±å…¥è®¨è®ºäº†ã€‚
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here are some examples of the generated instructions:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯ä¸€äº›ç”Ÿæˆçš„æŒ‡ä»¤ç¤ºä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Response Generation
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å“åº”ç”Ÿæˆ
- en: For each of the provided instructions, we will also ask for a response. As you
    can see in the following code snippet, I have specifically asked my responses
    to be *on-topic, informative, and concise*. By the end, I will have a list of
    `instruction` and `response` pairs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸€æ¡æä¾›çš„æŒ‡ä»¤ï¼Œæˆ‘ä»¬è¿˜å°†è¦æ±‚ç»™å‡ºå“åº”ã€‚æ­£å¦‚ä¸‹é¢çš„ä»£ç ç‰‡æ®µæ‰€ç¤ºï¼Œæˆ‘ç‰¹åˆ«è¦æ±‚æˆ‘çš„å“åº”æ˜¯*åˆ‡é¢˜ã€æœ‰ä¿¡æ¯é‡ä¸”ç®€æ´çš„*ã€‚åˆ°æœ€åï¼Œæˆ‘å°†å¾—åˆ°ä¸€å¯¹`æŒ‡ä»¤`å’Œ`å“åº”`çš„åˆ—è¡¨ã€‚
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Filtering Responses with Nemotron 4
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Nemotron 4ç­›é€‰å“åº”
- en: Even though we have our instruction/response pairs, not all of the responses
    are high-quality. They may be verbose, complex, or false. This is where Nvidiaâ€™s
    [**Nemotron 4 340B Reward**](https://huggingface.co/nvidia/Nemotron-4-340B-Reward)model
    comes into play. It is made exactly for our use case, as according to Nvidia,
    it *â€œcan be used as part of a synthetic data generation pipeline to create training
    data that helps researchers and developers build their own LLMs.â€*
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æˆ‘ä»¬æ‹¥æœ‰äº†æŒ‡ä»¤/å“åº”å¯¹ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„å“åº”éƒ½æ˜¯é«˜è´¨é‡çš„ã€‚å®ƒä»¬å¯èƒ½å†—é•¿ã€å¤æ‚æˆ–è€…æ˜¯é”™è¯¯çš„ã€‚è¿™æ—¶ï¼ŒNvidiaçš„[**Nemotron 4 340Bå¥–åŠ±**](https://huggingface.co/nvidia/Nemotron-4-340B-Reward)æ¨¡å‹å°±æ´¾ä¸Šç”¨åœºäº†ã€‚å®ƒæ­£æ˜¯ä¸ºæˆ‘ä»¬çš„ç”¨ä¾‹è®¾è®¡çš„ï¼Œå› ä¸ºæ®Nvidiaæ‰€è¯´ï¼Œå®ƒ*â€œå¯ä»¥ä½œä¸ºåˆæˆæ•°æ®ç”Ÿæˆç®¡é“çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ„å»ºè‡ªå·±çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚â€*
- en: '![](../Images/68716e2a3af1320dfe4da92f01e82d06.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68716e2a3af1320dfe4da92f01e82d06.png)'
- en: Example use of Nemotron 4\. (by author)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Nemotron 4çš„ç¤ºä¾‹ä½¿ç”¨ï¼ˆç”±ä½œè€…æä¾›ï¼‰
- en: We will give each one of our instruction/response pairs to Nemotron 4, and receive
    five scores ranging from 0 to 4\. These five scores are *helpfulness, correctness,
    coherence, complexity, and verbosity*. To use the model, I will first define a
    simple function to feed an instruction and a response to the model and receive
    the five scores in the shape of a dict.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æŠŠæ¯ä¸€å¯¹æŒ‡ä»¤/å“åº”äº¤ç»™Nemotron 4ï¼Œå¹¶è·å¾—äº”ä¸ªåˆ†æ•°ï¼ŒèŒƒå›´ä»0åˆ°4ã€‚è¿™äº”ä¸ªåˆ†æ•°æ˜¯*æœ‰ç”¨æ€§ã€æ­£ç¡®æ€§ã€ä¸€è‡´æ€§ã€å¤æ‚æ€§å’Œå†—é•¿æ€§*ã€‚ä¸ºäº†ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œæˆ‘å°†é¦–å…ˆå®šä¹‰ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œå°†æŒ‡ä»¤å’Œå“åº”ä¼ é€’ç»™æ¨¡å‹ï¼Œå¹¶ä»¥å­—å…¸çš„å½¢å¼æ¥æ”¶äº”ä¸ªåˆ†æ•°ã€‚
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After we have a score for each of the rows in our dataset, we can filter the
    dataset using each of the five provided criteria. I will filter out bad responses
    based on **helpfulness** and **verbosity**, as I want to keep my responses concise
    and informative.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬ä¸ºæ•°æ®é›†ä¸­çš„æ¯ä¸€è¡Œå¾—åˆ°äº†åˆ†æ•°ï¼Œå°±å¯ä»¥æ ¹æ®æä¾›çš„äº”ä¸ªæ ‡å‡†å¯¹æ•°æ®é›†è¿›è¡Œç­›é€‰ã€‚æˆ‘å°†æ ¹æ®**æœ‰ç”¨æ€§**å’Œ**å†—é•¿æ€§**ç­›é€‰å‡ºä¸è‰¯å“åº”ï¼Œå› ä¸ºæˆ‘å¸Œæœ›æˆ‘çš„å›ç­”ç®€æ´ä¸”å¯Œæœ‰ä¿¡æ¯ã€‚
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Push the Dataset to HuggingFace
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†æ•°æ®é›†æ¨é€åˆ°HuggingFace
- en: Finally, once you have the finished dataset, itâ€™s a good practice to push it
    to HuggingFace to use it later or to share it with other developers. To do this,
    first log in to HuggingFace and provide a **token**, following the provided link
    on the login page.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä¸€æ—¦ä½ æ‹¥æœ‰äº†å®Œæˆçš„æ•°æ®é›†ï¼Œæœ€å¥½å°†å…¶æ¨é€åˆ°HuggingFaceï¼Œä»¥ä¾¿ä»¥åä½¿ç”¨æˆ–ä¸å…¶ä»–å¼€å‘è€…åˆ†äº«ã€‚ä¸ºæ­¤ï¼Œé¦–å…ˆç™»å½•HuggingFaceå¹¶æä¾›ä¸€ä¸ª**ä»¤ç‰Œ**ï¼Œå…·ä½“æ­¥éª¤å¯å‚è€ƒç™»å½•é¡µé¢æä¾›çš„é“¾æ¥ã€‚
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Then you can load the saved dataset and upload it on your HuggingFace page.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä½ å¯ä»¥åŠ è½½ä¿å­˜çš„æ•°æ®é›†å¹¶å°†å…¶ä¸Šä¼ åˆ°ä½ çš„HuggingFaceé¡µé¢ã€‚
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Congrats ğŸ†! So far you have been able to use Llama 3.1 to create a dataset of
    instructions and responses, and Nemotron 4 to refine the dataset and filter out
    bad responses. In the end, we saw how easy it is to push the dataset to HuggingFace
    with no effort. [Create Synthetic Dataset from 1 TOPIC for Instruction Finetuning](https://www.youtube.com/watch?v=FAdRMVAWiak)
    is also a great inspiration for this article and I would suggest you watch it
    if you like this topic.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œ ğŸ†ï¼åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½ å·²ç»èƒ½å¤Ÿä½¿ç”¨ Llama 3.1 åˆ›å»ºä¸€ä¸ªæŒ‡ä»¤å’Œå›ç­”çš„æ•°æ®é›†ï¼Œå¹¶ä¸”ä½¿ç”¨ Nemotron 4 å¯¹æ•°æ®é›†è¿›è¡Œä¼˜åŒ–å¹¶è¿‡æ»¤æ‰ä¸å¥½çš„å›ç­”ã€‚æœ€åï¼Œæˆ‘ä»¬çœ‹åˆ°å°†æ•°æ®é›†æ¨é€åˆ°
    HuggingFace æ˜¯å¦‚æ­¤è½»æ¾ã€‚[ä»ä¸€ä¸ªä¸»é¢˜åˆ›å»ºåˆæˆæ•°æ®é›†ç”¨äºæŒ‡ä»¤å¾®è°ƒ](https://www.youtube.com/watch?v=FAdRMVAWiak)
    ä¹Ÿæ˜¯æœ¬æ–‡çš„ä¸€ä¸ªé‡è¦çµæ„Ÿæ¥æºï¼Œå¦‚æœä½ å–œæ¬¢è¿™ä¸ªä¸»é¢˜ï¼Œæˆ‘å»ºè®®ä½ è§‚çœ‹å®ƒã€‚
- en: Here is also the **repository** where you can find the complete code I have
    used. Donâ€™t forget to **star** â­the repo if you check it out.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œè¿˜æœ‰**ä»£ç ä»“åº“**ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­æ‰¾åˆ°æˆ‘ä½¿ç”¨çš„å®Œæ•´ä»£ç ã€‚å¦‚æœä½ æŸ¥çœ‹äº†å®ƒï¼Œåˆ«å¿˜äº†**ç»™ä»“åº“åŠ æ˜Ÿ**â­ã€‚
- en: '[***Creating Synthetic Dataset Using Llama 3.1 405B and Nemotron 4***](https://github.com/hesamsheikh/dataset_git_commands)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[***ä½¿ç”¨ Llama 3.1 405B å’Œ Nemotron 4 åˆ›å»ºåˆæˆæ•°æ®é›†***](https://github.com/hesamsheikh/dataset_git_commands)'
- en: T***hank you*** *for reading through the article!* Please share your opinions
    and suggestions if you think any modifications are required.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: T***æ„Ÿè°¢ä½ *** *é˜…è¯»æœ¬æ–‡ï¼* å¦‚æœä½ è®¤ä¸ºéœ€è¦ä»»ä½•ä¿®æ”¹ï¼Œè¯·åˆ†äº«ä½ çš„æ„è§å’Œå»ºè®®ã€‚
- en: Letâ€™s Connect!
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¿æŒè”ç³»ï¼
- en: '*Subscribe for FREE to be notified of new articles! You can also find me on*
    [*LinkedIn*](https://www.linkedin.com/in/hesamsheikh/) *and* [*Twitter*](https://x.com/itsHesamSheikh)*.*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…è´¹è®¢é˜…ä»¥è·å–æ–°æ–‡ç« çš„é€šçŸ¥ï¼ä½ ä¹Ÿå¯ä»¥åœ¨* [*LinkedIn*](https://www.linkedin.com/in/hesamsheikh/)
    *å’Œ* [*Twitter*](https://x.com/itsHesamSheikh)*æ‰¾åˆ°æˆ‘ã€‚*'
- en: '[](https://medium.com/@itshesamsheikh/subscribe?source=post_page-----9afc22fb6eef--------------------------------)
    [## Get an email whenever Hesam Sheikh publishes.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@itshesamsheikh/subscribe?source=post_page-----9afc22fb6eef--------------------------------)
    [## æ¯å½“ Hesam Sheikh å‘å¸ƒæ–°æ–‡ç« æ—¶ï¼Œæ‚¨å°†æ”¶åˆ°ç”µå­é‚®ä»¶é€šçŸ¥ã€‚'
- en: Get an email whenever Hesam Sheikh publishes. By signing up, you will create
    a Medium account if you don't already have notâ€¦
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¯å½“ Hesam Sheikh å‘å¸ƒæ–°æ–‡ç« æ—¶ï¼Œä½ å°†æ”¶åˆ°ç”µå­é‚®ä»¶é€šçŸ¥ã€‚é€šè¿‡æ³¨å†Œï¼Œå¦‚æœä½ è¿˜æ²¡æœ‰ Medium è´¦æˆ·ï¼Œä½ å°†åˆ›å»ºä¸€ä¸ªè´¦æˆ·â€¦â€¦
- en: medium.com](https://medium.com/@itshesamsheikh/subscribe?source=post_page-----9afc22fb6eef--------------------------------)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@itshesamsheikh/subscribe?source=post_page-----9afc22fb6eef--------------------------------)
- en: Further Reads
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±å…¥é˜…è¯»
- en: 'If you have reached so far, you might also find these articles interesting:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å·²ç»é˜…è¯»åˆ°è¿™é‡Œï¼Œä½ å¯èƒ½ä¼šå¯¹ä»¥ä¸‹æ–‡ç« æ„Ÿå…´è¶£ï¼š
- en: '[](/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=post_page-----9afc22fb6eef--------------------------------)
    [## What We Still Donâ€™t Understand About Machine Learning'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=post_page-----9afc22fb6eef--------------------------------)
    [## æˆ‘ä»¬ä»ç„¶ä¸ç†è§£çš„æœºå™¨å­¦ä¹ '
- en: Machine Learning unknowns that researchers struggle to understand â€” from Batch
    Norm to what SGD hides
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ ä¸­çš„æœªçŸ¥é—®é¢˜ï¼Œç ”ç©¶äººå‘˜ä¸€ç›´åœ¨åŠªåŠ›ç†è§£â€”â€”ä»æ‰¹é‡å½’ä¸€åŒ–åˆ° SGD éšè—çš„å†…å®¹
- en: towardsdatascience.com](/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=post_page-----9afc22fb6eef--------------------------------)
    [](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----9afc22fb6eef--------------------------------)
    [## A Comprehensive Guide to Collaborative AI Agents in Practice
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=post_page-----9afc22fb6eef--------------------------------)
    [](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----9afc22fb6eef--------------------------------)
    [## å®è·µä¸­çš„åä½œ AI ä»£ç†å…¨é¢æŒ‡å—
- en: the definition, and building a team of agents that refine your CV and Cover
    Letter for job applications
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®šä¹‰ï¼Œå¹¶å»ºç«‹ä¸€ä¸ªèƒ½å¤Ÿä¼˜åŒ–ä½ ç®€å†å’Œæ±‚èŒä¿¡çš„ä»£ç†å›¢é˜Ÿ
- en: towardsdatascience.com](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----9afc22fb6eef--------------------------------)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----9afc22fb6eef--------------------------------)
