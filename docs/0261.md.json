["```py\nimport cdsapi\nimport datetime\nimport functools\nfrom graphcast import autoregressive, casting, checkpoint, data_utils as du, graphcast, normalization, rollout\nimport haiku as hk\nimport isodate\nimport jax\nimport math\nimport numpy as np\nimport pandas as pd\nfrom pysolar.radiation import get_radiation_direct\nfrom pysolar.solar import get_altitude\nimport pytz\nimport scipy\nfrom typing import Dict\nimport xarray\n\nclient = cdsapi.Client() # Making a connection to CDS, to fetch data.\n\n# The fields to be fetched from the single-level source.\nsinglelevelfields = [\n                        '10m_u_component_of_wind',\n                        '10m_v_component_of_wind',\n                        '2m_temperature',\n                        'geopotential',\n                        'land_sea_mask',\n                        'mean_sea_level_pressure',\n                        'toa_incident_solar_radiation',\n                        'total_precipitation'\n                    ]\n\n# The fields to be fetched from the pressure-level source.\npressurelevelfields = [\n                        'u_component_of_wind',\n                        'v_component_of_wind',\n                        'geopotential',\n                        'specific_humidity',\n                        'temperature',\n                        'vertical_velocity'\n                    ]\n\n# The 13 pressure levels.\npressure_levels = [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]\n\n# Initializing other required constants.\npi = math.pi\ngap = 6 # There is a gap of 6 hours between each graphcast prediction.\npredictions_steps = 4 # Predicting for 4 timestamps.\nwatts_to_joules = 3600\nfirst_prediction = datetime.datetime(2024, 1, 1, 18, 0) # Timestamp of the first prediction.\nlat_range = range(-180, 181, 1) # Latitude range.\nlon_range = range(0, 360, 1) # Longitude range.\n\n# A utility function used for ease of coding.\n# Converting the variable to a datetime object.\ndef toDatetime(dt) -> datetime.datetime:\n    if isinstance(dt, datetime.date) and isinstance(dt, datetime.datetime):\n        return dt\n\n    elif isinstance(dt, datetime.date) and not isinstance(dt, datetime.datetime):\n        return datetime.datetime.combine(dt, datetime.datetime.min.time())\n\n    elif isinstance(dt, str):\n        if 'T' in dt:\n            return isodate.parse_datetime(dt)\n        else:\n            return datetime.datetime.combine(isodate.parse_date(dt), datetime.datetime.min.time())\n```", "```py\n# Getting the single and pressure level values.\ndef getSingleAndPressureValues():\n\n    client.retrieve(\n        'reanalysis-era5-single-levels',\n        {\n            'product_type': 'reanalysis',\n            'variable': singlelevelfields,\n            'grid': '1.0/1.0',\n            'year': [2024],\n            'month': [1],\n            'day': [1],\n            'time': ['00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00', '08:00', '09:00', '10:00', '11:00', '12:00'],\n            'format': 'netcdf'\n        },\n        'single-level.nc'\n    )\n    singlelevel = xarray.open_dataset('single-level.nc', engine = scipy.__name__).to_dataframe()\n    singlelevel = singlelevel.rename(columns = {col:singlelevelfields[ind] for ind, col in enumerate(singlelevel.columns.values.tolist())})\n    singlelevel = singlelevel.rename(columns = {'geopotential': 'geopotential_at_surface'})\n\n    # Calculating the sum of the last 6 hours of rainfall.\n    singlelevel = singlelevel.sort_index()\n    singlelevel['total_precipitation_6hr'] = singlelevel.groupby(level=[0, 1])['total_precipitation'].rolling(window = 6, min_periods = 1).sum().reset_index(level=[0, 1], drop=True)\n    singlelevel.pop('total_precipitation')\n\n    client.retrieve(\n        'reanalysis-era5-pressure-levels',\n        {\n            'product_type': 'reanalysis',\n            'variable': pressurelevelfields,\n            'grid': '1.0/1.0',\n            'year': [2024],\n            'month': [1],\n            'day': [1],\n            'time': ['06:00', '12:00'],\n            'pressure_level': pressure_levels,\n            'format': 'netcdf'\n        },\n        'pressure-level.nc'\n    )\n    pressurelevel = xarray.open_dataset('pressure-level.nc', engine = scipy.__name__).to_dataframe()\n    pressurelevel = pressurelevel.rename(columns = {col:pressurelevelfields[ind] for ind, col in enumerate(pressurelevel.columns.values.tolist())})\n\n    return singlelevel, pressurelevel\n\n# Adding sin and cos of the year progress.\ndef addYearProgress(secs, data):\n\n    progress = du.get_year_progress(secs)\n    data['year_progress_sin'] = math.sin(2 * pi * progress)\n    data['year_progress_cos'] = math.cos(2 * pi * progress)\n\n    return data\n\n# Adding sin and cos of the day progress.\ndef addDayProgress(secs, lon:str, data:pd.DataFrame):\n\n    lons = data.index.get_level_values(lon).unique()\n    progress:np.ndarray = du.get_day_progress(secs, np.array(lons))\n    prxlon = {lon:prog for lon, prog in list(zip(list(lons), progress.tolist()))}\n    data['day_progress_sin'] = data.index.get_level_values(lon).map(lambda x: math.sin(2 * pi * prxlon[x]))\n    data['day_progress_cos'] = data.index.get_level_values(lon).map(lambda x: math.cos(2 * pi * prxlon[x]))\n\n    return data\n\n# Adding day and year progress.\ndef integrateProgress(data:pd.DataFrame):\n\n    for dt in data.index.get_level_values('time').unique():\n        seconds_since_epoch = toDatetime(dt).timestamp()\n        data = addYearProgress(seconds_since_epoch, data)\n        data = addDayProgress(seconds_since_epoch, 'longitude' if 'longitude' in data.index.names else 'lon', data)\n\n    return data\n\n# Adding batch field and renaming some others.\ndef formatData(data:pd.DataFrame) -> pd.DataFrame:\n\n    data = data.rename_axis(index = {'latitude': 'lat', 'longitude': 'lon'})\n    if 'batch' not in data.index.names:\n        data['batch'] = 0\n        data = data.set_index('batch', append = True)\n\n    return data\n\nif __name__ == '__main__':\n\n    values:Dict[str, xarray.Dataset] = {}\n\n    single, pressure = getSingleAndPressureValues()\n    values['inputs'] = pd.merge(pressure, single, left_index = True, right_index = True, how = 'inner')\n    values['inputs'] = integrateProgress(values['inputs'])\n    values['inputs'] = formatData(values['inputs'])\n```", "```py\n# Includes the packages imported and constants assigned.\n# The functions created for the inputs also go here.\n\npredictionFields = [\n                        'u_component_of_wind',\n                        'v_component_of_wind',\n                        'geopotential',\n                        'specific_humidity',\n                        'temperature',\n                        'vertical_velocity',\n                        '10m_u_component_of_wind',\n                        '10m_v_component_of_wind',\n                        '2m_temperature',\n                        'mean_sea_level_pressure',\n                        'total_precipitation_6hr'\n                    ]\n\n# Creating an array full of nan values.\ndef nans(*args) -> list:\n    return np.full((args), np.nan)\n\n# Adding or subtracting time.\ndef deltaTime(dt, **delta) -> datetime.datetime:\n    return dt + datetime.timedelta(**delta)\n\ndef getTargets(dt, data:pd.DataFrame):\n\n    # Creating an array consisting of unique values of each index.\n    lat, lon, levels, batch = sorted(data.index.get_level_values('lat').unique().tolist()), sorted(data.index.get_level_values('lon').unique().tolist()), sorted(data.index.get_level_values('level').unique().tolist()), data.index.get_level_values('batch').unique().tolist()\n    time = [deltaTime(dt, hours = days * gap) for days in range(4)]\n\n    # Creating an empty dataset using latitude, longitude, the pressure levels and each prediction timestamp.\n    target = xarray.Dataset({field: (['lat', 'lon', 'level', 'time'], nans(len(lat), len(lon), len(levels), len(time))) for field in predictionFields}, coords = {'lat': lat, 'lon': lon, 'level': levels, 'time': time, 'batch': batch})\n\n    return target.to_dataframe()\n\nif __name__ == '__main__':\n\n    # The code for creating inputs will be here.\n\n    values['targets'] = getTargets(first_prediction, values['inputs'])\n```", "```py\n# Includes the packages imported and constants assigned.\n# The functions created for the inputs and targets also go here.\n\n# Adding a timezone to datetime.datetime variables.\ndef addTimezone(dt, tz = pytz.UTC) -> datetime.datetime:\n    dt = toDatetime(dt)\n    if dt.tzinfo == None:\n        return pytz.UTC.localize(dt).astimezone(tz)\n    else:\n        return dt.astimezone(tz)\n\n# Getting the solar radiation value wrt longitude, latitude and timestamp.\ndef getSolarRadiation(longitude, latitude, dt):\n\n    altitude_degrees = get_altitude(latitude, longitude, addTimezone(dt))\n    solar_radiation = get_radiation_direct(dt, altitude_degrees) if altitude_degrees > 0 else 0\n\n    return solar_radiation * watts_to_joules\n\n# Calculating the solar radiation values for timestamps to be predicted.\ndef integrateSolarRadiation(data:pd.DataFrame):\n\n    dates = list(data.index.get_level_values('time').unique())\n    coords = [[lat, lon] for lat in lat_range for lon in lon_range]\n    values = []\n\n    # For each data, getting the solar radiation value at a particular coordinate.\n    for dt in dates:\n        values.extend(list(map(lambda coord:{'time': dt, 'lon': coord[1], 'lat': coord[0], 'toa_incident_solar_radiation': getSolarRadiation(coord[1], coord[0], dt)}, coords)))\n\n    # Setting indices.\n    values = pd.DataFrame(values).set_index(keys = ['lat', 'lon', 'time'])\n\n    # The forcings dataset will now contain the solar radiation values.\n    return pd.merge(data, values, left_index = True, right_index = True, how = 'inner')\n\ndef getForcings(data:pd.DataFrame):\n\n    # Since forcings data does not contain batch as an index, it is dropped.\n    # So are all the columns, since forcings data only has 5, which will be created.\n    forcingdf = data.reset_index(level = 'level', drop = True).drop(labels = predictionFields, axis = 1)\n\n    # Keeping only the unique indices.\n    forcingdf = pd.DataFrame(index = forcingdf.index.drop_duplicates(keep = 'first'))\n\n    # Adding the sin and cos of day and year progress.\n    # Functions are included in the creation of inputs data section.\n    forcingdf = integrateProgress(forcingdf)\n\n    # Integrating the solar radiation values.\n    forcingdf = integrateSolarRadiation(forcingdf)\n\n    return forcingdf\n\nif __name__ == '__main__':\n\n    # The code for creating inputs and targets will be here.\n\n    values['forcings'] = getForcings(values['targets'])\n```", "```py\n# Includes the packages imported and constants assigned.\n# The functions created for the inputs, targets and forcings also go here.\n\n# A dictionary created, containing each coordinate a data variable requires.\nclass AssignCoordinates:\n\n    coordinates = {\n                    '2m_temperature': ['batch', 'lon', 'lat', 'time'],\n                    'mean_sea_level_pressure': ['batch', 'lon', 'lat', 'time'],\n                    '10m_v_component_of_wind': ['batch', 'lon', 'lat', 'time'],\n                    '10m_u_component_of_wind': ['batch', 'lon', 'lat', 'time'],\n                    'total_precipitation_6hr': ['batch', 'lon', 'lat', 'time'],\n                    'temperature': ['batch', 'lon', 'lat', 'level', 'time'],\n                    'geopotential': ['batch', 'lon', 'lat', 'level', 'time'],\n                    'u_component_of_wind': ['batch', 'lon', 'lat', 'level', 'time'],\n                    'v_component_of_wind': ['batch', 'lon', 'lat', 'level', 'time'],\n                    'vertical_velocity': ['batch', 'lon', 'lat', 'level', 'time'],\n                    'specific_humidity': ['batch', 'lon', 'lat', 'level', 'time'],\n                    'toa_incident_solar_radiation': ['batch', 'lon', 'lat', 'time'],\n                    'year_progress_cos': ['batch', 'time'],\n                    'year_progress_sin': ['batch', 'time'],\n                    'day_progress_cos': ['batch', 'lon', 'time'],\n                    'day_progress_sin': ['batch', 'lon', 'time'],\n                    'geopotential_at_surface': ['lon', 'lat'],\n                    'land_sea_mask': ['lon', 'lat'],\n                }\n\ndef modifyCoordinates(data:xarray.Dataset):\n\n    # Parsing through each data variable and removing unneeded indices.\n    for var in list(data.data_vars):\n        varArray:xarray.DataArray = data[var]\n        nonIndices = list(set(list(varArray.coords)).difference(set(AssignCoordinates.coordinates[var])))\n        data[var] = varArray.isel(**{coord: 0 for coord in nonIndices})\n    data = data.drop_vars('batch')\n\n    return data\n\ndef makeXarray(data:pd.DataFrame) -> xarray.Dataset:\n\n    # Converting to xarray.\n    data = data.to_xarray()\n    data = modifyCoordinates(data)\n\n    return data\n\nif __name__ == '__main__':\n\n    # The code for creating inputs, targets and forcings will be here.\n\n    values = {value:makeXarray(values[value]) for value in values}\n```", "```py\n.\n├── prediction.py\n├── model\n    ├── params\n        ├── GraphCast_small - ERA5 1979-2015 - resolution 1.0 - pressure levels 13 - mesh 2to5 - precipitation input and output.npz\n    ├── stats\n        ├── diffs_stddev_by_level.nc\n        ├── mean_by_level.nc\n        ├── stddev_by_level.nc\n```", "```py\n# Includes the packages imported and constants assigned.\n# The functions created for the inputs, targets and forcings also go here.\n\nwith open(r'model/params/GraphCast_small - ERA5 1979-2015 - resolution 1.0 - pressure levels 13 - mesh 2to5 - precipitation input and output.npz', 'rb') as model:\n    ckpt = checkpoint.load(model, graphcast.CheckPoint)\n    params = ckpt.params\n    state = {}\n    model_config = ckpt.model_config\n    task_config = ckpt.task_config\n\nwith open(r'model/stats/diffs_stddev_by_level.nc', 'rb') as f:\n    diffs_stddev_by_level = xarray.load_dataset(f).compute()\n\nwith open(r'model/stats/mean_by_level.nc', 'rb') as f:\n    mean_by_level = xarray.load_dataset(f).compute()\n\nwith open(r'model/stats/stddev_by_level.nc', 'rb') as f:\n    stddev_by_level = xarray.load_dataset(f).compute()\n\ndef construct_wrapped_graphcast(model_config:graphcast.ModelConfig, task_config:graphcast.TaskConfig):\n    predictor = graphcast.GraphCast(model_config, task_config)\n    predictor = casting.Bfloat16Cast(predictor)\n    predictor = normalization.InputsAndResiduals(predictor, diffs_stddev_by_level = diffs_stddev_by_level, mean_by_level = mean_by_level, stddev_by_level = stddev_by_level)\n    predictor = autoregressive.Predictor(predictor, gradient_checkpointing = True)\n    return predictor\n\n@hk.transform_with_state\ndef run_forward(model_config, task_config, inputs, targets_template, forcings):\n    predictor = construct_wrapped_graphcast(model_config, task_config)\n    return predictor(inputs, targets_template = targets_template, forcings = forcings)\n\ndef with_configs(fn):\n    return functools.partial(fn, model_config = model_config, task_config = task_config)\n\ndef with_params(fn):\n    return functools.partial(fn, params = params, state = state)\n\ndef drop_state(fn):\n    return lambda **kw: fn(**kw)[0]\n\nrun_forward_jitted = drop_state(with_params(jax.jit(with_configs(run_forward.apply))))\n\nclass Predictor:\n\n    @classmethod\n    def predict(cls, inputs, targets, forcings) -> xarray.Dataset:\n        predictions = rollout.chunked_prediction(run_forward_jitted, rng = jax.random.PRNGKey(0), inputs = inputs, targets_template = targets, forcings = forcings)\n        return predictions\n\nif __name__ == '__main__':\n\n    # The code for creating inputs, targets, forcings & processing will be here.\n\n    predictions = Predictor.predict(values['inputs'], values['targets'], values['forcings'])\n    predictions.to_dataframe().to_csv('predictions.csv', sep = ',')\n```"]