- en: 'Structure and Relationships: Graph Neural Networks and a Pytorch Implementation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/structure-and-relationships-graph-neural-networks-and-a-pytorch-implementation-c9d83b71c041?source=collection_archive---------1-----------------------#2024-03-05](https://towardsdatascience.com/structure-and-relationships-graph-neural-networks-and-a-pytorch-implementation-c9d83b71c041?source=collection_archive---------1-----------------------#2024-03-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding the mathematical background of graph neural networks and implementation
    for a regression problem in pytorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ns650?source=post_page---byline--c9d83b71c041--------------------------------)[![Najib
    Sharifi, Ph.D.](../Images/d94932c5e3633e32247d98a3c221b181.png)](https://medium.com/@ns650?source=post_page---byline--c9d83b71c041--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c9d83b71c041--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c9d83b71c041--------------------------------)
    [Najib Sharifi, Ph.D.](https://medium.com/@ns650?source=post_page---byline--c9d83b71c041--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c9d83b71c041--------------------------------)
    ·12 min read·Mar 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interconnected graphical data is all around us, ranging from molecular structures
    to social networks and design structures of cities. Graph Neural Networks (GNNs)
    are emerging as a powerful method of modelling and learning the spatial and graphical
    structure of such data. It has been applied to protein structures and other molecular
    applications such as drug discovery as well as modelling systems such as social
    networks. Recently the standard GNN has been combined with ideas from other ML
    models to develop exciting innovative applications. One such development is the
    integration of GNN with sequential models — Spatio-Temporal GNN that is able to
    capture both the temporal and spatial (hence the name) dependences of data, this
    alone could be applied to a number of challenges/problems in industry/research.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the exciting developments in GNN, there are very few resources on the
    topic which makes it inaccessible to many. In this short article, I want to provide
    a brief introduction to GNN covering both the mathematical description as well
    as a regression problem using the pytorch library. By unraveling the principles
    behind GNNs, we unlock a deeper comprehension of their capabilities and applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mathematical Description of GNNs**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A graph G can be defined as G = (V, E), where V is the set of nodes, and E are
    the edges between them. A graph is often represented by an adjacency matrix, A,
    which represents the presence or absence of edges between nodes i.e. aij takes
    values of 1 to indicate an edge (connection) between nodes i and j or 0 otherwise.
    If a graph has n nodes, A has a dimension of (n × n). The adjacency matrix is
    demonstrated in *Figure 1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1cd932311a015e28e70b76c55cf5f74.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Adjacency matrix for three different graphs
  prefs: []
  type: TYPE_NORMAL
- en: Each node (and edges! But we’ll come back to this later for simplicity) will
    have a set of features (e.g. if the node is a person, the features will be age,
    gender, height, job etc). If each node has f features, then the feature matrix
    X is (n × f). In some problems, each node may also have a target label which maybe
    a set of categorical labels or numerical values (shown in *Figure 2*).
  prefs: []
  type: TYPE_NORMAL
- en: '**Single Node Calculations**'
  prefs: []
  type: TYPE_NORMAL
- en: To learn the interdependence between any node and its neighbours, we need to
    consider the features of its neighbours. This is what enables GNNs to learn the
    structural representation of the data through a graph. Consider a node j with
    Nj neighbours, GNNs transform the features from each neighbour, aggregate them
    and then update node i’s feature space. Each of these steps are described as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0858dd5d95be8a673a8737ac19200814.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. A schematic of a node (j) with features xj and label yj and neighbour
    nodes (i, 2, 3), each with their on feature embeddings and corresponding label.
  prefs: []
  type: TYPE_NORMAL
- en: Neighbour feature transformation could be done a number of ways such as passing
    through an MLP network or by linear transformation such as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db48d782c7e6519c78c2b446c9387fb5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where w and b represent the weights and bias of the transformation. Information
    aggregation, the information from each neighboring nodes are then aggregated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e43f038327837089cafa9a3abafc5bf4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The nature of the aggregation step could be a number of different methods such
    as summation, averaging, min/max pooling and concatenation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f59a108b5b916e5cbaef95e8541114e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Following the aggregation step, the final step is to update node j:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/013599689b3480c1ca7ea900b40e5cbf.png)'
  prefs: []
  type: TYPE_IMG
- en: This updated could be done using MLP with the concatenated node features and
    neighbour information aggregation (mj) or we could use linear transformation i.e.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3fa2a673ce468e1b1ea4a29c32af8aff.png)'
  prefs: []
  type: TYPE_IMG
- en: Where U is a learnable weights matrix that combines the original node features
    (xj) with aggregated neighbour features (mj) through a non-linear activation function
    (ReLU in this case). This is it for the process of updating a single node in a
    single layer, the same process is applied to all other nodes in the graph, mathematically,
    this can be presented using the Adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**Graph Level Calculation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a graph with n nodes and each node has *f* features, we can concatenate
    all the features in a single matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9498e7314928b00500195a4c89693cb8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The neighbour feature transformation and aggregation steps can therefore be
    written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/630123572a8f0bc225a7721705bb0c4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where I is the identity matrix, this helps to include the each nodes own features
    too, otherwise, we are only considering the transformed features from the node
    j’s neighbours and not it’s own features. One final step is to normalise each
    node based on the number of connections i.e. for node j with Nj connections, the
    feature transformation can be done as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c08e50dcb433ee0320f2e71594840e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The equation above can be adjusted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f92e5051e1e2e2d6438067a44dff65a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Where D is the degree matrix, a diagonal matrix of number of connections for
    each node. However, more commonly, this normalisation step is done as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39326a6cee6a5013846a077ad197452b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the graph convolution network (GCN) method that enables GNN to learn
    the structure and relationship between nodes. However, an issue with GCN is that
    the weight vector for neighbour feature transformation is shared across all neighbours
    i.e. all neighbours are considered equal, but this is usually not the case so
    not a good representative of real systems. To address is, graph attention network
    (GATs) can be used to compute the importance of a neighbour’s feature to the target
    node, allowing the different neighbours to contribute differently to the feature
    update of the target node based on their relevance. The attention coefficients
    are determined using a learnable matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d2f4ddfbeb15c5f5c453135d44fdda98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where W is the shared learnable feature linear transformation, Wa is a learnable
    weight vector and eij is the raw attention score indicating importance of node
    i’s features to node j. The attention score is normalised using the SoftMax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6af7ddab8bee47d14788676c8a01cf14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now the feature aggregation step can be calculated using the attention coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a40d22ae79e2116e75022bfdf4fc1ad.png)'
  prefs: []
  type: TYPE_IMG
- en: This is it for a single layer, we can build multiple layers to increase the
    complexity of the model, this is demonstrated in *Figure 3*. Increasing the number
    of layers will allow the model to learn more global features and also capture
    more complex relationships, however, it is also likely to overfit so regularisation
    techniques should always be used to prevent this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/781e51a407ec5c36ad5efd87597a4996.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. An illustration of a multilayered GNN model
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, once final feature vectors for all nodes are obtained from the network,
    a feature matrix, H can be formed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/849a73048c3e915b58d2294221aa16f7.png)'
  prefs: []
  type: TYPE_IMG
- en: This feature matrix can be used to do a number of tasks e.g. node or graph classification.
    This brings us to the end of introduction into the mathematical description of
    GCN/GATs.
  prefs: []
  type: TYPE_NORMAL
- en: '**GCN Regression Example**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s implement a regression example where the aim is to train a network to
    predict the value of a node given the value of all other nodes i.e. each node
    has a single feature (which is a scalar value). The aim of this example is to
    leverage the inherent relational information encoded in the graph to accurately
    predict numerical values for each node. The key thing to note is that we input
    the numerical value for all nodes except the target node (we mask the target node
    value with 0) then predict the target node’s value. For each data point, we repeat
    the process for all nodes. Perhaps this might come across as a bizarre task but
    lets see if we can predict the expected value of any node given the values of
    the other nodes. The data used is the corresponding simulation data to a series
    of sensors from industry and the graph structure I have chosen in the example
    below is based on the actual process structure. I have provided comments in the
    code to make it easy to follow. You can find a copy of the dataset [here](https://github.com/Nsharifi650/GNNRegression.git)
    (Note: this is my own data, generated from simulations).'
  prefs: []
  type: TYPE_NORMAL
- en: This code and training procedure is far from being optimised but it’s aim is
    to illustrate the implementation of GNNs and get an intuition for how they work.
    An issue with the currently way I have done that should definitely not be done
    this way beyond learning purposes is the masking of node feature value and predicting
    it from the neighbours feature. Currently you’d have to loop over each node (not
    very efficient), a much better way to do is the stop the model from include it’s
    own features in the aggregation step and hence you wouldn’t need to do one node
    at a time but I thought it is easier to build intuition for the model with the
    current method:)
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing Data**'
  prefs: []
  type: TYPE_NORMAL
- en: Importing the necessary libraries and Sensor data from CSV file. Normalise all
    data in the range of 0 to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Defining the connectivity (edge index) between nodes in the graph using a PyTorch
    tensor — i.e. this provides the system’s graphical topology.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Data imported from csv has a tabular structure but to use this in GNNs,
    it must be transformed to a graphical structure. Each row of data (one observation)
    is represented as one graph. Iterate through Each Row to Create Graphical representation
    of the data
  prefs: []
  type: TYPE_NORMAL
- en: A mask is created for each node/sensor to indicate the presence (1) or absence
    (0) of data, allowing for flexibility in handling missing data. In most systems,
    there may be items with no data available hence the need for flexibility in handling
    missing data. Split the data into training and testing sets
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Graph Visualisation**'
  prefs: []
  type: TYPE_NORMAL
- en: The graph structure created above using the edge indices can be visualised using
    networkx.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Model Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s define the model. The model incorporates 2 GAT convolutional layers. The
    first layer transforms node features to an 8 dimensional space, and the second
    GAT layer further reduces this to an 8-dimensional representation.
  prefs: []
  type: TYPE_NORMAL
- en: GNNs are highly susceptible to overfitting, regularation (dropout) is applied
    after each GAT layer with a user defined probability to prevent over fitting.
    The dropout layer essentially randomly zeros some of the elements of the input
    tensor during training.
  prefs: []
  type: TYPE_NORMAL
- en: The GAT convolution layer output results are passed through a fully connected
    (linear) layer to map the 8-dimensional output to the final node feature which
    in this case is a scalar value per node.
  prefs: []
  type: TYPE_NORMAL
- en: Masking the value of the target Node; as mentioned earlier, the aim of this
    of task is to regress the value of the target node based on the value of it’s
    neighbours. This is the reason behind masking/replacing the target node’s value
    with zero.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Training the model**'
  prefs: []
  type: TYPE_NORMAL
- en: Initialising the model and defining the optimiser, loss function and the hyper
    parameters including learning rate, weight decay (for regularisation), batch_size
    and number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The training process is fairly standard, each graph (one data point) of data
    is passed through the forward pass of the model (iterating over each node and
    predicting the target node. The loss from the prediction is accumulated over the
    defined batch size before updating the GNN through backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Testing the trained model**'
  prefs: []
  type: TYPE_NORMAL
- en: Using the test dataset, pass each graph through the forward pass of the trained
    model and predict each node’s value based on it’s neighbours value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Visualising the test results**'
  prefs: []
  type: TYPE_NORMAL
- en: Using iplot we can visualise the predicted values of nodes against the ground
    truth values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cf42365780e85ca2cddbafe8d8993e00.png)'
  prefs: []
  type: TYPE_IMG
- en: Despite a lack of fine tuning the model architecture or hyperparameters, it
    has done a decent job actually, we could tune the model further to get improved
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the end of this article. GNNs are relatively newer than other
    branches of machine learning, it will be very exciting to see the developments
    of this field but also it’s application to different problems. Finally, thank
    you for taking the time to read this article, I hope you found it useful in your
    understanding of GNNs or their mathematical background.
  prefs: []
  type: TYPE_NORMAL
- en: '**Before you go**'
  prefs: []
  type: TYPE_NORMAL
- en: Personally I really enjoy spending time to learn new concepts and apply those
    concepts to new problems and challenges and I am sure most of the people reading
    these kind of articles do too. I believe its a privilege to have the opportunity
    to do that, a privilege that everyone should have but not everyone does. We all
    have a responsibility to change that towards a brighter future for everyone. Please
    consider donating to UniArk ([UniArk.org)](http://uniark.org) to help talented
    students from a demography in the world so often overlooked by universities and
    countries — the persecuted minority (ethnic, religious or otherwise). UniArk searches
    the furthest and deepest to find talent and potential — the remote areas of developing
    countries. Your donation would be the beacon of hope for someone from an oppressive
    society. I hope you can help UniArk keep this beacon alight.
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author*'
  prefs: []
  type: TYPE_NORMAL
