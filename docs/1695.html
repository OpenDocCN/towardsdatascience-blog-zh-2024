<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Prompt Engineering for Cognitive Flexibility</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Prompt Engineering for Cognitive Flexibility</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/prompt-engineering-for-cognitive-flexibility-44e490e3473d?source=collection_archive---------4-----------------------#2024-07-11">https://towardsdatascience.com/prompt-engineering-for-cognitive-flexibility-44e490e3473d?source=collection_archive---------4-----------------------#2024-07-11</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1c7a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Practical insights and analysis from experiments with MMLU-Pro</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@hominum_universalis?source=post_page---byline--44e490e3473d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Giuseppe Scalamogna" class="l ep by dd de cx" src="../Images/ff7b3bec7c26e5684fba26211b6f027a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*-G8aVXnU_tDQVRDkJ4PtCA@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--44e490e3473d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@hominum_universalis?source=post_page---byline--44e490e3473d--------------------------------" rel="noopener follow">Giuseppe Scalamogna</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--44e490e3473d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 11, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/436b39e193a137a79ca30655f218133f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*haACvh59_Lmra5EBopkvSg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Source: Image by Author and generated with MidJourney</figcaption></figure><h2 id="9727" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Introduction</h2><p id="2822" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">Developing AI agents that can, among other things, think, plan and decide with human-like proficiency is a prominent area of current research and discussion. For now, LLMs have taken the lead as the foundational building block for these agents. As we pursue ever increasingly complex capabilities, regardless of the LLM(s) being utilized, we inevitably encounter the same types of questions over and over, including:</p><ol class=""><li id="90bc" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os oy oz pa bk">Does the model have the necessary knowledge to complete a task accurately and efficiently?</li><li id="f4e8" class="oa ob fq oc b go pb oe of gr pc oh oi nn pd ok ol nr pe on oo nv pf oq or os oy oz pa bk">If the appropriate knowledge is available, how do we reliably activate it?</li><li id="e344" class="oa ob fq oc b go pb oe of gr pc oh oi nn pd ok ol nr pe on oo nv pf oq or os oy oz pa bk">Is the model capable of imitating complex cognitive behavior such as reasoning, planning and decision making to an acceptable level of proficiency?</li></ol><p id="cd7b" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">This article explores these questions through the lens of a recent mini-experiment I conducted that leverages the latest <a class="af pg" href="https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro" rel="noopener ugc nofollow" target="_blank">MMLU-Pro</a> benchmark. The findings lead to some interesting insights around cognitive flexibility and how we might apply this concept from cognitive science to our AI agent and prompt engineering efforts.</p><h2 id="868b" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk"><strong class="al">Background</strong></h2><p id="7023" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk"><strong class="oc fr">MMLU-Pro — A multiple choice gauntlet</strong></p><p id="154d" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The recently released MMLU-Pro (Massive Multitask Language Understanding) benchmark tests the capability boundaries of AI models by presenting a more robust and challenging set of tasks compared to its predecessor, MMLU [1]. The goal was to create a comprehensive evaluation covering a diverse array of subjects, requiring models to possess a broad base of knowledge and demonstrate the ability to apply it in varied contexts. To this end, MMLU-Pro tests models against very challenging, reasoning-oriented multiple-choice questions spread across 14 different knowledge domains.</p><p id="a0cb" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">We are all quite familiar with multiple-choice exams from our own academic journeys. The strategies we use on these types of tests often involve a combination of reasoning, problem solving, recall, elimination, inference, and educated guessing. Our ability to switch seamlessly between these strategies in underpinned by cognitive flexibility, which we employ to adapt our approach to the demands of each specific question.</p><p id="0060" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Cognitive flexibility encompasses mental capabilities such as switching between different concepts and thinking about multiple concepts simultaneously. It enables us to adapt our thinking in response to the situation at hand. Is this concept potentially useful in our AI agent and prompt engineering efforts? Before we explore that, let’s examine a sample question from MMLU-Pro in the “business” category:</p><p id="a81d" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Question 205: If the annual earnings per share has mean $8.6 and standard deviation $3.4, what is the chance that an observed EPS less than $5.5?</p><p id="17ab" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Answers: A: 0.3571, B: 0.0625, C: 0.2345, D: 0.5000, E: 0.4112, F: 0.1814, G: 0.3035, H: 0.0923, I: 0.2756, J: 0.1587</p><p id="389e" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Although categorically labelled as ‘business’, this question requires knowledge of statistics. We need to standardize the value and calculate how many standard deviations it is away from the mean to get a probability estimate. This is done by calculating the Z-score as follows:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ph"><img src="../Images/4861a4ae9b88f0ec3883ab3eb13c8381.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*8kySMxB4poxfiNsTqdHuPQ.png"/></div></figure><p id="8049" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Where:</p><p id="4821" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">X is the value in question ($5.50 in this case)</p><p id="0eee" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">μ​ is the mean (given as $8.6).</p><p id="f7f9" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">σ is the standard deviation (given as $3.4)</p><p id="db45" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">If we substitute those values into the formula we get -0.09118. We then consult the standard normal distribution table and find that the probability of Z being less than -0.9118 is approximately 18.14% which correspond to answer “F” from our choices.</p><p id="6f74" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">I think it is safe to say that this is a non-trivial problem for an LLM to solve. The correct answer cannot be memorized and needs to be calculated. Would an LLM have the knowledge and cognitive flexibility required to solve this type of problem? What prompt engineering strategies might we employ?</p><p id="b0c7" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">Prompt Engineering to the Rescue</strong></p><p id="c121" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">In approaching the above problem with an LLM, we might consider: does our chosen model have the knowledge of statistics needed? Assuming it does, how do we reliably activate the knowledge around standard normal distributions? And finally, can the model imitate the mathematical reasoning steps to arrive at the correct answer?</p><p id="7ea1" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The widely known “Chain-of-Thought” (CoT) prompt engineering strategy seems like a natural fit. The strategy relies on prompting the model to generate intermediate reasoning steps before arriving at the final answer. There are two basic approaches.</p><p id="8e45" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">Chain-of-Thought (CoT)</strong>: Involves few-shot prompting, where examples of the reasoning process are provided to guide the model [2].</p><p id="2bc1" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">Zero-Shot Chain-of-Thought (Zero-Shot CoT)</strong>: Involves prompting the model to generate reasoning steps without prior examples, often using phrases like “Let’s think step by step” [3].</p><p id="47cb" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">There are numerous other strategies, generally relying on a combination of pre-generation feature activation, i.e. focusing on activating knowledge in the initial prompt, and intra-generation feature activation, i.e. focusing on the LLM dynamically activating knowledge as it generates its output token by token.</p><h2 id="8c53" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk"><strong class="al">Mini-Experiment</strong></h2><p id="85ad" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk"><strong class="oc fr">Experiment Design</strong></p><p id="5f2c" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">In designing the mini-experiment, I utilized ChatGPT-4o and randomly sampled 10 questions from each of the 14 knowledge domains in the MMLU-Pro data set. The experiment aimed to evaluate two main aspects:</p><ol class=""><li id="040e" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os oy oz pa bk"><strong class="oc fr">Effectiveness of different prompt engineering techniques:</strong> Specifically, the impact of using different techniques for activating the necessary knowledge and desired behavior in the model. The techniques were selected to align with varying degrees of cognitive flexibility and were all zero-shot based.</li><li id="9fe7" class="oa ob fq oc b go pb oe of gr pc oh oi nn pd ok ol nr pe on oo nv pf oq or os oy oz pa bk"><strong class="oc fr">The impact from deliberately limiting reasoning and cognitive flexibility:</strong> Specifically, how limiting the model’s ability to reason openly (and by consequence severely limiting cognitive flexibility) affects accuracy.</li></ol><p id="94eb" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The different prompt techniques tested relied on the following templates:</p><p id="27a6" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">Direct Question</strong> — {Question}. Select the correct answer from the following answer choices: {Answers}. Respond with the letter and answer selected.</p><p id="331a" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">CoT </strong>— {Question}. Let’s think step by step and select the correct answer from the following answer choices: {Answers}. Respond with the letter and answer selected.</p><p id="633f" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">Knowledge Domain Activation</strong> — {Question}. Let’s think about the knowledge and concepts needed and select the correct answer from the following answer choices: {Answers}. Respond with the letter and answer selected.</p><p id="e06d" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">Contextual Scaffolds</strong> — {Question}. My expectations are that you will answer the question correctly. Create an operational context for yourself to maximize fulfillment of my expectations and select the correct answer from the following answer choices: {Answers}. Respond with the letter and answer selected. [4]</p><p id="e6a9" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The Direct Question approach served as the baseline, likely enabling the highest degree of cognitive flexibility from the model. CoT would likely lead to the least amount of cognitive flexibility as the model is instructed to proceed step-by-step. Knowledge Domain Activation and Contextual Scaffolds fall somewhere between Direct Question and CoT.</p><p id="d82b" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Deliberately constraining reasoning was accomplished by taking the last line of the above prompt templates, i.e. “Respond with the letter and answer selected.” and specifying instead “Respond <em class="pi">only</em> with the letter and answer selected <em class="pi">and nothing else</em>.”</p><p id="8f91" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">If you are interested in the code I used to run the experiment and results, they can be found in this GitHub repo linked <a class="af pg" href="https://github.com/gfranco78/cognitive-flex-exp" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="6200" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">Results</strong></p><p id="d441" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Here are the results for the different prompt techniques and their reasoning constrained variants:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pj"><img src="../Images/c292a17494973e36fe9755fcc37c890c.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*ASkefhlerZauVQT_dIdX0g.png"/></div></figure><p id="a84f" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">All unconstrained reasoning prompts performed comparably, with the Direct Question approach performing slightly better than the others. This was a bit of a surprise since the MMLU-Pro paper [1] reports significant underperformance in direct question and strong performance gains with few-shot CoT. I won’t dwell on the discrepancy here since the purpose of the mini-experiment was not to replicate their setup.</p><p id="2feb" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">More importantly for this mini-experiment, when reasoning was deliberately constrained, all techniques showed a comparable decline in accuracy, dropping from an average of 66% to 51%. This result is along the lines of what we expected. The more pertinent observation is that none of the techniques were successful in enhancing pre-generation knowledge activation beyond what would occur with Direct Question where pre-generation feature activation primarily occurs from the model being exposed to the text in the question and answer choices.</p><p id="2133" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The overall conclusion from these high-level results suggests that an optimal combination for prompt engineering effectiveness may very well involve:</p><ol class=""><li id="94f9" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os oy oz pa bk">Allowing the model to exercise some degree of cognitive flexibility as exemplified best in the Direct Question approach.</li><li id="fca1" class="oa ob fq oc b go pb oe of gr pc oh oi nn pd ok ol nr pe on oo nv pf oq or os oy oz pa bk">Allowing the model to reason openly such that the reasoning traces are an active part of the generation.</li></ol><p id="ea2d" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">The Compute Cost Dimension</strong></p><p id="b523" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Although not often discussed, token efficiency is becoming more and more important as LLMs find their way into varied industry use cases. The graph below shows the accuracy of each unconstrained prompt technique versus the average tokens generated in the answer.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pk"><img src="../Images/e2a62bbd03388c9ceb1c42210c2f8f4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kedb4vZVv3J-5y-oz7hGfg.png"/></div></div></figure><p id="f543" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">While the accuracy differential is not the primary focus, the efficiency of the Direct Question approach, generating an average of 180 tokens per answer, is notable compared to CoT, which produced approximately 339 tokens per answer (i.e. 88% more). Since accuracy was comparable, it leads us to speculate that CoT is on average less efficient compared to the other strategies when it comes to intra-generation knowledge activation, producing excessively verbose results. But what drove the excessive verbosity? To try and answer this it was helpful to examine the unconstrained reasoning prompts and number of times the model chose to answer with just the answer and no reasoning trace, even if not explicitly instructed to do so. The results were as follows:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pl"><img src="../Images/73ed58b64d7088592a93ed2d09a568e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*46I3efvRCKjNZd9x6bkNPw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">% of instances where only the answer was generated even if not strictly specified to do so</figcaption></figure><p id="783e" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">What was even more interesting was accuracy when the model chose to answer directly without any reasoning trace which is shown in the following table:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pm"><img src="../Images/51f7f765a6f040e10218fa1ad8c19735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*nBcDXISIjEuxX6XtALVptg.png"/></div></figure><p id="a24c" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The accuracy ranged from 64% to 70% without any reasoning traces being generated. Even with MMLU-Pro questions that are purposely designed to require reasoning and problem solving, when not over-constrained by the prompt, the model appears to demonstrate somethin akin to selecting different strategies based on the specific question at hand.</p><p id="3268" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">Practical Implications</strong></p><p id="0924" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The practical takeaway from these results is that straightforward prompt strategies can often be just as effective as overly structured ones. While CoT aims to simulate reasoning by inducing specific feature activations that are reasoning oriented, it may not always be necessary or optimal especially if excess token generation is a concern. Striving instead to allow the model to exercise cognitive flexibility can be a potentially more suitable approach.</p><h2 id="1e70" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk"><strong class="al">Conclusion: Paving the Way for Cognitive Flexibility in AI Agents</strong></h2><p id="8d72" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">The findings from this mini-experiment offer compelling insights into the importance of cognitive flexibility in LLMs and AI Agents. In human cognition, cognitive flexibility refers to the ability to adapt thinking and behavior in response to changing tasks or demands. It involves switching between different concepts, maintaining multiple concepts simultaneously, and shifting attention as needed. In the context of LLMs, it can be understood as the model’s ability to dynamically adjust its internal activations in response to textual stimuli.</p><p id="df9c" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Continued focus on developing technologies and techniques in this area could result in significant enhancements to AI agent proficiency in a variety of complex task settings. For example, exploring the idea alongside other insights such as those surfaced by Anthropic in their recent paper “<a class="af pg" href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html" rel="noopener ugc nofollow" target="_blank">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a>,” could yield techniques that unlock the ability to dynamically observe and tailor the level of cognitive flexibility employed based on the task’s complexity and domain.</p><p id="4b06" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">As we push the boundaries of AI, cognitive flexibility will likely be key to creating models that not only perform reliably but also understand and adapt to the complexities of the real world.</p><p id="6c9a" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Thanks for reading and follow me for insights that result from future explorations connected to this work. If you would like to discuss do not hesitate to connect with me on <a class="af pg" href="https://www.linkedin.com/in/giuseppe-scalamogna-8b389145/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>.</p><p id="6866" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Unless otherwise noted, all images in this article are by the author.</p><p id="e532" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">References:</strong></p><p id="9311" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">[1] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen: MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark. <a class="af pg" href="https://arxiv.org/abs/2406.01574" rel="noopener ugc nofollow" target="_blank"><em class="pi">arXiv:2406.01574</em></a><em class="pi">, 2024</em></p><p id="6fb4" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">[2] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. <a class="af pg" href="https://arxiv.org/abs/2201.11903v6" rel="noopener ugc nofollow" target="_blank"><em class="pi">arXiv:2201.11903v6</em></a><em class="pi"> , 2023</em></p><p id="96ea" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">[3] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa: Large Language Models are Zero-Shot Reasoners. <a class="af pg" href="https://arxiv.org/abs/2205.11916v4" rel="noopener ugc nofollow" target="_blank"><em class="pi">arXiv:2205.11916v4</em></a>, <em class="pi">2023</em></p><p id="cbf8" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">[4] Giuseppe Scalamogna, A Universal Roadmap for Prompt Engineering: The Contextual Scaffolds Framework (CSF), <a class="af pg" href="https://medium.com/towards-data-science/a-universal-roadmap-for-prompt-engineering-the-contextual-scaffolds-framework-csf-fdaf5a9fa86a" rel="noopener"><em class="pi">https://medium.com/towards-data-science/a-universal-roadmap-for-prompt-engineering-the-contextual-scaffolds-framework-csf-fdaf5a9fa86a</em></a><em class="pi">, 2023</em></p></div></div></div></div>    
</body>
</html>