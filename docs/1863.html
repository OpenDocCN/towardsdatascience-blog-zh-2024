<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Handling Feedback Loops in Recommender Systems — Deep Bayesian Bandits</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Handling Feedback Loops in Recommender Systems — Deep Bayesian Bandits</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/handling-feedback-loops-in-recommender-systems-deep-bayesian-bandits-e83f34e2566a?source=collection_archive---------6-----------------------#2024-07-31">https://towardsdatascience.com/handling-feedback-loops-in-recommender-systems-deep-bayesian-bandits-e83f34e2566a?source=collection_archive---------6-----------------------#2024-07-31</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="35e1" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Understanding fundamentals of exploration and Deep Bayesian Bandits to tackle feedback loops in recommender systems</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sachinhosmani?source=post_page---byline--e83f34e2566a--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sachin Hosmani" class="l ep by dd de cx" src="../Images/d32632e6175883b8ffcf8dd7b10f25c3.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*viy02kBkk0pgj2LFHRxVHA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e83f34e2566a--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@sachinhosmani?source=post_page---byline--e83f34e2566a--------------------------------" rel="noopener follow">Sachin Hosmani</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e83f34e2566a--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 31, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/1c9bffad3719c1dd1d0b753b0b3477d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JfzOcBq_2HMkNNWnyvolsQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image from ChatGPT-4o</figcaption></figure><h1 id="f31f" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Introduction</h1><p id="a628" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Recommender system models are typically trained to optimize for user engagement like clicks and purchases. The well-meaning intention behind this is to favor items that the user has previously engaged with. However, this creates a feedback loop that over time can manifest as the “cold start problem”. Simply put, the items that have historically been popular for a user tend to continue to be favored by the model. In contrast, new but highly relevant items don’t receive much exposure. In this article, I introduce exploration techniques from the basics and ultimately explain Deep Bayesian Bandits, a highly-effective algorithm described in a <a class="af ou" href="https://arxiv.org/abs/2008.00727" rel="noopener ugc nofollow" target="_blank">paper</a> by Guo, Dalin, et al [1].</p><h1 id="a84c" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">An ad recommender system</h1><p id="289c" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Let us use a simple ad recommender system as an example throughout this article.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ov"><img src="../Images/c41c07cace57419dd1e44597f074e5d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*-CrOCqEXzzAGh5LWKCW1og.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A simple three-component ad recommender system. Image by author</figcaption></figure><p id="069a" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">It is a three-component system</p><ul class=""><li id="d337" class="ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot pb pc pd bk"><strong class="oa fr">Retrieval</strong>: a component to efficiently retrieve candidates for ranking</li><li id="8ce4" class="ny nz fq oa b go pe oc od gr pf of og oh pg oj ok ol ph on oo op pi or os ot pb pc pd bk"><strong class="oa fr">Ranking</strong>: a deep neural network that predicts the click-through rate (CTR) as the score for an ad given a user<br/><code class="cx pj pk pl pm b">score = predict_ctr(user features, ad features)</code></li><li id="a732" class="ny nz fq oa b go pe oc od gr pf of og oh pg oj ok ol ph on oo op pi or os ot pb pc pd bk"><strong class="oa fr">Auction</strong>: a component that<br/>- retrieves candidate ads for the user<br/>- scores them using the ranking model<br/>- selects the highest-scored ad and returns it*</li></ul><p id="ba03" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">Our focus in this article will be exclusively on the ranking model.</p><p id="47c4" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk"><em class="pn">*real-world auction systems also take the ad’s bid amount into account, but we ignore that for simplicity</em></p><h2 id="ace2" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">Ranking model architecture</h2><p id="bc7a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The ranking model is a deep neural network that predicts the click-through rate (CTR) of an ad, given the user and ad features. For simplicity, I propose a simple fully connected DNN below, but one could very well enrich it with techniques like wide-and-deep network, DCN, and DeepFM without any loss of applicability of the methods I explain in this article.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/7337ff75e80fb643aa08375d018d36ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iz8__wd_BkiKU1zUWAMm2A.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A binary classifier deep neural network that predicts pCTR. Image by author</figcaption></figure><h2 id="b4b4" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">Training data</h2><p id="82c5" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The ranking model is trained on data that comprises clicks as binary labels and, concatenation of user and ad features. The exact set of features used is unimportant to this article, but I have assumed that some advertiser brand-related features are present to help the model learn the user’s affinity towards brands.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qg"><img src="../Images/0a80b00a3ecdd55e344fd7c219350e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*oztlUEdEYhtPlRCeVV9ugA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Training data with sample features. Image by author</figcaption></figure><h1 id="3a17" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">The cold-start problem</h1><p id="3851" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Imagine we successfully trained our ranking model on our ads click dataset, and the model has learned that one of our users Jane loves buying bags from the bag company “Vogue Voyage”. But there is a new bag company “Radiant Clutch” in the market and they sell great bags. However, despite “Radiant Clutch” running ad campaigns to reach users like Jane, Jane never sees their ads. This is because our ranking model has so firmly learned that Jane likes bags from “Vogue Voyage”, that only their ads are shown to her. She sometimes clicks on them and when the model is further trained on these new clicks, it only strengthens the model’s belief. This becomes a vicious cycle leading to some items remaining in the dark.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qh"><img src="../Images/57214dd9d1a58c9b133183f099eb4389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PHz417dasvkNkhEpFLr6tw.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The feedback loop in action, causing the cold-start problem: bags from Radiant Clutch don’t stand a chance. Image by author, thumbnails generated with ChatGPT-4o</figcaption></figure><p id="dd32" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">If we ponder about this, we’d realize that the model did not do anything wrong by learning that Jane likes bags from “Vogue Voyage”. But the problem is simply that the model is not being given a chance to learn about Jane’s interests in other companies’ bags.</p><h2 id="e0a2" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">Exploration vs exploitation</h2><p id="cff1" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">This is a great time to introduce the trade-off between exploration vs exploitation.</p><p id="6f1b" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk"><strong class="oa fr">Exploitation</strong>: During ad auction, once we get our CTR predictions from our ranking model, we simply select the ad with the highest score. This is a 100% exploitation strategy because we are completely acting on our current best knowledge to achieve the greatest immediate reward.</p><p id="44e6" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk"><strong class="oa fr">Exploration</strong>: What our approach has been lacking is the willingness to take some risk and show an ad even if it wasn’t assigned the highest score. If we did that, the user might click on it and the ranking model when updated on this data would learn something new about it. But if we never take the risk, the model will never learn anything new. This is the motivation behind exploration.</p><p id="46fd" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">Exploration vs exploitation is a balancing act. Too little exploration would leave us with the cold-start problem and too much exploration would risk showing highly irrelevant ads to users, thus losing user trust and money.</p><h1 id="95c5" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Exploration techniques</h1><p id="9dc2" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Now that we’ve set the stage for exploration, let us delve into some concrete techniques for controlled exploration.</p><h2 id="307d" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">ε-greedy policy</h2><p id="4f60" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The idea here is simple. In our auction service, when we have the scores for all the candidate ads, instead of just taking the top-scored ad, we do the following</p><ol class=""><li id="6ee1" class="ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot qi pc pd bk">select a random number r in [0, 1)</li><li id="5644" class="ny nz fq oa b go pe oc od gr pf of og oh pg oj ok ol ph on oo op pi or os ot qi pc pd bk">if r &lt; ε, select a random ad from our candidates (exploration)</li><li id="d5e4" class="ny nz fq oa b go pe oc od gr pf of og oh pg oj ok ol ph on oo op pi or os ot qi pc pd bk">else, select the top-scored ad (exploitation)</li></ol><p id="bb10" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">where ε is a constant that we carefully select in [0, 1) knowing that the algorithm will explore with ε probability and exploit with 1 — ε probability.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qj"><img src="../Images/3f247a0cc0bcb07442bdfaa118784004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_vx89Y_DEenXG5LEPFf-XQ.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Exploration with ε probability: pick any candidate ad at random. Image by author</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qk"><img src="../Images/f2bc4f67231e97099840ba606aba9311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mQostXE8I2EcahTmFBsyVQ.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Exploitation with 1 — ε probability: pick the highest CTR ad. Image by author</figcaption></figure><p id="e38d" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">This is a very simple yet powerful technique. However, it can be too naive because when it explores, it <strong class="oa fr">completely randomly</strong> selects an ad. Even if an ad has an absurdly low pCTR prediction that the user has repeatedly disliked in the past, we might still show the ad. This can be a bit harsh and can lead to a serious loss in revenue and user trust. We can certainly do better.</p><h2 id="75fc" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">Upper confidence bound (UCB)</h2><p id="c0b8" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Our motivation for exploration was to ensure that all ad candidates have an opportunity to be shown to the user. But as we give some exposure to an ad, if the user still doesn’t engage with it, it becomes prudent to cut future exposure to it. So, we need a mechanism by which we select the ad based on both its score estimate and also the amount of exposure it has already received.</p><p id="5c4f" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">Imagine our ranking model could produce not just the CTR score but also a confidence interval for it*.</p><p id="6e74" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk"><em class="pn">*how this is achieved is explained later in the article</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ql"><img src="../Images/249d31ed0d7a5cc98840265412dbd41a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G1066qvO7ZTiw_RUnANhBg.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The model predicts a confidence interval along with the score. Image by author</figcaption></figure><p id="b3be" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">Such a confidence interval is typically inversely proportional to the amount of exposure the ad has received because the more an ad is shown to the user, the more user feedback we have about it, which reduces the uncertainty interval.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qm"><img src="../Images/0a420875a92a76fc85f8ebc8ce683e3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_2CNUQmRSCGM3i1zbsBmmQ.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Increased exposure to an ad leads to a decrease in the confidence interval in the model’s score prediction. Image by author</figcaption></figure><p id="7f5f" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">During auction, instead of selecting the ad with the greatest pCTR, we select the ad with the highest upper confidence bound. This approach is called UCB. The philosophy here is “Optimism in the face of uncertainty”. This approach effectively takes into account both the ad’s score estimate and also the uncertainty around it.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qn"><img src="../Images/37c4d0861398242e13f207fca13acbaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eKiqqM5F9YcriB7bZI1HJg.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">UCB in action: Ad-1 wins auction at first on account of its large confidence interval, but as the model learns about it, its UCB falls leading to Ad-2 winning auction. Image by author</figcaption></figure><h2 id="86f0" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">Thompson sampling</h2><p id="a848" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The UCB approach went with the philosophy of “(complete) optimism in the face of uncertainty”. Thompson sampling softens this optimism a little. Instead of using the upper confidence bound as the score of an ad, why not sample a score in the posterior distribution?</p><p id="ed63" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">For this to be possible, imagine our ranking model could produce not just the CTR and the confidence interval but an actual score distribution*.</p><p id="1063" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk"><em class="pn">*how this is achieved is explained later in the article</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qo"><img src="../Images/748e502314178c5c76352adf69ca02f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LlXZva7n8gm2aov72Z_yWA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The model can predict a distribution of scores for one ad. Image by author</figcaption></figure><p id="43d5" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">Then, we just sample a score from this distribution and use that as the score during auction.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qp"><img src="../Images/423cd452e28b712a0e486725ab767961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZBxA8Mq94zrLpCFyOsqe9g.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Ad-1 wins auction due to a high sampled score from its wide distribution. Image by author</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qq"><img src="../Images/fa48fcf0ccee3990290847148543babc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rusoxiDwR87oBkWk7Xh1A.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Ad-1 has received exposure and the model has lesser uncertainty about it. Ad-2 wins auction due to its higher score distribution mass. Image by author</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qr"><img src="../Images/69793d78a025e54de269b45f81e10635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ieQSGSd3bgmTh92s2DjfJQ.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Ad-2’s score distribution stdev further shrinks as it gets more exposure. Image by author</figcaption></figure><h1 id="2ac1" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Importance of updating the model</h1><p id="313a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">For the UCB and Thompson sampling techniques to work, we must update our models as often as possible. Only then will it be able to update its uncertainty estimates in response to user feedback. The ideal setup is a <strong class="oa fr">continuous learning</strong> setup where user feedback events are sent in near-real time to the model to update its weights. However, periodically statefully updating the weights of the model is also a viable option if continuous learning infrastructure is too expensive to set up.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qs"><img src="../Images/f9491cf68aa60049e407d3c511c1e949.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kNLiNPylmgDrrBSfSLQjng.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A high-level continuous learning setup utilizing streaming infrastructure. Image by author, thumbnail generated by ChatGPT-4o</figcaption></figure><h1 id="f47c" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Posterior approximation techniques</h1><p id="2fdc" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In the UCB and Thompson sampling approaches, I explained the idea of our model producing not just one score but an uncertainty measure as well (either as a confidence interval or a distribution of scores). How can this be possible? Our DNN can produce just one output after all! Here are the approaches discussed in the paper.</p><h2 id="e2d9" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">Bootstrapping</h2><p id="3da1" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Bootstrapping in statistics simply means sampling with replacement. What this means for us is that we apply bootstrapping on our training dataset to create several closely related but slightly different datasets and train a separate model with each dataset. The models learned would thereby be slight variants of each other. If you have studied decision trees and bagging, you would already be familiar with the idea of training multiple related trees that are slight variants of each other.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qt"><img src="../Images/c67da12373ba080f7c7041586f32e942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZWVitg5n9wL2tYa7SdgdnQ.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Bootstrapped datasets are used to train separate models, resulting in a distribution of scores. Image by author</figcaption></figure><p id="a54e" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">During auction, for each ad, we get one score from each bootstrapped model. This gives us a distribution of scores which is exactly what we wanted for Thompson sampling. We can also extract a confidence interval from the distribution if we choose to use UCB.</p><p id="055c" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">The biggest drawback with this approach is the sheer computational and maintenance overhead of training and serving several models.</p><h2 id="fd6c" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">Multi-head bootstrapping</h2><p id="0f26" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">To mitigate the costs of several bootstrapped models, this approach unifies the several models into one multi-head model with one head for each output.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/6b2e9aa378f486bbc08c69b03f714da0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cp-MZj06PNno1Gugitcyow.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Multi-head model. Image by author</figcaption></figure><p id="3e65" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">The key cost reduction comes from the fact that all the layers except the last are shared.</p><p id="ddcb" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">Training is done as usual on bootstrapped subsets of data. While each bootstrapped subset of data should be used to update the weights of all the shared layers, care must be taken to update the weight of just one output head with a subset of data.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qv"><img src="../Images/280120ebb90c3e6dd641a1434a0cae09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zUnGseRc44LK08cxK20nQA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Constrained influence of each bootstrapped subset of data on one head during backprop. Image by author</figcaption></figure><h2 id="7546" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">Stochastic Gradient descent (SGD)</h2><p id="c1d2" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Instead of using separate bootstrapped datasets to train different models, we can just use one dataset, but train each model with SGD with random weight initialization thus utilizing the inherent stochasticity offered by SGD. Each model trained thus becomes a variant of the other.</p><h2 id="63c9" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">Multi-head SGD</h2><p id="5cda" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In the same way, using a multi-head architecture brought down the number of models trained with bootstrapping to one, we can use a multi-head architecture with SGD. We just have to randomly initialize the weights at each head so that upon training on the whole dataset, each head is learned to be a slight variant of the others.</p><h2 id="7b02" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">Forward-propagation dropout</h2><p id="2bab" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Dropout is a well-known regularization technique where during model training, some of the nodes of a layer are randomly dropped to prevent chances of overfitting. We borrow the same idea here except that we use it during forward propagation to create controlled randomness.</p><p id="dc95" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">We modify our ranking model’s last layer to introduce dropout. Then, when we want to score an ad, we pass it through the model several times, each time getting a slightly different score on account of the randomness introduced by dropout. This gives us the distribution and confidence interval that we seek.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qw"><img src="../Images/21ed429f2fef679f5290d50fe2bac778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6rsZhdn-ULaYY-e-c_mN0w.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The same model produces a distribution of scores through random dropout. Image by author</figcaption></figure><p id="9996" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">One significant disadvantage of this approach is that it requires several full forward passes through the network which can be quite costly during inference time.</p><h2 id="03df" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">Hybrid</h2><p id="e79d" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In the hybrid approach, we perform a key optimization to give us the advantages of dropout and bootstrapping while bringing down the serving and training costs:</p><ul class=""><li id="b9da" class="ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot pb pc pd bk">With dropout applied to just the last-but-one layer, we don’t have to run a full forward pass several times to generate our score distribution. We can do one forward pass until the dropout layer and then do several invocations of just the dropout layer in parallel. This gives us the same effect as the multi-head model where each dropout output acts like a multi-head output.</li></ul><p id="e7c3" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">Also, with dropout deactivating one or more nodes randomly, it serves as a Bernoulli mask on the higher-order features at its layer, thus producing an effect equivalent to bootstrapping with different subsets of the dataset.</p><h1 id="c2e4" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Which approach works best?</h1><p id="8f97" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Unfortunately, there is no easy answer. The best way is to experiment under the constraints of your problem and see what works best. But if the findings from the authors of the Deep Bayesian Bandits <a class="af ou" href="https://arxiv.org/pdf/2008.00727" rel="noopener ugc nofollow" target="_blank">paper</a> are anything to go by,</p><ol class=""><li id="dbae" class="ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot qi pc pd bk">ε-greedy unsurprisingly gives the lowest CTR improvement due to its unsophisticated exploration, however, the simplicity and low-cost nature of it make it very alluring.</li><li id="4001" class="ny nz fq oa b go pe oc od gr pf of og oh pg oj ok ol ph on oo op pi or os ot qi pc pd bk">UCB generally outperformed Thompson sampling.</li><li id="c9d9" class="ny nz fq oa b go pe oc od gr pf of og oh pg oj ok ol ph on oo op pi or os ot qi pc pd bk">Bootstrap UCB gave the highest CTR return but was also the most computationally expensive due to the need to work with multiple models.</li><li id="80f0" class="ny nz fq oa b go pe oc od gr pf of og oh pg oj ok ol ph on oo op pi or os ot qi pc pd bk">The hybrid model which relied on dropout at the penultimate layer needed more training epochs to perform well and was on par with SGD UCB’s performance but at lower computational cost.</li><li id="c53b" class="ny nz fq oa b go pe oc od gr pf of og oh pg oj ok ol ph on oo op pi or os ot qi pc pd bk">The model’s PrAuc measured offline was inversely related to the CTR gain: this is an important observation that shows that offline performance can be easily attained by giving the model easier training data (for example, data not containing significant exploration) but that will not always translate to online CTR uplifts. This underscores the significance of robust online tests.</li></ol><p id="f0a8" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">That said, the findings can be quite different for a different dataset and problem. Hence, real-world experimentation remains vital.</p><h1 id="55a7" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Conclusion</h1><p id="887b" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In this article, I introduced the cold-start problem created by feedback loops in recommender systems. Following the Deep Bayesian Bandits paper, we framed our ad recommender system as a k-arm bandit and saw many practical applications of reinforcement learning techniques to mitigate the cold-start problem. We also scratched the surface of capturing uncertainty in our neural networks which is a good segue into Bayesian networks.</p><p id="f78d" class="pw-post-body-paragraph ny nz fq oa b go ow oc od gr ox of og oh oy oj ok ol oz on oo op pa or os ot fj bk">[1] Guo, Dalin, et al. “Deep bayesian bandits: Exploring in online personalized recommendations.” <em class="pn">Proceedings of the 14th ACM Conference on Recommender Systems</em>. 2020.</p></div></div></div></div>    
</body>
</html>