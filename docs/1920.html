<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Create Synthetic Dataset Using Llama 3.1 to Fine-Tune Your LLM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Create Synthetic Dataset Using Llama 3.1 to Fine-Tune Your LLM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/create-a-synthetic-dataset-using-llama-3-1-405b-for-instruction-fine-tuning-9afc22fb6eef?source=collection_archive---------2-----------------------#2024-08-07">https://towardsdatascience.com/create-a-synthetic-dataset-using-llama-3-1-405b-for-instruction-fine-tuning-9afc22fb6eef?source=collection_archive---------2-----------------------#2024-08-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="6985" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Using the giant Llama 3.1 405B and Nvidia Nemotron 4 reward model to create a synthetic dataset for instruction fine-tuning.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@itshesamsheikh?source=post_page---byline--9afc22fb6eef--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Hesam Sheikh" class="l ep by dd de cx" src="../Images/b8d5f4f285eef77634e4c1d4321580ed.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*hEouYBx-IeJIslDqS20BjQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--9afc22fb6eef--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@itshesamsheikh?source=post_page---byline--9afc22fb6eef--------------------------------" rel="noopener follow">Hesam Sheikh</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--9afc22fb6eef--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/d59b24212f7745714c66c7591192d4a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*EXZMdt2H9Sj4N_k0zIISoA.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Created by AI using Leonardo.AI</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="d6b9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Data is the heart of AI and while it is a valuable asset, we know how challenging and costly it is to develop high-quality datasets. A well-curated and filtered dataset can make up for a lack of complexity in a model. This is also the case with Large Language Models where smaller-sized models have shown to outperform bigger LLMs by leveraging good data.</p><p id="009f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk of"><span class="l og oh oi bo oj ok ol om on ed">In</span> this article, we will explore how to use <strong class="nl fr">Llama 3.1 405B</strong> to create a synthetic dataset of <strong class="nl fr">git commands in natural language</strong>. I will show how you can use this 405B beast without running tens of GPUs in parallel. After having an initial dataset of instructions and responses, we will use <strong class="nl fr">Nvidia’s Nemotron 4</strong> as a reward model to filter out any bad prompt/response pairs. Finally, we will push this dataset to HuggingFace for later fine-tuning of our LLM.</p><p id="3d8d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This will be fast, free, and will leave you much in control.</p><p id="e18c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">I will keep this post concise and knowledge-packed, so make sure to <strong class="nl fr">read through the end</strong> and familiarize yourself with this essential skill.</p><h1 id="1126" class="oo op fq bf oq or os gq ot ou ov gt ow ox oy oz pa pb pc pd pe pf pg ph pi pj bk">🦙 Why Llama 3.1</h1><p id="0c4f" class="pw-post-body-paragraph nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe fj bk">Meta has gained a firm foothold with the release of their latest family of LLMs, <a class="af pp" href="https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f" rel="noopener ugc nofollow" target="_blank">Llama 3.1</a>. The new family includes an upgraded version of the previous 8B and 70B models with increased reasoning abilities and a giant 405B model.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pq"><img src="../Images/cbadf588e2561f45221657f93c764fa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-4xIbDs8W1W5j67b"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Llama 3.1 405 has been successful in reaching nearly the benchmark of the best closed-source models. (diagram by <span class="ia"><span class="ia" aria-hidden="false"><a class="pr ib ps" href="https://medium.com/u/dc89da634938?source=post_page---user_mention--9afc22fb6eef--------------------------------" rel="noopener" target="_blank">Maxime Labonne</a></span></span>, with permission)</figcaption></figure><p id="a038" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Llama 3.1 405B isn’t just impressive in terms of the sheer scale, but also by closing the gap between closed-source and open-source models, more than ever before (above figure).</p><p id="6924" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This capability of the 405B model makes it ideal for some of the most important and nuanced workflows, such as Retrieval-Augmented Generation (RAG), supervised fine-tuning (SFT), and most importantly <strong class="nl fr">synthetic data generation</strong>.</p><h2 id="b0ba" class="pt op fq bf oq pu pv pw ot px py pz ow ns qa qb qc nw qd qe qf oa qg qh qi qj bk">Why Synthetic Data?</h2><p id="b57b" class="pw-post-body-paragraph nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe fj bk">Synthetic data is created using an artificial model by reproducing the characteristics and features of real-world data. At some point, you will need to work with it <em class="qk">when you need more data than you have</em>.</p><p id="7085" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Our example of a dataset of git commands in natural language can show this perfectly. If we want to create an application that takes as input, what the user needs and then suggests the right git command for it, then at the heart of this application we will need an expert LLM. We could use GPT-4o or Claude and will most likely get good results. But there is the problem of the cost. So the alternative would be to <strong class="nl fr">fine-tune </strong>a Small Language Model (SML) such as Llama 3.1 8B or Gemma 2 2B (which I will get to in a later post).</p><p id="8d2e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">And guess what we need for fine-tuning… Data!</p><p id="6ba0" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Since I didn’t find the right dataset for this task, we are left with only one solution: to create our dataset <strong class="nl fr">synthetically </strong>using Llama 3.1 405B.</p><h1 id="a965" class="oo op fq bf oq or os gq ot ou ov gt ow ox oy oz pa pb pc pd pe pf pg ph pi pj bk">🛠️Building the Dataset</h1><p id="fe0b" class="pw-post-body-paragraph nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe fj bk">To build a synthetic dataset using AI, we will use the following outline. You can choose any other LLMs from what I have chosen.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ql"><img src="../Images/07a1f36079f8e08fdc8b2bb76c584e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G31VhEga4EIBuOHMSOwgaQ.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Our outline of creating a synthetic dataset. (by author)</figcaption></figure><h2 id="130a" class="pt op fq bf oq pu pv pw ot px py pz ow ns qa qb qc nw qd qe qf oa qg qh qi qj bk">Setting Up the API Key</h2><p id="40ef" class="pw-post-body-paragraph nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe fj bk">We will use the <strong class="nl fr">Nvidia NIM API </strong>to leverage these big LLMs without the hassle of running them locally. Running a model like Llama 3.1 405B on the device would normally require multiple H100 GPUs and unless you work in an organization with such resources, you need to use external APIs.</p><p id="d97a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">To access your free Nvidia credits, go to <a class="af pp" href="https://build.nvidia.com/explore/discover#llama-3_1-405b-instruct" rel="noopener ugc nofollow" target="_blank">Llama 3.1 on Nvidia NIM</a>, and click on <strong class="nl fr">Get API Key</strong>. This is what we will use in our code or a <code class="cx qm qn qo qp b">.env</code> file. Once we have the API, we can set up our connection to the Nvidia server to use the models remotely.</p><pre class="ms mt mu mv mw qq qp qr bp qs bb bk"><span id="9809" class="qt op fq qp b bg qu qv l qw qx">client = OpenAI(<br/>    base_url="https://integrate.api.nvidia.com/v1",<br/>    api_key=os.environ["NVIDIA_API_KEY"]<br/>)<br/>MODEL = "meta/llama-3.1-405b-instruct"</span></pre><h2 id="3317" class="pt op fq bf oq pu pv pw ot px py pz ow ns qa qb qc nw qd qe qf oa qg qh qi qj bk">Generate Subtopics</h2><p id="c6bd" class="pw-post-body-paragraph nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe fj bk">Ideally, we like our dataset to cover various scenarios and situations as much as possible. One way to ensure this is to define <strong class="nl fr">subtopics </strong>and ask Llama 3.1 to provide instructions/response pairs for each of the subtopics. We can choose these subtopics ourselves or leave it to the LLM to decide. I took the second approach in the following code snippet.</p><pre class="ms mt mu mv mw qq qp qr bp qs bb bk"><span id="3850" class="qt op fq qp b bg qu qv l qw qx">n_subtopics = 5<br/><br/>TOPIC_GENERATION_PROMPT_TEMPLATE = """\<br/>I want to create a synthetic dataset of natural language and Git commands. Based on this context, give me {n_subtopics} subtopics<br/>to cover what needs to be covered when working with Git. <br/><br/>The list must be without numbers, and without any description of the subtopics. The subtopics should be separated by a comma. There must be no other text than the list.<br/>"""<br/><br/>def generate_subtopics(client, n_subtopics):<br/>    prompt = TOPIC_GENERATION_PROMPT_TEMPLATE.format(n_subtopics=n_subtopics)<br/>    response = client.chat.completions.create(<br/>        model=MODEL,<br/>        messages=[<br/>            {"role": "user",<br/>             "content": prompt}<br/>        ],<br/>        temperature=0.2,<br/>        top_p=0.7,<br/>    )<br/>    return response<br/><br/>responses = generate_subtopics(client, n_subtopics=n_subtopics)<br/>print(responses.choices[0].message.content)</span></pre><p id="dcc0" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The LLM suggests five topics: Branching, Merging, Committing, Remote repositories, and Resolving conflicts. It seems like a fair selection of subjects to cover.</p><h2 id="815e" class="pt op fq bf oq pu pv pw ot px py pz ow ns qa qb qc nw qd qe qf oa qg qh qi qj bk">Generate Instructions</h2><p id="1387" class="pw-post-body-paragraph nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe fj bk">Having five subtopics of working with Git, we need Llama 3.1 to generate a set of instructions (or prompts) regarding each of the subtopics. I have asked for one hundred instructions per topic, so ideally, I should get 500 prompts.</p><p id="e5d6" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">One thing to keep in mind is that when asking for <em class="qk">N </em>number of instructions: it is rare that the model would return exactly as many as you want, even a big model like this.</p><p id="a68f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Eventually, I got a total of 335 instructions for 5 subtopics, which is very different from 500. There are methods to ensure this doesn’t happen but for the sake of simplicity, we won’t dwell on this.</p><pre class="ms mt mu mv mw qq qp qr bp qs bb bk"><span id="42ce" class="qt op fq qp b bg qu qv l qw qx">n_instructions = 100<br/><br/>INSTRUCTION_PROMPT_TEMPLATE = """\<br/>The objective is to create a dataset of user instructions in natural language that should be returned by Git commands.<br/>Given a topic in Git, generate {n_instructions} possible concise instructions that could be given to an AI assitant about that topic.<br/>Write some of these instructions as if given by someone with limited knowledge of Git terminologies and knowledge, <br/>like a beginner programmer. Your response should be in a list format.<br/><br/>The topic is: {sub_topic}<br/>The list must be without numbers. The questions/instructions should be separated by a newline character. There must be no other text than the list.<br/>"""<br/>subtopic_list = responses.choices[0].message.content.split(",")<br/>def generate_instructions(client, sub_topic, n_instructions):<br/>    print(f"Generating Instructions for {sub_topic}.")<br/>    prompt = INSTRUCTION_PROMPT_TEMPLATE.format(sub_topic=sub_topic, n_instructions=n_instructions)<br/>    response = client.chat.completions.create(<br/>        model=MODEL,<br/>        messages=[<br/>            {"role": "user",<br/>             "content": prompt}<br/>        ],<br/>        temperature=0.2,<br/>        top_p=0.7,<br/>    )<br/>    return response.choices[0].message.content<br/><br/><br/>def instructions_generator(client, subtopic_list, n_instructions):<br/>    instruction_list = [generate_instructions(client, subtopic, n_instructions) for subtopic in subtopic_list]<br/>    return instruction_list<br/><br/>instruction_list = instructions_generator(client, subtopic_list, n_instructions)<br/><br/>instruction_list_formatted = []<br/>for instruction_set in instruction_list:<br/>    instruction_list_formatted.extend([instruction.strip() for instruction in instruction_set.split("\n") if instruction])<br/>print(instruction_list_formatted)</span></pre><p id="5c51" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Here are some examples of the generated instructions:</p><pre class="ms mt mu mv mw qq qp qr bp qs bb bk"><span id="2440" class="qt op fq qp b bg qu qv l qw qx">'Make a branch that I can merge back into the main branch',<br/>'I want to make a branch that is based on an older version of the code',<br/>'Can you show me a log of all commits that have been made to the repository this year?',</span></pre><h2 id="f50f" class="pt op fq bf oq pu pv pw ot px py pz ow ns qa qb qc nw qd qe qf oa qg qh qi qj bk">Response Generation</h2><p id="04f4" class="pw-post-body-paragraph nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe fj bk">For each of the provided instructions, we will also ask for a response. As you can see in the following code snippet, I have specifically asked my responses to be <em class="qk">on-topic, informative, and concise</em>. By the end, I will have a list of <code class="cx qm qn qo qp b">instruction</code> and <code class="cx qm qn qo qp b">response</code> pairs.</p><pre class="ms mt mu mv mw qq qp qr bp qs bb bk"><span id="470a" class="qt op fq qp b bg qu qv l qw qx">RESPONSE_PROMPT_TEMPLATE = """\<br/>Given an question/instruction related to Git, generate a response that could be given. <br/>Keep the response on-topic, informative, concise.<br/><br/>The user prompt is: {instruction}<br/>"""<br/>def generate_responses(client, instruction):<br/>    prompt = RESPONSE_PROMPT_TEMPLATE.format(instruction=instruction)<br/>    response = client.chat.completions.create(<br/>        model=MODEL,<br/>        messages=[<br/>            {"role": "user",<br/>             "content": prompt}<br/>        ],<br/>        temperature=0.2,<br/>        top_p=0.7,<br/>        max_tokens=1024,<br/>    )<br/>    return response.choices[0].message.content<br/><br/>def response_generator(client, instruction_list):<br/>    response_list = [generate_responses(client, instruction) for instruction in instruction_list]<br/>    return response_list<br/><br/>instruction_response_list = response_generator(client, instruction_list_formatted)<br/>instruction_response_pair_list = []<br/>for instruction, response in zip(instruction_list_formatted, instruction_response_list):<br/>    instruction_response_pair_list.append(<br/>        {<br/>            "instruction": instruction,<br/>            "responses": response,<br/>        }<br/>    )</span></pre><pre class="qy qq qp qr bp qs bb bk"><span id="04ad" class="qt op fq qp b bg qu qv l qw qx">{"instruction": "Can you make a branch for me and name it \"new-feature\"", <br/>"responses": "To create a new branch named \"new-feature\", you can use the following Git command:\n\n`git branch new-feature`\n\nThis will create a new branch with the specified name. If you want to switch to the new branch immediately, you can use:\n\n`git checkout -b new-feature`\n\nThis will create the branch and switch to it in one step."}</span></pre><h2 id="edd8" class="pt op fq bf oq pu pv pw ot px py pz ow ns qa qb qc nw qd qe qf oa qg qh qi qj bk">Filtering Responses with Nemotron 4</h2><p id="5ff2" class="pw-post-body-paragraph nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe fj bk">Even though we have our instruction/response pairs, not all of the responses are high-quality. They may be verbose, complex, or false. This is where Nvidia’s <a class="af pp" href="https://huggingface.co/nvidia/Nemotron-4-340B-Reward" rel="noopener ugc nofollow" target="_blank"><strong class="nl fr">Nemotron 4 340B Reward</strong></a><strong class="nl fr"> </strong>model comes into play. It is made exactly for our use case, as according to Nvidia, it <em class="qk">“can be used as part of a synthetic data generation pipeline to create training data that helps researchers and developers build their own LLMs.”</em></p></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qz"><img src="../Images/68716e2a3af1320dfe4da92f01e82d06.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*okmsL_7dIpvWvzoeZ9HfpA.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Example use of Nemotron 4. (by author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="46b2" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We will give each one of our instruction/response pairs to Nemotron 4, and receive five scores ranging from 0 to 4. These five scores are <em class="qk">helpfulness, correctness, coherence, complexity, and verbosity</em>. To use the model, I will first define a simple function to feed an instruction and a response to the model and receive the five scores in the shape of a dict.</p><pre class="ms mt mu mv mw qq qp qr bp qs bb bk"><span id="f0c9" class="qt op fq qp b bg qu qv l qw qx">def get_scores_from_response(score_response_template):<br/>    logprobs = score_response_template.choices[0].logprobs.content<br/>    score_dict = {}<br/>    for score in logprobs:<br/>        score_dict[score.token] = score.logprob<br/>    return score_dict<br/><br/>def get_response_and_scores(client, model, question, response_content):<br/>    messages = [<br/>        {<br/>            "role": "user",<br/>            "content": question<br/>        },<br/>        {<br/>            "role": "assistant",<br/>            "content": response_content<br/>        }<br/>    ]<br/>    response = client.chat.completions.create(<br/>        model=model,<br/>        messages=messages,<br/>    )<br/>    scores = get_scores_from_response(response)<br/>    return scores</span></pre><p id="c170" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">After we have a score for each of the rows in our dataset, we can filter the dataset using each of the five provided criteria. I will filter out bad responses based on <strong class="nl fr">helpfulness </strong>and <strong class="nl fr">verbosity</strong>, as I want to keep my responses concise and informative.</p><pre class="ms mt mu mv mw qq qp qr bp qs bb bk"><span id="a117" class="qt op fq qp b bg qu qv l qw qx">helpfulness_THRESHOLD = 3<br/>verbosity_THRESHOLD = 2.5<br/>synthetic_data = [data for i, data in enumerate(synthetic_data) <br/>                  if not (score_list[i]["helpfulness"] &lt; helpfulness_THRESHOLD or <br/>                          score_list[i]["verbosity"] &gt; verbosity_THRESHOLD)]</span></pre><h2 id="a6e6" class="pt op fq bf oq pu pv pw ot px py pz ow ns qa qb qc nw qd qe qf oa qg qh qi qj bk">Push the Dataset to HuggingFace</h2><p id="8cfe" class="pw-post-body-paragraph nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe fj bk">Finally, once you have the finished dataset, it’s a good practice to push it to HuggingFace to use it later or to share it with other developers. To do this, first log in to HuggingFace and provide a <strong class="nl fr">token</strong>, following the provided link on the login page.</p><pre class="ms mt mu mv mw qq qp qr bp qs bb bk"><span id="214d" class="qt op fq qp b bg qu qv l qw qx">from huggingface_hub import login<br/>login()</span></pre><p id="b48e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Then you can load the saved dataset and upload it on your HuggingFace page.</p><pre class="ms mt mu mv mw qq qp qr bp qs bb bk"><span id="b6a2" class="qt op fq qp b bg qu qv l qw qx">with open(f'synthetic_data_filtered.jsonl', 'r') as f:<br/>    data = [json.loads(line) for line in f]<br/>dataset = Dataset.from_list(data)<br/>dataset_dict = DatasetDict({"train": dataset})<br/>dataset_dict.push_to_hub("hesamsheikh/git-prompt")</span></pre><p id="1521" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk of"><span class="l og oh oi bo oj ok ol om on ed">C</span>ongrats 🏆! So far you have been able to use Llama 3.1 to create a dataset of instructions and responses, and Nemotron 4 to refine the dataset and filter out bad responses. In the end, we saw how easy it is to push the dataset to HuggingFace with no effort. <a class="af pp" href="https://www.youtube.com/watch?v=FAdRMVAWiak" rel="noopener ugc nofollow" target="_blank">Create Synthetic Dataset from 1 TOPIC for Instruction Finetuning</a> is also a great inspiration for this article and I would suggest you watch it if you like this topic.</p><p id="73e1" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Here is also the <strong class="nl fr">repository </strong>where you can find the complete code I have used. Don’t forget to <strong class="nl fr">star </strong>⭐the repo if you check it out.</p><p id="c36d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><a class="af pp" href="https://github.com/hesamsheikh/dataset_git_commands" rel="noopener ugc nofollow" target="_blank"><strong class="nl fr"><em class="qk">Creating Synthetic Dataset Using Llama 3.1 405B and Nemotron 4</em></strong></a></p><p id="7f40" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk of"><span class="l og oh oi bo oj ok ol om on ed">T</span><strong class="nl fr"><em class="qk">hank you</em></strong><em class="qk"> for reading through the article! </em>Please share your opinions and suggestions if you think any modifications are required.</p><h2 id="8679" class="pt op fq bf oq pu pv pw ot px py pz ow ns qa qb qc nw qd qe qf oa qg qh qi qj bk">Let’s Connect!</h2><p id="514c" class="pw-post-body-paragraph nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe fj bk"><em class="qk">Subscribe for FREE to be notified of new articles! You can also find me on </em><a class="af pp" href="https://www.linkedin.com/in/hesamsheikh/" rel="noopener ugc nofollow" target="_blank"><em class="qk">LinkedIn</em></a><em class="qk"> and </em><a class="af pp" href="https://x.com/itsHesamSheikh" rel="noopener ugc nofollow" target="_blank"><em class="qk">Twitter</em></a><em class="qk">.</em></p><div class="ra rb rc rd re rf"><a href="https://medium.com/@itshesamsheikh/subscribe?source=post_page-----9afc22fb6eef--------------------------------" rel="noopener follow" target="_blank"><div class="rg ab ig"><div class="rh ab co cb ri rj"><h2 class="bf fr hw z io rk iq ir rl it iv fp bk">Get an email whenever Hesam Sheikh publishes.</h2><div class="rm l"><h3 class="bf b hw z io rk iq ir rl it iv dx">Get an email whenever Hesam Sheikh publishes. By signing up, you will create a Medium account if you don't already have not…</h3></div><div class="rn l"><p class="bf b dy z io rk iq ir rl it iv dx">medium.com</p></div></div><div class="ro l"><div class="rp l rq rr rs ro rt lr rf"/></div></div></a></div><h1 id="afc7" class="oo op fq bf oq or os gq ot ou ov gt ow ox oy oz pa pb pc pd pe pf pg ph pi pj bk">Further Reads</h1><p id="5c53" class="pw-post-body-paragraph nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe fj bk">If you have reached so far, you might also find these articles interesting:</p><div class="ra rb rc rd re rf"><a rel="noopener follow" target="_blank" href="/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=post_page-----9afc22fb6eef--------------------------------"><div class="rg ab ig"><div class="rh ab co cb ri rj"><h2 class="bf fr hw z io rk iq ir rl it iv fp bk">What We Still Don’t Understand About Machine Learning</h2><div class="rm l"><h3 class="bf b hw z io rk iq ir rl it iv dx">Machine Learning unknowns that researchers struggle to understand — from Batch Norm to what SGD hides</h3></div><div class="rn l"><p class="bf b dy z io rk iq ir rl it iv dx">towardsdatascience.com</p></div></div><div class="ro l"><div class="ru l rq rr rs ro rt lr rf"/></div></div></a></div><div class="ra rb rc rd re rf"><a rel="noopener follow" target="_blank" href="/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----9afc22fb6eef--------------------------------"><div class="rg ab ig"><div class="rh ab co cb ri rj"><h2 class="bf fr hw z io rk iq ir rl it iv fp bk">A Comprehensive Guide to Collaborative AI Agents in Practice</h2><div class="rm l"><h3 class="bf b hw z io rk iq ir rl it iv dx">the definition, and building a team of agents that refine your CV and Cover Letter for job applications</h3></div><div class="rn l"><p class="bf b dy z io rk iq ir rl it iv dx">towardsdatascience.com</p></div></div><div class="ro l"><div class="rv l rq rr rs ro rt lr rf"/></div></div></a></div></div></div></div></div>    
</body>
</html>