["```py\npytorch-tabnet==4.1.0\noptuna==3.6.1\npandas==2.1.4\n```", "```py\nimport os\n\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport pandas as pd\nimport numpy as np\n\nfrom utils import CostumPinballLoss\n\nclass mediumTabnetModel:\n\n   def __init__(self,\n                model_file_name,\n                dependent_variables=None,\n                independent_variables=None,\n                batch_size=16_000,\n                n_a=8,\n                n_steps=3,\n                n_independent=2,\n                n_shared=2,\n                cat_idxs=[],\n                cat_dims=[],\n                quantile=None):\n       self.model_file_name = model_file_name\n       self.quantile = quantile\n       self.clf = TabNetRegressor(n_d=n_a,\n                                  n_a=n_a,\n                                  cat_idxs=cat_idxs,\n                                  cat_dims=cat_dims,\n                                  n_steps=n_steps,\n                                  n_independent=n_independent,\n                                  n_shared=n_shared)\n       self.batch_size = batch_size\n       self.independent_variables = independent_variables\n       self.dependent_variables = dependent_variables\n       self.cat_idxs = cat_idxs  # Indexes for categorical values.\n       self.cat_dims = cat_dims  # Dimensions for categorical values.\n       self.ram_data = None\n\ndef fit(self, training_dir, train_date_split):\n\n   if self.ram_data is None:\n       data_path = os.path.join(training_dir, self.training_data_file)\n       df = pd.read_parquet(data_path)\n\n       df_train = df[df['dates'] < train_date_split]\n       df_val = df[df['dates'] >= train_date_split]\n\n       x_train = df_train[self.independent_variables].values.astype(np.int16)\n       y_train = df_train[self.dependent_variables].values.astype(np.int32)\n\n       x_valid = df_val[self.independent_variables].values.astype(np.int16)\n       y_valid = df_val[self.dependent_variables].values.astype(np.int32)\n\n       self.ram_data = {'x_train': x_train,\n                        'y_train': y_train,\n                        'x_val': x_valid,\n                        'y_val': y_valid}\n\n   self.clf.fit(self.ram_data['x_train'],\n                self.ram_data['y_train'],\n                eval_set=[(self.ram_data['x_val'],\n                           self.ram_data['y_val'])],\n                batch_size=self.batch_size,\n                drop_last=True,\n                loss_fn=CostumPinballLoss(quantile=self.quantile),\n                eval_metric=[CostumPinballLoss(quantile=self.quantile)],\n                patience=3)\n\n   feat_score = dict(zip(self.independent_variables, self.clf.feature_importances_))\n   feat_score = dict(sorted(feat_score.items(), key=lambda item: item[1]))\n   self.feature_importances_dict = feat_score \n   # Dict of feature importance and importance score, ordered.\n```", "```py\nimport optuna\nimport numpy as np\n\ndef define_model(trial):\n   n_shared = trial.suggest_int('n_shared', 1, 7)\n   logging.info(f'n_shared: {n_shared}')\n\n   n_independent = trial.suggest_int('n_independent', 1, 16)\n   logging.info(f'n_independent: {n_independent}')\n\n   n_steps = trial.suggest_int('n_steps', 2, 8)\n   logging.info(f'n_steps: {n_steps}')\n\n   n_a = trial.suggest_int('n_a', 4, 32)\n   logging.info(f'n_a: {n_a}')\n\n   batch_size = trial.suggest_int('batch_size', 256, 18000)\n   logging.info(f'batch_size: {batch_size}')\n\n   clf = mediumTabnetModel(model_file_name=model_file_name,\n                           dependent_variables=y_ls,\n                           independent_variables=x_ls,\n                           n_a=n_a,\n                           cat_idxs=cat_idxs,\n                           cat_dims=cat_dims,\n                           n_steps=n_steps,\n                           n_independent=n_independent,\n                           n_shared=n_shared,\n                           batch_size=batch_size,\n                           training_data_file=training_data_file)\n\n   return clf\n\ndef objective(trial):\n   clf = define_model(trial)\n\n   clf.fit(os.path.join(args.training_data_directory, args.dataset),\n           df[int(len(df) * split_test)])\n\n   y_pred = clf.predict(predict_data)\n   y_true = np.array(predict_data[y_ls].values).astype(np.int32)\n\n   metric_value = call_metrics(y_true, y_pred)\n\n   return metric_value\n\nstudy = optuna.create_study(direction='minimize',\n                            storage='sqlite:///db.sqlite3',\n                            study_name=model_name,\n                            load_if_exists=True)\n\nstudy.optimize(objective,\n               n_trials=50)\n```", "```py\npip install optuna-dashboard\n\ncd /path/to/directory_with-db.sqlite3/\n\noptuna-dashboard sqlite:///db.sqlite3\n```"]