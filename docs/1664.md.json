["```py\nimport opendatasets as od\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.inspection import permutation_importance\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import acf, pacf\nimport datetime\n\nimport warnings\nwarnings.filterwarnings('ignore')\n```", "```py\ndataset_url = \"https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption/\"\nod.download(dataset_url)\ndf = pd.read_csv(\".\\hourly-energy-consumption\" + \"\\AEP_hourly.csv\", index_col=0)\ndf.sort_index(inplace = True)\n```", "```py\nfig = make_subplots(rows=5, cols=4, shared_yaxes=True, horizontal_spacing=0.01, vertical_spacing=0.04)\n\n#  drawing a random sample of 5 indices without repetition\nsample = sorted([x for x in np.random.choice(range(0, len(df), 1), 5, replace=False)])\n\n# zoom x scales for plotting\nperiods = [9000, 3000, 720, 240]\n\ncolors = [\"#E56399\", \"#F0B67F\", \"#DE6E4B\", \"#7FD1B9\", \"#7A6563\"]\n\n# s for sample datetime start\nfor si, s in enumerate(sample):\n\n    # p for period length\n    for pi, p in enumerate(periods):\n        cdf = df.iloc[s:(s+p+1),:].copy()\n        fig.add_trace(go.Scatter(x=cdf.index,\n                                 y=cdf.AEP_MW.values,\n                                 marker=dict(color=colors[si])),\n                        row=si+1, col=pi+1)\n\nfig.update_layout(\n    font=dict(family=\"Arial\"),\n    margin=dict(b=8, l=8, r=8, t=8),\n    showlegend=False,\n    height=1000,\n    paper_bgcolor=\"#FFFFFF\",\n    plot_bgcolor=\"#FFFFFF\")\nfig.update_xaxes(griddash=\"dot\", gridcolor=\"#808080\")\nfig.update_yaxes(griddash=\"dot\", gridcolor=\"#808080\")\n```", "```py\n# splitting time series to train and test subsets\ny_train = df.iloc[:-8766, :].copy()\ny_test = df.iloc[-8766:, :].copy()\n\n# Unobserved Components model definition\nmodel = sm.tsa.UnobservedComponents(y_train,\n                                    level='dtrend',\n                                    irregular=True,\n                                    stochastic_level = False,\n                                    stochastic_trend = False,\n                                    stochastic_freq_seasonal = [False, False, False],\n                                    freq_seasonal=[{'period': 24, 'harmonics': 1},\n                                                    {'period': 168, 'harmonics': 1},\n                                                    {'period': 8766, 'harmonics': 2}])\n# fitting model to train data\nmodel_results = model.fit()\n\n# printing statsmodels summary for model\nprint(model_results.summary())\n```", "```py\nValue of `irregular` may be overridden when the trend component is specified using a model string.\n\n                           Unobserved Components Results                            \n====================================================================================\nDep. Variable:                       AEP_MW   No. Observations:               112530\nModel:                  deterministic trend   Log Likelihood            -1002257.017\n                     + freq_seasonal(24(1))   AIC                        2004516.033\n                    + freq_seasonal(168(1))   BIC                        2004525.664\n                   + freq_seasonal(8766(2))   HQIC                       2004518.941\nDate:                      Tue, 25 Jun 2024                                         \nTime:                              08:13:35                                         \nSample:                          10-01-2004                                         \n                               - 08-02-2017                                         \nCovariance Type:                        opg                                         \n====================================================================================\n                       coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nsigma2.irregular  3.168e+06    1.3e+04    244.095      0.000    3.14e+06    3.19e+06\n===================================================================================\nLjung-Box (L1) (Q):              104573.71   Jarque-Bera (JB):              2731.37\nProb(Q):                              0.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               1.04   Skew:                             0.35\nProb(H) (two-sided):                  0.00   Kurtosis:                         3.30\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n```", "```py\ndff = df.copy()\nacorr = acf(dff.AEP_MW.values, nlags=2*366)     # autocorrelation\npacorr = pacf(dff.AEP_MW.values, nlags=2*366)   # partial autocorrelation\n\nfig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0)\nfig.add_trace(go.Scatter(\n    x=np.linspace(0, len(acorr), len(acorr)+1),\n    y=acorr,\n    name=\"Autocorrelation\",\n    marker=dict(color=\"rgb(180, 120, 80)\")\n), row=1, col=1)\nfig.add_trace(go.Scatter(\n    x=np.linspace(0, len(pacorr), len(pacorr)+1),\n    y=pacorr,\n    name=\"Partial Autocorrelation\",\n    marker=dict(color=\"rgb(80, 180, 120)\")\n), row=2, col=1)\nfig.update_layout(\n    font=dict(family=\"Arial\"),\n    margin=dict(b=4, l=4, r=4, t=4),\n    showlegend=False,\n    height=500,\n    paper_bgcolor=\"#FFFFFF\",\n    plot_bgcolor=\"#FFFFFF\")\nfig.update_xaxes(griddash=\"dot\", gridcolor=\"#808080\", row=1, col=1)\nfig.update_xaxes(griddash=\"dot\", gridcolor=\"#808080\", title_text=\"No. of lags\", row=2, col=1)\nfig.update_yaxes(griddash=\"dot\", gridcolor=\"#808080\", title_text=\"Autocorrelation\", row=1, col=1)\nfig.update_yaxes(griddash=\"dot\", gridcolor=\"#808080\", title_text=\"Partial Autocorrelation\", row=2, col=1)\n```", "```py\ndff = df.reset_index(drop=False)\ndff[\"Datetime\"] = pd.to_datetime(dff.Datetime.values)\n\n# lags and difference of multiple days for capturing seasonal effects\nfor i in np.linspace(24, 15*24, 15, dtype=int):\n    dff[f\"lag_{i}\"] = dff.AEP_MW.shift(i)\n    dff[f\"difference_{i}\"] = dff.AEP_MW.diff(periods=i)\n\n# rolling mean and standard deviation up to 3 days for capturing seasonal effects better\nfor i in np.linspace(24, 72, 3, dtype=int):\n    dff[f\"rolling_mean_{i}\"] = dff.AEP_MW.rolling(window=i).mean()\n    dff[f\"rolling_std_{i}\"] = dff.AEP_MW.rolling(window=i).std()\n\n# lag, rolling mean, rolling standard deviation and difference up to 4 hours for capturing immediate effects\nfor i in range(2, 5, 1):\n    dff[f\"lag_{i}\"] = dff.AEP_MW.shift(i)\n    dff[f\"rolling_mean_{i}\"] = dff.AEP_MW.rolling(window=i).mean()\n    dff[f\"rolling_std_{i}\"] = dff.AEP_MW.rolling(window=i).std()\n    dff[f\"difference_{i}\"] = dff.AEP_MW.diff(periods=i)\n\n# categorical features\ndff[\"hour_of_day\"] = dff.Datetime.dt.hour\ndff[\"day_of_week\"] = dff.Datetime.dt.day_of_week\ndff[\"is_weekend\"] = dff[\"day_of_week\"].isin([5, 6]).astype(int)\n\n# grouping derived features for later use in feature importance analysis\ndaily_lags = [col for col in dff.columns if all([\"lag_\" in col, len(col)>5])]\nhourly_lags = [col for col in dff.columns if all([\"lag_\" in col, len(col)<=5])]\ndaily_differences = [col for col in dff.columns if all([\"difference_\" in col, len(col)>12])]\nhourly_differences = [col for col in dff.columns if all([\"difference_\" in col, len(col)<=12])]\ndaily_rolling_means = [col for col in dff.columns if all([\"rolling_mean_\" in col, len(col)>14])]\nhourly_rolling_means = [col for col in dff.columns if all([\"rolling_mean_\" in col, len(col)<=14])]\ndaily_rolling_stds = [col for col in dff.columns if all([\"rolling_std_\" in col, len(col)>13])]\nhourly_rolling_stds = [col for col in dff.columns if all([\"rolling_std_\" in col, len(col)<=13])]\ncategoricals = [\"hour_of_day\", \"day_of_week\", \"is_weekend\"]\n```", "```py\n# segmenting last year as test data\ninputs = dff.dropna().iloc[:, 2:].columns\nxs_train = dff.dropna().iloc[:-8766, 2:]\nxs_test = dff.dropna().iloc[-8766:, 2:]\nys_train = dff.dropna().iloc[:-8766, 1]\nys_test = dff.dropna().iloc[-8766:, 1]\nembedding_dim = 4       # potential hyperparameter\n\n# defining baseline NN model\nfloat_inputs = tf.keras.layers.Input(shape=(len(inputs)-3,), name=\"float_inputs\")           # floats can be directly used in model fitting\ninteger_inputs = tf.keras.layers.Input(shape=(3,), dtype=\"int32\", name=\"integer_inputs\")    # integers should be treated as categoricals ang get them embedded\nembedding_layer = tf.keras.layers.Embedding(input_dim=3, output_dim=embedding_dim)          # embedding will be performed during model fitting\nembedded_integer_inputs = embedding_layer(integer_inputs)\nflattened_embeddings = tf.keras.layers.Flatten()(embedded_integer_inputs)                   \npreprocessing_layers = tf.keras.layers.concatenate([float_inputs, flattened_embeddings])    # float and embedded inputs are combined\nhidden_layers = tf.keras.layers.Dense(units=64, activation=\"relu\")(preprocessing_layers)    # No. of hidden layers, No. of units, activation function are potential hyperparameters\nhidden_layers = tf.keras.layers.Dense(units=32, activation=\"relu\")(hidden_layers)\noutput = tf.keras.layers.Dense(units=1, activation=\"linear\")(hidden_layers)                 # single unit for one step ahead, multiple units for multiple step prediction\nmodel_NN_baseline = tf.keras.Model(inputs=[float_inputs, integer_inputs], outputs=output)\n\n# compiling baseline NN model\nmodel_NN_baseline.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.MeanSquaredError(),\n    jit_compile=True)\n\n# fitting baseline NN model\nmodel_NN_baseline.fit(\n    x=[xs_train.iloc[:, :-3], xs_train.iloc[:, -3:]],\n    y=ys_train,\n    validation_data=[[xs_test.iloc[:, :-3], xs_test.iloc[:, -3:]], ys_test],\n    epochs=128,\n    batch_size=64,\n    verbose=1\n)\n```", "```py\n# permutation feature importance\nfeatures = xs_test.columns\npermutation_importance_results = {}\nrmse = tf.keras.metrics.RootMeanSquaredError()\nrmse_permuted = tf.keras.metrics.RootMeanSquaredError()\nrmse.update_state(ys_test.values, model_NN_baseline.predict([xs_test.iloc[:, :-3], xs_test.iloc[:, -3:]], verbose=0).flatten())\n\nfor feature in features:\n\n    xs_test_permuted = xs_test.copy()\n    xs_test_permuted.loc[:, feature] = xs_test.loc[:, feature].sample(frac=1, axis=0, replace=False, random_state=42).values\n\n    rmse_permuted.reset_state()\n    rmse_permuted.update_state(ys_test.values, model_NN_baseline.predict([xs_test_permuted.iloc[:, :-3], xs_test_permuted.iloc[:, -3:]], verbose=0).flatten())\n\n    permutation_importance_results[feature] = rmse_permuted.result().numpy() / rmse.result().numpy()\n\npi_results_sorted_keys = sorted(permutation_importance_results, key=permutation_importance_results.get, reverse=True)\n\nfig3 = make_subplots()\nfig3.add_trace(go.Bar(\n    x=pi_results_sorted_keys,\n    y=[permutation_importance_results[key] for key in pi_results_sorted_keys]))\nfig3.update_layout(\n    title=\"<b>Permutation Feature Importance</b>\",\n    font=dict(family=\"Arial\"),\n    margin=dict(b=4, l=4, r=4, t=36),\n    showlegend=False,\n    height=500,\n    paper_bgcolor=\"#FFFFFF\",\n    plot_bgcolor=\"#FFFFFF\"\n)\nfig3.update_xaxes(griddash=\"dot\", gridcolor=\"#808080\", row=1, col=1)\nfig3.update_yaxes(griddash=\"dot\", gridcolor=\"#808080\", row=1, col=1)\n```", "```py\nfirst_index = -8766*5\nlast_index = -8766*2\nfinal_index = -8766\ninputs = dff.dropna().iloc[:, 2:].columns\nxs_train = dff.dropna().iloc[first_index:last_index, 2:]\nxs_train.iloc[:, :-3] = xs_train.iloc[:, :-3].astype(np.float32)\nxs_test = dff.dropna().iloc[last_index:final_index, 2:]\nxs_test.iloc[:, :-3] = xs_test.iloc[:, :-3].astype(np.float32)\nys_train = np.vstack([dff.dropna().iloc[i:i+8765, 1].astype(int).values for i in range(first_index, last_index, 1)])\nys_test = np.vstack([dff.dropna().iloc[i:i+8765, 1].astype(int).values for i in range(last_index, final_index, 1)])\nembedding_dim = 4\n\n# defining, compiling and training NN model for MULTIPLE STEP PREDICTIONS. Model architecture is the same, except output layer\nfloat_inputs = tf.keras.layers.Input(shape=(len(inputs)-3,), name=\"float_inputs\")\ninteger_inputs = tf.keras.layers.Input(shape=(3,), dtype=\"int32\", name=\"integer_inputs\")\nembedding_layer = tf.keras.layers.Embedding(input_dim=3, output_dim=embedding_dim)\nembedded_integer_inputs = embedding_layer(integer_inputs)\nflattened_embeddings = tf.keras.layers.Flatten()(embedded_integer_inputs)\npreprocessing_layers = tf.keras.layers.concatenate([float_inputs, flattened_embeddings])\nhidden_layers = tf.keras.layers.Dense(units=64, activation=\"relu\")(preprocessing_layers)\nhidden_layers = tf.keras.layers.Dense(units=32, activation=\"relu\")(hidden_layers)\noutput = tf.keras.layers.Dense(units=np.abs(final_index)-1, activation=\"linear\")(hidden_layers)\n\nmodel_NN_multistep = tf.keras.Model(inputs=[float_inputs, integer_inputs], outputs=output)\nmodel_NN_multistep.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.MeanSquaredError(),\n    jit_compile=True)\nmodel_NN_multistep.fit(\n    x=[xs_train.iloc[:, :-3], xs_train.iloc[:, -3:]],\n    y=ys_train,\n    validation_data=[[xs_test.iloc[:, :-3], xs_test.iloc[:, -3:]], ys_test],\n    epochs=128,\n    batch_size=64,\n    verbose=1\n)\n```", "```py\ny_train = df.iloc[:-8766, 0].values\ny_test = df.iloc[-8766:, 0].values\nobserved_integral = np.cumsum([y_test[x] + (y_test[x+1] - y_test[x]) / 2 for x in range(len(y_test)-1)])[-1]\nforecast = model_results.forecast(steps=8766)\nUC_integral = np.cumsum([forecast[x] + (forecast[x+1] - forecast[x]) / 2 for x in range(len(forecast)-1)])[-1]\n\n# calculating absolute and percentage error of forecast integral compared to observed integral\nfcast_integral_abserror = UC_integral - observed_integral\nfcast_integral_perror4 = (UC_integral - observed_integral) * 100 / observed_integral\n\nprint(f\"Observed yearly energy demand: {'%.3e' % observed_integral} MWh\")\nprint(f\"Forecast yearly energy demand: {'%.3e' % UC_integral} MWh\")\nprint(f\"Forecast error of yearly energy demand: {'%.3e' % fcast_integral_abserror} MWh or {'%.3f' % fcast_integral_perror4} %\")\n```", "```py\nObserved yearly energy demand: 1.312e+08 MWh\nForecast yearly energy demand: 1.283e+08 MWh\nForecast error of yearly energy demand: -2.832e+06 MWh or -2.159 %\n```", "```py\ny_test = dff.dropna().iloc[-8766:-1, 1].values\nobserved_integral = np.cumsum([y_test[x] + (y_test[x+1] - y_test[x]) / 2 for x in range(len(y_test)-1)])[-1]\nforecast = model_NN_multistep.predict([xs_test.iloc[-1:, :-3], xs_test.iloc[-1:, -3:]], verbose=0).flatten()\nmodel_NN_multistep_integral = np.cumsum([forecast[x] + (forecast[x+1] - forecast[x]) / 2 for x in range(len(forecast)-1)])[-1]\n\n# calculating absolute and percentage error of forecast integral compared to observed integral\nfcast_integral_abserror = model_NN_multistep_integral - observed_integral\nfcast_integral_perror4 = (model_NN_multistep_integral - observed_integral) * 100 / observed_integral\n\nprint(f\"Observed yearly energy demand: {'%.3e' % observed_integral} MWh\")\nprint(f\"Forecast yearly energy demand: {'%.3e' % model_NN_multistep_integral} MWh\")\nprint(f\"Forecast error of yearly energy demand: {'%.3e' % fcast_integral_abserror} MWh or {'%.3f' % fcast_integral_perror4} %\")\n```", "```py\nObserved yearly energy demand: 1.312e+08 MWh\nForecast yearly energy demand: 1.286e+08 MWh\nForecast error of yearly energy demand: -2.508e+06 MWh or -1.912 %\n```"]