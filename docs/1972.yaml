- en: 'Beyond Fine-Tuning: Merging Specialized LLMs Without the Data Burden'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越微调：合并专业化LLM而不增加数据负担
- en: 原文：[https://towardsdatascience.com/beyond-fine-tuning-merging-specialized-llms-without-the-data-burden-1c449c2060c4?source=collection_archive---------5-----------------------#2024-08-13](https://towardsdatascience.com/beyond-fine-tuning-merging-specialized-llms-without-the-data-burden-1c449c2060c4?source=collection_archive---------5-----------------------#2024-08-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/beyond-fine-tuning-merging-specialized-llms-without-the-data-burden-1c449c2060c4?source=collection_archive---------5-----------------------#2024-08-13](https://towardsdatascience.com/beyond-fine-tuning-merging-specialized-llms-without-the-data-burden-1c449c2060c4?source=collection_archive---------5-----------------------#2024-08-13)
- en: 'From Model Soup to Automated Evolutionary Merging: Leveraging Specialized LLM
    Fusion to Reduce Data Requirements and Eliminate Intensive Fine-Tuning.'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从模型融合到自动进化合并：利用专业化LLM融合以减少数据需求并消除大量的微调。
- en: '[](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--1c449c2060c4--------------------------------)[![Elahe
    Aghapour](../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png)](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--1c449c2060c4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1c449c2060c4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1c449c2060c4--------------------------------)
    [Elahe Aghapour](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--1c449c2060c4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--1c449c2060c4--------------------------------)[![Elahe
    Aghapour](../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png)](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--1c449c2060c4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1c449c2060c4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1c449c2060c4--------------------------------)
    [Elahe Aghapour](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--1c449c2060c4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1c449c2060c4--------------------------------)
    ·10 min read·Aug 13, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1c449c2060c4--------------------------------)
    ·10分钟阅读·2024年8月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Authors:** [**Elahe Aghapour**](https://medium.com/u/75214fb27311?source=post_page---user_mention--1c449c2060c4--------------------------------)**,**
    [**Salar Rahili**](https://medium.com/u/6dff1eb2cc9f?source=post_page---user_mention--1c449c2060c4--------------------------------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：** [**Elahe Aghapour**](https://medium.com/u/75214fb27311?source=post_page---user_mention--1c449c2060c4--------------------------------)**，**
    [**Salar Rahili**](https://medium.com/u/6dff1eb2cc9f?source=post_page---user_mention--1c449c2060c4--------------------------------)'
- en: 'Introduction:'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言：
- en: 'The field of computer vision and natural language processing is evolving rapidly,
    leading to a growing demand for specialized models fine-tuned for specific downstream
    tasks. However, having different fine-tuned models has multiple drawbacks:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉和自然语言处理领域正在迅速发展，这导致对为特定下游任务微调的专业化模型的需求不断增长。然而，拥有多个不同的微调模型也带来了多个缺点：
- en: 1\. For each task, a separate model must be stored and deployed (this issue
    can be resolved by applying methods like LoRA for fine-tuning).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 每个任务都必须存储和部署一个单独的模型（这个问题可以通过应用像LoRA这样的微调方法来解决）。
- en: 2\. Independently fine-tuned models cannot benefit from leveraging information
    from related tasks, which limits their generalization across both in-domain and
    out-of-domain tasks. However, multi-task learning requires access to datasets
    for each specific task, and integrating these datasets can be complicated. What
    if we do not have access to datasets for all downstream tasks, but the fine-tuned
    models are available? Imagine you need a large language model (LLM) fine-tuned
    on a set of specific tasks. Instead of collecting extensive datasets for downstream
    tasks and undergoing the resource-heavy process of fine-tuning, you can find LLMs
    fine-tuned on each task and merge these models to create the desired one. Note
    that finding such models is not a difficult task within the large Hugging Face
    repository, which hosts approximately 0.5 million fine-tuned models. Merging multiple
    models has recently gained significant attention, primarily because it requires
    lightweight computation and no training data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 独立微调的模型无法利用来自相关任务的信息，这限制了它们在领域内和领域外任务中的泛化能力。然而，多任务学习需要访问每个特定任务的数据集，且整合这些数据集可能会变得复杂。如果我们无法访问所有下游任务的数据集，但有可用的微调模型，该怎么办？假设你需要一个在一组特定任务上微调的大型语言模型（LLM）。与其收集大量下游任务的数据集并经历资源密集的微调过程，你可以找到已在每个任务上微调的LLM模型，并将这些模型合并成所需的模型。需要注意的是，在庞大的Hugging
    Face仓库中，寻找这样的模型并不困难，该仓库托管着大约50万个微调模型。近年来，合并多个模型已受到广泛关注，主要是因为它要求计算资源少，且不需要训练数据。
- en: '![](../Images/25ee63a959506e2d9587b14a354f7621.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25ee63a959506e2d9587b14a354f7621.png)'
- en: Fig.1 Model ensemble combines outputs from multiple models to boost accuracy
    but requires more computational resources. Multi-task learning trains one model
    on several tasks simultaneously, needing access to all datasets and high computational
    power. Model merging, however, fuses pre-trained models into one, leveraging their
    strengths with minimal computation and no extra training costs, offering a highly
    efficient solution (image from [paper](https://arxiv.org/pdf/2212.09849)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 模型集成通过结合多个模型的输出提高准确性，但需要更多的计算资源。多任务学习同时在多个任务上训练一个模型，要求访问所有数据集并具备高计算能力。而模型合并则将预训练的模型融合为一个，利用它们的优势，计算量小且无需额外的训练成本，提供了一种高效的解决方案（图片来源：[论文](https://arxiv.org/pdf/2212.09849)）。
- en: With the growing attention to merging, public libraries such as WEBUI and MergeKit
    have been developed to facilitate this process. WebUIs enables merging fine-tuned
    models such as Stable Diffusion using different merging techniques. MergeKit is
    an open-source, centralized library that offers different merging methods. It
    facilitates model merging by its efficient implementation of merging techniques,
    applicable on any hardware.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对模型合并的关注度不断增长，公共库如WEBUI和MergeKit也应运而生，以便简化这一过程。WebUIs使得合并微调过的模型（如Stable Diffusion）成为可能，且支持多种合并技术。MergeKit是一个开源的集中式库，提供不同的合并方法。它通过高效的合并技术实现，支持在任何硬件上进行模型合并。
- en: 'Here, we categorized merging methods into three main categories:'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这里，我们将合并方法分为三大类：
- en: 1\. merging models with identical architectures and initializations,
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1\. 合并具有相同架构和初始化的模型，
- en: 2\. merging models with identical architectures but different initializations,
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2\. 合并具有相同架构但初始化不同的模型，
- en: 3\. merging models with different architectures.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 3\. 合并具有不同架构的模型。
- en: Each category involves different techniques to effectively combine models, which
    will be explained below.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每个类别涉及不同的技术来有效地合并模型，下面将进行详细解释。
- en: '**1\. Merging Models with Both Identical Architectures and Initializations:**'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**1\. 合并具有相同架构和初始化的模型：**'
- en: '***1.a Merging With No Data Requirement:***'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '***1.a 无需数据的合并：***'
- en: The model merging methods in this section are all based on Linear Mode Connectivity
    ([**LMC**](https://proceedings.neurips.cc/paper_files/paper/2019/file/05e97c207235d63ceb1db43c60db7bbb-Paper.pdf)).
    LMC suggests that for models with identical architecture and initialization, the
    loss between their checkpoints can be connected by a low-loss linear path. This
    means that these models can be combined using linear interpolation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的模型合并方法都基于线性模式连接（[**LMC**](https://proceedings.neurips.cc/paper_files/paper/2019/file/05e97c207235d63ceb1db43c60db7bbb-Paper.pdf)）。LMC表明，对于具有相同架构和初始化的模型，它们的检查点之间的损失可以通过一条低损失的线性路径连接。这意味着这些模型可以通过线性插值进行合并。
- en: To fine-tune a model, various configurations, like different learning rates,
    random seeds, and data augmentation techniques can be applied which result in
    different model parameters. [**Model soup**](https://proceedings.mlr.press/v162/wortsman22a/wortsman22a.pdf)
    proposes averaging these parameters since these models have learned similar representations
    and are close in parameter space. Weighted model averaging leads to a flat local
    optimum with better generalization to out-of-distribution tasks [see [13](https://proceedings.neurips.cc/paper_files/paper/2022/file/69b5534586d6c035a96b49c86dbeece8-Paper-Conference.pdf),
    [14](https://proceedings.neurips.cc/paper_files/paper/2021/file/995f5e03890b029865f402e83a81c29d-Paper.pdf)]
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了微调一个模型，可以应用不同的配置，如不同的学习率、随机种子和数据增强技术，这些都会导致不同的模型参数。[**模型汤**](https://proceedings.mlr.press/v162/wortsman22a/wortsman22a.pdf)建议对这些参数进行平均化，因为这些模型已经学习到了相似的表示，并且在参数空间中彼此接近。加权模型平均可以得到一个平坦的局部最优解，并且对超出分布的任务具有更好的泛化能力[参见[13](https://proceedings.neurips.cc/paper_files/paper/2022/file/69b5534586d6c035a96b49c86dbeece8-Paper-Conference.pdf),
    [14](https://proceedings.neurips.cc/paper_files/paper/2021/file/995f5e03890b029865f402e83a81c29d-Paper.pdf)]。
- en: '![](../Images/003054738dc6d8ecf7c47c2b93be6c72.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/003054738dc6d8ecf7c47c2b93be6c72.png)'
- en: Fig. 2 Pl shows the result of model soup merging while Ps presents the result
    of SLERP merging (image by authors).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 Pl 显示了模型汤合并的结果，而 Ps 显示了 SLERP 合并的结果（图片由作者提供）。
- en: '**SLERP** (Spherical Linear Interpolation, first introduced [here](https://dl.acm.org/doi/pdf/10.1145/325334.325242))
    is a technique commonly used in computer graphics and animation for smoothly interpolating
    between rotations represented by quaternions. SLERP is also applicable in model
    merging. It merges two sets of model parameters by interpolating along a spherical
    path instead of a straight line. Fig. 2 shows that for the given two model parameters
    p1 and p2, SLERP merges these parameters along the globe’s surface, providing
    a smooth transition. This method is commonly used in merging LLMs.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**SLERP**（球面线性插值，首次介绍见[此处](https://dl.acm.org/doi/pdf/10.1145/325334.325242)）是一种在计算机图形学和动画中常用的技术，用于在由四元数表示的旋转之间平滑插值。SLERP
    也适用于模型合并。它通过沿球面路径插值来合并两组模型参数，而不是沿直线路径。图 2 显示，对于给定的两个模型参数 p1 和 p2，SLERP 沿着地球表面合并这些参数，提供平滑的过渡。这种方法常用于合并大型语言模型（LLMs）。'
- en: 'Assume two MLP models are given, each fine-tuned on a different downstream
    task. SLERP can merge these two models using the following steps:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设给定两个多层感知机（MLP）模型，每个模型都在不同的下游任务上进行了微调。SLERP 可以通过以下步骤合并这两个模型：
- en: '**Step 1:** For each model parameters, flatten and concatenate them into vectors
    v1, v2'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1**：对于每个模型参数，将其展平并连接成向量 v1、v2'
- en: '**Step 2**: Normalize the vectors v1​ and v2 to be on the unit hypersphere
    surface (resulting in v1′​ and v2′​).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2**：将向量 v1​ 和 v2 正规化到单位超球面上（得到 v1′​ 和 v2′​）。'
- en: '**Step 3**: Calculate the angle θ (in radians) between these two vectors.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3**：计算这两个向量之间的角度 θ（以弧度为单位）。'
- en: '**Step 4**: Calculate Vslerp​ using the SLERP formula as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4**：使用 SLERP 公式计算 Vslerp​：'
- en: '![](../Images/c155fcd8531a3c30355f80f9cceb40ac.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c155fcd8531a3c30355f80f9cceb40ac.png)'
- en: where t is the interpolation parameter as t=0 means only Model 1 is used, while
    t=1 means only Model 2 is used.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，t 是插值参数，当 t=0 时仅使用模型 1，而 t=1 时仅使用模型 2。
- en: Linear weight averaging techniques, such as model soup and SLERP, have been
    common in the field of computer vision from image processing and classification
    models to image generation models such as latent diffusion models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 线性加权平均技术，如模型汤和 SLERP，在计算机视觉领域中已广泛应用，从图像处理和分类模型到图像生成模型，如潜在扩散模型。
- en: '[**Task arithmetic**](https://arxiv.org/pdf/2212.04089) introduces a method
    based on task vectors. A task vector is calculated by subtracting the weights
    of a pretrained model (θpre​) from the weights of the same model fine-tuned for
    a specific task (θft​), as'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[**任务算术**](https://arxiv.org/pdf/2212.04089)提出了一种基于任务向量的方法。任务向量通过从同一模型的预训练权重（θpre​）中减去针对特定任务微调后的权重（θft​）来计算，公式如下：'
- en: τ = θft − θpre​. This vector represents a direction in the weight space of the
    pretrained model where moving in that direction enhances performance on that task.
    Task vectors can be combined together by arithmetic operations such as negation
    and addition. Negating a task vector (θpre — τ) reduces the model’s performance
    on the target task (forgetting) with minimal impact on control tasks. To enhance
    the performance of the pre-trained model across multiple tasks, we can initially
    learn a task vector for each task. By then summing these task vectors (θpre+∑τi),
    we improve the model’s capability to handle multiple tasks simultaneously.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: τ = θft − θpre​。这个向量表示一个方向，在预训练模型的权重空间中，沿着该方向移动能够提高该任务的性能。任务向量可以通过算术运算，如取反和加法，进行组合。取反任务向量
    (θpre — τ) 会减少模型在目标任务上的性能（遗忘），对控制任务的影响最小。为了提高预训练模型在多个任务上的表现，我们可以为每个任务最初学习一个任务向量。然后通过将这些任务向量相加
    (θpre+∑τi)，我们可以提升模型同时处理多个任务的能力。
- en: '[**TIES**](https://proceedings.neurips.cc/paper_files/paper/2023/file/1644c9af28ab7916874f6fd6228a9bcf-Paper-Conference.pdf)
    addresses performance drops due to parameter interference when combining task
    vectors (∑τi​). This issue can be solved through three steps (see Fig. 3):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[**TIES**](https://proceedings.neurips.cc/paper_files/paper/2023/file/1644c9af28ab7916874f6fd6228a9bcf-Paper-Conference.pdf)
    解决了在合并任务向量 (∑τi​) 时由于参数干扰而导致的性能下降问题。这个问题可以通过以下三步解决（见图 3）：'
- en: (1) trim each task vector to the top-k% (usually k=20) largest magnitude values,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 修剪每个任务向量至最大幅度的前 k%（通常 k = 20），
- en: (2) for each non-zero parameter, select the sign with the highest total magnitude
    across all task vectors to avoid conflicting changes, and
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 对于每个非零参数，选择所有任务向量中总幅度最大的符号，以避免冲突的变化，且
- en: (3) merging values only from task vectors with the same sign as the elected
    one.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 仅合并来自与选定符号相同的任务向量的值。
- en: '![](../Images/a397b5a6e7f5ac9b0e37f32f681f01be.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a397b5a6e7f5ac9b0e37f32f681f01be.png)'
- en: Fig. 3 A depiction of the steps involved in TIES. Each parameter in a model
    is visualized as a square. The arrows depict the update (task vector, τ ) to a
    parameter produced by fine-tuning on different tasks (coded by colors), with direction
    denoting sign and length denoting magnitude. 1- Trim the task vector values based
    on their magnitude, 2- Elect the sign for each parameter (γm, green vector containing
    +1 or −1) by resolving sign conflicts, 3- Pick only the values that align with
    the elected sign and take their mean as the final parameter value. (image from
    [paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/1644c9af28ab7916874f6fd6228a9bcf-Paper-Conference.pdf))
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3 TIES 涉及的步骤示意图。模型中的每个参数被表示为一个方框。箭头表示由在不同任务上微调所产生的更新（任务向量，τ）对参数的影响，箭头的方向表示符号，长度表示幅度。1-
    根据幅度修剪任务向量的值，2- 通过解决符号冲突，选择每个参数的符号（γm，绿色向量包含 +1 或 −1），3- 仅选择与选定符号对齐的值，并取它们的平均值作为最终的参数值。（图像来自
    [论文](https://proceedings.neurips.cc/paper_files/paper/2023/file/1644c9af28ab7916874f6fd6228a9bcf-Paper-Conference.pdf)）
- en: '[**DARE**](https://openreview.net/pdf?id=fq0NaiU8Ex) is mainly focused on LLM’s
    model merging and identifies the extreme redundancy in the task vector (τ = θft−θpre).
    It proposes a three step approach:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[**DARE**](https://openreview.net/pdf?id=fq0NaiU8Ex) 主要集中于LLM模型的融合，并识别任务向量中的极端冗余（τ
    = θft−θpre）。它提出了一个三步法：'
- en: 1- Randomly drop p% (usually p =90) of the task vector values,
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 随机丢弃 p%（通常 p = 90）任务向量的值，
- en: 2- Rescale the remaining ones by a factor of 1/(1 − p), and
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 2- 将其余参数按 1/(1 − p) 的因子进行重新缩放，且
- en: 3- Merge (θpre + λi ∑τi)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 3- 合并 (θpre + λi ∑τi)
- en: where λi is the scaling term, representing the importance of each task vector
    to be merged.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 λi 是缩放项，表示每个任务向量在合并时的重要性。
- en: '***1.b Merging With Data Requirement:***'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '***1.b 与数据需求的融合：***'
- en: The merging methods that we discussed above require no data. However, there
    are approaches that do need data to determine the optimal weights for merging
    the parameters. These methods use data to compute the activations and then adjust
    the weights accordingly.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上面讨论的合并方法不需要数据。然而，也有一些方法确实需要数据来确定合并参数的最优权重。这些方法通过使用数据计算激活值，然后相应地调整权重。
- en: One such approach is [**Fisher Merging**](https://arxiv.org/pdf/2111.09832).
    Given K fine-tuned models, each trained on a different downstream task starting
    from a specific pretrained checkpoint, Fisher Merging performs a weighted summation
    of each model’s parameters. The weights are calculated using the Fisher information
    matrix, which requires some data from each task for the matrix construction.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种方法是[**费舍尔合并**](https://arxiv.org/pdf/2111.09832)。给定K个微调模型，每个模型在不同的下游任务上进行训练，起始点是特定的预训练检查点，费舍尔合并对每个模型的参数进行加权求和。权重是通过费舍尔信息矩阵计算的，矩阵构造需要每个任务的数据。
- en: 'In a related development, [**RegMean**](https://arxiv.org/pdf/2212.09849) significantly
    outperforms Fisher-weighted merging by recasting the model merging task as a linear
    regression problem. This method derives closed-form solutions for the weights
    of linear layers and interpolates other weights (like layer normalization and
    bias terms) evenly. Given K fine-tuned models and some data Xi i= 1,..,K, for
    each task, the linear layers of the merged model can be determined as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在相关的发展中，[**RegMean**](https://arxiv.org/pdf/2212.09849)通过将模型合并任务重新表述为线性回归问题，显著超越了费舍尔加权合并。该方法为线性层的权重推导出闭式解，并均匀插值其他权重（如层归一化和偏置项）。给定K个微调模型和一些数据Xi，i=1,..,K，对于每个任务，可以按如下方式确定合并模型的线性层：
- en: '![](../Images/9ec5319efad0b1e71fc3d9fb24cce6ad.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ec5319efad0b1e71fc3d9fb24cce6ad.png)'
- en: Where Wi is the linear layer from the ith fine-tuned model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Wi是来自第i个微调模型的线性层。
- en: '**2\. Merging Models with Identical Architectures but Different Initializations**'
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**2\. 合并具有相同架构但不同初始化的模型**'
- en: Given models that have the same architecture and training dataset but different
    initializations, simple merging methods like linear model combination often fail
    to perform well. The main reason is that the weights of the models are not aligned.
    Hence, researchers have developed techniques to leverage the permutation symmetry
    of neural networks. By reordering the neurons of the models, their weights can
    align better, which makes the merging process more effective.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 给定具有相同架构和训练数据集但不同初始化的模型，像线性模型组合这样简单的合并方法通常无法很好地执行。主要原因是模型的权重没有对齐。因此，研究人员开发了利用神经网络置换对称性的技术。通过重新排列模型的神经元，可以使它们的权重更好地对齐，从而使合并过程更加有效。
- en: '[**Git-Rebasin**](https://arxiv.org/pdf/2209.04836) suggests permuting the
    weights of one model to match the configuration of another. Assume two models,
    A and B are given with the same architecture and training dataset, but their initializations
    and training data orders were different. The weights of each network can be permuted
    without changing its functionality, which means that swapping neurons in hidden
    layers can result in functionally equivalent models.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Git-Rebasin**](https://arxiv.org/pdf/2209.04836)建议置换一个模型的权重以匹配另一个模型的配置。假设给定两个模型A和B，它们具有相同的架构和训练数据集，但初始化和训练数据的顺序不同。每个网络的权重可以进行置换而不改变其功能，这意味着交换隐藏层中的神经元可以得到功能等效的模型。'
- en: 'They formulated this as an optimization problem to identify the optimal permutations
    of units across layers that align the two models’ parameters in the weight space.
    This alignment ensures that the models are in a similar “basin” of the loss landscape,
    which leads to a smooth and effective merging. To this goal, Git-Rebasin proposed
    the following three steps:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 他们将此问题表述为一个优化问题，以识别跨层单元的最佳置换，从而在权重空间中对齐两个模型的参数。这种对齐确保了模型处于损失景观中的相似“盆地”，从而导致平滑且有效的合并。为了达到这一目标，Git-Rebasin提出了以下三个步骤：
- en: 1\. For each layer, the problem of finding the best permutations is formulated
    as a Linear Assignment Problem (LAP). This step involves computing a matrix of
    activations and finding the optimal permutation matrix that aligns the activations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 对于每一层，寻找最佳置换的问题被公式化为线性分配问题（LAP）。这一步骤涉及计算激活矩阵，并找到对齐激活的最优置换矩阵。
- en: 2\. Given the optimal permutations for all layers, the weights of model B will
    be permuted.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 给定所有层的最优置换，模型B的权重将被置换。
- en: 3\. Linear model combination between the permuted weights of model B and the
    weights of model A lies within a low-loss basin in the loss landscape, which ensures
    that the merged model performs well.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 模型B的置换权重与模型A的权重之间的线性模型组合位于损失景观中的低损失区域，这确保了合并后的模型表现良好。
- en: '[**REPAIR**](https://arxiv.org/pdf/2211.08403) addresses a critical issue in
    the Rebasin merging method known as variance collapse, in which the hidden units
    have significantly smaller activation variance compared to the corresponding units
    of the original networks before they were interpolated. Therefore, the activations
    of neurons become nearly constant in deeper layers, hence the network will no
    longer even be able to differentiate between inputs. REPAIR resolves this issue
    by rescaling the activations of the interpolated networks to match the statistical
    properties of the original networks. By adjusting the means and variances of the
    activations, the interpolated network maintains functional variability throughout
    its layers. Applying the REPAIR method significantly reduces the interpolation
    barrier, improving the performance of interpolated models.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[**REPAIR**](https://arxiv.org/pdf/2211.08403)解决了Rebasin合并方法中的一个关键问题——方差崩溃问题，其中隐藏单元的激活方差显著小于原始网络相应单元的激活方差，尤其是在它们被插值之前。因此，神经元的激活在更深层次上几乎变得恒定，导致网络无法区分输入。REPAIR通过重新缩放插值网络的激活值，使其与原始网络的统计属性匹配，从而解决了这个问题。通过调整激活的均值和方差，插值网络保持了其各层的功能变异性。应用REPAIR方法显著降低了插值壁垒，提升了插值模型的性能。'
- en: 3\. **Merging Models with Different Architectures**
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. **合并具有不同架构的模型**
- en: In contrast to the methods discussed so far,[**Frankenmerging**](https://github.com/arcee-ai/mergekit)
    does not fuse models into a single one, and instead stacks different layers of
    different models sequentially. Therefore, it is able to merge models with different
    architectures.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与目前讨论的方法相比，[**Frankenmerging**](https://github.com/arcee-ai/mergekit)并不会将多个模型合并为一个模型，而是将不同模型的不同层按顺序堆叠。因此，它能够合并具有不同架构的模型。
- en: For example, to construct an LLM with 40 layers, one might stack the first 24
    layers from one LLM onto layers 25–40 from another LLM. This method has gained
    significant attention in style transfer in computer vision. Despite requiring
    a lot of trial and error and experimentation, it has led to impressive LLM models
    such as Goliath and Solar-10.7B [see [here](https://huggingface.co/models?sort=downloads&search=franken)].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了构建一个具有40层的LLM，可以将第一个LLM的前24层堆叠到另一个LLM的第25到40层。这种方法在计算机视觉中的风格迁移中得到了广泛关注。尽管需要大量的反复试验和实验，但它已经带来了如Goliath和Solar-10.7B等令人印象深刻的LLM模型[见[此处](https://huggingface.co/models?sort=downloads&search=franken)]。
- en: '![](../Images/5b75c40130d9085905106d79368ab407.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b75c40130d9085905106d79368ab407.png)'
- en: Fig.4 Overview of EvolutionaryOptimization approach (image from [paper](https://arxiv.org/pdf/2403.13187)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图4 进化优化方法概述（图像来自[论文](https://arxiv.org/pdf/2403.13187)）。
- en: '[**EvolutionaryOptimization**](https://arxiv.org/pdf/2403.13187) proposes a
    framework to automatically merge a given set of foundation models, such that the
    merged model outperforms any individual model in the given set. This approach
    involves two main phases (see Fig. 4):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[**进化优化**](https://arxiv.org/pdf/2403.13187)提出了一种框架，旨在自动合并给定的一组基础模型，使得合并后的模型在给定集合中超过任何单个模型的表现。该方法包括两个主要阶段（见图4）：'
- en: In the first phase, this method uses TIES-Merging with DARE for layer-wise merging
    of N foundational models. The process is optimized by using an evolutionary algorithm
    guided by task-specific metrics (e.g., accuracy for MGSM, ROUGE score for VQA).
    To find unknown variables such as dropout percentages in DARE and weights of each
    model’s parameters in merging, the evolutionary optimization begins with a group
    of possible solutions that evolve over time. Through mutation (small random changes)
    and crossover (combining parts of two solutions), the best solutions are selected
    to create a new group of candidates. This iterative process leads to progressively
    better solutions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，该方法使用TIES-Merging与DARE结合进行N个基础模型的逐层合并。该过程通过使用进化算法进行优化，算法由特定任务的度量标准指导（例如，MGSM的准确度、VQA的ROUGE得分）。为了寻找未知变量，如DARE中的丢弃率百分比以及在合并过程中每个模型参数的权重，进化优化从一组可能的解开始，并随着时间的推移不断演化。通过变异（小的随机变化）和交叉（组合两个解的部分），选择最优解来生成新的候选解组。这一迭代过程带来了逐步更好的解决方案。
- en: 'In the second phase, where a set of N models is given, the goal is to find
    an optimal model with T layers using Frankenmerging. To reduce the search space
    and make the optimization tractable, all layers are laid out in sequential order
    (i.e., all layers in the i-th model followed by those in the i + 1-th model) and
    repeated r times. In this phase, the goal is to find an optimal indicator which
    determines the inclusion/exclusion of layers: if Indicator(i)>0, the ith layer
    is included in the merged model; otherwise, it is excluded.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，给定一组 N 个模型，目标是通过 Frankenmerging 找到一个具有 T 层的最优模型。为了减少搜索空间并使优化过程可行，所有层按顺序排列（即第
    i 个模型中的所有层，紧接着是第 i+1 个模型中的层），并重复 r 次。在此阶段，目标是找到一个最优指示符，确定是否包含/排除某些层：如果 Indicator(i)
    > 0，则第 i 层包含在合并后的模型中；否则，排除该层。
- en: The **EvolutionaryOptimization** process begins with applying the first phase
    to a collection of models. Then, the merged model from the first step is added
    to the given collection and the second phase is applied on this enlarged collection
    to find an optimal indicator which selects T layers for the final merged model.
    This approach applied to merge a Japanese LLM with an English Math LLM to build
    a Japanese Math LLM. The merged model achieved state-of-the-art performance on
    a variety of established Japanese LLM benchmarks, even outperforming models with
    significantly more parameters, despite not being trained for such tasks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**EvolutionaryOptimization** 过程从将第一阶段应用于一组模型开始。然后，将第一阶段的合并模型添加到给定集合中，并在这个扩展的集合上应用第二阶段，找到一个最优指示符，选择
    T 层作为最终合并模型的层。此方法应用于将一个日语LLM与英语数学LLM合并，构建日语数学LLM。合并后的模型在多个已建立的日语LLM基准上取得了最先进的性能，甚至超过了具有显著更多参数的模型，尽管该模型并未针对这些任务进行训练。'
- en: The opinions expressed in this blog post are solely our own and do not reflect
    those of our employer.
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本博文中表达的观点仅代表我们个人的意见，并不反映我们雇主的立场。
- en: '***Also Read Our Previous Post:*** [From Unimodals to Multimodality: DIY Techniques
    for Building Foundational Models](/from-open-source-unimodal-to-multimodal-diy-techniques-for-building-foundational-models-e1df92276379)'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***另请阅读我们之前的文章：*** [从单模态到多模态：构建基础模型的DIY技术](/from-open-source-unimodal-to-multimodal-diy-techniques-for-building-foundational-models-e1df92276379)'
- en: '***References:***'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '***参考文献：***'
- en: '[1] [**Model soup**](https://proceedings.mlr.press/v162/wortsman22a/wortsman22a.pdf)**:**
    Wortsman, Mitchell, et al. “Model soups: averaging weights of multiple fine-tuned
    models improves accuracy without increasing inference time.” *(*2022).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [**Model soup**](https://proceedings.mlr.press/v162/wortsman22a/wortsman22a.pdf)**:**
    Wortsman, Mitchell, 等人。“模型汤：平均多个微调模型的权重可以提高准确性而不增加推理时间。” *（2022年）。'
- en: '[2] [**Task arithmetic**](https://arxiv.org/pdf/2212.04089)**:** Ilharco, Gabriel,
    et al. “Editing models with task arithmetic.” (2022).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [**Task arithmetic**](https://arxiv.org/pdf/2212.04089)**:** Ilharco, Gabriel,
    等人。“通过任务算术编辑模型。”（2022年）。'
- en: '[3] [**TIES**](https://proceedings.neurips.cc/paper_files/paper/2023/file/1644c9af28ab7916874f6fd6228a9bcf-Paper-Conference.pdf)**:**
    Yadav, Prateek, et al. “Ties-merging: Resolving interference when merging models.”
    (2024).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [**TIES**](https://proceedings.neurips.cc/paper_files/paper/2023/file/1644c9af28ab7916874f6fd6228a9bcf-Paper-Conference.pdf)**:**
    Yadav, Prateek, 等人。“Ties-merging：在合并模型时解决干扰问题。”（2024年）。'
- en: '[4] [**DARE**](https://openreview.net/pdf?id=fq0NaiU8Ex): Yu, Le, et al. “Language
    models are super mario: Absorbing abilities from homologous models as a free lunch.”
    *(*2024).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [**DARE**](https://openreview.net/pdf?id=fq0NaiU8Ex)：Yu, Le, 等人。“语言模型就是超级马里奥：从同源模型中吸收能力作为免费午餐。”
    *（2024年）。'
- en: '[5] [**Fisher Merging**](https://arxiv.org/pdf/2111.09832)Matena, Michael S.,
    et al. “Merging models with fisher-weighted averaging.” (2022).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [**Fisher Merging**](https://arxiv.org/pdf/2111.09832)Matena, Michael S.,
    等人。“通过Fisher加权平均合并模型。”（2022年）。'
- en: '[6] [**RegMean**](https://arxiv.org/pdf/2212.09849)**:** Jin, Xisen, et al.
    “Dataless knowledge fusion by merging weights of language models.” (2022).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [**RegMean**](https://arxiv.org/pdf/2212.09849)**:** Jin, Xisen, 等人。“通过合并语言模型的权重实现无数据知识融合。”（2022年）。'
- en: '[7] [**Git-Rebasin**](https://arxiv.org/pdf/2209.04836)**:** Ainsworth, Samuel
    K., et al. “Git re-basin: Merging models modulo permutation symmetries.” (2022).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [**Git-Rebasin**](https://arxiv.org/pdf/2209.04836)**:** Ainsworth, Samuel
    K., 等人。“Git re-basin：合并模型时考虑置换对称性。”（2022年）。'
- en: '[8] [**REPAIR**](https://arxiv.org/pdf/2211.08403)**:** Jordan, Keller, et
    al. “Repair: Renormalizing permuted activations for interpolation repair.” (2022).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [**REPAIR**](https://arxiv.org/pdf/2211.08403)**:** Jordan, Keller, 等人。“Repair：重新归一化置换激活以进行插值修复。”（2022年）。'
- en: '[9] [**Frankenmerging**](https://github.com/arcee-ai/mergekit)**:** Charles
    O. Goddard. 2024\. mergekit.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [**弗兰肯合并**](https://github.com/arcee-ai/mergekit)**：** Charles O. Goddard.
    2024\. mergekit.'
- en: '[10] [**EvolutionaryOptimization**](https://arxiv.org/pdf/2403.13187)**:**
    Akiba, Takuya, et al. “Evolutionary optimization of model merging recipes.” (2024).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] [**进化优化**](https://arxiv.org/pdf/2403.13187)**：** Akiba, Takuya 等人。“模型合并方案的进化优化。”（2024）。'
- en: '[11] Shoemake, Ken. “[Animating rotation with quaternion curves](https://dl.acm.org/doi/pdf/10.1145/325334.325242).”
    (1985).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Shoemake, Ken. “[使用四元数曲线进行旋转动画](https://dl.acm.org/doi/pdf/10.1145/325334.325242)。”（1985）。'
- en: '[12] [**LMC**](https://proceedings.neurips.cc/paper_files/paper/2019/file/05e97c207235d63ceb1db43c60db7bbb-Paper.pdf)**:**
    Nagarajan, Vaishnavh, et al. “Uniform convergence may be unable to explain generalization
    in deep learning.” (2019).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] [**LMC**](https://proceedings.neurips.cc/paper_files/paper/2019/file/05e97c207235d63ceb1db43c60db7bbb-Paper.pdf)**：**
    Nagarajan, Vaishnavh 等人。“均匀收敛可能无法解释深度学习中的泛化。”（2019）。'
- en: '[13] Kaddour, Jean, et al. “[When do flat minima optimizers work](https://proceedings.neurips.cc/paper_files/paper/2022/file/69b5534586d6c035a96b49c86dbeece8-Paper-Conference.pdf)?.”
    (2022)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Kaddour, Jean 等人。“[平坦最小值优化器何时有效](https://proceedings.neurips.cc/paper_files/paper/2022/file/69b5534586d6c035a96b49c86dbeece8-Paper-Conference.pdf)？”（2022）'
- en: '[14] Petzka, Henning, et al. “[Relative flatness and generalization](https://proceedings.neurips.cc/paper_files/paper/2021/file/995f5e03890b029865f402e83a81c29d-Paper.pdf).”
    (2021)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Petzka, Henning 等人。“[相对平坦性与泛化](https://proceedings.neurips.cc/paper_files/paper/2021/file/995f5e03890b029865f402e83a81c29d-Paper.pdf)。”（2021）'
