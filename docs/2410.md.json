["```py\n// On receiving a phone call, Twilio forwards the incoming call request to\n// a webhook we specify, which is this endpoint here. This allows us to \n// create programatic voice applications, for example using an AI agent\n// to handle the phone call\n// \n// So, here we are providing an initial response to the call, and creating\n// a websocket (called a MediaStream in Twilio, more on that below) to receive\n// any future audio that comes into the call\nfastify.all('/incoming', async (request, reply) => {\n    const twimlResponse = `<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n                          <Response>\n                              <Say>Please wait while we connect your call to the A. I. voice assistant, powered by Twilio and the Open-A.I. Realtime API</Say>\n                              <Pause length=\"1\"/>\n                              <Say>O.K. you can start talking!</Say>\n                              <Connect>\n                                  <Stream url=\"wss://${request.headers.host}/media-stream\" />\n                              </Connect>\n                          </Response>`;\n\n    reply.type('text/xml').send(twimlResponse);\n});\n\nfastify.register(async (fastify) => {\n\n    // Here we are connecting our application to the websocket media stream we\n    // setup above. That means all audio that comes though the phone will come\n    // to this websocket connection we have setup here\n    fastify.get('/media-stream', { websocket: true }, (connection, req) => {\n        console.log('Client connected');\n\n        // Now, we are creating websocket connection to the OpenAI Realtime API\n        // This is the second leg of the flow diagram above\n        const openAiWs = new WebSocket('wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01', {\n            headers: {\n                Authorization: `Bearer ${OPENAI_API_KEY}`,\n                \"OpenAI-Beta\": \"realtime=v1\"\n            }\n        });\n\n        ...\n\n        // Here we are setting up the listener on the OpenAI Realtime API \n        // websockets connection. We are specifying how we would like it to\n        // handle any incoming audio streams that have come back from the\n        // Realtime API.\n        openAiWs.on('message', (data) => {\n            try {\n                const response = JSON.parse(data);\n\n                ...\n\n        // This response type indicates an LLM responce from the Realtime API\n        // So we want to forward this response back to the Twilio Mediat Stream\n        // websockets connection, which the caller will hear as a response on\n        // on the phone\n                if (response.type === 'response.audio.delta' && response.delta) {\n                    const audioDelta = {\n                        event: 'media',\n                        streamSid: streamSid,\n                        media: { payload: Buffer.from(response.delta, 'base64').toString('base64') }\n                    };\n         // This is the actual part we are sending it back to the Twilio\n         // MediaStream websockets connection. Notice how we are sending the\n         // response back directly. No need for text to speech conversion from\n         // the OpenAI response. The OpenAI Realtime API already provides the\n         // response as an audio stream (i.e speech to speech)\n                    connection.send(JSON.stringify(audioDelta));\n                }\n            } catch (error) {\n                console.error('Error processing OpenAI message:', error, 'Raw message:', data);\n            }\n        });\n\n        // This parts specifies how we handle incoming messages to the Twilio\n        // MediaStream websockets connection i.e how we handle audio that comes\n        // into the phone from the caller\n        connection.on('message', (message) => {\n            try {\n                const data = JSON.parse(message);\n\n                switch (data.event) {\n        // This case ('media') is that state for when there is audio data \n        // available on the Twilio MediaStream from the caller\n                    case 'media':\n                        // we first check out OpenAI Realtime API websockets\n                        // connection is open \n                        if (openAiWs.readyState === WebSocket.OPEN) {\n                            const audioAppend = {\n                                type: 'input_audio_buffer.append',\n                                audio: data.media.payload\n                            };\n                        // and then forward the audio stream data to the\n                        // Realtime API. Again, notice how we are sending the\n                        // audio stream directly, not speech to text converstion\n                        // as would have been required previously\n                            openAiWs.send(JSON.stringify(audioAppend));\n                        }\n                        break;\n\n                  ...\n                }\n            } catch (error) {\n                console.error('Error parsing message:', error, 'Message:', message);\n            }\n        });\n\n...\n\nfastify.listen({ port: PORT }, (err) => {\n    if (err) {\n        console.error(err);\n        process.exit(1);\n    }\n    console.log(`Server is listening on port ${PORT}`);\n});\n```"]