["```py\n!pip install -q groq\n!pip install -U accelerate bitsandbytes datasets evaluate \n!pip install -U peft transformers trl \n```", "```py\n# For Google Colab settings\nfrom google.colab import userdata, drive\n\n# This will prompt for authorization\ndrive.mount('/content/drive')\n\n# Set the working directory\n%cd '/content/drive/MyDrive/postedBlogs/llama3RE'\n```", "```py\n# For Hugging Face Hub setting\nfrom huggingface_hub import login\n\n# Upload the HuggingFace token (should have WRITE access) from Colab secrets\nHF = userdata.get('HF')\n\n# This is needed to upload the model to HuggingFace\nlogin(token=HF,add_to_git_credential=True)\n```", "```py\n# Create a path variable for the data folder\ndata_path = '/content/drive/MyDrive/postedBlogs/llama3RE/datas/'\n\n# Full fine-tuning dataset\nsft_dataset_file = f'{data_path}sft_train_data.json'\n\n# Data collected from the the mini-test\nmini_data_path = f'{data_path}mini_data.json'\n\n# Test data containing all three outputs\nall_tests_data = f'{data_path}all_tests.json'\n\n# The adjusted training dataset\ntrain_data_path = f'{data_path}sft_train_data.json'\n\n# Create a path variable for the SFT model to be saved locally\nsft_model_path = '/content/drive/MyDrive/llama3RE/Llama3_RE/'\n```", "```py\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"databricks/databricks-dolly-15k\")\n\n# Choose the desired category from the dataset\nie_category = [e for e in dataset[\"train\"] if e[\"category\"]==\"information_extraction\"]\n\n# Retain only the context from each instance\nie_context = [e[\"context\"] for e in ie_category]\n\n# Split the text into sentences (at the period) and keep the first sentence\nreduced_context = [text.split('.')[0] + '.' for text in ie_context]\n\n# Retain sequences of specified lengths only (use character length)\nsampler = [e for e in reduced_context if 30 < len(e) < 170]\n```", "```py\nsystem_message = \"\"\"You are an experienced annontator. \nExtract all entities and the relations between them from the following text. \nWrite the answer as a triple entity1|relationship|entitity2\\. \nDo not add anything else.\nExample Text: Alice is from France.\nAnswer: Alice|is from|France.\n\"\"\"\n```", "```py\nmessages = [[\n    {\"role\": \"system\",\"content\": f\"{system_message}\"},\n    {\"role\": \"user\", \"content\": e}] for e in sampler]\n```", "```py\nimport os\nfrom groq import Groq\n\ngclient = Groq(\n    api_key=userdata.get(\"GROQ\"),\n)\n```", "```py\nimport time\nfrom tqdm import tqdm\n\ndef process_data(prompt):\n\n    \"\"\"Send one request and retrieve model's generation.\"\"\"\n\n    chat_completion = gclient.chat.completions.create(\n        messages=prompt, # input prompt to send to the model\n        model=\"llama3-70b-8192\", # according to GroqCloud labeling\n        temperature=0.5, # controls diversity\n        max_tokens=128, # max number tokens to generate\n        top_p=1, # proportion of likelihood weighted options to consider\n        stop=None, # string that signals to stop generating\n        stream=False, # if set partial messages are sent\n    )\n    return chat_completion.choices[0].message.content\n\ndef send_messages(messages):\n\n    \"\"\"Process messages in batches with a pause between batches.\"\"\"\n\n   batch_size = 10\n    answers = []\n\n    for i in tqdm(range(0, len(messages), batch_size)): # batches of size 10\n\n        batch = messages[i:i+10]  # get the next batch of messages\n\n        for message in batch:\n            output = process_data(message)\n            answers.append(output)\n\n        if i + 10 < len(messages):  # check if there are batches left\n            time.sleep(10)  # wait for 10 seconds\n\n    return answers\n```", "```py\n# Data generation with Llama3-70B\nanswers = send_messages(messages)\n\n# Combine input data with the generated dataset\ncombined_dataset = [{'text': user, 'gold_re': output} for user, output in zip(sampler, answers)]\n```", "```py\nimport random\nrandom.seed(17)\n\n# Select 20 random entries\nmini_data = random.sample(combined_dataset, 20)\n\n# Build conversational format\nparsed_mini_data = [[{'role': 'system', 'content': system_message},\n                     {'role': 'user', 'content': e['text']}] for e in mini_data]\n\n# Create the training set\ntrain_data = [item for item in combined_dataset if item not in mini_data]\n```", "```py\n{'text': 'Long before any knowledge of electricity existed, people were aware of shocks from electric fish.',\n 'gold_re': 'people|were aware of|shocks\\nshocks|from|electric fish\\nelectric fish|had|electricity',\n 'test_re': 'electric fish|were aware of|shocks'}\n```", "```py\ndef create_conversation(sample):\n    return {\n        \"messages\": [\n            {\"role\": \"system\",\"content\": system_message},\n            {\"role\": \"user\", \"content\": sample[\"text\"]},\n            {\"role\": \"assistant\", \"content\": sample[\"gold_re\"]}\n        ]\n    }\n\nfrom datasets import load_dataset, Dataset\n\ntrain_dataset = Dataset.from_list(train_data)\n\n# Transform to conversational format\ntrain_dataset = train_dataset.map(create_conversation,\n                      remove_columns=train_dataset.features,\n                      batched=False)\n```", "```py\nmodel_id  =  \"meta-llama/Meta-Llama-3-8B\"\n```", "```py\nfrom transformers import AutoTokenizer\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id,\n                                          use_fast=True,\n                                          trust_remote_code=True)\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token_id =  tokenizer.eos_token_id\ntokenizer.padding_side = 'left'\n\n# Set a maximum length\ntokenizer.model_max_length = 512\n```", "```py\nfrom transformers import BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n```", "```py\nfrom transformers import AutoModelForCausalLM\nfrom peft import prepare_model_for_kbit_training\nfrom trl import setup_chat_format\n\ndevice_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=device_map,\n    attn_implementation=\"flash_attention_2\",\n    quantization_config=bnb_config\n)\n\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = prepare_model_for_kbit_training(model)\n```", "```py\nfrom peft import LoraConfig\n\n# According to Sebastian Raschka findings\npeft_config = LoraConfig(\n        lora_alpha=128, #32\n        lora_dropout=0.05,\n        r=256,  #16\n        bias=\"none\",\n        target_modules=[\"q_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \n          \"down_proj\", \"k_proj\", \"v_proj\"],\n        task_type=\"CAUSAL_LM\",\n)\n```", "```py\nfrom transformers import TrainingArguments\n\n# Adapted from  Phil Schmid blogpost\nargs = TrainingArguments(\n    output_dir=sft_model_path,              # directory to save the model and repository id\n    num_train_epochs=2,                     # number of training epochs\n    per_device_train_batch_size=4,          # batch size per device during training\n    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,            # use gradient checkpointing to save memory, use in distributed training\n    optim=\"adamw_8bit\",                     # choose paged_adamw_8bit if not enough memory\n    logging_steps=10,                       # log every 10 steps\n    save_strategy=\"epoch\",                  # save checkpoint every epoch\n    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n    bf16=True,                              # use bfloat16 precision\n    tf32=True,                              # use tf32 precision\n    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n    push_to_hub=True,                       # push model to Hugging Face hub\n    hub_model_id=\"llama3-8b-sft-qlora-re\",\n    report_to=\"tensorboard\",               # report metrics to tensorboard\n    )\n```", "```py\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=sft_dataset,\n    peft_config=peft_config,\n    max_seq_length=512,\n    tokenizer=tokenizer,\n    packing=False, # True if the dataset is large\n    dataset_kwargs={\n        \"add_special_tokens\": False,  # the template adds the special tokens\n        \"append_concat_token\": False, # no need to add additional separator token\n    }\n)\n\ntrainer.train()\ntrainer.save_model()\n```", "```py\nimport torch\nimport gc\ndel model\ndel tokenizer\ngc.collect()\ntorch.cuda.empty_cache()\n```", "```py\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline\nimport torch\n\n# HF model\npeft_model_id = \"solanaO/llama3-8b-sft-qlora-re\"\n\n# Load Model with PEFT adapter\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n  peft_model_id,\n  device_map=\"auto\",\n  torch_dtype=torch.float16,\n  offload_buffers=True\n)\n```", "```py\nokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token_id =  tokenizer.eos_token_id\n```", "```py\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n```", "```py\ndef create_input_prompt(sample):\n    return {\n        \"messages\": [\n            {\"role\": \"system\",\"content\": system_message},\n            {\"role\": \"user\", \"content\": sample[\"text\"]},\n        ]\n    }\n\nfrom datasets import Dataset\n\ntest_dataset = Dataset.from_list(mini_data)\n\n# Transform to conversational format\ntest_dataset = test_dataset.map(create_input_prompt,\n                      remove_columns=test_dataset.features,\n                      batched=False)\n```", "```py\n Generate the input prompt\nprompt = pipe.tokenizer.apply_chat_template(test_dataset[2][\"messages\"][:2],\n                                            tokenize=False,\n                                            add_generation_prompt=True)\n# Generate the output\noutputs = pipe(prompt,\n              max_new_tokens=128,\n              do_sample=False,\n              temperature=0.1,\n              top_k=50,\n              top_p=0.1,\n              )\n# Display the results\nprint(f\"Question: {test_dataset[2]['messages'][1]['content']}\\n\")\nprint(f\"Gold-RE: {test_sampler[2]['gold_re']}\\n\")\nprint(f\"LLama3-8B-RE: {test_sampler[2]['test_re']}\\n\")\nprint(f\"SFT-Llama3-8B-RE: {outputs[0]['generated_text'][len(prompt):].strip()}\")\n```", "```py\nQuestion: Long before any knowledge of electricity existed, people were aware of shocks from electric fish.\n\nGold-RE: people|were aware of|shocks\n    shocks|from|electric fish\n    electric fish|had|electricity\n\nLLama3-8B-RE: electric fish|were aware of|shocks\n\nSFT-Llama3-8B-RE: people|were aware of|shocks\n         shocks|from|electric fish\n```"]