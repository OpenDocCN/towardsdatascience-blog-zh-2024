<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Why Scaling Works: Inductive Biases vs The Bitter Lesson</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Why Scaling Works: Inductive Biases vs The Bitter Lesson</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-scaling-works-inductive-biases-vs-the-bitter-lesson-9c2782f99b18?source=collection_archive---------8-----------------------#2024-10-22">https://towardsdatascience.com/why-scaling-works-inductive-biases-vs-the-bitter-lesson-9c2782f99b18?source=collection_archive---------8-----------------------#2024-10-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e0f5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Building deep insights with a toy problem</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@TarikDzekman?source=post_page---byline--9c2782f99b18--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Tarik Dzekman" class="l ep by dd de cx" src="../Images/0c66b22ecbdbbce79b2516e555c67432.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*89BjkQMI4MLzZQvGJu4k3Q.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--9c2782f99b18--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@TarikDzekman?source=post_page---byline--9c2782f99b18--------------------------------" rel="noopener follow">Tarik Dzekman</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--9c2782f99b18--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj bh"><figure class="mk ml mm mn mo mj bh paragraph-image"><img src="../Images/0183bdb92e1fdfde53b87a0823b4b64a.png" data-original-src="https://miro.medium.com/v2/resize:fit:3840/format:webp/1*UpgRZBoe6nA3GKYov5vnBA.png"/><figcaption class="mq mr ms mt mu mv mw bf b bg z dx">Source: All images by the author</figcaption></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e0d9" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Over the past decade we’ve witnessed the power of scaling deep learning models. Larger models, trained on heaps of data, consistently outperform previous methods in language modelling, image generation, playing games, and even protein folding. To understand why scaling works, let’s look at a toy problem.</p><h1 id="9607" class="nt nu fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Introducing a Toy Problem</h1><p id="61e5" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">We start with a 1D manifold weaving its way through the 2D plane and forming a spiral:</p><figure class="mk ml mm mn mo mj mt mu paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mt mu ou"><img src="../Images/51d4cee4cf66501ec6a0feaeb883bc77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*xIeXpXqQhWS5SyPVD7AIgA.png"/></div></div></figure><p id="ccc4" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Now we add a heatmap which represents the probability density of sampling a particular 2D point. Notably, this probability density is <em class="oz">independent </em>of the shape of the manifold:</p><figure class="mk ml mm mn mo mj mt mu paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mt mu ou"><img src="../Images/9b3c3785c731f0e4ca60a264159b21fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*sXrH5fyx2s5nXXZCgX8ZoQ.png"/></div></div></figure><p id="d2dd" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Let’s assume that the data on either side of the manifold is always completely separable (i.e. there is no noise). Datapoints on the outside of the manifold are blue and those on the inside are orange. If we draw a sample of N=1000 points it may look like this:</p><figure class="mk ml mm mn mo mj mt mu paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mt mu ou"><img src="../Images/542fc4ff7a0dabbe6f44e7f2e8e6b28c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*RwuyHosf4wNw3kfaR2monw.png"/></div></div></figure><blockquote class="pa"><p id="10d6" class="pb pc fq bf pd pe pf pg ph pi pj ns dx">Toy problem: How do we build a model which predicts the colour of a point based on its 2D coordinates?</p></blockquote><p id="dd28" class="pw-post-body-paragraph mx my fq mz b go pk nb nc gr pl ne nf ng pm ni nj nk pn nm nn no po nq nr ns fj bk">In the real world we often can’t sample uniformly from all parts of the feature space. For example, in image classification it’s easy to find images of trees in general but less easy to find many examples of specific trees. As a result, it may be harder for a model to learn the difference between species there aren’t many examples of. Similarly, in our toy problem, different parts of the space will become difficult to predict simply because they are harder to sample.</p><h1 id="0cf6" class="nt nu fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Solving the Toy Problem</h1><p id="17cc" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">First, we build a simple neural network with 3 layers, running for 1000 epochs. The neural network’s predictions are heavily influenced by the particulars of the sample. As a result, the trained model has difficulty inferring the shape of the manifold just because of sampling sparsity:</p><figure class="mk ml mm mn mo mj mt mu paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mt mu pp"><img src="../Images/1d1f434d9dde81d62fe6b51f6268f37c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9vaI5GKD1olVVARM8I-t3Q.png"/></div></div></figure><p id="e5d6" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Even knowing that the points are completely separable, there are infinitely many ways to draw a boundary around the sampled points. Based on the sample data, why should any one boundary be considered superior to another?</p><p id="b946" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">With regularisation techniques we could encourage the model to produce a smoother boundary rather than curving tightly around predicted points. That helps to an extent but it won’t solve our problem in regions of sparsity.</p><blockquote class="pa"><p id="dc01" class="pb pc fq bf pd pe pq pr ps pt pu ns dx">Since we already know the manifold is a spiral, can we encourage the model to make spiral-like predictions?</p></blockquote><p id="1932" class="pw-post-body-paragraph mx my fq mz b go pk nb nc gr pl ne nf ng pm ni nj nk pn nm nn no po nq nr ns fj bk">We can add what’s called an “<a class="af pv" href="https://en.wikipedia.org/wiki/Inductive_bias" rel="noopener ugc nofollow" target="_blank">inductive prior</a>”: something we put in the model architecture or the training process which contains information about the problem space. In this toy problem we can do some <a class="af pv" href="https://en.wikipedia.org/wiki/Feature_engineering" rel="noopener ugc nofollow" target="_blank">feature engineering</a> and adjust the way we present inputs to the model. Instead of 2D (x, y) coordinates, we transform the input into <a class="af pv" href="https://en.wikipedia.org/wiki/Polar_coordinate_system" rel="noopener ugc nofollow" target="_blank">polar coordinates</a> (r, θ).</p><p id="8553" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Now the neural network can make predictions based on the distance and angle from the origin. This biases the model towards producing decision boundaries which are more curved. Here is how the newly trained model predicts the decision boundary:</p><figure class="mk ml mm mn mo mj mt mu paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mt mu pp"><img src="../Images/86f45d8f83378a2d8c9c06d364b6e0e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h20alGwRDqx3DYUjYVSWCw.png"/></div></div></figure><p id="7c1b" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Notice how much better the model performs in parts of the input space where there are no samples. The features of those missing points remain similar to features of observed points and so the model can predict an effective boundary without seeing additional data.</p><p id="775c" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><em class="oz">Obviously, inductive priors are useful.</em></p><p id="7bcc" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Most architecture decisions will induce an inductive prior. Let’s try some enhancements and try to think about what kind of inductive prior they introduce:</p><ol class=""><li id="943c" class="mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns pw px py bk">Focal Loss — increases the loss on data points the model finds hard to predict. This might improve accuracy at the cost of increasing the model complexity around those points (as we would expect from the bias-variance trade-off). To reduce the impact of increased variance we can add some regularisation.</li><li id="17a0" class="mx my fq mz b go pz nb nc gr qa ne nf ng qb ni nj nk qc nm nn no qd nq nr ns pw px py bk">Weight Decay — L2 norm on the size of the weights prevents the model from learning features weighted too strongly to any one sample.</li><li id="84c1" class="mx my fq mz b go pz nb nc gr qa ne nf ng qb ni nj nk qc nm nn no qd nq nr ns pw px py bk">Layer Norm — has a lot of subtle effects, one of which could be that the model focuses more on the relationships between points rather than their magnitude, which might help offset the increased variance from using Focal Loss.</li></ol><p id="3124" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">After making all of these improvements, how much better does our predicted manifold look?</p><figure class="mk ml mm mn mo mj mt mu paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mt mu pp"><img src="../Images/75acb7830492f71044a8ae525f9a5599.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JCQuSoZghiHnk-RVGXF6Bw.png"/></div></div></figure><p id="a403" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz fr">Not much better at all</strong>. In fact, it’s introduced an artefact near the centre of the spiral. And it’s still failed to predict anything at the end of the spiral (in the upper-left quadrant) where there is no data. That said, it has managed to capture more of the curve near the origin which is a plus.</p><h1 id="fa6d" class="nt nu fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">The Bitter Lesson</h1><p id="beee" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">Now suppose that another research team has no idea that there’s a hard boundary in the shape of a single continuous spiral. For all they know there could be pockets inside pockets with fuzzy probabilistic boundaries.</p><p id="90ec" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">However, this team is able to collect a sample of 10,000 instead of 1,000. For their model they just use a k-Nearest Neighbour (kNN) approach with k=5.</p><p id="6e2c" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><em class="oz">Side note: k=5 is a poor inductive prior here. For this problem k=1 is generally better. C</em><strong class="mz fr"><em class="oz">hallenge: </em></strong><em class="oz">can you figure out why? Add a comment to this article with your answer.</em></p><p id="b668" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Now, kNN is not a particularly powerful algorithm compared to a neural network. However, even with a bad inductive prior here is how the kNN solution scales with 10x more data:</p></div></div><div class="mj"><div class="ab cb"><div class="lm qe ln qf lo qg cf qh cg qi ci bh"><figure class="mk ml mm mn mo mj qk ql paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mt mu qj"><img src="../Images/8aef06ef199cae51f7879b2258635d85.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*jHSrdUxTE9JFNvYCqVVuJg.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="743c" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">With 10x more data the kNN approach is performing closer to the neural network. In particular it’s better at predicting the shape at the tails of the spiral, although it’s still missing that hard to sample upper-left quadrant. It’s also making some mistakes, often producing a fuzzier border.</p><p id="6117" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">What if we added 100x or 1000x more data? Let’s see how both the kNN vs Neural Network approaches compare as we scale the amount of data used:</p></div></div><div class="mj"><div class="ab cb"><div class="lm qe ln qf lo qg cf qh cg qi ci bh"><figure class="mk ml mm mn mo mj qk ql paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mt mu qm"><img src="../Images/1b143968f78d15c4666d0119bbd22d6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*ocjqtygaWsI_Bq2vJXrkSg.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="87db" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">As we increase the size of the training data it largely doesn’t matter which model we use. What’s more, given enough data, the lowly kNN actually starts to perform better than our carefully crafted neural network with well thought out inductive priors.</p><blockquote class="qn qo qp"><p id="eecf" class="mx my oz mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">This is a big lesson. As a field, we still have not thoroughly learned it, as we are continuing to make the same kind of mistakes. To see this, and to effectively resist it, we have to understand the appeal of these mistakes. We have to learn the bitter lesson that building in how we think we think does not work in the long run. The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.</p><p id="a0fa" class="mx my oz mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">From Rich Sutton’s essay “<a class="af pv" href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" rel="noopener ugc nofollow" target="_blank">The Bitter Lesson</a>”</p></blockquote><p id="e4d3" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Superior inductive priors are no match for just using more compute to solve the problem. In this case, “more compute” just involves storing a larger sample of data in memory and using kNN to match to the nearest neighbours. We’ve seen this play out with transformer-based Large Language Models (LLMs). They continue to overpower other Natural Language Processing techniques simply by training larger and larger models, with more and more GPUs, on more and more text data.</p><h1 id="9c54" class="nt nu fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">But Surely…?</h1><p id="afb7" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">This toy example has a subtle issue we’ve seen pop up with both models: failing to predict that sparse section of the spiral in the upper-left quadrant. This is particularly relevant to Large Language Models, training reasoning capabilities, and our quest towards “Artificial General Intelligence” (AGI). To see what I mean let’s zoom in on that unusual shaped tail in the upper-left.</p><figure class="mk ml mm mn mo mj mt mu paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mt mu pp"><img src="../Images/fbbdcca1634e9a1ac90ec1566e3ee6c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SPqAzI_SzyFmwEEj4gH9KA.png"/></div></div></figure><p id="bf62" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">This region has a particularly low sampling density and the boundary is quite different to the rest of the manifold. Suppose this area is something we care a lot about, for example: generating “reasoning” from a Large Language Model (LLM). Not only is such data rare (if randomly sampled) but it is sufficiently different to the rest of the data, which means features from other parts of the space are not useful in making predictions here. Additionally, notice how sharp and specific the boundary is — points sampled near the tip could very easily fall on the outside.</p><p id="ffb7" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Let’s see how this compares to a simplified view of training an LLM on text-based reasoning:</p><ol class=""><li id="3c10" class="mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns pw px py bk">Reasoning is complicated and we probably won’t find a solution by fitting a “smooth” line that averages out a few samples. To solve a reasoning problem it’s not enough to follow an apparent pattern but it’s necessary to really understand the problem. Training a model to reason will likely need a lot of data.</li><li id="1a27" class="mx my fq mz b go pz nb nc gr qa ne nf ng qb ni nj nk qc nm nn no qd nq nr ns pw px py bk">Randomly sampling data from the internet doesn’t give us many samples where humans explain intricate mental reasoning steps required to get to an answer. Paying people to explicitly generate reasoning data may help increase the density. But it’s a slow process and the amount of data needed is actually quite high.</li><li id="a3c9" class="mx my fq mz b go pz nb nc gr qa ne nf ng qb ni nj nk qc nm nn no qd nq nr ns pw px py bk">We care a lot about getting it right because reasoning abilities would open up a lot more use cases for AI.</li></ol><p id="587c" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Of course reasoning is more complex than predicting the tip of this spiral. There are usually multiple ways to get to a correct answer, there may be many correct answers, and sometimes the boundary can be fuzzy. However, we are also not without inductive priors in deep learning architectures, including techniques using reinforcement learning.</p><p id="1629" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">In our toy problem there is regularity in the shape of the boundary and so we used an inductive prior to encourage the model to learn that shape. When modelling reasoning, if we could construct a <a class="af pv" href="https://en.wikipedia.org/wiki/Manifold_hypothesis" rel="noopener ugc nofollow" target="_blank">manifold in a higher dimensional space</a> representing concepts and ideas, there would be some regularity to its shape that could be exploited for an inductive prior. If<em class="oz"> The Bitter Lesson</em> continues to hold then we would assume the search for such an inductive prior is not the path forward. We just need to scale compute. And so far the best way to do that is to collect more data and throw it at larger models.</p><p id="4a59" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">But surely, I hear you say, transformers were so successful because the <em class="oz">attention mechanism</em> introduced a strong inductive prior into language modelling? The paper “<a class="af pv" href="https://arxiv.org/pdf/2410.01201" rel="noopener ugc nofollow" target="_blank">Were RNNs all we needed</a>” suggests that a simplified Recurrent Neural Network (RNN) can also perform well if scaled up. It’s not because of an inductive prior. It’s because the paper improved the speed with which we can train an RNN on large amounts of data. And that’s why transformers are so effective — parallelism allowed us to leverage much more compute. It’s an architecture straight from the heart of <em class="oz">The Bitter Lesson</em>.</p><h1 id="ffc1" class="nt nu fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Running Out of Data?</h1><p id="e3d9" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">There’s always more data. Synthetic data or reinforcement learning techniques like self-play can generate infinite data. Although without connection to the real world the validity of that data can get fuzzy. That’s why techniques like RLHF have hand crafted data as a base — so that the model of human preferences can be as accurate as possible. Also, given that reasoning is often mathematical, it may be easy to generate such data using automated methods.</p><p id="07ec" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Now the question is: given the current inductive priors we have, how much data would it take to train models with true reasoning capability?</p><p id="4296" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">If <em class="oz">The Bitter Lesson</em> continues to apply the answer is: it doesn’t matter, finding better ways to leverage more compute will continue to give better gains than trying to find superior inductive priors^. This means that the search for ever more powerful AI is firmly in the domain of the companies with the biggest budgets.</p><p id="6feb" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">And after writing all of this… I still hope that’s not true.</p></div></div></div><div class="ab cb qq qr qs qt" role="separator"><span class="qu by bm qv qw qx"/><span class="qu by bm qv qw qx"/><span class="qu by bm qv qw"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="437d" class="nt nu fq bf nv nw qy gq ny nz qz gt ob oc ra oe of og rb oi oj ok rc om on oo bk">About me</h1><p id="14a8" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">I’m the Lead AI Engineer @ <a class="af pv" href="https://www.affinda.com/" rel="noopener ugc nofollow" target="_blank">Affinda</a>. Check out our <a class="af pv" href="https://www.affinda.com/case-studies" rel="noopener ugc nofollow" target="_blank">AI Document Automation Case Studies</a> to learn more.</p><p id="0d6e" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz fr">Some of my long reads:</strong></p><ul class=""><li id="39f3" class="mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns rd px py bk"><a class="af pv" href="https://medium.com/towards-data-science/what-do-large-language-models-understand-befdb4411b77" rel="noopener">What do Large Language Models “Understand”?</a></li><li id="73c0" class="mx my fq mz b go pz nb nc gr qa ne nf ng qb ni nj nk qc nm nn no qd nq nr ns rd px py bk"><a class="af pv" href="https://medium.com/towards-data-science/exploring-the-ai-alignment-problem-with-gridworlds-2683f2f5af38" rel="noopener">Exploring the AI Alignment Problem with GridWorlds</a></li></ul><p id="b1ed" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz fr">More practical reads:</strong></p><ul class=""><li id="506b" class="mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns rd px py bk"><a class="af pv" href="https://medium.com/management-matters/managing-risks-in-deploying-generative-ai-393254259497" rel="noopener">5 Hidden Risks in Deploying Generative AI</a></li><li id="8d8f" class="mx my fq mz b go pz nb nc gr qa ne nf ng qb ni nj nk qc nm nn no qd nq nr ns rd px py bk"><a class="af pv" href="https://medium.com/towards-data-science/how-i-deal-with-hallucinations-at-an-ai-startup-9fc4121295cc" rel="noopener">How I Deal with Hallucinations at an AI Startup</a></li></ul><h1 id="cba3" class="nt nu fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Appendix</h1><p id="c099" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">^ It’s important to note that the essay “The Bitter Lesson” isn’t explicitly about inductive biases vs collecting more data. Throwing more data at bigger models is one way to leverage more compute. And in deep learning that usually means finding better ways to increase parallelism in training. Lately it’s also about leveraging more inference time compute (e.g. o1-preview). There may yet be other ways. The topic is slightly more nuanced than I’ve presented here in this short article.</p></div></div></div></div>    
</body>
</html>