- en: Stop the Count! Why Putting A Time Limit on Metrics is Critical for Fast and
    Accurate Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/stop-the-count-why-putting-a-time-limit-on-metrics-is-critical-for-fast-and-accurate-experiments-468123776f57?source=collection_archive---------4-----------------------#2024-07-17](https://towardsdatascience.com/stop-the-count-why-putting-a-time-limit-on-metrics-is-critical-for-fast-and-accurate-experiments-468123776f57?source=collection_archive---------4-----------------------#2024-07-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why your experiments might never reach significance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://zachlog.com/?source=post_page---byline--468123776f57--------------------------------)[![Zach
    Flynn](../Images/cfa6255e5dbc765a0838a594faafaf3c.png)](https://zachlog.com/?source=post_page---byline--468123776f57--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--468123776f57--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--468123776f57--------------------------------)
    [Zach Flynn](https://zachlog.com/?source=post_page---byline--468123776f57--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--468123776f57--------------------------------)
    ·6 min read·Jul 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39f680e5154346a462b53fb816d38ca1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Andrik Langfield](https://unsplash.com/@andriklangfield?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/pocket-watch-at-355-0rTCXZM7Xfo?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Experiments usually compare the frequency of an event (or some other sum metric)
    after either exposure (treatment) or non-exposure (control) to some intervention.
    For example: we might compare the number of purchases, minutes spent watching
    content, or number of clicks on a call-to-action.'
  prefs: []
  type: TYPE_NORMAL
- en: While this setup may seem plain, standard, and common, it is only “common”.
    It is a thorny analysis problem *unless* we cap the length of time post-exposure
    where we compute the metric.
  prefs: []
  type: TYPE_NORMAL
- en: The Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In general, for metrics that simply sum up a metric post-exposure (“unlimited
    metrics”), the following statements are NOT true:'
  prefs: []
  type: TYPE_NORMAL
- en: If I run the experiment longer, I will eventually reach significance if the
    experiment has some effect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The average treatment effect is well-defined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When computing the sample size, I can use normal sample sizing calculations
    to compute experiment length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To see why, suppose we have a metric *Y* that is the cumulative sum of *X,*
    a metric defined over a single time unit.For example, X might be the number of
    minutes watched today and Y would be the total minutes watched over the last t
    days. Assume discrete time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d19138bff32459c9fb06e072b9e6bafc.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *Y* is the experiment metric described above, a count of events, *t* is
    the current time of the experiment, and *i* indexes the individual unit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose traffic arrives to our experiment at a constant rate *r*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5fbf63cf982babb206f27710a4b8f68d.png)'
  prefs: []
  type: TYPE_IMG
- en: where *t* is the number of time periods our experiment has been active.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that each *X(i,s)* is independent and has identical variance (for simplicity;
    the same problem shows up to a greater or lesser extent depending on autocorrelation,
    etc) but not necessarily with constant mean. Then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/494ea92d49f92f3182d448db4fb4aca7.png)'
  prefs: []
  type: TYPE_IMG
- en: We start to see the problem. The variance of our metric is not constant over
    time. In fact, it is growing larger and larger.
  prefs: []
  type: TYPE_NORMAL
- en: In a typical experiment, we construct a t-test for the null hypothesis that
    the treatment effect is 0 and look for evidence against that null. If we find
    it, we will say the experiment is a statistically significant win or loss.
  prefs: []
  type: TYPE_NORMAL
- en: So what does the t-stat look like in this case, say for the hypothesis that
    the mean of *Y* is zero?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0b8d6bc72711a54106370d7bf20d57d.png)'
  prefs: []
  type: TYPE_IMG
- en: Plugging in *n = rt*, we can write the expression in terms of *t*,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9842cd8eabf1462cdc7cee7c671a1d94.png)'
  prefs: []
  type: TYPE_IMG
- en: As with any hypothesis test, we want that when the null hypothesis is not true,
    the test statistic should become large as sample size increases so that we reject
    the null hypothesis and go with the alternative. One implication of this requirement
    is that, under the alternative, the mean of the t-statistic should diverge to
    infinity. But…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93901ac881195003653053c91c91c9ae.png)'
  prefs: []
  type: TYPE_IMG
- en: The mean of the t-statistic at time *t* is just the mean of the metric up to
    time *t* times a constant that does not vary with sample size or experiment duration.
    Therefore, the only way it can diverge to infinity is if E[Y(t)] diverges to infinity!
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the only alternative hypothesis that our t-test is guaranteed
    to have arbitrary power for, is the hypothesis that the mean is infinite. There
    are alternative hypotheses that will never be rejected no matter how large the
    sample size is.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86084fc67294c58a7becf8e619f6b2ed.png)'
  prefs: []
  type: TYPE_IMG
- en: We are clearly in the alternative because the limiting mean is not zero, but
    the mean of t-statistic converges to 1, which is less than most standard critical
    values. So the power of the t-test could *never* reach 1, no matter how long we
    wait for the experiment to finish. We see this effect play out in experiments
    with unlimited metrics by the confidence interval refusing to shrink no matter
    how long the experiment runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'If E[Y(t)] does in fact diverge to infinity, then the *average* treatment effect
    will not be well-defined because the means of the metric do not exist. So we are
    in a scenario where either: we have low *asymptotic* power to detect average treatment
    effects or the average treatment effect does not exist. Not a good scenario!'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, this result is not what a standard sample sizing analysis assumes.
    It assumes that with a large enough sample size, any power level can be satisfied
    for a fixed, non-zero alternative. That doesn’t happen here because the individual
    level variance is not constant, as assumed more-or-less in the standard sample-size
    formulas. It increases with sample size. So standard sample-sizing formulas and
    methods are incorrect for unlimited metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to *time limit* metrics. We should define a fixed time post
    exposure to the experiment to stop counting new events. For example, instead of
    defining our metric as the number of minutes spent watching video post experiment
    exposure, we can define our metric as the number of minutes spent watching video
    in the 2 days (or some other fixed number) following experiment exposure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we do that, in the above model, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e56acd11b73f35aedc245fd5774ea15.png)'
  prefs: []
  type: TYPE_IMG
- en: The variance of the time-limited metric does not increase with *t*. So now,
    when we add new data, we only add more observations. We do not (after a few days)
    change the metric for existing users and increase the individual-level metric
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: Along with the statistical benefits, time-limiting our metrics makes them easier
    to compare across experiments with different durations.
  prefs: []
  type: TYPE_NORMAL
- en: Simulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To show this problem in action, I compare the unlimited and time limited versions
    of these metrics in the following data generating process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16843dced818081bafdfb9603a9f442b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where the metric of interest is *Y(i,t)*, as defined above: the cumulative
    sum of *X* in the unlimited case and the sum up to time *d* in the time-limited
    case. We set the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/985bd3b3fa50d5c761dd3cf26859308f.png)'
  prefs: []
  type: TYPE_IMG
- en: We then simulate the dataset and compute the mean of *Y* testing against the
    null hypothesis that the mean is 0 both in the case where the metric is time-limited
    to two time periods (*d*=2) and in the case where the metric is unlimited.
  prefs: []
  type: TYPE_NORMAL
- en: 'In both cases, we are in the alternative. The long-run mean of *Y(i,t)* in
    the unlimited case is: 0.2.'
  prefs: []
  type: TYPE_NORMAL
- en: We set the significance level at 0.05 and consider the power of the test in
    both scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: We can see from Figure 1 power never increases for the unlimited metric despite
    sample size increasing by 10x. The time limited metric approaches 100% power at
    the same sample sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2895b220e82ec7cbfa889201993ce99c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Power Simulation for Non-Zero Alternative (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: If we do not time limit count metrics, we may have very low power to find wins
    even if they exist, no matter how long we run the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Time-limiting your metrics is a simple thing to do, but it makes three things
    true that we, as experimenters, would very much like to be true:'
  prefs: []
  type: TYPE_NORMAL
- en: If there is an effect, we will eventually reach statistical significance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The average treatment effect is well-defined, and its interpretation remains
    constant throughout the experiment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normal sample sizing methods are valid (because variance is not constantly increasing).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As a side benefit, time-limiting metrics often increases power for another
    reason: it reduces variance from shocks long after experiment exposure (and, therefore,
    less likely to be related to the experiment).'
  prefs: []
  type: TYPE_NORMAL
- en: Zach
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect at: [https://linkedin.com/in/zlflynn/](https://linkedin.com/in/zlflynn/)
    .'
  prefs: []
  type: TYPE_NORMAL
