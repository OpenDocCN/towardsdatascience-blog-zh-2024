["```py\n# Not a great use of test code\ndef test_predict(model_instance, features):\n    prediction = model_instance.predict(features)\n    assert prediction == 0.133713371337\n```", "```py\ndef test_pipeline(pipeline_instance):\n    # Execute the entire pipeline over a set of test configurations that \n    # exemplify the important cases\n\n    # complex inputs, file paths, and whatever your pipeline needs\n    test_configuration = 'some config here'\n\n    # Run the entire flow. Data munging, feature engineering, prediction, \n    # post processing\n    result = pipline_instance.run(test_configuration)\n    assert np.testing.assert_almost_equal(result, 0.1337)\n```", "```py\ndef check_inputs(frame):\n  \"\"\" In this scenario we're checking areas\n  for a wide but  plausible range. The goal is really just to make sure\n  nothing has gone completely wrong in the inputs.\"\"\"\n\n  conforms = frame['square_footage'].apply(lambda x: 1 < x < 1000000)\n\n  if not conforms.all():\n    # Fail loud. We're no longer in the state the pipeline was designed for.\n    raise ValueError(\"Some square_footage values are not in plausible range\") \n```"]