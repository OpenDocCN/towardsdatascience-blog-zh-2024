- en: Extracting Information from Natural Language Using Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/extracting-information-from-natural-language-using-generative-ai-ed64dcf1de66?source=collection_archive---------5-----------------------#2024-05-03](https://towardsdatascience.com/extracting-information-from-natural-language-using-generative-ai-ed64dcf1de66?source=collection_archive---------5-----------------------#2024-05-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Extracting and structuring text elements with high accuracy using small models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@orenmatar?source=post_page---byline--ed64dcf1de66--------------------------------)[![Oren
    Matar](../Images/8b1fa6aa3585fc283d51828b53a0754c.png)](https://medium.com/@orenmatar?source=post_page---byline--ed64dcf1de66--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ed64dcf1de66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ed64dcf1de66--------------------------------)
    [Oren Matar](https://medium.com/@orenmatar?source=post_page---byline--ed64dcf1de66--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ed64dcf1de66--------------------------------)
    ·6 min read·May 3, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12de85af31069135ae128e15c4e03a46.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by an AI by the author
  prefs: []
  type: TYPE_NORMAL
- en: In this post, I’ll introduce a paradigm recently developed at Anaplan for extracting
    temporal information from natural language text, as part of an NLQ (natural language
    query) project. While I will focus on time extraction, the paradigm is versatile
    and applicable for parsing various unstructured texts and extracting diverse patterns
    of information. This includes named entity recognition, text-to-SQL conversion,
    quantity extraction, and more.
  prefs: []
  type: TYPE_NORMAL
- en: The paradigm's core lies in constructing a flexible pipeline, which provides
    maximal flexibility, making it easy to fine-tune a model to extract the meaning
    from any conceivable expression in the language. It is based on a deep learning
    model (transformers) but for us, it achieved a 99.98% accuracy which is relatively
    rare for ML methods. Additionally, it does not utilize LLMs (large language models),
    in fact, it requires a minimal transformer model. This yields a compact, adaptable
    ML model, exhibiting the precision of rule-based systems.
  prefs: []
  type: TYPE_NORMAL
- en: For those seeking time, numerical value, or phone number extraction, Facebook’s
    [Duckling package](https://github.com/facebook/duckling) offers a rule-based solution.
    However, if Duckling falls short of your requirements or you’re eager to explore
    a new ML paradigm, read on.
  prefs: []
  type: TYPE_NORMAL
- en: Can LLMs capture the meaning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs, despite their capabilities, face challenges in parsing such phrases and
    extracting their meaning comprehensively. Consider the expression “the first 15
    weeks of last year.” Converting this to a date range necessitates the model to
    determine the current year, subtract one, and calculate the position of the 15th
    week as it adjusts for leap years. Language models were not built for this kind
    of computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In my experience, LLMs can accurately output the correct date range around
    90–95% of the time but struggle with the remaining 5–10%, no matter the prompting
    techniques you use. Not to mention: LLMs are resource-intensive and slow.'
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, by following three principles, compact transformers can successfully
    accomplish the task
  prefs: []
  type: TYPE_NORMAL
- en: Separate information extraction from logical deduction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Auto-generate a dataset using structured patterns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Constrain the generative AI to the required structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this post, I will cover the first two, as the third one I covered in [a previous
    post](https://medium.com/towards-data-science/structured-generative-ai-e772123428e4).
  prefs: []
  type: TYPE_NORMAL
- en: Separate information extraction from logical deduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first principle is to ensure that the language model’s role is to extract
    information from free text, rather than to make any logical deduction: logical
    deductions can easily be implemented in code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the phrase: “How many movies came out two years ago?” The language
    model’s task should be to identify that the relevant year is: `**this_year - 2**`,
    without calculating the actual year (which means it doesn’t need to know the current
    year). Its focus is parsing the meaning and structuring unstructured language.
    Once that formula is extracted, we can implement its calculation in code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this to work, we introduce a Structured Time Language (STL) capable of
    expressing time elements. For instance, “on 2020” translates to “TIME.year==2020,”
    and “three months from now” becomes “NOW.month==3.” While the entire STL language
    isn’t detailed here, it should be relatively intuitive: you can reference attributes
    like year, quarter, and month for an absolute time or relative to NOW. The translation
    of “the last 12 weeks of last year” is “NOW.year==-1 AND TIME.week>=-12”'
  prefs: []
  type: TYPE_NORMAL
- en: By removing any logical deduction or calculation from the task, we take a huge
    burden off the language model and allow it to focus on information extraction.
    This division of labor will improve its accuracy significantly. After the translation
    process is complete, it is straightforward to develop code for a parser that reads
    the structured language and retrieves the necessary date range.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is a translation task — from natural language to STL — we used an
    encoder-decoder transformer. We used the [Bart model from Hugging Face](https://huggingface.co/docs/transformers/en/model_doc/bart),
    which can easily be fine-tuned for this task.
  prefs: []
  type: TYPE_NORMAL
- en: But how do we get the data for training the model?
  prefs: []
  type: TYPE_NORMAL
- en: '**Auto-generate a dataset using structured patterns**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since a training dataset for this translation task does not exist, we must
    generate it ourselves. This was done by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step one*: Write functions to map datetime objects to both “natural language”
    and STL formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Given a datetime object, these functions return a tuple of free text and its
    corresponding STL, for instance: “since 2020”, “TIME.year >= 2020”.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step two*: Sample a random function, and sample a random date within a specified
    range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: now insert the datetime to the function.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step three*: Append the free text to a random question (we can easily randomly
    generate questions or draw them from some question dataset, their quality and
    meaning is not very important).'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this pipeline, we can quickly generate 1000s of text-STL pairs, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: “What was the GDP growth in Q2–2019?”, “TIME.quarter==2 AND TIME.year==2019”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Since 2017, who won the most Oscars?”, “TIME.year>=2017”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Who was the president on 3 May 2020?”, “TIME.date==2020/05/03”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach ensures flexibility in adding new patterns effortlessly. If you
    find a time expression that is not covered by one of these functions (e.g. “In
    N years”), you can write a function that will generate examples for this pattern
    within seconds.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we can optimize the code efficiency further. Rather than separate
    functions for each pattern like “since 2020” and “until 2020,” we can randomly
    sample connective words like “since,” “until,” “on,” etc. This initial batch of
    functions may require some time to develop, but you can quickly scale to 100s
    of patterns. Subsequently, addressing any missing expressions becomes trivial,
    as the pipeline is already established. With a few iterations, nearly all relevant
    expressions can be covered.
  prefs: []
  type: TYPE_NORMAL
- en: '**Moreover, we don’t need to cover all the expressions**: Since the transformer
    model we used is pre-trained on a huge corpus of text, it will generalize from
    the provided patterns to new ones.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Finally, we can use an LLM to generate more examples**. Simply ask an LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And it may return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This data augmentation process can be automated too: sending numerous examples
    to an LLM, thus adding variety to our dataset. Given that the LLM’s role is solely
    in dataset creation, considerations of cost and speed become inconsequential.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Combining the flexibility of adding new patterns, the generalization of the
    pre-trained model, and data augmentation using an LLM, we can effectively cover
    almost any expression.**'
  prefs: []
  type: TYPE_NORMAL
- en: The final principle of this paradigm is to constrain the generative AI to produce
    only STL queries, ensuring adherence to the required structure. The method to
    achieve this, as well as a method for optimizing the tokenization process, was
    discussed [in a previous post](https://medium.com/towards-data-science/structured-generative-ai-e772123428e4).
  prefs: []
  type: TYPE_NORMAL
- en: By adhering to these three principles, we achieved an impressive accuracy of
    99.98% on our test dataset. Moreover, this paradigm gave us the flexibility to
    address new, unsupported, time expressions swiftly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) aren’t always the optimal solution for language
    tasks. With the right approach, shallower transformer models can efficiently extract
    information from natural language with high accuracy and flexibility, at a reduced
    time and cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key principles to remember are:'
  prefs: []
  type: TYPE_NORMAL
- en: Focusing the model only on information extraction, avoiding complex logical
    deductions. This may require generating a mediating language and implementing
    a parser and logical deduction in code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Establishing a pipeline for generating a dataset and training a model, so that
    adding new functionality (new language patterns) is straightforward and fast.
    This pipeline can include the use of an LLM, adding more variety to the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Confining the model generation to the constraints of a structured language.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While this post focused on extracting time elements, the paradigm applies to
    extracting any information from free text and structuring it into various formats.
    With this paradigm, you can achieve the accuracy of a rule-based engine, with
    the flexibility of a machine learning model.
  prefs: []
  type: TYPE_NORMAL
