- en: Graph-Based Prompting and Reasoning with Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/graph-based-prompting-and-reasoning-with-language-models-d6acbcd6b3d8?source=collection_archive---------0-----------------------#2024-01-03](https://towardsdatascience.com/graph-based-prompting-and-reasoning-with-language-models-d6acbcd6b3d8?source=collection_archive---------0-----------------------#2024-01-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding graph of thoughts prompting and several variants…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page---byline--d6acbcd6b3d8--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page---byline--d6acbcd6b3d8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d6acbcd6b3d8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d6acbcd6b3d8--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page---byline--d6acbcd6b3d8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d6acbcd6b3d8--------------------------------)
    ·22 min read·Jan 3, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b6c41b4f07a4b381d9da086ab84a80c.png)'
  prefs: []
  type: TYPE_IMG
- en: (Photo by [Alina Grubnyak](https://unsplash.com/@alinnnaaaa?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/low-angle-photography-of-metal-structure-ZiQkhI7417A?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: Advanced prompting techniques like chain of thought [8] and tree of thought
    [9] prompting have drastically improved the ability of large language models (LLMs)
    to solve complex, reasoning-based tasks. At a high level, forcing the LLM to construct
    a step-by-step response to a problem drastically improves its problem-solving
    capabilities. However, all of such techniques assume that the reasoning process
    should follow a linear patterns that progresses from one thought to the next.
    Notably, the reasoning process followed by humans tends to be quite different,
    following multiple different chains of thought and even combining insights from
    different thoughts to arrive at a final solution. Within this overview, we will
    be studying several prompting techniques that model the reasoning process as a
    graph structure — rather than a chain or tree — that better captures the various
    types of non-linear patterns that may occur when reasoning over a problem.
  prefs: []
  type: TYPE_NORMAL
- en: “Human thinking is often characterized by its ability to make sudden leaps and
    connections between seemingly unrelated ideas, which can lead to novel insights
    and solutions. This non-linear, jumping thought process is a hallmark of human
    creativity, reasoning, and problem-solving…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
