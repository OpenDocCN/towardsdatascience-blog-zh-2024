<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>An Introduction to Prompting for LLMs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>An Introduction to Prompting for LLMs</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-prompting-for-llms-61d36aec2048?source=collection_archive---------3-----------------------#2024-02-22">https://towardsdatascience.com/an-introduction-to-prompting-for-llms-61d36aec2048?source=collection_archive---------3-----------------------#2024-02-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e68d" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How do we communicate effectively with LLMs?</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@anand.subu10?source=post_page---byline--61d36aec2048--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Anand Subramanian" class="l ep by dd de cx" src="../Images/096dc5504d6ada2493e0ac26959e60f0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*IfxBBsal-XaXfAXh_c9g1A.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--61d36aec2048--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@anand.subu10?source=post_page---byline--61d36aec2048--------------------------------" rel="noopener follow">Anand Subramanian</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--61d36aec2048--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">30 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="ee5d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Unless you’ve been completely disconnected from the buzz on social media and in the news, it’s unlikely that you’d have missed the excitement around Large Language Models (LLMs).</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/eb60be0bd7ee67fd4f1a4f16239215c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q75Lv523Rr13bLRnQVSiXQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">The evolution of LLMs. Image borrowed from the paper [1] (<a class="af ny" href="https://github.com/Mooler0410/LLMsPracticalGuide/blob/main/imgs/tree.png" rel="noopener ugc nofollow" target="_blank">Source</a>). Even as I add this image, the pace of current LLM development makes this picture obsolete.</figcaption></figure><p id="ce0a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">LLMs have become ubiquitous, with new models being released almost daily. They’ve also been made more accessible to the general public, thanks to a thriving open-source community that has played a crucial role in reducing memory requirements and developing efficient fine-tuning methods for LLMs, even with limited compute resources.</p><p id="8022" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One of the most exciting use cases for LLMs is their remarkable ability to excel at tasks they were not explicitly trained for, using just a task description and, optionally, a few examples. You can now get a capable LLM to generate a story in the style of your favorite author, summarize long emails into concise ones, and develop innovative marketing campaigns by describing your task to the model without needing to fine-tune it. But how do you best communicate your requirements to the LLM? This is where prompting comes in.</p><h1 id="5146" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Table of Contents:</h1><ol class=""><li id="cdce" class="mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne pa pb pc bk"><a class="af ny" href="#0f7f" rel="noopener ugc nofollow">What is Prompting?</a></li><li id="3cf1" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af ny" href="#a04b" rel="noopener ugc nofollow">Why is Prompting Important?</a></li><li id="cac5" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af ny" href="#f3d4" rel="noopener ugc nofollow">Exploring different prompting strategies</a></li><li id="cc72" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af ny" href="#3a00" rel="noopener ugc nofollow">How can we implement these techniques?<br/></a>4.1. <a class="af ny" href="#5921" rel="noopener ugc nofollow">Prompting Llama 2 7B-Chat with a Zero-Shot Prompt</a><br/>4.2. <a class="af ny" href="#0ac4" rel="noopener ugc nofollow">Prompting Llama 2 7B-Chat with a Few-Shot Prompt</a><br/>4.3. <a class="af ny" href="#a04e" rel="noopener ugc nofollow">What happens if we don’t adhere to the chat template?</a><br/>4.4. <a class="af ny" href="#bfe1" rel="noopener ugc nofollow">Prompting Llama 2 7B-Chat with CoT Prompting</a><br/>4.5. <a class="af ny" href="#0ac0" rel="noopener ugc nofollow">Failure Modes in CoT for Llama 2</a><br/>4.6. <a class="af ny" href="#c938" rel="noopener ugc nofollow">Prompting GPT-3.5 with a Zero-Shot Prompt</a><br/>4.7. <a class="af ny" href="#a575" rel="noopener ugc nofollow">Prompting GPT-3.5 with a Few-Shot Prompt</a><br/>4.8. <a class="af ny" href="#18e9" rel="noopener ugc nofollow">Prompting GPT-3.5 with CoT Prompting</a></li><li id="a7bc" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af ny" href="#913f" rel="noopener ugc nofollow">Conclusion and Takeaways</a></li><li id="7eea" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af ny" href="#f8e9" rel="noopener ugc nofollow">Reproducibility</a></li><li id="6048" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af ny" href="#42e4" rel="noopener ugc nofollow">References</a></li></ol><h1 id="0f7f" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">What is Prompting?</strong></h1><p id="21e8" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Prompting, or Prompt Engineering, is a technique used to design inputs or prompts that guide artificial intelligence models — particularly those in natural language processing and image generation — to produce specific, desired outputs. Prompting involves structuring your requirements into an input format that effectively communicates the desired outcomes to the model, thereby obtaining the intended output.</p><p id="9287" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Large Language Models (LLMs) demonstrate an ability for <strong class="ml fr">in-context learning</strong> [2] [3]. This means these models can understand and execute various tasks based solely on task descriptions and examples provided to the model through a prompt without requiring specialized fine-tuning for each new task. Prompting is significant in this context as it is the primary interface between the user and the model to harness this ability. A well-defined prompt helps define the nature and expectations of the task to the LLM, along with how to provide the output in a utilizable manner to the user.</p><p id="2fa2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You might be inclined to think that prompting an LLM shouldn’t be that hard; after all, it’s just about describing your requirements to the model in natural language, right? In practice, it isn’t as straightforward. You will discover that different LLMs have varying strengths. Some might better adhere to your desired output format, while others may necessitate more detailed instructions. The task you wish the LLM to perform could be complex, requiring elaborate and precise instructions. Therefore, devising a suitable prompt often entails a lot of experimentation and benchmarking.</p><h1 id="a04b" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">Why is Prompting important?</strong></h1><p id="dd11" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">In practice, LLMs are sensitive to how the input is structured and provided to them. We can analyze this along various axes to better understand the situation:</p><ol class=""><li id="35d7" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pa pb pc bk"><strong class="ml fr">Adhering to Prompt Formats</strong>: LLMs often utilize varying prompt formats to accept user input. This is typically done when models are instruction-tuned or optimized for chat use cases [4] [5]. At a high level, most prompt formats include the <strong class="ml fr"><em class="pi">instruction</em></strong> and the <strong class="ml fr"><em class="pi">input</em></strong>. The instruction describes the task to be performed by the model, while the input contains the text on which the task needs to be executed. Let’s take the Alpaca Instruction format, for example (taken from <a class="af ny" href="https://github.com/tatsu-lab/stanford_alpaca" rel="noopener ugc nofollow" target="_blank">https://github.com/tatsu-lab/stanford_alpaca</a>):</li></ol><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="0ca6" class="pn oa fq pk b bg po pp l pq pr">Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.<br/><br/>### Instruction:<br/>{instruction}<br/><br/>### Input:<br/>{input}<br/><br/>### Response:</span></pre><p id="0004" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Given the models are instruction-tuned using a template like this, the model is expected to perform optimally when a user prompts it using the same format.</p><p id="a9ec" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">2. <strong class="ml fr">Describing output formats for parseability: </strong>Having provided a prompt to the model, you’d want to extract what you need from the model’s output. Ideally, these outputs should be in a format you can effortlessly parse through programming methods. Depending on the task, such as text classification, this might involve leveraging regular expressions (regex) to sift through the LLM’s output. In contrast, you might prefer a format like JSON for your output for tasks requiring more fine-grained data like Named Entity Recognition (NER).</p><p id="6cc4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, the more you work with LLMs, the faster you learn that obtaining parseable outputs can be challenging. LLMs often struggle to deliver outputs precisely in the format requested by the user. While strategies like few-shot prompting can significantly mitigate this issue, achieving consistent, programmatically parsable outputs from LLMs demands careful experimentation and adaptation.</p><p id="9005" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">3. <strong class="ml fr">Prompting for optimal performance: </strong>LLMs are quite sensitive to how the task is described. A prompt that is not well-crafted or leaves too much room for interpretation can lead to subpar performance. Imagine explaining a task to someone — the clearer and more detailed your explanation, the better the understanding on the other end. However, there is no magic formula for arriving at the ideal prompt. This requires careful experimentation and evaluation of different prompts to select the best-performing prompt.</p><h1 id="f3d4" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">Exploring different prompting strategies</strong></h1><p id="c250" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Hopefully, you’re convinced you need to take prompting seriously by this point. If prompting is a toolkit, what are the tools we can leverage?</p><p id="14b7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Zero-shot Prompting:</strong> Zero-shot prompting [2] [3] involves instructing an LLM to perform a task described solely in the prompt without providing examples. The term “zero-shot” signifies that the model must rely entirely on the task description in the prompt, as it receives no specific demonstrations related to the task.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng ps"><img src="../Images/bf12c12c151c864333b4f2fb77809d97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aLsbNozYW5jUil1sHka9eg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">An overview of Zero-Shot Prompting. (Image by the author)</figcaption></figure><p id="76b8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In many cases, zero-shot prompting can suffice for instructing an LLM to perform your desired task. However, zero-shot prompting may have limitations if your task is too ambiguous, open-ended, or vague. Suppose you want an LLM to rank an answer on a scale from 1 to 5. Although the model could perform this task with a zero-shot prompt, two possible problems can arise here:</p><ol class=""><li id="382c" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pa pb pc bk">The LLM might not have an objective understanding of what each number on the scoring scale signifies. It may struggle to decide when to assign a score of 3 or 4 to an answer if the task description lacks nuance.</li><li id="5ac7" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk">The LLM could have its own concept of scoring from 1 to 5, which might contradict your personal scoring rubrics. You might prioritize the factuality of an answer when scoring it, but the model could evaluate the answer based on how well it is written.</li></ol><p id="4d4c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To ground the model in your scoring expectations, you can provide a few examples of answers and how you might score them. Now, the model has more context and reference on how to score documents, thereby narrowing the ambiguity in the task. This brings us to few-shot prompting.</p><p id="c8b0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Few-shot Prompting:</strong> Few-shot prompting enriches the task description with a small number of example inputs and their corresponding outputs [3]. This technique enhances the model’s understanding by including several example pairs illustrating the task.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng pt"><img src="../Images/58cc2240a055d9a5cce738de853f7c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*99izvYAgrQ3OweitwOUOkw.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">An overview of Few-Shot Prompting. (Image by the author)</figcaption></figure><p id="d196" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For instance, to guide an LLM in sentiment classification of movie reviews, you would present a few reviews along with their sentiment ratings. The primary benefit of few-shot over zero-shot prompting is the ability to demonstrate examples of how to perform the task instead of expecting the LLM to perform the task with just a description.</p><p id="9e01" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Chain of Thought:</strong> Chain of Thought (CoT) prompting [6] is a technique that enables LLMs to solve complex problems by breaking them down into simpler, intermediate steps. This approach encourages the model to “think aloud,” making its reasoning process transparent and allowing the LLM to solve reasoning problems more effectively. As mentioned by the authors of the work [6], CoT mimics how humans try to solve reasoning problems by decomposing the problem into simpler steps and solving them one at a time rather than jumping directly to the answer.</p></div></div><div class="nn"><div class="ab cb"><div class="lm pu ln pv lo pw cf px cg py ci bh"><figure class="ni nj nk nl nm nn qa qb paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng pz"><img src="../Images/79baf1845e581cfe3140508309256a9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*TDcBawTo5ewZbwZrrwYRMQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">An overview of Chain-of-Thought Prompting. (Image by the author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8f96" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">CoT prompting is typically implemented as a few-shot prompt, where the model receives a task description and examples of input-output pairs. These examples include reasoning steps that systematically lead to the correct answer, demonstrating how to process the information. Thus, to perform CoT prompting effectively, users need high-quality demonstration examples. However, this can be challenging for tasks requiring specialized domain expertise. For instance, using an LLM for medical diagnosis based on a patient’s history would necessitate the assistance of domain experts, such as doctors or physicians, to articulate the correct reasoning steps. Moreover, CoT is particularly effective in models with a sufficiently large parameter scale. According to the paper [6], CoT is most effective for the 137B parameter LaMBDA [7], the 175B parameter GPT-3 [3], and the 540B parameter PaLM [8] models. This limitation can restrict its applicability for smaller-scale models.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qc"><img src="../Images/7e9847b192ff473c6e794fd82530c80e.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*lPXw3MFXsPYyPwc6yipF0g.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Figure taken from [6] (<a class="af ny" href="https://arxiv.org/abs/2201.11903" rel="noopener ugc nofollow" target="_blank">Source</a>) shows that the performance improvement provided by CoT prompting improves substantially with the scale of the model.</figcaption></figure><p id="09ec" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Another aspect of CoT prompting that sets it apart from standard prompting is that the model needs to generate significantly more tokens before arriving at the final answer. While not necessarily a drawback, this is a factor to consider if you are compute-bound at inference time.</p><p id="ef23" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you want a deeper overview, I recommend OpenAI’s prompting resources, available at <a class="af ny" href="https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions" rel="noopener ugc nofollow" target="_blank">https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions</a>.</p><h1 id="3a00" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">How can we implement these techniques?</strong></h1><blockquote class="qd qe qf"><p id="8aca" class="mj mk pi ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">All code and resources related to this article are made available at <a class="af ny" href="https://github.com/anand-subu/blog_resources/tree/main" rel="noopener ugc nofollow" target="_blank">this Github repository</a>, under the introduction_to_prompting folder. Feel free to pull the repository and run the notebooks directly to run these experiments. Please let me know if you have any feedback or observations or if you notice any mistakes!</p></blockquote><p id="c6d9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can explore these techniques on a sample dataset to make understanding easier. To this end, we will work with the MedQA dataset [9], which contains questions testing medical and clinical knowledge. We will specifically utilize the USMLE questions from this dataset. This task is ideal for analyzing various prompting techniques, as answering the questions requires knowledge and reasoning. We will test the capabilities of Llama-2 7B [10] and GPT-3.5 [11] on this dataset.</p><p id="058b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s first download the dataset. The MedQA dataset can be downloaded from <a class="af ny" href="https://drive.google.com/file/d/1ImYUSLk9JbgHXOemfvyiDiirluZHPeQw/view" rel="noopener ugc nofollow" target="_blank">this link</a>. After downloading the dataset, we can parse and begin processing the questions. The test set contains a total of 1,273 questions. We randomly sample 300 questions from the test set to evaluate the models and select 3 random examples from the training set as our few-shot demonstrations for the model.</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="993b" class="pn oa fq pk b bg po pp l pq pr">import json<br/>import random<br/>random.seed(42)<br/><br/>def read_jsonl_file(file_path):<br/>    """<br/>    Parses a JSONL (JSON Lines) file and returns a list of dictionaries.<br/><br/>    Args:<br/>        file_path (str): The path to the JSONL file to be read.<br/><br/>    Returns:<br/>        list of dict: A list where each element is a dictionary representing<br/>            a JSON object from the file.<br/>    """<br/>    jsonl_lines = []<br/>    with open(file_path, 'r', encoding="utf-8") as file:<br/>        for line in file:<br/>            json_object = json.loads(line)<br/>            jsonl_lines.append(json_object)<br/>            <br/>    return jsonl_lines<br/><br/>def write_jsonl_file(dict_list, file_path):<br/>    """<br/>    Write a list of dictionaries to a JSON Lines file.<br/><br/>    Args:<br/>    - dict_list (list): A list of dictionaries to write to the file.<br/>    - file_path (str): The path to the file where the data will be written.<br/>    """<br/>    with open(file_path, 'w') as file:<br/>        for dictionary in dict_list:<br/>            # Convert the dictionary to a JSON string and write it to the file.<br/>            json_line = json.dumps(dictionary)<br/>            file.write(json_line + '\n')<br/><br/><br/># read the contents of the train and test set<br/>train_set = read_jsonl_file("data_clean/questions/US/4_options/phrases_no_exclude_train.jsonl")<br/>test_set = read_jsonl_file("data_clean/questions/US/4_options/phrases_no_exclude_test.jsonl")<br/><br/># subsample test set samples and few-shot samples<br/>test_set_subsampled = random.sample(test_set, 300)<br/>few_shot_examples = random.sample(test_set, 3)<br/><br/># dump the sampled questions and few-shot samples as jsonl files<br/>write_jsonl_file(test_set_subsampled, "USMLE_test_samples_300.jsonl")<br/>write_jsonl_file(few_shot_examples, "USMLE_few_shot_samples.jsonl")</span></pre><h2 id="5921" class="qg oa fq bf ob qh qi qj oe qk ql qm oh ms qn qo qp mw qq qr qs na qt qu qv qw bk"><strong class="al">Prompting Llama 2 7B-Chat with a Zero-Shot Prompt</strong></h2><blockquote class="qd qe qf"><p id="b389" class="mj mk pi ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The Llama series of models were released by Meta. They are a decoder-only family of LLMs spanning parameter counts from 7B to 70B. The Llama-2 series of models comes in two variants: the base version and the chat/instruction-tuned variant. For this exercise, we’ll work with the chat-version of the Llama 2-7B model.</p></blockquote><p id="22b2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s see how well we can prompt the Llama model to answer these medical questions. We load the model into memory:</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="8030" class="pn oa fq pk b bg po pp l pq pr">import torch<br/>from transformers import AutoModelForCausalLM, AutoTokenizer<br/>from tqdm import tqdm<br/><br/>questions = read_jsonl_file("USMLE_test_samples_300.jsonl")<br/><br/>tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")<br/>model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf",  torch_dtype=torch.bfloat16).cuda()<br/>model.eval()</span></pre><blockquote class="qd qe qf"><p id="e25f" class="mj mk pi ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you’re working with Nvidia Ampere GPUs, you can load the model using torch.bfloat16. It offers speedups to inference and utilizes lesser GPU memory than normal FP16/FP32.</p></blockquote><p id="6911" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First, let’s now craft a basic prompt for our task:</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="8998" class="pn oa fq pk b bg po pp l pq pr">PROMPT = """You will be provided with a medical or clinical question, along with multiple possible answer choices. Pick the right answer from the choices. <br/>Your response should be in the format "The answer is &lt;correct_choice&gt;". Do not add any other unnecessary content in your response"""</span></pre><p id="b439" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Our prompt is straightforward. It includes information about the nature of the task and provides instructions on the format for the output. We’ll see how effectively this prompt works in practice.</p><p id="2360" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The Llama-2 chat models have a particular chat template to be followed for prompting them.</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="2ac1" class="pn oa fq pk b bg po pp l pq pr">&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;<br/>You will be provided with a medical or clinical question, along with multiple possible answer choices. Pick the right answer from the choices. <br/>Your response should be in the format "The answer is &lt;correct_choice&gt;". Do not add any other unnecessary content in your response<br/>&lt;&lt;/SYS&gt;&gt;<br/><br/>A 21-year-old male presents to his primary care provider for fatigue. He reports that he graduated from college last month and returned 3 days ago from a 2 week vacation to Vietnam and Cambodia. For the past 2 days, he has developed a worsening headache, malaise, and pain in his hands and wrists. The patient has a past medical history of asthma managed with albuterol as needed. He is sexually active with both men and women, and he uses condoms “most of the time.” On physical exam, the patient’s temperature is 102.5°F (39.2°C), blood pressure is 112/66 mmHg, pulse is 105/min, respirations are 12/min, and oxygen saturation is 98% on room air. He has tenderness to palpation over his bilateral metacarpophalangeal joints and a maculopapular rash on his trunk and upper thighs. Tourniquet test is negative. Laboratory results are as follows:<br/><br/>Hemoglobin: 14 g/dL<br/>Hematocrit: 44%<br/>Leukocyte count: 3,200/mm^3<br/>Platelet count: 112,000/mm^3<br/><br/>Serum:<br/>Na+: 142 mEq/L<br/>Cl-: 104 mEq/L<br/>K+: 4.6 mEq/L<br/>HCO3-: 24 mEq/L<br/>BUN: 18 mg/dL<br/>Glucose: 87 mg/dL<br/>Creatinine: 0.9 mg/dL<br/>AST: 106 U/L<br/>ALT: 112 U/L<br/>Bilirubin (total): 0.8 mg/dL<br/>Bilirubin (conjugated): 0.3 mg/dL<br/><br/>Which of the following is the most likely diagnosis in this patient?<br/>Options:<br/>A. Chikungunya<br/>B. Dengue fever<br/>C. Epstein-Barr virus<br/>D. Hepatitis A [/INST]</span></pre><p id="0f80" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The task description should be provided between the &lt;&lt;SYS&gt;&gt; tokens, followed by the actual question the model needs to answer. The prompt is concluded with a [/INST] token to indicate the end of the input text.</p><blockquote class="qd qe qf"><p id="dcb1" class="mj mk pi ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The role can be one of “<strong class="ml fr">user</strong>”, “<strong class="ml fr">system</strong>”, or “<strong class="ml fr">assistant</strong>”. The “system” role provides the model with the task description, and the “user” role contains the input to which the model needs to respond. This is the same convention we will utilize later on when interacting with GPT-3.5. It is equivalent to creating a fictional multi-turn conversation history provided to Llama-2, where each turn corresponds to an example demonstration and an ideal output from the model.</p></blockquote><p id="193c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Sounds complicated? Thankfully, the Huggingface Transformers library supports converting prompts to the chat template. We will utilize this functionality to make our lives easier. Let’s start with helper functionalities to process the dataset and create prompts.</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="7619" class="pn oa fq pk b bg po pp l pq pr">def create_query(item):<br/>    """<br/>    Creates the input for the model using the question and the multiple choice options.<br/><br/>    Args:<br/>        item (dict): A dictionary containing the question and options.<br/>            Expected keys are "question" and "options", where "options" is another<br/>            dictionary with keys "A", "B", "C", and "D".<br/><br/>    Returns:<br/>        str: A formatted query combining the question and options, ready for use.<br/>    """<br/>    query = item["question"] + "\nOptions:\n" + \<br/>            "A. " + item["options"]["A"] + "\n" + \<br/>            "B. " + item["options"]["B"] + "\n" + \<br/>            "C. " + item["options"]["C"] + "\n" + \<br/>            "D. " + item["options"]["D"]<br/>    return query<br/><br/>def build_zero_shot_prompt(system_prompt, question):<br/>    """<br/>    Builds the zero-shot prompt.<br/><br/>    Args:<br/>        system_prompt (str): Task Instruction<br/>        content (dict): The content for which to create a query, formatted as<br/>            required by `create_query`.<br/><br/>    Returns:<br/>        list of dict: A list of messages, including a system message defining<br/>            the task and a user message with the input question.<br/>    """<br/>    messages = [{"role": "system", "content": system_prompt},<br/>                {"role": "user", "content": create_query(question)}]<br/>    return messages</span></pre><p id="10d6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This function constructs the query to provide to the LLM. The MedQA dataset stores each question as a JSON element, with the questions and options provided as keys. We parse the JSON and construct the question along with the choices.</p><p id="95b9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s start obtaining outputs from the model. The current task involves answering the provided medical question by selecting the correct answer from various options. Unlike creative tasks such as content writing or summarization, which may require the model to be imaginative and creative in its output, this is a knowledge-based task designed to test the model’s ability to answer questions based on knowledge encoded in its parameters. Therefore, we will use greedy decoding while generating the answer. Let’s define a helper function for parsing the model responses and calculating accuracy.</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="431d" class="pn oa fq pk b bg po pp l pq pr">pattern = re.compile(r"([A-Z])\.\s*(.*)")<br/><br/>def parse_answer(response):<br/>    """<br/>    Extracts the answer option from the predicted string.<br/><br/>    Args:<br/>    - response (str): The string to search for the pattern.<br/><br/>    Returns:<br/>    - str: The matched answer option if found or an empty string otherwise.<br/>    """<br/>    match = re.search(pattern, response)<br/>    if match:<br/>        letter = match.group(1)<br/>    else:<br/>        letter = ""<br/>    <br/>    return letter<br/><br/>def calculate_accuracy(ground_truth, predictions):<br/>    """<br/>    Calculates the accuracy of predictions compared to ground truth labels.<br/><br/>    Args:<br/>    - ground_truth (list): A list of true labels.<br/>    - predictions (list): A list of predicted labels.<br/><br/>    Returns:<br/>    - float: The accuracy of predictions as a fraction of correct predictions over total predictions.<br/>    """<br/>    return sum([1 if x==y else 0 for x,y in zip(ground_truth, predictions)]) / len(ground_truth)</span></pre><pre class="qx pj pk pl bp pm bb bk"><span id="c80b" class="pn oa fq pk b bg po pp l pq pr">ground_truth = []<br/><br/>for item in questions:<br/>    ans_options = item["options"]<br/>    correct_ans_option = ""<br/>    for key,value in ans_options.items():<br/>        if value == item["answer"]:<br/>            correct_ans_option = key<br/>            break<br/>            <br/>    ground_truth.append(correct_ans_option)</span></pre><pre class="qx pj pk pl bp pm bb bk"><span id="60d6" class="pn oa fq pk b bg po pp l pq pr">zero_shot_llama_answers = []<br/>for item in tqdm(questions):<br/>    zero_shot_prompt_messages = build_zero_shot_prompt(PROMPT, item)<br/>    input_ids = tokenizer.apply_chat_template(zero_shot_prompt_messages, tokenize=True, return_tensors="pt").cuda()  #modified on 09/03/2024   <br/>    # prompt = tokenizer.apply_chat_template(zero_shot_prompt_messages, tokenize=False)<br/>    # input_ids = tokenizer(prompt, return_tensors="pt", truncation=True).input_ids.cuda()    <br/>    outputs = model.generate(input_ids=input_ids, max_new_tokens=10, do_sample=False)<br/>    # https://github.com/huggingface/transformers/issues/17117#issuecomment-1124497554<br/>    gen_text = tokenizer.batch_decode(outputs.detach().cpu().numpy()[:, input_ids.shape[1]:], skip_special_tokens=True)[0]<br/>    zero_shot_llama_answers.append(gen_text.strip())<br/><br/>zero_shot_llama_predictions = [parse_answer(x) for x in zero_shot_llama_answers]<br/>print(calculate_accuracy(ground_truth, zero_shot_llama_predictions))</span></pre><p id="1d89" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We get a performance of 3̶6̶%̶ 35% in the zero-shot setting. Not a bad start, but let’s see if we can push this performance further.</p><blockquote class="qd qe qf"><p id="da91" class="mj mk pi ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Edit on 09/03/2024 </strong>— I noticed a minor “bug” in the way the prompt was formatted and tokenized. Specifically, I used the tokenizer.apply_chat_template(zero_shot_prompt_messages, tokenize = False) and then called the tokenizer to get the input_ids earlier. This method adds an extra &lt;s&gt; token at the start to the prompt. The apply_chat_template already adds an &lt;s&gt; token to the sequence, but calling the tokenizer after creating the chat template adds the specical token &lt;s&gt; again at the start. I fixed this by changing tokenize=True in apply_chat_template. The new score I got was 35%, a small dip of 1% compared to the old setting where the score was 36%. It’s a bit amusing that fixing this “bug” leads to a minor score dip, but I’m making the correction here and in the code to avoid any confusion in using chat templates. None of the findings or takeaways in this article are otherwise affected by this fix.</p></blockquote><h2 id="0ac4" class="qg oa fq bf ob qh qi qj oe qk ql qm oh ms qn qo qp mw qq qr qs na qt qu qv qw bk">Prompting Llama 2 7B-Chat with a Few-Shot Prompt</h2><p id="0cfc" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Let’s now provide task demonstrations to the model. We use the three randomly sampled questions from the training set and append them to the model as task demonstrations. Fortunately, we can continue using the chat-template support provided by the Transformers library and the tokenizer to append our few-shot examples with minimal code changes.</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="5d32" class="pn oa fq pk b bg po pp l pq pr">def build_few_shot_prompt(system_prompt, content, few_shot_examples):<br/>    """<br/>    Builds the few-shot prompt using provided examples.<br/><br/>    Args:<br/>        system_prompt (str): Task description for the LLM<br/>        content (dict): The content for which to create a query, similar to the<br/>            structure required by `create_query`.<br/>        few_shot_examples (list of dict): Examples to simulate a hypothetical<br/>            conversation. Each dict must have "options" and an "answer".<br/><br/>    Returns:<br/>        list of dict: A list of messages, simulating a conversation with<br/>            few-shot examples, followed by the current user query.<br/>    """<br/>    messages = [{"role": "system", "content": system_prompt}]<br/>    for item in few_shot_examples:<br/>        ans_options = item["options"]<br/>        correct_ans_option = ""<br/>        for key, value in ans_options.items():<br/>            if value == item["answer"]:<br/>                correct_ans_option = key<br/>                break<br/>        messages.append({"role": "user", "content": create_query(item)})<br/>        messages.append({"role": "assistant", "content": "The answer is " + correct_ans_option + "."})<br/>    messages.append({"role": "user", "content": create_query(content)})<br/>    return messages<br/><br/>few_shot_prompts = read_jsonl_file("USMLE_few_shot_samples.jsonl")</span></pre><p id="e1db" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s visualize what our few-shot prompt looks like.</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="2feb" class="pn oa fq pk b bg po pp l pq pr">&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;<br/>You will be provided with a medical or clinical question, along with multiple possible answer choices. Pick the right answer from the choices. <br/>Your response should be in the format "The answer is &lt;correct_choice&gt;". Do not add any other unnecessary content in your response<br/>&lt;&lt;/SYS&gt;&gt;<br/><br/>A 30-year-old woman presents to the clinic because of fever, joint pain, and a rash on her lower extremities. She admits to intravenous drug use. Physical examination reveals palpable petechiae and purpura on her lower extremities. Laboratory results reveal a negative antinuclear antibody, positive rheumatoid factor, and positive serum cryoglobulins. Which of the following underlying conditions in this patient is responsible for these findings?<br/>Options:<br/>A. Hepatitis B infection<br/>B. Hepatitis C infection<br/>C. HIV infection<br/>D. Systemic lupus erythematosus (SLE) [/INST] The answer is B. &lt;/s&gt;&lt;s&gt;[INST] A 10-year-old child presents to your office with a chronic cough. His mother states that he has had a cough for the past two weeks that is non-productive along with low fevers of 100.5 F as measured by an oral thermometer. The mother denies any other medical history and states that he has been around one other friend who also has had this cough for many weeks. The patient's vitals are within normal limits with the exception of his temperature of 100.7 F. His chest radiograph demonstrated diffuse interstitial infiltrates. Which organism is most likely causing his pneumonia?<br/>Options:<br/>A. Mycoplasma pneumoniae<br/>B. Staphylococcus aureus<br/>C. Streptococcus pneumoniae<br/>D. Streptococcus agalactiae [/INST] The answer is A. &lt;/s&gt;&lt;s&gt;[INST] A 44-year-old with a past medical history significant for human immunodeficiency virus infection presents to the emergency department after he was found to be experiencing worsening confusion. The patient was noted to be disoriented by residents and staff at the homeless shelter where he resides. On presentation he reports headache and muscle aches but is unable to provide more information. His temperature is 102.2°F (39°C), blood pressure is 112/71 mmHg, pulse is 115/min, and respirations are 24/min. Knee extension with hips flexed produces significant resistance and pain. A lumbar puncture is performed with the following results:<br/><br/>Opening pressure: Normal<br/>Fluid color: Clear<br/>Cell count: Increased lymphocytes<br/>Protein: Slightly elevated<br/><br/>Which of the following is the most likely cause of this patient's symptoms?<br/>Options:<br/>A. Cryptococcus<br/>B. Group B streptococcus<br/>C. Herpes simplex virus<br/>D. Neisseria meningitidis [/INST] The answer is C. &lt;/s&gt;&lt;s&gt;[INST] A 21-year-old male presents to his primary care provider for fatigue. He reports that he graduated from college last month and returned 3 days ago from a 2 week vacation to Vietnam and Cambodia. For the past 2 days, he has developed a worsening headache, malaise, and pain in his hands and wrists. The patient has a past medical history of asthma managed with albuterol as needed. He is sexually active with both men and women, and he uses condoms “most of the time.” On physical exam, the patient’s temperature is 102.5°F (39.2°C), blood pressure is 112/66 mmHg, pulse is 105/min, respirations are 12/min, and oxygen saturation is 98% on room air. He has tenderness to palpation over his bilateral metacarpophalangeal joints and a maculopapular rash on his trunk and upper thighs. Tourniquet test is negative. Laboratory results are as follows:<br/><br/>Hemoglobin: 14 g/dL<br/>Hematocrit: 44%<br/>Leukocyte count: 3,200/mm^3<br/>Platelet count: 112,000/mm^3<br/><br/>Serum:<br/>Na+: 142 mEq/L<br/>Cl-: 104 mEq/L<br/>K+: 4.6 mEq/L<br/>HCO3-: 24 mEq/L<br/>BUN: 18 mg/dL<br/>Glucose: 87 mg/dL<br/>Creatinine: 0.9 mg/dL<br/>AST: 106 U/L<br/>ALT: 112 U/L<br/>Bilirubin (total): 0.8 mg/dL<br/>Bilirubin (conjugated): 0.3 mg/dL<br/><br/>Which of the following is the most likely diagnosis in this patient?<br/>Options:<br/>A. Chikungunya<br/>B. Dengue fever<br/>C. Epstein-Barr virus<br/>D. Hepatitis A [/INST]</span></pre><p id="4a62" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The prompt is quite long, given that we append three demonstrations. Let’s now run Llama-2 with the few-shot prompt and get the results:</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="7c97" class="pn oa fq pk b bg po pp l pq pr">few_shot_llama_answers = []<br/>for item in tqdm(questions):<br/>    few_shot_prompt_messages = build_few_shot_prompt(PROMPT, item, few_shot_prompts)<br/>    input_ids = tokenizer.apply_chat_template(few_shot_prompt_messages, tokenize=True, return_tensors="pt").cuda() #modified on 09/03/2024<br/>    # prompt = tokenizer.apply_chat_template(few_shot_prompt_messages, tokenize=False)<br/>    # input_ids = tokenizer(prompt, return_tensors="pt", truncation=True).input_ids.cuda()<br/>    outputs = model.generate(input_ids=input_ids, max_new_tokens=10, do_sample=False)<br/>    gen_text = tokenizer.batch_decode(outputs.detach().cpu().numpy()[:, input_ids.shape[1]:], skip_special_tokens=True)[0]<br/>    few_shot_llama_answers.append(gen_text.strip())<br/><br/>few_shot_llama_predictions = [parse_answer(x) for x in few_shot_llama_answers]<br/>print(calculate_accuracy(ground_truth, few_shot_llama_predictions))</span></pre><p id="fca2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We now get an overall accuracy of 4̵1̵.̵6̵7̵%̵ 40.33%. Not bad, nearly 6̵%̵ 5% improvement over zero-shot prompting with the same model!</p><blockquote class="qd qe qf"><p id="c5ea" class="mj mk pi ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Edit on 09/03/2024 </strong>— Similar to the zero-shot setting, I fixed the way the prompt is sent to the model for the few-shot setting. The new score I got was 40.33%, a small dip of ~1.3% compared to the old setting where the score was 41.67%. It’s again a bit amusing that fixing this “bug” leads to a minor score dip, but I’m making the correction here and in the code to avoid any confusion in using chat templates. None of the findings or takeaways in this article are otherwise affected by this fix.</p></blockquote><h2 id="a04e" class="qg oa fq bf ob qh qi qj oe qk ql qm oh ms qn qo qp mw qq qr qs na qt qu qv qw bk">What happens if we don’t adhere to the chat template?</h2><p id="135e" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Earlier, I observed that it is advisable to structure our prompt according to the prompt template that was used to fine-tune an LLM originally. Let’s verify if not adhering to the chat template hurts our performance. We create a function that builds a few-shot prompt using the same examples without adhering to the chat format.</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="ddf5" class="pn oa fq pk b bg po pp l pq pr">def build_few_shot_prompt_wo_chat_template(system_prompt, content, few_shot_examples):<br/>    """<br/>    Builds the few-shot prompt using provided examples, bypassing the chat-template<br/>    for Llama-2.<br/><br/>    Args:<br/>        system_prompt (str): Task description for the LLM<br/>        content (dict): The content for which to create a query, similar to the<br/>            structure required by `create_query`.<br/>        few_shot_examples (list of dict): Examples to simulate a hypothetical<br/>            conversation. Each dict must have "options" and an "answer".<br/><br/>    Returns:<br/>        str: few-shot prompt in non-chat format<br/>    """<br/>    few_shot_prompt = ""<br/>    few_shot_prompt += "Task: " + system_prompt + "\n"<br/>    for item in few_shot_examples:<br/>        ans_options = item["options"]<br/>        correct_ans_option = ""<br/>        for key, value in ans_options.items():<br/>            if value == item["answer"]:<br/>                correct_ans_option = key<br/>                break<br/>        few_shot_prompt += create_query(item) + "\n" + "The answer is " + correct_ans_option + "." + "\n"<br/>    <br/>    few_shot_prompt += create_query(content) + "\n"<br/>    return few_shot_prompt</span></pre><p id="962a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Our prompts now look like this:</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="9cf9" class="pn oa fq pk b bg po pp l pq pr">Task: You will be provided with a medical or clinical question, along with multiple possible answer choices. Pick the right answer from the choices. <br/>Your response should be in the format "The answer is &lt;correct_choice&gt;". Do not add any other unnecessary content in your response<br/>A 30-year-old woman presents to the clinic because of fever, joint pain, and a rash on her lower extremities. She admits to intravenous drug use. Physical examination reveals palpable petechiae and purpura on her lower extremities. Laboratory results reveal a negative antinuclear antibody, positive rheumatoid factor, and positive serum cryoglobulins. Which of the following underlying conditions in this patient is responsible for these findings?<br/>Options:<br/>A. Hepatitis B infection<br/>B. Hepatitis C infection<br/>C. HIV infection<br/>D. Systemic lupus erythematosus (SLE)<br/>The answer is B.<br/>A 10-year-old child presents to your office with a chronic cough. His mother states that he has had a cough for the past two weeks that is non-productive along with low fevers of 100.5 F as measured by an oral thermometer. The mother denies any other medical history and states that he has been around one other friend who also has had this cough for many weeks. The patient's vitals are within normal limits with the exception of his temperature of 100.7 F. His chest radiograph demonstrated diffuse interstitial infiltrates. Which organism is most likely causing his pneumonia?<br/>Options:<br/>A. Mycoplasma pneumoniae<br/>B. Staphylococcus aureus<br/>C. Streptococcus pneumoniae<br/>D. Streptococcus agalactiae<br/>The answer is A.<br/>A 44-year-old with a past medical history significant for human immunodeficiency virus infection presents to the emergency department after he was found to be experiencing worsening confusion. The patient was noted to be disoriented by residents and staff at the homeless shelter where he resides. On presentation he reports headache and muscle aches but is unable to provide more information. His temperature is 102.2°F (39°C), blood pressure is 112/71 mmHg, pulse is 115/min, and respirations are 24/min. Knee extension with hips flexed produces significant resistance and pain. A lumbar puncture is performed with the following results:<br/><br/>Opening pressure: Normal<br/>Fluid color: Clear<br/>Cell count: Increased lymphocytes<br/>Protein: Slightly elevated<br/><br/>Which of the following is the most likely cause of this patient's symptoms?<br/>Options:<br/>A. Cryptococcus<br/>B. Group B streptococcus<br/>C. Herpes simplex virus<br/>D. Neisseria meningitidis<br/>The answer is C.<br/>A 21-year-old male presents to his primary care provider for fatigue. He reports that he graduated from college last month and returned 3 days ago from a 2 week vacation to Vietnam and Cambodia. For the past 2 days, he has developed a worsening headache, malaise, and pain in his hands and wrists. The patient has a past medical history of asthma managed with albuterol as needed. He is sexually active with both men and women, and he uses condoms “most of the time.” On physical exam, the patient’s temperature is 102.5°F (39.2°C), blood pressure is 112/66 mmHg, pulse is 105/min, respirations are 12/min, and oxygen saturation is 98% on room air. He has tenderness to palpation over his bilateral metacarpophalangeal joints and a maculopapular rash on his trunk and upper thighs. Tourniquet test is negative. Laboratory results are as follows:<br/><br/>Hemoglobin: 14 g/dL<br/>Hematocrit: 44%<br/>Leukocyte count: 3,200/mm^3<br/>Platelet count: 112,000/mm^3<br/><br/>Serum:<br/>Na+: 142 mEq/L<br/>Cl-: 104 mEq/L<br/>K+: 4.6 mEq/L<br/>HCO3-: 24 mEq/L<br/>BUN: 18 mg/dL<br/>Glucose: 87 mg/dL<br/>Creatinine: 0.9 mg/dL<br/>AST: 106 U/L<br/>ALT: 112 U/L<br/>Bilirubin (total): 0.8 mg/dL<br/>Bilirubin (conjugated): 0.3 mg/dL<br/><br/>Which of the following is the most likely diagnosis in this patient?<br/>Options:<br/>A. Chikungunya<br/>B. Dengue fever<br/>C. Epstein-Barr virus<br/>D. Hepatitis A</span></pre><p id="337f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s now evaluate Llama 2 with these prompts and observe how it performs:</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="aee7" class="pn oa fq pk b bg po pp l pq pr">few_shot_llama_answers_wo_chat_template = []<br/>for item in tqdm(questions):<br/>    prompt = build_few_shot_prompt_wo_chat_template(PROMPT, item, few_shot_prompts)<br/>    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True).input_ids.cuda()<br/>    outputs = model.generate(input_ids=input_ids, max_new_tokens=10, do_sample=False)<br/>    gen_text = tokenizer.batch_decode(outputs.detach().cpu().numpy()[:, input_ids.shape[1]:], skip_special_tokens=True)[0]<br/>    few_shot_llama_answers_wo_chat_template.append(gen_text.strip())<br/><br/>few_shot_llama_predictions_wo_chat_template = [parse_answer(x) for x in few_shot_llama_answers_wo_chat_template]<br/>print(calculate_accuracy(ground_truth, few_shot_llama_predictions_wo_chat_template))</span></pre><p id="3616" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We achieve an accuracy of 36%. This is 6̶%̶ 4.3% lower than our earlier few-shot score. This reinforces our previous argument that it is crucial to structure our prompts according to the template used to fine-tune the LLM we intend to work with. Prompt templates matter!</p><h2 id="bfe1" class="qg oa fq bf ob qh qi qj oe qk ql qm oh ms qn qo qp mw qq qr qs na qt qu qv qw bk"><strong class="al">Prompting Llama 2 7B-Chat with CoT Prompting</strong></h2><p id="cb1f" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Let’s conclude by evaluating CoT prompting. Remember, our dataset includes questions designed to test medical knowledge through the USMLE exam. Such questions often require both factual recall and conceptual reasoning to answer. This makes it a perfect task for testing how well CoT works.</p><p id="5025" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First, we must provide an example CoT prompt to the model to demonstrate how to reason about a question. For this purpose, we will use one of the prompts from Google’s MedPALM paper [12].</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qy"><img src="../Images/e947fa6ba06d18bae65fe3354e5778aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*WKfo3-8_DDydCZwlr6bisg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Five-shot CoT prompt used for evaluating the MedPALM model on the MedQA dataset. Prompt borrowed from Table A.18, Page 41 of [12] (<a class="af ny" href="https://arxiv.org/abs/2212.13138" rel="noopener ugc nofollow" target="_blank">Source</a>).</figcaption></figure><p id="b349" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We use this five-shot prompt for evaluating the models. Since this prompt style differs slightly from our earlier prompts, let’s create some helper functions again to process them and obtain the outputs. While utilizing CoT prompting, we generate the output with a larger output token count to enable the model to “think” and “reason” before answering the question.</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="da4b" class="pn oa fq pk b bg po pp l pq pr">def create_query_cot(item):<br/>    """<br/>    Creates the input for the model using the question and the multiple choice options in the CoT format.<br/><br/>    Args:<br/>        item (dict): A dictionary containing the question and options.<br/>            Expected keys are "question" and "options", where "options" is another<br/>            dictionary with keys "A", "B", "C", and "D".<br/><br/>    Returns:<br/>        str: A formatted query combining the question and options, ready for use.<br/>    """<br/>    query = "Question: " + item["question"] + "\n" + \<br/>            "(A) " + item["options"]["A"] + " " +  \<br/>            "(B) " + item["options"]["B"] + " " +  \<br/>            "(C) " + item["options"]["C"] + " " +  \<br/>            "(D) " + item["options"]["D"]<br/>    return query<br/><br/>def build_cot_prompt(instruction, input_question, cot_examples):<br/>    """<br/>    Builds the few-shot prompt for the GPT API using provided examples.<br/><br/>    Args:<br/>        content (dict): The content for which to create a query, similar to the<br/>            structure required by `create_query`.<br/>        few_shot_examples (list of dict): Examples to simulate a hypothetical<br/>            conversation. Each dict must have "question" and an "explanation".<br/><br/>    Returns:<br/>        list of dict: A list of messages, simulating a conversation with<br/>            few-shot examples, followed by the current user query.<br/>    """<br/>    <br/>    messages = [{"role": "system", "content": instruction}]<br/>    for item in cot_examples:<br/>        messages.append({"role": "user", "content": item["question"]})<br/>        messages.append({"role": "assistant", "content": item["explanation"]})<br/><br/>    <br/>    messages.append({"role": "user", "content": create_query_cot(input_question)})<br/>    <br/>    return messages<br/><br/>def parse_answer_cot(text):<br/>    """<br/>    Extracts the choice from a string that follows the pattern "Answer: (Choice) Text".<br/><br/>    Args:<br/>    - text (str): The input string from which to extract the choice.<br/><br/>    Returns:<br/>    - str: The extracted choice or a message indicating no match was found.<br/>    """<br/>    # Regex pattern to match the answer part<br/>    pattern = r"Answer: (.*)"<br/><br/>    # Search for the pattern in the text and extract the matching group<br/>    match = re.search(pattern, text)<br/>    <br/>    if match:<br/>        if len(match.group(1)) &gt; 1:<br/>            return match.group(1)[1]<br/>        else:<br/>            return ""<br/>    else:<br/>        return ""</span></pre><pre class="qx pj pk pl bp pm bb bk"><span id="0e6b" class="pn oa fq pk b bg po pp l pq pr">cot_llama_answers = []<br/>for item in tqdm(questions):<br/>    cot_prompt = build_cot_prompt(COT_INSTRUCTION, item, COT_EXAMPLES)<br/>    input_ids = tokenizer.apply_chat_template(cot_prompt, tokenize=True, return_tensors="pt").cuda() #modified on 09/03/2024<br/>    # prompt = tokenizer.apply_chat_template(cot_prompt, tokenize=False)<br/>    # input_ids = tokenizer(prompt, return_tensors="pt", truncation=True).input_ids.cuda()    <br/>    outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=False)<br/>    gen_text = tokenizer.batch_decode(outputs.detach().cpu().numpy()[:, input_ids.shape[1]:], skip_special_tokens=True)[0]<br/>    cot_llama_answers.append(gen_text.strip())<br/><br/>cot_llama_predictions = [parse_answer_cot(x) for x in cot_llama_answers]<br/>print(calculate_accuracy(ground_truth, cot_llama_predictions))</span></pre><p id="47a9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Our performance dips to 2̶0̶%̶ 21.33% using CoT prompting for Llama 2–7B. This is generally in line with the findings of the CoT paper [6], where the authors mention that CoT is an emergent property for LLMs that improves with the scale of the model. That being said, let’s analyze why the performance dipped drastically.</p><blockquote class="qd qe qf"><p id="fda2" class="mj mk pi ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Edit on 09/03/2024 </strong>— Similar to the zero-shot setting, I fixed the way the prompt was sent to the model for the CoT setting. The new score I got was 21.33%, a small increase of ~1.33% compared to the old setting where the score was 20%. I’m making the correction here and in the code to avoid any confusion about using chat templates. None of the findings or takeaways in this article are otherwise affected by this fix.</p></blockquote><h2 id="0ac0" class="qg oa fq bf ob qh qi qj oe qk ql qm oh ms qn qo qp mw qq qr qs na qt qu qv qw bk">Failure Modes in CoT for Llama 2</h2><p id="c1ca" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">We sample a few of the responses provided by Llama 2 on some of the test set questions to analyze error cases:</p><blockquote class="qd qe qf"><p id="a005" class="mj mk pi ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The analysis of these CoT samples and the figures are not affected by the fixing of the minor “bug”. I’ve verified that the predictions used in these figures are identical in both the old and new setting.</p></blockquote><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng pt"><img src="../Images/bafde2d1cb6d802d5b8c1c826acfc01c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H1fbVPdN_-Y12b61oy4rRw.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Sample Prediction 1 — The model arrives at an answer but does not adhere to the format, making parsing the result hard. (Image by the author)</figcaption></figure><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qz"><img src="../Images/9192f54c14ae9038111571c6319de6cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ms3S2RwJXBpIB3IpgqkOQw.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Sample Prediction 2 — The model fails to adhere to the prompt format and arrive at a conclusive answer. (Image by the author)</figcaption></figure><p id="7339" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While CoT prompting allows the model to “think” before arriving at the final answer, in most cases, the model either does not arrive at a conclusive answer or mentions the answer in a format inconsistent with our example demonstrations. A failure mode I haven’t analyzed here, but potentially worth exploring, is to check cases in the test set where the model “reasons” incorrectly and, therefore, arrives at the wrong answer. This is beyond the scope of the current article and my medical knowledge, but it is certainly something I intend to revisit later.</p><h2 id="c938" class="qg oa fq bf ob qh qi qj oe qk ql qm oh ms qn qo qp mw qq qr qs na qt qu qv qw bk">Prompting GPT-3.5 with a Zero-Shot Prompt</h2><p id="262c" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Let’s begin defining some helper functions that help us process these inputs for utilizing the GPT API. You would need to generate an API key to use the GPT-3.5 API. You can set the API key in Windows using:</p><p id="3e61" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><code class="cx ra rb rc pk b">setx OPENAI_API_KEY "your-api-key-here"</code></p><p id="3d93" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">or in Linux using:</p><p id="db05" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><code class="cx ra rb rc pk b">export OPENAI_API_KEY "your-api-key-here"</code></p><p id="a0f1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">in the current session you are using.</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="1217" class="pn oa fq pk b bg po pp l pq pr">from openai import OpenAI<br/>import re<br/>from tqdm import tqdm<br/><br/># assuming you have already set the secret key using env variable<br/># if not, you can also instantiate the OpenAI client by providing the <br/># secret key directly like so:<br/># I highly recommend not doing this, as it is a best practice to not store<br/># the api key in your code directly or in any plain-text file for security <br/># reasons.<br/># client = OpenAI(api_key = "")<br/><br/>client = OpenAI() </span></pre><pre class="qx pj pk pl bp pm bb bk"><span id="f536" class="pn oa fq pk b bg po pp l pq pr"><br/>def get_response(messages, model_name, temperature = 0.0, max_tokens = 10):<br/>    """<br/>    Obtains the responses/answers of the model through the chat-completions API.<br/><br/>    Args:<br/>        messages (list of dict): The built messages provided to the API.<br/>        model_name (str): Name of the model to access through the API<br/>        temperature (float): A value between 0 and 1 that controls the randomness of the output.<br/>        A temperature value of 0 ideally makes the model pick the most likely token, making the outputs (mostly) deterministic.<br/>        max_tokens (int): Maximum number of tokens that the model should generate<br/><br/>    Returns:<br/>        str: The response message content from the model.<br/>    """<br/>    response = client.chat.completions.create(<br/>        model=model_name,<br/>        messages=messages,<br/>        temperature=temperature,<br/>        max_tokens=max_tokens<br/>    )<br/>    return response.choices[0].message.content</span></pre><p id="c750" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This function now constructs the prompt in the format for the GPT-3.5 API. We can interact with the GPT-3.5 model through the chat-completions API provided by the library. The API requires messages to be structured as a list of dictionaries for sending to the API. Each message must specify the role and the content. The conventions followed regarding the <strong class="ml fr"><em class="pi">“system”,</em></strong> “<strong class="ml fr"><em class="pi">user</em></strong>”, and “<strong class="ml fr"><em class="pi">assistant</em></strong>” roles are the same as those described earlier for the Llama-7B Chat Model.</p><p id="3111" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s now use the GPT-3.5 API to process the test set and obtain the responses. After receiving all the responses, we extract the options from the model’s responses and calculate the accuracy.</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="e8f1" class="pn oa fq pk b bg po pp l pq pr">zero_shot_gpt_answers = []<br/>for item in tqdm(questions):<br/>    zero_shot_prompt_messages = build_zero_shot_prompt(PROMPT, item)<br/>    answer = get_response(zero_shot_prompt_messages, model_name = "gpt-3.5-turbo", temperature = 0.0, max_tokens = 10)<br/>    zero_shot_gpt_answers.append(answer)<br/><br/>zero_shot_gpt_predictions = [parse_answer(x) for x in zero_shot_gpt_answers]<br/>print(calculate_accuracy(ground_truth, zero_shot_gpt_predictions))</span></pre><p id="e365" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Our performance now stands at 63%. This is a significant improvement from the performance of Llama 2–7B. This isn’t surprising, given that GPT-3.5 is likely much larger and trained on more data than Llama 2–7B, along with other proprietary optimizations that OpenAI may have included to the model. Let’s see how well few-shot prompting works now.</p><h2 id="a575" class="qg oa fq bf ob qh qi qj oe qk ql qm oh ms qn qo qp mw qq qr qs na qt qu qv qw bk">Prompting GPT-3.5 with a Few-Shot Prompt</h2><p id="9cff" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">To provide few-shot examples to the LLM, we reuse the three examples we sampled from the training set and append them to the prompt. For GPT-3.5, we create a list of messages with examples, similar to our earlier processing for Llama 2. The inputs are appended using the “user” role, and the corresponding option is presented in the “assistant” role. We reuse the earlier function for building few-shot prompts.</p><blockquote class="qd qe qf"><p id="35a3" class="mj mk pi ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is again equivalent to creating a fictional multi-turn conversation history provided to GPT-3.5, where each turn corresponds to an example demonstration.</p></blockquote><p id="b311" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s now obtain the outputs using GPT-3.5.</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="f179" class="pn oa fq pk b bg po pp l pq pr">few_shot_gpt_answers = []<br/>for item in tqdm(questions):<br/>    few_shot_prompt_messages = build_few_shot_prompt(PROMPT, item, few_shot_prompts)<br/>    answer = get_response(few_shot_prompt_messages, model_name= "gpt-3.5-turbo", temperature = 0.0, max_tokens = 10)<br/>    few_shot_gpt_answers.append(answer)<br/><br/>few_shot_gpt_predictions = [parse_answer(x) for x in few_shot_gpt_answers]<br/>print(calculate_accuracy(ground_truth, few_shot_gpt_predictions))</span></pre><p id="0b2c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We’ve managed to push the performance from 63% to 67% using few-shot prompting! This is a significant improvement, highlighting the value of providing task demonstrations to the model.</p><h2 id="18e9" class="qg oa fq bf ob qh qi qj oe qk ql qm oh ms qn qo qp mw qq qr qs na qt qu qv qw bk">Prompting GPT-3.5 with CoT Prompting</h2><p id="c954" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Let’s now evaluate GPT-3.5 with CoT prompting. We re-use the same CoT prompt and get the outputs:</p><pre class="ni nj nk nl nm pj pk pl bp pm bb bk"><span id="fc0c" class="pn oa fq pk b bg po pp l pq pr">cot_gpt_answers = []<br/>for item in tqdm(questions):<br/>    cot_prompt = build_cot_prompt(COT_INSTRUCTION, item, COT_EXAMPLES)<br/>    answer = get_response(cot_prompt, model_name= "gpt-3.5-turbo", temperature = 0.0, max_tokens = 100)<br/>    cot_gpt_answers.append(answer)<br/><br/>cot_gpt_predictions = [parse_answer_cot(x) for x in cot_gpt_answers]<br/>print(calculate_accuracy(ground_truth, cot_gpt_predictions))</span></pre><p id="160c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Using CoT prompting with GPT-3.5 results in an accuracy of 71%! This represents a further 4% improvement over few-shot prompting. It appears that enabling the model to “think” out loud before answering the question is beneficial for this task. This is also consistent with the findings of the paper [6] that CoT unlocked performance improvements for larger parameter models.</p><h1 id="913f" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion and Takeaways:</h1><p id="d6f5" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Prompting is a crucial skill for working with Large Language Models (LLMs), and understanding that there are various tools in the prompting toolkit that can help extract better performance from LLMs for your tasks depending on the context. I hope this article serves as a broad and (hopefully!) accessible introduction to this subject. However, it does not aim to provide a comprehensive overview of all prompting strategies. Prompting remains a highly active field of research, with numerous methods being introduced such as ReAct [13], Tree-of-Thought prompting [14] etc. I recommend exploring these techniques to better understand them and enhance your prompting toolkit.</p><h1 id="f8e9" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Reproducibility</h1><p id="4fbb" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">In this article, I’ve aimed to make all experiments as deterministic and reproducible as possible. We use greedy decoding to obtain our outputs for zero-shot, few-shot, and CoT prompting with Llama-2. While these scores should technically be reproducible, in rare cases, Cuda/GPU-related or library issues could lead to slightly different results.</p><p id="a7b4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Similarly, when obtaining responses from the GPT-3.5 API, we use a temperature of 0 to get results and choose only the next most likely token without sampling for all prompt settings. This makes the results <a class="af ny" href="https://platform.openai.com/docs/guides/text-generation/reproducible-outputs" rel="noopener ugc nofollow" target="_blank">“mostly deterministic”</a>, so it is possible that sending the same prompts to GPT-3.5 again may result in slightly different results.</p><p id="3f0c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I have provided the outputs of the models under all prompt settings, along with the sub-sampled test set, few-shot prompt examples, and CoT prompt (from the MedPALM paper) for reproducing the scores reported in this article.</p><h1 id="42e4" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">References:</strong></h1><blockquote class="qd qe qf"><p id="502d" class="mj mk pi ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">All papers referred to in this blog post are listed here. Please let me know if I might have missed out any references, and I will add them!</p></blockquote><p id="f6ec" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[1] Yang, J., Jin, H., Tang, R., Han, X., Feng, Q., Jiang, H., … &amp; Hu, X. (2023). Harnessing the power of llms in practice: A survey on chatgpt and beyond. <em class="pi">arXiv preprint arXiv:2304.13712</em>.</p><p id="dc50" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). Language models are unsupervised multitask learners. <em class="pi">OpenAI blog</em>, <em class="pi">1</em>(8), 9.</p><p id="53ee" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[3] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., … &amp; Amodei, D. (2020). Language models are few-shot learners. <em class="pi">Advances in neural information processing systems</em>, <em class="pi">33</em>, 1877–1901.</p><p id="0c2e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[4] Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., … &amp; Le, Q. V. (2021). Finetuned language models are zero-shot learners. <em class="pi">arXiv preprint arXiv:2109.01652</em>.</p><p id="6831" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[5] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., … &amp; Lowe, R. (2022). Training language models to follow instructions with human feedback. <em class="pi">Advances in Neural Information Processing Systems</em>, <em class="pi">35</em>, 27730–27744.</p><p id="b470" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[6] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., … &amp; Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. <em class="pi">Advances in Neural Information Processing Systems</em>, <em class="pi">35</em>, 24824–24837.</p><p id="f47d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[7] Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H. T., … &amp; Le, Q. (2022). Lamda: Language models for dialog applications. <em class="pi">arXiv preprint arXiv:2201.08239</em>.</p><p id="66d2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[8] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., … &amp; Fiedel, N. (2023). Palm: Scaling language modeling with pathways. <em class="pi">Journal of Machine Learning Research</em>, <em class="pi">24</em>(240), 1–113.</p><p id="79e3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[9] Jin, D., Pan, E., Oufattole, N., Weng, W. H., Fang, H., &amp; Szolovits, P. (2021). What disease does this patient have? a large-scale open domain question answering dataset from medical exams. <em class="pi">Applied Sciences</em>, <em class="pi">11</em>(14), 6421.</p><p id="ff20" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[10] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., … &amp; Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models. <em class="pi">arXiv preprint arXiv:2307.09288</em>.</p><p id="7167" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[11] <a class="af ny" href="https://platform.openai.com/docs/models/gpt-3-5-turbo" rel="noopener ugc nofollow" target="_blank">https://platform.openai.com/docs/models/gpt-3-5-turbo</a></p><p id="c12e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[12] Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., … &amp; Natarajan, V. (2023). Large language models encode clinical knowledge. <em class="pi">Nature</em>, <em class="pi">620</em>(7972), 172–180.</p><p id="3699" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[13] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., &amp; Cao, Y. (2022, September). ReAct: Synergizing Reasoning and Acting in Language Models. In <em class="pi">The Eleventh International Conference on Learning Representations</em>.</p><p id="7862" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[14] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., &amp; Narasimhan, K. (2024). Tree of thoughts: Deliberate problem solving with large language models. <em class="pi">Advances in Neural Information Processing Systems</em>, <em class="pi">36</em>.</p></div></div></div></div>    
</body>
</html>