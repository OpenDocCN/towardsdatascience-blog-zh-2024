- en: Metrics to Evaluate a Classification Machine Learning Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/metrics-to-evaluate-a-classification-machine-learning-model-f05f1facd569?source=collection_archive---------7-----------------------#2024-07-31](https://towardsdatascience.com/metrics-to-evaluate-a-classification-machine-learning-model-f05f1facd569?source=collection_archive---------7-----------------------#2024-07-31)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*A study case of credit card fraud*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lucasbraga461?source=post_page---byline--f05f1facd569--------------------------------)[![Lucas
    Braga](../Images/a652476cfec8d4f129d2d47a64c3e8c3.png)](https://medium.com/@lucasbraga461?source=post_page---byline--f05f1facd569--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f05f1facd569--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f05f1facd569--------------------------------)
    [Lucas Braga](https://medium.com/@lucasbraga461?source=post_page---byline--f05f1facd569--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f05f1facd569--------------------------------)
    ·7 min read·Jul 31, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**1\. Introduction**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we trained a supervised machine learning model to solve a classification
    problem, we’d be happy if this was the end of our work, and we could just throw
    them new data. We hope it will classify everything correctly. However, in reality,
    not all predictions that a model makes are correct. There is a famous quote well
    known in Data Science, created by a British Statistician that says:'
  prefs: []
  type: TYPE_NORMAL
- en: “All models are wrong; some are useful.” CLEAR, James, 1976.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, how do we know how good the model we have is? The short answer is that we
    do that by evaluating how correct the model’s predictions are. For that, there
    are several metrics that we could use.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. How does a model make predictions? i.e., How does a model classify data?**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4a1e2df439a5ffc27a6d116314a8674.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 1: Model making prediction'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we’ve trained a Machine Learning model to classify a credit card transaction
    and decide whether that particular transaction is Fraud or not Fraud. The model
    will consume the transaction data and give back a score that could be any number
    within the range of 0 to 1, e.g., 0.05, 0.24, 0.56, 0.9875\. For this article,
    we’ll define a default threshold of 0.5, which means if the model gave a score
    lower than 0.5, then the model has classified that transaction as not Fraud (**that’s
    a model prediction**!). If the model gave a score greater or equal to 0.5, then
    the model classified that transaction as Fraud (that’s also a model prediction!).
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we don’t work with the default of 0.5\. We look into different
    thresholds to see what is more appropriate to optimize the model’s performance,
    but that discussion is for another day.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Confusion Matrix**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The confusion matrix is a fundamental tool for visualizing the performance
    of a classification model. It helps in understanding the various outcomes of the
    predictions, which include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positive (TP)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive (FP)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negative (FN)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative (TN)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s break it down!
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate a model’s effectiveness, we need to compare its predictions against
    actual outcomes. Actual outcomes are also known as “the reality.” So, a model
    could have classified a transaction as Fraud, and in fact, the customer asked
    for his money back on that same transaction, claiming that his credit card was
    stolen.
  prefs: []
  type: TYPE_NORMAL
- en: In that scenario, the model correctly predicted the transaction as Fraud, a
    **True Positive (TP)**.
  prefs: []
  type: TYPE_NORMAL
- en: In Fraud detection contexts, the “positive” class is labeled as Fraud, and the
    “negative” class is labeled Non-Fraud.
  prefs: []
  type: TYPE_NORMAL
- en: A **False Positive (FP)**, on the other hand, occurs when the model also classifies
    a transaction as Fraud, but in that case, the customer did not report any fraudulent
    activity on their credit card usage. So, in this transaction, the Machine Learning
    model made a mistake.
  prefs: []
  type: TYPE_NORMAL
- en: A **True Negative (TN)** is when the model classified the transaction as Not
    Fraud, and in fact, it was not Fraud. So, the model has made the correct classification.
  prefs: []
  type: TYPE_NORMAL
- en: A **False Negative (FN)** was when the model classified the transaction as Not
    Fraud. Still, it was Fraud (the customer reported fraudulent activity on their
    credit card related to that transaction). In this case, the Machine Learning model
    also made a mistake, but it’s a different type of error than a False Positive.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at image 2
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c4a16c6c43ec99bb1ad30bfb9b2644a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 2: Confusion Matrix classifying a Machine Learning Model for Fraud'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see a different case, maybe more relatable. A test was designed to tell
    whether a patient has COVID. See image 3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/437092ea96fac940202c77c8082f29e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 3: Confusion Matrix for a COVID test'
  prefs: []
  type: TYPE_NORMAL
- en: So, for every transaction, you could check whether it’s TP, FP, TN, or FN. And
    you could do this for thousands of millions of transactions and write the results
    down on a 2x2 table with all the counts of TP, FP, TN and FN. This table is also
    known as a **Confusion Matrix**.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you compared the model predictions of 100,000 transactions against
    their actual outcomes and came up with the following Confusion Matrix (see image
    4).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4edc6fa1e484c3ab5934edb51059843b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 4: Confusion Matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Metrics to Evaluate Model Performance**'
  prefs: []
  type: TYPE_NORMAL
- en: and what a confusion matrix is, we are ready to explore the metrics used to
    evaluate a classification model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision = TP / (TP + FP)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'It answers the question: What’s the proportion of correct predictions among
    all predictions? It reflects the proportion of predicted fraud cases that were
    Fraud.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In simple language: What’s the proportion of when the model called it Fraud,
    and it was Fraud?'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the Confusion Matrix from image 4, we compute the Precision = 76.09%
    since Precision = 350 / (350 + 110).
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall = TP / (TP + FN)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall is also known as True Positive Rate (TPR). It answers the question:
    What’s the proportion of correct predictions among all positive actual outcomes?'
  prefs: []
  type: TYPE_NORMAL
- en: In simple language, what’s the proportion of times that the model caught the
    fraudster correctly in all actual fraud cases?
  prefs: []
  type: TYPE_NORMAL
- en: Using the Confusion Matrix from image 4, the Recall = 74.47%, since Recall =
    350 / (350 + 120).
  prefs: []
  type: TYPE_NORMAL
- en: '**Alert Rate = (TP + FP) / (TP + FP + TN + FN)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also known as Block Rate, this metric helps answer the question: What’s the
    proportion of positive predictions over all predictions?'
  prefs: []
  type: TYPE_NORMAL
- en: 'In simple language: What proportion of times the model predicted something
    was Fraud?'
  prefs: []
  type: TYPE_NORMAL
- en: Using the Confusion Matrix from image 4, the Alert Rate = 0.46%, since Alert
    Rate = (350 + 110) / (350 + 110 + 120 + 99420).
  prefs: []
  type: TYPE_NORMAL
- en: '**F1 Score = 2x (Precision x Recall) / (Precision + Recall)**'
  prefs: []
  type: TYPE_NORMAL
- en: The F1 Score is a harmonic mean of Precision and Recall. It is a balanced measure
    between Precision and Recall, providing a single score to assess the model.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Confusion Matrix from image 4, the F1-Score = 75.27%, since F1-Score
    = 2*(76.09% * 74.47%) / (76.09% + 74.47%).
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy = TP + TN / (TP + TN + FP + FN)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy helps answer this question: What’s the proportion of correctly classified
    transactions over all transactions?'
  prefs: []
  type: TYPE_NORMAL
- en: Using the Confusion Matrix from image 4, the Accuracy = 99.77%, since Accuracy
    = (350 + 120) / (350 + 110 + 120 + 99420).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82f2916f4a4144a638bce48a97268c0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 5: Confusion Matrix with Evaluation Metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. When to use what metric**'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is a go-to metric for evaluating many classification machine learning
    models. However, accuracy does not work well for cases where the target variable
    is imbalanced. In the case of Fraud detection, there is usually a tiny percentage
    of the data that is fraudulent; for example, in credit card fraud, it’s usually
    less than 1% of fraudulent transactions. So even if the model says that all transactions
    are fraudulent, which would be very incorrect, or that all transactions are not
    fraudulent, which would also be very wrong, the model’s accuracy would still be
    above 99%.
  prefs: []
  type: TYPE_NORMAL
- en: So what to do in those cases? Precision, Recall, and Alert Rate. Those are usually
    the metrics that give a good perspective on the model performance, even if the
    data is imbalanced. Which one exactly to use might depend on your stakeholders.
    I worked with stakeholders that said, whatever you do, please keep a Precision
    of at least 80%. So in that case, the stakeholder was very concerned about the
    user experience because if the Precision is very low, that means there will be
    a lot of False Positives, meaning that the model would incorrectly block good
    customers thinking they are placing fraudulent credit card transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, there is a trade-off between Precision and Recall: the higher
    the Precision, the lower the Recall. So, if the model has a very high Precision,
    it won’t be great at finding all the fraud cases. In some sense, it also depends
    on how much a fraud case costs the business (financial loss, compliance problems,
    fines, etc.) vs. how many false positive cases cost the business (customer lifetime,
    which impacts business profitability).'
  prefs: []
  type: TYPE_NORMAL
- en: So, in cases where the financial decision between Precision and Recall is unclear,
    a good metric to use is F1-Score, which helps provide a balance between Precision
    and Recall and optimizes for both of them.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, the Alert Rate is also a critical metric to consider because
    it gives an intuition about the number of transactions the Machine Learning model
    is planning to block. If the Alert Rate is very high, like 15%, that means that
    from all the orders placed by customers, 15% will be blocked, and only 85% will
    be accepted. So if you have a business with 1,000,000 orders daily, the machine
    learning model would block 150,000 of them thinking they’re fraudulent transactions.
    That’s a massive amount of orders blocked, and it’s important to have an instinct
    about the percentage of fraud cases. If fraud cases are about 1% or less, then
    a model blocking 15% is not only making a lot of mistakes but also blocking a
    big part of the business revenue.
  prefs: []
  type: TYPE_NORMAL
- en: '**6\. Conclusion**'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding these metrics allows data scientists and analysts to interpret
    the results of classification models better and enhance their performance. Precision
    and Recall offer more insights into the effectiveness of a model than mere accuracy,
    not only, but especially in fields like fraud detection where the class distribution
    is heavily skewed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Images: Unless otherwise noted, all images are by the author. Image 1’s robot
    face was created by DALL-E, and it''s for public use.*'
  prefs: []
  type: TYPE_NORMAL
