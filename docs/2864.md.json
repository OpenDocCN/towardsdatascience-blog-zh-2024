["```py\nclass PreProcessor(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom preprocessor for numeric features.\n\n    - Handles scaling of numeric data\n    - Performs imputation of missing values\n\n    Attributes:\n        transformer (Pipeline): Pipeline for numeric preprocessing\n        features (List[str]): Names of input features\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize preprocessor.\n\n        - Creates placeholder for transformer pipeline\n        \"\"\"\n        self.transformer = None\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fits the transformer on the provided dataset.\n\n        - Configures scaling for numeric features\n        - Sets up imputation for missing values\n        - Stores feature names for later use\n\n        Parameters:\n            X (pd.DataFrame): The input features to fit the transformer.\n            y (pd.Series, optional): Target variable, not used in this method.\n\n        Returns:\n            PreProcessor: The fitted transformer instance.\n        \"\"\"\n        self.features = X.columns.tolist()\n\n        if self.features:\n            self.transformer = Pipeline(steps=[\n                ('imputer', SimpleImputer(strategy='median')),\n                ('scaler', StandardScaler())\n            ])\n            self.transformer.fit(X[self.features])\n\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Transform input data using fitted pipeline.\n\n        - Applies scaling to numeric features\n        - Handles missing values through imputation\n\n        Parameters:\n            X (pd.DataFrame): Input features to transform\n\n        Returns:\n            pd.DataFrame: Transformed data with scaled and imputed features\n        \"\"\"\n        X_transformed = pd.DataFrame()\n\n        if self.features:\n            transformed_data = self.transformer.transform(X[self.features])\n            X_transformed[self.features] = transformed_data\n\n        X_transformed.index = X.index\n\n        return X_transformed\n\n    def fit_transform(self, X, y=None):\n        \"\"\"\n        Fits the transformer on the input data and then transforms it.\n\n        Parameters:\n            X (pd.DataFrame): The input features to fit and transform.\n            y (pd.Series, optional): Target variable, not used in this method.\n\n        Returns:\n            pd.DataFrame: The transformed data.\n        \"\"\"\n        self.fit(X, y)\n        return self.transform(X)\n```", "```py\n# Set parameters for synthetic data\nn_feature = 10\nn_inform = 4\nn_redundant = 0\nn_samples = 1000\n\n# Generate synthetic classification data\nX, y = make_classification(\n    n_samples=n_samples,\n    n_features=n_feature,\n    n_informative=n_inform,\n    n_redundant=n_redundant,\n    shuffle=False,\n    random_state=12\n)\n\n# Create feature names\nfeat_names = [f'inf_{i+1}' for i in range(n_inform)] + \\\n            [f'rand_{i+1}' for i in range(n_feature - n_inform)]\n\n# Convert to DataFrame with named features\nX = pd.DataFrame(X, columns=feat_names)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=22\n)\n```", "```py\nclass ML_PIPELINE(mlflow.pyfunc.PythonModel):\n    \"\"\"\n    Custom ML pipeline for classification and regression.\n\n    - work with any scikit-learn compatible model\n    - Combines preprocessing and model training\n    - Handles model predictions\n    - Compatible with MLflow tracking\n    - Supports MLflow deployment\n\n    Attributes:\n        model (BaseEstimator or None): A scikit-learn compatible model instance\n        preprocessor (Any or None): Data preprocessing pipeline\n        config (Any or None): Optional config for model settings \n        task(str): Type of ML task ('classification' or 'regression')\n    \"\"\"\n\n    def __init__(self, model=None, preprocessor=None, config=None):\n        \"\"\"\n        Initialize the ML_PIPELINE.\n\n        Parameters:\n            model (BaseEstimator, optional): \n                - Scikit-learn compatible model\n                - Defaults to None\n\n            preprocessor (Any, optional):\n                - Transformer or pipeline for data preprocessing\n                - Defaults to None\n\n            config (Any, optional):\n                - Additional model settings\n                - Defaults to None\n        \"\"\"\n        self.model = model\n        self.preprocessor = preprocessor\n        self.config = config\n        self.task = \"classification\" if hasattr(self.model, \"predict_proba\") else \"regression\"\n\n    def fit(self, X_train: pd.DataFrame, y_train: pd.Series):\n        \"\"\"\n        Train the model on provided data.\n\n        - Applies preprocessing to features\n        - Fits model on transformed data\n\n        Parameters:\n            X_train (pd.DataFrame): Training features\n            y_train (pd.Series): Target values\n        \"\"\"\n        X_train_preprocessed = self.preprocessor.fit_transform(X_train.copy())\n        self.model.fit(X_train_preprocessed, y_train)\n\n    def predict(\n            self, context: Any, model_input: pd.DataFrame\n            ) -> np.ndarray:\n        \"\"\"\n        Generate predictions using trained model.\n\n        - Applies preprocessing to new data\n        - Uses model to make predictions\n\n        Parameters:\n            context (Any): Optional context information provided \n                by MLflow during the prediction phase\n            model_input (pd.DataFrame): Input features\n\n        Returns:\n            Any: Model predictions or probabilities\n        \"\"\"\n        processed_model_input = self.preprocessor.transform(model_input.copy())\n        if self.task == \"classification\":\n            prediction = self.model.predict_proba(processed_model_input)[:,1]\n        elif self.task == \"regression\":\n            prediction = self.model.predict(processed_model_input)\n        return prediction\n```", "```py\n# define the ML pipeline instance with lightGBM classifier\nml_pipeline = ML_PIPELINE(model = lgb.LGBMClassifier(),\n                          preprocessor = PreProcessor())\n```", "```py\n# define the ML pipeline instance with random forest regressor\nml_pipeline = ML_PIPELINE(model = RandomForestRegressor(),\n                          preprocessor = PreProcessor())\n```", "```py\nparams = {\n    'n_estimators': 100,\n    'max_depth': 6,\n    'learning_rate': 0.1 \n}\nmodel = xgb.XGBClassifier(**params)\nml_pipeline = ML_PIPELINE(model = model,\n                          preprocessor = PreProcessor())\n```", "```py\n# train the ML pipeline\nml_pipeline.fit(X_train, y_train)\n\n# use the trained pipeline for prediction\ny_prob = ml_pipeline.predict(\n    context=None, # provide metadata for model in production\n    model_input=X_test\n)\nauc = roc_auc_score(y_test, y_prob)\nprint(f\"auc: {auc:.3f}\")\n```", "```py\n class PreProcessor_v2(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom transformer for data preprocessing.\n\n    - Scales numeric features\n    - Encodes categorical features\n    - Handles missing values via imputation\n    - Compatible with scikit-learn pipeline\n\n    Attributes:\n        num_impute_strategy (str): Numeric imputation strategy\n        cat_impute_strategy (str): Categorical imputation strategy\n        num_transformer (Pipeline): Numeric preprocessing pipeline\n        cat_transformer (Pipeline): Categorical preprocessing pipeline\n        transformed_cat_cols (List[str]): One-hot encoded column names\n        num_features (List[str]): Numeric feature names\n        cat_features (List[str]): Categorical feature names\n    \"\"\"\n\n    def __init__(self, num_impute_strategy='median', \n                 cat_impute_strategy='most_frequent'):\n        \"\"\"\n        Initialize the transformer.\n\n        - Sets up numeric data transformer\n        - Sets up categorical data transformer\n        - Configures imputation strategies\n\n        Parameters:\n            num_impute_strategy (str): Strategy for numeric missing values\n            cat_impute_strategy (str): Strategy for categorical missing values\n        \"\"\"\n        self.num_impute_strategy = num_impute_strategy\n        self.cat_impute_strategy = cat_impute_strategy\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit transformer on input data.\n\n        - Identifies feature types\n        - Configures feature scaling\n        - Sets up encoding\n        - Fits imputation strategies\n\n        Parameters:\n            X (pd.DataFrame): Input features\n            y (pd.Series, optional): Target variable, not used\n\n        Returns:\n            CustomTransformer: Fitted transformer\n        \"\"\"\n        self.num_features = X.select_dtypes(include=np.number).columns.tolist()\n        self.cat_features = X.select_dtypes(exclude=np.number).columns.tolist()\n\n        if self.num_features:\n            self.num_transformer = Pipeline(steps=[\n                ('imputer', SimpleImputer(strategy=self.num_impute_strategy)),\n                ('scaler', StandardScaler())\n            ])\n            self.num_transformer.fit(X[self.num_features])\n\n        if self.cat_features:\n            self.cat_transformer = Pipeline(steps=[\n                ('imputer', SimpleImputer(strategy=self.cat_impute_strategy)),\n                ('encoder', OneHotEncoder(handle_unknown='ignore'))\n            ])\n            self.cat_transformer.fit(X[self.cat_features])\n\n        return self\n\n    def get_transformed_cat_cols(self):\n        \"\"\"\n        Get transformed categorical column names.\n\n        - Creates names after one-hot encoding\n        - Combines category with encoded values\n\n        Returns:\n            List[str]: One-hot encoded column names\n        \"\"\"\n        cat_cols = []\n        cats = self.cat_features\n        cat_values = self.cat_transformer['encoder'].categories_\n        for cat, values in zip(cats, cat_values):\n            cat_cols += [f'{cat}_{value}' for value in values]\n\n        return cat_cols\n\n    def transform(self, X):\n        \"\"\"\n        Transform input data.\n\n        - Applies fitted scaling\n        - Applies fitted encoding\n        - Handles numeric and categorical features\n\n        Parameters:\n            X (pd.DataFrame): Input features\n\n        Returns:\n            pd.DataFrame: Transformed data\n        \"\"\"\n        X_transformed = pd.DataFrame()\n\n        if self.num_features:\n            transformed_num_data = self.num_transformer.transform(X[self.num_features])\n            X_transformed[self.num_features] = transformed_num_data\n\n        if self.cat_features:\n            transformed_cat_data = self.cat_transformer.transform(X[self.cat_features]).toarray()\n            self.transformed_cat_cols = self.get_transformed_cat_cols()\n            transformed_cat_df = pd.DataFrame(transformed_cat_data, columns=self.transformed_cat_cols)\n            X_transformed = pd.concat([X_transformed, transformed_cat_df], axis=1)\n\n        X_transformed.index = X.index\n\n        return X_transformed\n\n    def fit_transform(self, X, y=None):\n        \"\"\"\n        Fit and transform input data.\n\n        - Fits transformer to data\n        - Applies transformation\n        - Combines both operations\n\n        Parameters:\n            X (pd.DataFrame): Input features\n            y (pd.Series, optional): Target variable, not used\n\n        Returns:\n            pd.DataFrame: Transformed data\n        \"\"\"\n        self.fit(X, y)\n        return self.transform(X)\n```", "```py\n# Define a PreProcessor (V2) instance while specifying impute strategy\npreprocessor = PreProcessor_v2(\n    num_impute_strategy = 'mean'\n)\n# Define an ML Pipeline instance with this preprocessor\nml_pipeline = ML_PIPELINE(\n    model = xgb.XGBClassifier(), # switch ML algorithms\n    preprocessor = PreProcessor # switch pre-processors \n)\n```", "```py\n# add missings\nnp.random.seed(42) \nmissing_rate = 0.20 \nn_missing = int(np.floor(missing_rate * X.size))\nrows = np.random.randint(0, X.shape[0], n_missing)\ncols = np.random.randint(0, X.shape[1], n_missing)\nX.values[rows, cols] = np.nan\nactual_missing_rate = X.isna().sum().sum() / X.size\nprint(f\"Target missing rate: {missing_rate:.2%}\")\nprint(f\"Actual missing rate: {actual_missing_rate:.2%}\")\n\n# change X['inf_1] to categorical\npercentiles = [0, 0.1, 0.5, 0.9, 1]\nlabels = ['bottom', 'lower-mid', 'upper-mid', 'top']\nX['inf_1'] = pd.qcut(X['inf_1'], q=percentiles, labels=labels)\n```", "```py\n# create an ML pipeline instance with PreProcessor v1\nml_pipeline = ML_PIPELINE(\n    model = lgb.LGBMClassifier(verbose = -1), \n    preprocessor = PreProcessor()\n)\n\ntry:\n    ml_pipeline.fit(X_train, y_train)\nexcept Exception as e:\n    print(f\"Error: {e}\")\n```", "```py\nError: Cannot use median strategy with non-numeric data:\ncould not convert string to float: 'lower-mid'\n```", "```py\nshape = (n_samples, n_features) # 2d array\n```", "```py\nshape = (n_samples, n_features, n_classes) # 3d array\n```", "```py\nclass ML_PIPELINE(mlflow.pyfunc.PythonModel):\n    \"\"\"\n    Custom ML pipeline for classification and regression.\n\n    - Works with scikit-learn compatible models\n    - Handles data preprocessing\n    - Manages model training and predictions\n    - Provide global and local model explanation\n    - Compatible with MLflow tracking\n    - Supports MLflow deployment\n\n    Attributes:\n        model (BaseEstimator or None): A scikit-learn compatible model instance\n        preprocessor (Any or None): Data preprocessing pipeline\n        config (Any or None): Optional config for model settings \n        task(str): Type of ML task ('classification' or 'regression')\n        both_class (bool): Whether SHAP values include both classes\n        shap_values (shap.Explanation): SHAP values for model explanation\n        X_explain (pd.DataFrame): Processed features for SHAP explanation\n    \"\"\"\n\n    # ------- same code as above ---------\n\n    def explain_model(self,X):\n        \"\"\"\n        Generate SHAP values and plots for model interpretation. \n        This method:\n        1\\. Transforms the input data using the fitted preprocessor\n        2\\. Creates a SHAP explainer appropriate for the model type\n        3\\. Calculates SHAP values for feature importance\n        4\\. Generates a summary plot of feature importance\n\n        Parameters:\n            X : pd.DataFrame\n                Input features to generate explanations for. \n\n        Returns: None\n            The method stores the following attributes in the class:\n            - self.X_explain : pd.DataFrame\n                Transformed data with original numeric values for interpretation\n            - self.shap_values : shap.Explanation\n                SHAP values for each prediction\n            - self.both_class : bool\n                Whether the model outputs probabilities for both classes          \n        \"\"\"\n        X_transformed = self.preprocessor.transform(X.copy())\n        self.X_explain = X_transformed.copy()\n        # get pre-transformed values for numeric features\n        self.X_explain[self.preprocessor.num_features] = X[self.preprocessor.num_features]\n        self.X_explain.reset_index(drop=True)\n        try:\n            # Attempt to create an explainer that directly supports the model\n            explainer = shap.Explainer(self.model)\n        except:\n            # Fallback for models or shap versions where direct support may be limited\n            explainer = shap.Explainer(self.model.predict, X_transformed)\n        self.shap_values = explainer(X_transformed)  \n\n        # get the shape of shap values and extract accordingly\n        self.both_class = len(self.shap_values.values.shape) == 3\n        if self.both_class:\n            shap.summary_plot(self.shap_values[:,:,1])\n        elif self.both_class == False:\n            shap.summary_plot(self.shap_values)\n\n    def explain_case(self,n):\n        \"\"\"\n        Generate SHAP waterfall plot for one specific case.\n\n        - Shows feature contributions\n        - Starts from base value\n        - Ends at final prediction\n        - Shows original feature values for better interpretability\n\n        Parameters:\n            n (int): Case index (1-based)\n                     e.g., n=1 explains the first case.\n\n        Returns:\n            None: Displays SHAP waterfall plot\n\n        Notes:\n            - Requires explain_model() first\n            - Shows positive class for binary tasks\n        \"\"\"\n        if self.shap_values is None:\n            print(\"\"\"\n                  Please explain model first by running\n                  `explain_model()` using a selected dataset\n                  \"\"\")\n        else:\n            self.shap_values.data = self.X_explain\n            if self.both_class:\n                shap.plots.waterfall(self.shap_values[:,:,1][n-1])\n            elif self.both_class == False:\n                shap.plots.waterfall(self.shap_values[n-1]) \n```", "```py\n# Log the model with MLflow\nwith mlflow.start_run() as run:\n\n    # Log the custom model with auto-captured conda environment\n    model_info = mlflow.pyfunc.log_model(\n        artifact_path=\"model\",\n        python_model=ml_pipeline,\n        conda_env=mlflow.sklearn.get_default_conda_env()\n    )\n    # Log model parameters\n    mlflow.log_params(ml_pipeline.model.get_params())\n\n    # Log metrics\n    mlflow.log_metric(\"rmse\", rmse)\n\n    # Get the run ID\n    run_id = run.info.run_id\n```"]