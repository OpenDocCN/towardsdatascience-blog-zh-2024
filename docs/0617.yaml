- en: Building an AI Assistant with DSPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-an-ai-assistant-with-dspy-2e1e749a1a95?source=collection_archive---------0-----------------------#2024-03-07](https://towardsdatascience.com/building-an-ai-assistant-with-dspy-2e1e749a1a95?source=collection_archive---------0-----------------------#2024-03-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A way to program and tune prompt-agnostic LLM agent pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://lakshmanok.medium.com/?source=post_page---byline--2e1e749a1a95--------------------------------)[![Lak
    Lakshmanan](../Images/9faaaf72d600f592cbaf3e9089cbb913.png)](https://lakshmanok.medium.com/?source=post_page---byline--2e1e749a1a95--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2e1e749a1a95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2e1e749a1a95--------------------------------)
    [Lak Lakshmanan](https://lakshmanok.medium.com/?source=post_page---byline--2e1e749a1a95--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2e1e749a1a95--------------------------------)
    ·9 min read·Mar 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: I hate prompt engineering. For one thing, I do not want to prostrate before
    a LLM (“you are the world’s best copywriter … “), bribe it (“I will tip you $10
    if you …”), or nag it (“Make sure to …”). For another, prompts are brittle — small
    changes to prompts can cause major changes to the output. This makes it hard to
    develop repeatable functionality using LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, developing LLM-based applications today involves tuning and tweaking
    prompts. Moving from writing code in a programming language that the computer
    follows precisely to writing ambiguous natural language instructions that are
    imperfectly followed does not seem like progress. That’s why I found working with
    LLMs a frustrating exercise — I prefer writing and debugging computer programs
    that I can actually reason about.
  prefs: []
  type: TYPE_NORMAL
- en: What if, though, you can program on top of LLMs using a high-level programming
    framework, and let the framework write and tune prompts for you? Would be great,
    wouldn’t it? This — the ability to build agent pipelines programmatically without
    dealing with prompts and to tune these pipelines in a data-driven and LLM-agnostic
    way — is the key premise behind [DSPy](https://github.com/stanfordnlp/dspy).
  prefs: []
  type: TYPE_NORMAL
- en: An AI Assistant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To illustrate how DSPy works, I’ll build an AI assistant.
  prefs: []
  type: TYPE_NORMAL
- en: What’s an AI assistant? It’s a computer program that provides assistance to
    a human doing a task. The ideal AI assistant works *proactively* on behalf of
    the user (a chatbot can be a failsafe for functionality that is not easy to find
    in your product or a way end-users can reach out for customer support, but should
    not be the main/only AI assistance in your application). So, designing an AI assistant
    consists of thinking through a workflow and determining how you could want to
    streamline it using AI.
  prefs: []
  type: TYPE_NORMAL
- en: A typical AI assistant streamlines a workflow by (1) retrieving information
    such as company policies relevant to the task, (2) extracting information from
    documents such as those sent in by customers, (3) filling out forms or checklists
    based on textual analysis of the policies and documents, (4) collecting parameters
    and making function calls on the human’s behalf, and (5) identifying potential
    errors and highlighting risks.
  prefs: []
  type: TYPE_NORMAL
- en: The use case I will use to illustrate an AI assistant involves the card game
    bridge. Even though I’m building an AI assistant for bridge bidding, you don’t
    need to understand bridge to understand the concepts here. The reason I chose
    bridge is that there is a lot of jargon, quite a bit of human judgement involved,
    and several external tools that an advisor can use. These are the key characteristics
    of the industry problems and backoffice processes that you might want to build
    AI assistants for. But because it’s a game, there is no confidential information
    involved.
  prefs: []
  type: TYPE_NORMAL
- en: Agent Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The assistant, when asked a question like “What is Stayman?”, uses a number
    of backend services to carry out its task. These backend services are invoked
    via agents, which are themselves built using language models. As with microservices
    in software engineering, the use of agents and backend services allows for decoupling
    and specialization — the AI assistant does not need to know how things are done,
    only what it needs done and each agent can know how to do only its own thing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a96506f5a47510890f4aa8eae03392ed.png)'
  prefs: []
  type: TYPE_IMG
- en: An agent framework. Image by author. Sketches in the image were generated using
    Gemini.
  prefs: []
  type: TYPE_NORMAL
- en: In an agent framework, the agents can often be smaller language models (LMs)
    that need to be accurate, but don’t have world knowledge. The agents will be able
    to “reason” (through chain-of-thought), search (through Retrieval-Augmented-Generation),
    and do non-textual work (by extracting the parameters to pass into a backend function).
    Instead of having disparate capabilities or skills, the entire agent framework
    is fronted by an AI assistant that is an extremely fluent and coherent LLM. This
    LLM will know the intents it needs to handle and how to route those intents. It
    needs to have world knowledge as well. Often, there is a separate policy or guardrails
    LLM that acts as a filter. The AI assistant is invoked when the user makes a query
    (the chatbot use case) or when there is a triggering event (the proactive assistant
    use case).
  prefs: []
  type: TYPE_NORMAL
- en: Zero Shot prompting with DSPy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To build the whole architecture above, I’ll use DSPy. The [entire code is on
    GitHub](https://github.com/lakshmanok/lakblogs/tree/main/bridge_bidding_advisor);
    start with [bidding_advisor.py](https://github.com/lakshmanok/lakblogs/blob/main/bridge_bidding_advisor/bidding_advisor.py)
    in that directory and follow along.
  prefs: []
  type: TYPE_NORMAL
- en: 'In DSPy, the process of sending a prompt to an LLM and getting a response back
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There are four things happening in the snippet above:'
  prefs: []
  type: TYPE_NORMAL
- en: Write a subclass of dspy.Module
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the init method, set up a LM module. The simplest is dspy.Predict which is
    a single call.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Predict constructor takes a signature. Here, I say that there is one input
    (question) and one output (answer).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Write a forward() method that takes the input(s) specified (here: question)
    and returns the what was promised in the signature (here: answer). It does this
    by calling the dspy.Predict object created in the init method.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I could have just passed the question along as-is, but just to show you that
    I can somewhat affect the prompt, I added a bit of context.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the code above is completely LLM-agnostic, and there is no groveling,
    bribery, etc. in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'To call the above module, you first initialize dspy with an LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you invoke your module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When I did that, I got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Want to use a different LLM? Change the settings configuration lines to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Text Extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If all DSPy were doing was making it easier to call out to LLMs and abstract
    out the LLM, people wouldn’t be this excited about DSPy. Let’s continue to build
    out the AI assistant and tour some of the other advantages as we go along.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say that we want to use an LLM to do some entity extraction. We can do
    this by instructing the LLM to identify the thing we want to extract (date, product
    SKU, etc.). Here, we’ll ask it to find bridge jargon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: While we could have represented the signature of the module as “prompt -> terms”,
    we can also represent the signature as a Python class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calling this module on a statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note how concise and readable this is.
  prefs: []
  type: TYPE_NORMAL
- en: RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DSPy comes built-in with several retrievers. But these essentially just functions
    and you can wrap existing retrieval code into a dspy.Retriever. It supports several
    of the more popular ones, including ChromaDB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Of course, I had to get a document on bridge bidding, chunk it, and load it
    into ChromaDB. That code is in the repo if you are interested, but I’ll omit it
    as it’s not relevant to this article.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So now you have all the agents implemented, each as its own dspy.Module. Now,
    to build the orchestrator LLM, the one that receives the command or trigger and
    invokes the agent modules in some fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Orchestration of the modules also happens in a dspy.Module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Instead of using dspy.Predict for the final step, I’ve used a ChainOfThought
    (COT=3).
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the entire chain all set up, we can of course, simply call
    the orchestrator module to test it out. But more important, we can have dspy automatically
    tune the prompts for us based on example data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To load in these examples and ask dspy to tune it (this is called a teleprompter,
    but the name will be changed to Optimizer, a much more descriptive name for what
    it does), I do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: I used just 3 examples in the example above, but obviously, you’d use hundreds
    or thousands of examples to get a properly tuned set of prompts. Worth noting
    is that the tuning is done over the entire pipeline; you don’t have to mess around
    with the modules one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Is the optimized pipeline better?
  prefs: []
  type: TYPE_NORMAL
- en: 'While the original pipeline returned the following for this question (intermediate
    outputs are also shown, and Two spades is wrong):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The optimized pipeline returns the correct answer of “Smolen”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason is the prompt that dspy has created. For the question “What is Stayman?”,
    for example, note that it has built a rationale out of the term definitions, and
    several matches in the RAG:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2474b11fcd12ed06f0e50c8693631c41.png)'
  prefs: []
  type: TYPE_IMG
- en: Prompt created by dspy.ChainOfThought based on the term definitions, RAG, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Again, I didn’t write any of the tuned prompt above. It was all written for
    me. You can also see where this is headed in the future— you might be able to
    fine-tune the entire pipeline to run on a smaller LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Enjoy!
  prefs: []
  type: TYPE_NORMAL
- en: Next steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Look at my [code in GitHub](https://github.com/lakshmanok/lakblogs/tree/main/bridge_bidding_advisor),
    starting with [bidding_advisor.py](https://github.com/lakshmanok/lakblogs/blob/main/bridge_bidding_advisor/bidding_advisor.py).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Read more about DSPy here: [https://dspy-docs.vercel.app/docs/intro](https://dspy-docs.vercel.app/docs/intro)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Learn how to play bridge here: [https://www.trickybridge.com/](https://www.trickybridge.com/)
    (sorry, couldn’t resist).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
