- en: Why Retrieval-Augmented Generation Is Still Relevant in the Era of Long-Context
    Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么检索增强生成在长上下文语言模型时代依然重要
- en: 原文：[https://towardsdatascience.com/why-retrieval-augmented-generation-is-still-relevant-in-the-era-of-long-context-language-models-e36f509abac5?source=collection_archive---------5-----------------------#2024-12-12](https://towardsdatascience.com/why-retrieval-augmented-generation-is-still-relevant-in-the-era-of-long-context-language-models-e36f509abac5?source=collection_archive---------5-----------------------#2024-12-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-retrieval-augmented-generation-is-still-relevant-in-the-era-of-long-context-language-models-e36f509abac5?source=collection_archive---------5-----------------------#2024-12-12](https://towardsdatascience.com/why-retrieval-augmented-generation-is-still-relevant-in-the-era-of-long-context-language-models-e36f509abac5?source=collection_archive---------5-----------------------#2024-12-12)
- en: In this article we will explore why 128K tokens (and more) models can’t fully
    replace using RAG.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨为什么128K tokens（甚至更多）模型无法完全取代使用RAG。
- en: '[](https://medium.com/@jerome.o.diaz?source=post_page---byline--e36f509abac5--------------------------------)[![Jérôme
    DIAZ](../Images/da46d0d03587e88ec5bb56b7a997ec24.png)](https://medium.com/@jerome.o.diaz?source=post_page---byline--e36f509abac5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e36f509abac5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e36f509abac5--------------------------------)
    [Jérôme DIAZ](https://medium.com/@jerome.o.diaz?source=post_page---byline--e36f509abac5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jerome.o.diaz?source=post_page---byline--e36f509abac5--------------------------------)[![Jérôme
    DIAZ](../Images/da46d0d03587e88ec5bb56b7a997ec24.png)](https://medium.com/@jerome.o.diaz?source=post_page---byline--e36f509abac5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e36f509abac5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e36f509abac5--------------------------------)
    [Jérôme DIAZ](https://medium.com/@jerome.o.diaz?source=post_page---byline--e36f509abac5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e36f509abac5--------------------------------)
    ·7 min read·Dec 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e36f509abac5--------------------------------)
    ·阅读时间7分钟·2024年12月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: We’ll start with a brief reminder of the problems that can be solved with RAG,
    before looking at the improvements in LLMs and their impact on the **need** to
    use RAG.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从简要回顾RAG可以解决的问题开始，然后再讨论LLM的改进及其对**使用**RAG需求的影响。
- en: '![](../Images/e2ff22b6f4942641251ccd83b8a2446c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2ff22b6f4942641251ccd83b8a2446c.png)'
- en: Illustration by the author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 插图由作者提供。
- en: Let’s start by a bit of history
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们从一点历史背景开始
- en: RAG isn’t really new
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG其实并不是什么新鲜事物
- en: The idea of injecting a context to let a language model get access to up-to-date
    data is quite “old” (at the LLM level). It was first introduced by Facebook AI/Meta
    researcher in this 2020 paper “[Retrieval-Augmented Generation for Knowledge-Intensive
    NLP Tasks](https://arxiv.org/abs/2005.11401)”. In comparison the first version
    of ChatGPT was only released on November 2022.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 向语言模型注入上下文，以让其获取最新数据的想法在LLM层面上其实并不“新”。这一概念最早由Facebook AI/Meta的研究员在2020年的论文《[面向知识密集型NLP任务的检索增强生成](https://arxiv.org/abs/2005.11401)》中提出。相比之下，ChatGPT的第一个版本直到2022年11月才发布。
- en: 'In this paper they distinguish two kind of memory:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，他们区分了两种类型的记忆：
- en: the *parametric* one, which is what is inherent to the LLM, **what it learned**
    while being fed lot and lot of texts during training,
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*参数化*的记忆，指的是LLM固有的记忆，**它所学到的**内容，在训练期间通过大量文本的喂入获得。'
- en: the *non-parametric* one, which is the memory you can provide by *feeding a
    context to the prompt*.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*非参数化*的记忆，指的是你通过*向提示提供上下文*来实现的记忆。'
