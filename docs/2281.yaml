- en: 'Uncertainty in Markov Decisions Processes: a Robust Linear Programming approach'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/uncertainty-in-markov-decisions-processes-a-robust-linear-programming-approach-b01e6e26e463?source=collection_archive---------2-----------------------#2024-09-18](https://towardsdatascience.com/uncertainty-in-markov-decisions-processes-a-robust-linear-programming-approach-b01e6e26e463?source=collection_archive---------2-----------------------#2024-09-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Theoretical derivation of the Robust Counterpart of Markov Decision Processes
    (MDPs) as a Linear Program (LP)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@h.fellahi?source=post_page---byline--b01e6e26e463--------------------------------)[![Hussein
    Fellahi](../Images/b49c8620d8a490ab078b5d4dfe8d017a.png)](https://medium.com/@h.fellahi?source=post_page---byline--b01e6e26e463--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b01e6e26e463--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b01e6e26e463--------------------------------)
    [Hussein Fellahi](https://medium.com/@h.fellahi?source=post_page---byline--b01e6e26e463--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b01e6e26e463--------------------------------)
    ·8 min read·Sep 18, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ca68bd8a46abe36f974ccfa485f67ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [ZHENYU LUO](https://unsplash.com/@mrnuclear?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Markov Decision Processes are foundational to sequential decision-making problems
    and serve as the building block for reinforcement learning. They model the dynamic
    interaction between an agent having to make a series of actions and their environment.
    Due to their wide applicability in fields such as robotics, finance, operations
    research and AI, MDPs have been extensively studied in both theoretical and practical
    contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet, much of the existing MDP literature focuses on the idealized scenarios
    where model parameters — such as transition probabilities and reward functions
    — are assumed to be known with certainty. In practice, applying popular methods
    such as Policy Iteration and Value Iteration require precise estimates of these
    parameters, often obtained from real-world data. This reliance on data introduces
    significant challenges: the estimation process is inherently noisy and sensitive
    to limitations such as data scarcity, measurement errors and variability in the
    observed environment. Consequently, the performance of standard MDP methods can
    degrade substantially when applied to problems with uncertain or incomplete data.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we build on the Robust Optimization (RO) literature to propose
    a generic framework to address these issues. **We provide a Robust Linear Programming
    (RLP) formulation of MDPs that is capable of handling various sources of uncertainty
    and adversarial perturbations.**
  prefs: []
  type: TYPE_NORMAL
- en: MDP definition and LP formulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s start by giving a formal definition of MDPs:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A* ***Markov Decision Process*** *is a 5-tuple (S, A, R, P, γ) such that:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*S is the set of* ***states*** *the agent can be in*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A is the set of* ***actions*** *the agent can take*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R : S* x *A →* R *the* ***reward*** *function*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P is the set of* ***probability distributions*** *defined such that P(s’|s,a)
    is the probability of transitioning to state* ***s’*** *if the agent takes action*
    ***a*** *in state* ***s****. Note that MDPs are Markov processes, meaning that
    the Markov property holds on the transition probabilities*: P(Sₜ₊₁|S₀, A₀, …,
    Sₜ, Aₜ) = P(Sₜ₊₁|Sₜ, Aₜ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*γ ∈ (0, 1] is a* ***discount factor****. While we usually deal with discounted
    problems (i.e. γ < 1), the formulations presented are also valid for undiscounted
    MDPs (γ = 1)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We then define the **policy**, i.e. what dictates the agent’s behavior in an
    MDP:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A policy π is a* ***probability measure*** *over the action space defined
    as: π(a|s) is the probability of taking action* ***a*** *when the agent is in
    state* ***s****.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We finally introduce the **value function**, i.e. the agent’s objective in
    an MDP:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The value function of a policy π is the expected discounted reward under this
    policy, when starting at a given state* ***s****:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d82dc5a1a2e1a0b0d5228793fd75781.png)'
  prefs: []
  type: TYPE_IMG
- en: '*In particular, the value function of the optimal policy π* satisfies the Bellman
    optimality equation:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7daa90e2dd4bf9c84f0d01a1d142ddc2.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Which yields the deterministic optimal policy:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25de5914d7227557fccc869698b8a51c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Deriving the LP formulation of MDPs:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the above definitions, we can start by noticing that any value function
    V that satisfies
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/853b1072047df6981994647e5fdb183f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'is an upper bound on the optimal value function. To see it, we can start by
    noticing that such value function also satisfies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ead9064be99992fb736efcd2fb7162c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We recognize the value iteration operator applied to V:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/addbe4760a5cbd1d16e641580e000de3.png)'
  prefs: []
  type: TYPE_IMG
- en: i.e.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/529b5be7c352fdaac409965f0a558859.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Also noticing that the H*operator is increasing, we can apply it iteratively
    to have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1eb2f85b3fef8f0a3946982532d2528c.png)'
  prefs: []
  type: TYPE_IMG
- en: where we used the property of V* being the fixed point of H*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, finding V* comes down to finding the **tightest upper bound V that
    obeys the above equation**, which yields the following formulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd511b37256a48510693d9294a7427cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here we added a weight term corresponding to the probability of starting in
    state *s*. We can see that the above problem is linear in V and can be rewritten
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a95c4fe11f870f446a9cf2a04b8d975e.png)'
  prefs: []
  type: TYPE_IMG
- en: Further details can be found in [1] and [2].
  prefs: []
  type: TYPE_NORMAL
- en: Robust Optimization for Linear Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given the above linear program in standard form, the RO framework assumes an
    adversarial noise in the inputs (i.e. cost vector and constraints). To model this
    uncertainty, we define an **uncertainty set**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8632027096c26bd218ef09dce04263fd.png)'
  prefs: []
  type: TYPE_IMG
- en: In short, we want to find the minimum of all linear programs, i.e. for each
    occurrence in the uncertainty set. Naturally this yields a completely intractable
    model (potentially an infinite number of LPs) since we did not make any assumption
    on the form of *U*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before addressing these issues, we make the following assumptions — without
    loss of generality:'
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty in *w* and *b* is equivalent to uncertainty in the constraints for
    a slightly modified LP — for this reason we consider uncertainty only in *c*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial noise is applied constraint-wise, i.e. to each constraint individually
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The constraint of the robust problem are in the form:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/c3789208dc20154982543af2d706aa66.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where: \bar{c} is known as the *nominal constraint vector* (e.g. gotten from
    some estimation), *z* the uncertain factor and Q a fixed matrix intuitively corresponding
    to how the noise is applied to each coefficient of the constraint vector. *Q*
    can be used for instance to model correlation between the noise on difference
    components of *c*. See [3] for more details and proofs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: we made a slight abuse of notation and dropped the *(s, a)* subscripts
    for readability — yet *c, \bar{c}, Q* and *z* are all for a given state and action
    couple.'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than optimizing for each entry of the uncertainty set, **we optimize
    the worst case over *U*.** In the context of uncertainty on the constraints only,
    this mean **that the worst case over *U* must also be feasible**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All this leads to the following formulation of the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3909cb7130218128c2279000361bd40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this stage, we can make some assumptions on the form of *U* in order to
    further simplify the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: While *z* can be a vector of arbitrary dimension *L* — as *Q* will be a *|S|*
    x *L* matrix — we make the simplifying assumption that *z* is of size *|S|* and
    *Q* is a square diagonal matrix of size *|S|* as well. This will allow to model
    separately an adversarial noise on each coefficient on the constraint vector (and
    no correlation between noises)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We assume that the uncertainty set is a box of size *2d,* i.e. that each coordinate
    of *z* can take any value from the interval *[-d, d]*. This is equivalent to saying
    that the *L∞* norm of *z* is less than *d*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The optimization problem becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/419cb06e77e1678dee33007b6e287c4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'which is equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92e0dc89bd906107cf3daa4c57035b35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, looking closer to the maximization problem in the constraint, we see
    that it has a closed form. Therefore the final problem can be written as (**robust
    counterpart of a linear program with box uncertainty**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a024026d6db5e420aae02229ede1a5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A few comments on the above formulation:'
  prefs: []
  type: TYPE_NORMAL
- en: The uncertainty term disappeared — robustness is brought by an additional safety
    term
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the L1 norm can be linearized, this is a linear program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The above formulation does not depend on the form of *Q* — the assumption made
    will be useful in the next section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more details, the interested reader can refer to [3].
  prefs: []
  type: TYPE_NORMAL
- en: The RLP formulation of MDPs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Starting from the above formulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93c5fcdf66e345016b93c434216c00a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And finally, linearizing the absolute value in the constraints gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f21ead78a1601595332037d985dfdce.png)'
  prefs: []
  type: TYPE_IMG
- en: We notice that robustness translates into an additional **safety term** in the
    constraints — given the uncertainty on *c* (which mainly translates into uncertainty
    in the MDP’s transition probabilities).
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated previously, considering uncertainty on the rewards as well can be
    easily done with a similar derivation. Coming back to the linear program in standard
    form, we add another noise term on the right hand side of the constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1170d90147dfdaf5d285ec368f6caad5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After a similar reasoning as previously done, we have the all-in linear program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/304f023eec33f211753050b9b290e5dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Again similarly to before, additional robustness with regards to the reward
    function translates into another safety term in the constraints, which ultimately
    can yield a less optimal value function and policy but fills the constraints with
    margin. This tradeoff is controlled both by *Q* and the size of the uncertainty
    box *d*.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While this completes the derivation of the Robust MDP as a linear program, other
    robust MDP approaches have been developed in the literature, see for instance
    [4]. These approaches usually take a different route, for instance directly deriving
    a robust Policy Evaluation operator — which for instance has the advantage of
    having a better complexity as the LP approach. This proves particularly important
    when state and action spaces are large. Therefore why would we use such formulation?
  prefs: []
  type: TYPE_NORMAL
- en: The RLP formulation allows to benefit from all theoretical properties of linear
    programming. This entails guarantees of a solution (when the problem is feasible
    and bounded), as well as known results from duality theory and sensitivity analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LP approach allows to easily use different geometries of uncertainty sets
    — see [3] for details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This formulation allows to integrate naturally additional constraints on the
    MDP, while keeping the robustness properties
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can furthermore apply some projection or approximation methods (see for instance
    [5]) to improve the LP complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] M.L. Puterman, [Markov Decision Processes: Discrete Stochastic Dynamic
    Programming](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887) (1996),
    Wiley'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] P. Pouppart, Sequential Decision Making and Reinforcement Learning (2013),
    University of Waterloo'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] D. Bertsimas, D. Den Hertog, [Robust and Adaptive Optimization](https://www.dynamic-ideas.com/books/robust-and-adaptive-optimization)
    (2022), Dynamic Ideas'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] W. Wiesemann, D. Kuhn, B. Rustem, [Robust Markov Decision Processes](https://pubsonline.informs.org/doi/abs/10.1287/moor.1120.0566?journalCode=moor)
    (2013), INFORMS'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] K. Vu, P.-L. Poirion, L. Liberti, [Random Projections for Linear Programming](https://pubsonline.informs.org/doi/abs/10.1287/moor.2017.0894)
    (2018), INFORMS'
  prefs: []
  type: TYPE_NORMAL
