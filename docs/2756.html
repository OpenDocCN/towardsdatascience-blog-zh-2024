<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>NER in Czech Documents with XLM-RoBERTa using 🤗 Accelerate</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>NER in Czech Documents with XLM-RoBERTa using 🤗 Accelerate</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ner-in-czech-documents-with-xlm-roberta-using-accelerate-32a6baf3e91e?source=collection_archive---------9-----------------------#2024-11-12">https://towardsdatascience.com/ner-in-czech-documents-with-xlm-roberta-using-accelerate-32a6baf3e91e?source=collection_archive---------9-----------------------#2024-11-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f864" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><em class="hd">Decisions I made during the development of a document processing model that was successfully deployed</em></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@bbuso?source=post_page---byline--32a6baf3e91e--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Bohumir Buso" class="l ep by dd de cx" src="../Images/751ba491a8f5aca31add3dbc850841c5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*6nVQ0Dv7gU-LFQsa3DKTwQ.jpeg"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--32a6baf3e91e--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@bbuso?source=post_page---byline--32a6baf3e91e--------------------------------" rel="noopener follow">Bohumir Buso</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--32a6baf3e91e--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk lb lc ab q ee ld le" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lf"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap ie li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/874a04ed0f94f4a2b3e255bd98e3ae03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bxkP58D5uP1CtpHlXDfnXQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image generated by Dall-E</figcaption></figure><p id="6fb4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Although I have over 8 years of experience with ML projects, this was my first NLP project. I initially searched for existing resources and code but found limited material, particularly for NER in Czech-language documents. This inspired me to compile everything I learned during development into one place, hoping to help future newcomers progress more efficiently. As such,<strong class="ne fr"> this article offers a practical introduction rather than an in-depth theoretical analysis</strong>.</p><p id="cca0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Specific numerical results are omitted due to sensitivity considerations.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9020" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Data</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pc"><img src="../Images/36c928f19c6a3344476a2804e0a82cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*Kx4el3PvbDKfOTPgUAd-fQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Example of document with entities: <em class="hd">variable symbol of creditor</em> (red), surname (light green), given name (dark green), date of birth (blue). Sensitive information is blacked out.</figcaption></figure><p id="2c0f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Task<br/></strong>The <strong class="ne fr">main objective was to identify the client(s) associated with each document</strong> through one of the following identifiers:</p><ul class=""><li id="38da" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pd pe pf bk"><em class="pg">variable symbol of creditor</em> (present in about 20% of documents)</li><li id="9b04" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pd pe pf bk"><em class="pg">birth ID</em> (present in about 60% of documents)</li><li id="4b37" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pd pe pf bk">combination<em class="pg"> name</em> + <em class="pg">surname </em>+ <em class="pg">birth date</em> (present in about 50% of documents)</li></ul><p id="060d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Approximately 5% of the documents contained no identifying entities.</p><p id="a906" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Dataset<br/></strong>For development, I used 710 “true” PDF documents, dividing them into three sets: 600 for training, 55 for validation, and 55 for testing.</p><p id="14a8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Labels<br/></strong>I received an Excel file with entities extracted as plain text, requiring manual labeling of the document text. Using the <a class="af pm" href="https://natural-language-understanding.fandom.com/wiki/Named_entity_recognition#BIO" rel="noopener ugc nofollow" target="_blank">BIO</a> tagging format, I followed these steps:</p><ol class=""><li id="f2ba" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pn pe pf bk">Open each document (using <code class="cx po pp pq pr b">extract_text()</code> function from the <code class="cx po pp pq pr b">pdfminer.high_level</code> module)</li><li id="b2ed" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pn pe pf bk">Split text into words (using the SpaCy model “xx_sent_ud_sm” with adjustments, such as preventing splits on hyphens to handle birth number formats, e.g., ‘84–12–10/7869’)</li><li id="130d" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pn pe pf bk">Identify entities within the text</li><li id="7f38" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pn pe pf bk">Assign corresponding labels to entities, using the “O” label for all other words</li></ol><p id="f594" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Alternative Approach<br/></strong>Models like LayoutLM, which also consider bounding boxes for input tokens, might improve quality. However, I avoided this option since, as usual (😮‍💨), <strong class="ne fr">I had already spent most of the project time on data preparation </strong>(e.g., reformatting Excel files, correcting data errors, labeling). Pursuing bounding box-based models would have required even more time.</p><p id="57ca" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While regex and heuristics could theoretically work for simple entities like these, I believe this approach would be ineffective, as it would require overly complex rules to accurately identify the correct ones amidst other potential candidates (lawyer name, case number, other participants in the proceedings, etc.). The model, on the other hand, is capable of learning to distinguish the relevant entities, making the use of heuristics unnecessary.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="fea0" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Model (training)</h1><p id="4cea" class="pw-post-body-paragraph nc nd fq ne b go ps ng nh gr pt nj nk nl pu nn no np pv nr ns nt pw nv nw nx fj bk"><strong class="ne fr">🤗 Accelerate</strong><br/>Having started in a time when wrappers were less common, <strong class="ne fr">I became accustomed to writing my own training loops, which I find easier to debug -</strong> <strong class="ne fr">an approach that 🤗 Accelerate supports effectively</strong>. It proved beneficial in this project - I wasn’t entirely certain of the required data and label formats or shapes and my data didn’t match the well-organized examples often shown in tutorials, but having full access to intermediate computations during the training loop allowed me to iterate quickly.</p><p id="fbc9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Context Length<br/></strong>Most tutorials suggest using each sentence as a single training example. However, in this case, I decided <strong class="ne fr">a longer context would be more suitable as documents typically contain references to multiple entities</strong>, many of which are irrelevant (e.g. lawyers, other creditors, case numbers). This broader context enables the model to better identify the relevant client. I used 512 tokens from each document as one training example. This is a common maximum limit for models but comfortably accommodates all entities in most of my documents.</p><p id="fbd7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Labelling of Subtokens<br/></strong>In the 🤗 token classification tutorial [1], recommended approach is:</p><blockquote class="px py pz"><p id="e0df" class="nc nd pg ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Only labeling the first token of a given word. Assign <code class="cx po pp pq pr b"><em class="fq">-100</em></code> to other subtokens from the same word.</p></blockquote><p id="ec29" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, I found that the following method suggested in the token classification tutorial in their NLP course [2] works much better:</p><blockquote class="px py pz"><p id="070e" class="nc nd pg ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Each token gets the same label as the token that started the word it’s inside, since they are part of the same entity. For tokens inside a word but not at the beginning, we replace the <code class="cx po pp pq pr b"><em class="fq">B-</em></code> with <code class="cx po pp pq pr b"><em class="fq">I-</em></code></p></blockquote><p id="fa55" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Label “-100” is special label that is ignored<strong class="ne fr"> </strong>by loss function. Hence, I used their functions with minor changes:</p><pre class="mm mn mo mp mq qa pr qb bp qc bb bk"><span id="1264" class="qd oh fq pr b bg qe qf l qg qh">def align_labels_with_tokens(labels, word_ids):<br/>    new_labels = []<br/>    current_word = None<br/>    for word_id in word_ids:<br/>        if word_id != current_word:<br/>            # Start of a new word!<br/>            current_word = word_id<br/>            label = -100 if word_id is None else labels[word_id]<br/>            new_labels.append(label)<br/>        elif word_id is None:<br/>            # Special token<br/>            new_labels.append(-100)<br/>        else:<br/>            # Same word as previous token<br/>            label = labels[word_id]<br/>            # If the label is B-XXX we change it to I-XXX<br/>            if label % 2 == 1:<br/>                label += 1<br/>            new_labels.append(label)<br/><br/>    return new_labels<br/><br/><br/>def tokenize_and_align_labels(examples):<br/>    tokenizer = AutoTokenizer.from_pretrained("../model/xlm-roberta-large")<br/>    tokenized_inputs = tokenizer(<br/>        examples["tokens"], truncation=True, is_split_into_words=True,<br/>        padding="max_length", max_length=512)<br/>    all_labels = examples["ner_tags"]<br/>    new_labels = []<br/>    for i, labels in enumerate(all_labels):<br/>        word_ids = tokenized_inputs.word_ids(i)<br/>        new_labels.append(align_labels_with_tokens(labels, word_ids))<br/><br/>    tokenized_inputs["labels"] = new_labels<br/>    return tokenized_inputs</span></pre><p id="b08f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I also used their <code class="cx po pp pq pr b">postprocess()</code>function:</p><blockquote class="px py pz"><p id="3ecb" class="nc nd pg ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To simplify its evaluation part, we define this <code class="cx po pp pq pr b"><em class="fq">postprocess()</em></code> function that takes predictions and labels and converts them to lists of strings.</p></blockquote><pre class="mm mn mo mp mq qa pr qb bp qc bb bk"><span id="04eb" class="qd oh fq pr b bg qe qf l qg qh">def postprocess(predictions, labels):<br/>    predictions = predictions.detach().cpu().clone().numpy()<br/>    labels = labels.detach().cpu().clone().numpy()<br/><br/>    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]<br/>    true_predictions = [<br/>        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]<br/>        for prediction, label in zip(predictions, labels)<br/>    ]<br/>    return true_predictions, true_labels</span></pre><p id="1460" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Class Weights<br/>Incorporating class weights into the loss function significantly improved model performance.</strong> While this adjustment may seem straightforward — without it, the model overemphasized the majority “O” class — it’s surprisingly absent from most tutorials. I implemented a custom <code class="cx po pp pq pr b">compute_weights()</code> function to address this imbalance:</p><pre class="mm mn mo mp mq qa pr qb bp qc bb bk"><span id="de3a" class="qd oh fq pr b bg qe qf l qg qh">def compute_weights(trainset, num_labels):<br/>    c = Counter()<br/>    for t in trainset:<br/>        c += Counter(t['labels'].tolist())<br/>    weights = [sum(c.values())/(c[i]+1) for i in range(num_labels)]<br/>    return weights</span></pre><p id="243a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Training Loop<br/></strong>I defined two additional functions: PyTorch <code class="cx po pp pq pr b">DataLoader()</code> to manage batch processing, and a <code class="cx po pp pq pr b">main()</code> function to set up distributed training objects and execute the training loop.</p><pre class="mm mn mo mp mq qa pr qb bp qc bb bk"><span id="f3d2" class="qd oh fq pr b bg qe qf l qg qh">from accelerate import Accelerator, notebook_launcher<br/>from collections import Counter<br/>from datasets import Dataset<br/>from datetime import datetime<br/>import torch<br/>from torch.optim.lr_scheduler import ReduceLROnPlateau<br/>from torch.nn import CrossEntropyLoss<br/>from torch.utils.data import DataLoader<br/>from transformers import AutoTokenizer<br/>from transformers import AutoModelForTokenClassification<br/>from transformers import XLMRobertaConfig, XLMRobertaForTokenClassification<br/>from seqeval.metrics import classification_report, f1_score<br/><br/>def create_dataloaders(trainset, evalset, batch_size, num_workers):<br/>    train_dataloader = DataLoader(trainset, shuffle=True, <br/>                          batch_size=batch_size, num_workers=num_workers)<br/>    eval_dataloader = DataLoader(evalset, shuffle=False, <br/>                          batch_size=batch_size, num_workers=num_workers)<br/>    return train_dataloader, eval_dataloader<br/><br/>def main(batch_size, num_workers, epochs, model_path, dataset_tr, dataset_ev, training_type, model_params, dt):<br/>    accelerator = Accelerator(split_batches=True)<br/>    num_labels = model_params['num_labels']<br/><br/>    # Prepare data #<br/>    train_ds = Dataset.from_dict(<br/>                {"tokens": [d[2][:512] for d in dataset_tr], <br/>                 "ner_tags": [d[1][:512] for d in dataset_tr]})<br/>    eval_ds = Dataset.from_dict(<br/>                {"tokens": [d[2][:512] for d in dataset_ev],<br/>                 "ner_tags": [d[1][:512] for d in dataset_ev]})<br/>    trainset = train_ds.map(tokenize_and_align_labels, batched=True,<br/>                 remove_columns=["tokens", "ner_tags"])<br/>    evalset = eval_ds.map(tokenize_and_align_labels, batched=True,<br/>                 remove_columns=["tokens", "ner_tags"])<br/>    trainset.set_format("torch")<br/>    evalset.set_format("torch")<br/>    train_dataloader, eval_dataloader = create_dataloaders(trainset, evalset,<br/>                                          batch_size, num_workers)<br/><br/>    # Type of training #<br/>    if training_type=='from_scratch':<br/>        config = XLMRobertaConfig.from_pretrained(model_path, **model_params)<br/>        model = XLMRobertaForTokenClassification(config)<br/>    elif training_type=='transfer_learning':<br/>        model = AutoModelForTokenClassification.from_pretrained(model_path, <br/>                    ignore_mismatched_sizes=True, **model_params)<br/>        for param in model.parameters():<br/>            param.requires_grad=False<br/>        for param in model.classifier.parameters():<br/>            param.requires_grad=True<br/>    elif training_type=='fine_tuning':<br/>        model = AutoModelForTokenClassification.from_pretrained(model_path,<br/>                     **model_params)<br/>        for param in model.parameters():<br/>            param.requires_grad=True<br/>        for param in model.classifier.parameters():<br/>            param.requires_grad=True<br/><br/>    # Intantiate the optimizer #<br/>    optimizer = torch.optim.AdamW(params=model.parameters(), lr=2e-5)<br/><br/>    # Instantiate the learning rate scheduler #<br/>    lr_scheduler = ReduceLROnPlateau(optimizer, patience=5)<br/><br/>    # Define loss function #<br/>    weights = compute_weights(trainset, num_labels)<br/>    loss_fct = CrossEntropyLoss(weight=torch.tensor(weights))<br/><br/>    # Prepare objects for distributed training #<br/>    loss_fct, train_dataloader, model, optimizer, eval_dataloader, lr_scheduler = accelerator.prepare(<br/>        loss_fct, train_dataloader, model, optimizer, eval_dataloader, lr_scheduler)<br/><br/>    # Training loop #<br/>    max_f1 = 0 # for early stopping<br/>    for t in range(epochs):<br/>        # training<br/>        accelerator.print(f"\n\nEpoch {t+1}\n-------------------------------")<br/>        model.train()<br/>        tr_loss = 0<br/>        preds = list()<br/>        labs = list()<br/>        for batch in train_dataloader:<br/>            outputs = model(input_ids=batch['input_ids'],<br/>                            attention_mask=batch['attention_mask'])<br/>            labels = batch["labels"]<br/>            loss = loss_fct(outputs.logits.view(-1, num_labels), labels.view(-1))<br/>            accelerator.backward(loss)<br/>            optimizer.step()<br/>            optimizer.zero_grad()<br/>            tr_loss += loss<br/>            predictions = outputs.logits.argmax(dim=-1)<br/>            predictions_gathered = accelerator.gather(predictions)<br/>            labels_gathered = accelerator.gather(labels)<br/>            true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)<br/>            preds.extend(true_predictions)<br/>            labs.extend(true_labels)<br/><br/>        lr_scheduler.step(tr_loss)<br/><br/>        accelerator.print(f"Train loss: {tr_loss/len(train_dataloader):&gt;8f} \n")<br/>        accelerator.print(classification_report(labs, preds))<br/><br/>        # evaluation<br/>        model.eval()<br/>        ev_loss = 0<br/>        preds = list()<br/>        labs = list()<br/>        for batch in eval_dataloader:<br/>            with torch.no_grad():<br/>                outputs = model(input_ids=batch['input_ids'],<br/>                                attention_mask=batch['attention_mask'])<br/>                labels = batch["labels"]<br/>                loss = loss_fct(outputs.logits.view(-1, num_labels), labels.view(-1))<br/><br/>            ev_loss += loss<br/>            predictions = outputs.logits.argmax(dim=-1)<br/>            predictions_gathered = accelerator.gather(predictions)<br/>            labels_gathered = accelerator.gather(labels)<br/>            true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)<br/>            preds.extend(true_predictions)<br/>            labs.extend(true_labels)<br/><br/>        accelerator.print(f"Eval loss: {ev_loss/len(eval_dataloader):&gt;8f} \n")<br/>        accelerator.print(classification_report(labs, preds))<br/><br/>        accelerator.print(f"Current Learning Rate: {optimizer.param_groups[0]['lr']}")<br/><br/>        # checkpoint best model<br/>        if f1_score(labs, preds) &gt; max_f1:<br/>            accelerator.wait_for_everyone()<br/>            unwrapped_model = accelerator.unwrap_model(model)<br/>            unwrapped_model.save_pretrained(f"../model/xlml_ner/{dt}/",<br/>                               is_main_process=accelerator.is_main_process,<br/>                               save_function=accelerator.save)<br/>            accelerator.print(f"Model saved during {t+1}. epoch.")<br/>            max_f1 = f1_score(labs, preds)<br/>            best_epoch = t<br/><br/>        # early stopping<br/>        if (t - best_epoch) &gt; 10:<br/>            accelerator.print(f"Early stopping after {t+1}. epoch.")<br/>            break<br/><br/>    accelerator.print("Done!")<br/></span></pre><p id="6bd4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With everything prepared, the model is ready for training. I just need to initiate the process:</p><pre class="mm mn mo mp mq qa pr qb bp qc bb bk"><span id="97f2" class="qd oh fq pr b bg qe qf l qg qh">label_list = [<br/>    "O",<br/>    "B-evcu", "I-evcu", # variable symbol of creditor<br/>    "B-rc", "I-rc", # birth ID<br/>    "B-prijmeni", "I-prijmeni", # surname<br/>    "B-jmeno", "I-jmeno", # given name<br/>    "B-datum", "I-datum", # birth date<br/>]<br/>id2label = {a: b for a,b in enumerate(label_list)}<br/>label2id = {b: a for a,b in enumerate(label_list)}<br/><br/>num_workers = 6 # number of GPUs<br/>batch_size = num_workers*2<br/>epochs = 100<br/>model_path = "../model/xlm-roberta-large"<br/>training_type = "fine_tuning" # from_scratch / transfer_learning / fine_tuning<br/>model_params = {"id2label": id2label, "label2id": label2id, "num_labels": 11}<br/>dt = datetime.now().strftime("%Y%m%d_%H%M%S")<br/>os.mkdir(f"../model/xlml_ner/{dt}")<br/><br/>notebook_launcher(main, args=(batch_size, num_workers, epochs, model_path,<br/>                   dataset_tr, dataset_ev, training_type, model_params, dt),<br/>                   num_processes=num_workers, mixed_precision="fp16", use_port="29502")</span></pre><p id="be96" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">I find using</strong> <code class="cx po pp pq pr b">notebook_launcher()</code> <strong class="ne fr">convenient, as it allows me to run training in the console and easily work with results afterward.</strong></p><p id="1b76" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">XLM-RoBERTa base vs large vs Small-E-Czech<br/></strong>I experimented with fine-tuning three models. The XLM-RoBERTa base model [3] delivered satisfactory performance, but the server capacity also allowed me to try the XLM-RoBERTa large model [3], which has twice the parameters.</p><blockquote class="px py pz"><p id="2d88" class="nc nd pg ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.</p></blockquote><p id="f985" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The large model showed a slight improvement in results, so I ultimately deployed it. I also tested Small-E-Czech [4], an Electra-small model pre-trained on Czech web data, but its performance was poor.</p><p id="5183" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Fine-tuning vs Transfer learning vs Training from scratch<br/></strong>In addition to fine-tuning (updating all model weights), I tested transfer learning, as it is sometimes suggested that training only the final (classification) layer may suffice.. However, the performance difference was significant, favoring full fine-tuning. I also attempted training from scratch by importing only architecture of the model, initializing the weights randomly, and then training, but as expected, this approach was ineffective.</p><p id="685d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">RoBERTa vs LLM (Claude 3.5 Sonnet)<br/></strong>I briefly explored zero-shot LLMs, though with minimal prompt engineering (so 🥱). The model struggled even with basic requests, such as (I used Czech in the actual prompt):</p><blockquote class="px py pz"><p id="1d6f" class="nc nd pg ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Find variable symbol of creditor.<em class="fq"> </em>This number has exactly 9 consecutive digits 0–9 without letters or other special characters. It is usually preceded by one of the following abbreviations: ‘ev.č.’, ‘zn. opr’, ‘VS. O’, ‘evid. č. opr.’. On the contrary, I’m not interested in a transaction number with the abbreviation ‘č.j.’. This number does not appear often in documents, it may happen that you will not be able to find it, then write ‘cannot find’. If you’re not sure, write ‘not sure’.</p></blockquote><p id="a9a5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The model sometimes failed to output the 9-digit format accurately. Post-processing would filter out shorter numbers, but there were many false positives 9-digit numbers.</p><p id="a5da" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Occasionally the model inferred incorrect <em class="pg">birth ID</em>s based solely on birth dates (even with temperature set to 0). On the other hand, it excelled at extracting <em class="pg">names</em>, <em class="pg">surnames</em>, and <em class="pg">birth dates</em>.</p><p id="9902" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Overall, even in my previous experiments, <strong class="ne fr">I found that LLMs</strong> (at the time of writing) <strong class="ne fr">perform better with general tasks but lack accuracy and reliability for specific or unconventional tasks.</strong> The performance in identifying the client was fairly similar for both approaches. For internal reasons, the RoBERTa model was deployed.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="254b" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Post-processing</h1><p id="ee02" class="pw-post-body-paragraph nc nd fq ne b go ps ng nh gr pt nj nk nl pu nn no np pv nr ns nt pw nv nw nx fj bk">Notably, implementing <strong class="ne fr">post-processing can significantly reduce false positives</strong>, enhancing overall performance. Each entity was subject to customized filtering and validation rules:</p><ul class=""><li id="84a1" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pd pe pf bk"><em class="pg">variable symbol of debtor - </em>verify 9 digits format</li><li id="5c59" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pd pe pf bk"><em class="pg">birth ID - </em>enforce XXXXXX/XXX(X) format and check divisibility by eleven</li><li id="aa9f" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pd pe pf bk"><em class="pg">name </em>and <em class="pg">surname - </em>apply lemmatization using MorphoDiTa [5]</li><li id="586f" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pd pe pf bk"><em class="pg">date of birth - </em>enforce DD.MM.YYYY format</li></ul></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="95df" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Conclusion</h1><p id="a7af" class="pw-post-body-paragraph nc nd fq ne b go ps ng nh gr pt nj nk nl pu nn no np pv nr ns nt pw nv nw nx fj bk">The fine-tuned model was successfully deployed and <strong class="ne fr">performs superbly</strong>, <strong class="ne fr">exceeding expectations given the modest dataset</strong> of 710 documents.</p><p id="034c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While LLMs show promise for general tasks, they lack the accuracy and reliability for specialized tasks. That said, it’s likely that in the near future, even fine-tuning will become unnecessary for all but highly specialized cases as LLMs continue to improve.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="3e67" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Acknowledgments<br/></strong>I would like to thank <a class="af pm" href="https://www.linkedin.com/in/martin-munch/" rel="noopener ugc nofollow" target="_blank">Martin</a>, <a class="af pm" href="https://www.linkedin.com/in/tomas-duricek/" rel="noopener ugc nofollow" target="_blank">Tomáš</a> and <a class="af pm" href="https://www.linkedin.com/in/petr-petras-37b2b0160/" rel="noopener ugc nofollow" target="_blank">Petr</a> for their valuable suggestions for the improvement of this article.</p><p id="2edc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Sources<br/></strong>[1] Hugging Face, <a class="af pm" href="https://huggingface.co/docs/transformers/tasks/token_classification#preprocess" rel="noopener ugc nofollow" target="_blank">Transformers - Token classification</a> <br/>[2] Hugging Face, <a class="af pm" href="https://huggingface.co/learn/nlp-course/chapter7/2" rel="noopener ugc nofollow" target="_blank">NLP Course — Token classification</a> <br/>[3] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzman, E. Grave, M. Ott, L. Zettlemoyer and V. Stoyanov, <a class="af pm" href="https://arxiv.org/abs/1911.02116" rel="noopener ugc nofollow" target="_blank">Unsupervised Cross-lingual Representation Learning at Scale</a> (2019), CoRR abs/1911.02116 <br/>[4] M. Kocián, J. Náplava, D. Štancl and V. Kadlec, <a class="af pm" href="https://arxiv.org/abs/2112.01810" rel="noopener ugc nofollow" target="_blank">Siamese BERT-based Model for Web Search Relevance Ranking Evaluated on a New Czech Dataset</a> (2021)<br/>[5] J. Straková, M. Straka and J. Hajič . <a class="af pm" href="http://www.aclweb.org/anthology/P/P14/P14-5003.pdf" rel="noopener ugc nofollow" target="_blank">Open-Source Tools for Morphology, Lemmatization, POS Tagging and Named Entity Recognition</a> (2014), In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 13–18, Baltimore, Maryland, June 2014. Association for Computational Linguistics.</p></div></div></div></div>    
</body>
</html>