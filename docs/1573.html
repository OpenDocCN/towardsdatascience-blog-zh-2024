<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Mastering Object Counting in Videos</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Mastering Object Counting in Videos</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/mastering-object-counting-in-videos-3d49a9230bd2?source=collection_archive---------3-----------------------#2024-06-25">https://towardsdatascience.com/mastering-object-counting-in-videos-3d49a9230bd2?source=collection_archive---------3-----------------------#2024-06-25</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1f0e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Step-by-step guide to counting strolling ants on a tree using detection and tracking techniques.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@lihigurarie?source=post_page---byline--3d49a9230bd2--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lihi Gur Arie, PhD" class="l ep by dd de cx" src="../Images/7a1eb30725a95159401c3672fa5f43ab.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M0YTyQAxsWWqI8MLBi2xrA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3d49a9230bd2--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@lihigurarie?source=post_page---byline--3d49a9230bd2--------------------------------" rel="noopener follow">Lihi Gur Arie, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3d49a9230bd2--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Jun 25, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mj mk ml mm mn mo"><div class="mp io l ed"><div class="mq mr l"/></div><figcaption class="ms mt mu mv mw mx my bf b bg z dx">Ants counting in a video. In and Out counts appear on the upper left corner. Each ant is assigned a unique ID and color. Labels by Author, Original video by <a class="af mz" href="https://www.pexels.com/video/ants-carrying-leaves-on-tree-trunk-9888614/" rel="noopener ugc nofollow" target="_blank">Lui Lo Franco at Pexels</a></figcaption></figure><h1 id="a0f7" class="na nb fq bf nc nd ne gq nf ng nh gt ni nj nk nl nm nn no np nq nr ns nt nu nv bk"><strong class="al">Introduction</strong></h1><p id="f9b9" class="pw-post-body-paragraph nw nx fq ny b go nz oa ob gr oc od oe of og oh oi oj ok ol om on oo op oq or fj bk">Counting objects in videos is a challenging Computer Vision task. Unlike counting objects in static images, videos involve additional complexities, since objects can move, become occluded, or appear and disappear at different times, which complicates the counting process.</p><p id="21e5" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">In this tutorial, we‚Äôll demonstrate how to count ants moving along a tree, using Object Detection and tracking techniques. We‚Äôll harness Ultralytics platform to integrate YOLOv8 model for detection, BoT-SORT for tracking, and a line counter to count the ants.</p><blockquote class="ox oy oz"><p id="52aa" class="nw nx pa ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">If you don‚Äôt have a paid Medium account, you can read for free<em class="fq"> </em><a class="af mz" rel="noopener" target="_blank" href="/mastering-object-counting-in-videos-3d49a9230bd2?sk=8ec6a61e5dc66ec0ebc762ba01b6af73">here</a>.</p></blockquote><h1 id="3db0" class="na nb fq bf nc nd ne gq nf ng nh gt ni nj nk nl nm nn no np nq nr ns nt nu nv bk">Pipeline Overview</h1><p id="0c5a" class="pw-post-body-paragraph nw nx fq ny b go nz oa ob gr oc od oe of og oh oi oj ok ol om on oo op oq or fj bk">In a typical video object counting pipeline, each frame undergoes a sequence of processes: detection, tracking, and counting. Here‚Äôs a brief overview of each step:</p><ol class=""><li id="c896" class="nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or pb pc pd bk"><strong class="ny fr">Detection:</strong> An object detector identifies and locates objects in each frame, producing bounding boxes around them.</li><li id="41f6" class="nw nx fq ny b go pe oa ob gr pf od oe of pg oh oi oj ph ol om on pi op oq or pb pc pd bk"><strong class="ny fr">Tracking</strong>: A tracker follows these objects across frames, assigning unique IDs to each object to ensure they are counted only once.</li><li id="6abe" class="nw nx fq ny b go pe oa ob gr pf od oe of pg oh oi oj ph ol om on pi op oq or pb pc pd bk"><strong class="ny fr">Counting</strong>: The counting module aggregates this information and adds each new object to provide accurate results.</li></ol><figure class="mj mk ml mm mn mo mv mw paragraph-image"><div class="mv mw pj"><img src="../Images/020922d351e2affc484485704545edb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*3-CL-BtVteWe5YPUq7vWUA.png"/></div><figcaption class="ms mt mu mv mw mx my bf b bg z dx">Image by Author</figcaption></figure><p id="8321" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">Connecting an object detector, a tracker, and a counter might require extensive coding. Fortunately, the Ultralytics library [1] simplifies this process by providing a convenient pipeline that seamlessly integrates these components.</p><h1 id="9ca8" class="na nb fq bf nc nd ne gq nf ng nh gt ni nj nk nl nm nn no np nq nr ns nt nu nv bk">1. Detecting Objects with YOLOv8</h1><p id="049d" class="pw-post-body-paragraph nw nx fq ny b go nz oa ob gr oc od oe of og oh oi oj ok ol om on oo op oq or fj bk">The first step is to detect the ants in each frame produce bounding boxes around them. In this tutorial, we will use a YOLOv8 detector that I trained in advance to detect ants. I used <a class="af mz" rel="noopener" target="_blank" href="/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?sk=7c98df89b60ea49a6de9efd5278f645e">Grounding DINO</a> [2] to label the data, and then I used the annotated data to train the YOLOv8 model. If you want to learn more about training a YOLO model, refer to my previous post on <a class="af mz" rel="noopener" target="_blank" href="/the-practical-guide-for-object-detection-with-yolov5-algorithm-74c04aac4843?sk=00d2a9d6dd84d6ac4de153cab3dba7c0">training YOLOv5</a>, as the concepts are similar. For your application, you can use a pre-trained model or train a custom model of your own.</p><p id="5356" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">To get started, we need to initialize the detector with the pre-trained weights:</p><pre class="mj mk ml mm mn pl pm pn bp po bb bk"><span id="bc24" class="pp nb fq pm b bg pq pr l ps pt">from ultralytics import YOLO<br/><br/># Initialize YOLOv8 model with pre-trained weights<br/>model = YOLO("/path/to/your/yolo_model.pt")</span></pre><p id="5c6a" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">Later on, we will use the detector to detect ants in each frame within the video loop, integrating the detection with the tracking process.</p><h1 id="7d44" class="na nb fq bf nc nd ne gq nf ng nh gt ni nj nk nl nm nn no np nq nr ns nt nu nv bk">2. Tracking Objects with BoT-SORT</h1><p id="64f1" class="pw-post-body-paragraph nw nx fq ny b go nz oa ob gr oc od oe of og oh oi oj ok ol om on oo op oq or fj bk">Since ants appear multiple times across the video frames, it is essential to track each ant and assign it a unique ID, to ensure that each ant is counted only once. Ultralytics supports both BoT-SORT [3] and ByteTrack [4] for tracking.</p><ul class=""><li id="a25a" class="nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or pu pc pd bk"><strong class="ny fr">ByteTrack:</strong> Provides a balance between accuracy and speed, with lower computational complexity. It may not handle occlusions and camera motion as well as BoT-SORT.</li><li id="82e5" class="nw nx fq ny b go pe oa ob gr pf od oe of pg oh oi oj ph ol om on pi op oq or pu pc pd bk"><strong class="ny fr">BoT-SORT:</strong> Offers improved tracking accuracy and robustness over ByteTrack, especially in challenging scenarios with occlusions and camera motion. However, it comes at the cost of higher computational complexity and lower frame rates.</li></ul><p id="fa88" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">The choice between these algorithms depends on the specific requirements of your application.</p><p id="74bc" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk"><strong class="ny fr">How BoT-SORT Works</strong>: BoT-SORT is a multi-object tracker, meaning it can track multiple objects at the same time. It combines motion and appearance information along with camera motion compensation. The objects‚Äô positions are predicted using a Kalman filter, and the matches to existing tracks are based on both their location and visual features. This approach allows BoT-SORT to maintain accurate tracks even in the presence of occlusions or when the camera is moving.</p><p id="c91e" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">A well-configured tracker can compensate for the detector‚Äôs mild faults. For example if the object detector temporarily fails to detect an ant, the tracker can maintain the ant‚Äôs track using motion and appearance cues.</p><p id="5ec7" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">The detector and tracker are used iteratively on each frame within the video loop to produce the tracks. This is how you integrate it into your video processing loop:</p><pre class="mj mk ml mm mn pl pm pn bp po bb bk"><span id="8004" class="pp nb fq pm b bg pq pr l ps pt">tracks = model.track(frame, persist=True, tracker=‚Äôbotsort.yaml‚Äô, iou=0.2)</span></pre><p id="68f2" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">The tracker configuration is defined in the ‚Äòbotsort.yaml‚Äô file. You can adjust these parameters to best fit your needs. To change the tracker to ByteTrack, simply pass ‚Äòbytetrack.yaml‚Äô to the tracker parameter.</p><p id="83c0" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">Ensure that the Intersection Over Union (IoU) value fits your application requirements; the IoU threshold (used for non-maximum suppression) determines how close detections must be to be considered the same object. The <code class="cx pv pw px pm b">persist=True</code> argument tells the tracker that the current frame is part of a sequence and to expect tracks from the previous frame to persist into the current frame.</p><h1 id="2090" class="na nb fq bf nc nd ne gq nf ng nh gt ni nj nk nl nm nn no np nq nr ns nt nu nv bk">3. Counting Objects</h1><p id="8fac" class="pw-post-body-paragraph nw nx fq ny b go nz oa ob gr oc od oe of og oh oi oj ok ol om on oo op oq or fj bk">Now that we have detected and tracked the ants, the final step is to count the unique ants that crosses a designated line in the video. The <code class="cx pv pw px pm b">ObjectCounter</code> class from the Ultralytics library allows us to define a counting region, which can be a line or a polygon. For this tutorial, we will use a simple line as our counting region. This approach reduces errors by ensuring that an ant is counted only once when it crosses the line, even if its unique ID changes due to tracking errors.</p><p id="8228" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">First, we initialize the <code class="cx pv pw px pm b">ObjectCounter</code> before the video loop:</p><pre class="mj mk ml mm mn pl pm pn bp po bb bk"><span id="adc9" class="pp nb fq pm b bg pq pr l ps pt">counter = solutions.ObjectCounter( <br/>  view_img=True,                     # Display the image during processing <br/>  reg_pts=[(512, 320), (512, 1850)], # Region of interest points <br/>  classes_names=model.names,         # Class names from the YOLO model <br/>  draw_tracks=True,                  # Draw tracking lines for objects <br/>  line_thickness=2,                  # Thickness of the lines drawn <br/>  )</span></pre><p id="79ec" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">Inside the video loop, the <code class="cx pv pw px pm b">ObjectCounter</code> will count the tracks produced by the tracker. The points of the line are passed to the counter at the reg_pts parameter, in the [(x1, y1), (x2, y2)] format. When the center point of an ant‚Äôs bounding box crosses the line for the first time, it is added to the count according to its trajectory direction. Objects moving in a certain direction counted as ‚ÄòIn‚Äô, and objects moving to the other direction counted as ‚ÄòOut‚Äô.</p><pre class="mj mk ml mm mn pl pm pn bp po bb bk"><span id="76b3" class="pp nb fq pm b bg pq pr l ps pt">  # Use the Object Counter to count new objects <br/>  frame = counter.start_counting(frame, tracks)</span></pre><h1 id="b3cf" class="na nb fq bf nc nd ne gq nf ng nh gt ni nj nk nl nm nn no np nq nr ns nt nu nv bk">Full Code</h1><p id="8047" class="pw-post-body-paragraph nw nx fq ny b go nz oa ob gr oc od oe of og oh oi oj ok ol om on oo op oq or fj bk">Now that we have seen the counting components, let‚Äôs integrate the code with the video loop and save the resulting video.</p><pre class="mj mk ml mm mn pl pm pn bp po bb bk"><span id="4d30" class="pp nb fq pm b bg pq pr l ps pt"># Install and import Required Libraries<br/>%pip install ultralytics<br/>import cv2<br/>from ultralytics import YOLO, solutions<br/><br/># Define paths:<br/>path_input_video = '/path/to/your/input_video.mp4' <br/>path_output_video = "/path/to/your/output_video.avi"<br/>path_model = "/path/to/your/yolo_model.pt"<br/><br/># Initialize YOLOv8 Detection Model<br/>model = YOLO(path_model)<br/><br/># Initialize Object Counter<br/>counter = solutions.ObjectCounter( <br/>  view_img=True,                     # Display the image during processing <br/>  reg_pts=[(512, 320), (512, 1850)], # Region of interest points <br/>  classes_names=model.names,         # Class names from the YOLO model <br/>  draw_tracks=True,                  # Draw tracking lines for objects <br/>  line_thickness=2,                  # Thickness of the lines drawn <br/>  )<br/><br/># Open the Video File<br/>cap = cv2.VideoCapture(path_input_video) <br/>assert cap.isOpened(), "Error reading video file"<br/><br/># Initialize the Video Writer to save resulted video<br/>video_writer = cv2.VideoWriter(path_output_video, cv2.VideoWriter_fourcc(*"mp4v"), 30, (1080, 1920))<br/><br/># itterate over video frames:<br/>frame_count = 0 <br/>while cap.isOpened(): <br/>  success, frame = cap.read() <br/>  if not success: <br/>    print("Video frame is empty or video processing has been successfully completed.") <br/>    break <br/><br/>  # Perform object tracking on the current frame <br/>  tracks = model.track(frame, persist=True, tracker='botsort.yaml', iou=0.2) <br/><br/>  # Use the Object Counter to count objects in the frame and get the annotated image <br/>  frame = counter.start_counting(frame, tracks) <br/><br/>  # Write the annotated frame to the output video <br/>  video_writer.write(frame) <br/>  frame_count += 1<br/><br/># Release all Resources:<br/>cap.release() <br/>video_writer.release() <br/>cv2.destroyAllWindows()<br/><br/># Print counting results:<br/>print(f'In: {counter.in_counts}\nOut: {counter.out_counts}\nTotal: {counter.in_counts + counter.out_counts}')<br/>print(f'Saves output video to {path_output_video}')</span></pre><p id="7f57" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">The code above integrates object detection and tracking into a video processing loop to save the annotated video. Using OpenCV, we open the input video and set up a video writer for the output. In each frame, we perform object tracking with BoTSORT, count the objects, and annotate the frame. The annotated frames, including bounding boxes, unique IDs, trajectories, and ‚Äòin‚Äô and ‚Äòout‚Äô counts, are saved to the output video. The ‚Äòin‚Äô and ‚Äòout‚Äô counts can be retrieved from <code class="cx pv pw px pm b">counter.in_counts</code> and <code class="cx pv pw px pm b">counter.out_counts</code>, respectively, and are also printed on the output video.</p><figure class="mj mk ml mm mn mo mv mw paragraph-image"><div role="button" tabindex="0" class="pz qa ed qb bh qc"><div class="mv mw py"><img src="../Images/b7f437e548c99be9e341810585847d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mcCGKtCeRgv5m4BCC2EXrA.jpeg"/></div></div><figcaption class="ms mt mu mv mw mx my bf b bg z dx">An annotated frame. Each ant is assigned with a bounding box and a uniqe ID. Ants are counted as they cross the pink line. The counts of ants moving ‚Äòin‚Äô and ‚Äòout‚Äô are displayed at the corner of the image.</figcaption></figure><h1 id="3f98" class="na nb fq bf nc nd ne gq nf ng nh gt ni nj nk nl nm nn no np nq nr ns nt nu nv bk">Concluding Remarks</h1><p id="36ad" class="pw-post-body-paragraph nw nx fq ny b go nz oa ob gr oc od oe of og oh oi oj ok ol om on oo op oq or fj bk">In the annotated video, we correctly counted a total of 85 ants, with 34 entering and 51 exiting. For precise counts, it is crucial that the detector performs well and the tracker is well configured. A well-configured tracker can compensate for detector misses, ensuring continuity in tracking.</p><p id="d704" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">In the annotated video we can see that the tracker handled missing detections very well, as evidenced by the disappearance of the bounding box around an ant and its return in subsequent frames with the correct ID. Additionally, tracking mistakes that assigned different IDs to the same object (e.g., ant #42 turning into #48) did not affect the counts since only the ants that cross the line are counted.</p><p id="562d" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">In this tutorial, we explored how to count objects in videos using advanced object detection and tracking techniques. We utilized YOLOv8 for detecting ants and BoT-SORT for robust tracking, all integrated seamlessly with the Ultralytics library.</p><h1 id="2376" class="na nb fq bf nc nd ne gq nf ng nh gt ni nj nk nl nm nn no np nq nr ns nt nu nv bk">Thank you for reading!</h1><p id="a77b" class="pw-post-body-paragraph nw nx fq ny b go nz oa ob gr oc od oe of og oh oi oj ok ol om on oo op oq or fj bk">Congratulations on making it all the way here. Click üëç to show your appreciation and raise the algorithm self esteem ü§ì</p><p id="1bb9" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk"><strong class="ny fr">Want to learn more?</strong></p><ul class=""><li id="b6b0" class="nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or pu pc pd bk"><a class="af mz" href="https://medium.com/@lihigurarie" rel="noopener"><strong class="ny fr">Explore</strong></a> additional articles I‚Äôve written</li><li id="29e8" class="nw nx fq ny b go pe oa ob gr pf od oe of pg oh oi oj ph ol om on pi op oq or pu pc pd bk"><a class="af mz" href="https://medium.com/@lihigurarie/subscribe" rel="noopener"><strong class="ny fr">Subscribe</strong></a><strong class="ny fr"> </strong>to get notified when I publish articles</li><li id="5a89" class="nw nx fq ny b go pe oa ob gr pf od oe of pg oh oi oj ph ol om on pi op oq or pu pc pd bk">Follow me on <a class="af mz" href="https://www.linkedin.com/in/lihi-gur-arie/" rel="noopener ugc nofollow" target="_blank"><strong class="ny fr">Linkedin</strong></a></li></ul><blockquote class="ox oy oz"><p id="eacc" class="nw nx pa ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">References</p></blockquote><p id="6820" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">[1] <a class="af mz" href="https://github.com/ultralytics/ultralytics" rel="noopener ugc nofollow" target="_blank">Ultralytics GitHub</a></p><p id="f7f4" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">[2] <a class="af mz" href="https://arxiv.org/pdf/2303.05499" rel="noopener ugc nofollow" target="_blank">Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</a></p><p id="4ad4" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">[3] <a class="af mz" href="https://arxiv.org/pdf/2206.14651" rel="noopener ugc nofollow" target="_blank">BoT-SORT: Robust Associations Multi-Pedestrian Tracking</a></p><p id="4311" class="pw-post-body-paragraph nw nx fq ny b go os oa ob gr ot od oe of ou oh oi oj ov ol om on ow op oq or fj bk">[4] <a class="af mz" href="https://arxiv.org/pdf/2110.06864" rel="noopener ugc nofollow" target="_blank">ByteTrack: Multi-Object Tracking by Associating Every Detection Box</a></p></div></div></div></div>    
</body>
</html>