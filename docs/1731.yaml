- en: The Math Behind Multi-Head Attention in Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer中的多头注意力背后的数学原理
- en: 原文：[https://towardsdatascience.com/the-math-behind-multi-head-attention-in-transformers-c26cba15f625?source=collection_archive---------1-----------------------#2024-07-16](https://towardsdatascience.com/the-math-behind-multi-head-attention-in-transformers-c26cba15f625?source=collection_archive---------1-----------------------#2024-07-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-math-behind-multi-head-attention-in-transformers-c26cba15f625?source=collection_archive---------1-----------------------#2024-07-16](https://towardsdatascience.com/the-math-behind-multi-head-attention-in-transformers-c26cba15f625?source=collection_archive---------1-----------------------#2024-07-16)
- en: Deep Dive into Multi-Head Attention, the secret element in Transformers and
    LLMs. Let’s explore its math, and build it from scratch in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨Transformer和LLM中的秘密元素——多头注意力。让我们一起探索它的数学原理，并在Python中从零开始构建它。
- en: '[](https://medium.com/@cristianleo120?source=post_page---byline--c26cba15f625--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--c26cba15f625--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c26cba15f625--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c26cba15f625--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--c26cba15f625--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@cristianleo120?source=post_page---byline--c26cba15f625--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--c26cba15f625--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c26cba15f625--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c26cba15f625--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--c26cba15f625--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c26cba15f625--------------------------------)
    ·16 min read·Jul 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c26cba15f625--------------------------------)
    ·阅读时间16分钟·2024年7月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a64f2d99f5c4f6145d038295c2a7a8fd.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a64f2d99f5c4f6145d038295c2a7a8fd.png)'
- en: Image generated by DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由DALL-E生成
- en: '1: Introduction'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1：简介
- en: '1.1: Transformers Overview'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1：Transformer概述
- en: The Transformer architecture, introduced by Vaswani et al. in their paper “[Attention
    is All You Need](https://arxiv.org/abs/1706.03762)” has transformed deep learning,
    particularly in natural language processing (NLP). Transformers use a self-attention
    mechanism, enabling them to handle input sequences all at once. This parallel
    processing allows for faster computation and better management of long-range dependencies
    within the data. This doesn’t sound familiar? Don’t worry as it will be at the
    end of this article. Let’s first take a brief look at what a Transformer looks
    like.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构由Vaswani等人在他们的论文“[Attention is All You Need](https://arxiv.org/abs/1706.03762)”中提出，彻底改变了深度学习，特别是在自然语言处理（NLP）领域。Transformers使用自注意力机制，使其能够一次性处理输入序列。这种并行处理使得计算速度更快，同时更好地管理数据中的长距离依赖关系。这听起来不熟悉吗？别担心，文章的最后你会明白的。让我们首先简要了解一下Transformer的结构。
- en: '![](../Images/5314d8c6f16ac1928337de7e3026db5b.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5314d8c6f16ac1928337de7e3026db5b.png)'
- en: Transformer Architecture (Architecture in “[Attention is all you need](https://arxiv.org/abs/1706.03762)”)
    — Image by Author
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构（“[Attention is all you need](https://arxiv.org/abs/1706.03762)”中的架构）
    — 图片由作者提供
- en: 'A Transformer consists of two main parts: an **encoder** and a **decoder**.
    The encoder processes the input sequence to create a continuous representation,
    while the decoder generates the output sequence from this representation. Both
    the encoder and the decoder have multiple…'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer由两个主要部分组成：**编码器**和**解码器**。编码器处理输入序列以创建连续表示，而解码器则从这个表示中生成输出序列。编码器和解码器都有多个……
