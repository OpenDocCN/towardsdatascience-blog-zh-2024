- en: Using Self-Organizing Map To Bolster Retrieval-Augmented Generation In Large
    Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自组织映射增强大型语言模型中的检索增强生成
- en: 原文：[https://towardsdatascience.com/using-self-organizing-map-to-bolster-retrieval-augmented-generation-in-large-language-models-5d739ce21e9c?source=collection_archive---------3-----------------------#2024-03-16](https://towardsdatascience.com/using-self-organizing-map-to-bolster-retrieval-augmented-generation-in-large-language-models-5d739ce21e9c?source=collection_archive---------3-----------------------#2024-03-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/using-self-organizing-map-to-bolster-retrieval-augmented-generation-in-large-language-models-5d739ce21e9c?source=collection_archive---------3-----------------------#2024-03-16](https://towardsdatascience.com/using-self-organizing-map-to-bolster-retrieval-augmented-generation-in-large-language-models-5d739ce21e9c?source=collection_archive---------3-----------------------#2024-03-16)
- en: '*SOM is proposed to bolster efficient retrieval of LLM context for RAG…*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*SOM被提议用来增强LLM上下文的高效检索，以支持RAG……*'
- en: '[](https://murali-kashaboina.medium.com/?source=post_page---byline--5d739ce21e9c--------------------------------)[![Murali
    Kashaboina](../Images/ff1118f3c317dab87fe4b625a614fb93.png)](https://murali-kashaboina.medium.com/?source=post_page---byline--5d739ce21e9c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5d739ce21e9c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5d739ce21e9c--------------------------------)
    [Murali Kashaboina](https://murali-kashaboina.medium.com/?source=post_page---byline--5d739ce21e9c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://murali-kashaboina.medium.com/?source=post_page---byline--5d739ce21e9c--------------------------------)[![Murali
    Kashaboina](../Images/ff1118f3c317dab87fe4b625a614fb93.png)](https://murali-kashaboina.medium.com/?source=post_page---byline--5d739ce21e9c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5d739ce21e9c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5d739ce21e9c--------------------------------)
    [Murali Kashaboina](https://murali-kashaboina.medium.com/?source=post_page---byline--5d739ce21e9c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5d739ce21e9c--------------------------------)
    ·17 min read·Mar 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5d739ce21e9c--------------------------------)
    ·17分钟阅读·2024年3月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/77f2ba7f54a07f0a727c5ac6340c1a4e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77f2ba7f54a07f0a727c5ac6340c1a4e.png)'
- en: Photo by [Werclive 👹](https://unsplash.com/@werclive?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Werclive 👹](https://unsplash.com/@werclive?utm_source=medium&utm_medium=referral)来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Background
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 背景
- en: 'Large volumes of data are used to train Large Language Models (LLM) containing
    millions and billions of model parameters with the goal of text generation, such
    as text completion, text summarization, language translations, and answering questions.
    While LLMs develop a knowledge base per se from the training data sources, there
    is always a cut-off training date post which LLM will not know any newly generated
    data. For example, the cut-off date for training OpenAI’s GPT-3.5-turbo-instruct
    LLM is September 2021 (Ref: [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)),
    and as such, GPT-3.5-turbo-instruct LLM may not answer questions on 2022, 2023,
    or 2024 events accurately. Such data not part of the LLM’s original training data
    is called external data. Retrieval-Augmented Generation (RAG) is a technique meant
    to help in such cases by retrieving appropriate information contextual to the
    input prompt from authorized external sources and augmenting input so that LLM
    can generate accurate and relevant responses. Effectively, RAG forms the gateway
    between the LLM and the external data. Such augmentation eliminates the need to
    retrain or further fine-tune the LLM model.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大量数据被用于训练大型语言模型（LLM），这些模型包含数百万、数十亿个参数，目的是进行文本生成，如文本补全、文本摘要、语言翻译和问题回答。虽然LLM从训练数据中自然而然地建立了一个知识库，但训练数据有一个截止日期，截止日期之后，LLM将无法了解任何新生成的数据。例如，OpenAI的GPT-3.5-turbo-instruct
    LLM的训练截止日期是2021年9月（参考：[https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)），因此，GPT-3.5-turbo-instruct
    LLM可能无法准确回答2022年、2023年或2024年的事件。这类不属于LLM原始训练数据的数据称为外部数据。检索增强生成（RAG）是一种技术，旨在通过从授权的外部来源检索与输入提示相关的适当信息，并增强输入，以便LLM能够生成准确且相关的响应。实际上，RAG构成了LLM与外部数据之间的桥梁。这种增强避免了重新训练或进一步微调LLM模型的需要。
- en: LLM’s Typical M.O.
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM的典型操作模式
- en: 'LLMs are auto-regressive, generating a new token based on the input prompt
    tokenized into a sequence of tokens. The generation of the next best token is
    probability-based and can be expressed as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LLM是自回归的，根据输入提示将token化的序列生成一个新的token。下一个最佳token的生成是基于概率的，可以表达为以下形式：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Essentially, the probability of the newly generated nth token, Yn, is conditioned
    on the probability of the occurrence of the sequence of n-1 previous tokens X
    and the learned model parameters θ. It should be noted here that the tokenized
    input sequence X plays a crucial role in generating the next token. In addition,
    self-attention mechanisms complement effective auto-regression, where each input
    token in the sequence computes its representation by attending to and weighing
    the importance of other tokens in the sequence. Such intricate relationships and
    dependencies among the tokens in the sequence also enable the LLM to decipher
    the most probable next-best token that ‘gels well’ with the tokens in the input
    sequence. The LLM appends the new token to the previous tokens to form a new input
    sequence and repeats the auto-regressive process until a completion condition
    is met, such as reaching the maximum token count.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，新生成的第n个token Yn的概率是基于n-1个前序tokens X以及学习到的模型参数θ的概率。这里需要注意的是，token化的输入序列X在生成下一个token时起着至关重要的作用。此外，自注意力机制有效地补充了自回归，在这种机制下，序列中的每个输入token通过关注并加权序列中其他token的重要性来计算其表示。这种token之间复杂的关系和依赖性使得LLM能够解码出与输入序列中的tokens“契合”的下一个最佳token。LLM将新生成的token附加到之前的tokens上，形成新的输入序列，并重复自回归过程，直到满足结束条件，例如达到最大token数量。
- en: Such a self-attention-driven auto-regression implies that the LLM relies predominantly
    on the input sequence to generate the next best token. As long as the input sequence
    helps determine the next-best token through self-attention, the LLM continues
    in a ‘virtuous’ loop, generating coherent, comprehensible, and relevant outputs.
    On the contrary, the LLM will start relying on the model parameters if the prompt
    inputs do not help determine the next best token. In such a case, the model may
    succeed in generating the next best token if the model has been trained to contain
    sufficient ‘knowledge’ contextual to the input prompt. Conversely, the model may
    go into a ‘vicious’ loop, generating non-coherent, incomprehensible, and possibly
    irrelevant outputs if the prompt inputs pertain to ‘external data’ that the LLM
    has never been trained on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自注意力驱动的自回归模型意味着，LLM主要依赖于输入序列来生成下一个最佳的token。只要输入序列通过自注意力帮助确定下一个最佳token，LLM就会继续在一个“良性”循环中生成连贯、易理解且相关的输出。相反，如果提示输入无法帮助确定下一个最佳token，LLM将开始依赖模型参数。在这种情况下，如果模型已经经过足够的训练，能够包含与输入提示相关的‘知识’，它可能成功生成下一个最佳token。相反，如果提示输入涉及LLM从未训练过的‘外部数据’，模型可能会进入一个‘恶性’循环，生成不连贯、难以理解且可能无关的输出。
- en: Various techniques tackle this issue. Prompt engineering is one of them, where
    the goal is to address the ‘missing context’ by adjusting the prompt to enhance
    the context so that the LLM can generate relevant output. RAG is another technique
    where the goal is to specifically address the ‘missing context due to external
    data’ by retrieving the most appropriate information contextual to the input prompt
    from external data sources in an automated manner and augmenting the prompt.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种技术来应对这个问题。提示工程就是其中之一，其目标是通过调整提示来解决‘缺失的上下文’，以增强上下文，使得LLM可以生成相关的输出。RAG是另一种技术，其目标是通过从外部数据源自动检索与输入提示相关的最合适信息，并增强提示，专门解决由于‘外部数据’缺失的上下文。
- en: RAG’s Challenge
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG的挑战
- en: 'The primary responsibility of RAG is to search and retrieve data that is contextually
    related to the input prompt from external data sources such as informational databases,
    APIs, and other document repositories like Wikipedia. A simple keyword search
    would not cut it. Instead, RAG requires a semantic search. To facilitate semantic
    search, the textual information retrieved from external sources is transformed
    into numerical representations or vectors, commonly called text embeddings, and
    stored in vector databases. There are various models or algorithms for creating
    these embeddings from text. The prompt is first transformed into its vector representation
    to search and retrieve closest matching external data vectors. Vector similarities
    (or vector distances) are then computed between the prompt vector and the previously
    stored external data vectors. The most similar or nearest vectors are sorted and
    filtered using a threshold, and their corresponding textual information is retrieved
    to augment the prompt’s context. The following conceptual diagram captures the
    typical interactions between different components for enabling RAG:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的主要职责是从外部数据源（如信息数据库、API和其他文档库，如维基百科）中搜索并检索与输入提示相关的上下文数据。简单的关键词搜索是不够的，而是需要进行语义搜索。为了促进语义搜索，从外部源检索的文本信息会被转化为数值表示或向量，通常称为文本嵌入，并存储在向量数据库中。存在多种模型或算法用于从文本中创建这些嵌入。首先将提示转化为其向量表示，以便搜索和检索最接近的外部数据向量。然后计算提示向量与先前存储的外部数据向量之间的向量相似度（或向量距离）。根据相似度排序并使用阈值过滤最相似或最近的向量，最终检索相应的文本信息以增强提示的上下文。以下概念图展示了启用RAG时不同组件之间的典型交互：
- en: '![](../Images/b2ae4ffe86cb15e19dafd2de1bbfbf70.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2ae4ffe86cb15e19dafd2de1bbfbf70.png)'
- en: Conceptual View of Primary System Component Interactions for Enabling RAG —
    Image by Author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 启用RAG的主要系统组件交互的概念视图 — 作者提供的图片
- en: RAG’s challenge is that conducting a vector-driven semantic search is non-trivial
    and requires significant computational resources because it involves calculating
    vector similarities or distances against potentially a vast number of vectors
    within the database. Computing similarity or distance measures for each stored
    vector from a vast vector database for every input prompt will become infeasible.
    Besides, the lower the semantic match quality, the lower the LLM’s generative
    output quality. Therefore, finding a way to conduct the semantic search efficiently
    becomes crucial.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: RAG面临的挑战是，进行基于向量的语义搜索并非简单任务，且需要大量计算资源，因为它涉及到对可能是庞大数据库中大量向量进行相似度或距离计算。对于每一个输入提示，从庞大的向量数据库中计算每个存储向量的相似度或距离将变得不可行。而且，语义匹配质量越低，LLM的生成输出质量也越低。因此，找到一种高效进行语义搜索的方法变得至关重要。
- en: Solution
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Several algorithmic solutions are employed to conduct efficient semantic searches.
    The typical approach of such algorithms is to group or cluster external data vectors
    as nearest neighbors and index them by mapping to such clusters. Such indexing
    is offered as a built-in capability by most vector databases. The matched clusters
    are first evaluated for the input prompt vector during semantic search. For each
    evaluated cluster, indexed vectors are selected. Similarities between the input
    prompt vector and the selected vectors are then computed. The expectation here
    is that finding the ‘nearest neighbors’ as an intermediate step reduces the number
    of similarity computations significantly. Finally, the textual information is
    retrieved corresponding to the most similar or nearest vectors filtered through
    thresholding. Algorithms such as k-Nearest Neighbors, Ball-of-Radius-R, Locality-Sensitive-Hashing,
    DBSCAN-Clustering, Tree-Like hierarchies, and Graph-Like hierarchies are typically
    implemented by vector databases to facilitate semantic searches.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行高效的语义搜索，采用了几种算法解决方案。这些算法的典型方法是将外部数据向量按最近邻进行分组或聚类，并通过映射到这些聚类来进行索引。大多数向量数据库提供了这种内建的索引功能。在语义搜索过程中，首先会评估输入提示向量与匹配到的聚类。对于每个评估过的聚类，选择相应的索引向量。然后计算输入提示向量与所选向量之间的相似度。此处的预期是，通过找到“最近邻”作为一个中间步骤，显著减少了相似度计算的次数。最后，依据阈值过滤，通过相似度最高或最近的向量来检索相应的文本信息。像k-最近邻、半径球体R、局部敏感哈希、DBSCAN聚类、树状层次结构和图状层次结构等算法，通常由向量数据库实现，用以促进语义搜索。
- en: There is no one-size-fits-all solution because different families of algorithms
    have different trade-offs in terms of memory efficiency, compute efficiency, latency,
    accuracy, vector dimensionality, dataset sizing, etc. For example, clustering
    methods enable speed by narrowing the vector space for semantic search, while
    tree-like or graph-like methods offer improved accuracy for low-dimensional vector
    data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一种适用于所有情况的解决方案，因为不同类型的算法在内存效率、计算效率、延迟、准确性、向量维度、数据集大小等方面有不同的权衡。例如，聚类方法通过缩小语义搜索的向量空间来提高速度，而类似树形或图形的方法则为低维向量数据提供更高的准确性。
- en: Self-Organizing Maps
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自组织映射
- en: A Self-Organizing Map (SOM) is a neural network-based dimensionality reduction
    algorithm developed by Teuvo Kohonen in the 1980s. It is typically used to reduce
    high-dimensional feature vectors to low-dimensional (typically two-dimensional)
    feature vectors. The core idea behind SOM is to represent high-dimensional data
    vectors as specific nodes in a low-dimensional space while retaining the vectors’
    topology in the original space. The number of nodes in the low-dimensional space
    (SOM Nodes) is fixed (hyper-parameter). The exact locations of SOM nodes are evaluated
    through multiple training epochs. The goal of the iterative training is to adjust
    the locations of the SOM nodes in the low-dimensional space so that they get mapped
    to the nearest neighboring vectors in the high-dimensional feature space. In other
    words, the goal is to map nearest-neighbor vectors in the high-dimensional space
    to SOM nodes that are also nearest neighbors in the low-dimensional space.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 自组织映射（SOM）是一种基于神经网络的降维算法，由Teuvo Kohonen在1980年代开发。它通常用于将高维特征向量降至低维（通常是二维）特征向量。SOM的核心思想是将高维数据向量表示为低维空间中的特定节点，同时保留向量在原始空间中的拓扑结构。低维空间中的节点数量（SOM节点）是固定的（超参数）。SOM节点的精确位置通过多个训练周期来评估。迭代训练的目标是调整SOM节点在低维空间中的位置，使其映射到高维特征空间中最邻近的向量。换句话说，目标是将高维空间中最近邻的向量映射到低维空间中也为最近邻的SOM节点。
- en: SOM for RAG
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SOM用于RAG
- en: 'In this write-up, I wanted to share notes and findings from my experiments
    with SOM as a possible algorithm to propel RAG’s semantic search. There are three
    crucial reasons SOM could be ideal compared to other algorithms:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我想分享我在实验中对SOM的笔记和发现，作为一种推动RAG语义搜索的可能算法。SOM相比其他算法可能理想的三个关键原因是：
- en: Vectors’ high dimensionality can become a bottleneck for most other algorithms,
    such as Trees and Graphs—the so-called curse of dimensionality. On the contrary,
    SOM is built for dimensionality reduction, and therefore, it can be effectively
    applied in both high-dimensional and low-dimensional scenarios.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量的高维度可能成为大多数其他算法的瓶颈，例如树和图——这就是所谓的维度灾难。相反，SOM是为降维而设计的，因此它可以在高维和低维场景中有效应用。
- en: SOM is less sensitive to random variations that may trickle into the original
    high-dimensional vector space, resulting in noise. Other algorithms can be sensitive
    to such noise, impacting the way they cluster or group high-dimensional vectors
    as nearest neighbors. Since SOM employs intermediate SOM nodes in a lower-dimensional
    vector space which get evaluated as local averages of the mapped vectors from
    the higher-dimensional space, it effectively reduces noise.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SOM对可能渗入原始高维向量空间的随机变化不太敏感，从而避免了噪音的影响。其他算法可能对这些噪音敏感，从而影响它们将高维向量聚类或分组为最近邻的方式。由于SOM在低维向量空间中使用中间的SOM节点，这些节点被评估为映射自高维空间向量的局部平均值，因此它有效地减少了噪音。
- en: The large size of the external dataset may constrain other algorithms to create
    semantic vector spaces, which can impact semantic matching's latency and accuracy.
    On the other hand, SOM can tackle massive datasets because the number of SOM nodes
    in the low-dimensional space can be fine-tuned through a hyper-parameter proportional
    to the underlying dataset size. While training a SOM using a large dataset may
    take longer, query time mapping remains quicker once training is done.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 外部数据集的庞大规模可能会限制其他算法在创建语义向量空间时的表现，这可能会影响语义匹配的延迟和准确性。另一方面，SOM能够处理大规模数据集，因为低维空间中的SOM节点数量可以通过与底层数据集大小成比例的超参数来精细调整。尽管使用大数据集训练SOM可能需要更长的时间，但训练完成后，查询映射仍然会更快。
- en: 'I demonstrate a simple example of using SOM to conduct RAG’s semantic search
    to augment the context for question/answer using OpenAI’s GPT-3.5-turbo-instruct
    LLM. The primary reason for using OpenAI’s GPT-3.5-turbo-instruct LLM is because
    the cut-off date for training OpenAI’s GPT-3.5-turbo-instruct LLM is September
    2021 (Ref: [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)),
    and as such, GPT-3.5-turbo-instruct LLM may not answer questions on 2022, 2023,
    or 2024 events accurately. Therefore, information about 2022, 2023, 0r 2024 events
    can become ‘external data’ for OpenAI’s GPT-3.5-turbo-instruct LLM. I used Wikipedia
    API as the source for such ‘external data’ to fetch events’ information. The following
    are the steps I used to develop and train the example, along with the sample code.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我展示了一个简单的示例，使用 SOM 来进行 RAG 的语义搜索，以增强基于 OpenAI GPT-3.5-turbo-instruct LLM 的问答上下文。使用
    OpenAI GPT-3.5-turbo-instruct LLM 的主要原因是因为 OpenAI GPT-3.5-turbo-instruct LLM 的训练截止日期为
    2021 年 9 月（参考：[https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)），因此，GPT-3.5-turbo-instruct
    LLM 可能无法准确回答 2022 年、2023 年或 2024 年的事件问题。因此，关于 2022 年、2023 年或 2024 年的事件信息可能成为 OpenAI
    GPT-3.5-turbo-instruct LLM 的“外部数据”。我使用了 Wikipedia API 作为这种“外部数据”的来源，来获取事件信息。以下是我用来开发和训练示例的步骤，以及示例代码。
- en: 'Step 1: PyTorch-Based Kohonen’s SOM implementation'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步：基于 PyTorch 的 Kohonen SOM 实现
- en: 'I utilized PyTorch Tensors to represent vectors and implemented Kohonen’s SOM
    using PyTorch. This algorithm uses a two-dimensional lattice whose size becomes
    a hyper-parameter. The algorithm’s mathematical aspects were derived from a well-crafted
    perspective with lucid explanations mentioned in the following article:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了 PyTorch 张量来表示向量，并使用 PyTorch 实现了 Kohonen 的 SOM。该算法使用一个二维格子，其大小成为一个超参数。算法的数学方面来源于以下文章中的清晰解释：
- en: '[](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
    [## SOM tutorial part 1'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
    [## SOM 教程第 1 部分'
- en: neural network tutorial in plain english
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络教程（通俗易懂）
- en: www.ai-junkie.com](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: www.ai-junkie.com](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
- en: The following code snippet shows the Python class for Kohonen’s SOM. The complete
    code is available at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/kohonen_som.py).
    It’s worth noting that this implementation is standalone, so it can be used outside
    of RAG example.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了 Kohonen SOM 的 Python 类。完整代码可在[这个 GitHub 链接](https://github.com/kbmurali/som-driven-qa-rag/blob/main/kohonen_som.py)找到。值得注意的是，这个实现是独立的，因此可以在
    RAG 示例之外使用。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Step 2: SOM-Based Vector Indexer Implementation'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步：基于 SOM 的向量索引器实现
- en: The vector indexer is a utility that uses Kohonen’s SOM to train SOM nodes with
    data vectors from an external dataset. Its primary purpose is to map each data
    vector to the closest top-k SOM nodes, enabling efficient indexing of the data
    vectors. The following code snippet shows the train and indexing function of the
    vector indexer Python class. Its complete code is available at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/vector_indexer.py).
    Although its implementation is currently limited to the example’s needs, it can
    be extended to meet other requirements.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 向量索引器是一个工具，利用 Kohonen 的 SOM 来训练 SOM 节点，使用来自外部数据集的数据向量。其主要目的是将每个数据向量映射到最接近的 top-k
    SOM 节点，从而实现高效的数据向量索引。以下代码片段展示了向量索引器 Python 类的训练和索引功能。其完整代码可在[这个 GitHub 链接](https://github.com/kbmurali/som-driven-qa-rag/blob/main/vector_indexer.py)找到。尽管其实现目前仅限于示例的需求，但可以扩展以满足其他需求。
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Step 3: OpenAI Embeddings-Based Text-To-Vector Encoder'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步：基于 OpenAI 嵌入的文本到向量编码器
- en: The encoder’s primary function is to convert text into vector representations
    using OpenAI’s text embedding API. It is worth noting that an OpenAI account and
    API key are required to use the embedding API. Upon opening an account for the
    first time, OpenAI provides complementary credit grants, which are more than enough
    to access the API for testing purposes. Below is a code snippet showcasing the
    batch encode function of the OpenAI encoder Python class. The complete code is
    available at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/openai_vector_encoder.py).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的主要功能是使用OpenAI的文本嵌入API将文本转换为向量表示。值得注意的是，使用嵌入API需要一个OpenAI帐户和API密钥。在首次开通账户时，OpenAI会提供免费的信用额度，足以用于API测试。以下是展示OpenAI编码器Python类的批量编码功能的代码片段，完整代码可在[这个GitHub位置](https://github.com/kbmurali/som-driven-qa-rag/blob/main/openai_vector_encoder.py)找到。
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the OpenAI vector encoder class extends a generic parent class, ‘VectorEncoder,’
    that defines abstract encoding functions to be implemented through inheritance.
    It is possible to implement other types of vector encoders by inheriting from
    this parent class for the pluggability of other encoding schemes. The complete
    code for the parent vector encoder class can be found at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/vector_encoder_parent.py).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，OpenAI向量编码器类扩展了一个通用的父类‘VectorEncoder’，该父类定义了抽象的编码函数，需通过继承来实现。可以通过继承该父类实现其他类型的向量编码器，从而实现其他编码方案的插件化。父向量编码器类的完整代码可以在[这个GitHub位置](https://github.com/kbmurali/som-driven-qa-rag/blob/main/vector_encoder_parent.py)找到。
- en: 'Step 4: Wikipedia API-Driven DataSource Implementation'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤4：基于Wikipedia API的数据源实现
- en: This utility class is designed to encapsulate the data retrieval logic that
    integrates with Wikipedia API. Its main function is to fetch events for a specified
    array of calendar years, format the retrieved events, and load them into a Pandas
    dataframe. The code snippet below captures the primary function of the utility
    class, while the complete code is available at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/wiki_datasource.py).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工具类旨在封装与Wikipedia API集成的数据检索逻辑。它的主要功能是获取指定日历年份范围内的事件，格式化检索到的事件，并将它们加载到Pandas数据框中。以下代码片段展示了该工具类的主要功能，完整代码可以在[这个GitHub位置](https://github.com/kbmurali/som-driven-qa-rag/blob/main/wiki_datasource.py)找到。
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Step 5: SOM-Based RAG Utility Implementation'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤5：基于SOM的RAG工具实现
- en: The SOM-based RAG utility is a crucial element of the example implementation.
    It utilizes the vector encoder, indexer, and data source to implement the core
    logic for the underlying semantic search. The complete code for the SOM-based
    RAG utility is available at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/som_based_rag.py).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 基于SOM的RAG工具是示例实现中的一个关键元素。它利用向量编码器、索引器和数据源实现底层语义搜索的核心逻辑。基于SOM的RAG工具的完整代码可以在[这个GitHub位置](https://github.com/kbmurali/som-driven-qa-rag/blob/main/som_based_rag.py)找到。
- en: The utility implements three primary functions. The first function is to load
    data from an external data source and encode it into vectors, as shown in the
    following code snippet.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该工具实现了三个主要功能。第一个功能是从外部数据源加载数据并将其编码为向量，如下面的代码片段所示。
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The second function is to train the SOM-based indexer to construct Kohonen’s
    SOM nodes and then index the data vectors, as shown in the following code snippet.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个功能是训练基于SOM的索引器，构建Kohonen的SOM节点，然后对数据向量进行索引，如下面的代码片段所示。
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The third function is to find similar information from the previously stored
    external dataset based on a query text. This function uses the encoder to convert
    the query text into a vector and then searches through the SOM-based indexer for
    the most likely matches. This function then calculates the similarity between
    the query vector and the discovered data vectors using Cosine similarity or another
    specified similarity evaluator. Finally, this function filters the data vectors
    whose similarities are greater than or equal to the specified similarity threshold.
    The following code snippet captures the function implementation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个功能是基于查询文本从先前存储的外部数据集找到相似的信息。该功能使用编码器将查询文本转换为向量，然后通过基于SOM的索引器搜索最可能的匹配项。接着，使用余弦相似度或其他指定的相似度评估器计算查询向量与发现的数据向量之间的相似度。最后，该功能筛选出与指定相似度阈值大于或等于的相似度的数据向量。下面的代码片段展示了该功能的实现。
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'An example output from a semantic search by SOM-based RAG utility function
    is shown below:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是通过基于SOM的RAG实用函数进行语义搜索的示例输出：
- en: '![](../Images/0495dc8a7325e42a26216f4edea1b957.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0495dc8a7325e42a26216f4edea1b957.png)'
- en: An Example Semantic Search Output — Image by Author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例的语义搜索输出——作者提供的图像
- en: 'Step 6: Abstract Question/Answer ChatBot And Its OpenAI-Based Implementation'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6步：抽象问题/回答聊天机器人及其基于OpenAI的实现
- en: An abstract ‘QuestionAnswerChatBot’ Python class is developed to facilitate
    chatbot-like implementations. It augments the prompted question by using a standard
    instruction template and populating it with contextually similar information retrieved
    from the RAG utility.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个抽象的‘QuestionAnswerChatBot’ Python类被开发出来，以便于类似聊天机器人的实现。它通过使用标准的指令模板，并用从RAG实用工具中检索到的语境相似信息填充它，来增强问题提示。
- en: The specified maximum number of new tokens limits the text size for context
    augmentation, while token counting is deferred to underlying implementations.
    In LLM economics, tokens are like currency. Each token the model processes requires
    computational resources — memory, processing power, and time. Thus, the more tokens
    an LLM has to process, the greater the computational cost.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 指定的最大新标记数限制了上下文增强的文本大小，而标记计数则推迟到底层实现。在LLM经济学中，标记就像货币一样。模型处理的每一个标记都需要计算资源——内存、处理能力和时间。因此，LLM需要处理的标记越多，计算成本就越高。
- en: Finally, this class delegates prompting of the LLM model to the underlying implementation
    once the QA instruction has been populated. The following code snippet captures
    the primary function; the complete code is available at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/qa_chatbot.py).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦问答指令被填充，该类会将LLM模型的提示工作委托给底层实现。以下代码片段展示了主要功能；完整代码可以在[这个GitHub位置](https://github.com/kbmurali/som-driven-qa-rag/blob/main/qa_chatbot.py)找到。
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The Python class ‘OpenAIQuestionAnswerChatBot’ extends the abstract ‘QuestionAnswerChatBot’
    and implements the chatbot functionality using the OpenAI LLM API. The following
    code snippet shows the class’s primary function. The complete code is available
    at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/openai_qa_chatbot.py).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Python类‘OpenAIQuestionAnswerChatBot’扩展了抽象类‘QuestionAnswerChatBot’，并使用OpenAI
    LLM API实现了聊天机器人功能。以下代码片段展示了该类的主要功能。完整代码可以在[这个GitHub位置](https://github.com/kbmurali/som-driven-qa-rag/blob/main/openai_qa_chatbot.py)找到。
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following is an example of how a prompted question gets augmented with
    context using similar information retrieved through semantic search:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何通过语义搜索检索到的类似信息来增强问题提示的示例：
- en: '![](../Images/77dd0ecb2e734bb5bb300ed137e03cf6.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77dd0ecb2e734bb5bb300ed137e03cf6.png)'
- en: An Example Context Augmented Question Prompt — Image by Author
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例的上下文增强问题提示——作者提供的图像
- en: 'Step 7: Sample Questions for Testing'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7步：用于测试的示例问题
- en: The following are sample questions for testing the RAG using OpenAI’s GPT-3.5-turbo-instruct
    LLM. They were developed to ensure that their answers pertain to events that occurred
    in 2022, 2023, and 2024.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于测试RAG的示例问题，使用的是OpenAI的GPT-3.5-turbo-instruct LLM。这些问题是为了确保它们的答案与2022年、2023年和2024年发生的事件相关。
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Step 8: Putting Everything Together'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8步：将所有内容整合在一起
- en: The complete Jupyter notebook that brings all the components together can be
    found at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/OpenAI_Based_SOM_GPT2_Bot.ipynb).
    The following code snippet shows the initiation of the main OpenAI-based QA chatbot.
    Note that OpenAI’s text embedding algorithm, “text-embedding-ada-002,” is used
    for vector encoding. Likewise, the chatbot uses OpenAI’s tokenizer, “cl100k_base,”
    to count the tokens to limit the contextual text to augment the question prompt
    by leveraging the inbuilt functions of the TikToken Python library.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 整合所有组件的完整Jupyter笔记本可以在[这个GitHub位置](https://github.com/kbmurali/som-driven-qa-rag/blob/main/OpenAI_Based_SOM_GPT2_Bot.ipynb)找到。以下代码片段展示了基于OpenAI的主要问答聊天机器人的初始化过程。请注意，OpenAI的文本嵌入算法“text-embedding-ada-002”用于向量编码。同样，聊天机器人使用OpenAI的分词器“cl100k_base”来计算标记数，以限制上下文文本，并通过利用TikToken
    Python库的内置函数增强问题提示。
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The following sequence diagrams help visualize all the component interactions
    during the initialization and actual question/answering phases.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下序列图有助于可视化初始化和实际问答阶段中所有组件的交互。
- en: '![](../Images/acffe90601416d6f17621a9d2d29cddb.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acffe90601416d6f17621a9d2d29cddb.png)'
- en: Interactions of Various Components During Initialization — Image by Author
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化过程中的各组件交互——作者提供的图像
- en: '![](../Images/320862ead248a3bb9360d7ac2eea63bc.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/320862ead248a3bb9360d7ac2eea63bc.png)'
- en: Interactions of Various Components During Question/Answering — Image by Author
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 问答过程中各个组件的交互 —— 图片由作者提供
- en: Findings
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 研究发现
- en: The following image captures the question/answers from OpenAI’s GPT-3.5-turbo-instruct
    LLM with and without context augmentation.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了在有无上下文增强的情况下，OpenAI的GPT-3.5-turbo-instruct LLM的问答。
- en: '![](../Images/e55fe942772a20c25c4906ad9d6d3a5b.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e55fe942772a20c25c4906ad9d6d3a5b.png)'
- en: OpenAI’s GPT-3.5-turbo-instruct LLM’s Answers With and Without Context Augmentation
    — Image by Author
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT-3.5-turbo-instruct LLM的有无上下文增强回答 —— 图片由作者提供
- en: Understandably, the LLM finds it challenging to answer questions about events
    that occurred after its September 2021 cut-off date. In most cases, it clearly
    responds that the questions are from a future time relative to its training cut-off
    date. On the contrary, the same LLM answers all the questions accurately to perfection
    when the context of the prompted questions is augmented with relevant information
    from years 2022, 2023, and 2024 retrieved from Wikipedia. The real credit here
    goes to the SOM that formed the basis for RAG’s semantic search to retrieve and
    augment the prompted question’s context with relevant information.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 可以理解的是，LLM发现回答2021年9月之后发生的事件相关问题时存在困难。在大多数情况下，它明确回应说这些问题相对于其训练截止日期而言来自未来。相反，当通过从维基百科中检索到的2022、2023和2024年的相关信息增强问题上下文时，同一LLM能够准确无误地回答所有问题。真正的功劳在于为RAG的语义搜索提供基础的SOM，它使得能够通过检索并增强问题的上下文，提供相关信息。
- en: Suggested Next Steps
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建议的下一步
- en: While the above example served as a proof-of-concept to assess the suitability
    of a Self-Organizing Map to enable Retrieval-Augmented Generation of text by an
    LLM, a more comprehensive benchmarking is suggested to evaluate its performance
    in comparison to other algorithms using a much larger external dataset, where
    performance is measured in terms of the quality of LLM outputs (something like
    perplexity + accuracy). In addition, since the current example enables a pluggable
    framework, it is suggested that other open-source and free QA LLMs be used to
    conduct such benchmarking to minimize the LLM usage expenses.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述示例作为概念验证，评估了自组织映射（Self-Organizing Map，SOM）是否适合用于使LLM通过检索增强生成（RAG）文本，但建议进行更全面的基准测试，以评估其与其他算法的性能对比，使用更大规模的外部数据集，在此过程中，性能将通过LLM输出的质量来衡量（类似困惑度+准确度）。此外，由于当前示例启用了可插拔框架，建议使用其他开源且免费的QA
    LLM进行此类基准测试，以减少LLM的使用费用。
- en: To help run the example in local environments, I included the ‘requirements.txt’
    file, which contains various versions of Python libraries I used in my environment
    to run and test the above example. This file is available at [this GitHub location](https://github.com/kbmurali/som-driven-qa-rag/blob/main/requirements.txt).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助在本地环境中运行示例，我附上了‘requirements.txt’文件，其中包含我在环境中用于运行和测试上述示例的各种版本的Python库。该文件可在[此GitHub位置](https://github.com/kbmurali/som-driven-qa-rag/blob/main/requirements.txt)找到。
- en: I conclude by promising to share my findings in a separate write-up if I conduct
    any such benchmarks. Please stay tuned!!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我最后承诺，如果我进行任何此类基准测试，将会在单独的文章中分享我的发现。请继续关注！！
- en: References
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
    [## SOM tutorial part 1'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
    [## SOM教程第1部分'
- en: neural network tutorial in plain english
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络教程（简明英语版）
- en: www.ai-junkie.com](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
    [](/understanding-self-organising-map-neural-network-with-python-code-7a77f501e985?source=post_page-----5d739ce21e9c--------------------------------)
    [## Understanding Self-Organising Map Neural Network with Python Code
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.ai-junkie.com](http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------)
    [](/understanding-self-organising-map-neural-network-with-python-code-7a77f501e985?source=post_page-----5d739ce21e9c--------------------------------)
    [## 通过Python代码理解自组织映射神经网络'
- en: Brain-inspired unsupervised machine learning through competition, cooperation
    and adaptation
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过竞争、合作和适应进行的脑启发式无监督机器学习
- en: towardsdatascience.com](/understanding-self-organising-map-neural-network-with-python-code-7a77f501e985?source=post_page-----5d739ce21e9c--------------------------------)
    [](https://arxiv.org/abs/2005.11401?source=post_page-----5d739ce21e9c--------------------------------)
    [## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/understanding-self-organising-map-neural-network-with-python-code-7a77f501e985?source=post_page-----5d739ce21e9c--------------------------------)
    [](https://arxiv.org/abs/2005.11401?source=post_page-----5d739ce21e9c--------------------------------)
    [## 面向知识密集型自然语言处理任务的检索增强生成'
- en: Large pre-trained language models have been shown to store factual knowledge
    in their parameters, and achieve…
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型预训练语言模型已被证明能够在其参数中存储事实知识，并实现…
- en: arxiv.org](https://arxiv.org/abs/2005.11401?source=post_page-----5d739ce21e9c--------------------------------)
    [](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?source=post_page-----5d739ce21e9c--------------------------------)
    [## What Is Retrieval-Augmented Generation aka RAG?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: arxiv.org](https://arxiv.org/abs/2005.11401?source=post_page-----5d739ce21e9c--------------------------------)
    [](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?source=post_page-----5d739ce21e9c--------------------------------)
    [## 什么是检索增强生成（RAG）？
- en: Retrieval-augmented generation (RAG) is a technique for enhancing the accuracy
    and reliability of generative AI models…
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）是一种增强生成型人工智能模型准确性和可靠性的技术…
- en: blogs.nvidia.com](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?source=post_page-----5d739ce21e9c--------------------------------)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: blogs.nvidia.com](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?source=post_page-----5d739ce21e9c--------------------------------)
- en: '[https://www.sciencedirect.com/topics/engineering/self-organizing-map](https://www.sciencedirect.com/topics/engineering/self-organizing-map)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.sciencedirect.com/topics/engineering/self-organizing-map](https://www.sciencedirect.com/topics/engineering/self-organizing-map)'
- en: '[https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)'
- en: '[https://platform.openai.com/docs/guides/text-generation/chat-completions-api](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://platform.openai.com/docs/guides/text-generation/chat-completions-api](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)'
