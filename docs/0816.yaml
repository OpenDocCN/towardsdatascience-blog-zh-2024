- en: Named Entity Recognition Unmasked — The Essential Guide
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命名实体识别揭秘——必备指南
- en: 原文：[https://towardsdatascience.com/named-entity-recognition-unmasked-the-essential-guide-404ad0568964?source=collection_archive---------3-----------------------#2024-03-29](https://towardsdatascience.com/named-entity-recognition-unmasked-the-essential-guide-404ad0568964?source=collection_archive---------3-----------------------#2024-03-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/named-entity-recognition-unmasked-the-essential-guide-404ad0568964?source=collection_archive---------3-----------------------#2024-03-29](https://towardsdatascience.com/named-entity-recognition-unmasked-the-essential-guide-404ad0568964?source=collection_archive---------3-----------------------#2024-03-29)
- en: How to Extract Personal Information from Text Corpus Using NER Like a Pro
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何像专家一样使用NER从文本语料库中提取个人信息
- en: '[](https://rechitasingh.medium.com/?source=post_page---byline--404ad0568964--------------------------------)[![Rechitasingh](../Images/1ffc761b5ab3b365791a89923c210190.png)](https://rechitasingh.medium.com/?source=post_page---byline--404ad0568964--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--404ad0568964--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--404ad0568964--------------------------------)
    [Rechitasingh](https://rechitasingh.medium.com/?source=post_page---byline--404ad0568964--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rechitasingh.medium.com/?source=post_page---byline--404ad0568964--------------------------------)[![Rechitasingh](../Images/1ffc761b5ab3b365791a89923c210190.png)](https://rechitasingh.medium.com/?source=post_page---byline--404ad0568964--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--404ad0568964--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--404ad0568964--------------------------------)
    [Rechitasingh](https://rechitasingh.medium.com/?source=post_page---byline--404ad0568964--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--404ad0568964--------------------------------)
    ·9 min read·Mar 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--404ad0568964--------------------------------)
    ·9分钟阅读·2024年3月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bb7553bd263d44b692cfca95b7ceb39c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb7553bd263d44b692cfca95b7ceb39c.png)'
- en: Photo by [Christopher Gower](https://unsplash.com/@cgower?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Christopher Gower](https://unsplash.com/@cgower?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**Introduction**'
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: Okay, imagine this — you’ve got mountains of articles, journals, and blogs stuffed
    with information that you want to process. Now imagine you think it will be helpful
    for the community too if they get a chance to work with this data, HOWEVER, you
    wouldn’t want to share the data right away as it may contain certain personal
    information that shouldn’t be shared without the consent of those people.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，假设这样一种情况——你有大量的文章、期刊和博客，里面充满了你想处理的信息。现在假设你认为，如果社区能够有机会处理这些数据，可能对他们也会有所帮助，但是，你不想立即分享这些数据，因为它可能包含某些个人信息，未经这些人的同意不应共享。
- en: Since it’s not viable to ask for permission from all those people, you decide
    to use your skills and mask any personal information under FERPA guidelines. It
    is common for companies to mask their data when sharing it outside for analysis
    or demo purposes and it’s easier with numeric data. And here we want to do the
    same but with textual data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由于向所有这些人请求许可不可行，你决定运用自己的技能，在FERPA指南下掩盖任何个人信息。公司在将数据分享出去进行分析或演示时，通常会对数据进行掩盖，数字数据更容易做到这一点。我们在这里也想做同样的事情，但针对的是文本数据。
- en: Now here, since we are talking about text data, we will be employing a technique
    in Natural Language Processing (NLP). Enter Named Entity Recognition (NER), a
    trusty NLP detective unlocking those hidden data treasures. The purpose here is
    to identify the personal information.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在这里，由于我们讨论的是文本数据，我们将使用自然语言处理（NLP）中的一种技术。引入命名实体识别（NER），一个可靠的NLP侦探，揭示那些隐藏的数据宝藏。这里的目的是识别个人信息。
- en: Let’s dive deeper into how NER works, the concept behind the NER mechanism,
    ways to implement NER, which solution approach to pick from and why, and how to
    implement the solution to this problem in Python.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解NER是如何工作的，NER机制背后的概念，如何实现NER，选择哪种解决方案方法以及为什么选择它，以及如何在Python中实现这一问题的解决方案。
- en: '**Named Entity Recognition (NER): The Technical Breakdown**'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**命名实体识别（NER）：技术解析**'
- en: 'In simple terms, NER is all about teaching computers to spot those specific
    ‘entities’ within texts, in this case Personally Identifiable Information (PII).
    Picture it like giving your program a set of highlighters — one for names, one
    for places, one for companies, one for college, student ID, email address, or
    anything that can identify a person personally in this case and risk his/her identity.
    Here’s a peek at how NER works:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，NER就是教计算机在文本中识别特定的“实体”，在这种情况下是个人身份信息（PII）。你可以把它想象成给程序一套荧光笔——一种标记名字，一种标记地点，一种标记公司，一种标记大学、学生ID、电子邮件地址或任何能识别个人身份并危及其隐私的内容。下面是NER如何工作的一个简要介绍：
- en: '**Rule-based Systems:** The old-school approach. We create hand-written rules,
    like “a name usually starts with a capital letter.” Works decently for basic cases
    but can get extremely complex. Also, if you have many rules, it can become messier
    and messier'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于规则的系统：** 传统的方法。我们创建手写规则，例如“一个名字通常以大写字母开头”。对于基础案例有效，但可能变得非常复杂。此外，如果有很多规则，系统会变得越来越混乱。'
- en: '**Machine Learning Approach:** Statistical models learn from massive datasets.
    Think of it like showing your NER system tons of examples so it can find patterns
    on its own. That’s how machine learning works for everything. However, it can
    still have performance issues when it comes to textual data.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习方法：** 统计模型通过大量数据集进行学习。可以把它理解为向你的命名实体识别（NER）系统展示大量示例，让它自己找出模式。这就是机器学习在所有领域中的工作方式。然而，涉及文本数据时，它仍然可能存在性能问题。'
- en: '**Deep Learning Superstars:** Neural networks are the most famous model approach
    for textual, image, and video data-related problems. Handling complex language
    similar to what we humans do. These models understand context, making them incredibly
    accurate. The only condition here, you need to use lots and lots of data, or else
    the model will end up memorizing most of the training data (overfitting). Although
    there are techniques to control that, however, it still works best with a large
    corpus of data.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习明星：** 神经网络是解决文本、图像和视频数据相关问题的最著名模型方法。处理复杂语言，类似我们人类的方式。这些模型能够理解上下文，使其非常精确。唯一的条件是，你需要使用大量的数据，否则模型将大部分训练数据记住（过拟合）。虽然有控制过拟合的技术，但它仍然在大量数据的语料库中效果最好。'
- en: '**Techniques in Detail: The Brains Behind NER**'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**详细的技术分析：NER背后的“大脑”**'
- en: 'We’ve seen that NER can leverage various techniques, each with its strengths.
    Here’s a closer look under the hood:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，NER可以利用多种技术，每种技术都有其优势。下面是更深入的分析：
- en: '**Conditional Random Fields (CRFs):**'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**条件随机场（CRFs）：**'
- en: Imagine you’re teaching your NER system to spot locations. You show examples
    of addresses like “10 Made UP Street, London, UK.” CRFs excel at this because
    they look at the entire sequence of words and their relationships. They consider
    that “London” following a number and “UK” following a city strongly suggests a
    location entity. This makes CRFs powerful for tasks like NER where context is
    very crucial.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你在教NER系统识别位置。你展示像“10 Made UP Street, London, UK”这样的地址示例。CRFs在这方面表现出色，因为它们会查看整个词序列及其关系。它们考虑到“London”跟在数字后面，且“UK”跟在城市后面，这强烈暗示这是一个位置实体。这使得CRFs在像NER这样的任务中非常强大，因为上下文非常重要。
- en: 'Read this great article on TDS about CRF and the math behind it here: [Conditional
    Random Fields Explained](/conditional-random-fields-explained-e5b8256da776) by
    [Nikos Kafritsas](https://medium.com/u/bec849d9e1d2?source=post_page---user_mention--404ad0568964--------------------------------)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这篇关于CRF及其背后数学原理的精彩文章：[条件随机场解释](/conditional-random-fields-explained-e5b8256da776)，作者：[Nikos
    Kafritsas](https://medium.com/u/bec849d9e1d2?source=post_page---user_mention--404ad0568964--------------------------------)
- en: '**2\. LSTM Networks (Long Short-Term Memory):**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. LSTM网络（长短期记忆网络）：**'
- en: Let’s say you want to identify people’s names in text. LSTMs were a great invention
    after RNN because they have a special ability — a memory! Yes, they can hold memory
    or context and predict as per the context. Unlike CRFs (that only consider the
    current word), LSTMs can remember previous words in the sequence and not derail
    from context. This is important for NER. Why? Because that will help you understand
    whether it was Apple the company or Apple the fruit.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想识别文本中的人名。LSTM是继RNN之后的一项伟大发明，因为它们具有一种特殊的能力——记忆！是的，它们可以保持记忆或上下文，并根据上下文进行预测。与仅考虑当前单词的CRFs不同，LSTM可以记住序列中的前一个单词，并不会偏离上下文。这对NER非常重要。为什么？因为这将帮助你理解它是指“Apple”公司，还是指“Apple”水果。
- en: 'Let’s take another example: in the sentence “Dr. Smith is a renowned cardiologist,”
    an LSTM can remember the title “Dr.” and use that context to correctly classify
    “Smith” as a person’s name.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 再举一个例子：“史密斯博士是著名的心脏病专家”这句话中，LSTM能够记住头衔“博士”，并利用这个上下文正确地将“史密斯”分类为一个人名。
- en: 'Here’s a real-world example: Imagine you’re building a NEWS classifier model
    that categorizes articles based on the people mentioned. An LSTM-based NER system
    could surely identify entities like “Barack Obama” or “Elon Musk” even when their
    names appear within complex sentences and are classified. Great implementation,
    isn’t it?'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个现实世界的例子：假设你正在构建一个新闻分类模型，依据提到的人物来分类文章。基于LSTM的NER系统肯定可以识别诸如“巴拉克·奥巴马”或“Elon
    Musk”这样的实体，即使他们的名字出现在复杂的句子中并被正确分类。不错的实现，对吧？
- en: Read this article for a [Comprehensive Introduction to LSTM](/lstm-networks-a-detailed-explanation-8fae6aefc7f9)
    by [Rian Dolphin](https://medium.com/u/2f79d1e1bf6d?source=post_page---user_mention--404ad0568964--------------------------------)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这篇文章了解[Rian Dolphin](https://medium.com/u/2f79d1e1bf6d?source=post_page---user_mention--404ad0568964--------------------------------)的[《LSTM全面介绍》](/lstm-networks-a-detailed-explanation-8fae6aefc7f9)。
- en: '**3\. Transformers:**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. Transformers：**'
- en: Transformers are the current hot topic of NLP, and NER is no exception. These
    models use an attention mechanism, like focusing all your attention on a specific
    detail in a painting. What they do is, pay attention to relevant words across
    the entire sentence and not just nearby ones. Imagine it like a human looking
    at a text he/she hasn’t read before. We glace here and there, paying attention
    (spotlight kind of) to various sections and picking up meanings. This technique
    empowers them to understand complex relationships and identify even obscure entities.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers是当前NLP领域的热门话题，NER也不例外。这些模型使用一种注意力机制，就像你将所有注意力集中在画中的某个细节上一样。它们的做法是，关注整个句子中的相关词汇，而不仅仅是附近的词。可以想象成一个人看着他/她从未读过的文本。我们在各处扫视，聚焦（像聚光灯一样）不同的部分并捕捉意义。这种技术使得它们能够理解复杂的关系，甚至识别出那些不太显眼的实体。
- en: For example, consider the sentence “The CEO of Acme Corp, based in California,
    announced a new product launch.” A Transformer-based NER system can pay attention
    to “CEO” and “Acme Corp” even though they are separated by several words. It can
    then use this attention to correctly classify “Acme Corp” as an organization.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑这句话：“位于加利福尼亚的Acme公司首席执行官宣布了一款新产品的发布。” 基于Transformer的命名实体识别（NER）系统可以关注“CEO”和“Acme公司”，即使它们之间被几个词分开。然后，它可以利用这种关注来正确地将“Acme公司”分类为一个组织。
- en: This capability makes Transformers ideal for tasks like identifying medical
    terms in research papers or specific product names in social media data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种能力使得Transformer非常适合用于诸如在研究论文中识别医学术语或在社交媒体数据中识别特定产品名称等任务。
- en: 'Read this article to see what fine-tuning can do: [NER with Transformers and
    Spacy](/ner-with-transformers-and-spacy-b3240bc65eb4) by [James Briggs](https://jamescalam.medium.com/).
    And if you are still wondering about Attention, start by reading this article,
    it talks about Attention in detail: [All you need to know about ‘Attention’ and
    ‘Transformers’ — In-depth Understanding — Part 1](/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021)
    by [Arjun Sarkar](https://arjun-sarkar786.medium.com/)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这篇文章了解微调能做什么：[使用Transformers和Spacy进行NER](/ner-with-transformers-and-spacy-b3240bc65eb4)
    by [James Briggs](https://jamescalam.medium.com/)。如果你仍在疑惑Attention是什么，先阅读这篇文章，它详细讲解了Attention：[你需要了解的所有关于‘Attention’和‘Transformers’的信息
    — 深入理解 — 第一部分](/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021)
    by [Arjun Sarkar](https://arjun-sarkar786.medium.com/)
- en: '**Beyond the Basics: Emerging Techniques**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**超越基础：新兴技术**'
- en: 'The world of NER is constantly evolving. Here are some exciting developments
    to keep an eye on:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: NER领域正在不断发展。以下是一些值得关注的令人兴奋的发展：
- en: '**Bidirectional LSTMs (BiLSTMs):** These are LSTMs on steroids, processing
    text forwards and backward. This grants them an even deeper understanding of context.
    It can have drawbacks because you can’t predict using this, as you feed the sentence
    both forwards and backward. So the system knows about the context.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双向LSTM（BiLSTM）：** 这些是升级版的LSTM，能够正向和反向处理文本。这使得它们能够更深入地理解上下文。它也有一些缺点，因为你无法使用它来进行预测，因为你要同时将句子正向和反向输入。所以系统能知道上下文。'
- en: '**Named Entity Disambiguation (NED):** Again, let’s take the Apple example.
    Imagine you see the name “Apple” in the text. Is it the tech giant or a reference
    to the fruit? NER can be coupled with NED to identify the most likely meaning
    in context.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体消歧（NED）：** 再次以苹果为例。假设你在文本中看到了“Apple”这个名字。是指科技巨头还是指水果？NER可以与NED结合，以识别在上下文中最可能的含义。'
- en: By understanding these techniques and staying updated on the latest advancements,
    you can use the power of NER to unlock valuable personal information from text
    data and fuel your research endeavors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解这些技术并保持对最新进展的更新，你可以利用NER的力量从文本数据中提取有价值的个人信息，并推动你的研究工作。
- en: '**NER in Action: Your Project Code Sneak Peek**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**命名实体识别（NER）实践：你的项目代码一瞥**'
- en: 'It''s time for some hands-on stuff! Let’s say you’re using Python and the awesome
    spaCy library:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候动手实践了！假设你正在使用Python和强大的spaCy库：
- en: Python
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Output
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: '![](../Images/08aa2028b9eb4b43c75316e60c4e60fc.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08aa2028b9eb4b43c75316e60c4e60fc.png)'
- en: Author’s image — Output of the above code
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的图片 — 上述代码的输出
- en: You can see above that although NER was able to pick up the Name and Organization
    perfectly, it missed out on their email addresses. Let’s explore why that may
    be the case and how to correct it.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，虽然NER能够完美识别人名和组织名，但却漏掉了它们的电子邮件地址。让我们探索一下为什么会这样以及如何解决。
- en: '**Reasons for Missing Email Addresses**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**漏掉电子邮件地址的原因**'
- en: '**NER Model Limitations:** Standard NER models are usually trained on categories
    like names, organizations, locations, etc. While they *might* pick up on some
    email patterns, it’s not their primary strength. Hence it missed it in this case'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**NER模型的局限性：** 标准的NER模型通常训练于诸如人名、组织、地点等类别。虽然它们*可能*会识别一些电子邮件模式，但这并不是它们的主要强项。因此，在本例中它漏掉了电子邮件地址。'
- en: '**Email Address Complexity:** Email formats can be surprisingly diverse. Simple
    ones like “Gmail and Yahoo” might be recognized, but more intricate patterns may
    be missed. Say, it can pick up gmail IDs but may miss out on some organizational-specific
    IDs. Again, this is what happened in this case.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**电子邮件地址的复杂性：** 电子邮件格式可能出奇地多样。像“Gmail”和“Yahoo”这样的简单格式可能会被识别，但更复杂的模式可能会被漏掉。例如，它可能识别gmail
    ID，但可能遗漏一些组织特定的ID。再次强调，这就是在本例中发生的情况。'
- en: Although we know the reason, we may be more focused on how to correct the problem
    here!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们知道原因，但我们可能更关注如何解决问题！
- en: 'Let’s use one of the three techniques mentioned below to personalize and solve
    the task at hand:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下三种技术中的一种来个性化并解决当前任务：
- en: '**Regular Expressions (Regex):** Regex allows crafting specific patterns to
    match email addresses. This was developed way back and it is used a lot. You can
    think of it as somewhat hard coding to recognize a pattern in programming. Here’s
    a basic example:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则表达式（Regex）：** 正则表达式允许构建特定的模式来匹配电子邮件地址。这一方法很早就被开发出来并且被广泛使用。你可以将它视为一种在编程中识别模式的硬编码方式。以下是一个基本示例：'
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Specialized Libraries:** You can use libraries like email_validator. These
    are dedicated to the task of email identification and validation if validating
    email is your use case.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专门化库：** 你可以使用像email_validator这样的库。这些库专门用于电子邮件识别和验证，如果验证电子邮件是你的用例。'
- en: '**Enhancing Your NER Model:** You could fine-tune your existing model by providing
    examples of email addresses as an additional entity type. However, it will require
    more data and potentially more complex model training. This includes using pre-trained
    models like BERT etc. Again, read this article [NER with Transformers and Spacy](/ner-with-transformers-and-spacy-b3240bc65eb4)
    by [James Briggs](https://jamescalam.medium.com/). It talks about fine-tuning
    roBERTA and using spaCy. And this option will become clearer to you.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强你的NER模型：** 你可以通过提供电子邮件地址作为额外的实体类型来微调你现有的模型。然而，这将需要更多的数据，并且可能需要更复杂的模型训练。这包括使用像BERT这样的预训练模型等。再次，阅读这篇文章[NER与Transformers和Spacy](/ner-with-transformers-and-spacy-b3240bc65eb4)由[James
    Briggs](https://jamescalam.medium.com/)撰写。它讨论了如何微调roBERTA并使用spaCy。这样，你会更加清楚这一选项。'
- en: 'Let’s pick the first approach for demonstration purposes and implement it into
    the code. We could add a section dedicated to email extraction as below:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，我们选择第一种方法并将其实现到代码中。我们可以增加一个专门提取电子邮件的部分，如下所示：
- en: Python
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Python
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Output
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: '![](../Images/33562dd9d9f290a94fe26a1527f83026.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33562dd9d9f290a94fe26a1527f83026.png)'
- en: Author’s image — Output of the above code
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的图片 — 上述代码的输出
- en: '**Choosing the Best Approach**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**选择最佳方法**'
- en: 'The ideal solution depends on your specific project:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的解决方案取决于你的具体项目：
- en: '**Simple Emails + Accuracy:** Regex is likely sufficient.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单的电子邮件 + 准确性：** 正则表达式可能足够了。'
- en: '**Complex Emails + Reliability:** A specialized email validation library is
    safest.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂的电子邮件 + 可靠性：** 专门的电子邮件验证库是最安全的选择。'
- en: '**Extensive NER Retraining:** If NER accuracy for other entities is vital,
    and you have lots of email-focused data, retraining your model could be the long-term
    solution. You guessed it right, you use those fancy techniques like using a BERT
    pre-trained model by fine-tuning and using it for your problem. Some important
    considerations before using this would be:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广泛的NER再训练：** 如果其他实体的NER准确性很重要，并且你有大量以电子邮件为重点的数据，重新训练你的模型可能是长期解决方案。你没猜错，你可以使用一些先进的技术，如通过微调BERT预训练模型，并将其应用于你的问题。使用这些技术之前需要考虑一些重要事项：'
- en: '- **Data:** Fine-tuning often requires a decent amount of labeled data. If
    you have limited data, other techniques (like regex) might be initially more practical.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- **数据：** 微调通常需要相当数量的标注数据。如果你的数据有限，其他技术（如正则表达式）可能一开始更实用。'
- en: '- **Complexity:** Fine-tuning involves more setup and potentially more computational
    resources than using regex or basic libraries.'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- **复杂性：** 微调涉及更多的设置，并且可能需要比使用正则表达式或基础库更多的计算资源。'
- en: '**Resources to Power Up Your NER Skills**'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**提升你的NER技能的资源**'
- en: '**spaCy:** Stellar NLP library with excellent NER support ([https://spacy.io/](https://spacy.io/)).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**spaCy：** 出色的NLP库，提供卓越的NER支持 ([https://spacy.io/](https://spacy.io/))。'
- en: '**NLTK:** The classic NLP toolkit ([https://www.nltk.org/](https://www.nltk.org/)).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NLTK：** 经典的NLP工具包 ([https://www.nltk.org/](https://www.nltk.org/))。'
- en: '**Stanford CoreNLP:** Powerful suite of NLP tools ([https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/))'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**斯坦福CoreNLP：** 强大的NLP工具套件 ([https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/))'
- en: '**The Road Ahead**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**前路如何**'
- en: NER is still a hot research area — get ready for models that grasp complex relationships,
    spot custom entities, and work across languages! This tech is revolutionizing
    how we extract and use information from the vast sea of text around us.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: NER仍然是一个热门的研究领域——准备迎接那些能理解复杂关系、识别自定义实体并跨语言工作的模型吧！这项技术正在革新我们从浩瀚的文本海洋中提取和利用信息的方式。
- en: '**Feel free to request even more technical details or real-world examples to
    add to the article. Let’s make this a powerhouse guide to NER!**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**随时提出更技术性的问题或真实世界的例子，帮助我们完善这篇关于命名实体识别（NER）的指南！**'
- en: About The Author
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于作者
- en: '*Dear Readers, I am passionate about the subject and love writing about data
    science topics and food for thought articles. The most important thing is, that
    I’m open to feedback!*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*亲爱的读者们，我对这个话题充满热情，喜欢写关于数据科学的文章和发人深省的文章。最重要的是，我愿意接受反馈！*'
- en: '*I’d love to know your comments. If this article helped you in any way, or
    you have feedback, please don’t hesitate to drop a note! Also, feel free to drop
    a comment if you want further explanation of the topic and I’ll try to solve it
    here, or write another article on it!*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*我很想知道你的评论。如果这篇文章对你有帮助，或者你有任何反馈，请不要犹豫，留言告诉我！如果你希望进一步解释某个话题，也可以留言，我会尽力在这里解答，或者写另一篇相关文章！*'
- en: '*To know more about me, I have a small article right here for you:*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果想了解更多关于我的信息，我这里有一篇小文章供你阅读：*'
- en: '[](https://medium.com/about-me-stories/about-me-rechita-singh-745f9f0511ad?source=post_page-----404ad0568964--------------------------------)
    [## About Me — Rechita Singh'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/about-me-stories/about-me-rechita-singh-745f9f0511ad?source=post_page-----404ad0568964--------------------------------)
    [## 关于我 — Rechita Singh'
- en: Data Wizard, Mind Explorer, Mastering Art of Poor Jokes along the way
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据奇才，思维探索者，在这过程中精通制作烂笑话的艺术
- en: medium.com](https://medium.com/about-me-stories/about-me-rechita-singh-745f9f0511ad?source=post_page-----404ad0568964--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/about-me-stories/about-me-rechita-singh-745f9f0511ad?source=post_page-----404ad0568964--------------------------------)'
- en: '*Let’s get Chatting…*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*让我们开始聊天吧……*'
