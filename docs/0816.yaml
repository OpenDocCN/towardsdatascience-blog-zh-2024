- en: Named Entity Recognition Unmasked — The Essential Guide
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/named-entity-recognition-unmasked-the-essential-guide-404ad0568964?source=collection_archive---------3-----------------------#2024-03-29](https://towardsdatascience.com/named-entity-recognition-unmasked-the-essential-guide-404ad0568964?source=collection_archive---------3-----------------------#2024-03-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to Extract Personal Information from Text Corpus Using NER Like a Pro
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rechitasingh.medium.com/?source=post_page---byline--404ad0568964--------------------------------)[![Rechitasingh](../Images/1ffc761b5ab3b365791a89923c210190.png)](https://rechitasingh.medium.com/?source=post_page---byline--404ad0568964--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--404ad0568964--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--404ad0568964--------------------------------)
    [Rechitasingh](https://rechitasingh.medium.com/?source=post_page---byline--404ad0568964--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--404ad0568964--------------------------------)
    ·9 min read·Mar 29, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb7553bd263d44b692cfca95b7ceb39c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Christopher Gower](https://unsplash.com/@cgower?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Okay, imagine this — you’ve got mountains of articles, journals, and blogs stuffed
    with information that you want to process. Now imagine you think it will be helpful
    for the community too if they get a chance to work with this data, HOWEVER, you
    wouldn’t want to share the data right away as it may contain certain personal
    information that shouldn’t be shared without the consent of those people.
  prefs: []
  type: TYPE_NORMAL
- en: Since it’s not viable to ask for permission from all those people, you decide
    to use your skills and mask any personal information under FERPA guidelines. It
    is common for companies to mask their data when sharing it outside for analysis
    or demo purposes and it’s easier with numeric data. And here we want to do the
    same but with textual data.
  prefs: []
  type: TYPE_NORMAL
- en: Now here, since we are talking about text data, we will be employing a technique
    in Natural Language Processing (NLP). Enter Named Entity Recognition (NER), a
    trusty NLP detective unlocking those hidden data treasures. The purpose here is
    to identify the personal information.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive deeper into how NER works, the concept behind the NER mechanism,
    ways to implement NER, which solution approach to pick from and why, and how to
    implement the solution to this problem in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '**Named Entity Recognition (NER): The Technical Breakdown**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In simple terms, NER is all about teaching computers to spot those specific
    ‘entities’ within texts, in this case Personally Identifiable Information (PII).
    Picture it like giving your program a set of highlighters — one for names, one
    for places, one for companies, one for college, student ID, email address, or
    anything that can identify a person personally in this case and risk his/her identity.
    Here’s a peek at how NER works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rule-based Systems:** The old-school approach. We create hand-written rules,
    like “a name usually starts with a capital letter.” Works decently for basic cases
    but can get extremely complex. Also, if you have many rules, it can become messier
    and messier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine Learning Approach:** Statistical models learn from massive datasets.
    Think of it like showing your NER system tons of examples so it can find patterns
    on its own. That’s how machine learning works for everything. However, it can
    still have performance issues when it comes to textual data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Learning Superstars:** Neural networks are the most famous model approach
    for textual, image, and video data-related problems. Handling complex language
    similar to what we humans do. These models understand context, making them incredibly
    accurate. The only condition here, you need to use lots and lots of data, or else
    the model will end up memorizing most of the training data (overfitting). Although
    there are techniques to control that, however, it still works best with a large
    corpus of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Techniques in Detail: The Brains Behind NER**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve seen that NER can leverage various techniques, each with its strengths.
    Here’s a closer look under the hood:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional Random Fields (CRFs):**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Imagine you’re teaching your NER system to spot locations. You show examples
    of addresses like “10 Made UP Street, London, UK.” CRFs excel at this because
    they look at the entire sequence of words and their relationships. They consider
    that “London” following a number and “UK” following a city strongly suggests a
    location entity. This makes CRFs powerful for tasks like NER where context is
    very crucial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read this great article on TDS about CRF and the math behind it here: [Conditional
    Random Fields Explained](/conditional-random-fields-explained-e5b8256da776) by
    [Nikos Kafritsas](https://medium.com/u/bec849d9e1d2?source=post_page---user_mention--404ad0568964--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. LSTM Networks (Long Short-Term Memory):**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you want to identify people’s names in text. LSTMs were a great invention
    after RNN because they have a special ability — a memory! Yes, they can hold memory
    or context and predict as per the context. Unlike CRFs (that only consider the
    current word), LSTMs can remember previous words in the sequence and not derail
    from context. This is important for NER. Why? Because that will help you understand
    whether it was Apple the company or Apple the fruit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take another example: in the sentence “Dr. Smith is a renowned cardiologist,”
    an LSTM can remember the title “Dr.” and use that context to correctly classify
    “Smith” as a person’s name.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a real-world example: Imagine you’re building a NEWS classifier model
    that categorizes articles based on the people mentioned. An LSTM-based NER system
    could surely identify entities like “Barack Obama” or “Elon Musk” even when their
    names appear within complex sentences and are classified. Great implementation,
    isn’t it?'
  prefs: []
  type: TYPE_NORMAL
- en: Read this article for a [Comprehensive Introduction to LSTM](/lstm-networks-a-detailed-explanation-8fae6aefc7f9)
    by [Rian Dolphin](https://medium.com/u/2f79d1e1bf6d?source=post_page---user_mention--404ad0568964--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Transformers:**'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers are the current hot topic of NLP, and NER is no exception. These
    models use an attention mechanism, like focusing all your attention on a specific
    detail in a painting. What they do is, pay attention to relevant words across
    the entire sentence and not just nearby ones. Imagine it like a human looking
    at a text he/she hasn’t read before. We glace here and there, paying attention
    (spotlight kind of) to various sections and picking up meanings. This technique
    empowers them to understand complex relationships and identify even obscure entities.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider the sentence “The CEO of Acme Corp, based in California,
    announced a new product launch.” A Transformer-based NER system can pay attention
    to “CEO” and “Acme Corp” even though they are separated by several words. It can
    then use this attention to correctly classify “Acme Corp” as an organization.
  prefs: []
  type: TYPE_NORMAL
- en: This capability makes Transformers ideal for tasks like identifying medical
    terms in research papers or specific product names in social media data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read this article to see what fine-tuning can do: [NER with Transformers and
    Spacy](/ner-with-transformers-and-spacy-b3240bc65eb4) by [James Briggs](https://jamescalam.medium.com/).
    And if you are still wondering about Attention, start by reading this article,
    it talks about Attention in detail: [All you need to know about ‘Attention’ and
    ‘Transformers’ — In-depth Understanding — Part 1](/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021)
    by [Arjun Sarkar](https://arjun-sarkar786.medium.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Beyond the Basics: Emerging Techniques**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The world of NER is constantly evolving. Here are some exciting developments
    to keep an eye on:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bidirectional LSTMs (BiLSTMs):** These are LSTMs on steroids, processing
    text forwards and backward. This grants them an even deeper understanding of context.
    It can have drawbacks because you can’t predict using this, as you feed the sentence
    both forwards and backward. So the system knows about the context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Named Entity Disambiguation (NED):** Again, let’s take the Apple example.
    Imagine you see the name “Apple” in the text. Is it the tech giant or a reference
    to the fruit? NER can be coupled with NED to identify the most likely meaning
    in context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By understanding these techniques and staying updated on the latest advancements,
    you can use the power of NER to unlock valuable personal information from text
    data and fuel your research endeavors.
  prefs: []
  type: TYPE_NORMAL
- en: '**NER in Action: Your Project Code Sneak Peek**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It''s time for some hands-on stuff! Let’s say you’re using Python and the awesome
    spaCy library:'
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08aa2028b9eb4b43c75316e60c4e60fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Author’s image — Output of the above code
  prefs: []
  type: TYPE_NORMAL
- en: You can see above that although NER was able to pick up the Name and Organization
    perfectly, it missed out on their email addresses. Let’s explore why that may
    be the case and how to correct it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reasons for Missing Email Addresses**'
  prefs: []
  type: TYPE_NORMAL
- en: '**NER Model Limitations:** Standard NER models are usually trained on categories
    like names, organizations, locations, etc. While they *might* pick up on some
    email patterns, it’s not their primary strength. Hence it missed it in this case'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Email Address Complexity:** Email formats can be surprisingly diverse. Simple
    ones like “Gmail and Yahoo” might be recognized, but more intricate patterns may
    be missed. Say, it can pick up gmail IDs but may miss out on some organizational-specific
    IDs. Again, this is what happened in this case.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Although we know the reason, we may be more focused on how to correct the problem
    here!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use one of the three techniques mentioned below to personalize and solve
    the task at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regular Expressions (Regex):** Regex allows crafting specific patterns to
    match email addresses. This was developed way back and it is used a lot. You can
    think of it as somewhat hard coding to recognize a pattern in programming. Here’s
    a basic example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Specialized Libraries:** You can use libraries like email_validator. These
    are dedicated to the task of email identification and validation if validating
    email is your use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhancing Your NER Model:** You could fine-tune your existing model by providing
    examples of email addresses as an additional entity type. However, it will require
    more data and potentially more complex model training. This includes using pre-trained
    models like BERT etc. Again, read this article [NER with Transformers and Spacy](/ner-with-transformers-and-spacy-b3240bc65eb4)
    by [James Briggs](https://jamescalam.medium.com/). It talks about fine-tuning
    roBERTA and using spaCy. And this option will become clearer to you.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s pick the first approach for demonstration purposes and implement it into
    the code. We could add a section dedicated to email extraction as below:'
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33562dd9d9f290a94fe26a1527f83026.png)'
  prefs: []
  type: TYPE_IMG
- en: Author’s image — Output of the above code
  prefs: []
  type: TYPE_NORMAL
- en: '**Choosing the Best Approach**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ideal solution depends on your specific project:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simple Emails + Accuracy:** Regex is likely sufficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex Emails + Reliability:** A specialized email validation library is
    safest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extensive NER Retraining:** If NER accuracy for other entities is vital,
    and you have lots of email-focused data, retraining your model could be the long-term
    solution. You guessed it right, you use those fancy techniques like using a BERT
    pre-trained model by fine-tuning and using it for your problem. Some important
    considerations before using this would be:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- **Data:** Fine-tuning often requires a decent amount of labeled data. If
    you have limited data, other techniques (like regex) might be initially more practical.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- **Complexity:** Fine-tuning involves more setup and potentially more computational
    resources than using regex or basic libraries.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Resources to Power Up Your NER Skills**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**spaCy:** Stellar NLP library with excellent NER support ([https://spacy.io/](https://spacy.io/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NLTK:** The classic NLP toolkit ([https://www.nltk.org/](https://www.nltk.org/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stanford CoreNLP:** Powerful suite of NLP tools ([https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Road Ahead**'
  prefs: []
  type: TYPE_NORMAL
- en: NER is still a hot research area — get ready for models that grasp complex relationships,
    spot custom entities, and work across languages! This tech is revolutionizing
    how we extract and use information from the vast sea of text around us.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feel free to request even more technical details or real-world examples to
    add to the article. Let’s make this a powerhouse guide to NER!**'
  prefs: []
  type: TYPE_NORMAL
- en: About The Author
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Dear Readers, I am passionate about the subject and love writing about data
    science topics and food for thought articles. The most important thing is, that
    I’m open to feedback!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*I’d love to know your comments. If this article helped you in any way, or
    you have feedback, please don’t hesitate to drop a note! Also, feel free to drop
    a comment if you want further explanation of the topic and I’ll try to solve it
    here, or write another article on it!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*To know more about me, I have a small article right here for you:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/about-me-stories/about-me-rechita-singh-745f9f0511ad?source=post_page-----404ad0568964--------------------------------)
    [## About Me — Rechita Singh'
  prefs: []
  type: TYPE_NORMAL
- en: Data Wizard, Mind Explorer, Mastering Art of Poor Jokes along the way
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/about-me-stories/about-me-rechita-singh-745f9f0511ad?source=post_page-----404ad0568964--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Let’s get Chatting…*'
  prefs: []
  type: TYPE_NORMAL
