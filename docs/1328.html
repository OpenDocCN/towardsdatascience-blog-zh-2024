<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Simple Ways to Speed Up Your PyTorch Model Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Simple Ways to Speed Up Your PyTorch Model Training</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/simple-ways-to-speed-up-your-pytorch-model-training-9c9d4899313d?source=collection_archive---------3-----------------------#2024-05-28">https://towardsdatascience.com/simple-ways-to-speed-up-your-pytorch-model-training-9c9d4899313d?source=collection_archive---------3-----------------------#2024-05-28</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="51ec" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">If all machine learning engineers want one thing, <strong class="al">it‚Äôs faster model training</strong> ‚Äî maybe after good test metrics</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Alex Dremov" class="l ep by dd de cx" src="../Images/8afeaa6bae03d3b6c436d81127c75a0c.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*HIMoY5jJZXzIWCCsvNLwwA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------" rel="noopener follow">Alex Dremov</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">May 28, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/eb75109bbcb6622409f76f7e507791cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CbFxa1v7Mq4YSRdQ"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@julianhochgesang?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit" rel="noopener ugc nofollow" target="_blank">Julian Hochgesang</a> / <a class="af nb" href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="810f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Does this topic even need an introduction?</p><p id="88d9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Speeding up machine learning model training is one thing that all machine learning engineers want. Faster training equals faster experiments equals faster iterations for your product. Also, it means that one model training will require fewer resources. So, straight to the point</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="c7f1" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Containerization</h1><p id="9be0" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Yes, this will not speed up your training on its own. But this targets another important aspect ‚Äî reproducibility. Sometimes virtualenv with fixed library versions is enough, but I encourage you to take one step further and build an all-in-one docker container for your model training.</p><p id="5552" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This ensures that the environment is fully consistent during debugging, profiling, and final training. The last thing you want is to optimize a part of code that is no longer a bottleneck due to python12 speed up, for example. Or even a bug that is not reproducible on different CUDA versions.</p><p id="ca8d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As a starting point, you can use pre-built images from NVIDIA. They already have CUDA, PyTorch, and other popular libs installed:</p><div class="ph pi pj pk pl pm"><a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch?ref=alexdremov.me&amp;source=post_page-----9c9d4899313d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab ig"><div class="po ab co cb pp pq"><h2 class="bf fr hw z io pr iq ir ps it iv fp bk">PyTorch | NVIDIA NGC</h2><div class="pt l"><h3 class="bf b hw z io pr iq ir ps it iv dx">PyTorch is a GPU accelerated tensor computational framework. Functionality can be extended with common Python libraries‚Ä¶</h3></div><div class="pu l"><p class="bf b dy z io pr iq ir ps it iv dx">catalog.ngc.nvidia.com</p></div></div></div></a></div><blockquote class="pv pw px"><p id="5544" class="nc nd py ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">üí° A Docker container is the ultimate solution for problems like<br/>‚ÄúHey, it works on my machine. I have no idea why it doesn‚Äôt on yours.‚Äù</p></blockquote></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="a88f" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Get comfortable with PyTorch profiler</h1><p id="2351" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Before optimizing anything, you have to understand how long some parts of your code run. Pytorch profiler is <em class="py">almost</em> an all-in-one tool for profiling training. It‚Äôs able to record:</p><ul class=""><li id="5a3b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pz qa qb bk">CPU operations timings</li><li id="2dd1" class="nc nd fq ne b go qc ng nh gr qd nj nk nl qe nn no np qf nr ns nt qg nv nw nx pz qa qb bk">CUDA kernels timings</li><li id="cbee" class="nc nd fq ne b go qc ng nh gr qd nj nk nl qe nn no np qf nr ns nt qg nv nw nx pz qa qb bk">Memory consumption history</li></ul><p id="ccdc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">That‚Äôs all you need. And it‚Äôs easy to enable!</p><p id="9716" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To record events, all you need is to embed training into a profiler context like this:</p><pre class="ml mm mn mo mp qh qi qj bp qk bb bk"><span id="3f98" class="ql oh fq qi b bg qm qn l qo qp">import torch.autograd.profiler as profiler<br/><br/>with profiler.profile(<br/>  activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],<br/>  on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs'),<br/>) as prof:<br/>  train(args)</span></pre><p id="7cb8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After that, you can launch the tensorboard and view profiling traces. Do not forget to install <a class="af nb" href="https://pypi.org/project/torch-tb-profiler/?ref=alexdremov.me" rel="noopener ugc nofollow" target="_blank">torch-tb-profiler</a>.</p><div class="ph pi pj pk pl pm"><a href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me&amp;source=post_page-----9c9d4899313d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab ig"><div class="po ab co cb pp pq"><h2 class="bf fr hw z io pr iq ir ps it iv fp bk">PyTorch Profiler With TensorBoard - PyTorch Tutorials 2.3.0+cu121 documentation</h2><div class="pt l"><h3 class="bf b hw z io pr iq ir ps it iv dx">Prepare the data and model Use profiler to record execution events Run the profiler Use TensorBoard to view results and‚Ä¶</h3></div><div class="pu l"><p class="bf b dy z io pr iq ir ps it iv dx">pytorch.org</p></div></div><div class="qq l"><div class="qr l qs qt qu qq qv lq pm"/></div></div></a></div><p id="d209" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Profiler has a lot of different options, but the most important are <code class="cx qw qx qy qi b">activities</code> and <code class="cx qw qx qy qi b">profile_memory</code>. You can experiment with other options, but keep in mind a simple rule: <strong class="ne fr">the fewer options you've enabled, the less overhead you have</strong>.</p><p id="e583" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, if you want to profile CUDA kernel execution timings, it is a good idea to turn off CPU profiling and all other features. In this mode, profiling will be as close to the real execution as possible.</p><p id="0c92" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To make traces easier to understand, consider adding profiling contexts that describe core parts of your code. If profiling is not enabled, those are no-op.</p><pre class="ml mm mn mo mp qh qi qj bp qk bb bk"><span id="5e50" class="ql oh fq qi b bg qm qn l qo qp">with profiler.record_function("forward_pass"):<br/>  result = model(**batch)<br/><br/>with profiler.record_function("train_step"):<br/>  step(**result)</span></pre><p id="de73" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This way, the labels that you use will be visible in traces. So, it will be easier to identify code blocks. Or even more granular inside mode‚Äôs forward:</p><pre class="ml mm mn mo mp qh qi qj bp qk bb bk"><span id="e6af" class="ql oh fq qi b bg qm qn l qo qp">with profiler.record_function("transformer_layer:self_attention"):<br/>  data = self.self_attention(**data)<br/><br/>...<br/><br/>with profiler.record_function("transformer_layer:encoder_attention"):<br/>  data = self.encoder_attention(**data, **encoder_data)</span></pre></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9262" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Understanding PyTorch traces</h1><p id="07ba" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">After you gather traces, open them in the tensorboard. That‚Äôs what the CPU + CUDA profile looks like:</p></div></div><div class="mq bh"><figure class="ml mm mn mo mp mq bh paragraph-image"><img src="../Images/b207f5f91e4560da06402145f3cfb486.png" data-original-src="https://miro.medium.com/v2/resize:fit:4172/format:webp/1*docJYzaTa0IXhTDgLv5QoA.png"/><figcaption class="mw mx my mi mj mz na bf b bg z dx">¬© Copyright 2024, PyTorch | <a class="af nb" href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html</a></figcaption></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e8a4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Straight away, find the core parts of any training:</p><ul class=""><li id="80f4" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pz qa qb bk">data loading</li><li id="10db" class="nc nd fq ne b go qc ng nh gr qd nj nk nl qe nn no np qf nr ns nt qg nv nw nx pz qa qb bk">forward pass</li><li id="9e05" class="nc nd fq ne b go qc ng nh gr qd nj nk nl qe nn no np qf nr ns nt qg nv nw nx pz qa qb bk">backward pass</li></ul><p id="d46f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Backward pass is handled by PyTorch in a separate thread (thread 16893 on the image above), so it is easy to identify.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="7cba" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Data loading</h1><p id="d758" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">For data loading, we want near-zero timings.</p><p id="6289" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">No compromises.</p><p id="7ec0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">That‚Äôs because during data loading GPU does nothing, which under-utilizes available resources. However, data processing can be overlapped with GPU computing as those are independent parts.</p><p id="cde3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can easily identify areas where GPU is idle ‚Äî just look at <em class="py">GPU Est. SM Efficiency</em> and <em class="py">GPU Utilization</em> figures in the profiler‚Äôs trace. Areas with zero activity are our patients. That‚Äôs where GPU does nothing.</p><p id="c6bc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A simple solution for that is:</p><ul class=""><li id="07ca" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pz qa qb bk">process data in the background process (no GIL)</li><li id="132f" class="nc nd fq ne b go qc ng nh gr qd nj nk nl qe nn no np qf nr ns nt qg nv nw nx pz qa qb bk">process data augmentations and transforms in parallel processes</li></ul><p id="6d62" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you use PyTorch DataLoader, then it can be easily achieved by specifying <code class="cx qw qx qy qi b">num_workers</code>. It's more complicated if you use <code class="cx qw qx qy qi b">IterableDataset</code>, as then data will be duplicated. However, this issue still can be solved by using <a class="af nb" href="https://pytorch.org/docs/stable/data.html?ref=alexdremov.me#torch.utils.data.IterableDataset" rel="noopener ugc nofollow" target="_blank">get_worker_info()</a> - you need to adjust iteration in a way so that each worker receives different, non-intersecting rows.</p><p id="9e18" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For more configurable processing, you may consider implementing multi-process transforms yourself with <code class="cx qw qx qy qi b">multiprocessing</code></p><blockquote class="pv pw px"><p id="161e" class="nc nd py ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">üí° If you never checked your code‚Äôs data processing speed, then this slight modification can yield <strong class="ne fr">dramatic speedups</strong></p></blockquote></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6492" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Making friends with memory allocator</h1><p id="71e2" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">You want to be friends with PyTorch‚Äôs CUDA caching allocator.</p><p id="c309" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When you allocate tensors with PyTorch on a CUDA device, PyTorch will use a caching allocator. That‚Äôs because <code class="cx qw qx qy qi b">cudaMalloc</code>/ <code class="cx qw qx qy qi b">cudaFree</code> are expensive operations that we want to avoid, so PyTorch has its allocator that will try to reuse previously allocated through <code class="cx qw qx qy qi b">cudaMalloc</code> blocks. That is, if PyTorch's allocator has an appropriate block available, it will give it straight away without calling <code class="cx qw qx qy qi b">cudaMalloc</code>. That way, <code class="cx qw qx qy qi b">cudaMalloc</code> is called only at the beginning.</p><p id="c16c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, if you‚Äôre dealing with data of variable length, different forward passes will require intermediate tensors of different sizes. So, PyTorch‚Äôs allocator may not have an appropriate block of data available. In this case, the allocator panics and releases allocated previously bocks by calling <code class="cx qw qx qy qi b">cudaFree</code> to free up space for new allocations.</p><p id="ae55" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After that, the allocator starts building its cache again, doing tons of <code class="cx qw qx qy qi b">cudaMalloc</code>, which is an expensive operation. You can spot this problem by looking at the memory profiler section of the tensorboard profiler viewer.</p><blockquote class="pv pw px"><p id="b902" class="nc nd py ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">üí° You also can spot this problem in the traces. It will be visible as calls to <code class="cx qw qx qy qi b">cudaMalloc</code>and <code class="cx qw qx qy qi b">cudaFree</code></p></blockquote></div></div><div class="mq"><div class="ab cb"><div class="ll qz lm ra ln rb cf rc cg rd ci bh"><figure class="ml mm mn mo mp mq re rf paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/414070253ac9c94f59f8f522399d7fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*glGIB0gdoQOAw7Dpflp2uQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">PyTorch allocator freaks out | Image by the author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="be7e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As you see, a red line that corresponds to the allocator‚Äôs reserved memory constantly changes. That means that PyTorch allocator is not able to efficiently handle allocation requests.</p><p id="4aed" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When allocations are handled without the allocator panicking, the red line is completely straight</p></div></div><div class="mq"><div class="ab cb"><div class="ll qz lm ra ln rb cf rc cg rd ci bh"><figure class="ml mm mn mo mp mq re rf paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/e8e801553645665ba7dff138173a00cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*cnclQLarMTifDCbc9_MBrw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">PyTorch allocator works as expected | Image by the author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="ceaa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As I said, that is usually due to variable shapes of tensors. How to fix that?</p><h2 id="d129" class="rg oh fq bf oi rh ri rj ol rk rl rm oo nl rn ro rp np rq rr rs nt rt ru rv rw bk"><strong class="al">Expandable Segments</strong></h2><p id="232e" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">The first thing that is worth trying is to set PyTorch‚Äôs relatively new allocator mode:</p><pre class="ml mm mn mo mp qh qi qj bp qk bb bk"><span id="f26c" class="ql oh fq qi b bg qm qn l qo qp">PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"</span></pre><blockquote class="pv pw px"><p id="3a3a" class="nc nd py ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="fq">If set to </em><code class="cx qw qx qy qi b"><em class="fq">True</em></code><em class="fq">, this setting instructs the allocator to create CUDA allocations that can later be expanded to better handle cases where a job changes allocation sizes frequently, such as having a changing batch size.</em></p></blockquote><p id="95d8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, this tells PyTorch allocator to allocate blocks that could be expanded in the future, which is exactly our case. Though, if size variations are too big, it still may fail to solve the issue. In this case, move to the next option.</p><h2 id="83cd" class="rg oh fq bf oi rh ri rj ol rk rl rm oo nl rn ro rp np rq rr rs nt rt ru rv rw bk"><strong class="al">Make allocations variate less</strong></h2><p id="1bd6" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Another possible solution is to make data shapes consistent. That way it will be easier for the allocator to find an appropriate data block to reuse.</p><p id="73f3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To accomplish that, you may pad data to the same sizes. Or you can preheat the allocator by running a model with maximum input sizes.</p><p id="29b2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can learn more about PyTorch allocator modification in the following article</p><div class="ph pi pj pk pl pm"><a href="https://pytorch.org/docs/stable/notes/cuda.html?ref=alexdremov.me&amp;source=post_page-----9c9d4899313d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab ig"><div class="po ab co cb pp pq"><h2 class="bf fr hw z io pr iq ir ps it iv fp bk">CUDA semantics - PyTorch 2.3 documentation</h2><div class="pt l"><h3 class="bf b hw z io pr iq ir ps it iv dx">A guide to torch.cuda, a PyTorch module to run CUDA operations</h3></div><div class="pu l"><p class="bf b dy z io pr iq ir ps it iv dx">pytorch.org</p></div></div></div></a></div></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="966e" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Tidy up allocations history</h1><p id="4d3b" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">We want to use all available GPU memory ‚Äî that allows us to run big batches and process data faster. However, at some point, you will encounter a <em class="py">CUDA out-of-memory</em> error when increasing batch size. What causes this error?</p><p id="907f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To debug this, we can view the allocator‚Äôs memory history. It can be recorded through PyTorch and then visualized at <a class="af nb" href="https://pytorch.org/memory_viz?ref=alexdremov.me" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/memory_viz</a></p><ul class=""><li id="7fa9" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pz qa qb bk"><strong class="ne fr">Start:</strong> <code class="cx qw qx qy qi b">torch.cuda.memory._record_memory_history(max_entries=100000)</code></li><li id="3661" class="nc nd fq ne b go qc ng nh gr qd nj nk nl qe nn no np qf nr ns nt qg nv nw nx pz qa qb bk"><strong class="ne fr">Save:</strong> <code class="cx qw qx qy qi b">torch.cuda.memory._dump_snapshot(file_name)</code></li><li id="c615" class="nc nd fq ne b go qc ng nh gr qd nj nk nl qe nn no np qf nr ns nt qg nv nw nx pz qa qb bk"><strong class="ne fr">Stop:</strong> <code class="cx qw qx qy qi b">torch.cuda.memory._record_memory_history(enabled=None)</code></li></ul><p id="4860" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Visualization will draw something like this:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rx"><img src="../Images/270f72428251b625a672af134cf2f454.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*39xeSNRORSW48rjIhO5MCQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">¬© Copyright 2024, PyTorch | <a class="af nb" href="https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/blog/understanding-gpu-memory-1/</a></figcaption></figure><p id="b027" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The x-axis represents time, the y-axis represents total used memory, and colourful blocks represent tensors. So, it shows when tensors were allocated and when it was released.</p><p id="08f9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You may notice narrow spikes ‚Äî those are short-lasting tensors that take up a lot of space. By clicking on a tensor, you can get information on where this tensor was allocated. We want to minimize those spikes as they limit efficient memory usage. Check out what caused this spike and consider other ways of computing what you intended.</p><p id="16e1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Apart from spikes, it‚Äôs easy to detect memory leaks:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rx"><img src="../Images/d257a3b5af410d7d0bb7b3e46ecda12d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4R1oLVWtLl7xssZe1YNwJw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">¬© Copyright 2024, PyTorch | <a class="af nb" href="https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/blog/understanding-gpu-memory-1/</a></figcaption></figure><p id="9a2a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As you see, some data after the first forward is not cleared. By clicking on blocks you can get the idea where these tensors come from. In the image is the case when gradients are not cleared after the training step, so they lay dead during the forward pass, limiting the ability to increase the batch size to fit more data.</p><div class="ph pi pj pk pl pm"><a href="https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me&amp;source=post_page-----9c9d4899313d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab ig"><div class="po ab co cb pp pq"><h2 class="bf fr hw z io pr iq ir ps it iv fp bk">Understanding GPU Memory 1: Visualizing All Allocations over Time</h2><div class="pt l"><h3 class="bf b hw z io pr iq ir ps it iv dx">During your time with PyTorch on GPUs, you may be familiar with this common error message:</h3></div><div class="pu l"><p class="bf b dy z io pr iq ir ps it iv dx">pytorch.org</p></div></div><div class="qq l"><div class="ry l qs qt qu qq qv lq pm"/></div></div></a></div></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2252" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Speed up the model and use less memory</h1><p id="223d" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">What can be better than this? We can achieve so by using the <strong class="ne fr">FlashAttention</strong> kernel for calculating dot-product attention.</p><div class="ph pi pj pk pl pm"><a href="https://github.com/Dao-AILab/flash-attention?ref=alexdremov.me&amp;source=post_page-----9c9d4899313d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab ig"><div class="po ab co cb pp pq"><h2 class="bf fr hw z io pr iq ir ps it iv fp bk">GitHub - Dao-AILab/flash-attention: Fast and memory-efficient exact attention</h2><div class="pt l"><h3 class="bf b hw z io pr iq ir ps it iv dx">Fast and memory-efficient exact attention. Contribute to Dao-AILab/flash-attention development by creating an account‚Ä¶</h3></div><div class="pu l"><p class="bf b dy z io pr iq ir ps it iv dx">github.com</p></div></div><div class="qq l"><div class="rz l qs qt qu qq qv lq pm"/></div></div></a></div><p id="03ce" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you haven‚Äôt heard about it, it is a way of calculating precise dot product attention without constructing the attention matrix explicitly. That optimizes GPU‚Äôs io operations which improves speed and also <strong class="ne fr">dramatically</strong> minimizes memory consumption. There‚Äôs simply no reason not to use it.</p><blockquote class="pv pw px"><p id="c22c" class="nc nd py ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">üò° Unfortunately, there‚Äôs one reason not to use it ‚Äî hardware.</p><p id="a496" class="nc nd py ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Flash attention only works with <code class="cx qw qx qy qi b">fp16</code> and <code class="cx qw qx qy qi b">bf16</code> precision on compatible hardware. That is NVIDIA Ampere, Hooper, etc</p></blockquote><p id="7308" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Other libraries use flash attention under the hood, so you may consider using other variants that better fit your codebase.</p><h2 id="f047" class="rg oh fq bf oi rh ri rj ol rk rl rm oo nl rn ro rp np rq rr rs nt rt ru rv rw bk"><strong class="al">XFormers</strong></h2><div class="ph pi pj pk pl pm"><a href="https://github.com/facebookresearch/xformers?ref=alexdremov.me&amp;source=post_page-----9c9d4899313d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab ig"><div class="po ab co cb pp pq"><h2 class="bf fr hw z io pr iq ir ps it iv fp bk">GitHub - facebookresearch/xformers: Hackable and optimized Transformers building blocks, supporting‚Ä¶</h2><div class="pt l"><h3 class="bf b hw z io pr iq ir ps it iv dx">Hackable and optimized Transformers building blocks, supporting a composable construction. - facebookresearch/xformers</h3></div><div class="pu l"><p class="bf b dy z io pr iq ir ps it iv dx">github.com</p></div></div><div class="qq l"><div class="sa l qs qt qu qq qv lq pm"/></div></div></a></div><h2 id="92b3" class="rg oh fq bf oi rh ri rj ol rk rl rm oo nl rn ro rp np rq rr rs nt rt ru rv rw bk"><strong class="al">Transformer Engine</strong></h2><div class="ph pi pj pk pl pm"><a href="https://github.com/NVIDIA/TransformerEngine?ref=alexdremov.me&amp;source=post_page-----9c9d4899313d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab ig"><div class="po ab co cb pp pq"><h2 class="bf fr hw z io pr iq ir ps it iv fp bk">GitHub - NVIDIA/TransformerEngine: A library for accelerating Transformer models on NVIDIA GPUs‚Ä¶</h2><div class="pt l"><h3 class="bf b hw z io pr iq ir ps it iv dx">A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on‚Ä¶</h3></div><div class="pu l"><p class="bf b dy z io pr iq ir ps it iv dx">github.com</p></div></div><div class="qq l"><div class="sb l qs qt qu qq qv lq pm"/></div></div></a></div><h2 id="c033" class="rg oh fq bf oi rh ri rj ol rk rl rm oo nl rn ro rp np rq rr rs nt rt ru rv rw bk"><strong class="al">PyTorch itself!</strong></h2><p id="6c72" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">That is true, new versions of PyTorch may use flash attention when applicable. To activate this mode, you need to execute attention blocks in the context manager that specify which attention strategy to use:</p><div class="ph pi pj pk pl pm"><a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html?ref=alexdremov.me&amp;source=post_page-----9c9d4899313d--------------------------------#torch-nn-functional-scaled-dot-product-attention" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab ig"><div class="po ab co cb pp pq"><h2 class="bf fr hw z io pr iq ir ps it iv fp bk">torch.nn.functional.scaled_dot_product_attention - PyTorch 2.3 documentation</h2><div class="pt l"><h3 class="bf b hw z io pr iq ir ps it iv dx">Read the PyTorch Domains documentation to learn more about domain-specific libraries</h3></div><div class="pu l"><p class="bf b dy z io pr iq ir ps it iv dx">pytorch.org</p></div></div></div></a></div></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="12ac" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Optimize multi-GPU data redundancy ‚Äî FSDP</h1><p id="0731" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">If you use multiple GPUs to run your training, the basic solution is to use the <code class="cx qw qx qy qi b">DistributedDataParallel</code> class. This way, several identical processes are spawned, and gradients are aggregated during the backward step.</p><p id="44e0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, that is sub-optimal!</p><p id="9438" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The problem is as we spawned identical processes, then we have identical models and optimiser states on each GPU, which is redundant. The solution is to shard data across. We can do so using the Fully Sharded Data Parallel PyTorch wrapper.</p></div></div><div class="mq"><div class="ab cb"><div class="ll qz lm ra ln rb cf rc cg rd ci bh"><figure class="ml mm mn mo mp mq re rf paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/b542604a9a173f89aed4bc0375e2e515.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*Ge8f-RxzQGit6cwO5iHbgw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">¬© Copyright 2024, PyTorch | https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="9905" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">How does it work?</p><p id="6f1c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As I said, when training on several GPUs, each process has exact copies of the same data when training with DDP. We can optimize it, by implementing several enhancements:</p><h2 id="cd62" class="rg oh fq bf oi rh ri rj ol rk rl rm oo nl rn ro rp np rq rr rs nt rt ru rv rw bk">Shard optimizer state (ZeRO 1)</h2><p id="d7f7" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">When training with DDP, each process holds a complete copy of the optimizer states. With ZeRO1, we shard these optimizer states across all ranks such that each rank holds only a portion of the optimizer states. During the backward pass, each rank only needs to gather the optimizer states relevant to its parameters to make an optimization step. This reduction in redundancy helps conserve memory.</p><blockquote class="pv pw px"><p id="64ec" class="nc nd py ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">üí° In case of the Adam, which holds parameters at roughly twice the model size, sharding the optimizer state among 8 ranks means each rank <strong class="ne fr">stores only one quarter (2/8) of the total state size.</strong></p></blockquote><h2 id="8063" class="rg oh fq bf oi rh ri rj ol rk rl rm oo nl rn ro rp np rq rr rs nt rt ru rv rw bk">Shard gradients (ZeRO 2)</h2><p id="3a95" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">We shard optimizer states. Now, we will modify the optimizer step to shard gradients too. If one rank has optimizer states for a portion of parameters, then we will:</p><ul class=""><li id="1e92" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pz qa qb bk">aggregate all gradients relevant to the states the rank holds</li><li id="8d72" class="nc nd fq ne b go qc ng nh gr qd nj nk nl qe nn no np qf nr ns nt qg nv nw nx pz qa qb bk">calculate optimization step</li><li id="5e1a" class="nc nd fq ne b go qc ng nh gr qd nj nk nl qe nn no np qf nr ns nt qg nv nw nx pz qa qb bk">send optimization step for a portion of parameters to all other ranks</li></ul><p id="397d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As you noticed, now each rank does not need to hold a full replica of gradients. We can send gradients to a relevant rank as soon as they are available. So, we can reduce peak memory consumption even further.</p><h2 id="8974" class="rg oh fq bf oi rh ri rj ol rk rl rm oo nl rn ro rp np rq rr rs nt rt ru rv rw bk">Shard model parameters (ZeRO 3)</h2><p id="e72c" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">This is about to be epic.</p><p id="41f0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Why do we need to store a full copy of the model on each rank? Let‚Äôs shard model parameters between all ranks. Then, we‚Äôre going to fetch the required parameters just in time during forward and backward.</p><blockquote class="pv pw px"><p id="b8d7" class="nc nd py ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">üí° In case of large models, these optimisations can drammaticaly decrease memory consumption</p></blockquote><h2 id="30ef" class="rg oh fq bf oi rh ri rj ol rk rl rm oo nl rn ro rp np rq rr rs nt rt ru rv rw bk">How to use FSDP?</h2><p id="394e" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Quite simple actually. All we need is to wrap the model with FSDP:</p><pre class="ml mm mn mo mp qh qi qj bp qk bb bk"><span id="2e6a" class="ql oh fq qi b bg qm qn l qo qp">import torch<br/>import torch.nn as nn<br/>import torch.optim as optim<br/>from torch.distributed.fsdp import FullyShardedDataParallel as FSDP<br/><br/><br/>model = FSDP(model)<br/><br/># it's critical to get parameters from the wrapped model<br/># as only a portion of them returned (sharded part)<br/>optimizer = optim.Adam(model.parameters())<br/><br/># consuct training as usual<br/>train(model, optimizer)</span></pre><p id="45e9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can also specify the sharding strategy of FSDP. For example, we can select the <code class="cx qw qx qy qi b">SHARD_GRAD_OP</code> strategy to achieve behaviour similar to that of ZeRO2. You can learn about other strategies here:</p><div class="ph pi pj pk pl pm"><a href="https://pytorch.org/docs/stable/fsdp.html?ref=alexdremov.me&amp;source=post_page-----9c9d4899313d--------------------------------#torch.distributed.fsdp.ShardingStrategy" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab ig"><div class="po ab co cb pp pq"><h2 class="bf fr hw z io pr iq ir ps it iv fp bk">FullyShardedDataParallel - PyTorch 2.3 documentation</h2><div class="pt l"><h3 class="bf b hw z io pr iq ir ps it iv dx">A wrapper for sharding module parameters across data parallel workers. This is inspired by Xu et al. as well as the‚Ä¶</h3></div><div class="pu l"><p class="bf b dy z io pr iq ir ps it iv dx">pytorch.org</p></div></div></div></a></div><p id="c25e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Also, you can wrap with FSDP submodules. In the example above, only one FSDP module is used, which will reduce computation efficiency and memory efficiency. The way it works is that, suppose your model contains 100 Linear layers. If you do FSDP(model), there will only be one FSDP unit which wraps the entire model. In that case, the allgather would collect the full parameters for all 100 linear layers, and hence won‚Äôt save CUDA memory for parameter sharding.</p><p id="672b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can wrap submodules explicitly or define an auto-wrap policy. To learn more about FSDP, read the PyTorch guide:</p><div class="ph pi pj pk pl pm"><a href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?ref=alexdremov.me&amp;source=post_page-----9c9d4899313d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab ig"><div class="po ab co cb pp pq"><h2 class="bf fr hw z io pr iq ir ps it iv fp bk">Getting Started with Fully Sharded Data Parallel(FSDP) - PyTorch Tutorials 2.3.0+cu121‚Ä¶</h2><div class="pt l"><h3 class="bf b hw z io pr iq ir ps it iv dx">Note View and edit this tutorial in github. Training AI models at a large scale is a challenging task that requires a‚Ä¶</h3></div><div class="pu l"><p class="bf b dy z io pr iq ir ps it iv dx">pytorch.org</p></div></div><div class="qq l"><div class="sc l qs qt qu qq qv lq pm"/></div></div></a></div></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6363" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Magic speedup with <code class="cx qw qx qy qi b">torch.compile</code></h1><p id="741b" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">That is, torch compile can speed up your code by several percent by just enabling it.</p><p id="b2e8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Torch traces your execution graph and tries to compile it into an efficient format so that the model can be executed almost without Python invocation.</p><p id="f6ec" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Basic usage is to wrap the model with compile:</p><pre class="ml mm mn mo mp qh qi qj bp qk bb bk"><span id="b6eb" class="ql oh fq qi b bg qm qn l qo qp">import torch<br/><br/>model = torch.compile(model)</span></pre><p id="447a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This will execute almost instantly. The actual tracing will happen only during the first forward.</p><p id="b1e5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It also has a lot of options that are worth to try:</p><div class="ph pi pj pk pl pm"><a href="https://pytorch.org/docs/stable/generated/torch.compile.html?ref=alexdremov.me&amp;source=post_page-----9c9d4899313d--------------------------------#torch.compile" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab ig"><div class="po ab co cb pp pq"><h2 class="bf fr hw z io pr iq ir ps it iv fp bk">torch.compile - PyTorch 2.3 documentation</h2><div class="pt l"><h3 class="bf b hw z io pr iq ir ps it iv dx">Optimizes given model/function using TorchDynamo and specified backend. Concretely, for every frame executed within the‚Ä¶</h3></div><div class="pu l"><p class="bf b dy z io pr iq ir ps it iv dx">pytorch.org</p></div></div></div></a></div><blockquote class="pv pw px"><p id="b954" class="nc nd py ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">üí° Torch compiler is a big feature that will be covered in the next posts! <br/>Stay tuned</p></blockquote><p id="4b85" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Learn more about torch compile here:</p><div class="ph pi pj pk pl pm"><a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html?ref=alexdremov.me&amp;source=post_page-----9c9d4899313d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab ig"><div class="po ab co cb pp pq"><h2 class="bf fr hw z io pr iq ir ps it iv fp bk">Introduction to torch.compile - PyTorch Tutorials 2.3.0+cu121 documentation</h2><div class="pt l"><h3 class="bf b hw z io pr iq ir ps it iv dx">torch.compile is included in the latest PyTorch. Running TorchInductor on GPU requires Triton, which is included with‚Ä¶</h3></div><div class="pu l"><p class="bf b dy z io pr iq ir ps it iv dx">pytorch.org</p></div></div></div></a></div><h1 id="f210" class="og oh fq bf oi oj sd gq ol om se gt oo op sf or os ot sg ov ow ox sh oz pa pb bk">Conclusion</h1><p id="0a93" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">This post is in no way complete with explanations. Rather, that is a list of speed-ups that are worth trying straight away. Hope that it was helpful. Feel free to leave a comment!</p><p id="affa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Consider subscribing</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f793" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="py">Originally published at </em><a class="af nb" href="https://alexdremov.me/simple-ways-to-speedup-your-pytorch-model-training/" rel="noopener ugc nofollow" target="_blank"><em class="py">https://alexdremov.me</em></a><em class="py"> on May 28, 2024.</em></p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="5c13" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="py">Images from PyTorch Blog were used, which is a project of the Linux Foundation and is subject to the Linux Foundation </em><a class="af nb" href="https://www.linuxfoundation.org/legal/terms" rel="noopener ugc nofollow" target="_blank"><em class="py">policies</em></a><em class="py">. So, all images may be used as allowed by </em><a class="af nb" href="http://creativecommons.org/licenses/by/3.0/" rel="noopener ugc nofollow" target="_blank"><em class="py">Creative Commons Attribution 3.0 License</em></a></p></div></div></div></div>    
</body>
</html>