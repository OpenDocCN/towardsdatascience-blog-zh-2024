- en: Local LLM Fine-Tuning on Mac (M1 16GB)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/local-llm-fine-tuning-on-mac-m1-16gb-f59f4f598be7?source=collection_archive---------2-----------------------#2024-08-01](https://towardsdatascience.com/local-llm-fine-tuning-on-mac-m1-16gb-f59f4f598be7?source=collection_archive---------2-----------------------#2024-08-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Beginner-friendly Python code walkthrough (ft. MLX)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/?source=post_page---byline--f59f4f598be7--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page---byline--f59f4f598be7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f59f4f598be7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f59f4f598be7--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page---byline--f59f4f598be7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f59f4f598be7--------------------------------)
    ·8 min read·Aug 1, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: This article is part of a [larger series](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c)
    on using large language models (LLMs) in practice. In a [previous post](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32),
    I showed how to fine-tune an LLM using a single (free) GPU on Google Colab. While
    that example (and many others) readily runs on Nvidia hardware, they are not easily
    adapted to M-series Macs. In this article, I walk through an easy way to fine-tune
    an LLM locally on a Mac.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff42af007f050edf17f7fa56426c6535.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Myron Mott](https://unsplash.com/@s2killa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: With the rise of open-source large language models (LLMs) and efficient fine-tuning
    methods, building custom ML solutions has never been easier. Now, anyone with
    **a single GPU can fine-tune an LLM on their local machine**.
  prefs: []
  type: TYPE_NORMAL
- en: However, Mac users have been largely left out of this trend due to Apple’s M-series
    chips. These chips employ a unified memory framework, which precludes the need
    for a GPU. Thus, many (GPU-centric) open-source tools for running and training
    LLMs are not compatible with (or don’t fully utilize) modern Mac computing power.
  prefs: []
  type: TYPE_NORMAL
- en: I had **almost** given up on my dreams of training LLMs locally until I discovered
    the MLX Python library.
  prefs: []
  type: TYPE_NORMAL
