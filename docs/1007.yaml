- en: 'Mistral vs Mixtral: Comparing the 7B, 8x7B, and 8x22B Large Language Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mistral-vs-mixtral-comparing-the-7b-8x7b-and-8x22b-large-language-models-58ab5b2cc8ee?source=collection_archive---------2-----------------------#2024-04-20](https://towardsdatascience.com/mistral-vs-mixtral-comparing-the-7b-8x7b-and-8x22b-large-language-models-58ab5b2cc8ee?source=collection_archive---------2-----------------------#2024-04-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Running the 7B and 22B Models in Google Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dmitryelj.medium.com/?source=post_page---byline--58ab5b2cc8ee--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page---byline--58ab5b2cc8ee--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--58ab5b2cc8ee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--58ab5b2cc8ee--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page---byline--58ab5b2cc8ee--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--58ab5b2cc8ee--------------------------------)
    ·10 min read·Apr 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f41d7b4081f76c11b35c9131f4fe12a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Img IX](https://unsplash.com/@imgix), Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: Not so long ago, all IT news channels reported about the new open Mixtral 8x22B
    model, which outperforms ChatGPT 3.5 on benchmarks like MMLU (massive multitask
    language understanding) or WinoGrande (commonsense reasoning). This is a great
    achievement for the world of open models. Naturally, academic benchmarks are interesting,
    but how does this model practically work? What system requirements does it have,
    and is it really better compared to previous language models? In this article,
    I will test four different models (7B, 8x7B, 22B, and 8x22B, with and without
    a “Mixture of Experts” architecture), and we will see the results.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: As an aside note, I have no business relationship with Mistral AI, and all tests
    here are done on my own.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Mixture of Experts (SMoE)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Already at the beginning of the LLM era, it became known that larger models
    are, generally speaking, smarter, have more knowledge, and can achieve better
    results. But larger models are also more computationally expensive. Nobody will
    wait for the chatbot’s response if it takes 5 minutes. The intuitive idea behind
    the “mixture of experts” is…
  prefs: []
  type: TYPE_NORMAL
