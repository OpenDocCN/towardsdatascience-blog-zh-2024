# é«˜æ–¯æœ´ç´ è´å¶æ–¯è§£é‡Šï¼šä¸ºåˆå­¦è€…æä¾›çš„å¸¦æœ‰ä»£ç ç¤ºä¾‹çš„å¯è§†åŒ–æŒ‡å—

> åŸæ–‡ï¼š[`towardsdatascience.com/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c?source=collection_archive---------2-----------------------#2024-10-12`](https://towardsdatascience.com/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c?source=collection_archive---------2-----------------------#2024-10-12)

## åˆ†ç±»ç®—æ³•

## **é’Ÿå½¢æ›²çº¿å‡è®¾å¸¦æ¥æ›´å¥½çš„é¢„æµ‹**

[](https://medium.com/@samybaladram?source=post_page---byline--04949cef383c--------------------------------)![Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--04949cef383c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--04949cef383c--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--04949cef383c--------------------------------) [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--04949cef383c--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--04949cef383c--------------------------------) Â·é˜…è¯»æ—¶é—´ï¼š9 åˆ†é’ŸÂ·2024 å¹´ 10 æœˆ 12 æ—¥

--

![](img/3461eb23bbba050007a17f806c580568.png)

`â›³ï¸ æ›´å¤š[åˆ†ç±»ç®—æ³•](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c)è®²è§£ï¼šÂ· è™šæ‹Ÿåˆ†ç±»å™¨ Â· K æœ€è¿‘é‚»åˆ†ç±»å™¨ Â· ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ â–¶ é«˜æ–¯æœ´ç´ è´å¶æ–¯ Â· å†³ç­–æ ‘åˆ†ç±»å™¨ Â· é€»è¾‘å›å½’ Â· æ”¯æŒå‘é‡æœºåˆ†ç±»å™¨ Â· å¤šå±‚æ„ŸçŸ¥æœº`

åœ¨æˆ‘ä»¬ä¹‹å‰å…³äºä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯çš„æ–‡ç« åŸºç¡€ä¸Šï¼Œè¿™ä¸€æ–¹æ³•å¤„ç†çš„æ˜¯äºŒå…ƒæ•°æ®ï¼Œæˆ‘ä»¬ç°åœ¨æ¢è®¨çš„æ˜¯ç”¨äºè¿ç»­æ•°æ®çš„é«˜æ–¯æœ´ç´ è´å¶æ–¯ã€‚ä¸äºŒå…ƒæ–¹æ³•ä¸åŒï¼Œè¯¥ç®—æ³•å‡è®¾æ¯ä¸ªç‰¹å¾éµå¾ªæ­£æ€ï¼ˆé«˜æ–¯ï¼‰åˆ†å¸ƒã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†çœ‹åˆ°é«˜æ–¯æœ´ç´ è´å¶æ–¯å¦‚ä½•å¤„ç†è¿ç»­çš„é’Ÿå½¢æ•°æ®â€”â€”ä¸ºå‡†ç¡®é¢„æµ‹æ‰“ä¸‹åŸºç¡€â€”â€”**è€Œæ— éœ€æ·±å…¥æ¢è®¨è´å¶æ–¯å®šç†çš„å¤æ‚æ•°å­¦**ã€‚

![](img/c31e0cb8eff429b558a8a46174d03a76.png)

æ‰€æœ‰è§†è§‰å†…å®¹ï¼šä½œè€…ä½¿ç”¨ Canva Pro åˆ›ä½œã€‚ä¼˜åŒ–äº†ç§»åŠ¨ç«¯æ˜¾ç¤ºï¼Œå¯èƒ½åœ¨æ¡Œé¢ç«¯æ˜¾ç¤ºè¿‡å¤§ã€‚

# å®šä¹‰

ä¸å…¶ä»–æœ´ç´ è´å¶æ–¯å˜ç§ä¸€æ ·ï¼Œé«˜æ–¯æœ´ç´ è´å¶æ–¯åšå‡ºäº†â€œæœ´ç´ â€çš„ç‰¹å¾ç‹¬ç«‹æ€§å‡è®¾ã€‚å®ƒå‡è®¾åœ¨ç»™å®šç±»æ ‡ç­¾çš„æ¡ä»¶ä¸‹ï¼Œç‰¹å¾æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚

ç„¶è€Œï¼Œè™½ç„¶ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯é€‚ç”¨äºå…·æœ‰äºŒå…ƒç‰¹å¾çš„æ•°æ®é›†ï¼Œä½†é«˜æ–¯æœ´ç´ è´å¶æ–¯å‡è®¾ç‰¹å¾æœä»**è¿ç»­çš„æ­£æ€ï¼ˆé«˜æ–¯ï¼‰**åˆ†å¸ƒã€‚å°½ç®¡è¿™ä¸€å‡è®¾åœ¨ç°å®ä¸­å¹¶ä¸æ€»æ˜¯æˆç«‹ï¼Œä½†å®ƒç®€åŒ–äº†è®¡ç®—ï¼Œå¹¶ä¸”å¸¸å¸¸èƒ½å¤Ÿå¾—åˆ°å‡ºäººæ„æ–™çš„å‡†ç¡®ç»“æœã€‚

![](img/0d0e6eec6460b4247cf38d8dc3c8600e.png)

ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯å‡è®¾æ•°æ®æ˜¯äºŒå…ƒçš„ï¼Œ multinomial æœ´ç´ è´å¶æ–¯é€‚ç”¨äºç¦»æ•£è®¡æ•°æ•°æ®ï¼Œè€Œé«˜æ–¯æœ´ç´ è´å¶æ–¯å¤„ç†è¿ç»­æ•°æ®ï¼Œå‡è®¾æ•°æ®æœä»æ­£æ€åˆ†å¸ƒã€‚

# ä½¿ç”¨çš„æ•°æ®é›†

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªäººå·¥é«˜å°”å¤«æ•°æ®é›†ï¼ˆç”±ä½œè€…åˆ¶ä½œï¼‰ä½œä¸ºç¤ºä¾‹ã€‚è¯¥æ•°æ®é›†é¢„æµ‹ä¸€ä¸ªäººæ˜¯å¦ä¼šæ ¹æ®å¤©æ°”æ¡ä»¶å»æ‰“é«˜å°”å¤«ã€‚

![](img/fc1873cedde634b5e602a6129e33ac2e.png)

åˆ—ï¼šâ€˜é™æ°´é‡â€™ï¼ˆå•ä½ï¼šæ¯«ç±³ï¼‰ã€â€˜æ¸©åº¦â€™ï¼ˆå•ä½ï¼šæ‘„æ°åº¦ï¼‰ã€â€˜æ¹¿åº¦â€™ï¼ˆå•ä½ï¼šç™¾åˆ†æ¯”ï¼‰ã€â€˜é£é€Ÿâ€™ï¼ˆå•ä½ï¼šåƒç±³/å°æ—¶ï¼‰å’Œâ€˜æ˜¯å¦ä¸‹æ£‹â€™ï¼ˆæ˜¯/å¦ï¼Œç›®æ ‡ç‰¹å¾ï¼‰

```py
# IMPORTING DATASET #
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np

dataset_dict = {
    'Rainfall': [0.0, 2.0, 7.0, 18.0, 3.0, 3.0, 0.0, 1.0, 0.0, 25.0, 0.0, 18.0, 9.0, 5.0, 0.0, 1.0, 7.0, 0.0, 0.0, 7.0, 5.0, 3.0, 0.0, 2.0, 0.0, 8.0, 4.0, 4.0],
    'Temperature': [29.4, 26.7, 28.3, 21.1, 20.0, 18.3, 17.8, 22.2, 20.6, 23.9, 23.9, 22.2, 27.2, 21.7, 27.2, 23.3, 24.4, 25.6, 27.8, 19.4, 29.4, 22.8, 31.1, 25.0, 26.1, 26.7, 18.9, 28.9],
    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],
    'WindSpeed': [2.1, 21.2, 1.5, 3.3, 2.0, 17.4, 14.9, 6.9, 2.7, 1.6, 30.3, 10.9, 3.0, 7.5, 10.3, 3.0, 3.9, 21.9, 2.6, 17.3, 9.6, 1.9, 16.0, 4.6, 3.2, 8.3, 3.2, 2.2],
    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']
}
df = pd.DataFrame(dataset_dict)

# Set feature matrix X and target vector y
X, y = df.drop(columns='Play'), df['Play']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)
print(pd.concat([X_train, y_train], axis=1), end='\n\n')
print(pd.concat([X_test, y_test], axis=1))
```

# ä¸»è¦æœºåˆ¶

é«˜æ–¯æœ´ç´ è´å¶æ–¯é€‚ç”¨äºè¿ç»­æ•°æ®ï¼Œå‡è®¾æ¯ä¸ªç‰¹å¾éƒ½æœä»é«˜æ–¯ï¼ˆæ­£æ€ï¼‰åˆ†å¸ƒã€‚

1.  è®¡ç®—è®­ç»ƒæ•°æ®ä¸­æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚

1.  å¯¹äºæ¯ä¸ªç‰¹å¾å’Œç±»åˆ«ï¼Œä¼°è®¡è¯¥ç±»åˆ«ä¸­è¯¥ç‰¹å¾å€¼çš„å‡å€¼å’Œæ–¹å·®ã€‚

1.  å¯¹äºæ–°å®ä¾‹ï¼š

    a. å¯¹äºæ¯ä¸ªç±»åˆ«ï¼Œè®¡ç®—è¯¥ç±»åˆ«ä¸‹æ¯ä¸ªç‰¹å¾å€¼åœ¨è¯¥ç‰¹å¾é«˜æ–¯åˆ†å¸ƒä¸‹çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰ã€‚

    b. å°†ç±»åˆ«æ¦‚ç‡ä¸æ‰€æœ‰ç‰¹å¾çš„ PDF å€¼çš„ä¹˜ç§¯ç›¸ä¹˜ã€‚

1.  é¢„æµ‹å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ç±»åˆ«ã€‚

![](img/a090060bd91a5d5c69880a4926db7175.png)

é«˜æ–¯æœ´ç´ è´å¶æ–¯ä½¿ç”¨æ­£æ€åˆ†å¸ƒæ¥å»ºæ¨¡æ¯ä¸ªç±»åˆ«ä¸­ä¸åŒç‰¹å¾å€¼çš„å¯èƒ½æ€§ã€‚ç„¶åï¼Œå®ƒå°†è¿™äº›å¯èƒ½æ€§ç»“åˆèµ·æ¥è¿›è¡Œé¢„æµ‹ã€‚

## è½¬æ¢éé«˜æ–¯åˆ†å¸ƒæ•°æ®

è®°ä½è¿™ä¸ªç®—æ³•å¤©çœŸåœ°å‡è®¾æ‰€æœ‰è¾“å…¥ç‰¹å¾éƒ½æœä»é«˜æ–¯/æ­£æ€åˆ†å¸ƒå—ï¼Ÿ

ç”±äºæˆ‘ä»¬ä¸ç¡®å®šæ•°æ®çš„åˆ†å¸ƒï¼Œå°¤å…¶æ˜¯å¯¹äºé‚£äº›æ˜æ˜¾ä¸éµå¾ªé«˜æ–¯åˆ†å¸ƒçš„ç‰¹å¾ï¼Œåœ¨ä½¿ç”¨é«˜æ–¯æœ´ç´ è´å¶æ–¯ä¹‹å‰åº”ç”¨å¹‚å˜æ¢ï¼ˆå¦‚ Box-Cox å˜æ¢ï¼‰å¯èƒ½ä¼šæœ‰å¸®åŠ©ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¸®åŠ©æ•°æ®æ›´æ¥è¿‘é«˜æ–¯åˆ†å¸ƒï¼Œä»è€Œæ›´ç¬¦åˆç®—æ³•çš„å‡è®¾ã€‚

![](img/90d0627b51b81e1b87930635d3497adc.png)

æ‰€æœ‰åˆ—éƒ½ä½¿ç”¨å¹‚å˜æ¢ï¼ˆBox-Cox å˜æ¢ï¼‰è¿›è¡Œç¼©æ”¾ï¼Œç„¶åæ ‡å‡†åŒ–ã€‚

```py
from sklearn.preprocessing import PowerTransformer

# Initialize and fit the PowerTransformer
pt = PowerTransformer(standardize=True) # Standard Scaling already included
X_train_transformed = pt.fit_transform(X_train)
X_test_transformed = pt.transform(X_test)
```

ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¼€å§‹è®­ç»ƒã€‚

# è®­ç»ƒæ­¥éª¤

1. **ç±»åˆ«æ¦‚ç‡è®¡ç®—**ï¼šå¯¹äºæ¯ä¸ªç±»åˆ«ï¼Œè®¡ç®—å…¶æ¦‚ç‡ï¼šï¼ˆè¯¥ç±»åˆ«ä¸­çš„å®ä¾‹æ•°é‡ï¼‰/ï¼ˆå®ä¾‹æ€»æ•°ï¼‰

![](img/c06266178b47854c1548da3d97e124f7.png)

```py
from fractions import Fraction

def calc_target_prob(attr):
    total_counts = attr.value_counts().sum()
    prob_series = attr.value_counts().apply(lambda x: Fraction(x, total_counts).limit_denominator())
    return prob_series

print(calc_target_prob(y_train))
```

2\. **ç‰¹å¾æ¦‚ç‡è®¡ç®—**ï¼šå¯¹äºæ¯ä¸ªç‰¹å¾å’Œæ¯ä¸ªç±»åˆ«ï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®è®¡ç®—è¯¥ç±»åˆ«ä¸­ç‰¹å¾å€¼çš„å‡å€¼ï¼ˆÎ¼ï¼‰å’Œæ ‡å‡†å·®ï¼ˆÏƒï¼‰ã€‚ç„¶åï¼Œä½¿ç”¨é«˜æ–¯æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰å…¬å¼è®¡ç®—æ¦‚ç‡ã€‚

![](img/b1e80a1082f80606387c89d90f62303c.png)

å¯¹äºæ¯ä¸ªå¤©æ°”æ¡ä»¶ï¼Œç¡®å®šâ€œYESâ€å’Œâ€œNOâ€å®ä¾‹çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚ç„¶åä½¿ç”¨æ­£æ€/é«˜æ–¯åˆ†å¸ƒçš„ PDF å…¬å¼è®¡ç®—å®ƒä»¬çš„ PDFã€‚

![](img/a40cfb56adeb49c44ff0899ca2e3dbc1.png)

ç›¸åŒçš„è¿‡ç¨‹åº”ç”¨äºå…¶ä»–æ‰€æœ‰ç‰¹å¾ã€‚

```py
def calculate_class_probabilities(X_train_transformed, y_train, feature_names):
    classes = y_train.unique()
    equations = pd.DataFrame(index=classes, columns=feature_names)

    for cls in classes:
        X_class = X_train_transformed[y_train == cls]
        mean = X_class.mean(axis=0)
        std = X_class.std(axis=0)
        k1 = 1 / (std * np.sqrt(2 * np.pi))
        k2 = 2 * (std ** 2)

        for i, column in enumerate(feature_names):
            equation = f"{k1[i]:.3f}Â·exp(-(x-({mean[i]:.2f}))Â²/{k2[i]:.3f})"
            equations.loc[cls, column] = equation

    return equations

# Use the function with the transformed training data
equation_table = calculate_class_probabilities(X_train_transformed, y_train, X.columns)

# Display the equation table
print(equation_table)
```

![](img/9b65e3ad3d83feb7d761c151b3593320.png)

3\. **å¹³æ»‘**ï¼šé«˜æ–¯æœ´ç´ è´å¶æ–¯ä½¿ç”¨ä¸€ç§ç‹¬ç‰¹çš„å¹³æ»‘æ–¹æ³•ã€‚ä¸å…¶ä»–å˜ç§ä¸­çš„æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ä¸åŒï¼Œå®ƒå°†ä¸€ä¸ªå¾®å°çš„å€¼ï¼ˆ0.000000001 å€çš„æœ€å¤§æ–¹å·®ï¼‰æ·»åŠ åˆ°æ‰€æœ‰æ–¹å·®ä¸­ã€‚è¿™å¯ä»¥é˜²æ­¢ç”±äºé™¤ä»¥é›¶æˆ–éå¸¸å°çš„æ•°å­—å¯¼è‡´çš„æ•°å€¼ä¸ç¨³å®šã€‚

# é¢„æµ‹/åˆ†ç±»æ­¥éª¤

ç»™å®šä¸€ä¸ªå…·æœ‰è¿ç»­ç‰¹å¾çš„æ–°å®ä¾‹ï¼š

1\. **æ¦‚ç‡æ”¶é›†**ï¼š

å¯¹äºæ¯ä¸ªå¯èƒ½çš„ç±»åˆ«ï¼š

Â· ä»è¯¥ç±»åˆ«å‘ç”Ÿçš„æ¦‚ç‡ï¼ˆç±»åˆ«æ¦‚ç‡ï¼‰å¼€å§‹ã€‚

Â· å¯¹äºæ–°å®ä¾‹ä¸­çš„æ¯ä¸ªç‰¹å¾ï¼Œè®¡ç®—è¯¥ç‰¹å¾åœ¨è¯¥ç±»åˆ«ä¸­çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚

![](img/bf29443203d3ddc521fc69caf88fa8bf.png)

å¯¹äº ID 14ï¼Œæˆ‘ä»¬è®¡ç®—æ¯ä¸ªç‰¹å¾åœ¨â€œYESâ€å’Œâ€œNOâ€å®ä¾‹ä¸­çš„ PDFã€‚

2\. **åˆ†æ•°è®¡ç®—ä¸é¢„æµ‹**ï¼š

å¯¹äºæ¯ä¸ªç±»åˆ«ï¼š

Â· å°†æ‰€æœ‰æ”¶é›†åˆ°çš„ PDF å€¼ç›¸ä¹˜ã€‚

Â· ç»“æœæ˜¯è¯¥ç±»åˆ«çš„åˆ†æ•°ã€‚

Â· å¾—åˆ†æœ€é«˜çš„ç±»åˆ«å°±æ˜¯é¢„æµ‹ç»“æœã€‚

![](img/8c0e57938597cec51337f265f8a137f9.png)

```py
from scipy.stats import norm

def calculate_class_probability_products(X_train_transformed, y_train, X_new, feature_names, target_name):
    classes = y_train.unique()
    n_features = X_train_transformed.shape[1]

    # Create column names using actual feature names
    column_names = [target_name] + list(feature_names) + ['Product']

    probability_products = pd.DataFrame(index=classes, columns=column_names)

    for cls in classes:
        X_class = X_train_transformed[y_train == cls]
        mean = X_class.mean(axis=0)
        std = X_class.std(axis=0)

        prior_prob = np.mean(y_train == cls)
        probability_products.loc[cls, target_name] = prior_prob

        feature_probs = []
        for i, feature in enumerate(feature_names):
            prob = norm.pdf(X_new[0, i], mean[i], std[i])
            probability_products.loc[cls, feature] = prob
            feature_probs.append(prob)

        product = prior_prob * np.prod(feature_probs)
        probability_products.loc[cls, 'Product'] = product

    return probability_products

# Assuming X_new is your new sample reshaped to (1, n_features)
X_new = np.array([-1.28, 1.115, 0.84, 0.68]).reshape(1, -1)

# Calculate probability products
prob_products = calculate_class_probability_products(X_train_transformed, y_train, X_new, X.columns, y.name)

# Display the probability product table
print(prob_products)
```

![](img/9c97ff87d73e99807c7113e900035931.png)

# è¯„ä¼°æ­¥éª¤

![](img/5a17bec3594c45626bd3222026fed233.png)

å¯¹äºè¿™ä¸ªç‰¹å®šçš„æ•°æ®é›†ï¼Œè¿™ä¸ªå‡†ç¡®åº¦è¢«è®¤ä¸ºæ˜¯ç›¸å½“å¥½çš„ã€‚

```py
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Initialize and train the Gaussian Naive Bayes model
gnb = GaussianNB()
gnb.fit(X_train_transformed, y_train)

# Make predictions on the test set
y_pred = gnb.predict(X_test_transformed)

# Calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)

# Print the accuracy
print(f"Accuracy: {accuracy:.4f}")
```

# å…³é”®å‚æ•°

GaussianNB å› å…¶ç®€æ´æ€§å’Œæœ‰æ•ˆæ€§è€Œé—»åã€‚å…³äºå…¶å‚æ•°ï¼Œä¸»è¦éœ€è¦è®°ä½çš„æ˜¯ï¼š

1.  **priors**ï¼šè¿™æ˜¯æœ€æ˜¾è‘—çš„å‚æ•°ï¼Œç±»ä¼¼äºä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä½ ä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®å®ƒã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒæ˜¯ä»ä½ çš„è®­ç»ƒæ•°æ®ä¸­è®¡ç®—å‡ºæ¥çš„ï¼Œé€šå¸¸æ•ˆæœå¾ˆå¥½ã€‚

1.  **var_smoothing**ï¼šè¿™æ˜¯ä¸€ä¸ªç¨³å®šæ€§å‚æ•°ï¼Œé€šå¸¸ä¸éœ€è¦è°ƒæ•´ã€‚ï¼ˆé»˜è®¤å€¼ä¸º 0.000000001ï¼‰

å…³é”®ç»“è®ºæ˜¯ï¼Œè¿™ä¸ªç®—æ³•è®¾è®¡å¾—éå¸¸æ˜“ç”¨ï¼Œé€šå¸¸åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œè€Œä¸å¿…æ‹…å¿ƒå‚æ•°è°ƒæ•´ã€‚

# ä¼˜ç¼ºç‚¹

## ä¼˜ç‚¹ï¼š

1.  **ç®€æ´æ€§**ï¼šä¿æŒäº†æ˜“äºå®ç°å’Œç†è§£çš„ç‰¹ç‚¹ã€‚

1.  **æ•ˆç‡**ï¼šåœ¨è®­ç»ƒå’Œé¢„æµ‹æ—¶ä¿æŒå¿«é€Ÿï¼Œä½¿å…¶é€‚ç”¨äºå…·æœ‰è¿ç»­ç‰¹å¾çš„å¤§è§„æ¨¡åº”ç”¨ã€‚

1.  **æ•°æ®çš„çµæ´»æ€§**ï¼šèƒ½å¤Ÿå¾ˆå¥½åœ°å¤„ç†å°å‹å’Œå¤§å‹æ•°æ®é›†ï¼Œæ ¹æ®æ‰‹å¤´é—®é¢˜çš„è§„æ¨¡è¿›è¡Œè°ƒæ•´ã€‚

1.  **è¿ç»­ç‰¹å¾å¤„ç†**ï¼šæ“…é•¿å¤„ç†è¿ç»­å’Œå®å€¼ç‰¹å¾ï¼Œéå¸¸é€‚åˆé¢„æµ‹å®å€¼è¾“å‡ºæˆ–å¤„ç†ç‰¹å¾åœ¨è¿ç»­æ€§ä¸Šå˜åŒ–çš„æ•°æ®ã€‚

## ç¼ºç‚¹ï¼š

1.  **ç‹¬ç«‹æ€§å‡è®¾**ï¼šä»ç„¶å‡è®¾ç‰¹å¾åœ¨ç»™å®šç±»åˆ«çš„æ¡ä»¶ä¸‹æ˜¯ç‹¬ç«‹çš„ï¼Œä½†åœ¨æ‰€æœ‰å®é™…åœºæ™¯ä¸­ï¼Œè¿™ä¸€å‡è®¾å¯èƒ½å¹¶ä¸æˆç«‹ã€‚

1.  **é«˜æ–¯åˆ†å¸ƒå‡è®¾**ï¼šå½“ç‰¹å¾å€¼çœŸæ­£ç¬¦åˆæ­£æ€åˆ†å¸ƒæ—¶æ•ˆæœæœ€å¥½ã€‚éæ­£æ€åˆ†å¸ƒå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ç†æƒ³ï¼ˆä½†å¯ä»¥é€šè¿‡æˆ‘ä»¬è®¨è®ºçš„å¹‚å˜æ¢æ¥ä¿®æ­£ï¼‰

1.  **å¯¹å¼‚å¸¸å€¼çš„æ•æ„Ÿæ€§**ï¼šè®­ç»ƒæ•°æ®ä¸­çš„å¼‚å¸¸å€¼å¯èƒ½ä¼šæ˜¾è‘—å½±å“å…¶è¡¨ç°ï¼Œå› ä¸ºå¼‚å¸¸å€¼ä¼šæ‰­æ›²å‡å€¼å’Œæ–¹å·®çš„è®¡ç®—ã€‚

# æœ€åçš„å¤‡æ³¨

é«˜æ–¯æœ´ç´ è´å¶æ–¯æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„åˆ†ç±»å™¨ï¼Œé€‚ç”¨äºå¤„ç†æ¶‰åŠè¿ç»­æ•°æ®çš„å¹¿æ³›åº”ç”¨ã€‚å®ƒå¤„ç†å®å€¼ç‰¹å¾çš„èƒ½åŠ›ä½¿å…¶åœ¨äºŒåˆ†ç±»ä»»åŠ¡ä¹‹å¤–æœ‰æ›´å¹¿æ³›çš„åº”ç”¨ï¼Œå› æ­¤æˆä¸ºè®¸å¤šåº”ç”¨ä¸­çš„é¦–é€‰ã€‚

è™½ç„¶å®ƒå¯¹æ•°æ®ï¼ˆç‰¹å¾ç‹¬ç«‹æ€§å’Œæ­£æ€åˆ†å¸ƒï¼‰åšå‡ºäº†ä¸€äº›å‡è®¾ï¼Œä½†å½“è¿™äº›æ¡ä»¶æ»¡è¶³æ—¶ï¼Œå®ƒèƒ½æä¾›ç¨³å¥çš„è¡¨ç°ï¼Œå› æ­¤åœ¨åˆå­¦è€…å’Œç»éªŒä¸°å¯Œçš„æ•°æ®ç§‘å­¦å®¶ä¸­éƒ½å¾ˆå—æ¬¢è¿ï¼Œå› ä¸ºå®ƒåœ¨ç®€æ´æ€§å’Œå¼ºå¤§åŠŸèƒ½ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ã€‚

# ğŸŒŸ é«˜æ–¯æœ´ç´ è´å¶æ–¯ç®€åŒ–ç‰ˆ

```py
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import PowerTransformer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Load the dataset
dataset_dict = {
    'Rainfall': [0.0, 2.0, 7.0, 18.0, 3.0, 3.0, 0.0, 1.0, 0.0, 25.0, 0.0, 18.0, 9.0, 5.0, 0.0, 1.0, 7.0, 0.0, 0.0, 7.0, 5.0, 3.0, 0.0, 2.0, 0.0, 8.0, 4.0, 4.0],
    'Temperature': [29.4, 26.7, 28.3, 21.1, 20.0, 18.3, 17.8, 22.2, 20.6, 23.9, 23.9, 22.2, 27.2, 21.7, 27.2, 23.3, 24.4, 25.6, 27.8, 19.4, 29.4, 22.8, 31.1, 25.0, 26.1, 26.7, 18.9, 28.9],
    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],
    'WindSpeed': [2.1, 21.2, 1.5, 3.3, 2.0, 17.4, 14.9, 6.9, 2.7, 1.6, 30.3, 10.9, 3.0, 7.5, 10.3, 3.0, 3.9, 21.9, 2.6, 17.3, 9.6, 1.9, 16.0, 4.6, 3.2, 8.3, 3.2, 2.2],
    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']
}

df = pd.DataFrame(dataset_dict)

# Prepare data for model
X, y = df.drop('Play', axis=1), (df['Play'] == 'Yes').astype(int)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)

# Apply PowerTransformer
pt = PowerTransformer(standardize=True)
X_train_transformed = pt.fit_transform(X_train)
X_test_transformed = pt.transform(X_test)

# Train the model
nb_clf = GaussianNB()
nb_clf.fit(X_train_transformed, y_train)

# Make predictions
y_pred = nb_clf.predict(X_test_transformed)

# Check accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
```

## è¿›ä¸€æ­¥é˜…è¯»

å¯¹äº[GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)åŠå…¶åœ¨ scikit-learn ä¸­çš„å®ç°çš„è¯¦ç»†è§£é‡Šï¼Œè¯»è€…å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼Œè¯¥æ–‡æ¡£æä¾›äº†å…³äºå…¶ä½¿ç”¨å’Œå‚æ•°çš„å…¨é¢ä¿¡æ¯ã€‚

## æŠ€æœ¯ç¯å¢ƒ

æœ¬æ–‡ä½¿ç”¨çš„æ˜¯ Python 3.7 å’Œ scikit-learn 1.5ã€‚è™½ç„¶è®¨è®ºçš„æ¦‚å¿µé€šå¸¸é€‚ç”¨ï¼Œä½†ä¸åŒç‰ˆæœ¬çš„å…·ä½“ä»£ç å®ç°å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚

## å…³äºæ’å›¾

é™¤éå¦æœ‰æ³¨æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…åˆ›ä½œï¼ŒåŒ…å«æ¥è‡ª Canva Pro çš„æˆæƒè®¾è®¡å…ƒç´ ã€‚

ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¾ğ™¡ğ™–ğ™¨ğ™¨ğ™ğ™›ğ™ğ™˜ğ™–ğ™©ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:

![Samy Baladram](img/835013c69e08fec04ad9ca465c2adf6c.png)

[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------)

## åˆ†ç±»ç®—æ³•

[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----04949cef383c--------------------------------)8 ä¸ªæ•…äº‹![](img/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](img/6ea70d9d2d9456e0c221388dbb253be8.png)![](img/7221f0777228e7bcf08c1adb44a8eb76.png)

ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:

![Samy Baladram](img/835013c69e08fec04ad9ca465c2adf6c.png)

[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------)

## å›å½’ç®—æ³•

[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----04949cef383c--------------------------------)5 ä¸ªæ•…äº‹![ä¸€åªå¸¦ç€è¾«å­å’Œç²‰è‰²å¸½å­çš„å¡é€šå¨ƒå¨ƒã€‚è¿™ä¸ªâ€œå‡äººâ€å¨ƒå¨ƒï¼Œå‡­å€Ÿå…¶ç®€å•çš„è®¾è®¡å’Œå¿ƒå½¢è£…é¥°çš„è¡¬è¡«ï¼Œå½¢è±¡åœ°ä»£è¡¨äº†æœºå™¨ä¸­çš„â€œå‡å›å½’æ¨¡å‹â€æ¦‚å¿µã€‚æ­£å¦‚è¿™ä¸ªç©å…·èˆ¬çš„äººç‰©æ˜¯ç®€åŒ–çš„é™æ€äººç±»å½¢è±¡ï¼Œå‡å›å½’æ¨¡å‹æ˜¯ä½œä¸ºæ›´å¤æ‚åˆ†æçš„åŸºçº¿çš„åŸºæœ¬æ¨¡å‹ã€‚](img/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](img/44e6d84e61c895757ff31e27943ee597.png)![](img/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy Baladram](img/835013c69e08fec04ad9ca465c2adf6c.png)

[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------)

## é›†æˆå­¦ä¹ 

[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----04949cef383c--------------------------------)4 ä¸ªæ•…äº‹![](img/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](img/22a5d43568e70222eb89fd36789a9333.png)![](img/8ea1a2f29053080a5feffc709f5b8669.png)
