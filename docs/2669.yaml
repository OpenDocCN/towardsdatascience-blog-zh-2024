- en: On the Programmability of AWS Trainium and Inferentia
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于 AWS Trainium 和 Inferentia 的可编程性
- en: 原文：[https://towardsdatascience.com/on-the-programmability-of-aws-trainium-and-inferentia-cd455826e26c?source=collection_archive---------7-----------------------#2024-11-01](https://towardsdatascience.com/on-the-programmability-of-aws-trainium-and-inferentia-cd455826e26c?source=collection_archive---------7-----------------------#2024-11-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/on-the-programmability-of-aws-trainium-and-inferentia-cd455826e26c?source=collection_archive---------7-----------------------#2024-11-01](https://towardsdatascience.com/on-the-programmability-of-aws-trainium-and-inferentia-cd455826e26c?source=collection_archive---------7-----------------------#2024-11-01)
- en: Accelerating AI/ML Model Training with Custom Operators — Part 4
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速 AI/ML 模型训练与自定义运算符 — 第 4 部分
- en: '[](https://chaimrand.medium.com/?source=post_page---byline--cd455826e26c--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--cd455826e26c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cd455826e26c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cd455826e26c--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--cd455826e26c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page---byline--cd455826e26c--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--cd455826e26c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cd455826e26c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cd455826e26c--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--cd455826e26c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cd455826e26c--------------------------------)
    ·12 min read·Nov 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cd455826e26c--------------------------------)
    ·阅读时间：12 分钟·2024 年 11 月 1 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a73e650c804e6ccb2a840de147054903.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a73e650c804e6ccb2a840de147054903.png)'
- en: Photo by [Agata Bres](https://unsplash.com/@bresia?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Agata Bres](https://unsplash.com/@bresia?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In this post we continue our exploration of the opportunities for runtime optimization
    of machine learning (ML) workloads through custom operator development. This time,
    we focus on the tools provided by the [AWS Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)
    for developing and running new kernels on [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/)
    and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/). With
    the rapid development of the low-level model components (e.g., [attention layers](https://en.wikipedia.org/wiki/Attention_(machine_learning)))
    driving the AI revolution, the programmability of the accelerators used for training
    and running ML models is crucial. Dedicated AI chips, in particular, must offer
    a worthy alternative to the widely used and highly impactful general-purpose GPU
    (GPGPU) development frameworks, such as [CUDA](https://developer.nvidia.com/cuda-toolkit)
    and [Triton](https://triton-lang.org/main/index.html).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们继续探索通过自定义运算符开发来优化机器学习（ML）工作负载运行时性能的机会。这次，我们重点介绍 [AWS Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)
    提供的工具，用于在 [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/) 和
    [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/) 上开发和运行新内核。随着推动
    AI 革新的低层次模型组件（例如，[注意力层](https://en.wikipedia.org/wiki/Attention_(machine_learning)))的快速发展，用于训练和运行
    ML 模型的加速器的可编程性变得至关重要。特别是专用 AI 芯片，必须提供一种有价值的替代方案，以应对广泛使用且具有深远影响的通用 GPU（GPGPU）开发框架，如
    [CUDA](https://developer.nvidia.com/cuda-toolkit) 和 [Triton](https://triton-lang.org/main/index.html)。
- en: In previous posts (e.g., [here](/ai-model-optimization-on-aws-inferentia-and-trainium-cfd48e85d5ac)
    and [here](/a-first-look-at-aws-trainium-1e0605071970)) we explored the opportunity
    for building and running ML models on AWS’s custom-built AI chips using the the
    dedicated [AWS Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/).
    In its most recent release of the SDK (version [2.20.0](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html#id8)),
    AWS introduced the [Neuron Kernel Interface (NKI)](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/index.html)
    for developing custom kernels for [NeuronCore-v2](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html),
    the underlying accelerator powering both [Trainium](https://aws.amazon.com/machine-learning/trainium/)
    and [Inferentia2](https://aws.amazon.com/machine-learning/inferentia/). The NKI
    interface joins another API that enables [NeuronCore-v2](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html)
    programmability, [Neuron Custom C++ Operators](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/index.html).
    In this post we will explore both opportunities and demonstrate them in action.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的文章中（例如，[这里](/ai-model-optimization-on-aws-inferentia-and-trainium-cfd48e85d5ac)和[这里](/a-first-look-at-aws-trainium-1e0605071970)），我们探讨了在AWS定制的AI芯片上构建和运行ML模型的机会，使用的是专用的[AWS
    Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/)。在SDK的最新版本（[2.20.0](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html#id8)）中，AWS引入了用于开发自定义内核的[Neuron内核接口（NKI）](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/index.html)，该内核支持底层加速器[NeuronCore-v2](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html)，这一加速器为[Trainium](https://aws.amazon.com/machine-learning/trainium/)和[Inferentia2](https://aws.amazon.com/machine-learning/inferentia/)提供动力。NKI接口与另一个API配合使用，该API使[NeuronCore-v2](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html)能够进行编程，即[Neuron自定义C++操作符](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/index.html)。在本文中，我们将探讨这两种机会并展示其应用。
- en: Disclaimers
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 免责声明
- en: Importantly, this post should not be viewed as a substitute for the official
    [AWS Neuron SDK documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html).
    At the time of this writing the Neuron SDK APIs for custom kernel development
    is in Beta, and may change by the time you read this. The examples we share are
    intended for demonstrative purposes, only. We make no claims as to their optimality,
    robustness, durability, or accuracy. Please do not view our mention of any platforms,
    tools, APIs, etc., as an endorsement for their use. The best choices for any project
    depend on the specifics of the use-case at hand and warrant appropriate investigation
    and analysis.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，本篇文章不应被视为官方[AWS Neuron SDK文档](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)的替代。在撰写本文时，Neuron
    SDK的自定义内核开发API仍处于Beta阶段，在你阅读本文时可能会发生变化。我们分享的示例仅供演示用途，不能保证其最优性、鲁棒性、耐用性或准确性。请不要将我们提到的任何平台、工具、API等视为对其使用的支持。任何项目的最佳选择取决于具体的使用案例，需进行适当的调查和分析。
- en: Developing Custom Kernels for Neuron Cores
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为神经核心开发自定义内核
- en: Although the list of ML models supported by the Neuron SDK is continuously growing,
    some operations remain either unsupported or implemented suboptimally. By exposing
    APIs for Neuron kernel customization, the SDK empowers developers to create and/or
    optimize the low-level operations that they need, greatly increasing the opportunity
    for running ML workloads on Trainium and Inferentia.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Neuron SDK支持的ML模型列表不断增长，但一些操作仍然不受支持或实现不尽如人意。通过开放Neuron内核自定义API，SDK使开发者能够创建和/或优化他们所需的底层操作，极大地增加了在Trainium和Inferentia上运行ML工作负载的机会。
- en: As discussed in our [previous posts](/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12)
    in this series, fully leveraging the power of these AI chips requires a detailed
    understanding of their low-level architecture.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在本系列的[上一篇文章](/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12)中讨论的，要充分利用这些AI芯片的强大性能，需要详细了解其底层架构。
- en: The Neuron Core Architecture
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经核心架构
- en: The NKI documentation includes a [dedicated section](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#)
    on the architecture design of NeuronCore-v2 and its implications on custom operator
    development. Importantly, there are many differences between Neuron cores and
    their AI accelerator counterparts (e.g., GPUs and TPUs). Optimizing for Neuron
    cores requires a unique set of strategies and skills.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: NKI文档包含了一个[专门的章节](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#)介绍NeuronCore-v2的架构设计及其对自定义操作符开发的影响。重要的是，Neuron核心与其AI加速器对等体（例如GPU和TPU）之间有许多差异。针对Neuron核心的优化需要一套独特的策略和技能。
- en: Similar to other dedicated AI chips, NeuronCore-v2 includes several internal
    [acceleration engines](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#neuroncore-v2-compute-engines),
    each of which specializes in performing certain types of computations. The engines
    can be run asynchronously and in parallel. The [Neuron Compiler](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/compiler/index.html)
    is responsible for transforming ML models into low-level operations and optimizing
    the choice of compute engine for each one.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他专用AI芯片类似，NeuronCore-v2包括多个内部[加速引擎](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#neuroncore-v2-compute-engines)，每个引擎专门执行某些类型的计算。各引擎可以异步并行运行。[Neuron编译器](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/compiler/index.html)负责将机器学习模型转化为低级操作，并优化每个操作所使用的计算引擎。
- en: The [**Tensor engine**](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#tensor-engine)
    specializes in matrix multiplication. The [**Vector**](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#vector-engine)
    and [**Scalar**](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#scalar-engine)
    engines both operate on tensors with the Vector engine specializing in reduction
    operations and the Scalar engine in non-linear functions. [**GpSimd**](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#gpsimd-engine)
    is a general purpose engine capable of running arbitrary C/C++ programs. Note
    that while the [NKI](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/index.html)
    interface exposes access to all four compute engines, [custom C++ operators](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/index.html)
    are designed specifically for the [GpSimd](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#gpsimd-engine).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Tensor引擎**](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#tensor-engine)专门用于矩阵乘法。[**Vector引擎**](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#vector-engine)和[**Scalar引擎**](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#scalar-engine)都操作张量，其中Vector引擎专注于归约操作，Scalar引擎则专注于非线性函数。[**GpSimd引擎**](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#gpsimd-engine)是一个通用引擎，能够运行任意C/C++程序。请注意，尽管[NKI](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/index.html)接口暴露了对所有四个计算引擎的访问，[自定义C++操作符](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/index.html)是专门为[GpSimd](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#gpsimd-engine)引擎设计的。'
- en: More details on the capabilities of each engine can be found in the architecture
    documentation. Furthermore, the [NKI Instruction Set Architecture (ISA)](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/nki.isa.html)
    documentation provides details on the engines on which different low-level operations
    are run.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个引擎的能力的更多细节可以在架构文档中找到。此外，[NKI指令集架构（ISA）](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/nki.isa.html)文档提供了不同低级操作所运行的引擎的详细信息。
- en: Another important aspect of the Neuron chip is its [memory architecture](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#data-movement).
    A Neuron device includes three types of memory, HBM, SBUF, and PSUM. An intimate
    understanding of the capacities and capabilities of each one is crucial for optimal
    kernel development.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Neuron芯片的另一个重要方面是其[内存架构](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#data-movement)。一个Neuron设备包括三种类型的内存，HBM、SBUF和PSUM。深入了解每种内存的容量和功能对优化内核开发至关重要。
- en: Given the architecture overview, you might conclude that Neuron kernel development
    requires high expertise. While this may be true for creating fully optimized kernels
    that leverage all the capabilities of the Neuron core, our aim is to demonstrate
    the accessibility, value, and potential of the Neuron custom kernel APIs — even
    for non-expert developers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 根据架构概述，你可能会得出结论，Neuron内核开发需要高度专业的知识。虽然这对于创建充分优化、充分利用Neuron核心所有功能的内核来说是正确的，但我们的目标是展示Neuron自定义内核API的可访问性、价值和潜力——即使对于非专家开发者也是如此。
- en: Custom NKI Kernels
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义NKI内核
- en: The [NKI](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/index.html)
    interface is a Python-level API that exposes the use of the Neuron core compute
    engines and memory resources to ML developers. The [NKI Getting Started](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/getting_started.html)
    guide details the setup instructions and provides a soft landing with a simple
    “hello world” kernel. The [NKI Programming Model](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html)
    guide details the three stages of a typical NKI kernel (loading inputs, running
    operations on the computation engines, and storing outputs) and introduces the
    NKI Tile and [Tile-based operations](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html#nki-pm-tile).
    The [NKI tutorials](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/tutorials.html)
    demonstrate a variety of NKI kernel sample applications, with each one introducing
    new core NKI APIs and capabilities. Given the presumed optimality of the sample
    kernels, one possible strategy for developing new kernels could be to 1) identify
    a sample that is similar to the operation you wish to implement and then 2) use
    it as a baseline and iteratively refine and adjust it to achieve the specific
    functionality you require.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[NKI](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/index.html)接口是一个Python级API，它向ML开发者公开了Neuron核心计算引擎和内存资源的使用。
    [NKI入门指南](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/getting_started.html)详细介绍了设置说明，并通过一个简单的“hello
    world”内核提供了一个平稳的入门体验。[NKI编程模型](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html)指南详细说明了典型NKI内核的三个阶段（加载输入、在计算引擎上运行操作和存储输出），并介绍了NKI
    Tile和[基于Tile的操作](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html#nki-pm-tile)。[NKI教程](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/tutorials.html)展示了多种NKI内核示例应用程序，每个示例都引入了新的核心NKI
    API和功能。考虑到这些示例内核的假定最优性，开发新内核的一种可能策略是：1) 确定一个与你想要实现的操作相似的示例，2) 然后将其作为基准，迭代优化并调整，以实现你所需的特定功能。'
- en: The [NKI API Reference Manual](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/index.html#nki-api-reference)
    details the Python API for kernel development. With a syntax and semantics that
    are similar to [Triton](https://triton-lang.org/main/index.html) and [NumPy](https://numpy.org/doc/stable/),
    the [NKI language](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/nki.language.html)
    definition aims to maximize accessibility and ease of use. However, it is important
    to note that NKI kernel development is limited to the operations defined in the
    [NKI](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/nki.html)
    library, which (as of the time of this writing) are fewer and more constrained
    than in libraries such as [Triton](https://triton-lang.org/main/index.html) and
    [NumPy](https://numpy.org/doc/stable/).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[NKI API参考手册](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/index.html#nki-api-reference)详细介绍了用于内核开发的Python
    API。其语法和语义与[Triton](https://triton-lang.org/main/index.html)和[NumPy](https://numpy.org/doc/stable/)类似，[NKI语言](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/nki.language.html)的定义旨在最大化可访问性和易用性。然而，需要注意的是，NKI内核开发仅限于[NKI](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/nki.html)库中定义的操作，这些操作（截至本文撰写时）比[Triton](https://triton-lang.org/main/index.html)和[NumPy](https://numpy.org/doc/stable/)等库中的操作要少且受限。'
- en: Toy Example — A GIOU Kernel
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 — 一个GIOU内核
- en: As in our [previous posts](/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12),
    we assess the use of NKI by building a custom implementation of the [Generalized
    Intersection Over Union (GIOU)](https://giou.stanford.edu/) operation on a pair
    of batches of input boxes. Since GIOU involves pixel-wise operations, we used
    the [*exp* kernel](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html#tile-size-considerations)
    from the [NKI Programming](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html)
    guide as a reference point and incorporated the use of NKI’s [advanced tensor
    indexing](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html#advanced-tensor-indexing)
    in our implementation. To facilitate debugging in a CPU environment, we also added
    options to run the code using the [nki.simulate_kernel](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.simulate_kernel.html#nki.simulate_kernel)
    and [nki.language.device_print.html](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.language.device_print.html)
    APIs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的[上一篇文章](/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12)一样，我们通过构建[广义交集比率（GIOU）](https://giou.stanford.edu/)操作的自定义实现，来评估NKI的使用。由于GIOU涉及逐像素操作，我们参考了[NKI编程](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html)指南中的[*exp*内核](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html#tile-size-considerations)，并将NKI的[高级张量索引](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html#advanced-tensor-indexing)技术融入到我们的实现中。为了在CPU环境中便于调试，我们还添加了使用[nki.simulate_kernel](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.simulate_kernel.html#nki.simulate_kernel)和[nki.language.device_print.html](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.language.device_print.html)API运行代码的选项。
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To run our GIOU kernel, we generate two batches of random boxes and feed them
    to our function:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行我们的GIOU内核，我们生成了两批随机框并将其输入到我们的函数中：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To assess the performance of our NKI kernel, we will compare it with the following
    naive implementation of GIOU in PyTorch:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们NKI内核的性能，我们将其与以下在PyTorch中实现的GIOU朴素实现进行比较：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We use the following benchmarking utility to compare the runtime performance
    of our two functions:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下基准测试工具来比较我们两个函数的运行时性能：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Runtime Environment
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行环境
- en: We ran our script on an [Amazon EC2 inf2.xlarge](https://aws.amazon.com/ec2/instance-types/inf2/)
    instance (containing two [Neuron cores](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html#neuroncores-v2-arch)
    and four vCPUs). We used the most recent version of the [Deep Learning AMI for
    Neuron](https://aws.amazon.com/releasenotes/aws-deep-learning-ami-neuron-ubuntu-22-04/)
    available at the time of this writing, “Deep Learning AMI Neuron (Ubuntu 22.04)
    20241027”, with [AWS Neuron 2.20.1](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html#neuron-2-20-1-10-25-2024)
    and [PyTorch 2.1](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/torch-neuronx/introducing-pytorch-2-1.html).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一个[Amazon EC2 inf2.xlarge](https://aws.amazon.com/ec2/instance-types/inf2/)实例上运行了脚本（该实例包含两个[Neuron核心](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html#neuroncores-v2-arch)和四个vCPU）。我们使用了在撰写本文时最新版本的[Deep
    Learning AMI for Neuron](https://aws.amazon.com/releasenotes/aws-deep-learning-ami-neuron-ubuntu-22-04/)，即“Deep
    Learning AMI Neuron (Ubuntu 22.04) 20241027”，以及[AWS Neuron 2.20.1](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html#neuron-2-20-1-10-25-2024)和[PyTorch
    2.1](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/torch-neuronx/introducing-pytorch-2-1.html)。
- en: Results
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: Our custom GIOU kernel demonstrated an average runtime of 0.211 milliseconds
    compared to 0.293, amounting to a 39% performance boost. Keep in mind that these
    results are unique to our toy example. Other operators, particularly ones that
    include matrix multiplications (and utilize the Tensor engine) are likely to exhibit
    different comparative results.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们自定义的GIOU内核展示了平均运行时间为0.211毫秒，相比之下，原始实现为0.293毫秒，性能提升了39%。请注意，这些结果仅适用于我们的示例。其他操作符，特别是包含矩阵乘法（并利用Tensor引擎）的操作，可能会展示不同的比较结果。
- en: Optimizing NKI Kernel Performance
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化NKI内核性能
- en: The next step in our kernel development — beyond the scope of this post — would
    to be to analyze the performance of the GIOU kernel using the dedicated [Neuron
    Profiler](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/neuron_profile_for_nki.html)
    in order to identify bottlenecks and optimize our implementation. Please see the
    [NKI performance guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/nki_perf_guide.html#nki-perf-guide)
    for more details.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们内核开发的下一步——超出本文范围——是使用专用的[Neuron Profiler](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/neuron_profile_for_nki.html)分析
    GIOU 内核的性能，以识别瓶颈并优化实现。有关更多详情，请参阅[NKI 性能指南](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/nki_perf_guide.html#nki-perf-guide)。
- en: Neuron Custom C++ Operators
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Neuron 自定义 C++ 运算符
- en: The second method for creating a custom Neuron kernel is to build a C++ operator
    for the [GpSimd engine](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#gpsimd-engine).
    This method is described in the [Neuron Custom C++ Operators Developer Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/programming-guide/custom-c%2B%2B-operators-devguide.html#feature-custom-operators-devguide)
    and demonstrated in the [Neuron Custom C++ Operators in MLP](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/tutorials/customop-mlp-training.html#neuronx-customop-mlp-tutorial)
    and [Neuron Custom C++ Operators Performance Optimization](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/tutorials/customop-mlp-perf-opt.html#neuronx-customop-mlp-perf)
    tutorials.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自定义 Neuron 内核的第二种方法是为[GpSimd 引擎](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#gpsimd-engine)构建
    C++ 运算符。这种方法在[Neuron 自定义 C++ 运算符开发者指南](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/programming-guide/custom-c%2B%2B-operators-devguide.html#feature-custom-operators-devguide)中进行了描述，并在[Neuron
    自定义 C++ 运算符在 MLP 中的应用](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/tutorials/customop-mlp-training.html#neuronx-customop-mlp-tutorial)和[Neuron
    自定义 C++ 运算符性能优化](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/tutorials/customop-mlp-perf-opt.html#neuronx-customop-mlp-perf)教程中进行了演示。
- en: 'Neuron Custom C++ Operators presents an opportunity for “kernel fusion” on
    the GpSimd engine by facilitating the combination of multiple low-level operations
    into a single kernel execution. This approach can significantly reduce the overhead
    associated with: 1) loading multiple individual kernels, and 2) transferring data
    between different memory regions.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Neuron 自定义 C++ 运算符为 GpSimd 引擎上的“内核融合”提供了机会，通过将多个低级操作组合成一个内核执行。这种方法可以显著减少与以下两项操作相关的开销：1）加载多个单独的内核，2）在不同的内存区域之间传输数据。
- en: Toy Example — A GIOU C++ Kernel
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 玩具示例——一个 GIOU C++ 内核
- en: In the code block below we implement a C++ GIOU operator for Neuron and save
    it to a file named *giou.cpp*. Our kernel uses the [TCM accessor](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/api-reference-guide/custom-ops-ref-guide.html#tcm-accessor)
    for optimizing memory read and write performance and applies the *multicore* setting
    in order to [use all eight of the GpSimd’s internal processors](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/api-reference-guide/custom-ops-ref-guide.html#using-multiple-gpsimd-cores).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们实现了一个用于 Neuron 的 C++ GIOU 运算符，并将其保存到名为*giou.cpp*的文件中。我们的内核使用[TCM
    访问器](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/api-reference-guide/custom-ops-ref-guide.html#tcm-accessor)来优化内存读写性能，并应用*多核*设置，以便[使用
    GpSimd 的八个内部处理器](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/api-reference-guide/custom-ops-ref-guide.html#using-multiple-gpsimd-cores)。
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We require a separate *shape.cpp* file that defines the output shape of our
    GIOU function and registers our custom operator with the Neuron library:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个单独的*shape.cpp*文件，用于定义 GIOU 函数的输出形状并将自定义运算符注册到 Neuron 库中：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The *build.py* script compiles the C++ operator and exposes it as a Python
    API:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*build.py* 脚本编译 C++ 运算符，并将其作为 Python API 暴露：'
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The compilation script generates a *libgiou.so* library containing the implementation
    of our C++ GIOU operator. In the code block below we load the library and measure
    the performance of our custom kernel using the benchmarking utility defined above:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 编译脚本生成一个*libgiou.so*库，其中包含我们 C++ GIOU 运算符的实现。在下面的代码块中，我们加载库并使用上面定义的基准工具来衡量自定义内核的性能：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Runtime Environment
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行时环境
- en: We used the same Neuron environment from our NKI experiments to compile and
    test our C++ kernel. Please note the [installation steps](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/programming-guide/custom-c%2B%2B-operators-devguide.html#setup-installation)
    that are required for custom C++ operator development.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与NKI实验相同的Neuron环境来编译和测试我们的C++内核。请注意[安装步骤](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/programming-guide/custom-c%2B%2B-operators-devguide.html#setup-installation)，这些步骤是开发自定义C++运算符所必需的。
- en: Results
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: Our C++ GIOU kernel demonstrated an average runtime of 0.061 milliseconds —
    nearly five times faster than our baseline implementation. This is presumably
    a result of “kernel fusion”, as discussed above.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的C++ GIOU内核展示了平均运行时间为0.061毫秒——几乎比我们的基线实现快五倍。这大概是“内核融合”所带来的结果，如上所述。
- en: Conclusion
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The table below summarizes the runtime results of our experiments.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了我们实验的运行时结果。
- en: '![](../Images/a4a93747c3b62268e577bf26e1df984e.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4a93747c3b62268e577bf26e1df984e.png)'
- en: Avg time of different GIOU implementations (lower is better) — by Author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 不同GIOU实现的平均时间（越低越好）——作者
- en: Please keep in mind that these results are specific to the toy example and runtime
    environment used in this study. The comparative results of other kernels might
    be very different — depending on the degree to which they can leverage the Neuron
    core’s internal compute engines.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这些结果仅适用于本研究中使用的玩具示例和运行环境。其他内核的比较结果可能会大不相同——这取决于它们能够在多大程度上利用Neuron核心的内部计算引擎。
- en: The table below summarizes some of the differences we observed between the two
    methods of AWS Neuron kernel customization.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了我们观察到的AWS Neuron内核定制的两种方法之间的一些差异。
- en: '![](../Images/301bdafd4bab9a99c908d04758d1ef91.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/301bdafd4bab9a99c908d04758d1ef91.png)'
- en: Comparison between kernel customization tools (by Author)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 内核定制工具的比较（作者）
- en: Through its high-level Python interface, the NKI APIs expose the power of the
    Neuron acceleration engines to ML developers in an accessible and user-friendly
    manner. The low-level C++ Custom Operators library enables even greater programmability,
    but is limited to the [GpSimd engine](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#gpsimd-engine).
    By effectively combining both tools, developers can fully leverage the AWS Neuron
    architecture’s capabilities.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过其高级Python接口，NKI API以一种易于访问和用户友好的方式向机器学习开发者暴露了Neuron加速引擎的强大功能。低级C++自定义运算符库则提供了更高的可编程性，但仅限于[GpSimd引擎](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#gpsimd-engine)。通过有效结合这两种工具，开发者可以充分利用AWS
    Neuron架构的能力。
- en: Summary
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: With the AI revolution in full swing, many companies are developing advanced
    new AI chips to meet the growing demand for compute. While public announcements
    often highlight these chips’ runtime performance, cost savings, and energy efficiency,
    several core capabilities are essential to making these chips and their software
    stacks truly viable for ML development. These capabilities include robust debugging
    tools, performance analysis and optimization utilities, **programmability**, and
    more.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能革命的全面推进，许多公司正在开发先进的AI芯片，以满足日益增长的计算需求。尽管公开公告通常强调这些芯片的运行时性能、成本节约和能效，若要使这些芯片及其软件栈在机器学习开发中真正可行，还需要一些核心能力。这些能力包括强大的调试工具、性能分析与优化工具、**可编程性**等。
- en: In this post, we focused on the utilities available for programming AWS’s homegrown
    AI accelerators, [Trainium](https://aws.amazon.com/machine-learning/trainium/)
    and [Inferentia](https://aws.amazon.com/machine-learning/inferentia/), and demonstrated
    their use in building custom ML operations. These tools empower developers to
    optimize the performance of their ML models on AWS’s AI chips and open up new
    opportunities for innovation and creativity.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们聚焦于用于编程AWS自研AI加速器的工具，[Trainium](https://aws.amazon.com/machine-learning/trainium/)和[Inferentia](https://aws.amazon.com/machine-learning/inferentia/)，并展示了它们在构建自定义机器学习操作中的应用。这些工具使开发者能够优化AWS
    AI芯片上机器学习模型的性能，并开辟了创新和创意的新机遇。
