<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Interpretable kNN (ikNN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Interpretable kNN (ikNN)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpretable-knn-iknn-33d38402b8fc?source=collection_archive---------3-----------------------#2024-05-14">https://towardsdatascience.com/interpretable-knn-iknn-33d38402b8fc?source=collection_archive---------3-----------------------#2024-05-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="98f9" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">An interpretable classifier</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@wkennedy934?source=post_page---byline--33d38402b8fc--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="W Brett Kennedy" class="l ep by dd de cx" src="../Images/b3ce55ffd028167326c117d47c64c467.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*v8pf2r3SPMLuHoSmF4IwlA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--33d38402b8fc--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@wkennedy934?source=post_page---byline--33d38402b8fc--------------------------------" rel="noopener follow">W Brett Kennedy</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--33d38402b8fc--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ld le">2</span></p></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="lj k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lk an ao ap ig ll lm ln" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lo cn"><div class="l ae"><div class="ab cb"><div class="lp lq lr ls lt lu ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="e3b6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Very often in when working on classification or regression problems in machine learning, we’re strictly interested in getting the most accurate model we can. In some cases, though, we’re interested also in the interpretability of the model. While models like XGBoost, CatBoost, and LGBM can be very strong models, it can be difficult to determine why they’ve made the predictions they have, or how they will behave with unseen data. These are what are called black-box models, models where we do not understand specifically why the make the predictions they do.</p><p id="03ea" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In many contexts this is fine; so long as we know they are reasonably accurate most of the time, they can be very useful, and it is understood they will be incorrect on occasion. For example, on a website, we may have a model that predicts which ads will be most likely to generate sales if shown to the current user. If the model behaves poorly on the rare occasion, this may affect revenues, but there are no major issues; we just have a model that’s sub-optimal, but generally useful.</p><p id="e0b5" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">But, in other contexts, it can be very important to know why the models make the predictions that they do. This includes high-stakes environments, such as in medicine and security. It also includes environments where we need to ensure there are no biases in the models related to race, gender or other protected classes. It’s important, as well, in environments that are audited: where it’s necessary to understand the models to determine they are performing as they should.</p><p id="e37b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Even in these cases, it is sometimes possible to use black-box models (such as boosted models, neural networks, Random Forests and so on) and then perform what is called post-hoc analysis. This provides an explanation, after the fact, of why the model likely predicted as it did. This is the field of Explainable AI (XAI), which uses techniques such as proxy models, feature importances (e.g. SHAP), counterfactuals, or ALE plots. These are very useful tools, but, everything else equal, it is preferable to have a model that is interpretable in the first place, at least where possible. XAI methods are very useful, but they do have limitations.</p><p id="b402" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">With proxy models, we train a model that is interpretable (for example, a shallow decision tree) to learn the behavior of the black-box model. This can provide some level of explanation, but will not always be accurate and will provide only approximate explanations.</p><p id="7562" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Feature importances are also quite useful, but indicate only what the relevant features are, not how they relate to the prediction, or how they interact with each other to form the prediction. They also have no ability to determine if the model will work reasonably with unseen data.</p><p id="b5ed" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">With interpretable models, we do not have these issues. The model is itself comprehensible and we can know exactly why it makes each prediction. The problem, though, is: interpretable models can have lower accuracy then black-box models. Most interpretable models, for most problems, will not be competitive with boosted models or neural networks. For any given problem, it may be necessary to try several interpretable models before an interpretable model of sufficient accuracy can be found, if any can be.</p><p id="9d62" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">There are a number of interpretable models available today, but unfortunately, very few. Among these are decision trees, rules lists (and rule sets), GAMs (Generalized Additive Models, such as Explainable Boosted Machines), and linear/logistic regression. These can each be useful where they work well, but the options are limited. The implication is: it can be impossible for many projects to find an interpretable model that performs satisfactorily. There can be real benefits in having more options available.</p><p id="960e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We introduce here another interpretable model, called ikNN, or interpretable k Nearest Neighbors. This is based on an ensemble of 2d kNN models. While the idea is straightforward, it is also surprisingly effective. And quite interpretable. While it is not competitive in terms of accuracy with state of the art models for prediction on tabular data such as CatBoost, it can often provide accuracy that is close and that is sufficient for the problem. It is also quite competitive with decision trees and other existing interpretable models.</p><p id="93d2" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Interestingly, it also appears to have stronger accuracy than plain kNN models.</p><p id="5a12" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The main page for the project is: <a class="af ni" href="https://github.com/Brett-Kennedy/ikNN" rel="noopener ugc nofollow" target="_blank">https://github.com/Brett-Kennedy/ikNN</a></p><p id="cf5e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The project defines a single class called iKNNClassifier. This can be included in any project copying the interpretable_knn.py file and importing it. It provides an interface consistent with scikit-learn classifiers. That is, we generally simply need to create an instance, call fit(), and call predict(), similar to using Random Forest or other scikit-learn models.</p><p id="d9aa" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Using, under the hood, an ensemble of 2d kNN’s provides a number of advantages. One is the normal advantage we always see with ensembling: we get more reliable predictions than when relying on a single model.</p><p id="b1c0" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Another is that 2d spaces are straightforward to visualize. The model currently requires numeric input (as is the case with kNN), so all categorical features need to be encoded, but once this is done, every 2d space can be visualized as a scatter plot. This provides a high degree of interpretability.</p><p id="20a2" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">And, it’s possible to determine the most relevant 2d spaces for each prediction, which allows us to present a small number of plots for each record. This allows fairly simple as well as complete visual explanations for each record.</p><p id="9b64" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">ikNN is, then, an interesting model, as it is based on ensembling, but actually increases interpretability, while the opposite is more often the case.</p><h1 id="20ed" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Standard kNN Classifiers</h1><p id="85cc" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">kNN models are less-used than many others, as they are not usually as accurate as boosted models or neural networks, or as interpretable as decision trees. They are, though, still widely used. They work based on an intuitive idea: the class of an item can be predicted based on the class of most of the items that are most similar to it.</p><p id="f706" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, if we look at the iris dataset (as is used in an example below), we have three classes, representing three types of iris. If we collect another sample of iris and wish to predict which of the three types of iris it is, we can look at the most similar, say, 10 records from the training data, determine what their classes are, and take the most common of these.</p><p id="478f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this example, we chose 10 to be the number of nearest neighbors to use to estimate the class of each record, but other values may be used. This is specified as a hyperparameter (the k parameter) with kNN and ikNN models. We wish set k so as to use to a reasonable number of similar records. If we use too few, the results may be unstable (each prediction is based on very few other records). If we use too many, the results may be based on some other records that aren’t that similar.</p><p id="5699" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We also need a way to determine which are the most similar items. For this, at least by default, we use the Euclidean distance. If the dataset has 20 features and we use k=10, then we find the closest 10 points in the 20-dimensional space, based on their Euclidean distances.</p><p id="4cd4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Predicting for one record, we would find the 10 closest records from the training data and see what their classes are. If 8 of the 10 are class Setosa (one of the 3 types of iris), then we can assume this row is most likely also Setosa, or at least this is the best guess we can make.</p><p id="e7ba" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">One issue with this is, it breaks down when there are many features, due to what’s called <em class="ok">the curse of dimensionality</em>. An interesting property of high-dimensional spaces is that with enough features, distances between the points start to become meaningless.</p><p id="148a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">kNN also uses all features equally, though some may be much more predictive of the target than others. The distances between points, being based on Euclidean (or sometimes Manhattan or other distance metrics) are calculated considering all features equally. This is simple, but not always the most effective, given many features may be irrelevant to the target. Assuming some feature selection has been performed, this is less likely, but the relevance of the features will still not be equal.</p><p id="cef6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">And, the predictions made by kNN predictors are uninterpretable. The algorithm is quite intelligible, but the predictions can be difficult to understand. It’s possible to list the k nearest neighbors for any record, which provides some insight into the prediction made for that record, but it’s difficult to see why a given set of records are the most similar, particularly where there are many features.</p><h1 id="954a" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">The ikNN Algorithm</h1><p id="5097" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">The ikNN model first takes each pair of features and creates a standard 2d kNN classifier using these features. So, if a table has 10 features, this creates (10 choose 2), or 45 models, one for each unique pair of features.</p><p id="7b41" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It then assesses their accuracies with respect to predicting the target column using the training data. Given this, the ikNN model determines the predictive power of each 2d subspace. In the case of 45 2d models, some will be more predictive than others. To make a prediction, the 2d subspaces known to be most predictive are used, optionally weighted by their predictive power on the training data.</p><p id="e03c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Further, at inference, the purity of the set of nearest neighbors around a given row within each 2d space may be considered, allowing the model to weight more heavily both the subspaces proven to be more predictive with training data and the subspaces that appear to be the most consistent in their prediction with respect to the current instance.</p><p id="6dee" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Consider two subspaces and a point shown here as a star. In both cases, we can find the set of k points closest to the point. Here we draw a green circle around the star, though the set of points do not actually form a circle (though there is a radius to the kth nearest neighbor that effectively defines a neighborhood).</p><figure class="oo op oq or os ot ol om paragraph-image"><div class="ol om on"><img src="../Images/8c88aef07e3d9300cff2dcf6367e5b1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*ZmZmqnBxTSFyAA_ogv0_jA.png"/></div></figure><p id="257e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">These plots each represent a pair of features. In the case of the left plot, there is very high consistency among the neighbors of the star: they are entirely red. In the right plot, there is no little consistency among the neigbhors: some are red and some are blue. The first pair of features appears to be more predictive of the record than the second pair of features, which ikNN takes advantage of.</p><p id="918a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This approach allows the model to consider the influence all input features, but weigh them in a manner that magnifies the influence of more predictive features, and diminishes the influence of less-predictive features.</p><h1 id="3f1b" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Example</h1><p id="6aaf" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">We first demonstrate ikNN with a toy dataset, specifically the iris dataset. We load in the data, do a train-test split, and make predictions on the test set.</p><pre class="oo op oq or os ov ow ox bp oy bb bk"><span id="8c6b" class="oz nk fq ow b bg pa pb l pc pd">from sklearn.datasets import load_iris<br/>from interpretable_knn import ikNNClassifier<br/><br/>iris = load_iris()<br/>X, y = iris.data, iris.target<br/><br/>clf = ikNNClassifier()<br/>X_train, X_test, y_train, y_test = train_test_split(<br/>   X, y, test_size=0.33, random_state=42)<br/>clf.fit(X_train, y_train)<br/>y_pred = clf.predict(X_test)</span></pre><p id="2946" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For prediction, this is all that is required. But, ikNN also provides tools for understanding the model, specifically the graph_model() and graph_predictions() APIs.</p><p id="5def" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For an example of graph_model():</p><pre class="oo op oq or os ov ow ox bp oy bb bk"><span id="4ed1" class="oz nk fq ow b bg pa pb l pc pd">ikNN.graph_model(X.columns)</span></pre><figure class="oo op oq or os ot ol om paragraph-image"><div role="button" tabindex="0" class="pf pg ed ph bh pi"><div class="ol om pe"><img src="../Images/12d6dfed1bd65f1421b38f373615b451.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1xdTJegJKx2UO7q65khUxg.png"/></div></div></figure><p id="3cfe" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This provides a quick overview of the dataspace, plotting, by default, five 2d spaces. The dots show the classes of the training data. The background color shows the predictions made by the 2d kNN for each region of the 2d space.</p><p id="72d7" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The graph_predictions() API will explain a specific row, for example:</p><figure class="oo op oq or os ot ol om paragraph-image"><div role="button" tabindex="0" class="pf pg ed ph bh pi"><div class="ol om pj"><img src="../Images/09682e05b2511bea57fee9ebb839f0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0dUw9THvJJ96sC9EIjJ3Mg.png"/></div></div></figure><p id="6e3f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Here, the row being explained (Row 0) is shown as a red star. Again, by default, five plots are used by default, but for simplicity, this uses just two. In both plots, we can see where Row 0 is located relative to the training data and the predictions made by the 2D kNN for this 2D space.</p><h1 id="80d1" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Visualizations</h1><p id="cbc5" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">Although it is configurable, by default only five 2d spaces are used by each ikNN prediction. This ensures the prediction times are fast and the visualizations simple. It also means that the visualizations are showing the true predictions, not a simplification of the predictions, ensuring the predictions are completely interpretable</p><p id="8e3c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For most datasets, for most rows, all or almost all 2d spaces tend to agree on the prediction. However, where the predictions are incorrect, it may be useful to examine more 2d plots in order to better tune the hyperparameters to suit the current dataset.</p><h1 id="2826" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Accuracy Tests</h1><p id="0947" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">A set of tests were performed using a random set of 100 classification datasets from OpenML. Comparing the F1 (macro) scores of standard kNN and ikNN models, ikNN had higher scores for 58 datasets and kNN for 42.</p><p id="f6af" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">ikNN’s do even a bit better when performing grid search to search for the best hyperparameters. After doing this for both models on all 100 datasets, ikNN performed the best in 76 of the 100 cases. It also tends to have smaller gaps between the train and test scores, suggesting more stable models than standard kNN models.</p><p id="08af" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">ikNN models can be somewhat slower, but they tend to still be considerably faster than boosted models, and still very fast, typically taking well under a minute for training, usually only seconds.</p><p id="3837" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The github page provides some more examples and analysis of the accuracy.</p><h1 id="b1ad" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Conclusions</h1><p id="7fb7" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">While ikNN is likely not the strongest model where accuracy is the primary goal (though, as with any model, it can be on occasion), it is likely a model that should be tried where an interpretable model is necessary.</p><p id="5860" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This page provided the basic information necessary to use the tool. It simply necessary to download the .py file (<a class="af ni" href="https://github.com/Brett-Kennedy/ikNN/blob/main/ikNN/interpretable_knn.py" rel="noopener ugc nofollow" target="_blank">https://github.com/Brett-Kennedy/ikNN/blob/main/ikNN/interpretable_knn.py</a>), import it into your code, create an instance, train and predict, and (where desired), call graph_predictions() to view the explanations for any records you wish.</p><p id="8da8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">All images are by author.</p></div></div></div></div>    
</body>
</html>