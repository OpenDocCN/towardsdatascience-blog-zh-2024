<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Binary Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Binary Classification</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/binary-classification-unpacking-the-real-significance-and-limitations-of-traditional-metrics-810069be630c?source=collection_archive---------11-----------------------#2024-01-12">https://towardsdatascience.com/binary-classification-unpacking-the-real-significance-and-limitations-of-traditional-metrics-810069be630c?source=collection_archive---------11-----------------------#2024-01-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="2be7" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Unpacking the Real Significance and Limitations of Traditional Metrics</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@costaleirbag?source=post_page---byline--810069be630c--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="gabriel costa" class="l ep by dd de cx" src="../Images/7ae80a70ac22128573243d5cb9764102.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*BQE-trP_BHKKb9tUr0hnEg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--810069be630c--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@costaleirbag?source=post_page---byline--810069be630c--------------------------------" rel="noopener follow">gabriel costa</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--810069be630c--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="f4dc" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Introduction</h1><p id="fbd1" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The work of classification can be seen as a way of summarizing the complexity of structures into finite classes, and it is often useful to make life boring enough that we can reduce such structures into <em class="oa">two single types</em>. These labels may have obvious interpretations, as in the case where we differentiate Statisticians from Data Scientists (possibly using income as a unique plausible feature), or they may even be a suffocating attempt to reduce experimental evidence into sentences, rejecting or not a null hypothesis. Embracing the metalanguage, we attribute a classification label to the work itself of sum up stuff into two different types:<strong class="ng fr"> Binary Classification</strong>. This work is an attempt at a deeper description of this specific label, <em class="oa">bringing a probabilistic interpretation of what the decision process means and the metrics we use to evaluate our results.</em></p><h1 id="b400" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Modeling populations as distributions</h1><p id="7542" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">When we try to describe and differentiate an object, we aim to find specific characteristics that highlight its uniqueness. <em class="oa">It is expected that for many attributes this difference is not consistently accurate across a population.</em></p><p id="8028" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">For an usual problem of two classes with different n features (V1, … Vn), we can se how these feature values distribute and try to make conclusions from them:</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh oi"><img src="../Images/b5ab6ecf21e310b75e2beb2c58d51dcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6iXgughMitPNHS57Sk5Vw.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Figure 1: Distribution of 8 different features Vn for two different classes (negative class: red; positive class: blue). This data is from a Kaggle challenge which is referenced at the end of the text.</figcaption></figure><p id="31c9" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">If the task were to use one of those features to guide our decision process, the strategy of deciding, given an individual’s <em class="oa">v_n</em> value, <br/>it would be intuitive to predict the class with the highest frequency (or highest probability, if our histograms are good estimators of our distributions). For instance, if v4 measure for a individual were higher than 5, then it would be very likely that it is a positive.</p><p id="bfdb" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">However, <em class="oa">we can do more than that</em>, and take advantage different features, so that we can synthesize the information into a single distribution. <strong class="ng fr">This is the job of the score function <em class="oa">S(x)</em></strong>. The score function will do a regression to squeeze the feature distributions in one unique distribution <strong class="ng fr"><em class="oa">P(s, y)</em></strong>, which can be conditioned to each of the classes, with <strong class="ng fr"><em class="oa">P(s|y=0)</em> for negatives</strong> <em class="oa">(y=0) </em>and<strong class="ng fr"> <em class="oa">P(s|y=1)</em> for positives </strong><em class="oa">(y=1)</em>.</p><p id="8881" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">From this single distribution, <strong class="ng fr">we choose</strong> <strong class="ng fr">a decision threshold t </strong>that determines whether our estimate for a given point — which is represented by <em class="oa">ŷ </em>— will be positive or negative. <strong class="ng fr">If s is greater than <em class="oa">t</em>, we assign a positive label; otherwise, a negative.</strong></p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh oz"><img src="../Images/1d44cfd6bb8e63197363e22a89615bc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sy-wR_taxtWXJwaY_oLMNw.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Figure 2: graphical illustration of binary classification process. The score function squeezes the n-dimensional feature space into a distribution P(s, y).</figcaption></figure><p id="7647" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk"><strong class="ng fr"><em class="oa">Given the distribution</em></strong><em class="oa"> </em><strong class="ng fr"><em class="oa">P(s, y, t)</em></strong><em class="oa"> where s, y and t represents the values of the score, class and threshold respectively, </em><strong class="ng fr"><em class="oa">we have a complete description of our classifier.</em></strong></p><h1 id="9e63" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Metrics for our classifier (marginal and conditional distributions)</h1><p id="a3d5" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk"><em class="oa">Developing metrics for our classifier can be regarded as the pursuit of quantifying the discriminative nature of p(s|P) and p(s|N).</em></p><p id="ae9f" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">Typically will be a overlap over the two distributions <em class="oa">p(s|P)</em> and <em class="oa">p(s|N)</em> that makes a perfect classification impossible. So, given a threshold, one could ask what is the probability <em class="oa">p(s &gt; t|N) </em>— False Positive Rate (FPR) — that we are misclassifying negative individuals as positives, for example.</p><p id="0c74" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">Surely we can pile up a bunch of metrics and even give names — inconsistently — for each of them. But, for all purposes, it will be sufficient define four probabilities and their associated rates for the classifier:</p><ol class=""><li id="354c" class="ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz pa pb pc bk"><strong class="ng fr">True Positive Rate (<em class="oa">tpr</em>)</strong>: <em class="oa">p(s &gt; t|P) = TP/(TP+FN)</em>;</li><li id="ff41" class="ne nf fq ng b go pd ni nj gr pe nl nm nn pf np nq nr pg nt nu nv ph nx ny nz pa pb pc bk"><strong class="ng fr">False Positive Rate (<em class="oa">fpr</em>): </strong><em class="oa">p(s &gt; t|N) = FP/(FP+TN)</em>;</li><li id="aeee" class="ne nf fq ng b go pd ni nj gr pe nl nm nn pf np nq nr pg nt nu nv ph nx ny nz pa pb pc bk"><strong class="ng fr">True Negative Rate (<em class="oa">tnr</em>): </strong><em class="oa">p(s ≤ t|N) = TN/(FP+TN)</em>;</li><li id="c51b" class="ne nf fq ng b go pd ni nj gr pe nl nm nn pf np nq nr pg nt nu nv ph nx ny nz pa pb pc bk"><strong class="ng fr">False Negative Rate (<em class="oa">fnr</em>): </strong><em class="oa">p(s ≤ t|P) = FN/(TP+FN)</em>.</li></ol><p id="6140" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">If you are already familiar with the subject, you may have noticed that <em class="oa">these are the metrics that we define from confusion matrix of our classifier</em>. As this matrix is defined for each chosen decision threshold, we can view it as a representation of the conditional distribution <em class="oa">P(ŷ, y|t)</em>, where each of these objects is part of a class of confusion matrices that completely describe the performance of our classifier.</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh pi"><img src="../Images/6a5663e68de330b06703f6189c7735c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LgKRQ1Zr6AcPrISqtvCDBQ.jpeg"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Figure 3: Each confusion matrix can be viewed as a conditional <em class="pj">p(ŷ, y|t), and </em>a class of all possible confusion matrices becomes a complete description of the classifier’s performance.</figcaption></figure><p id="915e" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">Therefore, the error ratios <em class="oa">fpr</em> and <em class="oa">fnr</em> are metrics that quantifies how and how much the two conditional score distributions intersect:</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh pk"><img src="../Images/15e9c69be544b6caf8b2aaaeb74a15e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNLVR6qeE-0ZMkxrIh1f9g.jpeg"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Figure 4: the performance estimators for a classifier are different measures used to describe the overlap or separation between the two distributions, p(s|P) and p(s|N).</figcaption></figure><h2 id="025c" class="pl mj fq bf mk pm pn po mn pp pq pr mq nn ps pt pu nr pv pw px nv py pz qa qb bk">Summarizing performance: the ROC curve</h2><p id="144d" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk"><em class="oa">Since the ratios are constrained</em> by <em class="oa">tpr</em> + <em class="oa">fnr</em> = 1 and <em class="oa">fpr</em> + <em class="oa">tnr</em> = 1, this would mean that w<em class="oa">e have just 2 degrees of freedom to describe our performance</em>.</p><p id="ec06" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">The ROC curve is a curve parameterized by <em class="oa">t</em>, described by <em class="oa">(x(t), y(t)) = (fpr(t), tpr(t))</em> as points against orthogonal axes. This will provide a digestible summary to visualize the performance of our classifier for all different threshold values, rather than just a single one.</p><blockquote class="qc"><p id="6dbc" class="qd qe fq bf qf qg qh qi qj qk ql nz dx">The best choice of <strong class="al"><em class="pj">t</em></strong> is not generally known in advance but must be determined as part of the classifier construction. — ROC Curves for Continuous Data (2009, Chapman and Hall).</p></blockquote><p id="486a" class="pw-post-body-paragraph ne nf fq ng b go qm ni nj gr qn nl nm nn qo np nq nr qp nt nu nv qq nx ny nz fj bk">We are aiming to explore the concept of treating probability distributions, so let's imagine <em class="oa">what would be the base case for a completely inefficient classifier.</em> Since the effectiveness of our predictions hinges on the discriminative nature <em class="oa">p(s|P)</em> and <em class="oa">p(s|N)</em> are, in the case where <em class="oa">p(s|P) = p(s|N) = p(s)</em>, we encounter a prime example of this inefficiency.</p><p id="1981" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">If we do the exercise of modeling each of these conditionals as gaussians with means separated by different values, we can see how the performance varies clearly:</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh qr"><img src="../Images/c9469a4a07ec4f96ca8f33cb2fb04275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yi0f99pu2bpIg5FoyBnMJw.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Figure 5: The performance of a classifier improves as score distributions become more discriminative.</figcaption></figure><p id="a77b" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">This visualization will serve as a valuable aid in comprehending the probabilistic interpretation of a crucial classifier metric — named Area Under the Curve (AUC) — that we will delve into later.</p><h2 id="0ff0" class="pl mj fq bf mk pm pn po mn pp pq pr mq nn ps pt pu nr pv pw px nv py pz qa qb bk">Some properties of the ROC curve</h2><p id="af00" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The ROC curve can be described as a function <em class="oa">y = h(x) </em>where <em class="oa">x</em> and <em class="oa">y</em> are the false and true positive rates respectively, which are in turn parameterized by <em class="oa">t</em> in the form <em class="oa">x(t) = p(s &gt; t|N)</em> and and <em class="oa">y(t) = p(s &gt; t|P)</em>.</p><p id="0cb9" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">We can take advantage of this to derive the following properties:</p><ol class=""><li id="e94c" class="ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz pa pb pc bk"><strong class="ng fr"><em class="oa">y = h(x) is a monotone increasing function that lies above the line defined by (0, 0) and (1, 1);</em></strong></li><li id="3b18" class="ne nf fq ng b go pd ni nj gr pe nl nm nn pf np nq nr pg nt nu nv ph nx ny nz pa pb pc bk"><strong class="ng fr"><em class="oa">The ROC curve is unaltered if the classification scores undergo a strictly increasing transformation;</em></strong></li></ol><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh qs"><img src="../Images/9c111b8bd9edefdaf4d0406cd739c011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wpgAlBah4MCNipbgaehqEQ.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Figure 6: A monotonically increasing transformation applied to the score distribution does not alter the ROC curve, as it preserves the order of the regression.</figcaption></figure><p id="b397" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk"><strong class="ng fr">This property is what makes the calibration process for a classifier possible.</strong></p><p id="14ad" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">3. <strong class="ng fr"><em class="oa">For a well-defined slope of ROC at the point with threshold value t:</em></strong></p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh qt"><img src="../Images/f837e538b46b20c86912cb92e4f3d8bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*Wjl_5dnvN8Tyog1QXFId5w.png"/></div></figure><p id="320b" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk"><strong class="ng fr"><em class="oa">where p(t|P) represents the density distribution for the cumulative distribution p(s ≤ t | P) (and same for p(t|N)).</em></strong></p><h2 id="e515" class="pl mj fq bf mk pm pn po mn pp pq pr mq nn ps pt pu nr pv pw px nv py pz qa qb bk">Binary Classification as Hypothesis Testing: justifying our approach</h2><p id="bb3b" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">When viewing the classification process through a Bayesian inference lens, our objective is to deduce the posterior probability <em class="oa">p(P|t)</em>, which represents the odds of a point with threshold value <em class="oa">t </em>belonging to the positive class. Therefore, <strong class="ng fr">the slope derivative defined on property 3 can be viewed as the likelihood ratio<em class="oa"> L(t).</em></strong></p><blockquote class="qc"><p id="d672" class="qd qe fq bf qf qg qh qi qj qk ql nz dx">This ratio <em class="pj">[L(t)] </em>tell us how much probable is a value of t of the classifier to have occurred in population P than in population N, which in turn can be interpreted as a measure of confidence in allocation to population P. — ROC Curves for Continuous Data (2009, Chapman and Hall).</p></blockquote><p id="e76b" class="pw-post-body-paragraph ne nf fq ng b go qm ni nj gr qn nl nm nn qo np nq nr qp nt nu nv qq nx ny nz fj bk">This is an important fact because, by establishing an equivalence relationship between the binary classification process and hypothesis testing, we have a justification for why we classify based on a threshold.</p><p id="89d2" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">If we formulate the classification problem with the <em class="oa">null hypothesis H0 that the individual belongs to population N against the alternative H1 that he belongs to population P</em>, we can draw the relationship:</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh qu"><img src="../Images/a7f611c319c6a896fb27d9548d706474.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lvhwoyoKX8g6lxf8Rat45Q.jpeg"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Figure 7: The connection between the Binary Classification process and Hypothesis Testing is visualized through the confusion matrix. The marginal ratio metrics are related to the standard alpha and beta values in Hypothesis Testing.</figcaption></figure><p id="e096" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">The <strong class="ng fr">Neyman-Pearson Lemma</strong> establishes that the most powerful test — which achieves the highest <em class="oa">1-β</em> value — with a significance level <em class="oa">α</em>, possesses a region<em class="oa"> R</em> encompassing all <em class="oa">s</em> values of <em class="oa">S</em> for which:</p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh qv"><img src="../Images/0fb1caf9c90e053cf9bdcbed5b2e3079.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*8SXZ3eeUfmb7PbAy1VbtMA.png"/></div></figure><p id="c647" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">where <em class="oa">α </em>is sufficient to determine <em class="oa">k</em> by the condition <em class="oa">p(s </em>∈ <em class="oa">R|N) = α.</em></p><p id="0b66" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">This implies that when we score the population in a manner where <em class="oa">L(s)</em> is monotonically increasing, a one-to-one relationship between <em class="oa">s</em> and <em class="oa">k</em> ensures that choosing a rule that exceeds a particular threshold is the optimal decision.</p><p id="a67c" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">For our fictitious cases where our classifier assigns a normal distribution for each class, it's direct the likelihood will satisfy this condition:</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh qw"><img src="../Images/5675ac3396f75ae4ed22aa74a237e118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kq18JTVDTLVPFwNpwXhj5w.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Figure 8: the likelihood ratio is strictly increasing (and, more precisely, exponentially) for the case of binormal distribution of scores.</figcaption></figure><p id="4458" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">This is not always the case for a real-world problem, where score distributions are not necessarily well-behaved in this sense. We can use the <em class="oa">Kaggle</em> dataset to understand this, estimating the density of our conditionals by Kernel Density Estimation (KDE).</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh qx"><img src="../Images/89a056e1b2baeb8734a3932afdf2a67f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DPFdosRE1jInKiviUJkm9A.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Figure 9: This is the assignment of scores made by a logistic regression in a balanced data situation. The classifier was trained without parameter tuning.</figcaption></figure><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh qy"><img src="../Images/49ec9627282ae6bc704c6e8f4198dfe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WwU6_w84h1ufFgNQUWHBvA.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Figure 10: for a real case (Kaggle dataset), we can see that the likelihood is not necessarily monotonic.</figcaption></figure><blockquote class="qz ra rb"><p id="340e" class="ne nf oa ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk"><strong class="ng fr">This means that higher scores are not necessarily associated with a greater chance of the individual being from a positive class.</strong></p></blockquote><h1 id="aceb" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk"><strong class="al">ROC-AUC probabilistic interpretation, and why we should take care with it</strong></h1><p id="15fa" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The <em class="oa">Area Under the Curve (AUC)</em> is probably the most widely used value to summarize the results expressed by a ROC curve. <em class="oa">It is defined as the integral of y(x) from 0 to 1</em>, as the name suggests. Notably, a perfect classifier’s performance is epitomized by the point (0, 1) on the orthogonal axis, denoting zero probability of misclassification negative points and an unambiguous assurance of correctly classifying positives.</p><p id="5d4f" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">The approach treated on figure 5 give us a hint that the probabilistic interpretation of a good fit must have to do with consistency in assigning high score values for positive individuals and low score for negatives ones. This is exactly the case, since one can prove — as referred in [1] — that <em class="oa">the AUC is equivalent to the probability of a positive individual have a score value (Sp) higher than a negative individual score (Sn):</em></p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh rc"><img src="../Images/a108b375acccbb36861e8533a4aa65e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*s-NXUn1FktOS7oGuoqTfLQ.png"/></div></figure><p id="14fe" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">The critical point to consider is this: AUC is designed to provide a single-number estimate of your classifier’s performance. However, when it comes to making practical decisions, a threshold <em class="oa">t</em> must be chosen to suit the specific requirements of your problem. The challenge arises because, as discussed earlier, the optimal decision based on a threshold occurs when the likelihood ratio is monotonically increasing, which is not always the case in practice.</p><p id="a422" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk"><em class="oa">Consequently, even if you have a high AUC value, very close to 1, it may not be sufficient to determine whether your classifier is truly capable of optimal classification based on a decision boundary. In such cases, </em><strong class="ng fr"><em class="oa">achieving a high AUC value alone may not guarantee the effectiveness of your classifier in practical decision-making scenarios.</em></strong></p><h1 id="103a" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Conclusion</h1><p id="96ab" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">This probability interpretation of binary classification may offers a profound understanding of the intricacies involved in the process. By modeling populations as distributions, we can make informed decisions based on the likelihood of an individual belonging to a particular class. The ROC curve serves as a valuable tool for <strong class="ng fr">summarize</strong> how the choice of threshold impacts classification efficiency. Furthermore,<strong class="ng fr"> the connection between binary classification and hypothesis testing emphasizes the reason why we classify by threshold values</strong>. It is essential to remember that while the Area Under the Curve (AUC) is a commonly used performance metric, <strong class="ng fr">it may not always guarantee optimal practical decision-making</strong>, underscoring the significance of choosing the right threshold. <strong class="ng fr">This probabilistic interpretation enriches our understanding of binary classification, making it a powerful framework for tackling real-world problems.</strong></p><h1 id="eaa2" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Acknowledgements</h1><p id="879d" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Special thanks to Renato Vicente, who introduced me to visualizing the classifier through the space of confusion matrices, and encouraged me to write this article.</p></div></div></div><div class="ab cb rd re rf rg" role="separator"><span class="rh by bm ri rj rk"/><span class="rh by bm ri rj rk"/><span class="rh by bm ri rj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="76ce" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk"><em class="oa">All images and graphs are by the author.</em></p><p id="9d5c" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk"><em class="oa">Also, you can find me on </em><a class="af rl" href="https://www.linkedin.com/in/gcosta98/" rel="noopener ugc nofollow" target="_blank"><strong class="ng fr"><em class="oa">Linkedin</em></strong></a><em class="oa">.</em></p></div></div></div><div class="ab cb rd re rf rg" role="separator"><span class="rh by bm ri rj rk"/><span class="rh by bm ri rj rk"/><span class="rh by bm ri rj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="e8dd" class="mi mj fq bf mk ml rm gq mn mo rn gt mq mr ro mt mu mv rp mx my mz rq nb nc nd bk">References</h1><p id="a879" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">[1] Krzanowski, Wojtek J., and David J. Hand. <em class="oa">ROC curves for continuous data</em>. Crc Press, 2009</p><p id="8b1d" class="pw-post-body-paragraph ne nf fq ng b go ob ni nj gr oc nl nm nn od np nq nr oe nt nu nv of nx ny nz fj bk">[2] Muschelli, John (2019–12–23). “ROC and AUC with a binary predictor: a potentially misleading metric”. <em class="oa">Journal of Classification</em>. Springer Science and Business Media LLC. <strong class="ng fr">37</strong> (3): 696–708. <a class="af rl" href="https://en.wikipedia.org/wiki/Doi_(identifier)" rel="noopener ugc nofollow" target="_blank">doi</a>:<a class="af rl" href="https://doi.org/10.1007%2Fs00357-019-09345-1" rel="noopener ugc nofollow" target="_blank">10.1007/s00357–019–09345–1</a>. <a class="af rl" href="https://en.wikipedia.org/wiki/ISSN_(identifier)" rel="noopener ugc nofollow" target="_blank">ISSN</a> <a class="af rl" href="https://www.worldcat.org/issn/0176-4268" rel="noopener ugc nofollow" target="_blank">0176–4268</a>.</p><h1 id="fb1d" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Dataset</h1><p id="9860" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk"><a class="af rl" href="https://www.kaggle.com/datasets/joebeachcapital/credit-card-fraud" rel="noopener ugc nofollow" target="_blank">Credit Card Fraud (kaggle.com)</a></p></div></div></div></div>    
</body>
</html>