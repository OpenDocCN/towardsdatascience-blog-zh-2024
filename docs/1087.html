<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Practical Computer Simulations for Product Analysts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Practical Computer Simulations for Product Analysts</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/practical-computer-simulations-for-product-analysts-4d3a17957f64?source=collection_archive---------7-----------------------#2024-04-30">https://towardsdatascience.com/practical-computer-simulations-for-product-analysts-4d3a17957f64?source=collection_archive---------7-----------------------#2024-04-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ddcc" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Part 2: Using bootstrap for observations and A/B tests</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://miptgirl.medium.com/?source=post_page---byline--4d3a17957f64--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mariya Mansurova" class="l ep by dd de cx" src="../Images/b1dd377b0a1887db900cc5108bca8ea8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*7fFHr8XBAuR_SgJknIyODA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4d3a17957f64--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://miptgirl.medium.com/?source=post_page---byline--4d3a17957f64--------------------------------" rel="noopener follow">Mariya Mansurova</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4d3a17957f64--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">21 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/203b2849d9accb8982770f471c74a6a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*1GPwhSp39gQnUtH46OKQCg.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by DALL-E 3</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="5a22" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In <a class="af of" href="https://medium.com/towards-data-science/practical-computer-simulations-for-product-analysts-90b5deb6a54e" rel="noopener">the first part</a> of this series, we've discussed the basic ideas of computer simulations and how you can leverage them to answer "what-if" questions. It's impossible to talk about simulations without bootstrap.</p><p id="8c4e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Bootstrap in statistics is a practical computer method for estimating the statistics of probability distributions. It is based on the repeated generation of samples using the Monte Carlo method from an existing sample. This method allows for simple and fast estimation of various statistics (such as confidence intervals, variance, correlation, etc.) for complex models.</p><p id="2b9c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">When I learned about bootstrap in the statistics course, it felt a bit hacky. Instead of learning multiple formulas and criteria for different cases, you can just write a couple of lines of code and get confidence interval estimations for any custom and complicated use case. It sounds like magic.</p><p id="d05d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">And it really is. Now, when even your laptop can run thousands of simulations in minutes or even seconds, bootstrap is a powerful tool in your analytical toolkit that can help you in many situations. So, I believe that it's worth learning or refreshing your knowledge about it.</p><p id="180e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In this article, we will talk about the idea behind bootstrap, understand when you should use it, learn how to get confidence intervals for different metrics and analyse the results of A/B tests.</p><h1 id="4600" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">What is bootstrap?</h1><p id="9992" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Actually, bootstrap is exceptionally straightforward. We need to run simulations drawing elements from our sample distribution with replacement, and then we can make conclusions based on this distribution.</p><p id="d6fe" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let's look at the simple example when we have four elements: 1, 2, 3 and 4. Then, we can simulate many other collections of 4 elements where each element might be 1, 2, 3 or 4 with equal probabilities and use these simulations to understand, for example, how the mean value might change.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ph"><img src="../Images/92dd778412b7a02e18fa442f7acb1b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zh4ld7184xYqiACkUJePGA.png"/></div></div></figure><p id="845d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The statistical meaning behind bootstrap is that we consider that the actual population has precisely the same distribution as our sample (or the population consists of an infinite number of our sample copies). Then, we just assume that we know the general population and use it to understand the variability in our data.</p><p id="f0fd" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Usually, when using a classical statistical approach, we assume that our variable follows some known distribution (for example, normal). However, we don't need to make any assumptions regarding the nature of the distribution in Bootstrap. It's pretty handy and helps to analyse even very complex custom metrics.</p><p id="7818" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It's almost impossible to mess up the bootstrap estimations. So, in many cases, I would prefer it to the classical statistical methods. The only drawback is computational time. If you're working with big data, simulations might take hours, while you can get classical statistics estimations within seconds.</p><p id="c5c4" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">However, there are cases when it's pretty challenging to get estimations without bootstrap. Let's discuss the best use cases for bootstrap:</p><ul class=""><li id="7b5d" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pi pj pk bk">if you have <a class="af of" rel="noopener" target="_blank" href="/linear-regression-models-and-influential-points-4ee844adac6d">outliers or influential points</a> in your data;</li><li id="9b69" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk">if your sample is relatively small (roughly less than 100 cases);</li><li id="a8df" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk">if your data distribution is quite far from normal or other theoretical distribution, for example, it has several modes;</li><li id="dc39" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk">if you're working with custom metrics (for example, the share of cases closed within SLA or percentiles).</li></ul><p id="ef60" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Bootstrap is a wonderful and powerful statistical concept. Let's try to use it for descriptive statistics.</p><h1 id="5d61" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Working with observational data</h1><p id="1d2f" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">First, let's start with the observational data and work with a synthetic dataset. Imagine we are helping a fitness club to set up a new fitness program that will help clients prepare for the London Marathon. We got the first trial group of 12 customers and measured their results.</p><p id="c89f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Here is the data we have.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pq"><img src="../Images/ac17472b9283e4e11147d5428164d721.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6u2pHjkz4T98HdAxhhU8cA.png"/></div></div></figure><p id="e153" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We collected just three fields for each of the 12 customers:</p><ul class=""><li id="64f1" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pi pj pk bk"><code class="cx pr ps pt pu b">races_before</code> — numbers of races customers had before our program,</li><li id="94e9" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk"><code class="cx pr ps pt pu b">kms_during_program</code> — kilometres clients run during our program,</li><li id="4177" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk"><code class="cx pr ps pt pu b">finished_marathon</code> — whether the program was successful and a customer has finished the London Marathon.</li></ul><p id="6301" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We aim to set up a goal-focused fair program that incentivises our clients to train with us more and achieve better results. So, we would like to return the money if the client has run at least 150 kilometres during the preparation but couldn't complete the marathon. However, before launching this program, we would like to make some estimations: what distance clients cover during preparation and the estimated share of refunds. We need it to ensure that our business is profitable and sustainable.</p><h2 id="c9f0" class="pv oh fq bf oi pw px py ol pz qa qb oo ns qc qd qe nw qf qg qh oa qi qj qk ql bk">Estimating average</h2><p id="84c4" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Let's start with estimating the average distance. We can try to leverage our knowledge of mathematical statistics and use formulas for confidence intervals.</p><p id="f68e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">To do so, we need to make an assumption about the distribution of this variable. The most commonly used is a normal distribution. Let's try it.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="1493" class="qp oh fq pu b bg qq qr l qs qt">import numpy as np<br/>from scipy.stats import norm, t<br/><br/>def get_normal_confidence_interval(data, confidence=0.95):<br/>    # Calculate sample mean and standard deviation<br/>    sample_mean = np.mean(data)<br/>    sample_std = np.std(data, ddof=1)  <br/>    n = len(data)<br/><br/>    # Calculate the critical value (z) based on the confidence level<br/>    z = norm.ppf((1 + confidence) / 2)<br/><br/>    # Calculate the margin of error using standard error<br/>    margin_of_error = z * sample_std / np.sqrt(n)<br/><br/>    # Calculate the confidence interval<br/>    lower_bound = sample_mean - margin_of_error<br/>    upper_bound = sample_mean + margin_of_error<br/><br/>    return lower_bound, upper_bound<br/><br/>get_normal_confidence_interval(df.kms_during_program.values)<br/># (111.86, 260.55)</span></pre><p id="30e2" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The other option often used with real-life data is t-test distribution, which gives a broader confidence interval (since it assumes fatter tales than normal distribution).</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="8114" class="qp oh fq pu b bg qq qr l qs qt">def get_ttest_confidence_interval(data, confidence=0.95):<br/>    # Calculate sample mean and standard deviation<br/>    sample_mean = np.mean(data)<br/>    sample_std = np.std(data, ddof=1)  <br/>    n = len(data)<br/><br/>    # Calculate the critical value (z) based on the confidence level<br/>    z = t.ppf((1 + confidence) / 2, df=len(data) - 1)<br/><br/>    # Calculate the margin of error using standard error<br/>    margin_of_error = z * sample_std / np.sqrt(n)<br/><br/>    # Calculate the confidence interval<br/>    lower_bound = sample_mean - margin_of_error<br/>    upper_bound = sample_mean + margin_of_error<br/><br/>    return lower_bound, upper_bound<br/><br/>get_ttest_confidence_interval(df.kms_during_program.values)<br/># (102.72, 269.69)</span></pre><p id="94ba" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We have a few examples in our sample. Also, there's an outlier: a client with 12 races who managed to run almost 600 km preparing for the marathon, while most other clients run less than 200 km.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qu"><img src="../Images/4f875275a3782cd77b94ddb1006ae8ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-hnhE18PSmfLFmtC9e7J6w.png"/></div></div></figure><p id="e129" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">So, it's an excellent case to use the bootstrap technique to understand the distribution and confidence interval better.</p><p id="9aec" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let's create a function to calculate and visualise the confidence interval:</p><ul class=""><li id="7cee" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pi pj pk bk">We run <code class="cx pr ps pt pu b">num_batches</code> simulations, doing samples with replacement, and calculating the average distance.</li><li id="ce32" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk">Then, based on these variables, we can get a 95% confidence interval: 2.5% and 97.5% percentiles of this distribution.</li><li id="b9e7" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk">Finally, we can visualise the distribution on a chart.</li></ul><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="1db7" class="qp oh fq pu b bg qq qr l qs qt">import tqdm<br/>import matplotlib.pyplot as plt<br/><br/>def get_kms_confidence_interval(num_batches, confidence = 0.95):<br/>    # Running simulations<br/>    tmp = []<br/>    for i in tqdm.tqdm(range(num_batches)):<br/>        tmp_df = df.sample(df.shape[0], replace = True)<br/>        tmp.append(<br/>            {<br/>                'iteration': i,<br/>                'mean_kms': tmp_df.kms_during_program.mean()<br/>            }<br/>        )<br/>    # Saving data<br/>    bootstrap_df = pd.DataFrame(tmp)<br/>    <br/>    # Calculating confidence interval<br/>    lower_bound = bootstrap_df.mean_kms.quantile((1 - confidence)/2)<br/>    upper_bound = bootstrap_df.mean_kms.quantile(1 - (1 - confidence)/2)<br/><br/>    # Creating a chart<br/>    ax = bootstrap_df.mean_kms.hist(bins = 50, alpha = 0.6, <br/>        color = 'purple')<br/>    ax.set_title('Average kms during the program, iterations = %d' % num_batches)<br/>    <br/>    plt.axvline(x=lower_bound, color='navy', linestyle='--', <br/>        label='lower bound = %.2f' % lower_bound)<br/>    <br/>    plt.axvline(x=upper_bound, color='navy', linestyle='--', <br/>        label='upper bound = %.2f' % upper_bound)<br/>    <br/>    ax.annotate('CI lower bound: %.2f' % lower_bound, <br/>                xy=(lower_bound, ax.get_ylim()[1]), <br/>                xytext=(-10, -20), <br/>                textcoords='offset points',  <br/>                ha='center', va='top',  <br/>                color='navy', rotation=90) <br/>    ax.annotate('CI upper bound: %.2f' % upper_bound, <br/>                xy=(upper_bound, ax.get_ylim()[1]), <br/>                xytext=(-10, -20), <br/>                textcoords='offset points',  <br/>                ha='center', va='top',  <br/>                color='navy', rotation=90) <br/>    plt.xlim(ax.get_xlim()[0] - 20, ax.get_xlim()[1] + 20)<br/>    plt.show()</span></pre><p id="4e3a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let's start with a small number of batches to see the first results quickly.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="2104" class="qp oh fq pu b bg qq qr l qs qt">get_kms_confidence_interval(100)</span></pre><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qv"><img src="../Images/63d618d07ccafb044e63995f57c17217.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r321pdKQd7IXuDIq6hcbrw.png"/></div></div></figure><p id="68ec" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We got a bit narrower and skewed to the right confidence interval with bootstrap, which is in line with our actual distribution: <code class="cx pr ps pt pu b">(139.31, 297.99)</code> vs <code class="cx pr ps pt pu b">(102.72, 269.69)</code>.</p><p id="4ea9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">However, with 100 bootstrap simulations, the distribution is not very clear. Let's try to add more iterations. We can see that our distribution consists of multiple modes — for samples with one occurrence of outliers, two occurrences, three, etc.</p></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qv"><img src="../Images/9c3a6753c398f62c7e3c54bd0b3e9491.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*_EuLTtAbBA6GPwNdj4QNUg.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="dc88" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With more iterations, we can see more modes (since more occurrences of the outlier are rarer), but all the confidence intervals are pretty close.</p><p id="97cd" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In the case of bootstrap, adding more iterations doesn't lead to overfitting (because each iteration is independent). I would think about it as increasing the resolution of your image.</p><p id="5b08" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Since our sample is small, running many simulations doesn't take much time. Even 1 million bootstrap iterations take around 1 minute.</p><h2 id="7ff7" class="pv oh fq bf oi pw px py ol pz qa qb oo ns qc qd qe nw qf qg qh oa qi qj qk ql bk">Estimating custom metrics</h2><p id="9c3f" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">As we discussed, bootstrap is handy when working with metrics that are not as straightforward as averages. For example, you might want to estimate the median or share of tasks closed within SLA.</p><p id="df84" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">You might even use bootstrap for something more unusual. Imagine you want to give customers discounts if your delivery is late: 5% discount for 15 minutes delay, 10% — for 1 hour delay and 20% — for 3 hours delay.</p><p id="e1e1" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Getting a confidence interval for such cases theoretically using plain statistics might be challenging, so bootstrap will be extremely valuable.</p><p id="85db" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let's return to our running program and estimate the share of refunds (when a customer ran 150 km but didn't manage to finish the marathon). We will use a similar function but will calculate the refund share for each iteration instead of the mean value.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="bb72" class="qp oh fq pu b bg qq qr l qs qt">import tqdm<br/>import matplotlib.pyplot as plt<br/><br/>def get_refund_share_confidence_interval(num_batches, confidence = 0.95):<br/>    # Running simulations<br/>    tmp = []<br/>    for i in tqdm.tqdm(range(num_batches)):<br/>        tmp_df = df.sample(df.shape[0], replace = True)<br/>        tmp_df['refund'] = list(map(<br/>            lambda kms, passed: 1 if (kms &gt;= 150) and (passed == 0) else 0,<br/>            tmp_df.kms_during_program,<br/>            tmp_df.finished_marathon<br/>        ))<br/>        <br/>        tmp.append(<br/>            {<br/>                'iteration': i,<br/>                'refund_share': tmp_df.refund.mean()<br/>            }<br/>        )<br/><br/>    # Saving data<br/>    bootstrap_df = pd.DataFrame(tmp)<br/><br/>    # Calculating confident interval<br/>    lower_bound = bootstrap_df.refund_share.quantile((1 - confidence)/2)<br/>    upper_bound = bootstrap_df.refund_share.quantile(1 - (1 - confidence)/2)<br/>    <br/>    # Creating a chart<br/>    ax = bootstrap_df.refund_share.hist(bins = 50, alpha = 0.6, <br/>        color = 'purple')<br/>    ax.set_title('Share of refunds, iterations = %d' % num_batches)<br/>    plt.axvline(x=lower_bound, color='navy', linestyle='--',<br/>        label='lower bound = %.2f' % lower_bound)<br/>    plt.axvline(x=upper_bound, color='navy', linestyle='--', <br/>        label='upper bound = %.2f' % upper_bound)<br/>    ax.annotate('CI lower bound: %.2f' % lower_bound, <br/>                xy=(lower_bound, ax.get_ylim()[1]), <br/>                xytext=(-10, -20), <br/>                textcoords='offset points',  <br/>                ha='center', va='top',  <br/>                color='navy', rotation=90) <br/>    ax.annotate('CI upper bound: %.2f' % upper_bound, <br/>                xy=(upper_bound, ax.get_ylim()[1]), <br/>                xytext=(-10, -20), <br/>                textcoords='offset points',  <br/>                ha='center', va='top',  <br/>                color='navy', rotation=90) <br/>    plt.xlim(-0.1, 1)<br/>    plt.show()</span></pre><p id="8176" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Even with 12 examples, we got a 2+ times smaller confidence interval. We can conclude with 95% confidence that less than 42% of customers will be eligible for a refund.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qw"><img src="../Images/f3e12dd8de15bd4f9eb3f8ec61586b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YGCsPoRDdBPyJ9jsTa4AuQ.png"/></div></div></figure><p id="ab5e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">That's a good result with such a small amount of data. However, we can go even further and try to get an estimation of causal effects.</p><h2 id="ae85" class="pv oh fq bf oi pw px py ol pz qa qb oo ns qc qd qe nw qf qg qh oa qi qj qk ql bk">Estimation of effects</h2><p id="00ba" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">We have data about the previous races before this marathon, and we can see how this value is correlated with the expected distance. We can use bootstrap for this as well. We only need to add the linear regression step to our current process.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="a96f" class="qp oh fq pu b bg qq qr l qs qt">def get_races_coef_confidence_interval(num_batches, confidence = 0.95):<br/>    # Running simulations<br/>    tmp = []<br/>    for i in tqdm.tqdm(range(num_batches)):<br/>        tmp_df = df.sample(df.shape[0], replace = True)<br/>        # Linear regression model<br/>        model = smf.ols('kms_during_program ~ races_before', data = tmp_df).fit()<br/>        <br/>        tmp.append(<br/>            {<br/>                'iteration': i,<br/>                'races_coef': model.params['races_before']<br/>            }<br/>        )<br/>    <br/>    # Saving data<br/>    bootstrap_df = pd.DataFrame(tmp)<br/><br/>    # Calculating confident interval<br/>    lower_bound = bootstrap_df.races_coef.quantile((1 - confidence)/2)<br/>    upper_bound = bootstrap_df.races_coef.quantile(1 - (1 - confidence)/2)<br/><br/>    # Creating a chart<br/>    ax = bootstrap_df.races_coef.hist(bins = 50, alpha = 0.6, color = 'purple')<br/>    ax.set_title('Coefficient between kms during the program and previous races, iterations = %d' % num_batches)<br/>    plt.axvline(x=lower_bound, color='navy', linestyle='--', label='lower bound = %.2f' % lower_bound)<br/>    plt.axvline(x=upper_bound, color='navy', linestyle='--', label='upper bound = %.2f' % upper_bound)<br/>    ax.annotate('CI lower bound: %.2f' % lower_bound, <br/>                xy=(lower_bound, ax.get_ylim()[1]), <br/>                xytext=(-10, -20), <br/>                textcoords='offset points',  <br/>                ha='center', va='top',  <br/>                color='navy', rotation=90) <br/>    ax.annotate('CI upper bound: %.2f' % upper_bound, <br/>                xy=(upper_bound, ax.get_ylim()[1]), <br/>                xytext=(10, -20), <br/>                textcoords='offset points',  <br/>                ha='center', va='top',  <br/>                color='navy', rotation=90) <br/>    # plt.legend() <br/>    plt.xlim(ax.get_xlim()[0] - 5, ax.get_xlim()[1] + 5)<br/>    plt.show()<br/><br/>    return bootstrap_df</span></pre><p id="75bd" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can look at the distribution. The confidence interval is above 0, so we can say there's an effect with 95% confidence.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qx"><img src="../Images/010875e9356554ca9c7fb98b1b8bc0e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qjE-ORdKKEaoWqavnxNX3w.png"/></div></div></figure><p id="f6fe" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">You can spot that distribution is bimodal, and each mode corresponds to one of the scenarios:</p><ul class=""><li id="19b0" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pi pj pk bk">The component around 12 is related to samples without an outlier — it's an estimation of the effect of previous races on the expected distance during the program if we disregard the outlier.</li><li id="cbbd" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk">The second component corresponds to the samples when one or several outliers were in the dataset.</li></ul><p id="543c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">So, it's super cool that we can make even estimations for different scenarios if we look at the bootstrap distribution.</p><p id="0e4e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We've learned how to use bootstrap with observational data, but its bread and butter is A/B testing. So, let's move on to our second example.</p><h1 id="38e6" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Simulations for A/B testing</h1><p id="e806" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">The other everyday use case for bootstrap is designing and analysing A/B tests. Let's look at the example. It will also be based on a synthetic dataset that shows the effect of the discount on customer retention. Imagine we are working on an e-grocery product and want to test whether our marketing campaign with a 20 EUR discount will affect customers' spending.</p><p id="cb7c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">About each customer, we know his country of residence, the number of family members that live with them, the average annual salary in the country, and how much money they spend on products in our store.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qy"><img src="../Images/06501e237f7f3ae792bd396a38e63d11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oip64NYvzdblQAo4aXu3SQ.png"/></div></div></figure><h2 id="ed30" class="pv oh fq bf oi pw px py ol pz qa qb oo ns qc qd qe nw qf qg qh oa qi qj qk ql bk">Power analysis</h2><p id="cbdf" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">First, we need to design the experiment and understand how many clients we need in each experiment group to make conclusions confidently. This step is called power analysis.</p><p id="3e96" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let's quickly recap the basic statistical theory about A/B tests and main metrics. Every test is based on the null hypothesis (which is the current status quo). In our case, the null hypothesis is "<em class="qz">discount does not affect customers' spending on our product</em>". Then, we need to collect data on customers' spending for control and experiment groups and estimate the probability of seeing such or more extreme results if the null hypothesis is valid. This probability is called the p-value, and if it's small enough, we can conclude that we have enough data to reject the null hypothesis and say that treatment affects customers' spending or retention.</p><p id="1dd9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In this approach, there are three main metrics:</p><ul class=""><li id="6dcf" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pi pj pk bk"><strong class="nl fr">effect size </strong>— the minimal change in our metric we would like to be able to detect,</li><li id="7c88" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk"><strong class="nl fr">statistical significance </strong>equals the false positive rate (probability of rejecting the null hypothesis when there was no effect). The most commonly used significance is 5%. However, you might choose other values depending on your false-positive tolerance. For example, if implementing the change is expensive, you might want to use a lower significance threshold.</li><li id="efe9" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk"><strong class="nl fr">statistical power </strong>shows the probability of rejecting the null hypothesis given that we actually had an effect equal to or higher than the effect size<strong class="nl fr">.</strong> People often use an 80% threshold, but in some cases (i.e. you want to be more confident that there are no negative effects), you might use 90% or even 99%.</li></ul><p id="17f9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We need all these values to estimate the number of clients in the experiment. Let's try to define them in our case to understand their meaning better.</p><p id="7013" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We will start with effect size:</p><ul class=""><li id="de92" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pi pj pk bk">we expect the retention rate to change by at least 3% points as a result of our campaign,</li><li id="ab46" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk">we would like to spot changes in customers' spending by 20 or more EUR.</li></ul><p id="a265" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">For statistical significance, I will use the default 5% threshold (so if we see the effect as a result of A/B test analysis, we can be confident with 95% that the effect is present). Let's target a 90% statistical power threshold so that if there's an actual effect equal to or bigger than the effect size, we will spot this change in 90% of cases.</p><p id="f229" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let's start with statistical formulas that will allow us to get estimations quickly. Statistical formulas imply that our variable has a particular distribution, but they can usually help you estimate the magnitude of the number of samples. Later, we will use bootstrap to get more accurate results.</p><p id="ef7b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">For retention, we can use the standard test of proportions. We need to know the actual value to estimate the normed effect size. We can get it from the historical data before the experiment.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="5ff6" class="qp oh fq pu b bg qq qr l qs qt">import statsmodels.stats.power as stat_power<br/>import statsmodels.stats.proportion as stat_prop<br/><br/>base_retention = before_df.retention.mean()<br/>ret_effect_size = stat_prop.proportion_effectsize(base_retention + 0.03, <br/>    base_retention)<br/><br/><br/>sample_size = 2*stat_power.tt_ind_solve_power(<br/>    effect_size = ret_effect_size,<br/>    alpha = 0.05, power = 0.9,<br/>    nobs1 = None, # we specified nobs1 as None to get an estimation for it<br/>    alternative='larger'<br/>)<br/><br/># ret_effect_size = 0.0632, sample_size = 8573.86</span></pre><p id="bd7f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We used a one-sided test because there's no difference in whether there's a negative or no effect from the business perspective since we won't implement this change. Using a one-sided instead of a two-sided test increases the statistical power.</p><p id="5c69" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can similarly estimate the sample size for the customer value, assuming the normal distribution. However, the distribution is not normal actually, so we should expect more precise results from bootstrap.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ra"><img src="../Images/62466b93bdc08c3c004a1b41c77eda9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dfFhH3OjNATGzjHGrfz0TQ.png"/></div></div></figure><p id="e095" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let's write code.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="1545" class="qp oh fq pu b bg qq qr l qs qt">val_effect_size = 20/before_df.customer_value.std()<br/><br/>sample_size = 2*stat_power.tt_ind_solve_power(<br/>    effect_size = val_effect_size,<br/>    alpha = 0.05, power = 0.9, <br/>    nobs1 = None, <br/>    alternative='larger'<br/>)<br/># val_effect_size = 0.0527, sample_size = 12324.13</span></pre><p id="643c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We got estimations for the needed sample sizes for each test. However, there are cases when you have a limited number of clients and want to understand the statistical power you can get.</p><p id="bc5a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Suppose we have only 5K customers (2.5K in each group). Then, we will be able to achieve 72.2% statistical power for retention analysis and 58.7% — for customer value (given the desired statistical significance and effect sizes).</p><p id="6906" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The only difference in the code is that this time, we've specified <code class="cx pr ps pt pu b">nobs1 = 2500</code> and left <code class="cx pr ps pt pu b">power</code> as <code class="cx pr ps pt pu b">None</code>.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="6352" class="qp oh fq pu b bg qq qr l qs qt">stat_power.tt_ind_solve_power(<br/>    effect_size = ret_effect_size,<br/>    alpha = 0.05, power = None,<br/>    nobs1 = 2500, <br/>    alternative='larger'<br/>)<br/># 0.7223<br/><br/>stat_power.tt_ind_solve_power(<br/>    effect_size = val_effect_size,<br/>    alpha = 0.05, power = None,<br/>    nobs1 = 2500, <br/>    alternative='larger'<br/>)<br/># 0.5867</span></pre><p id="1c8d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, it's time to use bootstrap for the power analysis, and we will start with the customer value test since it's easier to implement.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rb"><img src="../Images/ada854ce8bf8eda594fcef32e652f4da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CP_AyMiWU7IxmPICMXp9oQ.png"/></div></div></figure><p id="2c30" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let's discuss the basic idea and steps of power analysis using bootstrap. First, we need to define our goal clearly. We want to estimate the statistical power depending on the sample size. If we put it in more practical terms, we want to know the percentage of cases when there was an increase in customer spending by 20 or more EUR, and we were able to reject the null hypothesis and implement this change in production. So, we need to simulate a bunch of such experiments and calculate the share of cases when we can see statistically significant changes in our metric.</p><p id="01d9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let's look at one experiment and break it into steps. The first step is to generate the experimental data. For that, we need to get a random subset from the population equal to the sample size, randomly split these customers into control and experiment groups and add an effect equal to the effect size for the treatment group. All this logic is implemented in <code class="cx pr ps pt pu b">get_sample_for_value</code> function below.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="b509" class="qp oh fq pu b bg qq qr l qs qt">def get_sample_for_value(pop_df, sample_size, effect_size):<br/>  # getting sample of needed size<br/>  sample_df = pop_df.sample(sample_size)<br/><br/>  # randomly assign treatment<br/>  sample_df['treatment'] = sample_df.index.map(<br/>    lambda x: 1 if np.random.uniform() &gt; 0.5 else 0)<br/><br/>  # add efffect for the treatment group<br/>  sample_df['predicted_value'] = sample_df['customer_value'] \<br/>    + effect_size * sample_df.treatment<br/>  <br/>  return sample_df</span></pre><p id="a336" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, we can treat this synthetic experiment data as we usually do with A/B test analysis, run a bunch of bootstrap simulations, estimate effects, and then get a confidence interval for this effect.</p><p id="2210" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We will be using linear regression to estimate the effect of treatment. As discussed in <a class="af of" href="https://medium.com/towards-data-science/linear-regressions-for-causal-conclusions-34c6317c5a11" rel="noopener">the previous article</a>, it's worth adding to linear regression features that explain the outcome variable (customers' spending). We will add the number of family members and average salary to the regression since they are positively correlated.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="8ba0" class="qp oh fq pu b bg qq qr l qs qt">import statsmodels.formula.api as smf<br/>val_model = smf.ols('customer_value ~ num_family_members + country_avg_annual_earning', <br/>    data = before_df).fit(disp = 0)<br/>val_model.summary().tables[1]</span></pre><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rb"><img src="../Images/ada854ce8bf8eda594fcef32e652f4da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CP_AyMiWU7IxmPICMXp9oQ.png"/></div></div></figure><p id="0af0" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We will put all the logic of doing multiple bootstrap simulations and estimating treatment effects into the <code class="cx pr ps pt pu b">get_ci_for_value</code> function.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="75ee" class="qp oh fq pu b bg qq qr l qs qt">def get_ci_for_value(df, boot_iters, confidence_level):<br/>    tmp_data = []<br/>    <br/>    for iter in range(boot_iters):<br/>        sample_df = df.sample(df.shape[0], replace = True)<br/>        val_model = smf.ols('predicted_value ~ treatment + num_family_members + country_avg_annual_earning', <br/>          data = sample_df).fit(disp = 0)<br/>        tmp_data.append(<br/>            {<br/>                'iteration': iter,<br/>                'coef': val_model.params['treatment']<br/>            }<br/>        )<br/><br/>    coef_df = pd.DataFrame(tmp_data)<br/>    return coef_df.coef.quantile((1 - confidence_level)/2), <br/>        coef_df.coef.quantile(1 - (1 - confidence_level)/2)</span></pre><p id="187d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The next step is to put this logic together, run a bunch of such synthetic experiments, and save results.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="9a99" class="qp oh fq pu b bg qq qr l qs qt">def run_simulations_for_value(pop_df, sample_size, effect_size, <br/>    boot_iters, confidence_level, num_simulations):<br/><br/>    tmp_data = []<br/>    <br/>    for sim in tqdm.tqdm(range(num_simulations)):<br/>        sample_df = get_sample_for_value(pop_df, sample_size, effect_size)<br/>        num_users_treatment = sample_df[sample_df.treatment == 1].shape[0]<br/>        value_treatment = sample_df[sample_df.treatment == 1].predicted_value.mean()<br/>        num_users_control = sample_df[sample_df.treatment == 0].shape[0]<br/>        value_control = sample_df[sample_df.treatment == 0].predicted_value.mean()<br/><br/>        ci_lower, ci_upper = get_ci_for_value(sample_df, boot_iters, confidence_level)<br/><br/>        tmp_data.append(<br/>            {<br/>                'experiment_id': sim,<br/>                'num_users_treatment': num_users_treatment,<br/>                'value_treatment': value_treatment,<br/>                'num_users_control': num_users_control,<br/>                'value_control': value_control,<br/>                'sample_size': sample_size,<br/>                'effect_size': effect_size,<br/>                'boot_iters': boot_iters,<br/>                'confidence_level': confidence_level,<br/>                'ci_lower': ci_lower,<br/>                'ci_upper': ci_upper<br/>            }<br/>        )<br/><br/>    return pd.DataFrame(tmp_data)</span></pre><p id="adb3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let's run this simulation for <code class="cx pr ps pt pu b">sample_size = 100</code> and see the results.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="5085" class="qp oh fq pu b bg qq qr l qs qt">val_sim_df = run_simulations_for_value(before_df, sample_size = 100, <br/>    effect_size = 20, boot_iters = 1000, confidence_level = 0.95, <br/>    num_simulations = 20)<br/>val_sim_df.set_index('simulation')[['sample_size', 'ci_lower', 'ci_upper']].head()</span></pre><p id="6980" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We've got the following data for 20 simulated experiments. We know the confidence interval for each experiment, and now we can estimate the power.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qu"><img src="../Images/8d3def66d5ffc6d720061e145fb7631d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FTFEG3TTgRJZaLAnNp_fYg.png"/></div></div></figure><p id="57b7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We would have rejected the null hypothesis if the lower bound of the confidence interval was above zero, so let's calculate the share of such experiments.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="b7b7" class="qp oh fq pu b bg qq qr l qs qt">val_sim_df['successful_experiment'] = val_sim_df.ci_lower.map(<br/>  lambda x: 1 if x &gt; 0 else 0)<br/><br/>val_sim_df.groupby(['sample_size', 'effect_size']).aggregate(<br/>    {<br/>        'successful_experiment': 'mean',<br/>        'experiment_id': 'count'<br/>    }<br/>)<br/></span></pre><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rc"><img src="../Images/7f1b330a368956463c81ccda4aa3ada7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tDXiV_0FEULupk4A3a5HNA.png"/></div></div></figure><p id="475d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We've started with just 20 simulated experiments and 1000 bootstrap simulations to estimate their confidence interval. Such a few simulations can help us get a low-resolution picture quite quickly. Keeping in mind the estimation we got from the classic statistics, we should expect that numbers around 10K will give us the desired statistical power.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="c4ea" class="qp oh fq pu b bg qq qr l qs qt">tmp_dfs = []<br/>for sample_size in [100, 250, 500, 1000, 2500, 5000, 10000, 25000]:<br/>    print('Simulation for sample size = %d' % sample_size)<br/>    tmp_dfs.append(<br/>        run_simulations_for_value(before_df, sample_size = sample_size, effect_size = 20,<br/>                              boot_iters = 1000, confidence_level = 0.95, num_simulations = 20)<br/>    )<br/><br/>val_lowres_sim_df = pd.concat(tmp_dfs)</span></pre><p id="c543" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We got results similar to those of our theoretical estimations. Let's try to run estimations with more simulated experiments (100 and 500 experiments). We can see that 12.5K clients will be enough to achieve 90% statistical power.</p><p id="4d12" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">I've added all the power analysis results to the chart so that we can see the relation clearly.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rd"><img src="../Images/9cfd0affee2e30603d64edd2a7ab2f89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wWVADCcUqvn-NpjELV9_VA.png"/></div></div></figure><p id="c98a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In that case, you might already see that bootstrap can take a significant amount of time. For example, accurately estimating power with 500 experiment simulations for just 3 sample sizes took me almost 2 hours.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq re"><img src="../Images/18a54f89d1ddcc2b1fe53735207068ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BlwiFjaB0LzbIpDTaZ60EA.png"/></div></div></figure><p id="1dc2" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, we can estimate the relationship between effect size and power for a 12.5K sample size.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="8bad" class="qp oh fq pu b bg qq qr l qs qt">tmp_dfs = []<br/>for effect_size in [1, 5, 10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 90, 100]:<br/>    print('Simulation for effect size = %d' % effect_size)<br/>    tmp_dfs.append(<br/>        run_simulations_for_value(before_df, sample_size = 12500, effect_size = effect_size,<br/>                              boot_iters = 1000, confidence_level = 0.95, num_simulations = 100)<br/>    )<br/><br/>val_effect_size_sim_df = pd.concat(tmp_dfs)<br/></span></pre><p id="1f7a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can see that if the actual effect on customers’ spending is higher than 20 EUR, we will get even higher statistical power, and we will be able to reject the null hypothesis in more than 90% of cases. But we will be able to spot the 10 EUR effect in less than 50% of cases.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rf"><img src="../Images/bf5823e1bf3c00693153435d78223891.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pyc5jfg-M0e9Nzq7FxugJg.png"/></div></div></figure><p id="86e5" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let's move on and conduct power analysis for retention as well. The complete code is structured similarly to the customer spending analysis. We will discuss nuances in detail below.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="9c19" class="qp oh fq pu b bg qq qr l qs qt">import tqdm<br/><br/>def get_sample_for_retention(pop_df, sample_size, effect_size):<br/>    base_ret_model = smf.logit('retention ~ num_family_members', data = pop_df).fit(disp = 0)<br/>    tmp_pop_df = pop_df.copy()<br/>    tmp_pop_df['predicted_retention_proba'] = base_ret_model.predict()<br/>    sample_df = tmp_pop_df.sample(sample_size)<br/>    sample_df['treatment'] = sample_df.index.map(lambda x: 1 if np.random.uniform() &gt; 0.5 else 0)<br/>    sample_df['predicted_retention_proba'] = sample_df['predicted_retention_proba'] + effect_size * sample_df.treatment<br/>    sample_df['retention'] = sample_df.predicted_retention_proba.map(lambda x: 1 if x &gt;= np.random.uniform() else 0)<br/>    return sample_df<br/><br/>def get_ci_for_retention(df, boot_iters, confidence_level):<br/>    tmp_data = []<br/>    <br/>    for iter in range(boot_iters):<br/>        sample_df = df.sample(df.shape[0], replace = True)<br/>        ret_model = smf.logit('retention ~ treatment + num_family_members', data = sample_df).fit(disp = 0)<br/>        tmp_data.append(<br/>            {<br/>                'iteration': iter,<br/>                'coef': ret_model.params['treatment']<br/>            }<br/>        )<br/><br/>    coef_df = pd.DataFrame(tmp_data)<br/>    return coef_df.coef.quantile((1 - confidence_level)/2), coef_df.coef.quantile(1 - (1 - confidence_level)/2)<br/><br/>def run_simulations_for_retention(pop_df, sample_size, effect_size, <br/>                                  boot_iters, confidence_level, num_simulations):<br/>    tmp_data = []<br/>    <br/>    for sim in tqdm.tqdm(range(num_simulations)):<br/>        sample_df = get_sample_for_retention(pop_df, sample_size, effect_size)<br/>        num_users_treatment = sample_df[sample_df.treatment == 1].shape[0]<br/>        retention_treatment = sample_df[sample_df.treatment == 1].retention.mean()<br/>        num_users_control = sample_df[sample_df.treatment == 0].shape[0]<br/>        retention_control = sample_df[sample_df.treatment == 0].retention.mean()<br/><br/>        ci_lower, ci_upper = get_ci_for_retention(sample_df, boot_iters, confidence_level)<br/><br/>        tmp_data.append(<br/>            {<br/>                'experiment_id': sim,<br/>                'num_users_treatment': num_users_treatment,<br/>                'retention_treatment': retention_treatment,<br/>                'num_users_control': num_users_control,<br/>                'retention_control': retention_control,<br/>                'sample_size': sample_size,<br/>                'effect_size': effect_size,<br/>                'boot_iters': boot_iters,<br/>                'confidence_level': confidence_level,<br/>                'ci_lower': ci_lower,<br/>                'ci_upper': ci_upper<br/>            }<br/>        )<br/><br/>    return pd.DataFrame(tmp_data)</span></pre><p id="ac61" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">First, since we have a binary outcome for retention (whether the customer returns next month or not), we will use a logistic regression model instead of linear regression. We can see that retention is correlated with the size of the family. It might be the case that when you buy many different types of products for family members, it's more difficult to find another service that will cover all your needs.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="0530" class="qp oh fq pu b bg qq qr l qs qt">base_ret_model = smf.logit('retention ~ num_family_members', data = before_df).fit(disp = 0)<br/>base_ret_model.summary().tables[1]</span></pre><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qu"><img src="../Images/1ddeb7384347e3d71911c74ac2721967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S5C7SQI_7ryO6DKzESXCdA.png"/></div></div></figure><p id="66b5" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Also, the function<code class="cx pr ps pt pu b">get_sample_for_retention</code> has a bit trickier logic to adjust results for the treatment group. Let's look at it step by step.</p><p id="3ca6" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">First, we are fitting a logistic regression on the whole population data and using this model to predict the probability of retaining using this model.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="f71b" class="qp oh fq pu b bg qq qr l qs qt">base_ret_model = smf.logit('retention ~ num_family_members', data = pop_df)\<br/>  .fit(disp = 0)<br/>tmp_pop_df = pop_df.copy()<br/>tmp_pop_df['predicted_retention_proba'] = base_ret_model.predict()</span></pre><p id="309d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Then, we got a random sample equal to the size and split it into a control and test group.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="b7ef" class="qp oh fq pu b bg qq qr l qs qt">sample_df = tmp_pop_df.sample(sample_size)<br/>sample_df['treatment'] = sample_df.index.map(<br/>  lambda x: 1 if np.random.uniform() &gt; 0.5 else 0)</span></pre><p id="38e0" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">For the treatment group, we increase the probability of retaining by the expected effect size.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="8e54" class="qp oh fq pu b bg qq qr l qs qt">sample_df['predicted_retention_proba'] = sample_df['predicted_retention_proba'] \<br/>    + effect_size * sample_df.treatment</span></pre><p id="7457" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The last step is to define, based on probability, whether the customer is retained or not. We used uniform distribution (random number between 0 and 1) for that:</p><ul class=""><li id="fdcf" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pi pj pk bk">if a random value from a uniform distribution is below probability, then a customer is retained (it happens with specified probability),</li><li id="2021" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk">otherwise, the customer has churned.</li></ul><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="f307" class="qp oh fq pu b bg qq qr l qs qt">sample_df['retention'] = sample_df.predicted_retention_proba.map(<br/>    lambda x: 1 if x &gt; np.random.uniform() else 0)</span></pre><p id="4437" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">You can run a few simulations to ensure our sampling function works as intended. For example, with this call, we can see that for the control group, retention is equal to 64% like in the population, and it's 93.7% for the experiment group (as expected with <code class="cx pr ps pt pu b">effect_size = 0.3</code> )</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="6509" class="qp oh fq pu b bg qq qr l qs qt">get_sample_for_retention(before_df, 10000, 0.3)\<br/>  .groupby('treatment', as_index = False).retention.mean()<br/><br/># |    |   treatment |   retention |<br/># |---:|------------:|------------:|<br/># |  0 |           0 |    0.640057 |<br/># |  1 |           1 |    0.937648 |<br/></span></pre><p id="212d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, we can also run simulations to see the optimal number of samples to reach 90% of statistical power for retention. We can see that the 12.5K sample size also will be good enough for retention.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rg"><img src="../Images/b7331f964aa85e0d71b4d54ce62ba00f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XQ-IukwnVMMcgBzQJCQ6AA.png"/></div></div></figure><h2 id="836b" class="pv oh fq bf oi pw px py ol pz qa qb oo ns qc qd qe nw qf qg qh oa qi qj qk ql bk">Analysing results</h2><p id="c3f7" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">We can use linear or logistic regression to analyse results or leverage the functions we already have for bootstrap CI.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="07ab" class="qp oh fq pu b bg qq qr l qs qt">value_model = smf.ols(<br/>  'customer_value ~ treatment + num_family_members + country_avg_annual_earning', <br/>  data = experiment_df).fit(disp = 0)<br/>value_model.summary().tables[1]</span></pre><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rh"><img src="../Images/853fb14a89e81f475b7005a0bcb024dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*foOu2VMr6h6QajQiHR64gQ.png"/></div></div></figure><p id="dc05" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">So, we got the statistically significant result for the customer spending equal to 25.84 EUR with a 95% confidence interval equal to <code class="cx pr ps pt pu b">(16.82, 34.87)</code> .</p><p id="1e08" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With the bootstrap function, the CI will be pretty close.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="9497" class="qp oh fq pu b bg qq qr l qs qt">get_ci_for_value(experiment_df.rename(<br/>    columns = {'customer_value': 'predicted_value'}), 1000, 0.95)<br/># (16.28, 34.63)</span></pre><p id="f28d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Similarly, we can use logistic regression for retention analysis.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="39f4" class="qp oh fq pu b bg qq qr l qs qt">retention_model = smf.logit('retention ~ treatment + num_family_members',<br/>    data = experiment_df).fit(disp = 0)<br/>retention_model.summary().tables[1]</span></pre><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ra"><img src="../Images/50b7ae6e1f8999db0b45baacf8f01883.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ci3fSBJlv4IGpKgs-sTFg.png"/></div></div></figure><p id="1a40" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Again, the bootstrap approach gives close estimations for CI.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="b90e" class="qp oh fq pu b bg qq qr l qs qt">get_ci_for_retention(experiment_df, 1000, 0.95)<br/># (0.072, 0.187)</span></pre><p id="cc4b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With logistic regression, it might be tricky to interpret the coefficient. However, we can use a hacky approach: for each customer in our dataset, calculate probability in case the customer was in control and treatment using our model and then look at the average difference between probabilities.</p><pre class="ms mt mu mv mw qm pu qn bp qo bb bk"><span id="38ab" class="qp oh fq pu b bg qq qr l qs qt">experiment_df['treatment_eq_1'] = 1<br/>experiment_df['treatment_eq_0'] = 0<br/><br/>experiment_df['retention_proba_treatment'] = retention_model.predict(<br/>    experiment_df[['retention', 'treatment_eq_1', 'num_family_members']]\<br/>        .rename(columns = {'treatment_eq_1': 'treatment'}))<br/><br/>experiment_df['retention_proba_control'] = retention_model.predict(<br/>    experiment_df[['retention', 'treatment_eq_0', 'num_family_members']]\<br/>      .rename(columns = {'treatment_eq_0': 'treatment'}))<br/><br/>experiment_df['proba_diff'] = experiment_df.retention_proba_treatment \<br/>    - experiment_df.retention_proba_control<br/><br/>experiment_df.proba_diff.mean()<br/># 0.0281</span></pre><p id="96fe" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">So, we can estimate the effect on retention to be 2.8%.</p><p id="d0de" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Congratulations! We’ve finally finished the full A/B test analysis and were able to estimate the effect both on average customer spending and retention. Our experiment is successful, so in real life, we would start thinking about rolling it to production.</p><blockquote class="ri rj rk"><p id="1a5e" class="nj nk qz nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">You can find the full code for this example on <a class="af of" href="https://github.com/miptgirl/miptgirl_medium/tree/main/simulations" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p></blockquote><h1 id="c280" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Summary</h1><p id="7ba4" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Let me quickly recap what we’ve discussed today:</p><ul class=""><li id="f923" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pi pj pk bk">The main idea of bootstrap is simulations with replacements from your sample, assuming that the general population has the same distribution as the data we have.</li><li id="c0d7" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk">Bootstrap shines in cases when you have few data points, your data has outliers or is far from any theoretical distribution. Bootstrap can also help you estimate custom metrics.</li><li id="0080" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk">You can use bootstrap to work with observational data, for example, to get confidence intervals for your values.</li><li id="346e" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe pi pj pk bk">Also, bootstrap is broadly used for A/B testing analysis — both to estimate the impact of treatment and do a power analysis to design an experiment.</li></ul><blockquote class="ri rj rk"><p id="8131" class="nj nk qz nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Thank you a lot for reading this article. If you have any follow-up questions or comments, please leave them in the comments section.</p></blockquote><h1 id="5574" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Reference</h1><p id="7214" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk"><em class="qz">All the images are produced by the author unless otherwise stated.</em></p><p id="842b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This article was inspired by the book <a class="af of" href="https://www.oreilly.com/library/view/behavioral-data-analysis/9781492061366/" rel="noopener ugc nofollow" target="_blank">“Behavioral Data Analysis with R and Python”</a> by Florent Buisson.</p></div></div></div></div>    
</body>
</html>