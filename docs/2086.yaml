- en: Boosting LLM Inference Speed Using Speculative Decoding
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用推测性解码提升大语言模型推理速度
- en: 原文：[https://towardsdatascience.com/boosting-llm-inference-speed-using-speculative-decoding-0cb0bf36d001?source=collection_archive---------8-----------------------#2024-08-27](https://towardsdatascience.com/boosting-llm-inference-speed-using-speculative-decoding-0cb0bf36d001?source=collection_archive---------8-----------------------#2024-08-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/boosting-llm-inference-speed-using-speculative-decoding-0cb0bf36d001?source=collection_archive---------8-----------------------#2024-08-27](https://towardsdatascience.com/boosting-llm-inference-speed-using-speculative-decoding-0cb0bf36d001?source=collection_archive---------8-----------------------#2024-08-27)
- en: A practical guide on using cutting-edge optimization techniques to speed up
    inference
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用前沿优化技术加速推理的实用指南
- en: '[](https://medium.com/@het.trivedi05?source=post_page---byline--0cb0bf36d001--------------------------------)[![Het
    Trivedi](../Images/f6f11a66f60cacc6b553c7d1682b2fc6.png)](https://medium.com/@het.trivedi05?source=post_page---byline--0cb0bf36d001--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0cb0bf36d001--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0cb0bf36d001--------------------------------)
    [Het Trivedi](https://medium.com/@het.trivedi05?source=post_page---byline--0cb0bf36d001--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@het.trivedi05?source=post_page---byline--0cb0bf36d001--------------------------------)[![Het
    Trivedi](../Images/f6f11a66f60cacc6b553c7d1682b2fc6.png)](https://medium.com/@het.trivedi05?source=post_page---byline--0cb0bf36d001--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0cb0bf36d001--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0cb0bf36d001--------------------------------)
    [Het Trivedi](https://medium.com/@het.trivedi05?source=post_page---byline--0cb0bf36d001--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0cb0bf36d001--------------------------------)
    ·6 min read·Aug 27, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0cb0bf36d001--------------------------------)
    ·6 分钟阅读·2024年8月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/96708875ea858ad9b494372501b5c7bb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96708875ea858ad9b494372501b5c7bb.png)'
- en: Image generated using Flux Schnell
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 Flux Schnell 生成
- en: Intro
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Large language models are extremely power-hungry and require a significant amount
    of GPU resources to perform well. However, the transformer architecture does not
    take full advantage of the GPU.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型非常耗电，需要大量的 GPU 资源才能有效运行。然而，变换器架构并未充分利用 GPU 的优势。
- en: GPUs, by design, can process things in parallel, but the transformer architecture
    is auto-regressive. In order for the next token to get generated it has to look
    at all of the previous tokens that came before it. Transformers don’t allow you
    to predict the next `n` tokens in parallel. Ultimately, this makes the generation
    phase of LLMs quite slow as each new token **must** be produced sequentially.
    Speculative decoding is a novel optimization technique that aims to solve this
    issue.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 从设计上来说能够并行处理任务，但变换器架构是自回归的。为了生成下一个标记，必须查看所有之前的标记。变换器不允许你并行预测下一个 `n` 个标记。最终，这使得大型语言模型（LLM）的生成过程非常缓慢，因为每个新标记**必须**按顺序生成。推测性解码是一种新颖的优化技术，旨在解决这一问题。
- en: '![](../Images/069f5d5ee327f9a495bcc31e9233eb6b.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/069f5d5ee327f9a495bcc31e9233eb6b.png)'
- en: Each forward pass produces a new token generated by the LLM
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每次前向传递会生成一个新的标记，由大型语言模型生成
- en: There are a few different methods for speculative decoding. The technique described
    in this article uses the two model approach.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 推测性解码有几种不同的方法。本文所描述的技术使用的是两模型方法。
- en: Speculative Decoding
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推测性解码
- en: Speculative decoding works by having two models, a large **main** model and
    a…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 推测性解码的工作原理是使用两个模型，一个是大型的**主**模型，另一个是...
