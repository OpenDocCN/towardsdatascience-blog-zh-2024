- en: Boosting LLM Inference Speed Using Speculative Decoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/boosting-llm-inference-speed-using-speculative-decoding-0cb0bf36d001?source=collection_archive---------8-----------------------#2024-08-27](https://towardsdatascience.com/boosting-llm-inference-speed-using-speculative-decoding-0cb0bf36d001?source=collection_archive---------8-----------------------#2024-08-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical guide on using cutting-edge optimization techniques to speed up
    inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@het.trivedi05?source=post_page---byline--0cb0bf36d001--------------------------------)[![Het
    Trivedi](../Images/f6f11a66f60cacc6b553c7d1682b2fc6.png)](https://medium.com/@het.trivedi05?source=post_page---byline--0cb0bf36d001--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0cb0bf36d001--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0cb0bf36d001--------------------------------)
    [Het Trivedi](https://medium.com/@het.trivedi05?source=post_page---byline--0cb0bf36d001--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0cb0bf36d001--------------------------------)
    ·6 min read·Aug 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96708875ea858ad9b494372501b5c7bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated using Flux Schnell
  prefs: []
  type: TYPE_NORMAL
- en: Intro
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models are extremely power-hungry and require a significant amount
    of GPU resources to perform well. However, the transformer architecture does not
    take full advantage of the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs, by design, can process things in parallel, but the transformer architecture
    is auto-regressive. In order for the next token to get generated it has to look
    at all of the previous tokens that came before it. Transformers don’t allow you
    to predict the next `n` tokens in parallel. Ultimately, this makes the generation
    phase of LLMs quite slow as each new token **must** be produced sequentially.
    Speculative decoding is a novel optimization technique that aims to solve this
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/069f5d5ee327f9a495bcc31e9233eb6b.png)'
  prefs: []
  type: TYPE_IMG
- en: Each forward pass produces a new token generated by the LLM
  prefs: []
  type: TYPE_NORMAL
- en: There are a few different methods for speculative decoding. The technique described
    in this article uses the two model approach.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Speculative Decoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Speculative decoding works by having two models, a large **main** model and
    a…
  prefs: []
  type: TYPE_NORMAL
