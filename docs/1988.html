<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>From Basics to Advanced: Exploring LangGraph</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>From Basics to Advanced: Exploring LangGraph</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-basics-to-advanced-exploring-langgraph-e8c1cf4db787?source=collection_archive---------0-----------------------#2024-08-15">https://towardsdatascience.com/from-basics-to-advanced-exploring-langgraph-e8c1cf4db787?source=collection_archive---------0-----------------------#2024-08-15</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f1d8" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Building single- and multi-agent workflows with human-in-the-loop interactions</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://miptgirl.medium.com/?source=post_page---byline--e8c1cf4db787--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mariya Mansurova" class="l ep by dd de cx" src="../Images/b1dd377b0a1887db900cc5108bca8ea8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*7fFHr8XBAuR_SgJknIyODA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e8c1cf4db787--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://miptgirl.medium.com/?source=post_page---byline--e8c1cf4db787--------------------------------" rel="noopener follow">Mariya Mansurova</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e8c1cf4db787--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">21 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 15, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/9be549ca0d6991e179d95d55d5d5388d.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*dv8GOCe4X9FOHEs0vKx83A.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by DALL-E 3</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c20b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><a class="af of" href="https://www.langchain.com/" rel="noopener ugc nofollow" target="_blank">LangChain</a> is one of the leading frameworks for building applications powered by Lardge Language Models. With the <a class="af of" href="https://python.langchain.com/v0.1/docs/expression_language/" rel="noopener ugc nofollow" target="_blank">LangChain Expression Language</a> (LCEL), defining and executing step-by-step action sequences — also known as chains — becomes much simpler. In more technical terms, LangChain allows us to create DAGs (directed acyclic graphs).</p><p id="ed56" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As LLM applications, particularly LLM agents, have evolved, we’ve begun to use LLMs not just for execution but also as reasoning engines. This shift has introduced interactions that frequently involve repetition (cycles) and complex conditions. In such scenarios, LCEL is not sufficient, so LangChain implemented a new module — <a class="af of" href="https://langchain-ai.github.io/langgraph/" rel="noopener ugc nofollow" target="_blank">LangGraph</a>.</p><p id="d06b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">LangGraph (as you might guess from the name) models all interactions as cyclical graphs. These graphs enable the development of advanced workflows and interactions with multiple loops and if-statements, making it a handy tool for creating both agent and multi-agent workflows.</p><p id="6ede" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In this article, I will explore LangGraph’s key features and capabilities, including multi-agent applications. We’ll build a system that can answer different types of questions and dive into how to implement a human-in-the-loop setup.</p><p id="98aa" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In <a class="af of" href="https://medium.com/towards-data-science/multi-ai-agent-systems-101-bac58e3bcc47" rel="noopener">the previous article</a>, we tried using CrewAI, another popular framework for multi-agent systems. LangGraph, however, takes a different approach. While CrewAI is a high-level framework with many predefined features and ready-to-use components, LangGraph operates at a lower level, offering extensive customization and control.</p><p id="fec0" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With that introduction, let’s dive into the fundamental concepts of LangGraph.</p><h1 id="8d42" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">LangGraph basics</h1><p id="802b" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">LangGraph is part of the LangChain ecosystem, so we will continue using well-known concepts like prompt templates, tools, etc. However, LangGraph brings a bunch of <a class="af of" href="https://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph" rel="noopener ugc nofollow" target="_blank">additional concepts.</a> Let’s discuss them.</p><p id="0b0c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">LangGraph is created to define cyclical graphs. Graphs consist of the following elements:</p><ul class=""><li id="9a12" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk">Nodes represent actual actions and can be either LLMs, agents or functions. Also, a special END node marks the end of execution.</li><li id="25ff" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">Edges connect nodes and determine the execution flow of your graph. There are basic edges that simply link one node to another and conditional edges that incorporate if-statements and additional logic.</li></ul><p id="934e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Another important concept is the state of the graph. The state serves as a foundational element for collaboration among the graph’s components. It represents a snapshot of the graph that any part — whether nodes or edges — can access and modify during execution to retrieve or update information.</p><p id="00b7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Additionally, the state plays a crucial role in persistence. It is automatically saved after each step, allowing you to pause and resume execution at any point. This feature supports the development of more complex applications, such as those requiring error correction or incorporating human-in-the-loop interactions.</p><h1 id="e247" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Single-agent workflow</h1><h2 id="c25f" class="pp oh fq bf oi pq pr ps ol pt pu pv oo ns pw px py nw pz qa qb oa qc qd qe qf bk">Building agent from scratch</h2><p id="e38b" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Let’s start simple and try using LangGraph for a basic use case — an agent with tools.</p><p id="b062" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">I will try to build similar applications to those we did with CrewAI in <a class="af of" href="https://medium.com/towards-data-science/multi-ai-agent-systems-101-bac58e3bcc47" rel="noopener">the previous article</a>. Then, we will be able to compare the two frameworks. For this example, let’s create an application that can automatically generate documentation based on the table in the database. It can save us quite a lot of time when creating documentation for our data sources.</p><p id="837d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As usual, we will start by defining the tools for our agent. Since I will use the ClickHouse database in this example, I’ve defined a function to execute any query. You can use a different database if you prefer, as we won’t rely on any database-specific features.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="d456" class="qk oh fq qh b bg ql qm l qn qo">CH_HOST = 'http://localhost:8123' # default address <br/>import requests<br/><br/>def get_clickhouse_data(query, host = CH_HOST, connection_timeout = 1500):<br/>  r = requests.post(host, params = {'query': query}, <br/>    timeout = connection_timeout)<br/>  if r.status_code == 200:<br/>      return r.text<br/>  else: <br/>      return 'Database returned the following error:\n' + r.text</span></pre><p id="64f5" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It’s crucial to make LLM tools reliable and error-prone. If a database returns an error, I provide this feedback to the LLM rather than throwing an exception and halting execution. Then, the LLM agent will have an opportunity to fix an error and call the function again.</p><p id="7c51" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s define one tool named <code class="cx qp qq qr qh b">execute_sql</code> , which enables the execution of any SQL query. We use <code class="cx qp qq qr qh b">pydantic</code> to specify the tool’s structure, ensuring that the LLM agent has all the needed information to use the tool effectively.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="3fd2" class="qk oh fq qh b bg ql qm l qn qo">from langchain_core.tools import tool<br/>from pydantic.v1 import BaseModel, Field<br/>from typing import Optional<br/><br/>class SQLQuery(BaseModel):<br/>  query: str = Field(description="SQL query to execute")<br/><br/>@tool(args_schema = SQLQuery)<br/>def execute_sql(query: str) -&gt; str:<br/>  """Returns the result of SQL query execution"""<br/>  return get_clickhouse_data(query)</span></pre><p id="d01a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can print the parameters of the created tool to see what information is passed to LLM.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="a3f6" class="qk oh fq qh b bg ql qm l qn qo">print(f'''<br/>name: {execute_sql.name}<br/>description: {execute_sql.description}<br/>arguments: {execute_sql.args}<br/>''')<br/><br/># name: execute_sql<br/># description: Returns the result of SQL query execution<br/># arguments: {'query': {'title': 'Query', 'description': <br/>#   'SQL query to execute', 'type': 'string'}}</span></pre><p id="a723" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Everything looks good. We’ve set up the necessary tool and can now move on to defining an LLM agent. As we discussed above, the cornerstone of the agent in LangGraph is its state, which enables the sharing of information between different parts of our graph.</p><p id="2e3f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Our current example is relatively straightforward. So, we will only need to store the history of messages. Let’s define the agent state.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="81ea" class="qk oh fq qh b bg ql qm l qn qo"># useful imports<br/>from langgraph.graph import StateGraph, END<br/>from typing import TypedDict, Annotated<br/>import operator<br/>from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage<br/><br/># defining agent state<br/>class AgentState(TypedDict):<br/>   messages: Annotated[list[AnyMessage], operator.add]</span></pre><p id="99ad" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We’ve defined a single parameter in <code class="cx qp qq qr qh b">AgentState</code> — <code class="cx qp qq qr qh b">messages</code> — which is a list of objects of the class <code class="cx qp qq qr qh b">AnyMessage</code> . Additionally, we annotated it with <code class="cx qp qq qr qh b">operator.add</code> (reducer). This annotation ensures that each time a node returns a message, it is appended to the existing list in the state. Without this operator, each new message would replace the previous value rather than being added to the list.</p><p id="25c6" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The next step is to define the agent itself. Let’s start with <code class="cx qp qq qr qh b">__init__</code> function. We will specify three arguments for the agent: model, list of tools and system prompt.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="058a" class="qk oh fq qh b bg ql qm l qn qo">class SQLAgent:<br/>  # initialising the object<br/>  def __init__(self, model, tools, system_prompt = ""):<br/>    self.system_prompt = system_prompt<br/>    <br/>    # initialising graph with a state <br/>    graph = StateGraph(AgentState)<br/><br/>    # adding nodes <br/>    graph.add_node("llm", self.call_llm)<br/>    graph.add_node("function", self.execute_function)<br/>    graph.add_conditional_edges(<br/>        "llm",<br/>        self.exists_function_calling,<br/>        {True: "function", False: END}<br/>    )<br/>    graph.add_edge("function", "llm")<br/><br/>    # setting starting point<br/>    graph.set_entry_point("llm")<br/><br/>    self.graph = graph.compile()<br/>    self.tools = {t.name: t for t in tools}<br/>    self.model = model.bind_tools(tools)</span></pre><p id="69c2" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In the initialisation function, we’ve outlined the structure of our graph, which includes two nodes: <code class="cx qp qq qr qh b">llm</code> and <code class="cx qp qq qr qh b">action</code>. Nodes are actual actions, so we have functions associated with them. We will define functions a bit later.</p><p id="5882" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Additionally, we have one conditional edge that determines whether we need to execute the function or generate the final answer. For this edge, we need to specify the previous node (in our case, <code class="cx qp qq qr qh b">llm</code>), a function that decides the next step, and mapping of the subsequent steps based on the function’s output (formatted as a dictionary). If <code class="cx qp qq qr qh b">exists_function_calling</code> returns True, we follow to the function node. Otherwise, execution will conclude at the special <code class="cx qp qq qr qh b">END</code> node, which marks the end of the process.</p><p id="48d1" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We’ve added an edge between <code class="cx qp qq qr qh b">function</code> and <code class="cx qp qq qr qh b">llm</code>. It just links these two steps and will be executed without any conditions.</p><p id="56d0" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With the main structure defined, it’s time to create all the functions outlined above. The first one is <code class="cx qp qq qr qh b">call_llm</code>. This function will execute LLM and return the result.</p><p id="bdaf" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The agent state will be passed to the function automatically so we can use the saved system prompt and model from it.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="0f26" class="qk oh fq qh b bg ql qm l qn qo">class SQLAgent:<br/>  &lt;...&gt;<br/><br/>  def call_llm(self, state: AgentState):<br/>    messages = state['messages']<br/>    # adding system prompt if it's defined<br/>    if self.system_prompt:<br/>        messages = [SystemMessage(content=self.system_prompt)] + messages<br/>    <br/>    # calling LLM<br/>    message = self.model.invoke(messages)<br/><br/>    return {'messages': [message]}</span></pre><p id="0614" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As a result, our function returns a dictionary that will be used to update the agent state. Since we used <code class="cx qp qq qr qh b">operator.add</code> as a reducer for our state, the returned message will be appended to the list of messages stored in the state.</p><p id="4e13" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The next function we need is <code class="cx qp qq qr qh b">execute_function</code> which will run our tools. If the LLM agent decides to call a tool, we will see it in the<code class="cx qp qq qr qh b">message.tool_calls</code> parameter.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="a79d" class="qk oh fq qh b bg ql qm l qn qo">class SQLAgent:<br/>  &lt;...&gt;  <br/><br/>  def execute_function(self, state: AgentState):<br/>    tool_calls = state['messages'][-1].tool_calls<br/><br/>    results = []<br/>    for tool in tool_calls:<br/>      # checking whether tool name is correct<br/>      if not t['name'] in self.tools:<br/>      # returning error to the agent <br/>      result = "Error: There's no such tool, please, try again" <br/>      else:<br/>      # getting result from the tool<br/>      result = self.tools[t['name']].invoke(t['args'])<br/>      <br/>      results.append(<br/>        ToolMessage(<br/>          tool_call_id=t['id'], <br/>          name=t['name'], <br/>          content=str(result)<br/>        )<br/>    )<br/>    return {'messages': results}</span></pre><p id="8366" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In this function, we iterate over the tool calls returned by LLM and either invoke these tools or return the error message. In the end, our function returns the dictionary with a single key <code class="cx qp qq qr qh b">messages</code> that will be used to update the graph state.</p><p id="d4f4" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">There’s only one function left —the function for the conditional edge that defines whether we need to execute the tool or provide the final result. It’s pretty straightforward. We just need to check whether the last message contains any tool calls.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="1954" class="qk oh fq qh b bg ql qm l qn qo">class SQLAgent:<br/>  &lt;...&gt;  <br/><br/>  def exists_function_calling(self, state: AgentState):<br/>    result = state['messages'][-1]<br/>    return len(result.tool_calls) &gt; 0</span></pre><p id="099b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It’s time to create an agent and LLM model for it. I will use the new OpenAI GPT 4o mini model (<a class="af of" href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/" rel="noopener ugc nofollow" target="_blank">doc</a>) since it’s cheaper and better performing than GPT 3.5.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="b579" class="qk oh fq qh b bg ql qm l qn qo">import os<br/><br/># setting up credentioals<br/>os.environ["OPENAI_MODEL_NAME"]='gpt-4o-mini'  <br/>os.environ["OPENAI_API_KEY"] = '&lt;your_api_key&gt;'<br/><br/># system prompt<br/>prompt = '''You are a senior expert in SQL and data analysis. <br/>So, you can help the team to gather needed data to power their decisions. <br/>You are very accurate and take into account all the nuances in data.<br/>Your goal is to provide the detailed documentation for the table in database <br/>that will help users.'''<br/><br/>model = ChatOpenAI(model="gpt-4o-mini")<br/>doc_agent = SQLAgent(model, [execute_sql], system=prompt)</span></pre><p id="815d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">LangGraph provides us with quite a handy feature to visualise graphs. To use it, you need to install <code class="cx qp qq qr qh b">pygraphviz</code> .</p><p id="4560" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It’s a bit tricky for Mac with M1/M2 chips, so here is the lifehack for you (<a class="af of" href="https://github.com/pygraphviz/pygraphviz/issues/398" rel="noopener ugc nofollow" target="_blank">source</a>):</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="834b" class="qk oh fq qh b bg ql qm l qn qo">! brew install graphviz<br/>! python3 -m pip install -U --no-cache-dir  \<br/>    --config-settings="--global-option=build_ext" \<br/>    --config-settings="--global-option=-I$(brew --prefix graphviz)/include/" \<br/>    --config-settings="--global-option=-L$(brew --prefix graphviz)/lib/" \<br/>    pygraphviz</span></pre><p id="5dfc" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">After figuring out the installation, here’s our graph.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="d6c2" class="qk oh fq qh b bg ql qm l qn qo">from IPython.display import Image<br/>Image(doc_agent.graph.get_graph().draw_png())</span></pre><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qs"><img src="../Images/accdadbbf3700cb7f49695388ad48fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eJ3paG6HiT7dGBwuilrchA.png"/></div></div></figure><p id="1ee5" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As you can see, our graph has cycles. Implementing something like this with LCEL would be quite challenging.</p><p id="4ec5" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Finally, it’s time to execute our agent. We need to pass the initial set of messages with our questions as <code class="cx qp qq qr qh b">HumanMessage</code>.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="e021" class="qk oh fq qh b bg ql qm l qn qo">messages = [HumanMessage(content="What info do we have in ecommerce_db.users table?")]<br/>result = doc_agent.graph.invoke({"messages": messages})</span></pre><p id="f051" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In the <code class="cx qp qq qr qh b">result</code> variable, we can observe all the messages generated during execution. The process worked as expected:</p><ul class=""><li id="79a5" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk">The agent decided to call the function with the query <code class="cx qp qq qr qh b">describe ecommerce.db_users</code>.</li><li id="af3c" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">LLM then processed the information from the tool and provided a user-friendly answer.</li></ul><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="9056" class="qk oh fq qh b bg ql qm l qn qo">result['messages']<br/><br/># [<br/>#   HumanMessage(content='What info do we have in ecommerce_db.users table?'), <br/>#   AIMessage(content='', tool_calls=[{'name': 'execute_sql', 'args': {'query': 'DESCRIBE ecommerce_db.users;'}, 'id': 'call_qZbDU9Coa2tMjUARcX36h0ax', 'type': 'tool_call'}]), <br/>#   ToolMessage(content='user_id\tUInt64\t\t\t\t\t\ncountry\tString\t\t\t\t\t\nis_active\tUInt8\t\t\t\t\t\nage\tUInt64\t\t\t\t\t\n', name='execute_sql', tool_call_id='call_qZbDU9Coa2tMjUARcX36h0ax'), <br/>#   AIMessage(content='The `ecommerce_db.users` table contains the following columns: &lt;...&gt;')<br/># ]</span></pre><p id="1cb0" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Here’s the final result. It looks pretty decent.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="2ad5" class="qk oh fq qh b bg ql qm l qn qo">print(result['messages'][-1].content)<br/><br/># The `ecommerce_db.users` table contains the following columns:<br/># 1. **user_id**: `UInt64` - A unique identifier for each user.<br/># 2. **country**: `String` - The country where the user is located.<br/># 3. **is_active**: `UInt8` - Indicates whether the user is active (1) or inactive (0).<br/># 4. **age**: `UInt64` - The age of the user.</span></pre><h2 id="ae6d" class="pp oh fq bf oi pq pr ps ol pt pu pv oo ns pw px py nw pz qa qb oa qc qd qe qf bk">Using prebuilt agents</h2><p id="97df" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">We’ve learned how to build an agent from scratch. However, we can leverage LangGraph's built-in functionality for simpler tasks like this one.</p><p id="3250" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can use a <a class="af of" href="https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/" rel="noopener ugc nofollow" target="_blank">prebuilt ReAct agent</a> to get a similar result: an agent that can work with tools.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="e9d7" class="qk oh fq qh b bg ql qm l qn qo">from langgraph.prebuilt import create_react_agent<br/>prebuilt_doc_agent = create_react_agent(model, [execute_sql],<br/>  state_modifier = system_prompt)</span></pre><p id="d030" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It is the same agent as we built previously. We will try it out in a second, but first, we need to understand two other important concepts: persistence and streaming.</p><h2 id="e01c" class="pp oh fq bf oi pq pr ps ol pt pu pv oo ns pw px py nw pz qa qb oa qc qd qe qf bk">Persistence and streaming</h2><p id="63b8" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Persistence refers to the ability to maintain context across different interactions. It’s essential for agentic use cases when an application can get additional input from the user.</p><p id="87fc" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">LangGraph automatically saves the state after each step, allowing you to pause or resume execution. This capability supports the implementation of advanced business logic, such as error recovery or human-in-the-loop interactions.</p><p id="6c5f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The easiest way to add persistence is to use an in-memory SQLite database.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="e941" class="qk oh fq qh b bg ql qm l qn qo">from langgraph.checkpoint.sqlite import SqliteSaver<br/>memory = SqliteSaver.from_conn_string(":memory:")</span></pre><p id="437d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">For the off-the-shelf agent, we can pass memory as an argument while creating an agent.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="a610" class="qk oh fq qh b bg ql qm l qn qo">prebuilt_doc_agent = create_react_agent(model, [execute_sql], <br/>  checkpointer=memory)</span></pre><p id="797a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">If you’re working with a custom agent, you need to pass memory as a check pointer while compiling a graph.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="5d5a" class="qk oh fq qh b bg ql qm l qn qo">class SQLAgent:<br/>  def __init__(self, model, tools, system_prompt = ""):<br/>    &lt;...&gt;<br/>    self.graph = graph.compile(checkpointer=memory)<br/>    &lt;...&gt;</span></pre><p id="5928" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s execute the agent and explore another feature of LangGraph: streaming. With streaming, we can receive results from each step of execution as a separate event in a stream. This feature is crucial for production applications when multiple conversations (or threads) need to be processed simultaneously.</p><p id="9fda" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">LangGraph supports not only event streaming but also token-level streaming. The only use case I have in mind for token streaming is to display the answers in real-time word by word (similar to ChatGPT implementation).</p><p id="ea25" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s try using streaming with our new prebuilt agent. I will also use the <code class="cx qp qq qr qh b">pretty_print</code> function for messages to make the result more readable.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="8f1b" class="qk oh fq qh b bg ql qm l qn qo"><br/># defining thread<br/>thread = {"configurable": {"thread_id": "1"}}<br/>messages = [HumanMessage(content="What info do we have in ecommerce_db.users table?")]<br/><br/>for event in prebuilt_doc_agent.stream({"messages": messages}, thread):<br/>    for v in event.values():<br/>        v['messages'][-1].pretty_print()<br/><br/># ================================== Ai Message ==================================<br/># Tool Calls:<br/>#  execute_sql (call_YieWiChbFuOlxBg8G1jDJitR)<br/>#  Call ID: call_YieWiChbFuOlxBg8G1jDJitR<br/>#   Args:<br/>#     query: SELECT * FROM ecommerce_db.users LIMIT 1;<br/># ================================= Tool Message =================================<br/># Name: execute_sql<br/># 1000001 United Kingdom 0 70<br/># <br/># ================================== Ai Message ==================================<br/># <br/># The `ecommerce_db.users` table contains at least the following information for users:<br/># <br/># - **User ID** (e.g., `1000001`)<br/># - **Country** (e.g., `United Kingdom`)<br/># - **Some numerical value** (e.g., `0`)<br/># - **Another numerical value** (e.g., `70`)<br/># <br/># The specific meaning of the numerical values and additional columns <br/># is not clear from the single row retrieved. Would you like more details <br/># or a broader query?</span></pre><p id="85b8" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Interestingly, the agent wasn’t able to provide a good enough result. Since the agent didn’t look up the table schema, it struggled to guess all columns’ meanings. We can improve the result by using follow-up questions in the same thread.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="402d" class="qk oh fq qh b bg ql qm l qn qo"><br/>followup_messages = [HumanMessage(content="I would like to know the column names and types. Maybe you could look it up in database using describe.")]<br/><br/>for event in prebuilt_doc_agent.stream({"messages": followup_messages}, thread):<br/>    for v in event.values():<br/>        v['messages'][-1].pretty_print()<br/><br/># ================================== Ai Message ==================================<br/># Tool Calls:<br/>#   execute_sql (call_sQKRWtG6aEB38rtOpZszxTVs)<br/>#  Call ID: call_sQKRWtG6aEB38rtOpZszxTVs<br/>#   Args:<br/>#     query: DESCRIBE ecommerce_db.users;<br/># ================================= Tool Message =================================<br/># Name: execute_sql<br/># <br/># user_id UInt64     <br/># country String     <br/># is_active UInt8     <br/># age UInt64     <br/># <br/># ================================== Ai Message ==================================<br/># <br/># The `ecommerce_db.users` table has the following columns along with their data types:<br/># <br/># | Column Name | Data Type |<br/># |-------------|-----------|<br/># | user_id     | UInt64    |<br/># | country     | String    |<br/># | is_active   | UInt8     |<br/># | age         | UInt64    |<br/># <br/># If you need further information or assistance, feel free to ask!</span></pre><p id="e438" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This time, we got the full answer from the agent. Since we provided the same thread, the agent was able to get the context from the previous discussion. That’s how persistence works.</p><p id="048f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s try to change the thread and ask the same follow-up question.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="b310" class="qk oh fq qh b bg ql qm l qn qo">new_thread = {"configurable": {"thread_id": "42"}}<br/>followup_messages = [HumanMessage(content="I would like to know the column names and types. Maybe you could look it up in database using describe.")]<br/><br/>for event in prebuilt_doc_agent.stream({"messages": followup_messages}, new_thread):<br/>    for v in event.values():<br/>        v['messages'][-1].pretty_print()<br/><br/># ================================== Ai Message ==================================<br/># Tool Calls:<br/>#   execute_sql (call_LrmsOGzzusaLEZLP9hGTBGgo)<br/>#  Call ID: call_LrmsOGzzusaLEZLP9hGTBGgo<br/>#   Args:<br/>#     query: DESCRIBE your_table_name;<br/># ================================= Tool Message =================================<br/># Name: execute_sql<br/># <br/># Database returned the following error:<br/># Code: 60. DB::Exception: Table default.your_table_name does not exist. (UNKNOWN_TABLE) (version 23.12.1.414 (official build))<br/># <br/># ================================== Ai Message ==================================<br/># <br/># It seems that the table `your_table_name` does not exist in the database. <br/># Could you please provide the actual name of the table you want to describe?</span></pre><p id="9322" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It was not surprising that the agent lacked the context needed to answer our question. Threads are designed to isolate different conversations, ensuring that each thread maintains its own context.</p><p id="9112" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In real-life applications, managing memory is essential. Conversations might become pretty lengthy, and at some point, it won’t be practical to pass the whole history to LLM every time. Therefore, it’s worth trimming or filtering messages. We won’t go deep into the specifics here, but you can find guidance on it in <a class="af of" href="https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/" rel="noopener ugc nofollow" target="_blank">the LangGraph documentation</a>. Another option to compress the conversational history is using summarization (<a class="af of" href="https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/#how-to-add-summary-of-the-conversation-history" rel="noopener ugc nofollow" target="_blank">example</a>).</p><p id="e24d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We’ve learned how to build systems with single agents using LangGraph. The next step is to combine multiple agents in one application.</p><h1 id="b374" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Multi-Agent Systems</h1><p id="9c40" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">As an example of a multi-agent workflow, I would like to build an application that can handle questions from various domains. We will have a set of expert agents, each specializing in different types of questions, and a router agent that will find the best-suited expert to address each query. Such an application has numerous potential use cases: from automating customer support to answering questions from colleagues in internal chats.</p><p id="1035" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">First, we need to create the agent state — the information that will help agents to solve the question together. I will use the following fields:</p><ul class=""><li id="7251" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk"><code class="cx qp qq qr qh b">question</code> — initial customer request;</li><li id="9aa4" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk"><code class="cx qp qq qr qh b">question_type</code> — the category that defines which agent will be working on the request;</li><li id="2280" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk"><code class="cx qp qq qr qh b">answer</code> — the proposed answer to the question;</li><li id="bd6f" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk"><code class="cx qp qq qr qh b">feedback</code> — a field for future use that will gather some feedback.</li></ul><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="42bb" class="qk oh fq qh b bg ql qm l qn qo">class MultiAgentState(TypedDict):<br/>    question: str<br/>    question_type: str<br/>    answer: str<br/>    feedback: str</span></pre><p id="7323" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">I don’t use any reducers, so our state will store only the latest version of each field.</p><p id="d1ce" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Then, let’s create a router node. It will be a simple LLM model that defines the category of question (database, LangChain or general questions).</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="5cd4" class="qk oh fq qh b bg ql qm l qn qo">question_category_prompt = '''You are a senior specialist of analytical support. Your task is to classify the incoming questions. <br/>Depending on your answer, question will be routed to the right team, so your task is crucial for our team. <br/>There are 3 possible question types: <br/>- DATABASE - questions related to our database (tables or fields)<br/>- LANGCHAIN- questions related to LangGraph or LangChain libraries<br/>- GENERAL - general questions<br/>Return in the output only one word (DATABASE, LANGCHAIN or  GENERAL).<br/>'''<br/><br/>def router_node(state: MultiAgentState):<br/>  messages = [<br/>    SystemMessage(content=question_category_prompt), <br/>    HumanMessage(content=state['question'])<br/>  ]<br/>  model = ChatOpenAI(model="gpt-4o-mini")<br/>  response = model.invoke(messages)<br/>  return {"question_type": response.content}</span></pre><p id="fe75" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now that we have our first node — the router — let’s build a simple graph to test the workflow.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="1f22" class="qk oh fq qh b bg ql qm l qn qo">memory = SqliteSaver.from_conn_string(":memory:")<br/><br/>builder = StateGraph(MultiAgentState)<br/>builder.add_node("router", router_node)<br/><br/>builder.set_entry_point("router")<br/>builder.add_edge('router', END)<br/><br/>graph = builder.compile(checkpointer=memory)</span></pre><p id="0376" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s test our workflow with different types of questions to see how it performs in action. This will help us evaluate whether the router agent correctly assigns questions to the appropriate expert agents.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="968f" class="qk oh fq qh b bg ql qm l qn qo">thread = {"configurable": {"thread_id": "1"}}<br/>for s in graph.stream({<br/>    'question': "Does LangChain support Ollama?",<br/>}, thread):<br/>    print(s)<br/><br/># {'router': {'question_type': 'LANGCHAIN'}}<br/><br/>thread = {"configurable": {"thread_id": "2"}}<br/>for s in graph.stream({<br/>    'question': "What info do we have in ecommerce_db.users table?",<br/>}, thread):<br/>    print(s)<br/># {'router': {'question_type': 'DATABASE'}}<br/><br/>thread = {"configurable": {"thread_id": "3"}}<br/>for s in graph.stream({<br/>    'question': "How are you?",<br/>}, thread):<br/>    print(s)<br/><br/># {'router': {'question_type': 'GENERAL'}}</span></pre><p id="e9d2" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It’s working well. I recommend you build complex graphs incrementally and test each step independently. With such an approach, you can ensure that each iteration works expectedly and can save you a significant amount of debugging time.</p><p id="d6f2" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Next, let’s create nodes for our expert agents. We will use the ReAct agent with the SQL tool we previously built as the database agent.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="0ab4" class="qk oh fq qh b bg ql qm l qn qo"># database expert<br/>sql_expert_system_prompt = '''<br/>You are an expert in SQL, so you can help the team <br/>to gather needed data to power their decisions. <br/>You are very accurate and take into account all the nuances in data. <br/>You use SQL to get the data before answering the question.<br/>'''<br/><br/>def sql_expert_node(state: MultiAgentState):<br/>    model = ChatOpenAI(model="gpt-4o-mini")<br/>    sql_agent = create_react_agent(model, [execute_sql],<br/>        state_modifier = sql_expert_system_prompt)<br/>    messages = [HumanMessage(content=state['question'])]<br/>    result = sql_agent.invoke({"messages": messages})<br/>    return {'answer': result['messages'][-1].content}<br/><br/></span></pre><p id="72a7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">For LangChain-related questions, we will use the ReAct agent. To enable the agent to answer questions about the library, we will equip it with a search engine tool. I chose <a class="af of" href="https://tavily.com/" rel="noopener ugc nofollow" target="_blank">Tavily</a> for this purpose as it provides the search results optimised for LLM applications.</p><p id="a80d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">If you don’t have an account, you can register to use Tavily for free (up to 1K requests per month). To get started, you will need to specify the Tavily API key in an environment variable.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="c041" class="qk oh fq qh b bg ql qm l qn qo"># search expert <br/>from langchain_community.tools.tavily_search import TavilySearchResults<br/>os.environ["TAVILY_API_KEY"] = 'tvly-...'<br/>tavily_tool = TavilySearchResults(max_results=5)<br/><br/>search_expert_system_prompt = '''<br/>You are an expert in LangChain and other technologies. <br/>Your goal is to answer questions based on results provided by search.<br/>You don't add anything yourself and provide only information baked by other sources. <br/>'''<br/><br/>def search_expert_node(state: MultiAgentState):<br/>    model = ChatOpenAI(model="gpt-4o-mini")<br/>    sql_agent = create_react_agent(model, [tavily_tool],<br/>        state_modifier = search_expert_system_prompt)<br/>    messages = [HumanMessage(content=state['question'])]<br/>    result = sql_agent.invoke({"messages": messages})<br/>    return {'answer': result['messages'][-1].content}</span></pre><p id="8dde" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">For general questions, we will leverage a simple LLM model without specific tools.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="c423" class="qk oh fq qh b bg ql qm l qn qo"># general model<br/>general_prompt = '''You're a friendly assistant and your goal is to answer general questions.<br/>Please, don't provide any unchecked information and just tell that you don't know if you don't have enough info.<br/>'''<br/><br/>def general_assistant_node(state: MultiAgentState):<br/>    messages = [<br/>        SystemMessage(content=general_prompt), <br/>        HumanMessage(content=state['question'])<br/>    ]<br/>    model = ChatOpenAI(model="gpt-4o-mini")<br/>    response = model.invoke(messages)<br/>    return {"answer": response.content}</span></pre><p id="ccff" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The last missing bit is a conditional function for routing. This will be quite straightforward—we just need to propagate the question type from the state defined by the router node.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="2e81" class="qk oh fq qh b bg ql qm l qn qo">def route_question(state: MultiAgentState):<br/>    return state['question_type']</span></pre><p id="3124" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, it’s time to create our graph.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="bf0d" class="qk oh fq qh b bg ql qm l qn qo">builder = StateGraph(MultiAgentState)<br/>builder.add_node("router", router_node)<br/>builder.add_node('database_expert', sql_expert_node)<br/>builder.add_node('langchain_expert', search_expert_node)<br/>builder.add_node('general_assistant', general_assistant_node)<br/>builder.add_conditional_edges(<br/>    "router", <br/>    route_question,<br/>    {'DATABASE': 'database_expert', <br/>     'LANGCHAIN': 'langchain_expert', <br/>     'GENERAL': 'general_assistant'}<br/>)<br/><br/><br/>builder.set_entry_point("router")<br/>builder.add_edge('database_expert', END)<br/>builder.add_edge('langchain_expert', END)<br/>builder.add_edge('general_assistant', END)<br/>graph = builder.compile(checkpointer=memory)</span></pre><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qt"><img src="../Images/211fb819f5f309883906e7f790e155b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NUDsZaHJc7gWVZrDQX7L4g.png"/></div></div></figure><p id="aa9c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, we can test the setup on a couple of questions to see how well it performs.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="41e3" class="qk oh fq qh b bg ql qm l qn qo">thread = {"configurable": {"thread_id": "2"}}<br/>results = []<br/>for s in graph.stream({<br/>  'question': "What info do we have in ecommerce_db.users table?",<br/>}, thread):<br/>  print(s)<br/>  results.append(s)<br/>print(results[-1]['database_expert']['answer'])<br/><br/># The `ecommerce_db.users` table contains the following columns:<br/># 1. **User ID**: A unique identifier for each user.<br/># 2. **Country**: The country where the user is located.<br/># 3. **Is Active**: A flag indicating whether the user is active (1 for active, 0 for inactive).<br/># 4. **Age**: The age of the user.<br/># Here are some sample entries from the table:<br/># <br/># | User ID | Country        | Is Active | Age |<br/># |---------|----------------|-----------|-----|<br/># | 1000001 | United Kingdom  | 0         | 70  |<br/># | 1000002 | France         | 1         | 87  |<br/># | 1000003 | France         | 1         | 88  |<br/># | 1000004 | Germany        | 1         | 25  |<br/># | 1000005 | Germany        | 1         | 48  |<br/># <br/># This gives an overview of the user data available in the table.</span></pre><p id="c1d1" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Good job! It gives a relevant result for the database-related question. Let’s try asking about LangChain.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="9b97" class="qk oh fq qh b bg ql qm l qn qo"><br/>thread = {"configurable": {"thread_id": "42"}}<br/>results = []<br/>for s in graph.stream({<br/>    'question': "Does LangChain support Ollama?",<br/>}, thread):<br/>    print(s)<br/>    results.append(s)<br/><br/>print(results[-1]['langchain_expert']['answer'])<br/><br/># Yes, LangChain supports Ollama. Ollama allows you to run open-source <br/># large language models, such as Llama 2, locally, and LangChain provides <br/># a flexible framework for integrating these models into applications. <br/># You can interact with models run by Ollama using LangChain, and there are <br/># specific wrappers and tools available for this integration.<br/># <br/># For more detailed information, you can visit the following resources:<br/># - [LangChain and Ollama Integration](https://js.langchain.com/v0.1/docs/integrations/llms/ollama/)<br/># - [ChatOllama Documentation](https://js.langchain.com/v0.2/docs/integrations/chat/ollama/)<br/># - [Medium Article on Ollama and LangChain](https://medium.com/@abonia/ollama-and-langchain-run-llms-locally-900931914a46)</span></pre><p id="157e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Fantastic! Everything is working well, and it’s clear that Tavily's search is effective for LLM applications.</p><h1 id="2349" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Adding human-in-the-loop interactions</h1><p id="dbe5" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">We’ve done an excellent job creating a tool to answer questions. However, in many cases, it’s beneficial to keep a human in the loop to approve proposed actions or provide additional feedback. Let’s add a step where we can collect feedback from a human before returning the final result to the user.</p><p id="9fb5" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The simplest approach is to add two additional nodes:</p><ul class=""><li id="1ad8" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk">A <code class="cx qp qq qr qh b">human</code> node to gather feedback,</li><li id="23af" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">An <code class="cx qp qq qr qh b">editor</code> node to revisit the answer, taking into account the feedback.</li></ul><p id="45c1" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s create these nodes:</p><ul class=""><li id="21e3" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk"><strong class="nl fr">Human node:</strong> This will be a dummy node, and it won’t perform any actions.</li><li id="f598" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk"><strong class="nl fr">Editor node: </strong>This will be an LLM model that receives all the relevant information (customer question, draft answer and provided feedback) and revises the final answer.</li></ul><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="015f" class="qk oh fq qh b bg ql qm l qn qo">def human_feedback_node(state: MultiAgentState):<br/>    pass<br/><br/>editor_prompt = '''You're an editor and your goal is to provide the final answer to the customer, taking into account the feedback. <br/>You don't add any information on your own. You use friendly and professional tone.<br/>In the output please provide the final answer to the customer without additional comments.<br/>Here's all the information you need.<br/><br/>Question from customer: <br/>----<br/>{question}<br/>----<br/>Draft answer:<br/>----<br/>{answer}<br/>----<br/>Feedback: <br/>----<br/>{feedback}<br/>----<br/>'''<br/><br/>def editor_node(state: MultiAgentState):<br/>  messages = [<br/>    SystemMessage(content=editor_prompt.format(question = state['question'], answer = state['answer'], feedback = state['feedback']))<br/>  ]<br/>  model = ChatOpenAI(model="gpt-4o-mini")<br/>  response = model.invoke(messages)<br/>  return {"answer": response.content}<br/></span></pre><p id="acd7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s add these nodes to our graph. Additionally, we need to introduce an interruption before the human node to ensure that the process pauses for human feedback.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="9945" class="qk oh fq qh b bg ql qm l qn qo">builder = StateGraph(MultiAgentState)<br/>builder.add_node("router", router_node)<br/>builder.add_node('database_expert', sql_expert_node)<br/>builder.add_node('langchain_expert', search_expert_node)<br/>builder.add_node('general_assistant', general_assistant_node)<br/>builder.add_node('human', human_feedback_node)<br/>builder.add_node('editor', editor_node)<br/><br/>builder.add_conditional_edges(<br/>  "router", <br/>  route_question,<br/>  {'DATABASE': 'database_expert', <br/>  'LANGCHAIN': 'langchain_expert', <br/>  'GENERAL': 'general_assistant'}<br/>)<br/><br/><br/>builder.set_entry_point("router")<br/><br/>builder.add_edge('database_expert', 'human')<br/>builder.add_edge('langchain_expert', 'human')<br/>builder.add_edge('general_assistant', 'human')<br/>builder.add_edge('human', 'editor')<br/>builder.add_edge('editor', END)<br/>graph = builder.compile(checkpointer=memory, interrupt_before = ['human'])</span></pre><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qu"><img src="../Images/4087a0dfd9d81b876f3270713c20e8ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oxvi8GMekRuo2liQcJb_gQ.png"/></div></div></figure><p id="bbe0" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, when we run the graph, the execution will be stopped before the human node.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="7f67" class="qk oh fq qh b bg ql qm l qn qo">thread = {"configurable": {"thread_id": "2"}}<br/><br/>for event in graph.stream({<br/>    'question': "What are the types of fields in ecommerce_db.users table?",<br/>}, thread):<br/>    print(event)<br/><br/><br/># {'question_type': 'DATABASE', 'question': 'What are the types of fields in ecommerce_db.users table?'}<br/># {'router': {'question_type': 'DATABASE'}}<br/># {'database_expert': {'answer': 'The `ecommerce_db.users` table has the following fields:\n\n1. **user_id**: UInt64\n2. **country**: String\n3. **is_active**: UInt8\n4. **age**: UInt64'}}</span></pre><p id="bcc7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s get the customer input and update the state with the feedback.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="f1a1" class="qk oh fq qh b bg ql qm l qn qo">user_input = input("Do I need to change anything in the answer?")<br/># Do I need to change anything in the answer? <br/># It looks wonderful. Could you only make it a bit friendlier please?<br/><br/>graph.update_state(thread, {"feedback": user_input}, as_node="human")</span></pre><p id="b30d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can check the state to confirm that the feedback has been populated and that the next node in the sequence is <code class="cx qp qq qr qh b">editor</code>.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="d471" class="qk oh fq qh b bg ql qm l qn qo">print(graph.get_state(thread).values['feedback'])<br/># It looks wonderful. Could you only make it a bit friendlier please?<br/><br/>print(graph.get_state(thread).next)<br/># ('editor',)</span></pre><p id="5d7e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can just continue the execution. Passing <code class="cx qp qq qr qh b">None</code> as input will resume the process from the point where it was paused.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="6154" class="qk oh fq qh b bg ql qm l qn qo">for event in graph.stream(None, thread, stream_mode="values"):<br/>  print(event)<br/><br/>print(event['answer'])<br/><br/># Hello! The `ecommerce_db.users` table has the following fields:<br/># 1. **user_id**: UInt64<br/># 2. **country**: String<br/># 3. **is_active**: UInt8<br/># 4. **age**: UInt64<br/># Have a nice day!</span></pre><p id="7815" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The editor took our feedback into account and added some polite words to our final message. That’s a fantastic result!</p><p id="f790" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can implement human-in-the-loop interactions in a more agentic way by equipping our editor with the <a class="af of" href="https://python.langchain.com/v0.2/docs/integrations/tools/human_tools/" rel="noopener ugc nofollow" target="_blank">Human</a> tool.</p><p id="7b1c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s adjust our editor. I’ve slightly changed the prompt and added the tool to the agent.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="d68e" class="qk oh fq qh b bg ql qm l qn qo">from langchain_community.tools import HumanInputRun<br/>human_tool = HumanInputRun()<br/><br/>editor_agent_prompt = '''You're an editor and your goal is to provide the final answer to the customer, taking into the initial question.<br/>If you need any clarifications or need feedback, please, use human. Always reach out to human to get the feedback before final answer.<br/>You don't add any information on your own. You use friendly and professional tone. <br/>In the output please provide the final answer to the customer without additional comments.<br/>Here's all the information you need.<br/><br/>Question from customer: <br/>----<br/>{question}<br/>----<br/>Draft answer:<br/>----<br/>{answer}<br/>----<br/>'''<br/><br/>model = ChatOpenAI(model="gpt-4o-mini")<br/>editor_agent = create_react_agent(model, [human_tool])<br/>messages = [SystemMessage(content=editor_agent_prompt.format(question = state['question'], answer = state['answer']))]<br/>editor_result = editor_agent.invoke({"messages": messages})<br/><br/># Is the draft answer complete and accurate for the customer's question about the types of fields in the ecommerce_db.users table?<br/># Yes, but could you please make it friendlier.<br/><br/>print(editor_result['messages'][-1].content)<br/># The `ecommerce_db.users` table has the following fields:<br/># 1. **user_id**: UInt64<br/># 2. **country**: String<br/># 3. **is_active**: UInt8<br/># 4. **age**: UInt64<br/># <br/># If you have any more questions, feel free to ask!</span></pre><p id="b904" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">So, the editor reached out to the human with the question, “Is the draft answer complete and accurate for the customer’s question about the types of fields in the ecommerce_db.users table?”. After receiving feedback, the editor refined the answer to make it more user-friendly.</p><p id="28f8" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s update our main graph to incorporate the new agent instead of using the two separate nodes. With this approach, we don’t need interruptions any more.</p><pre class="ms mt mu mv mw qg qh qi bp qj bb bk"><span id="1727" class="qk oh fq qh b bg ql qm l qn qo">def editor_agent_node(state: MultiAgentState):<br/>  model = ChatOpenAI(model="gpt-4o-mini")<br/>  editor_agent = create_react_agent(model, [human_tool])<br/>  messages = [SystemMessage(content=editor_agent_prompt.format(question = state['question'], answer = state['answer']))]<br/>  result = editor_agent.invoke({"messages": messages})<br/>  return {'answer': result['messages'][-1].content}<br/><br/>builder = StateGraph(MultiAgentState)<br/>builder.add_node("router", router_node)<br/>builder.add_node('database_expert', sql_expert_node)<br/>builder.add_node('langchain_expert', search_expert_node)<br/>builder.add_node('general_assistant', general_assistant_node)<br/>builder.add_node('editor', editor_agent_node)<br/><br/>builder.add_conditional_edges(<br/>  "router", <br/>  route_question,<br/>  {'DATABASE': 'database_expert', <br/>   'LANGCHAIN': 'langchain_expert', <br/>    'GENERAL': 'general_assistant'}<br/>)<br/><br/>builder.set_entry_point("router")<br/><br/>builder.add_edge('database_expert', 'editor')<br/>builder.add_edge('langchain_expert', 'editor')<br/>builder.add_edge('general_assistant', 'editor')<br/>builder.add_edge('editor', END)<br/><br/>graph = builder.compile(checkpointer=memory)<br/><br/>thread = {"configurable": {"thread_id": "42"}}<br/>results = []<br/><br/>for event in graph.stream({<br/>  'question': "What are the types of fields in ecommerce_db.users table?",<br/>}, thread):<br/>  print(event)<br/>  results.append(event)</span></pre><p id="7914" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This graph will work similarly to the previous one. I personally prefer this approach since it leverages tools, making the solution more agile. For example, agents can reach out to humans multiple times and refine questions as needed.</p><p id="1adc" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">That’s it. We’ve built a multi-agent system that can answer questions from different domains and take into account human feedback.</p><blockquote class="qv qw qx"><p id="6d3a" class="nj nk qy nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">You can find the complete code on <a class="af of" href="https://github.com/miptgirl/miptgirl_medium/blob/main/langgraph_answering_questions/langgraph.ipynb" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p></blockquote><h1 id="ef7e" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Summary</h1><p id="ad38" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">In this article, we’ve explored the LangGraph library and its application for building single and multi-agent workflows. We’ve examined a range of its capabilities, and now it's time to summarise its strengths and weaknesses. Also, it will be useful to compare LangGraph with CrewAI, which we discussed in <a class="af of" href="https://medium.com/towards-data-science/multi-ai-agent-systems-101-bac58e3bcc47" rel="noopener">my previous article</a>.</p><p id="2fc8" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Overall, I find LangGraph quite a powerful framework for building complex LLM applications:</p><ul class=""><li id="8a31" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk">LangGraph is a low-level framework that offers extensive customisation options, allowing you to build precisely what you need.</li><li id="9cc0" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">Since LangGraph is built on top of LangChain, it’s seamlessly integrated into its ecosystem, making it easy to leverage existing tools and components.</li></ul><p id="70fd" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">However, there are areas where LangGrpah could be improved:</p><ul class=""><li id="8445" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk">The agility of LangGraph comes with a higher entry barrier. While you can understand the concepts of CrewAI within 15–30 minutes, it takes some time to get comfortable and up to speed with LangGraph.</li><li id="5d67" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">LangGraph provides you with a higher level of control, but it misses some cool prebuilt features of CrewAI, such as <a class="af of" href="https://docs.crewai.com/core-concepts/Collaboration/" rel="noopener ugc nofollow" target="_blank">collaboration</a> or ready-to-use <a class="af of" href="https://docs.crewai.com/core-concepts/Tools/#available-crewai-tools" rel="noopener ugc nofollow" target="_blank">RAG</a> tools.</li><li id="b5b7" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">LangGraph doesn’t enforce best practices like CrewAI does (for example, role-playing or guardrails). So it can lead to poorer results.</li></ul><p id="3533" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">I would say that CrewAI is a better framework for newbies and common use cases because it helps you get good results quickly and provides guidance to prevent mistakes.</p><p id="3228" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">If you want to build an advanced application and need more control, LangGraph is the way to go. Keep in mind that you’ll need to invest time in learning LangGraph and should be fully responsible for the final solution, as the framework won’t provide guidance to help you avoid common mistakes.</p><blockquote class="qv qw qx"><p id="3600" class="nj nk qy nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Thank you a lot for reading this article. I hope this article was insightful for you. If you have any follow-up questions or comments, please leave them in the comments section.</p></blockquote><h1 id="c95b" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Reference</h1><p id="7431" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">This article is inspired by the <a class="af of" href="https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/" rel="noopener ugc nofollow" target="_blank">“AI Agents in LangGraph”</a> short course from DeepLearning.AI.</p></div></div></div></div>    
</body>
</html>