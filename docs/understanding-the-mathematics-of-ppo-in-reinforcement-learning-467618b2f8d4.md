# 理解强化学习中PPO的数学原理

> 原文：[https://towardsdatascience.com/understanding-the-mathematics-of-ppo-in-reinforcement-learning-467618b2f8d4?source=collection_archive---------2-----------------------#2024-12-21](https://towardsdatascience.com/understanding-the-mathematics-of-ppo-in-reinforcement-learning-467618b2f8d4?source=collection_archive---------2-----------------------#2024-12-21)

## 深入探索适用于初学者的强化学习中的PPO

[](https://manellenouar.medium.com/?source=post_page---byline--467618b2f8d4--------------------------------)[![Manelle Nouar](../Images/292d0f9007b102476fd0f2831bcdafb1.png)](https://manellenouar.medium.com/?source=post_page---byline--467618b2f8d4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--467618b2f8d4--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--467618b2f8d4--------------------------------) [Manelle Nouar](https://manellenouar.medium.com/?source=post_page---byline--467618b2f8d4--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--467618b2f8d4--------------------------------) ·7分钟阅读·2024年12月21日

--

![](../Images/5025e297ec787bcf8be293d57e481e27.png)

图片由[ThisisEngineering](https://unsplash.com/@thisisengineering?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

# 介绍

强化学习（RL）是人工智能的一个分支，使得智能体能够学习如何与环境进行交互。这些智能体，包括机器人、软件功能或自主系统，通过试错学习。他们根据所采取的行动获得奖励或惩罚，这些反馈指导他们未来的决策。

在最著名的强化学习算法中，近端策略优化（PPO）因其稳定性和高效性而被广泛青睐。PPO解决了强化学习中的若干挑战，特别是在控制策略（智能体的决策制定策略）演变方面。与其他算法不同，PPO确保策略更新不会过大，从而防止训练过程中出现不稳定现象。这种稳定性至关重要，因为过大的更新可能导致智能体偏离最优解，使得学习过程变得不稳定。因此，PPO在探索（尝试新行动）与利用（专注于获得最大奖励的行动）之间保持平衡。

此外，PPO在计算资源和学习速度方面也非常高效。通过有效优化智能体的策略，同时避免过于复杂的计算，PPO已成为机器人、游戏和自主系统等各个领域的实用解决方案。其简洁性使得它易于实现，这也是它在学术研究和工业界广泛应用的原因之一。

本文探讨了强化学习（RL）的数学基础以及PPO所引入的关键概念，帮助深入理解为何PPO成为现代强化学习研究中的常用算法。

# 1\. 强化学习基础：马尔可夫决策过程（MDP）

强化学习问题通常使用马尔可夫决策过程（MDP）进行建模，MDP是一个数学框架，有助于在结果不确定的环境中形式化决策过程。

马尔可夫链模型描述了一个系统，该系统在状态之间转移，其中转移到新状态的概率仅依赖于当前状态，而与之前的状态无关。这一原理被称为*马尔可夫性质*。在MDP的背景下，这一简化对于建模决策至关重要，因为它使得智能体在做决策时仅需要关注当前状态，而不必考虑系统的整个历史。

MDP由以下元素定义：

- S: 可能的状态集合。

- A: 可能的动作集合。

- P(s’|s, a): 在状态s下采取动作a后到达状态s’的转移概率。

- R(s, a): 在状态s下采取动作a后获得的奖励。

- γ: 折扣因子（介于0和1之间的值），反映了未来奖励的重要性。

折扣因子γ在决策问题中对建模未来奖励的重要性至关重要。当智能体做出决策时，它不仅要评估即时奖励，还必须考虑潜在的未来奖励。折扣γ减少了由于不确定性导致的未来奖励的影响，因此，接近1的γ值意味着未来奖励几乎与即时奖励同等重要，而接近0的值则更侧重于即时奖励。

时间折扣反映了智能体对快速收益相对于未来收益的偏好，这通常源于不确定性或环境可能发生变化的原因。例如，除非未来的奖励足够显著，智能体通常会更倾向于选择即时奖励，而不是远期奖励。因此，这一折扣因子建模了优化行为，其中智能体在做决策时会同时考虑短期和长期的收益。

目标是找到一个动作策略π(a|s)，它最大化随时间累积的期望奖励总和，通常称为价值函数：

该函数表示智能体从状态s开始并遵循策略π时，能够积累的期望总奖励。

# 2\. 策略优化：策略梯度

策略梯度方法通过最大化一个目标函数，直接优化策略πθ的参数θ，目标函数表示在给定环境中遵循该策略所获得的期望奖励。

目标函数定义为：

其中R(s, a)表示在状态s下采取动作a所获得的奖励，目标是最大化随时间变化的期望奖励。术语dπ(s)表示在策略π下的状态的平稳分布，指示智能体在遵循策略π时访问每个状态的频率。

策略梯度定理给出了目标函数的梯度，提供了一种更新策略参数的方法：

该方程展示了如何根据过去的经验调整策略参数，从而帮助智能体随着时间推移学习更高效的行为。

# 3. PPO的数学增强

PPO（Proximal Policy Optimization）引入了几个重要特性，旨在提高强化学习的稳定性和效率，特别是在大型复杂环境中。PPO由John Schulman等人于2017年提出，作为对早期策略优化算法（如信任域策略优化（TRPO））的改进。PPO的主要动机是寻求在样本效率、实现简便性和稳定性之间取得平衡，同时避免TRPO的二阶优化方法的复杂性。虽然TRPO通过对策略变化施加严格约束来确保策略更新的稳定性，但它依赖于计算开销较大的二阶导数和共轭梯度方法，使其实现和扩展具有挑战性。此外，TRPO中的严格约束有时会过度限制策略更新，导致收敛速度变慢。PPO通过使用简单的截断目标函数来解决这些问题，允许策略以稳定且可控的方式更新，避免每次更新时遗忘之前的策略，从而提高训练效率并减少策略崩溃的风险。这使得PPO成为广泛强化学习任务的热门选择。

## a. 概率比率

PPO的一个关键组成部分是概率比率，它比较当前策略πθ下采取某个动作的概率与旧策略πθold下采取同一动作的概率：

这个比率提供了一个度量，表示策略在更新之间的变化程度。通过监控这个比率，PPO确保更新不会过于剧烈，从而帮助防止学习过程中的不稳定性。

## b. 截断函数

在Proximal Policy Optimization（PPO）中，截断比调整学习率更受偏爱，因为它直接限制了策略更新的幅度，防止了可能导致学习过程不稳定的过度变化。尽管学习率均匀地缩放更新的大小，截断确保更新保持接近先前的策略，从而增强稳定性并减少不稳定行为。

裁剪的主要优点在于它可以更好地控制更新，确保更稳定的进展。然而，潜在的缺点是，它可能会通过限制探索显著不同的策略而减缓学习过程。不过，当稳定性至关重要时，裁剪在 PPO 和其他算法中是被青睐的。

为了避免对策略进行过多的修改，PPO 使用了一个裁剪函数，该函数通过修改目标函数来限制策略更新的幅度。这一点至关重要，因为强化学习中的大幅度更新可能导致不稳定的行为。裁剪后的目标函数是：

裁剪函数将概率比值限制在一个特定范围内，防止那些会与之前的策略偏离过远的更新。这有助于避免可能导致学习过程不稳定的突然而大的变化。

## c. 使用 GAE 的优势估计

在强化学习中，估计优势是非常重要的，因为它有助于智能体判断在每个状态下哪些动作比其他动作更好。然而，这之间存在一个权衡：仅使用即时奖励（或非常短的视野）可能会引入较高的方差，而使用较长的视野则可能引入偏差。

广义优势估计（GAE）通过使用 n 步回报和价值估计的加权平均值，在这两者之间找到平衡，使得它对噪声不那么敏感，进而提高了学习的稳定性。

为什么使用 GAE？

- **稳定性**：GAE 通过考虑多个步骤来减少方差，使智能体不会对奖励中的噪声或环境中的暂时波动做出反应。

- **效率**：GAE 在偏差和方差之间取得了良好的平衡，通过不需要过长的奖励序列，同时仍然保持可靠的估计，从而提高了学习效率。

- **更好的动作比较**：通过不仅仅考虑即时奖励，还考虑更广泛的奖励视角，智能体可以更好地比较随时间变化的动作，并做出更加明智的决策。

优势函数 At 用于评估一个动作相对于当前策略下期望行为的优劣。为了减少方差并确保更可靠的估计，PPO 使用了广义优势估计（GAE）。这种方法在控制偏差的同时，平滑了优势的变化：

这种技术提供了一个更加稳定和准确的优势衡量方式，从而提高了智能体做出更好决策的能力。

## d. 熵以鼓励探索

PPO 在目标函数中加入了一个熵项，以鼓励智能体更多地探索环境，而不是过早地收敛到次优解。熵项增加了智能体决策的随机性，这有助于防止过度拟合某一特定策略：

其中H(πθ)表示策略的熵。通过加入这一项，PPO确保智能体不会过早收敛，并鼓励其继续探索不同的动作和策略，从而提高整体学习效率。

# 结论

PPO的数学基础展示了该算法如何实现稳定和高效的学习。通过概率比率、裁剪、优势估计和熵等概念，PPO在探索和利用之间提供了强大的平衡。这些特性使得它成为研究人员和从业人员在复杂环境中工作的稳健选择。PPO的简单性结合其高效性和有效性，使其成为强化学习中受欢迎且有价值的算法。

# 参考文献

+   [https://books.google.fr/books?hl=fr&lr=&id=sWV0DwAAQBAJ&oi=fnd&pg=PR7&dq=reinforcement+learning+an+introduction&ots=1-9av2aqTb&sig=qMSnFC56yqPQugvqS3_uwCN78z0#v=onepage&q=reinforcement%20learning%20an%20introduction&f=false](https://books.google.fr/books?hl=fr&lr=&id=sWV0DwAAQBAJ&oi=fnd&pg=PR7&dq=reinforcement+learning+introduction&ots=1-9av2asXb&sig=ZDKNYXZc8gMIzEqeonvMxcN8skE#v=onepage&q=reinforcement%20learning%20introduction&f=false)

+   [https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)

*本文部分内容通过* [*DeepL*](https://www.deepl.com/fr/translator)*翻译自法语。*
