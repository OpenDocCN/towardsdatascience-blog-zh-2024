- en: 'AdaBoost Classifier, Explained: A Visual Guide with Code Examples'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaBoost 分类器解析：带代码示例的可视化指南
- en: 原文：[https://towardsdatascience.com/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b?source=collection_archive---------1-----------------------#2024-11-10](https://towardsdatascience.com/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b?source=collection_archive---------1-----------------------#2024-11-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b?source=collection_archive---------1-----------------------#2024-11-10](https://towardsdatascience.com/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b?source=collection_archive---------1-----------------------#2024-11-10)
- en: ENSEMBLE LEARNING
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成学习
- en: '**Putting the weight where weak learners need it most**'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**将权重放在弱学习器最需要的地方**'
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)
    ·11 min read·Nov 10, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)
    ·阅读时长11分钟·2024年11月10日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=post_page-----fc0f25326d7b--------------------------------)
    [## Random Forest, Explained: A Visual Guide with Code Examples'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=post_page-----fc0f25326d7b--------------------------------)
    [## 随机森林解析：带代码示例的可视化指南'
- en: Making tree-mendous predictions with random trees
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用随机树做出惊人的预测
- en: towardsdatascience.com](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=post_page-----fc0f25326d7b--------------------------------)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=post_page-----fc0f25326d7b--------------------------------)
- en: 'Everyone makes mistakes — even the simplest [decision trees](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    in machine learning. Instead of ignoring them, AdaBoost (Adaptive Boosting) algorithm
    does something different: it learns (or *adapts*) from these mistakes to get better.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人都会犯错——即便是最简单的[决策树](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)也会在机器学习中出错。AdaBoost（自适应提升）算法并没有忽视这些错误，而是做了不同的事情：它从这些错误中学习（或*适应*），不断提升。
- en: Unlike [Random Fores](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c)t,
    which makes many trees at once, AdaBoost starts with a single, simple tree and
    identifies the instances it misclassifies. It then builds new trees to fix those
    errors, learning from its mistakes and getting better with each step.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 与[随机森林](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c)不同，随机森林是同时构建多棵树，而AdaBoost则从一棵简单的树开始，识别它误分类的实例。然后，它构建新的树来修正这些错误，边学习边改进。
- en: Here, we’ll illustrate exactly how AdaBoost makes its predictions, building
    strength by combining targeted weak learners just like a workout routine that
    turns focused exercises into full-body power.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将具体说明AdaBoost是如何进行预测的，通过结合多个针对性的弱学习器逐步增强其预测能力，就像一项将专注的锻炼转化为全身力量的训练计划。
- en: '![](../Images/38e85c5674011760cafd45ee4d7c85c0.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38e85c5674011760cafd45ee4d7c85c0.png)'
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可视化图像：作者使用Canva Pro创建，已针对手机优化；在桌面上可能显示为过大。
- en: Definition
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义
- en: AdaBoost is an ensemble machine learning model that creates a sequence of weighted
    decision trees, typically using shallow trees (often just single-level “stumps”).
    Each tree is trained on the entire dataset, but with adaptive sample weights that
    give more importance to previously misclassified examples.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 是一种集成机器学习模型，通过加权的决策树序列来创建模型，通常使用浅层树（通常是单层“树桩”）。每棵树都在整个数据集上进行训练，但使用自适应的样本权重，这些权重给与之前被错误分类的样本更高的优先级。
- en: For classification tasks, AdaBoost combines the trees through a weighted voting
    system, where better-performing trees get more influence in the final decision.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，AdaBoost 通过加权投票系统将树结合起来，表现更好的树在最终决策中有更大的影响力。
- en: The model’s strength comes from its adaptive learning process — while each simple
    tree might be a “weak learner” that performs only slightly better than random
    guessing, the weighted combination of trees creates a “strong learner” that **progressively
    focuses on and corrects mistakes**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的优势来自于其自适应学习过程——尽管每棵简单的树可能是一个“弱学习器”，其表现仅比随机猜测稍好，但树的加权组合创建了一个“强学习器”，它**逐步聚焦并修正错误**。
- en: '![](../Images/5860f3e88183ae0db9f76a78c52525eb.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5860f3e88183ae0db9f76a78c52525eb.png)'
- en: AdaBoost is part of the boosting family of algorithms because it builds trees
    one at a time. Each new tree tries to fix the mistakes made by the previous trees.
    It then uses a weighted vote to combine their answers and make its final prediction.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 属于提升算法家族的一部分，因为它一次构建一棵树。每棵新树尝试修正之前树的错误。然后，它通过加权投票将各棵树的结果结合起来，做出最终的预测。
- en: Dataset Used
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的数据集
- en: Throughout this article, we’ll focus on the classic golf dataset as an example
    for classification.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将以经典的高尔夫数据集作为分类的示例。
- en: '![](../Images/05586d1dcea17f8a18206b58019181ea.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05586d1dcea17f8a18206b58019181ea.png)'
- en: 'Columns: ‘Outlook (one-hot-encoded into 3 columns)’, ’Temperature’ (in Fahrenheit),
    ‘Humidity’ (in %), ‘Windy’ (Yes/No) and ‘Play’ (Yes/No, target feature)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 列：‘Outlook（经过独热编码为3列）’，‘Temperature’（以华氏度表示），‘Humidity’（以百分比表示），‘Windy’（是/否）和‘Play’（是/否，目标特征）
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Main Mechanism
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要机制
- en: 'Here’s how AdaBoost works:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 AdaBoost 的工作原理：
- en: '**Initialize Weights:** Assign equal weight to each training example.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化权重：** 为每个训练样本分配相等的权重。'
- en: '**Iterative Learning:** In each step, a simple decision tree is trained and
    its performance is checked. Misclassified examples get more weight, making them
    a priority for the next tree. Correctly classified examples stay the same, and
    all weights are adjusted to add up to 1.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迭代学习：** 在每一步，训练一棵简单的决策树并检查其表现。被错误分类的样本将获得更多的权重，使其成为下一棵树的优先考虑对象。正确分类的样本保持不变，所有权重都会调整，以确保它们的总和为1。'
- en: '**Build Weak Learners:** Each new, simple tree targets the mistakes of the
    previous ones, creating a sequence of specialized weak learners.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**构建弱学习器：** 每棵新的简单树都针对之前树的错误进行修正，创建了一系列专门化的弱学习器。'
- en: '**Final Prediction:** Combine all trees through weighted voting, where each
    tree’s vote is based on its importance value, giving more influence to more accurate
    trees.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最终预测：** 通过加权投票将所有树的结果结合起来，每棵树的投票基于其重要性值，更准确的树会有更大的影响力。'
- en: '![](../Images/0e77ad190e2d8b5466b6debd4ecff4cf.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e77ad190e2d8b5466b6debd4ecff4cf.png)'
- en: An AdaBoost Classifier makes predictions by using many simple decision trees
    (usually 50–100). Each tree, called a “stump,” focuses on one important feature,
    like temperature or humidity. The final prediction is made by combining all the
    trees’ votes, each weighted by how important that tree is (“alpha”).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 分类器通过使用许多简单的决策树（通常是50到100棵树）来进行预测。每棵树，称为“树桩”，专注于一个重要特征，如温度或湿度。最终的预测是通过结合所有树的投票来做出的，每棵树的投票根据该树的重要性（“alpha”）加权。
- en: Training Steps
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练步骤
- en: Here, we’ll follow the SAMME (Stagewise Additive Modeling using a Multi-class
    Exponential loss function) algorithm, the standard approach in scikit-learn that
    handles both binary and multi-class classification.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将遵循 SAMME（使用多类指数损失函数的阶段性加法建模）算法，这是 scikit-learn 中的标准方法，能够处理二分类和多分类问题。
- en: 1.1\. Decide the weak learner to be used. A one-level decision tree (or “stump”)
    is the default choice.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 1.1\. 决定要使用的弱学习器。默认选择是一棵单层决策树（或称“树桩”）。
- en: 1.2\. Decide how many weak learner (in this case the number of trees) you want
    to build (the default is 50 trees).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 1.2\. 决定要构建多少个弱学习器（在这个例子中是树的数量，默认是50棵树）。
- en: '![](../Images/9e914ecac19ae6b4d06dffb719872b0c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e914ecac19ae6b4d06dffb719872b0c.png)'
- en: We begin with depth-1 decision trees (stumps) as our weak learners. Each stump
    makes just one split, and we’ll train 50 of them sequentially, adjusting weights
    along the way.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从深度为1的决策树（树桩）开始作为我们的弱学习器。每个树桩仅做一次分裂，我们将顺序训练50个树桩，并在训练过程中调整权重。
- en: '1.3\. Start by giving each training example equal weight:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3\. 从给每个训练示例相同的权重开始：
- en: · Each sample gets weight = 1/*N* (*N* is total number of samples)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: · 每个样本的权重 = 1/*N*（*N* 是样本总数）
- en: · All weights together sum to 1
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: · 所有权重加起来等于1
- en: '![](../Images/a10a476d7edac1311aefbcf5e3b50dc4.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a10a476d7edac1311aefbcf5e3b50dc4.png)'
- en: All data points start with equal weights (0.0714), with the total weight adding
    up to 1\. This ensures every example is equally important when training begins.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据点开始时的权重相同（0.0714），总权重加起来为1。这确保了在训练开始时，每个示例都是同等重要的。
- en: For the First Tree
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对于第一棵树
- en: 2.1\. Build a decision stump while considering sample weights
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 2.1\. 在考虑样本权重的情况下构建决策树桩
- en: '![](../Images/68a18d476059d011467b9290fe6574e7.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68a18d476059d011467b9290fe6574e7.png)'
- en: Before making the first split, the algorithm examines all data points with their
    weights to find the best splitting point. These weights influence how important
    each example is in making the split decision.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行第一次分裂之前，算法会检查所有数据点及其权重，以找到最佳的分裂点。这些权重影响每个示例在做出分裂决策时的重要性。
- en: a. Calculate initial weighted Gini impurity for the root node
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: a. 计算根节点的初始加权基尼不纯度
- en: '![](../Images/f8ad9f1f34d4f6cf8aa4ffa1096cec83.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8ad9f1f34d4f6cf8aa4ffa1096cec83.png)'
- en: The algorithm calculates the Gini impurity score at the root node, but now considers
    the weights of all data points.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 算法计算根节点的基尼不纯度得分，但现在考虑了所有数据点的权重。
- en: 'b. For each feature:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: b. 对每个特征：
- en: · Sort data by feature values (exactly like in [Decision Tree](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    classifier)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: · 按特征值对数据进行排序（与[决策树](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)分类器中的操作完全相同）
- en: '![](../Images/b62db5393a1ed6bdb676b65a3b9207dc.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b62db5393a1ed6bdb676b65a3b9207dc.png)'
- en: For each feature, the algorithm sorts the data and identifies potential split
    points, exactly like the standard Decision Tree.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个特征，算法对数据进行排序并识别潜在的分裂点，完全类似于标准的决策树。
- en: '· For each possible split point:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: · 对每个可能的分裂点：
- en: ·· Split samples into left and right groups
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ·· 将样本分为左组和右组
- en: ·· Calculate weighted Gini impurity for both groups
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ·· 计算两个组的加权基尼不纯度
- en: ·· Calculate weighted Gini impurity reduction for this split
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ·· 计算此分裂的加权基尼不纯度减少量
- en: '![](../Images/a825f39a5d40c52a1a8255495e984c53.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a825f39a5d40c52a1a8255495e984c53.png)'
- en: The algorithm calculates weighted Gini impurity for each potential split and
    compares it to the parent node. For feature “sunny” with split point 0.5, this
    impurity reduction (0.066) shows how much this split improves the data separation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 算法计算每个潜在分裂点的加权基尼不纯度，并将其与父节点进行比较。对于特征“sunny”（分裂点为0.5），该不纯度减少量（0.066）显示了此分裂如何改善数据的分离。
- en: c. Pick the split that gives the largest Gini impurity reduction
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: c. 选择能够带来最大基尼不纯度减少的分裂点
- en: '![](../Images/5a4349476737e871ffda440a8e995b28.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a4349476737e871ffda440a8e995b28.png)'
- en: After checking all possible splits across features, the column ‘overcast’ (with
    split point 0.5) gives the highest impurity reduction of 0.102\. This means it’s
    the most effective way to separate the classes, making it the best choice for
    the first split.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查所有可能的特征分裂后，‘overcast’列（分裂点为0.5）提供了最高的不纯度减少（0.102）。这意味着它是最有效的分类分离方式，因此成为第一次分裂的最佳选择。
- en: d. Create a simple one-split tree using this decision
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: d. 使用这个决策创建一个简单的单分裂树
- en: '![](../Images/e9cf90d6e8fe547e552f0af06cc220b8.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9cf90d6e8fe547e552f0af06cc220b8.png)'
- en: Using the best split point found, the algorithm divides the data into two groups,
    each keeping their original weights. This simple decision tree is purposely kept
    small and imperfect, making it just slightly better than random guessing.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用找到的最佳分裂点，算法将数据分为两个组，每个组保持其原始权重。这个简单的决策树故意保持较小且不完美，使其仅略微优于随机猜测。
- en: 2.2\. Evaluate how good this tree is
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2\. 评估这棵树的好坏
- en: a. Use the tree to predict the label of the training set.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: a. 使用树来预测训练集的标签。
- en: b. Add up the weights of all **misclassified samples** to get error rate
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: b. 将所有**误分类样本**的权重相加，得到误差率
- en: '![](../Images/d3dcfbdd4971811d91c3ba1fb5d763c3.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3dcfbdd4971811d91c3ba1fb5d763c3.png)'
- en: The first weak learner makes predictions **on the training data**, and we check
    where it made mistakes (marked with X). The error rate of 0.357 shows this simple
    tree gets some predictions wrong, which is expected and will help guide the next
    steps of training.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第一棵弱学习器对**训练数据**进行预测，我们检查它在哪些地方犯了错误（标记为X）。错误率为0.357，显示这棵简单的树在一些预测上是错误的，这是可以预期的，并且有助于指导下一步的训练。
- en: 'c. Calculate tree importance (*α*) using:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: c. 使用以下公式计算树的权重 (*α*)：
- en: '*α* = learning_rate × log((1-error)/error)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*α* = 学习率 × log((1-错误)/错误)'
- en: '![](../Images/e4a210b84e6605415fb8f6268044a9c9.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4a210b84e6605415fb8f6268044a9c9.png)'
- en: Using the error rate, we calculate the tree’s influence score (α = 0.5878).
    Higher scores mean more accurate trees, and this tree earned moderate importance
    for its decent performance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 使用错误率，我们计算树的影响分数（α = 0.5878）。较高的分数意味着树的准确性更高，这棵树因其良好的表现获得了适中的重要性。
- en: 2.3\. Update sample weights
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 2.3\. 更新样本权重
- en: a. Keep the original weights for correctly classified samples
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: a. 对于正确分类的样本，保留原始权重
- en: b. Multiply the weights of misclassified samples by e^(*α*).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: b. 将错误分类样本的权重乘以 e^(*α*)。
- en: c. Divide each weight by the sum of all weights. This normalization ensures
    all weights still sum to 1 while maintaining their relative proportions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: c. 将每个权重除以所有权重的总和。这一归一化过程确保所有权重仍然加和为1，同时保持它们的相对比例。
- en: '![](../Images/b4490670bf0192e1521f0f479a4c1873.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4490670bf0192e1521f0f479a4c1873.png)'
- en: Cases where the tree made mistakes (marked with X) get higher weights for the
    next round. After increasing these weights, all weights are normalized to sum
    to 1, ensuring misclassified examples get more attention in the next tree.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 树错误分类的情况（标记为X）会在下一轮中获得更高的权重。增加这些权重后，所有权重会归一化，使得错误分类的样本在下一棵树中得到更多关注。
- en: For the Second Tree
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对于第二棵树
- en: 2.1\. Build a new stump, but now using the updated weights
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 2.1\. 构建一个新的桩决策树，但这次使用更新后的权重
- en: 'a. Calculate new weighted Gini impurity for root node:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: a. 计算根节点的新的加权基尼不纯度：
- en: · Will be different because misclassified samples now have bigger weights
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: · 由于错误分类的样本权重较大，结果会有所不同
- en: · Correctly classified samples now have smaller weights
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: · 正确分类的样本现在具有较小的权重
- en: '![](../Images/7e820aa41f2204a8607d14793adc59d2.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e820aa41f2204a8607d14793adc59d2.png)'
- en: Using the updated weights (where misclassified examples now have higher importance),
    the algorithm calculates the weighted Gini impurity at the root node. This begins
    the process of building the second decision tree.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更新后的权重（此时错误分类的样本权重更高），算法计算根节点的加权基尼不纯度。这将开始构建第二棵决策树的过程。
- en: 'b. For each feature:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: b. 对于每个特征：
- en: · Same process as before, but the weights have changed
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: · 与之前相同的过程，但权重已发生变化
- en: c. Pick the split with best weighted Gini impurity reduction
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: c. 选择最佳的加权基尼不纯度减少划分
- en: · Often completely different from the first tree’s split
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: · 通常与第一棵树的划分完全不同
- en: · Focuses on samples the first tree got wrong
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: · 重点关注第一个树错误分类的样本
- en: '![](../Images/8380dccedc96e8aad94425ca3ab24b95.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8380dccedc96e8aad94425ca3ab24b95.png)'
- en: With updated weights, different split points show different effectiveness. Notice
    that “overcast” is no longer the best split — the algorithm now finds temperature
    (84.0) gives the highest impurity reduction, showing how weight changes affect
    split selection.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更新后的权重，不同的划分点展现出不同的效果。注意，“阴天”不再是最佳划分点——算法现在发现温度（84.0）给出的加权基尼不纯度减少最大，显示出权重变化如何影响划分选择。
- en: d. Create the second stump
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: d. 创建第二棵桩决策树
- en: '![](../Images/f1d85397a88dd3805b964813e8a5fd14.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1d85397a88dd3805b964813e8a5fd14.png)'
- en: Using temperature ≤ 84.0 as the split point, the algorithm assigns YES/NO to
    each leaf based on which class has more total weight in that group, not just by
    counting examples. This weighted voting helps correct the previous tree’s mistakes.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用温度 ≤ 84.0 作为划分点，算法根据每组中总权重较大的类别来为每个叶节点分配 YES/NO，而不仅仅是通过计数示例。这种加权投票有助于纠正第一棵树的错误。
- en: 2.2\. Evaluate this new tree
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2\. 评估这棵新树
- en: a. Calculate error rate with current weights
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: a. 使用当前权重计算错误率
- en: b. Calculate its importance (*α*) using the same formula as before
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: b. 使用与之前相同的公式计算其重要性 (*α*)
- en: '2.3\. Update weights again — Same process: increase weights for mistakes then
    normalize.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 2.3\. 再次更新权重 — 同样的过程：增加错误分类的权重，然后进行归一化。
- en: '![](../Images/0773fcc8ffd93058f55eebccc00b9c1f.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0773fcc8ffd93058f55eebccc00b9c1f.png)'
- en: The second tree achieves a lower error rate (0.222) and higher importance score
    (α = 1.253) than the first tree. Like before, misclassified examples get higher
    weights for the next round.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第二棵树的错误率较低（0.222），且重要性得分较高（α = 1.253），与第一棵树相比。像之前一样，误分类的示例在下一轮会获得更高的权重。
- en: For the Third Tree onwards
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从第三棵树开始
- en: Repeat Step 2.1–2.3 for all remaining trees.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有剩余的树重复步骤2.1–2.3。
- en: '![](../Images/d19ad8eddbd4588dac4781b288276203.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d19ad8eddbd4588dac4781b288276203.png)'
- en: The algorithm builds 50 simple decision trees sequentially, each with its own
    importance score (α). Each tree learns from previous mistakes by focusing on different
    aspects of the data, creating a strong combined model. Notice how some trees (like
    Tree 2) get higher importance scores when they perform better.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法依次构建50棵简单的决策树，每棵树都有自己的重要性得分（α）。每棵树通过关注数据的不同方面来从之前的错误中学习，创建一个强大的组合模型。请注意，一些树（如树2）在表现更好的时候会获得更高的权重得分。
- en: '**Step 3: Final Ensemble** 3.1\. Keep all trees and their importance scores'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**第3步：最终集成** 3.1\. 保留所有树及其重要性得分'
- en: '![](../Images/4f0e5ae526c79b2e46766e4e85bfdb13.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f0e5ae526c79b2e46766e4e85bfdb13.png)'
- en: The 50 simple decision trees work together as a team, each with its own importance
    score (α). When making predictions, trees with higher α values (like Tree 2 with
    1.253) have more influence on the final decision than trees with lower scores.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这50棵简单的决策树作为一个团队共同工作，每棵树都有自己的重要性得分（α）。在进行预测时，具有较高α值的树（如树2，α = 1.253）对最终决策的影响大于得分较低的树。
- en: '[PRE1]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/c3c0699734afabfd3e5c095b1571e9ce.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3c0699734afabfd3e5c095b1571e9ce.png)'
- en: Each node shows its ‘value’ parameter as [weight_NO, weight_YES], which represents
    the weighted proportion of each class at that node. These weights come from the
    sample weights we calculated during training.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点显示其“值”参数，格式为[weight_NO, weight_YES]，表示该节点每个类别的加权比例。这些权重来自我们在训练过程中计算的样本权重。
- en: Testing Step
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试步骤
- en: 'For predicting:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 用于预测：
- en: a. Get each tree’s prediction
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: a. 获取每棵树的预测结果
- en: b. Multiply each by its importance score (*α*)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: b. 将每个值乘以其重要性得分（*α*）
- en: c. Add them all up
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: c. 将它们全部相加
- en: d. The class with higher total weight will be the final prediction
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: d. 总权重较高的类别将作为最终预测
- en: '![](../Images/c9a49e61ccbf0b28a73a1ace1a9fd700.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9a49e61ccbf0b28a73a1ace1a9fd700.png)'
- en: When predicting for new data, each tree makes its prediction and multiplies
    it by its importance score (α). The final decision comes from adding up all weighted
    votes — here, the NO class gets a higher total score (23.315 vs 15.440), so the
    model predicts NO for this unseen example.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当预测新数据时，每棵树都会做出自己的预测，并将其乘以自己的重要性得分（α）。最终的决策来自于将所有加权投票结果相加——在这里，NO类别获得了更高的总分（23.315对15.440），因此模型预测该示例为NO。
- en: Evaluation Step
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估步骤
- en: After building all the trees, we can evaluate the test set.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 构建完所有树之后，我们可以评估测试集。
- en: '![](../Images/0e4cb581d6d058a44ad7f63b4a7dd494.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e4cb581d6d058a44ad7f63b4a7dd494.png)'
- en: By iteratively training and weighting weak learners to focus on misclassified
    examples, AdaBoost creates a strong classifier that achieves high accuracy — typically
    better than single decision trees or simpler models!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反复训练和加权弱学习器来专注于误分类的示例，AdaBoost创建了一个强大的分类器，能够实现高精度——通常比单一决策树或更简单的模型更好！
- en: '[PRE2]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Key Parameters
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键参数
- en: 'Here are the key parameters for AdaBoost, particularly in `scikit-learn`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是AdaBoost的关键参数，特别是在`scikit-learn`中：
- en: '`estimator`: This is the base model that AdaBoost uses to build its final solution.
    The 3 most common weak learners are:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`estimator`：这是AdaBoost用来构建最终解决方案的基础模型。最常见的三种弱学习器是：'
- en: '**a. Decision Tree with depth 1 (Decision Stump)**: This is the default and
    most popular choice. Because it only has one split, it is considered a very weak
    learner that is just a bit better than random guessing, exactly what is needed
    for boosting process.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**a. 深度为1的决策树（决策树桩）**：这是默认的也是最流行的选择。由于它只有一个分裂，因此被认为是一个非常弱的学习器，仅比随机猜测略好，这正是提升过程所需要的。'
- en: '**b. Logistic Regression**: Logistic regression (especially with high-penalty)
    can also be used here even though it is not really a weak learner. It could be
    useful for data that has linear relationship.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**b. 逻辑回归**：逻辑回归（特别是高惩罚的情况下）也可以在这里使用，尽管它并不是一个真正的弱学习器。它对于具有线性关系的数据可能很有用。'
- en: '**c. Decision Trees with small depth (e.g., depth 2 or 3)**: These are slightly
    more complex than decision stumps. They’re still fairly simple, but can handle
    slightly more complex patterns than the decision stump.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**c. 小深度的决策树（例如，深度为2或3）：**这些树比决策树桩稍微复杂一些。它们仍然相对简单，但可以处理比决策树桩稍微复杂一些的模式。'
- en: '![](../Images/72829a3081a99f628be3a36d2be8ad47.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72829a3081a99f628be3a36d2be8ad47.png)'
- en: AdaBoost’s base models can be simple decision stumps (depth=1), small trees
    (depth 2–3), or penalized linear models. Each type is kept simple to avoid overfitting
    while offering different ways to capture patterns.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost的基模型可以是简单的决策树桩（深度=1）、小深度的树（深度为2-3）或惩罚的线性模型。每种类型都保持简单，以避免过拟合，同时提供不同的方式来捕捉模式。
- en: '`n_estimators`: The number of weak learners to combine, typically around 50–100\.
    Using more than 100 rarely helps.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_estimators`：要组合的弱学习器数量，通常在50到100之间。超过100通常不会带来显著的改善。'
- en: '`learning_rate`: Controls how much each classifier affects the final result.
    Common starting values are 0.1, 0.5, or 1.0\. Lower numbers (like 0.1) and a bit
    higher `n_estimator` usually work better.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate`：控制每个分类器对最终结果的影响。常见的初始值有0.1、0.5或1.0。较小的值（如0.1）和稍微更高的`n_estimators`通常效果更好。'
- en: Key differences from Random Forest
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与随机森林的主要区别
- en: 'As both Random Forest and AdaBoost works with multiple trees, it is easy to
    confuse the parameters involved. The key difference is that Random Forest combines
    many trees **independently** (bagging) while AdaBoost builds trees **one after
    another** to fix mistakes (boosting). Here are some other details about their
    differences:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机森林和AdaBoost都使用多棵树，因此很容易混淆其中的参数。关键区别在于，随机森林是**独立**地（袋装法）结合多棵树，而AdaBoost是**一个接一个地**构建树以修正错误（提升法）。以下是它们差异的其他一些细节：
- en: No `bootstrap` parameter because AdaBoost uses all data but with changing weights
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有`bootstrap`参数，因为AdaBoost使用所有数据，但权重会发生变化
- en: No `oob_score` because AdaBoost doesn't use bootstrap sampling
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有`oob_score`，因为AdaBoost不使用自助采样法
- en: '`learning_rate` becomes crucial (not present in Random Forest)'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`learning_rate`变得至关重要（在随机森林中不存在）'
- en: Tree depth is typically kept very shallow (usually just stumps) unlike Random
    Forest’s deeper trees
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 树的深度通常保持非常浅（通常只是树桩），与随机森林的较深树不同
- en: The focus shifts from parallel independent trees to sequential dependent trees,
    making parameters like `n_jobs` less relevant
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 焦点从并行独立树转移到顺序依赖树，这使得像`n_jobs`这样的参数变得不那么相关
- en: Pros & Cons
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优缺点
- en: 'Pros:'
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优点：
- en: '**Adaptive Learning:** AdaBoost gets better by giving more weight to mistakes
    it made. Each new tree pays more attention to the hard cases it got wrong.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应学习：**AdaBoost通过增加对错误的权重来提升性能。每棵新树会更加关注它之前做错的难例。'
- en: '**Resists Overfitting:** Even though it keeps adding more trees one by one,
    AdaBoost usually doesn’t get too focused on training data. This is because it
    uses weighted voting, so no single tree can control the final answer too much.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抗过拟合：**尽管它会一个接一个地添加更多的树，但AdaBoost通常不会过于专注于训练数据。这是因为它使用加权投票，因此没有任何一棵树能过多地控制最终的答案。'
- en: '**Built-in Feature Selection:** AdaBoost naturally finds which features matter
    most. Each simple tree picks the most useful feature for that round, which means
    it automatically selects important features as it trains.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内置特征选择：**AdaBoost自然地找出哪些特征最重要。每棵简单的树都会选择该轮中最有用的特征，这意味着它在训练时会自动选择重要特征。'
- en: 'Cons:'
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点：
- en: '**Sensitive to Noise:** Because it gives more weight to mistakes, AdaBoost
    can have trouble with messy or wrong data. If some training examples have wrong
    labels, it might focus too much on these bad examples, making the whole model
    worse.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对噪声敏感：**由于AdaBoost给错误更多的权重，它可能会在数据杂乱或错误时出现问题。如果一些训练样本标签错误，它可能会过于关注这些错误样本，从而使整个模型变得更差。'
- en: '**Must Be Sequential:** Unlike Random Forest which can train many trees at
    once, AdaBoost must train one tree at a time because each new tree needs to know
    how the previous trees did. This makes it slower to train.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**必须是顺序的：**与随机森林可以同时训练多棵树不同，AdaBoost必须一次训练一棵树，因为每棵新树需要知道前一棵树的表现。这使得训练速度较慢。'
- en: '**Learning Rate Sensitivity:** While it has fewer settings to tune than Random
    Forest, the learning rate really affects how well it works. If it’s too high,
    it might learn the training data too exactly. If it’s too low, it needs many more
    trees to work well.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率敏感性：**虽然它的调整项比随机森林少，但学习率确实影响其效果。如果学习率过高，模型可能会过拟合训练数据。如果学习率过低，则需要更多的树才能取得良好的效果。'
- en: Final Remarks
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的备注
- en: AdaBoost is a key boosting algorithm that many newer methods learned from. Its
    main idea — getting better by focusing on mistakes — has helped shape many modern
    machine learning tools. While other methods try to be perfect from the start,
    AdaBoost tries to show that sometimes the best way to solve a problem is to learn
    from your errors and keep improving.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 是一种关键的提升算法，许多新方法都从中汲取了灵感。它的主要思想——通过关注错误来变得更好——帮助塑造了许多现代机器学习工具。虽然其他方法试图一开始就做到完美，但
    AdaBoost 则试图表明，有时候解决问题的最佳方式是从错误中学习并不断改进。
- en: AdaBoost also works best in binary classification problems and when your data
    is clean. While Random Forest might be better for more general tasks (like predicting
    numbers) or messy data, AdaBoost can give really good results when used in the
    right way. The fact that people still use it after so many years shows just how
    well the core idea works!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 最适用于二分类问题，并且当数据清晰时表现最佳。虽然随机森林可能更适用于更一般的任务（例如预测数字）或杂乱的数据，但在正确使用的情况下，AdaBoost
    可以产生非常好的结果。人们在多年后仍然使用它，足以证明其核心思想的有效性！
- en: 🌟 AdaBoost Classifier Code Summarized
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🌟 AdaBoost 分类器代码概述
- en: '[PRE3]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Further Reading
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: For a detailed explanation of the [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)
    and its implementation in scikit-learn, readers can refer to the official documentation,
    which provides comprehensive information on its usage and parameters.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)
    及其在 scikit-learn 中的实现，读者可以参考官方文档，文档提供了有关其用法和参数的详细信息。
- en: Technical Environment
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术环境
- en: This article uses Python 3.7 and scikit-learn 1.6\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用 Python 3.7 和 scikit-learn 1.6。尽管所讨论的概念普遍适用，但具体的代码实现可能会因不同版本而略有不同。
- en: About the Illustrations
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于插图
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图像均由作者创作，并结合了 Canva Pro 的授权设计元素。
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙀𝙣𝙨𝙚𝙢𝙗𝙡𝙚 𝙇𝙚𝙖𝙧𝙣𝙞𝙣𝙜 𝙝𝙚𝙧𝙚:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙀𝙣𝙨𝙚𝙢𝙗𝙡𝙚 𝙇𝙚𝙖𝙧𝙣𝙞𝙣𝙜 𝙝𝙚𝙧𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fc0f25326d7b--------------------------------)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fc0f25326d7b--------------------------------)'
- en: Ensemble Learning
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成学习
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----fc0f25326d7b--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----fc0f25326d7b--------------------------------)4篇故事![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fc0f25326d7b--------------------------------)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fc0f25326d7b--------------------------------)'
- en: Classification Algorithms
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类算法
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----fc0f25326d7b--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----fc0f25326d7b--------------------------------)8篇故事![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
