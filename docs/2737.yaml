- en: 'AdaBoost Classifier, Explained: A Visual Guide with Code Examples'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b?source=collection_archive---------1-----------------------#2024-11-10](https://towardsdatascience.com/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b?source=collection_archive---------1-----------------------#2024-11-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ENSEMBLE LEARNING
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Putting the weight where weak learners need it most**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------)
    ¬∑11 min read¬∑Nov 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=post_page-----fc0f25326d7b--------------------------------)
    [## Random Forest, Explained: A Visual Guide with Code Examples'
  prefs: []
  type: TYPE_NORMAL
- en: Making tree-mendous predictions with random trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=post_page-----fc0f25326d7b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Everyone makes mistakes ‚Äî even the simplest [decision trees](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    in machine learning. Instead of ignoring them, AdaBoost (Adaptive Boosting) algorithm
    does something different: it learns (or *adapts*) from these mistakes to get better.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike [Random Fores](/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c)t,
    which makes many trees at once, AdaBoost starts with a single, simple tree and
    identifies the instances it misclassifies. It then builds new trees to fix those
    errors, learning from its mistakes and getting better with each step.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we‚Äôll illustrate exactly how AdaBoost makes its predictions, building
    strength by combining targeted weak learners just like a workout routine that
    turns focused exercises into full-body power.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38e85c5674011760cafd45ee4d7c85c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AdaBoost is an ensemble machine learning model that creates a sequence of weighted
    decision trees, typically using shallow trees (often just single-level ‚Äústumps‚Äù).
    Each tree is trained on the entire dataset, but with adaptive sample weights that
    give more importance to previously misclassified examples.
  prefs: []
  type: TYPE_NORMAL
- en: For classification tasks, AdaBoost combines the trees through a weighted voting
    system, where better-performing trees get more influence in the final decision.
  prefs: []
  type: TYPE_NORMAL
- en: The model‚Äôs strength comes from its adaptive learning process ‚Äî while each simple
    tree might be a ‚Äúweak learner‚Äù that performs only slightly better than random
    guessing, the weighted combination of trees creates a ‚Äústrong learner‚Äù that **progressively
    focuses on and corrects mistakes**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5860f3e88183ae0db9f76a78c52525eb.png)'
  prefs: []
  type: TYPE_IMG
- en: AdaBoost is part of the boosting family of algorithms because it builds trees
    one at a time. Each new tree tries to fix the mistakes made by the previous trees.
    It then uses a weighted vote to combine their answers and make its final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this article, we‚Äôll focus on the classic golf dataset as an example
    for classification.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05586d1dcea17f8a18206b58019181ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: ‚ÄòOutlook (one-hot-encoded into 3 columns)‚Äô, ‚ÄôTemperature‚Äô (in Fahrenheit),
    ‚ÄòHumidity‚Äô (in %), ‚ÄòWindy‚Äô (Yes/No) and ‚ÄòPlay‚Äô (Yes/No, target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Main Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here‚Äôs how AdaBoost works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialize Weights:** Assign equal weight to each training example.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterative Learning:** In each step, a simple decision tree is trained and
    its performance is checked. Misclassified examples get more weight, making them
    a priority for the next tree. Correctly classified examples stay the same, and
    all weights are adjusted to add up to 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Build Weak Learners:** Each new, simple tree targets the mistakes of the
    previous ones, creating a sequence of specialized weak learners.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Final Prediction:** Combine all trees through weighted voting, where each
    tree‚Äôs vote is based on its importance value, giving more influence to more accurate
    trees.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/0e77ad190e2d8b5466b6debd4ecff4cf.png)'
  prefs: []
  type: TYPE_IMG
- en: An AdaBoost Classifier makes predictions by using many simple decision trees
    (usually 50‚Äì100). Each tree, called a ‚Äústump,‚Äù focuses on one important feature,
    like temperature or humidity. The final prediction is made by combining all the
    trees‚Äô votes, each weighted by how important that tree is (‚Äúalpha‚Äù).
  prefs: []
  type: TYPE_NORMAL
- en: Training Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we‚Äôll follow the SAMME (Stagewise Additive Modeling using a Multi-class
    Exponential loss function) algorithm, the standard approach in scikit-learn that
    handles both binary and multi-class classification.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1\. Decide the weak learner to be used. A one-level decision tree (or ‚Äústump‚Äù)
    is the default choice.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. Decide how many weak learner (in this case the number of trees) you want
    to build (the default is 50 trees).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e914ecac19ae6b4d06dffb719872b0c.png)'
  prefs: []
  type: TYPE_IMG
- en: We begin with depth-1 decision trees (stumps) as our weak learners. Each stump
    makes just one split, and we‚Äôll train 50 of them sequentially, adjusting weights
    along the way.
  prefs: []
  type: TYPE_NORMAL
- en: '1.3\. Start by giving each training example equal weight:'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑ Each sample gets weight = 1/*N* (*N* is total number of samples)
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑ All weights together sum to 1
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a10a476d7edac1311aefbcf5e3b50dc4.png)'
  prefs: []
  type: TYPE_IMG
- en: All data points start with equal weights (0.0714), with the total weight adding
    up to 1\. This ensures every example is equally important when training begins.
  prefs: []
  type: TYPE_NORMAL
- en: For the First Tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Build a decision stump while considering sample weights
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68a18d476059d011467b9290fe6574e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Before making the first split, the algorithm examines all data points with their
    weights to find the best splitting point. These weights influence how important
    each example is in making the split decision.
  prefs: []
  type: TYPE_NORMAL
- en: a. Calculate initial weighted Gini impurity for the root node
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8ad9f1f34d4f6cf8aa4ffa1096cec83.png)'
  prefs: []
  type: TYPE_IMG
- en: The algorithm calculates the Gini impurity score at the root node, but now considers
    the weights of all data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'b. For each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑ Sort data by feature values (exactly like in [Decision Tree](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    classifier)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b62db5393a1ed6bdb676b65a3b9207dc.png)'
  prefs: []
  type: TYPE_IMG
- en: For each feature, the algorithm sorts the data and identifies potential split
    points, exactly like the standard Decision Tree.
  prefs: []
  type: TYPE_NORMAL
- en: '¬∑ For each possible split point:'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑¬∑ Split samples into left and right groups
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑¬∑ Calculate weighted Gini impurity for both groups
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑¬∑ Calculate weighted Gini impurity reduction for this split
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a825f39a5d40c52a1a8255495e984c53.png)'
  prefs: []
  type: TYPE_IMG
- en: The algorithm calculates weighted Gini impurity for each potential split and
    compares it to the parent node. For feature ‚Äúsunny‚Äù with split point 0.5, this
    impurity reduction (0.066) shows how much this split improves the data separation.
  prefs: []
  type: TYPE_NORMAL
- en: c. Pick the split that gives the largest Gini impurity reduction
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a4349476737e871ffda440a8e995b28.png)'
  prefs: []
  type: TYPE_IMG
- en: After checking all possible splits across features, the column ‚Äòovercast‚Äô (with
    split point 0.5) gives the highest impurity reduction of 0.102\. This means it‚Äôs
    the most effective way to separate the classes, making it the best choice for
    the first split.
  prefs: []
  type: TYPE_NORMAL
- en: d. Create a simple one-split tree using this decision
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9cf90d6e8fe547e552f0af06cc220b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the best split point found, the algorithm divides the data into two groups,
    each keeping their original weights. This simple decision tree is purposely kept
    small and imperfect, making it just slightly better than random guessing.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Evaluate how good this tree is
  prefs: []
  type: TYPE_NORMAL
- en: a. Use the tree to predict the label of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: b. Add up the weights of all **misclassified samples** to get error rate
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3dcfbdd4971811d91c3ba1fb5d763c3.png)'
  prefs: []
  type: TYPE_IMG
- en: The first weak learner makes predictions **on the training data**, and we check
    where it made mistakes (marked with X). The error rate of 0.357 shows this simple
    tree gets some predictions wrong, which is expected and will help guide the next
    steps of training.
  prefs: []
  type: TYPE_NORMAL
- en: 'c. Calculate tree importance (*Œ±*) using:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Œ±* = learning_rate √ó log((1-error)/error)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4a210b84e6605415fb8f6268044a9c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the error rate, we calculate the tree‚Äôs influence score (Œ± = 0.5878).
    Higher scores mean more accurate trees, and this tree earned moderate importance
    for its decent performance.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Update sample weights
  prefs: []
  type: TYPE_NORMAL
- en: a. Keep the original weights for correctly classified samples
  prefs: []
  type: TYPE_NORMAL
- en: b. Multiply the weights of misclassified samples by e^(*Œ±*).
  prefs: []
  type: TYPE_NORMAL
- en: c. Divide each weight by the sum of all weights. This normalization ensures
    all weights still sum to 1 while maintaining their relative proportions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4490670bf0192e1521f0f479a4c1873.png)'
  prefs: []
  type: TYPE_IMG
- en: Cases where the tree made mistakes (marked with X) get higher weights for the
    next round. After increasing these weights, all weights are normalized to sum
    to 1, ensuring misclassified examples get more attention in the next tree.
  prefs: []
  type: TYPE_NORMAL
- en: For the Second Tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Build a new stump, but now using the updated weights
  prefs: []
  type: TYPE_NORMAL
- en: 'a. Calculate new weighted Gini impurity for root node:'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑ Will be different because misclassified samples now have bigger weights
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑ Correctly classified samples now have smaller weights
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e820aa41f2204a8607d14793adc59d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the updated weights (where misclassified examples now have higher importance),
    the algorithm calculates the weighted Gini impurity at the root node. This begins
    the process of building the second decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'b. For each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑ Same process as before, but the weights have changed
  prefs: []
  type: TYPE_NORMAL
- en: c. Pick the split with best weighted Gini impurity reduction
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑ Often completely different from the first tree‚Äôs split
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑ Focuses on samples the first tree got wrong
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8380dccedc96e8aad94425ca3ab24b95.png)'
  prefs: []
  type: TYPE_IMG
- en: With updated weights, different split points show different effectiveness. Notice
    that ‚Äúovercast‚Äù is no longer the best split ‚Äî the algorithm now finds temperature
    (84.0) gives the highest impurity reduction, showing how weight changes affect
    split selection.
  prefs: []
  type: TYPE_NORMAL
- en: d. Create the second stump
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1d85397a88dd3805b964813e8a5fd14.png)'
  prefs: []
  type: TYPE_IMG
- en: Using temperature ‚â§ 84.0 as the split point, the algorithm assigns YES/NO to
    each leaf based on which class has more total weight in that group, not just by
    counting examples. This weighted voting helps correct the previous tree‚Äôs mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Evaluate this new tree
  prefs: []
  type: TYPE_NORMAL
- en: a. Calculate error rate with current weights
  prefs: []
  type: TYPE_NORMAL
- en: b. Calculate its importance (*Œ±*) using the same formula as before
  prefs: []
  type: TYPE_NORMAL
- en: '2.3\. Update weights again ‚Äî Same process: increase weights for mistakes then
    normalize.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0773fcc8ffd93058f55eebccc00b9c1f.png)'
  prefs: []
  type: TYPE_IMG
- en: The second tree achieves a lower error rate (0.222) and higher importance score
    (Œ± = 1.253) than the first tree. Like before, misclassified examples get higher
    weights for the next round.
  prefs: []
  type: TYPE_NORMAL
- en: For the Third Tree onwards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Repeat Step 2.1‚Äì2.3 for all remaining trees.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d19ad8eddbd4588dac4781b288276203.png)'
  prefs: []
  type: TYPE_IMG
- en: The algorithm builds 50 simple decision trees sequentially, each with its own
    importance score (Œ±). Each tree learns from previous mistakes by focusing on different
    aspects of the data, creating a strong combined model. Notice how some trees (like
    Tree 2) get higher importance scores when they perform better.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Final Ensemble** 3.1\. Keep all trees and their importance scores'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f0e5ae526c79b2e46766e4e85bfdb13.png)'
  prefs: []
  type: TYPE_IMG
- en: The 50 simple decision trees work together as a team, each with its own importance
    score (Œ±). When making predictions, trees with higher Œ± values (like Tree 2 with
    1.253) have more influence on the final decision than trees with lower scores.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c3c0699734afabfd3e5c095b1571e9ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Each node shows its ‚Äòvalue‚Äô parameter as [weight_NO, weight_YES], which represents
    the weighted proportion of each class at that node. These weights come from the
    sample weights we calculated during training.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For predicting:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Get each tree‚Äôs prediction
  prefs: []
  type: TYPE_NORMAL
- en: b. Multiply each by its importance score (*Œ±*)
  prefs: []
  type: TYPE_NORMAL
- en: c. Add them all up
  prefs: []
  type: TYPE_NORMAL
- en: d. The class with higher total weight will be the final prediction
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9a49e61ccbf0b28a73a1ace1a9fd700.png)'
  prefs: []
  type: TYPE_IMG
- en: When predicting for new data, each tree makes its prediction and multiplies
    it by its importance score (Œ±). The final decision comes from adding up all weighted
    votes ‚Äî here, the NO class gets a higher total score (23.315 vs 15.440), so the
    model predicts NO for this unseen example.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After building all the trees, we can evaluate the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e4cb581d6d058a44ad7f63b4a7dd494.png)'
  prefs: []
  type: TYPE_IMG
- en: By iteratively training and weighting weak learners to focus on misclassified
    examples, AdaBoost creates a strong classifier that achieves high accuracy ‚Äî typically
    better than single decision trees or simpler models!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Key Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are the key parameters for AdaBoost, particularly in `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`estimator`: This is the base model that AdaBoost uses to build its final solution.
    The 3 most common weak learners are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**a. Decision Tree with depth 1 (Decision Stump)**: This is the default and
    most popular choice. Because it only has one split, it is considered a very weak
    learner that is just a bit better than random guessing, exactly what is needed
    for boosting process.'
  prefs: []
  type: TYPE_NORMAL
- en: '**b. Logistic Regression**: Logistic regression (especially with high-penalty)
    can also be used here even though it is not really a weak learner. It could be
    useful for data that has linear relationship.'
  prefs: []
  type: TYPE_NORMAL
- en: '**c. Decision Trees with small depth (e.g., depth 2 or 3)**: These are slightly
    more complex than decision stumps. They‚Äôre still fairly simple, but can handle
    slightly more complex patterns than the decision stump.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72829a3081a99f628be3a36d2be8ad47.png)'
  prefs: []
  type: TYPE_IMG
- en: AdaBoost‚Äôs base models can be simple decision stumps (depth=1), small trees
    (depth 2‚Äì3), or penalized linear models. Each type is kept simple to avoid overfitting
    while offering different ways to capture patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: The number of weak learners to combine, typically around 50‚Äì100\.
    Using more than 100 rarely helps.'
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate`: Controls how much each classifier affects the final result.
    Common starting values are 0.1, 0.5, or 1.0\. Lower numbers (like 0.1) and a bit
    higher `n_estimator` usually work better.'
  prefs: []
  type: TYPE_NORMAL
- en: Key differences from Random Forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As both Random Forest and AdaBoost works with multiple trees, it is easy to
    confuse the parameters involved. The key difference is that Random Forest combines
    many trees **independently** (bagging) while AdaBoost builds trees **one after
    another** to fix mistakes (boosting). Here are some other details about their
    differences:'
  prefs: []
  type: TYPE_NORMAL
- en: No `bootstrap` parameter because AdaBoost uses all data but with changing weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No `oob_score` because AdaBoost doesn't use bootstrap sampling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`learning_rate` becomes crucial (not present in Random Forest)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tree depth is typically kept very shallow (usually just stumps) unlike Random
    Forest‚Äôs deeper trees
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The focus shifts from parallel independent trees to sequential dependent trees,
    making parameters like `n_jobs` less relevant
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pros & Cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Adaptive Learning:** AdaBoost gets better by giving more weight to mistakes
    it made. Each new tree pays more attention to the hard cases it got wrong.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resists Overfitting:** Even though it keeps adding more trees one by one,
    AdaBoost usually doesn‚Äôt get too focused on training data. This is because it
    uses weighted voting, so no single tree can control the final answer too much.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Built-in Feature Selection:** AdaBoost naturally finds which features matter
    most. Each simple tree picks the most useful feature for that round, which means
    it automatically selects important features as it trains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Sensitive to Noise:** Because it gives more weight to mistakes, AdaBoost
    can have trouble with messy or wrong data. If some training examples have wrong
    labels, it might focus too much on these bad examples, making the whole model
    worse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Must Be Sequential:** Unlike Random Forest which can train many trees at
    once, AdaBoost must train one tree at a time because each new tree needs to know
    how the previous trees did. This makes it slower to train.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning Rate Sensitivity:** While it has fewer settings to tune than Random
    Forest, the learning rate really affects how well it works. If it‚Äôs too high,
    it might learn the training data too exactly. If it‚Äôs too low, it needs many more
    trees to work well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AdaBoost is a key boosting algorithm that many newer methods learned from. Its
    main idea ‚Äî getting better by focusing on mistakes ‚Äî has helped shape many modern
    machine learning tools. While other methods try to be perfect from the start,
    AdaBoost tries to show that sometimes the best way to solve a problem is to learn
    from your errors and keep improving.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost also works best in binary classification problems and when your data
    is clean. While Random Forest might be better for more general tasks (like predicting
    numbers) or messy data, AdaBoost can give really good results when used in the
    right way. The fact that people still use it after so many years shows just how
    well the core idea works!
  prefs: []
  type: TYPE_NORMAL
- en: üåü AdaBoost Classifier Code Summarized
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of the [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)
    and its implementation in scikit-learn, readers can refer to the official documentation,
    which provides comprehensive information on its usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.6\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: 'ùôéùôöùôö ùô¢ùô§ùôßùôö ùôÄùô£ùô®ùôöùô¢ùôóùô°ùôö ùôáùôöùôñùôßùô£ùôûùô£ùôú ùôùùôöùôßùôö:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fc0f25326d7b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----fc0f25326d7b--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ùôîùô§ùô™ ùô¢ùôûùôúùôùùô© ùôñùô°ùô®ùô§ ùô°ùôûùô†ùôö:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fc0f25326d7b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----fc0f25326d7b--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  prefs: []
  type: TYPE_NORMAL
