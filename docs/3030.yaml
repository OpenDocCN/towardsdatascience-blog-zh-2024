- en: 100 Years of (eXplainable) AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/100-years-of-explainable-ai-2c7ecee2e51a?source=collection_archive---------0-----------------------#2024-12-18](https://towardsdatascience.com/100-years-of-explainable-ai-2c7ecee2e51a?source=collection_archive---------0-----------------------#2024-12-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Reflecting on advances and challenges in deep learning and explainability in
    the ever-evolving era of LLMs and AI governance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://slipnitskaya.medium.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)[![Sofya
    Lipnitskaya](../Images/9ea0dd0af32232eb4c8db0cb96f66449.png)](https://slipnitskaya.medium.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)
    [Sofya Lipnitskaya](https://slipnitskaya.medium.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)
    ·20 min read·Dec 18, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d4fa646c7204a553bb4945d735973b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine you are navigating a self-driving car, relying entirely on its onboard
    computer to make split-second decisions. It detects objects, identifies pedestrians,
    and even can anticipate behavior of other vehicles on the road. But here’s the
    catch: you know it works, of course, but you have no idea **how**. If something
    unexpected happens, there’s no clear way to understand the reasoning behind the
    outcome. This is where eXplainable AI (XAI) steps in. Deep learning models, often
    seen as “black boxes”, are increasingly used to leverage automated predictions
    and decision-making across domains. Explainability is all about opening up that
    box. We can think of it as a toolkit that helps us understand not only what these
    models do, but also **why** they make the decisions they do, ensuring these systems
    function as intended.'
  prefs: []
  type: TYPE_NORMAL
- en: The field of XAI has made significant strides in recent years, offering insights
    into model internal workings. As AI becomes integral to critical sectors, addressing
    responsibility aspects becomes essential for maintaining reliability and trust
    in such systems [[Göllner & a Tropmann-Frick, 2023](https://ceur-ws.org/Vol-3580/paper2.pdf),
    [Baker&Xiang, 2023](https://arxiv.org/abs/2312.01555)]. This is especially crucial
    for high-stakes applications like automotive, aerospace, and healthcare, where
    understanding model decisions ensures robustness, reliability, and safe real-time
    operations [[Sutthithatip et al., 2022](https://ieeexplore.ieee.org/document/9843612),
    [Borys et al., 2023,](https://www.sciencedirect.com/science/article/pii/S0720048X23001006)
    [Bello et al., 2024](https://www.arxiv.org/abs/2409.08666)]. Whether explaining
    why a medical scan was flagged as concerning for a specific patient or identifying
    factors contributing to model misclassification in bird detection for wind power
    risk assessments, XAI methods allow a peek inside the model’s reasoning process.
  prefs: []
  type: TYPE_NORMAL
- en: We often hear about boxes and their kinds in relation to models and transparency
    levels, but what does it really mean to have an explainable AI system? How does
    this apply to deep learning? And it’s not just about satisfying our curiosity.
    In this article, we are going to talk about XAI and why it’s having a bit of a
    moment in 2024 and beyond. We will explore how explainability has evolved over
    the past decades to reshape the landscape of computer vision, and vice versa (Section
    1). We will discuss what it took, starting from early research, for XAI to evolve
    into a practical, industry-spanning technology, as well as what its future may
    hold in light of global regulatory frameworks and responsible AI practices (Section
    2). Here, attention will be also given to a human-centered approach to explainability,
    where we will review stakeholder groups, their needs, and potential solutions
    to address ongoing challenges in fostering trust and ensuring the safe AI deployment
    (Section 3.1). Additionally, you will learn about commonly used XAI methods and
    examine metrics for evaluating how well these explanations work (Section 3.2).
    The final part (Section 4) will demonstrate how explainability can be effectively
    applied to image classification to enhance understanding and validate model decisions.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Back to the roots**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Over the past century, the field of deep learning and computer vision has witnessed
    significant breakthroughs that have not only shaped modern AI but have also contributed
    to the development and refinement of explainability methods and frameworks. Let’s
    take a moment to explore the key historical milestones in deep learning that brought
    us here, showcasing their impact on the evolution of XAI for vision (coverage:
    1920s — Present):'
  prefs: []
  type: TYPE_NORMAL
- en: '**1924:** Franz Breisig, a German mathematician, regards the explicit use of
    quadripoles in electronics as a “black box” — a notion used to refer to a system
    where only terminals are visible, with internal mechanisms hidden.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1943:** Warren McCulloch and Walter Pitts publish in their seminal work “A
    Logical Calculus of the Ideas Immanent in Nervous Activity” the McCulloch-Pitts
    (MCP) neuron, the first mathematical model of an artificial neuron, forming the
    basis of neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1949:** Donald O. Hebb, introduces a neuropsychological concept of Hebbian
    learning, explaining a basic mechanism for synaptic plasticity, suggesting that
    (brain) neural connections strengthen with use (cells that fire together, wire
    together), thus being able to be re-modelled via learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1950:** Alan Turing publishes “Computing Machinery and Intelligence”, presenting
    his groundbreaking idea of what came to be known as the Turing test for determining
    whether a machine can “think”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1958:** Frank Rosenblatt, an American psychologist, proposes perceptron,
    a first artificial neural network in his “The perceptron: A probabilistic model
    for information storage and organisation in the brain”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/c084ab5fe3b350dc88a1d62b2f4744bf.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1\. Rosenblatt’s perceptron schematic representation (Source:* [*Rosenblatt,
    1958*](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)*)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1962:** Frank Rosenblatt introduces the back-propagation error correction,
    a fundamental concept for computer learning, that inspired further DL works.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1963:** Mario Bunge, an Argentine-Canadian philosopher and physicist, publishes
    “A General Black Box Theory”, contributing to the development of black box theory
    and defining it as an abstraction that represents “a set of concrete systems into
    which stimuli S impinge and output of which reactions R emerge”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1967:** Shunichi Amari, a Japanese engineer and neuroscientist, pioneers
    the first multilayer perceptron trained with stochastic gradient descent for classifying
    non-linearly separable patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1969:** Kunihiko Fukushima, a Japanese computer scientist, introduces Rectified
    Linear Unit (ReLU), which has since become the most widely adopted activation
    function in deep learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1970:** Seppo Linnainmaa, a Finnish mathematician and computer scientist,
    proposes the “reverse mode of automatic differentiation” in his master’s thesis,
    a modern variant of backpropagation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1980:** Kunihiko Fukushima introduces Neocognitron, an early deep learning
    architecture for convolutional neural networks (CNNs), which does not use backpropagation
    for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1989:** Yann LeCun, a French-American computer scientist, presents LeNet,
    the first CNN architecture to successfully apply backpropagation for handwritten
    ZIP code recognition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1995:** Morch et al. introduce saliency maps, offering one of the first explainability
    approaches for unveiling internal workings of deep neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2000s:** Further advances including development of CUDA, enabling parallel
    processing on GPUs for high-performance scientific computing, alongside ImageNet,
    a large-scale manually curated visual dataset, pushing forward fundamental and
    applied AI research.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2010s:** Continued breakthroughs in computer vision, such as Krizhevsky,
    Sutskever, and Hinton’s deep convolutional network for ImageNet classification,
    drive widespread AI adoption across industries. The field of XAI flourishes with
    the emergence of CNN saliency maps, LIME, Grad-CAM, and SHAP, among others*.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/0622d464e98ab68457d714b48a30e33e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2\. ImageNet classification SOTA benchmark for vision models, 2014–2024
    (Source: PapersWithCode)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**2020s:** The AI boom gains momentum with the 2017 paper “Attention Is All
    You Need”, which introduces an encoder-decoder architecture, named Transformer,
    which catalyzes the development of more advanced transformer-based architectures.
    Building on early successes such as Allen AI’s ELMo, Google’s BERT, and OpenAI’s
    GPT, Transformer is applied across modalities and domains, including vision, accelerating
    progress in multimodal research. In 2021, OpenAI introduces CLIP, a model capable
    of learning visual concepts from natural language supervision, paving the way
    for generative AI innovations, including DALL-E, Vision Transformer (ViT), Stable
    Diffusion, and Sora, enhancing image and video generation capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2024:** The EU AI Act comes into effect, establishing legal requirements
    for AI systems in Europe, including mandates for transparency, reliability, and
    fairness. For example, [Recital 27](https://ai-act-law.eu/recital/) defines transparency
    for AI systems as: “developed and used in a way that allows appropriate traceability
    and explainability […] contributing to the design of coherent, trustworthy and
    human-centric AI”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we can see, early works primarily focused on foundational approaches and
    algorithms. with later advancements targeting specific domains, including computer
    vision. In the late 20th century, key concepts began to emerge, setting the stage
    for future breakthroughs like backpropagation-trained CNNs in the 1980s. Over
    time, the field of explainable AI has rapidly evolved, enhancing our understanding
    of reasoning behind prediction and enabling better-informed decisions through
    increased research and industry applications. As (X)AI gained traction, the focus
    shifted to balancing system efficiency with interpretability, aiding model understanding
    at scale and integrating XAI solutions throughout the ML lifecycle [[Bhatt et
    al., 2019](https://arxiv.org/abs/1909.06342), [Decker et al., 2023](https://link.springer.com/chapter/10.1007/978-3-031-35891-3_13)].
    Essentially, it is only in the past two decades that these technologies have become
    practical enough to result in widespread adoption. More lately, legislative measures
    and regulatory frameworks, such as the EU AI Act (August 2024) and China TC260’s
    AI Safety Governance Framework (September 2024), have emerged, [marking the start](https://artificialintelligenceact.eu/implementation-timeline/)
    of more stringent regulations for AI development and deployment, including the
    right enforcing “to obtain from the deployer clear and meaningful explanations
    of the role of the AI system in the decision-making procedure and the main elements
    of the decision taken” ([Article 86, 2026](https://artificialintelligenceact.eu/article/86/)).
    This is where XAI can prove itself at its best. Still, despite years of rigorous
    research and growing emphasis on explainability, the topic seems to have faded
    from the spotlight. Is that really the case? Now, let’s consider it all from a
    bird’s eye view.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. AI delight then and now
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today is an exciting time to be in the world of technology. In the 1990s, Gartner
    introduced something called the Hype cycle to describe how emerging technologies
    evolve over time — from the initial spark of interest to societal application.
    According to this methodology, technologies typically begin with innovation breakthroughs
    (referred to as the “Technology trigger”), followed by a steep rise in excitement,
    culminating at the “Peak of inflated expectations”. However, when the technology
    doesn’t deliver as expected, it plunges into the “Trough of disillusionment,”
    where enthusiasm wanes, and people become frustrated. The process can be described
    as a steep upward curve that eventually descends into a low point, before leveling
    off into a more gradual ascent, representing a sustainable plateau, the so-called
    “Plateau of productivity”. The latter implies that, over time, a technology can
    become genuinely productive, regardless of the diminished hype surrounding it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bcd77e527061f7e0de8a492739ea3500.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3\. Hype Cycle for Artificial Intelligence in 2024 (Source: Gartner)*'
  prefs: []
  type: TYPE_NORMAL
- en: Look at previous technologies that were supposed to solve everything — intelligent
    agents, cloud computing, blockchain, brain-computer interfaces, big data, and
    even deep learning. They all came up to have fantastic places in the tech world,
    but, of course, none of them became a silver bullet. Similar goes with the explainability
    topic now. And we can see over and over that history repeats itself. As highlighted
    by the Gartner Hype Cycle for AI 2024 (Fig. 3), **Responsible AI** (RAI) is gaining
    prominence (top left) and is expected to reach maturity within the next five years.
    Explainability provides a foundation for RAI practices, promoting transparency,
    accountability, and fairness in AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Figure below overviews XAI research trends and applications, derived from scientific
    literatures published between 2018 and 2022 to cover various concepts within the
    XAI field, including “explainable artificial intelligence”, “interpretable artificial
    intelligence”, and “responsible artificial intelligence”[[Clement et al., 2023](https://www.mdpi.com/2504-4990/5/1/6)].
    Figure 4a outlines key XAI research areas based on the meta-review results. The
    largest focus (44%) is on designing explainability methods, followed by 15% on
    XAI applications across specific use cases. Domain-dependent studies (e.g., finance)
    account for 12%, with smaller areas — requirements analysis, data types, and human-computer
    interaction — each making up around 5–6%.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fce491a1a2f8d354a4114eb5fd596907.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4\. XAI research perspectives (a) and application domains (b) (Source:*
    [*Clement et al., 2023*](https://www.mdpi.com/2504-4990/5/1/6)*)*'
  prefs: []
  type: TYPE_NORMAL
- en: Next to it are common application fields (Fig. 4b), with healthcare leading
    (23%), driven by the need for trust-building and decision-making support. Industry
    4.0 (6%) and security (4%) follow, where explainability is applied to industrial
    optimization and fraud detection. Other fields include natural sciences, legal
    studies, robotics, autonomous driving, education, and social sciences [[Atakishiyev
    et al., 2024](https://ieeexplore.ieee.org/document/10604830), [Chen et al., 2023](https://www.nature.com/articles/s41551-023-01056-8),
    [Loh et al., 2022](https://www.sciencedirect.com/science/article/abs/pii/S0169260722005429)].
    As XAI progresses toward a sustainable state, research and developments become
    increasingly focused on addressing fairness, transparency, and accountability
    [[Arrieta et al., 2020](https://dl.acm.org/doi/10.1016/j.inffus.2019.12.012),
    [Responsible AI Institute Standards](https://www.responsible.ai/ai-standards-deep-dive-decoding-different-ai-standards-and-the-eus-approach/),
    [Stanford AI Index Report](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter3.pdf)].
    These dimensions are crucial for ensuring equitable outcome, clarifying decision-making
    processes, and establishing responsibility for those decisions, thereby fostering
    user confidence, and aligning with regulatory frameworks and industry standards.
    Reflecting the trajectory of past technological advances, the rise of XAI highlights
    both the challenges and opportunities for building AI-driven solutions, establishing
    it as an important element in responsible AI practices, enhancing AI’s long-term
    relevance in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Putting **the spotlight back on XAI**
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 3.1\. Why and when model understanding — Explainability 101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is a common perception of AI systems: You put data in, and then, there
    is a black box processing it, producing an output, but we cannot examine the system’s
    internal workings. But is that really the case? As AI continues to proliferate,
    the development of reliable, scalable, and transparent systems becomes increasingly
    vital. Put simply: the idea of explainable AI can be described as doing something
    to provide a clearer **understanding** of what happens between the input and output.
    In a broad sense, one can think about it as a collection of methods allowing us
    to build systems capable of delivering desirable results. Practically, model understanding
    can be defined as the capacity to generate explanations of the model’s behaviour
    that users can comprehend. This understanding is crucial in a variety of use cases
    across industries, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Model debugging and quality assurance (e.g., manufacturing, robotics);
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring system trustability for end-users (medicine, finance);
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving system performance by identifying scenarios where the model is likely
    to fail (fraud detection in banking, e-commerce);
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing system robustness against adversaries (cybersecurity, autonomous vehicles);
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining decision-making processes (finance for credit scoring, legal for
    judicial decisions);
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting data mislabelling and other issues (customer behavior analysis in
    retail, medical imaging in healthcare).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The growing adoption of AI has led to its widespread use across domains and
    risk applications. What is comes down to is that human understanding is not the
    same as model understanding. While AI models process information in ways that
    are not inherently intuitive to humans, one of the primary objectives of XAI is
    to create systems that effectively communicate their reasoning — in other words,
    to “speak” in terms that are accessible and meaningful to end users. So, the question
    then becomes: how can we bridge that gap between what a model “knows” and how
    humans comprehend its outputs?'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Who is it for — Stakeholders desiderata on XAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Explainable AI is not just about interpreting models but enabling machines to
    effectively support humans by transferring knowledge. To address these aspects,
    one can think on how explainability can be tied to expectations of diverse personas
    and stakeholders involved in AI ecosystems. These groups usually include users,
    developers, deployers, affected parties, and regulators [[Leluschko&Tholen,2023](https://www.dfki.de/fileadmin/user_upload/import/14563_Goals_and_Stakeholder_Involvement_in_XAI_for_Remote_Sensing_A_Structured_Literature_Review_-_978-3-031-47994-6_47.pdf),
    [Sadek et al., 2024](https://link.springer.com/article/10.1007/s00146-024-01880-9)].
    Accordingly, their desiderata — i.e. features and results they expect from AI
    — also vary widely, suggesting that explainability needs to cater to a wide array
    of needs and challenges. In the study, [Langer et al., 2021](https://arxiv.org/pdf/2102.07817)
    highlight that understanding plays a critical role in addressing the epistemic
    facet, referring to stakeholders’ ability to assess whether a system meets their
    expectations, such as fairness or transparency. For instance, in high-stakes scenarios
    like medical diagnosis, the depth of explanations required for trust calibration
    might be greater [[Saraswat et al., 2022](https://ieeexplore.ieee.org/abstract/document/9852458)].
    Figure 5 presents a conceptual model that outlines the pathway from explainability
    approaches to fulfilling stakeholders’ needs, which, in turn, affects how well
    their desiderata are met. But what constitutes a “good” explanation? The study
    argues that it should be not only accurate, representative, and context-specific
    with respect to a system and its functioning, but also align with socio-ethical
    and legal considerations, which can be decisive in justifying certain desiderata.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2baa045787201bf9a6cb1b9012029e65.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5: Relation of explainability with stakeholders’ desiderata (Source:*
    [*Langer et al., 2021*](https://arxiv.org/pdf/2102.07817))'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can say that the success of XAI as technology hinges on how effectively
    it facilitates human understanding through explanatory information, emphasizing
    the need for careful navigation of trade-offs among stakeholders. For instance,
    for **domain experts and users** (e.g., doctors, judges), who deal with interpreting
    and auditing AI system outputs for decision-making, it is important to ensure
    explainability results are concise and domain-specific to align them with expert
    intuition, while not creating information overload, which is especially relevant
    for human-in-the-loop applications. Here, the challenge may arise due to uncertainty
    and the lack of clear causality between inputs and outputs, which can be addressed
    through local post-hoc explanations tailored to specific use cases [[Metta et
    al., 2024](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11048122/)]. **Affected
    parties** (e.g., job applicants, patients) are individuals impacted by AI’s decisions,
    with fairness and ethics being key concerns, especially in contexts like hiring
    or healthcare. Here, explainability approaches can aid in identifying factors
    contributing to biases in decision-making processes, allowing for their mitigation
    or, at the very least, acknowledgment and elimination [[Dimanov et al., 2020](https://mlg.eng.cam.ac.uk/adrian/ECAI20-You_Shouldn%E2%80%99t_Trust_Me.pdf)].
    Similarly, **regulators** may seek to determine whether a system is biassed toward
    any group to ensure compliance with ethical and regulatory standards, with a particular
    focus on transparency, traceability, and non-discrimination in high-risk applications
    [[Gasser & Almeida, 2017](https://dash.harvard.edu/handle/1/34390353), [Floridi
    et al., 2018](https://link.springer.com/article/10.1007/s11023-018-9482-5), [The
    EU AI Act 2024](https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f130b8164835537c7ade7eb898b95ed.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6\. Explainability in the ML lifecycle process (Source:* [*Decker et
    al., 2023*](https://link.springer.com/chapter/10.1007/978-3-031-35891-3_13)*)*'
  prefs: []
  type: TYPE_NORMAL
- en: For **businesses and organisations** adopting AI, the challenge may lie in ensuring
    responsible implementation in line with regulations and industry standards, while
    also maintaining user trust [[Ali et al., 2023](https://www.sciencedirect.com/science/article/pii/S1566253523001148),
    [](https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/Microsoft-Responsible-AI-Standard-General-Requirements.pdf)
    [Saeed & Omlin, 2021](https://arxiv.org/pdf/2111.06420)]. In this context, using
    global explanations and incorporating XAI into the ML lifecycle (Figure 6), can
    be particularly effective [[Saeed & Omlin, 2021](https://arxiv.org/pdf/2111.06420),
    [Microsoft Responsible AI Standard v2 General Requirements](https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/Microsoft-Responsible-AI-Standard-General-Requirements.pdf),
    [Google Responsible AI Principles](https://ai.google/responsibility/principles/)].
    Overall, both regulators and deployers aim to understand the entire system to
    minimize implausible corner cases. When it comes to **practitioners** (e.g., developers
    and researchers), who build and maintain AI systems, these can be interested in
    leveraging XAI tools for diagnosing and improving model performance, along with
    advancing existing solutions with interpretability interface that can provide
    details about model’s reasoning [[Bhatt et al., 2020](https://dl.acm.org/doi/abs/10.1145/3351095.3375624)].
    However, these can come with high computational costs, making large-scale deployment
    challenging. Here, the XAI development stack can include both open-source and
    proprietary toolkits, frameworks, and libraries, such as [PyTorch Captum](https://github.com/pytorch/captum),
    [Google Model Card Toolkit](https://github.com/tensorflow/model-card-toolkit),
    [Microsoft Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox#responsible-ai-toolbox),
    [IBM AI Fairness 360](https://github.com/Trusted-AI/AIF360), for ensuring that
    systems built are safe, reliable, and trustworthy from development through deployment
    and beyond.
  prefs: []
  type: TYPE_NORMAL
- en: And as we can see — one size does not fit all. One of the ongoing challenges
    is to provide explanations that are both accurate and meaningful for different
    stakeholders while balancing transparency and usability in real-world applications
    [[Islam et al., 2022](https://link.springer.com/chapter/10.1007/978-3-030-96630-0_1),
    [Tate et al., 2023](https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2023.1117848/full),
    [Hutsen, 2023](https://www.mdpi.com/2673-2688/4/3/34)]. Now, let’s talk about
    XAI in a more practical sense.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Model explainability for vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 4.1\. Feature attribution methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As AI systems have advanced, modern approaches have demonstrated substantial
    improvements in performance on complex tasks, such as image classification (Fig.
    2), surpassing earlier image processing techniques that relied heavily on handcrafted
    algorithms for visual feature extraction and detection [[Sobel and Feldman, 1973](https://www.researchgate.net/publication/281104656_An_Isotropic_3x3_Image_Gradient_Operator),
    [Canny, 1987](https://www.sciencedirect.com/science/article/abs/pii/B9780080515816500246)].
    While modern deep learning architectures are not inherently interpretable, various
    solutions have been devised to provide explanations on model behavior for given
    inputs, allowing to bridge the gap between human (understanding) and machine (processes).
    Following the breakthroughs in deep learning, various XAI approaches have emerged
    to enhance explainability aspects in the domain of computer vision. Focusing on
    image classification and object detection applications, the Figure 7 below outlines
    several commonly used XAI methods developed over the past decades:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f4bad26a548ead967cd25deed0e3d70.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7\. Explainability methods for computer vision (Image by author)*'
  prefs: []
  type: TYPE_NORMAL
- en: XAI methods can be broadly categorized based on their methodology into backpropagation-
    and perturbation-based methods, while the explanation scope is either local or
    global. In computer vision, these methods or combinations of them are used to
    uncover the decision criteria behind model predictions. **Backpropagation-based**
    approaches propagate a signal from the output to the input, assigning weights
    to each intermediate value computed during the forward pass. A gradient function
    then updates each parameter at the model to align the output with the ground truth,
    making these techniques also known as gradient-based methods. Examples include
    saliency maps [[Simonyan et al., 201](https://arxiv.org/abs/1312.6034)3], integrated
    gradient [[Sundararajan et al., 2017](https://arxiv.org/pdf/1703.01365)], Grad-CAM
    [[Selvaraju et al, 2017](https://ieeexplore.ieee.org/document/8237336)]. In contrast,
    **perturbation-based** methods modify the input through techniques like occlusion
    [[Zeiler & Fergus, 2014](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53)],
    LIME [[Ribeiro et al., 2016](https://dl.acm.org/doi/10.1145/2939672.2939778)],
    RISE [[Petsiuk et al., 2018](https://arxiv.org/abs/1806.07421)], evaluating how
    these slight changes impact the network output. Unlike backpropagation-based methods,
    perturbation techniques don’t require gradients, as a single forward pass is sufficient
    to assess how the input changes influence the output.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability for “black box” architectures is typically achieved through external
    post-hoc methods after the model has been trained (e.g., gradients for CNN). In
    contrast, “white-box” architectures are interpretable by design, where explainability
    can be achieved as a byproduct of the model training. For example, in linear regression,
    coefficients derived from solving a system of linear equations can be used directly
    to assign weights to input features. However, while feature importance is straightforward
    in the case of linear regression, more complex tasks and advanced architectures
    consider highly non-linear relationships between inputs and outputs, thus requiring
    external explainability methods to understand and validate which features have
    the greatest influence on predictions. That being said, using linear regression
    for computer vision isn’t a viable approach.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Evaluation metrics for XAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluating explanations is essential to ensure that the insights derived from
    the model and their presentation to end-users — through the explainability interface
    — are meaningful, useful, and trustworthy [[Ali et al., 2023](https://www.sciencedirect.com/science/article/pii/S1566253523001148)].
    The increasing variety of XAI methods necessitates systematic evaluation and comparison,
    shifting away from subjective “I know it when I see it” approaches[.](https://arxiv.org/pdf/1702.08608)
    To address this challenge, researchers have devised numerous algorithmic and user-based
    evaluation techniques, along with frameworks and taxonomies, to capture both subjective
    and objective quantitative and qualitative properties of explanations [[Doshi-Velez
    & Kim, 2017,](https://arxiv.org/pdf/1702.08608) [Sokol & Flach, 2020](https://dl.acm.org/doi/10.1145/3351095.3372870)].
    Explainability is a spectrum, not a binary characteristic, and its effectiveness
    can be quantified by assessing the extent to which certain properties are to be
    fulfilled. One of the ways to categorize XAI evaluation methods is along the so-called
    Co-12 properties [[Nauta et al., 2023](https://dl.acm.org/doi/10.1145/3583558)],
    grouped by content, presentation, and user dimensions, as summarized in Table
    1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3565cd39b1546752e472f4244dfa3a81.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 1\. Co-12 explanation quality properties for evaluation (Source:* [*Nauta
    et al., 2023*](https://dl.acm.org/doi/10.1145/3583558)*)*'
  prefs: []
  type: TYPE_NORMAL
- en: At a more granular level, quantitative evaluation methods for XAI can incorporate
    metrics, such as faithfulness, stability, fidelity, and explicitness [[Alvarez-Melis
    & Jaakkola, 2018](https://proceedings.neurips.cc/paper_files/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf),
    [Agarwal et al., 2022](https://openreview.net/pdf?id=BfxZAuWOg9), [Kadir et al.,
    2023](https://ieeexplore.ieee.org/document/10297629)], enabling the measurement
    of the intrinsic quality of explanations. For instance, **faithfulness** measures
    how well the explanation aligns with the model’s behavior, focusing on the importance
    of selected features for the target class prediction. [Qi et al., 2020](https://openaccess.thecvf.com/content_CVPRW_2019/papers/Explainable%20AI/Qi_Visualizing_Deep_Networks_by_Optimizing_with_Integrated_Gradients_CVPRW_2019_paper.pdf)
    demonstrated a method for feature importance analysis with Integrated Gradients,
    emphasizing the importance of producing faithful representations of model behavior.
    **Stability** refers to the consistency of explanations across similar inputs.
    A study by [Ribeiro et al., 2016](https://dl.acm.org/doi/10.1145/2939672.2939778)
    on LIME highlights the importance of stability in generating reliable explanations
    that do not vary drastically with slight input changes. **Fidelity** reflects
    how accurately an explanation reflects the model’s decision-making process. [Doshi-Velez
    & Kim, 2017](https://arxiv.org/pdf/1702.08608) emphasize fidelity in their framework
    for interpretable machine learning, arguing that high fidelity is essential for
    trustworthy AI systems. **Explicitness** involves how easily a human can understand
    the explanation. [Alvarez-Melis & Jaakkola, 2018](https://proceedings.neurips.cc/paper_files/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf)
    discussed robustness in interpretability through self-explaining neural networks
    (SENN), which strive for explicitness alongside stability and faithfulness.
  prefs: []
  type: TYPE_NORMAL
- en: 'To link the concepts, the correctness property, as described in Table 1, refers
    to the faithfulness of the explanation in relation to the model being explained,
    indicating how truthful the explanation reflects the “true” behavior of the black
    box. This property is distinct from the model’s predictive accuracy, but rather
    descriptive to the XAI method with respect to the model’s functioning [[Naute
    et al., 2023](https://dl.acm.org/doi/10.1145/3583558), [Sokol & Vogt, 2024](https://dl.acm.org/doi/abs/10.1145/3613905.3651047)].
    Ideally, an explanation is “nothing but the truth”, so high correctness is therefore
    desired. The faithfulness via deletion score can be obtained by calculating normalized
    area under the curve representing the difference between two feature importance
    functions: the one built by gradually removing features (starting with the Least
    Relevant First — LeRF) and evaluating the model performance at every step, and
    another one, for which the deletion order is random (Random Order — RaO) [[Won
    et al., 2023](https://arxiv.org/pdf/2303.14608)]. Computing points for both types
    of curves starts with providing the full image to the model and continues with
    a gradual removal of pixels, whose importance, assigned by an attribution method,
    lies below a certain threshold. A higher score implies that the model has a better
    ability to retain important information even when redundant features are deleted
    (Eq. 1).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3836ab2a4e7daab43cf9f74790f28cc3.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Equation 1\. Faithfulness metric computation for feature importance assessment
    via deletion*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach for evaluating faithfulness is to compute feature importance
    via insertion, similar to the method described above, but by gradually showing
    the model the most relevant image regions as identified by the attribution method.
    The key idea here: include important features and see what happens. In the demo,
    we will explore both qualitative and quantitative approaches for evaluating model
    explanations.'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Feature importance for fine-grained classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In fine-grained classification tasks, such as distinguishing between different
    vehicle types or identifying bird species, small variations in visual appearance
    can significantly affect model predictions. Determining which features are most
    important for the model’s decision-making process can help to shed light on misclassification
    issues, thus allowing to optimize the model on the task. To demonstrate how explainability
    can be effectively applied to leverage understanding on deep learning models for
    vision, we will consider a use case of bird classification. Bird populations are
    important biodiversity indicators, so collecting reliable data of species and
    their interactions across environmental contexts is quite important to ecologists
    [[Atanbori et al., 2016](https://dl.acm.org/doi/10.1016/j.patrec.2015.08.015)].
    In addition, automated bird monitoring systems can also benefit windfarm producers,
    since the construction requires preliminary collision risk assessment and mitigation
    at the design stages [[Croll et al., 2022](https://www.sciencedirect.com/science/article/pii/S0006320722003482)].
    This part will showcase how to apply XAI methods and metrics to enhance model
    explainability in bird species classification (more on the topic can be found
    in the related [article](/bird-by-bird-using-deep-learning-4c0fa81365d7) and [tutorials](https://github.com/slipnitskaya/computer-vision-birds)).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8 below presents the feature importance analysis results for fine-grained
    image classification using ResNet-50 pretrained on ImageNet and fine-tuned on
    the Caltech-UCSD Birds-200–2011 dataset. The qualitative assessment of faithfulness
    was conducted for the Guided Grad-CAM method to evaluate the significance of the
    selected features given the model. Quantitative XAI metrics included faithfulness
    via deletion (FTHN), with higher values indicating better faithfulness, alongside
    metrics that reflect the degree of non-robustness and instability, such as maximum
    sensitivity (SENS) and infidelity (INFD), where lower values are preferred. The
    latter metrics are perturbation-based and rely on the assumption that explanations
    should remain consistent with small changes in input data or the model itself
    [[Yeh et al., 2019](https://proceedings.neurips.cc/paper_files/paper/2019/file/a7471fdc77b3435276507cc8f2dc2569-Paper.pdf)].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/329204b9401f7128998f0a56ba6b0305.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8\. Evaluating explainability metrics for fine-grained image classification
    (Image by author)*'
  prefs: []
  type: TYPE_NORMAL
- en: When evaluating our model on an independent test image of [Northern Cardinal](https://www.allaboutbirds.org/guide/Northern_Cardinal/photo-gallery/63667291),
    we notice that slight changes in the model’s scores during the initial iterations
    are followed by a sharp increase toward the final iteration as the most critical
    features are progressively incorporated (Fig. 8). These results suggest two key
    interpretations regarding the model’s faithfulness with respect to the evaluated
    XAI methods. Firstly, attribution-based interpretability using Guided Grad-CAM
    is faithful to the model, as adding regions identified as redundant (90% of LeRF,
    axis-x) caused minimal changes in the model’s score (less than 0.1 predicted probability
    score). This implies that the model did not rely on these regions when making
    predictions, in contrast to the remaining top 10% of the most relevant features
    identified. Another category — robustness — refers to the model resilience to
    small input variations. Here, we can see that changes in around 90% of the original
    image had little impact on the overall model’s performance, maintaining the target
    probability score despite changes to the majority of pixels, suggesting its stability
    and generalization capabilities for the target class prediction.
  prefs: []
  type: TYPE_NORMAL
- en: To further assess the robustness of our model, we compute additional metrics,
    such as sensitivity and infidelity [[Yeh et al., 2019](https://proceedings.neurips.cc/paper_files/paper/2019/file/a7471fdc77b3435276507cc8f2dc2569-Paper.pdf)].
    Results indicate that while the model is not overly sensitive to slight perturbations
    in the input (SENS=0.21), the alterations to the top-important regions may potentially
    have an influence on model decisions, in particular, for the top-10% (Fig. 8).
    To perform a more in-depth assessment of the sensitivity of the explanations for
    our model, we can further extend the list of explainability methods, for instance,
    using Integrated Gradients and SHAP [[Lundberg & Lee, 2017](https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html)].
    In addition, to assess model resistance to adversarial attacks, the next steps
    may include quantifying further robustness metrics [[Goodfellow et al., 2015](https://www.semanticscholar.org/reader/bee044c8e8903fb67523c1f8c105ab4718600cdb),
    [Dong et al., 2023](https://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_Benchmarking_Adversarial_Robustness_on_Image_Classification_CVPR_2020_paper.pdf)].
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article provides a comprehensive overview of scientific literature published
    over past decades encompassing key milestones in deep learning and computer vision
    that laid the foundation of the research in the field of XAI. Reflecting on recent
    technological advances and perspectives in the field, we discussed potential implications
    of XAI in light of emerging AI regulatory frameworks and responsible AI practices,
    anticipating the increased relevance of explainability in the future. Furthermore,
    we examined application domains and explored stakeholders’ groups and their desiderata
    to provide practical suggestions on how XAI can address current challenges and
    needs for creating reliable and trustworthy AI systems. We have also covered fundamental
    concepts and taxonomies related to explainability, commonly used methods and approaches
    used for vision, along with qualitative and quantitative metrics to evaluate post-hoc
    explanations. Finally, to demonstrate how explainability can be applied to leverage
    understanding on deep learning models, the last section presented a case in which
    XAI methods and metrics were effectively applied to a fine-grained classification
    task to identify relevant features affecting model decisions and to perform quantitative
    and qualitative assessment of results to validate quality of the derived explanations
    with respect to model reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: '**What’s next?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the upcoming article, we will further explore the topic of explainability
    and its practical applications, focusing on how to leverage XAI in design for
    optimizing model performance and reducing classification errors. Interested to
    keep it on? Stay updated on more materials at — [https://github.com/slipnitskaya/computer-vision-birds](https://github.com/slipnitskaya/computer-vision-birds)
    and [https://medium.com/@slipnitskaya](https://medium.com/@slipnitskaya).
  prefs: []
  type: TYPE_NORMAL
