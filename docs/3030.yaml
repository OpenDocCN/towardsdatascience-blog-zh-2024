- en: 100 Years of (eXplainable) AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 100年的（可解释）人工智能
- en: 原文：[https://towardsdatascience.com/100-years-of-explainable-ai-2c7ecee2e51a?source=collection_archive---------0-----------------------#2024-12-18](https://towardsdatascience.com/100-years-of-explainable-ai-2c7ecee2e51a?source=collection_archive---------0-----------------------#2024-12-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/100-years-of-explainable-ai-2c7ecee2e51a?source=collection_archive---------0-----------------------#2024-12-18](https://towardsdatascience.com/100-years-of-explainable-ai-2c7ecee2e51a?source=collection_archive---------0-----------------------#2024-12-18)
- en: Reflecting on advances and challenges in deep learning and explainability in
    the ever-evolving era of LLMs and AI governance
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反思在不断发展的LLM和人工智能治理时代中，深度学习与可解释性方面的进展与挑战
- en: '[](https://slipnitskaya.medium.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)[![Sofya
    Lipnitskaya](../Images/9ea0dd0af32232eb4c8db0cb96f66449.png)](https://slipnitskaya.medium.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)
    [Sofya Lipnitskaya](https://slipnitskaya.medium.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://slipnitskaya.medium.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)[![Sofya
    Lipnitskaya](../Images/9ea0dd0af32232eb4c8db0cb96f66449.png)](https://slipnitskaya.medium.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)
    [Sofya Lipnitskaya](https://slipnitskaya.medium.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)
    ·20 min read·Dec 18, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2c7ecee2e51a--------------------------------)
    ·20分钟阅读·2024年12月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6d4fa646c7204a553bb4945d735973b7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d4fa646c7204a553bb4945d735973b7.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Background
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'Imagine you are navigating a self-driving car, relying entirely on its onboard
    computer to make split-second decisions. It detects objects, identifies pedestrians,
    and even can anticipate behavior of other vehicles on the road. But here’s the
    catch: you know it works, of course, but you have no idea **how**. If something
    unexpected happens, there’s no clear way to understand the reasoning behind the
    outcome. This is where eXplainable AI (XAI) steps in. Deep learning models, often
    seen as “black boxes”, are increasingly used to leverage automated predictions
    and decision-making across domains. Explainability is all about opening up that
    box. We can think of it as a toolkit that helps us understand not only what these
    models do, but also **why** they make the decisions they do, ensuring these systems
    function as intended.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你正在驾驶一辆自动驾驶汽车，完全依赖它的车载计算机做出瞬间的决策。它能够检测物体、识别行人，甚至能够预测其他车辆的行为。但是，有一个问题：你知道它有效，但你完全不知道**如何**实现的。如果发生了意外情况，你无法清楚地了解结果背后的推理过程。这就是可解释人工智能（XAI）发挥作用的地方。深度学习模型，常常被视为“黑盒”，如今在各个领域被广泛应用于自动化预测和决策。可解释性就是要打开那个黑盒。我们可以把它看作是一个工具包，它不仅帮助我们理解这些模型做了什么，还帮助我们理解**为什么**它们做出这些决策，从而确保这些系统按预期工作。
- en: The field of XAI has made significant strides in recent years, offering insights
    into model internal workings. As AI becomes integral to critical sectors, addressing
    responsibility aspects becomes essential for maintaining reliability and trust
    in such systems [[Göllner & a Tropmann-Frick, 2023](https://ceur-ws.org/Vol-3580/paper2.pdf),
    [Baker&Xiang, 2023](https://arxiv.org/abs/2312.01555)]. This is especially crucial
    for high-stakes applications like automotive, aerospace, and healthcare, where
    understanding model decisions ensures robustness, reliability, and safe real-time
    operations [[Sutthithatip et al., 2022](https://ieeexplore.ieee.org/document/9843612),
    [Borys et al., 2023,](https://www.sciencedirect.com/science/article/pii/S0720048X23001006)
    [Bello et al., 2024](https://www.arxiv.org/abs/2409.08666)]. Whether explaining
    why a medical scan was flagged as concerning for a specific patient or identifying
    factors contributing to model misclassification in bird detection for wind power
    risk assessments, XAI methods allow a peek inside the model’s reasoning process.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: XAI领域近年来取得了显著进展，提供了关于模型内部工作机制的深入见解。随着人工智能成为关键领域的核心，处理责任问题变得至关重要，这对于保持系统的可靠性和信任至关重要
    [[Göllner & a Tropmann-Frick, 2023](https://ceur-ws.org/Vol-3580/paper2.pdf),
    [Baker&Xiang, 2023](https://arxiv.org/abs/2312.01555)]。这对于高风险应用尤为重要，如汽车、航空航天和医疗保健等领域，在这些领域中，理解模型决策能够确保系统的稳健性、可靠性以及安全的实时操作
    [[Sutthithatip et al., 2022](https://ieeexplore.ieee.org/document/9843612), [Borys
    et al., 2023,](https://www.sciencedirect.com/science/article/pii/S0720048X23001006)
    [Bello et al., 2024](https://www.arxiv.org/abs/2409.08666)]。无论是解释为何一份医疗扫描在特定患者中被标记为值得关注，还是识别风力发电风险评估中鸟类检测模型误分类的因素，XAI方法都能让我们窥见模型的推理过程。
- en: We often hear about boxes and their kinds in relation to models and transparency
    levels, but what does it really mean to have an explainable AI system? How does
    this apply to deep learning? And it’s not just about satisfying our curiosity.
    In this article, we are going to talk about XAI and why it’s having a bit of a
    moment in 2024 and beyond. We will explore how explainability has evolved over
    the past decades to reshape the landscape of computer vision, and vice versa (Section
    1). We will discuss what it took, starting from early research, for XAI to evolve
    into a practical, industry-spanning technology, as well as what its future may
    hold in light of global regulatory frameworks and responsible AI practices (Section
    2). Here, attention will be also given to a human-centered approach to explainability,
    where we will review stakeholder groups, their needs, and potential solutions
    to address ongoing challenges in fostering trust and ensuring the safe AI deployment
    (Section 3.1). Additionally, you will learn about commonly used XAI methods and
    examine metrics for evaluating how well these explanations work (Section 3.2).
    The final part (Section 4) will demonstrate how explainability can be effectively
    applied to image classification to enhance understanding and validate model decisions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常听到关于与模型和透明度等级相关的“盒子”及其种类，但拥有一个可解释的AI系统到底意味着什么呢？这如何应用于深度学习？而且，这不仅仅是满足我们的好奇心。本文将讨论XAI以及为什么它在2024年及以后会成为一个热点话题。我们将探讨可解释性如何在过去几十年中发展，并如何重塑计算机视觉的格局，反之亦然（第1节）。我们将讨论从早期研究开始，XAI是如何发展成为一项涵盖整个行业的实用技术，以及在全球监管框架和负责任的AI实践背景下，它的未来可能会如何（第2节）。在这里，我们还将关注以人为中心的可解释性方法，我们将回顾利益相关者群体、他们的需求以及可能的解决方案，以应对在促进信任和确保安全AI部署过程中持续存在的挑战（第3.1节）。此外，您将了解常用的XAI方法，并检查评估这些解释效果的指标（第3.2节）。最后一部分（第4节）将展示如何将可解释性有效应用于图像分类，以增强对模型决策的理解并验证其正确性。
- en: '**1\. Back to the roots**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**1\. 回归根本**'
- en: 'Over the past century, the field of deep learning and computer vision has witnessed
    significant breakthroughs that have not only shaped modern AI but have also contributed
    to the development and refinement of explainability methods and frameworks. Let’s
    take a moment to explore the key historical milestones in deep learning that brought
    us here, showcasing their impact on the evolution of XAI for vision (coverage:
    1920s — Present):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的一个世纪里，深度学习和计算机视觉领域经历了重大的突破，这些突破不仅塑造了现代人工智能，还推动了解释性方法和框架的发展与完善。让我们花一点时间回顾深度学习历史中的关键里程碑，这些里程碑促成了我们今天的成就，并展示了它们对视觉领域XAI（可解释人工智能）发展的影响（涵盖：1920年代
    — 现在）：
- en: '**1924:** Franz Breisig, a German mathematician, regards the explicit use of
    quadripoles in electronics as a “black box” — a notion used to refer to a system
    where only terminals are visible, with internal mechanisms hidden.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1924年：** 德国数学家弗朗茨·布赖西格（Franz Breisig）将电子学中四端子网络的明确使用视为“黑箱”——这个概念用于指代仅可见端口的系统，其内部机制被隐藏。'
- en: '**1943:** Warren McCulloch and Walter Pitts publish in their seminal work “A
    Logical Calculus of the Ideas Immanent in Nervous Activity” the McCulloch-Pitts
    (MCP) neuron, the first mathematical model of an artificial neuron, forming the
    basis of neural networks.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1943年：** 沃伦·麦卡洛克（Warren McCulloch）和沃尔特·皮茨（Walter Pitts）在其开创性工作《神经活动中固有思想的逻辑演算》中提出了麦卡洛克-皮茨（MCP）神经元，这是第一个人工神经元的数学模型，为神经网络的基础奠定了基础。'
- en: '**1949:** Donald O. Hebb, introduces a neuropsychological concept of Hebbian
    learning, explaining a basic mechanism for synaptic plasticity, suggesting that
    (brain) neural connections strengthen with use (cells that fire together, wire
    together), thus being able to be re-modelled via learning.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1949年：** 唐纳德·O·赫布（Donald O. Hebb）提出了赫布学习的神经心理学概念，解释了突触可塑性的基本机制，认为（大脑中的）神经连接在使用中会变得更强（同时激活的神经元会“联合”），因此能够通过学习重新塑造。'
- en: '**1950:** Alan Turing publishes “Computing Machinery and Intelligence”, presenting
    his groundbreaking idea of what came to be known as the Turing test for determining
    whether a machine can “think”.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1950年：** 艾伦·图灵（Alan Turing）出版了《计算机与智能》，提出了他开创性的想法——图灵测试，用于判断机器是否能够“思考”。'
- en: '**1958:** Frank Rosenblatt, an American psychologist, proposes perceptron,
    a first artificial neural network in his “The perceptron: A probabilistic model
    for information storage and organisation in the brain”.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1958年：** 美国心理学家弗兰克·罗森布拉特在其著作《感知机：大脑中信息存储和组织的概率模型》中提出了感知机，这是第一个人工神经网络。'
- en: '![](../Images/c084ab5fe3b350dc88a1d62b2f4744bf.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c084ab5fe3b350dc88a1d62b2f4744bf.png)'
- en: '*Figure 1\. Rosenblatt’s perceptron schematic representation (Source:* [*Rosenblatt,
    1958*](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)*)*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1\. 罗森布拉特感知机示意图（来源：* [*Rosenblatt, 1958*](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)*)*'
- en: '**1962:** Frank Rosenblatt introduces the back-propagation error correction,
    a fundamental concept for computer learning, that inspired further DL works.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1962年：** 弗兰克·罗森布拉特（Frank Rosenblatt）提出了反向传播误差修正，这一计算机学习的基本概念，启发了后续的深度学习研究。'
- en: '**1963:** Mario Bunge, an Argentine-Canadian philosopher and physicist, publishes
    “A General Black Box Theory”, contributing to the development of black box theory
    and defining it as an abstraction that represents “a set of concrete systems into
    which stimuli S impinge and output of which reactions R emerge”.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1963年：** 阿根廷-加拿大哲学家兼物理学家马里奥·布恩格（Mario Bunge）出版了《一般黑箱理论》，为黑箱理论的发展做出了贡献，并将其定义为一种抽象，表示“一个具体系统的集合，其中刺激S作用于系统并产生反应R的输出”。'
- en: '**1967:** Shunichi Amari, a Japanese engineer and neuroscientist, pioneers
    the first multilayer perceptron trained with stochastic gradient descent for classifying
    non-linearly separable patterns.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1967年：** 日本工程师兼神经科学家天野俊一（Shunichi Amari）率先提出了第一个使用随机梯度下降训练的多层感知机，用于分类非线性可分模式。'
- en: '**1969:** Kunihiko Fukushima, a Japanese computer scientist, introduces Rectified
    Linear Unit (ReLU), which has since become the most widely adopted activation
    function in deep learning.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1969年：** 日本计算机科学家福岛邦彦（Kunihiko Fukushima）提出了修正线性单元（ReLU），它后来成为深度学习中最广泛采用的激活函数。'
- en: '**1970:** Seppo Linnainmaa, a Finnish mathematician and computer scientist,
    proposes the “reverse mode of automatic differentiation” in his master’s thesis,
    a modern variant of backpropagation.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1970年：** 芬兰数学家兼计算机科学家塞坡·林内马（Seppo Linnainmaa）在他的硕士论文中提出了“反向自动微分模式”，这是反向传播的现代变体。'
- en: '**1980:** Kunihiko Fukushima introduces Neocognitron, an early deep learning
    architecture for convolutional neural networks (CNNs), which does not use backpropagation
    for training.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1980年：** 福岛邦彦提出了Neocognitron，这是早期的深度学习架构之一，适用于卷积神经网络（CNN），但并不使用反向传播进行训练。'
- en: '**1989:** Yann LeCun, a French-American computer scientist, presents LeNet,
    the first CNN architecture to successfully apply backpropagation for handwritten
    ZIP code recognition.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1989年：** 法美籍计算机科学家Yann LeCun提出了LeNet，这是第一个成功应用反向传播进行手写ZIP代码识别的卷积神经网络（CNN）架构。'
- en: '**1995:** Morch et al. introduce saliency maps, offering one of the first explainability
    approaches for unveiling internal workings of deep neural networks.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1995年：** Morch等人提出了显著性图，这为揭示深度神经网络内部工作原理提供了最早的可解释性方法之一。'
- en: '**2000s:** Further advances including development of CUDA, enabling parallel
    processing on GPUs for high-performance scientific computing, alongside ImageNet,
    a large-scale manually curated visual dataset, pushing forward fundamental and
    applied AI research.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2000年代：** 进一步的进展包括CUDA的开发，使得GPU并行处理能够支持高性能科学计算，此外还有ImageNet，这个大规模的手动策划的视觉数据集，推动了基础和应用人工智能研究的进展。'
- en: '**2010s:** Continued breakthroughs in computer vision, such as Krizhevsky,
    Sutskever, and Hinton’s deep convolutional network for ImageNet classification,
    drive widespread AI adoption across industries. The field of XAI flourishes with
    the emergence of CNN saliency maps, LIME, Grad-CAM, and SHAP, among others*.*'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2010年代：** 计算机视觉领域持续突破，如Krizhevsky、Sutskever和Hinton提出的用于ImageNet分类的深度卷积网络，推动了人工智能在各行各业的广泛应用。可解释人工智能（XAI）领域蓬勃发展，出现了CNN显著性图、LIME、Grad-CAM和SHAP等技术*。'
- en: '![](../Images/0622d464e98ab68457d714b48a30e33e.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0622d464e98ab68457d714b48a30e33e.png)'
- en: '*Figure 2\. ImageNet classification SOTA benchmark for vision models, 2014–2024
    (Source: PapersWithCode)*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2：* ImageNet分类的视觉模型SOTA基准，2014–2024年（来源：PapersWithCode）'
- en: '**2020s:** The AI boom gains momentum with the 2017 paper “Attention Is All
    You Need”, which introduces an encoder-decoder architecture, named Transformer,
    which catalyzes the development of more advanced transformer-based architectures.
    Building on early successes such as Allen AI’s ELMo, Google’s BERT, and OpenAI’s
    GPT, Transformer is applied across modalities and domains, including vision, accelerating
    progress in multimodal research. In 2021, OpenAI introduces CLIP, a model capable
    of learning visual concepts from natural language supervision, paving the way
    for generative AI innovations, including DALL-E, Vision Transformer (ViT), Stable
    Diffusion, and Sora, enhancing image and video generation capabilities.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2020年代：** 人工智能热潮随着2017年论文《Attention Is All You Need》的发布而加速，该论文提出了一种名为Transformer的编码器-解码器架构，推动了更先进的基于Transformer的架构的发展。在Allen
    AI的ELMo、谷歌的BERT和OpenAI的GPT等早期成功的基础上，Transformer被应用于多个领域和模式，包括视觉，加速了多模态研究的进展。2021年，OpenAI推出了CLIP，一个能够从自然语言监督中学习视觉概念的模型，为生成型人工智能创新铺平道路，包括DALL-E、Vision
    Transformer（ViT）、Stable Diffusion和Sora，提升了图像和视频生成能力。'
- en: '**2024:** The EU AI Act comes into effect, establishing legal requirements
    for AI systems in Europe, including mandates for transparency, reliability, and
    fairness. For example, [Recital 27](https://ai-act-law.eu/recital/) defines transparency
    for AI systems as: “developed and used in a way that allows appropriate traceability
    and explainability […] contributing to the design of coherent, trustworthy and
    human-centric AI”.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2024年：** 欧盟人工智能法案生效，建立了针对欧洲人工智能系统的法律要求，包括透明度、可靠性和公平性的规定。例如，[第27条](https://ai-act-law.eu/recital/)将人工智能系统的透明度定义为：“以一种允许适当追溯性和可解释性的方式开发和使用
    […] 有助于设计一致、可信和以人为本的人工智能”。'
- en: As we can see, early works primarily focused on foundational approaches and
    algorithms. with later advancements targeting specific domains, including computer
    vision. In the late 20th century, key concepts began to emerge, setting the stage
    for future breakthroughs like backpropagation-trained CNNs in the 1980s. Over
    time, the field of explainable AI has rapidly evolved, enhancing our understanding
    of reasoning behind prediction and enabling better-informed decisions through
    increased research and industry applications. As (X)AI gained traction, the focus
    shifted to balancing system efficiency with interpretability, aiding model understanding
    at scale and integrating XAI solutions throughout the ML lifecycle [[Bhatt et
    al., 2019](https://arxiv.org/abs/1909.06342), [Decker et al., 2023](https://link.springer.com/chapter/10.1007/978-3-031-35891-3_13)].
    Essentially, it is only in the past two decades that these technologies have become
    practical enough to result in widespread adoption. More lately, legislative measures
    and regulatory frameworks, such as the EU AI Act (August 2024) and China TC260’s
    AI Safety Governance Framework (September 2024), have emerged, [marking the start](https://artificialintelligenceact.eu/implementation-timeline/)
    of more stringent regulations for AI development and deployment, including the
    right enforcing “to obtain from the deployer clear and meaningful explanations
    of the role of the AI system in the decision-making procedure and the main elements
    of the decision taken” ([Article 86, 2026](https://artificialintelligenceact.eu/article/86/)).
    This is where XAI can prove itself at its best. Still, despite years of rigorous
    research and growing emphasis on explainability, the topic seems to have faded
    from the spotlight. Is that really the case? Now, let’s consider it all from a
    bird’s eye view.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，早期的工作主要集中在基础方法和算法上，随着后期的进展，逐渐聚焦于特定领域，包括计算机视觉。到20世纪末，关键概念开始出现，为未来的突破奠定了基础，例如1980年代的反向传播训练卷积神经网络（CNNs）。随着时间的推移，可解释人工智能（XAI）领域迅速发展，增强了我们对预测背后推理的理解，并通过增加的研究和行业应用促进了更为明智的决策。随着（X）AI的兴起，焦点转向了平衡系统效率与可解释性，在大规模模型理解中起到辅助作用，并在整个机器学习生命周期中整合XAI解决方案[[Bhatt
    et al., 2019](https://arxiv.org/abs/1909.06342)，[Decker et al., 2023](https://link.springer.com/chapter/10.1007/978-3-031-35891-3_13)]。实际上，直到过去二十年，这些技术才足够成熟，得以广泛应用。最近，立法措施和监管框架，如欧盟人工智能法案（2024年8月）和中国TC260的人工智能安全治理框架（2024年9月），相继出台，
    [标志着](https://artificialintelligenceact.eu/implementation-timeline/)人工智能开发和部署的更严格监管的开始，包括强制执行“从部署者那里获得对AI系统在决策过程中角色的清晰且有意义的解释，以及所作决策的主要要素”（[2026年第86条](https://artificialintelligenceact.eu/article/86/)）。在这一点上，XAI可以发挥其最佳作用。然而，尽管经过多年的严格研究和对可解释性的日益重视，这一话题似乎已经逐渐淡出人们的视野。真的是这样吗？现在，让我们从全局角度重新审视这一切。
- en: 2\. AI delight then and now
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 人工智能的兴奋与现在
- en: Today is an exciting time to be in the world of technology. In the 1990s, Gartner
    introduced something called the Hype cycle to describe how emerging technologies
    evolve over time — from the initial spark of interest to societal application.
    According to this methodology, technologies typically begin with innovation breakthroughs
    (referred to as the “Technology trigger”), followed by a steep rise in excitement,
    culminating at the “Peak of inflated expectations”. However, when the technology
    doesn’t deliver as expected, it plunges into the “Trough of disillusionment,”
    where enthusiasm wanes, and people become frustrated. The process can be described
    as a steep upward curve that eventually descends into a low point, before leveling
    off into a more gradual ascent, representing a sustainable plateau, the so-called
    “Plateau of productivity”. The latter implies that, over time, a technology can
    become genuinely productive, regardless of the diminished hype surrounding it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 今天是处于科技世界的一个激动人心的时刻。在1990年代，Gartner提出了一个叫做炒作周期的方法论，用来描述新兴技术如何随时间演变——从最初的兴趣激发到社会应用。根据这一方法论，技术通常从创新突破（即“技术触发”）开始，随后迎来激动人心的快速上升，最终达到“过高期望的顶峰”。然而，当技术未能按预期交付时，它会跌入“失望的谷底”，此时热情消退，人们感到沮丧。这个过程可以描述为一条陡峭的上升曲线，最终会下行至低点，然后平稳上升，进入所谓的“生产力平台”。后者意味着，随着时间的推移，某项技术可以变得真正具有生产力，尽管围绕它的炒作已不再那么强烈。
- en: '![](../Images/bcd77e527061f7e0de8a492739ea3500.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcd77e527061f7e0de8a492739ea3500.png)'
- en: '*Figure 3\. Hype Cycle for Artificial Intelligence in 2024 (Source: Gartner)*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3\. 2024年人工智能的炒作周期（来源：Gartner）*'
- en: Look at previous technologies that were supposed to solve everything — intelligent
    agents, cloud computing, blockchain, brain-computer interfaces, big data, and
    even deep learning. They all came up to have fantastic places in the tech world,
    but, of course, none of them became a silver bullet. Similar goes with the explainability
    topic now. And we can see over and over that history repeats itself. As highlighted
    by the Gartner Hype Cycle for AI 2024 (Fig. 3), **Responsible AI** (RAI) is gaining
    prominence (top left) and is expected to reach maturity within the next five years.
    Explainability provides a foundation for RAI practices, promoting transparency,
    accountability, and fairness in AI systems.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以前那些被认为能够解决一切的技术——智能代理、云计算、区块链、大脑-计算机接口、大数据，甚至深度学习。它们都在科技界取得了辉煌的地位，但当然，它们都没有成为万能的解决方案。现在，关于可解释性的话题也有类似的情况。我们可以一次又一次地看到历史的重复。如2024年Gartner人工智能热潮周期所强调的（图3），**负责任的人工智能**（RAI）正逐渐获得更多关注（左上角），预计将在未来五年内达到成熟。可解释性为RAI实践提供了基础，促进了人工智能系统中的透明度、问责制和公平性。
- en: Figure below overviews XAI research trends and applications, derived from scientific
    literatures published between 2018 and 2022 to cover various concepts within the
    XAI field, including “explainable artificial intelligence”, “interpretable artificial
    intelligence”, and “responsible artificial intelligence”[[Clement et al., 2023](https://www.mdpi.com/2504-4990/5/1/6)].
    Figure 4a outlines key XAI research areas based on the meta-review results. The
    largest focus (44%) is on designing explainability methods, followed by 15% on
    XAI applications across specific use cases. Domain-dependent studies (e.g., finance)
    account for 12%, with smaller areas — requirements analysis, data types, and human-computer
    interaction — each making up around 5–6%.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下图概述了XAI（可解释人工智能）研究的趋势和应用，来源于2018年至2022年间发表的科学文献，涵盖了XAI领域的各个概念，包括“可解释人工智能”、“可解释性人工智能”和“负责任的人工智能”[[Clement
    et al., 2023](https://www.mdpi.com/2504-4990/5/1/6)]。图4a根据元综述结果概述了XAI的关键研究领域。最大的一部分（44%）集中在设计可解释性方法，其次是15%专注于XAI在特定应用场景中的应用。依赖领域的研究（如金融）占12%，而其他小领域——需求分析、数据类型和人机交互——各自占约5-6%。
- en: '![](../Images/fce491a1a2f8d354a4114eb5fd596907.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fce491a1a2f8d354a4114eb5fd596907.png)'
- en: '*Figure 4\. XAI research perspectives (a) and application domains (b) (Source:*
    [*Clement et al., 2023*](https://www.mdpi.com/2504-4990/5/1/6)*)*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4．XAI研究视角（a）和应用领域（b）（来源：* [*Clement et al., 2023*](https://www.mdpi.com/2504-4990/5/1/6)*)*'
- en: Next to it are common application fields (Fig. 4b), with healthcare leading
    (23%), driven by the need for trust-building and decision-making support. Industry
    4.0 (6%) and security (4%) follow, where explainability is applied to industrial
    optimization and fraud detection. Other fields include natural sciences, legal
    studies, robotics, autonomous driving, education, and social sciences [[Atakishiyev
    et al., 2024](https://ieeexplore.ieee.org/document/10604830), [Chen et al., 2023](https://www.nature.com/articles/s41551-023-01056-8),
    [Loh et al., 2022](https://www.sciencedirect.com/science/article/abs/pii/S0169260722005429)].
    As XAI progresses toward a sustainable state, research and developments become
    increasingly focused on addressing fairness, transparency, and accountability
    [[Arrieta et al., 2020](https://dl.acm.org/doi/10.1016/j.inffus.2019.12.012),
    [Responsible AI Institute Standards](https://www.responsible.ai/ai-standards-deep-dive-decoding-different-ai-standards-and-the-eus-approach/),
    [Stanford AI Index Report](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter3.pdf)].
    These dimensions are crucial for ensuring equitable outcome, clarifying decision-making
    processes, and establishing responsibility for those decisions, thereby fostering
    user confidence, and aligning with regulatory frameworks and industry standards.
    Reflecting the trajectory of past technological advances, the rise of XAI highlights
    both the challenges and opportunities for building AI-driven solutions, establishing
    it as an important element in responsible AI practices, enhancing AI’s long-term
    relevance in real-world applications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 旁边是常见的应用领域（图4b），其中医疗保健居领先地位（23%），由建立信任和支持决策的需求推动。工业4.0（6%）和安全（4%）紧随其后，可解释性被应用于工业优化和欺诈检测。其他领域包括自然科学、法学、机器人技术、自动驾驶、教育和社会科学
    [[Atakishiyev et al., 2024](https://ieeexplore.ieee.org/document/10604830), [Chen
    et al., 2023](https://www.nature.com/articles/s41551-023-01056-8), [Loh et al.,
    2022](https://www.sciencedirect.com/science/article/abs/pii/S0169260722005429)]。随着XAI向可持续状态发展，研究和开发越来越集中于解决公平性、透明性和问责制问题
    [[Arrieta et al., 2020](https://dl.acm.org/doi/10.1016/j.inffus.2019.12.012),
    [Responsible AI Institute Standards](https://www.responsible.ai/ai-standards-deep-dive-decoding-different-ai-standards-and-the-eus-approach/),
    [Stanford AI Index Report](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter3.pdf)]。这些维度对于确保公平结果、澄清决策过程并为这些决策建立责任至关重要，从而促进用户信任，并与监管框架和行业标准保持一致。反映过去技术进步的轨迹，XAI的崛起突显了在构建AI驱动解决方案方面的挑战和机遇，确立了它作为负责任AI实践中一个重要元素的地位，增强了AI在现实应用中的长期相关性。
- en: 3\. Putting **the spotlight back on XAI**
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 将**焦点重新放回到XAI**（可解释的人工智能）
- en: 3.1\. Why and when model understanding — Explainability 101
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1. 为什么以及何时需要模型理解——可解释性基础
- en: 'Here is a common perception of AI systems: You put data in, and then, there
    is a black box processing it, producing an output, but we cannot examine the system’s
    internal workings. But is that really the case? As AI continues to proliferate,
    the development of reliable, scalable, and transparent systems becomes increasingly
    vital. Put simply: the idea of explainable AI can be described as doing something
    to provide a clearer **understanding** of what happens between the input and output.
    In a broad sense, one can think about it as a collection of methods allowing us
    to build systems capable of delivering desirable results. Practically, model understanding
    can be defined as the capacity to generate explanations of the model’s behaviour
    that users can comprehend. This understanding is crucial in a variety of use cases
    across industries, including:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对人工智能系统的一个普遍看法：你输入数据，然后系统就像一个黑箱一样处理它，产生输出，但我们无法检查系统的内部工作原理。但真的是这样吗？随着人工智能的不断发展，开发可靠、可扩展且透明的系统变得越来越重要。简而言之：可解释人工智能的理念可以描述为采取某些方法来提供更清晰的**理解**，说明输入和输出之间发生了什么。从广义上讲，可以将其视为一组方法，允许我们构建能够提供理想结果的系统。在实践中，模型理解可以定义为生成用户能够理解的模型行为解释的能力。这种理解在各行各业的各种应用场景中都至关重要，包括：
- en: Model debugging and quality assurance (e.g., manufacturing, robotics);
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型调试和质量保证（例如，制造业、机器人技术）；
- en: Ensuring system trustability for end-users (medicine, finance);
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保系统对终端用户的可信度（医学、金融）；
- en: Improving system performance by identifying scenarios where the model is likely
    to fail (fraud detection in banking, e-commerce);
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过识别模型可能失败的场景来提高系统性能（银行欺诈检测、电子商务）；
- en: Enhancing system robustness against adversaries (cybersecurity, autonomous vehicles);
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强系统对抗敌对行为的鲁棒性（网络安全、自动驾驶汽车）；
- en: Explaining decision-making processes (finance for credit scoring, legal for
    judicial decisions);
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释决策过程（如信用评分中的金融、司法决策中的法律）；
- en: Detecting data mislabelling and other issues (customer behavior analysis in
    retail, medical imaging in healthcare).
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测数据标注错误及其他问题（零售中的客户行为分析、医疗健康中的医学影像）。
- en: 'The growing adoption of AI has led to its widespread use across domains and
    risk applications. What is comes down to is that human understanding is not the
    same as model understanding. While AI models process information in ways that
    are not inherently intuitive to humans, one of the primary objectives of XAI is
    to create systems that effectively communicate their reasoning — in other words,
    to “speak” in terms that are accessible and meaningful to end users. So, the question
    then becomes: how can we bridge that gap between what a model “knows” and how
    humans comprehend its outputs?'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的广泛采用使其在各个领域和风险应用中得到了广泛使用。归根结底，人类理解与模型理解并不相同。虽然人工智能模型处理信息的方式对人类来说并不直观，但可解释人工智能的主要目标之一是创建能够有效传达推理过程的系统——换句话说，能够以对最终用户可理解且有意义的方式“发声”。因此，问题就变成了：我们如何弥合模型“知道”的内容与人类如何理解其输出之间的差距？
- en: 3.2\. Who is it for — Stakeholders desiderata on XAI
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2. 它是为谁服务的——可解释人工智能的利益相关者期望
- en: Explainable AI is not just about interpreting models but enabling machines to
    effectively support humans by transferring knowledge. To address these aspects,
    one can think on how explainability can be tied to expectations of diverse personas
    and stakeholders involved in AI ecosystems. These groups usually include users,
    developers, deployers, affected parties, and regulators [[Leluschko&Tholen,2023](https://www.dfki.de/fileadmin/user_upload/import/14563_Goals_and_Stakeholder_Involvement_in_XAI_for_Remote_Sensing_A_Structured_Literature_Review_-_978-3-031-47994-6_47.pdf),
    [Sadek et al., 2024](https://link.springer.com/article/10.1007/s00146-024-01880-9)].
    Accordingly, their desiderata — i.e. features and results they expect from AI
    — also vary widely, suggesting that explainability needs to cater to a wide array
    of needs and challenges. In the study, [Langer et al., 2021](https://arxiv.org/pdf/2102.07817)
    highlight that understanding plays a critical role in addressing the epistemic
    facet, referring to stakeholders’ ability to assess whether a system meets their
    expectations, such as fairness or transparency. For instance, in high-stakes scenarios
    like medical diagnosis, the depth of explanations required for trust calibration
    might be greater [[Saraswat et al., 2022](https://ieeexplore.ieee.org/abstract/document/9852458)].
    Figure 5 presents a conceptual model that outlines the pathway from explainability
    approaches to fulfilling stakeholders’ needs, which, in turn, affects how well
    their desiderata are met. But what constitutes a “good” explanation? The study
    argues that it should be not only accurate, representative, and context-specific
    with respect to a system and its functioning, but also align with socio-ethical
    and legal considerations, which can be decisive in justifying certain desiderata.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释人工智能不仅仅是对模型的解释，还涉及到通过知识转移让机器有效地支持人类。为了应对这些方面，可以思考如何将可解释性与涉及人工智能生态系统的各种角色和利益相关者的期望联系起来。这些群体通常包括用户、开发者、部署者、受影响方和监管者[[Leluschko&Tholen,2023](https://www.dfki.de/fileadmin/user_upload/import/14563_Goals_and_Stakeholder_Involvement_in_XAI_for_Remote_Sensing_A_Structured_Literature_Review_-_978-3-031-47994-6_47.pdf),
    [Sadek等人，2024](https://link.springer.com/article/10.1007/s00146-024-01880-9)]。因此，他们的期望——即他们对人工智能的功能和结果的预期——也会有所不同，这表明可解释性需要满足广泛的需求和挑战。在这项研究中，[Langer等人，2021](https://arxiv.org/pdf/2102.07817)强调了理解在解决认识论方面的关键作用，指的是利益相关者评估系统是否满足他们期望的能力，比如公平性或透明度。例如，在医学诊断等高风险场景中，可能需要更深层次的解释以建立信任[[Saraswat等人，2022](https://ieeexplore.ieee.org/abstract/document/9852458)]。图5展示了一个概念模型，概述了从可解释性方法到满足利益相关者需求的路径，而这些需求又反过来影响它们是否得到满足。那么，什么构成一个“好的”解释呢？该研究认为，它不仅应该准确、具有代表性，并且与系统及其功能的上下文相关，还应该符合社会伦理和法律考虑，这些因素可能在为某些期望提供正当理由时起决定性作用。
- en: '![](../Images/2baa045787201bf9a6cb1b9012029e65.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2baa045787201bf9a6cb1b9012029e65.png)'
- en: '*Figure 5: Relation of explainability with stakeholders’ desiderata (Source:*
    [*Langer et al., 2021*](https://arxiv.org/pdf/2102.07817))'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5：可解释性与利益相关者期望的关系（来源：* [*Langer等人，2021*](https://arxiv.org/pdf/2102.07817))'
- en: Here, we can say that the success of XAI as technology hinges on how effectively
    it facilitates human understanding through explanatory information, emphasizing
    the need for careful navigation of trade-offs among stakeholders. For instance,
    for **domain experts and users** (e.g., doctors, judges), who deal with interpreting
    and auditing AI system outputs for decision-making, it is important to ensure
    explainability results are concise and domain-specific to align them with expert
    intuition, while not creating information overload, which is especially relevant
    for human-in-the-loop applications. Here, the challenge may arise due to uncertainty
    and the lack of clear causality between inputs and outputs, which can be addressed
    through local post-hoc explanations tailored to specific use cases [[Metta et
    al., 2024](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11048122/)]. **Affected
    parties** (e.g., job applicants, patients) are individuals impacted by AI’s decisions,
    with fairness and ethics being key concerns, especially in contexts like hiring
    or healthcare. Here, explainability approaches can aid in identifying factors
    contributing to biases in decision-making processes, allowing for their mitigation
    or, at the very least, acknowledgment and elimination [[Dimanov et al., 2020](https://mlg.eng.cam.ac.uk/adrian/ECAI20-You_Shouldn%E2%80%99t_Trust_Me.pdf)].
    Similarly, **regulators** may seek to determine whether a system is biassed toward
    any group to ensure compliance with ethical and regulatory standards, with a particular
    focus on transparency, traceability, and non-discrimination in high-risk applications
    [[Gasser & Almeida, 2017](https://dash.harvard.edu/handle/1/34390353), [Floridi
    et al., 2018](https://link.springer.com/article/10.1007/s11023-018-9482-5), [The
    EU AI Act 2024](https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以说，XAI（可解释人工智能）作为技术的成功，依赖于它如何通过解释性信息有效地促进人类理解，强调了在利益相关者之间仔细平衡权衡的重要性。例如，对于**领域专家和用户**（如医生、法官），他们需要解读和审核AI系统输出结果以进行决策，因此确保可解释性结果简洁且具有领域针对性非常重要，以便与专家直觉对接，同时避免信息过载，尤其在涉及人类干预的应用中更为重要。在这种情况下，挑战可能出现在输入和输出之间的不确定性和缺乏清晰因果关系上，这可以通过针对特定用例的局部事后解释来解决[[Metta等人，2024](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11048122/)]。**受影响方**（例如，求职者、患者）是受AI决策影响的个体，公平性和伦理性是关键问题，特别是在招聘或医疗等领域。在这种情况下，可解释性方法可以帮助识别在决策过程中导致偏见的因素，从而进行缓解或至少承认并消除偏见[[Dimanov等人，2020](https://mlg.eng.cam.ac.uk/adrian/ECAI20-You_Shouldn%E2%80%99t_Trust_Me.pdf)]。同样，**监管机构**可能希望确定一个系统是否对某一群体存在偏见，以确保其符合伦理和监管标准，特别关注透明度、可追溯性和高风险应用中的非歧视性[[Gasser
    & Almeida, 2017](https://dash.harvard.edu/handle/1/34390353)，[Floridi等人，2018](https://link.springer.com/article/10.1007/s11023-018-9482-5)，[欧盟AI法案2024](https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)]。
- en: '![](../Images/5f130b8164835537c7ade7eb898b95ed.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f130b8164835537c7ade7eb898b95ed.png)'
- en: '*Figure 6\. Explainability in the ML lifecycle process (Source:* [*Decker et
    al., 2023*](https://link.springer.com/chapter/10.1007/978-3-031-35891-3_13)*)*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6\. 机器学习生命周期中的可解释性 (来源：* [*Decker等人，2023*](https://link.springer.com/chapter/10.1007/978-3-031-35891-3_13)*)*'
- en: For **businesses and organisations** adopting AI, the challenge may lie in ensuring
    responsible implementation in line with regulations and industry standards, while
    also maintaining user trust [[Ali et al., 2023](https://www.sciencedirect.com/science/article/pii/S1566253523001148),
    [](https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/Microsoft-Responsible-AI-Standard-General-Requirements.pdf)
    [Saeed & Omlin, 2021](https://arxiv.org/pdf/2111.06420)]. In this context, using
    global explanations and incorporating XAI into the ML lifecycle (Figure 6), can
    be particularly effective [[Saeed & Omlin, 2021](https://arxiv.org/pdf/2111.06420),
    [Microsoft Responsible AI Standard v2 General Requirements](https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/Microsoft-Responsible-AI-Standard-General-Requirements.pdf),
    [Google Responsible AI Principles](https://ai.google/responsibility/principles/)].
    Overall, both regulators and deployers aim to understand the entire system to
    minimize implausible corner cases. When it comes to **practitioners** (e.g., developers
    and researchers), who build and maintain AI systems, these can be interested in
    leveraging XAI tools for diagnosing and improving model performance, along with
    advancing existing solutions with interpretability interface that can provide
    details about model’s reasoning [[Bhatt et al., 2020](https://dl.acm.org/doi/abs/10.1145/3351095.3375624)].
    However, these can come with high computational costs, making large-scale deployment
    challenging. Here, the XAI development stack can include both open-source and
    proprietary toolkits, frameworks, and libraries, such as [PyTorch Captum](https://github.com/pytorch/captum),
    [Google Model Card Toolkit](https://github.com/tensorflow/model-card-toolkit),
    [Microsoft Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox#responsible-ai-toolbox),
    [IBM AI Fairness 360](https://github.com/Trusted-AI/AIF360), for ensuring that
    systems built are safe, reliable, and trustworthy from development through deployment
    and beyond.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于采用**人工智能**的**企业和组织**来说，挑战可能在于确保在符合监管和行业标准的前提下负责任地实施AI，同时保持用户的信任[[Ali et al.,
    2023](https://www.sciencedirect.com/science/article/pii/S1566253523001148), [](https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/Microsoft-Responsible-AI-Standard-General-Requirements.pdf)
    [Saeed & Omlin, 2021](https://arxiv.org/pdf/2111.06420)]。在这种情况下，使用全球性解释并将XAI融入机器学习生命周期（图6）中，可能特别有效[[Saeed
    & Omlin, 2021](https://arxiv.org/pdf/2111.06420), [Microsoft Responsible AI Standard
    v2 General Requirements](https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/Microsoft-Responsible-AI-Standard-General-Requirements.pdf),
    [Google Responsible AI Principles](https://ai.google/responsibility/principles/)]。总体而言，监管机构和部署者都旨在了解整个系统，以尽量减少不合理的极端情况。在**从业者**（如开发者和研究人员）方面，他们可能对利用XAI工具诊断和改善模型性能感兴趣，同时推进现有解决方案，通过可解释性界面提供关于模型推理的详细信息[[Bhatt
    et al., 2020](https://dl.acm.org/doi/abs/10.1145/3351095.3375624)]。然而，这些工具可能伴随高昂的计算成本，使大规模部署变得具有挑战性。在这种情况下，XAI开发堆栈可以包括开源和专有工具包、框架和库，例如[PyTorch
    Captum](https://github.com/pytorch/captum)、[Google Model Card Toolkit](https://github.com/tensorflow/model-card-toolkit)、[Microsoft
    Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox#responsible-ai-toolbox)、[IBM
    AI Fairness 360](https://github.com/Trusted-AI/AIF360)，以确保从开发到部署以及后续阶段，构建的系统是安全、可靠和值得信赖的。
- en: And as we can see — one size does not fit all. One of the ongoing challenges
    is to provide explanations that are both accurate and meaningful for different
    stakeholders while balancing transparency and usability in real-world applications
    [[Islam et al., 2022](https://link.springer.com/chapter/10.1007/978-3-030-96630-0_1),
    [Tate et al., 2023](https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2023.1117848/full),
    [Hutsen, 2023](https://www.mdpi.com/2673-2688/4/3/34)]. Now, let’s talk about
    XAI in a more practical sense.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的——“一刀切”并不适用。持续的挑战之一是提供既准确又有意义的解释，满足不同利益相关者的需求，同时在实际应用中平衡透明度和可用性[[Islam
    et al., 2022](https://link.springer.com/chapter/10.1007/978-3-030-96630-0_1),
    [Tate et al., 2023](https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2023.1117848/full),
    [Hutsen, 2023](https://www.mdpi.com/2673-2688/4/3/34)]。现在，让我们以更实际的角度来讨论XAI。
- en: 4\. Model explainability for vision
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 视觉模型的可解释性
- en: 4.1\. Feature attribution methods
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1\. 特征归因方法
- en: 'As AI systems have advanced, modern approaches have demonstrated substantial
    improvements in performance on complex tasks, such as image classification (Fig.
    2), surpassing earlier image processing techniques that relied heavily on handcrafted
    algorithms for visual feature extraction and detection [[Sobel and Feldman, 1973](https://www.researchgate.net/publication/281104656_An_Isotropic_3x3_Image_Gradient_Operator),
    [Canny, 1987](https://www.sciencedirect.com/science/article/abs/pii/B9780080515816500246)].
    While modern deep learning architectures are not inherently interpretable, various
    solutions have been devised to provide explanations on model behavior for given
    inputs, allowing to bridge the gap between human (understanding) and machine (processes).
    Following the breakthroughs in deep learning, various XAI approaches have emerged
    to enhance explainability aspects in the domain of computer vision. Focusing on
    image classification and object detection applications, the Figure 7 below outlines
    several commonly used XAI methods developed over the past decades:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能系统的进步，现代方法在复杂任务中的表现有了显著改善，例如图像分类（图 2），超越了早期依赖手工算法进行视觉特征提取和检测的图像处理技术[[Sobel
    and Feldman, 1973](https://www.researchgate.net/publication/281104656_An_Isotropic_3x3_Image_Gradient_Operator),
    [Canny, 1987](https://www.sciencedirect.com/science/article/abs/pii/B9780080515816500246)]。虽然现代深度学习架构本身并不具备可解释性，但已经提出了多种解决方案，用于为给定输入提供模型行为的解释，从而弥合人类（理解）与机器（过程）之间的差距。继深度学习突破之后，各种XAI（可解释人工智能）方法应运而生，旨在增强计算机视觉领域的可解释性方面。下面的图
    7 描述了过去几十年中开发的几种常用的 XAI 方法，专注于图像分类和目标检测应用：
- en: '![](../Images/0f4bad26a548ead967cd25deed0e3d70.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f4bad26a548ead967cd25deed0e3d70.png)'
- en: '*Figure 7\. Explainability methods for computer vision (Image by author)*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7\. 计算机视觉的可解释性方法（作者提供的图像）*'
- en: XAI methods can be broadly categorized based on their methodology into backpropagation-
    and perturbation-based methods, while the explanation scope is either local or
    global. In computer vision, these methods or combinations of them are used to
    uncover the decision criteria behind model predictions. **Backpropagation-based**
    approaches propagate a signal from the output to the input, assigning weights
    to each intermediate value computed during the forward pass. A gradient function
    then updates each parameter at the model to align the output with the ground truth,
    making these techniques also known as gradient-based methods. Examples include
    saliency maps [[Simonyan et al., 201](https://arxiv.org/abs/1312.6034)3], integrated
    gradient [[Sundararajan et al., 2017](https://arxiv.org/pdf/1703.01365)], Grad-CAM
    [[Selvaraju et al, 2017](https://ieeexplore.ieee.org/document/8237336)]. In contrast,
    **perturbation-based** methods modify the input through techniques like occlusion
    [[Zeiler & Fergus, 2014](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53)],
    LIME [[Ribeiro et al., 2016](https://dl.acm.org/doi/10.1145/2939672.2939778)],
    RISE [[Petsiuk et al., 2018](https://arxiv.org/abs/1806.07421)], evaluating how
    these slight changes impact the network output. Unlike backpropagation-based methods,
    perturbation techniques don’t require gradients, as a single forward pass is sufficient
    to assess how the input changes influence the output.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: XAI 方法可以根据其方法论大致分为基于反向传播的方法和基于扰动的方法，而解释范围则可以是局部的或全局的。在计算机视觉中，这些方法或它们的组合用于揭示模型预测背后的决策标准。**基于反向传播**的方法将信号从输出传播到输入，为在前向传播过程中计算的每个中间值分配权重。然后，梯度函数更新模型中的每个参数，以使输出与真实值对齐，这些技术也被称为基于梯度的方法。示例包括显著性图[[Simonyan
    et al., 2013](https://arxiv.org/abs/1312.6034)]、集成梯度[[Sundararajan et al., 2017](https://arxiv.org/pdf/1703.01365)]、Grad-CAM[[Selvaraju
    et al., 2017](https://ieeexplore.ieee.org/document/8237336)]。相比之下，**基于扰动**的方法通过遮挡[[Zeiler
    & Fergus, 2014](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53)]、LIME[[Ribeiro
    et al., 2016](https://dl.acm.org/doi/10.1145/2939672.2939778)]、RISE[[Petsiuk et
    al., 2018](https://arxiv.org/abs/1806.07421)]等技术修改输入，评估这些轻微变化如何影响网络输出。与基于反向传播的方法不同，扰动技术不需要梯度，因为单次前向传播就足以评估输入变化如何影响输出。
- en: Explainability for “black box” architectures is typically achieved through external
    post-hoc methods after the model has been trained (e.g., gradients for CNN). In
    contrast, “white-box” architectures are interpretable by design, where explainability
    can be achieved as a byproduct of the model training. For example, in linear regression,
    coefficients derived from solving a system of linear equations can be used directly
    to assign weights to input features. However, while feature importance is straightforward
    in the case of linear regression, more complex tasks and advanced architectures
    consider highly non-linear relationships between inputs and outputs, thus requiring
    external explainability methods to understand and validate which features have
    the greatest influence on predictions. That being said, using linear regression
    for computer vision isn’t a viable approach.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“黑箱”架构，通常通过模型训练后采用外部事后方法实现可解释性（例如，CNN 的梯度）。相对而言，“白箱”架构本身具有可解释性，在这种架构中，可解释性可以作为模型训练的副产品来实现。例如，在线性回归中，通过求解线性方程组得到的系数可以直接用于为输入特征分配权重。然而，尽管在处理线性回归时，特征重要性较为直观，但更复杂的任务和高级架构会考虑输入与输出之间高度非线性的关系，因此需要外部的可解释性方法来理解和验证哪些特征对预测有最大的影响。也就是说，使用线性回归来进行计算机视觉任务并不是一个可行的方案。
- en: 4.2\. Evaluation metrics for XAI
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2\. XAI 的评估指标
- en: Evaluating explanations is essential to ensure that the insights derived from
    the model and their presentation to end-users — through the explainability interface
    — are meaningful, useful, and trustworthy [[Ali et al., 2023](https://www.sciencedirect.com/science/article/pii/S1566253523001148)].
    The increasing variety of XAI methods necessitates systematic evaluation and comparison,
    shifting away from subjective “I know it when I see it” approaches[.](https://arxiv.org/pdf/1702.08608)
    To address this challenge, researchers have devised numerous algorithmic and user-based
    evaluation techniques, along with frameworks and taxonomies, to capture both subjective
    and objective quantitative and qualitative properties of explanations [[Doshi-Velez
    & Kim, 2017,](https://arxiv.org/pdf/1702.08608) [Sokol & Flach, 2020](https://dl.acm.org/doi/10.1145/3351095.3372870)].
    Explainability is a spectrum, not a binary characteristic, and its effectiveness
    can be quantified by assessing the extent to which certain properties are to be
    fulfilled. One of the ways to categorize XAI evaluation methods is along the so-called
    Co-12 properties [[Nauta et al., 2023](https://dl.acm.org/doi/10.1145/3583558)],
    grouped by content, presentation, and user dimensions, as summarized in Table
    1.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 评估解释对于确保从模型中提取的见解及其通过可解释性界面展示给最终用户的方式是有意义的、有用的且值得信赖的至关重要 [[Ali et al., 2023](https://www.sciencedirect.com/science/article/pii/S1566253523001148)]。XAI
    方法日益多样化，迫使我们进行系统化的评估和比较，摆脱主观的“眼见为实”的方法[.](https://arxiv.org/pdf/1702.08608) 为了解决这一挑战，研究人员设计了多种算法和基于用户的评估技术，并提出了框架和分类法，用以捕捉解释的主观与客观、定量与定性属性
    [[Doshi-Velez & Kim, 2017,](https://arxiv.org/pdf/1702.08608) [Sokol & Flach,
    2020](https://dl.acm.org/doi/10.1145/3351095.3372870)]。可解释性是一个谱系，而非二元特征，其有效性可以通过评估某些属性的实现程度来量化。对
    XAI 评估方法进行分类的一种方式是基于所谓的 Co-12 属性 [[Nauta et al., 2023](https://dl.acm.org/doi/10.1145/3583558)]，这些属性按内容、展示和用户维度分组，概括如表
    1 所示。
- en: '![](../Images/3565cd39b1546752e472f4244dfa3a81.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3565cd39b1546752e472f4244dfa3a81.png)'
- en: '*Table 1\. Co-12 explanation quality properties for evaluation (Source:* [*Nauta
    et al., 2023*](https://dl.acm.org/doi/10.1145/3583558)*)*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 1\. 用于评估的 Co-12 解释质量属性（来源：* [*Nauta et al., 2023*](https://dl.acm.org/doi/10.1145/3583558)*)*'
- en: At a more granular level, quantitative evaluation methods for XAI can incorporate
    metrics, such as faithfulness, stability, fidelity, and explicitness [[Alvarez-Melis
    & Jaakkola, 2018](https://proceedings.neurips.cc/paper_files/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf),
    [Agarwal et al., 2022](https://openreview.net/pdf?id=BfxZAuWOg9), [Kadir et al.,
    2023](https://ieeexplore.ieee.org/document/10297629)], enabling the measurement
    of the intrinsic quality of explanations. For instance, **faithfulness** measures
    how well the explanation aligns with the model’s behavior, focusing on the importance
    of selected features for the target class prediction. [Qi et al., 2020](https://openaccess.thecvf.com/content_CVPRW_2019/papers/Explainable%20AI/Qi_Visualizing_Deep_Networks_by_Optimizing_with_Integrated_Gradients_CVPRW_2019_paper.pdf)
    demonstrated a method for feature importance analysis with Integrated Gradients,
    emphasizing the importance of producing faithful representations of model behavior.
    **Stability** refers to the consistency of explanations across similar inputs.
    A study by [Ribeiro et al., 2016](https://dl.acm.org/doi/10.1145/2939672.2939778)
    on LIME highlights the importance of stability in generating reliable explanations
    that do not vary drastically with slight input changes. **Fidelity** reflects
    how accurately an explanation reflects the model’s decision-making process. [Doshi-Velez
    & Kim, 2017](https://arxiv.org/pdf/1702.08608) emphasize fidelity in their framework
    for interpretable machine learning, arguing that high fidelity is essential for
    trustworthy AI systems. **Explicitness** involves how easily a human can understand
    the explanation. [Alvarez-Melis & Jaakkola, 2018](https://proceedings.neurips.cc/paper_files/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf)
    discussed robustness in interpretability through self-explaining neural networks
    (SENN), which strive for explicitness alongside stability and faithfulness.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在更细粒度的层面上，XAI（可解释人工智能）的定量评估方法可以纳入一些指标，如忠实度、稳定性、保真度和明确性[[Alvarez-Melis & Jaakkola,
    2018](https://proceedings.neurips.cc/paper_files/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf)、[Agarwal
    et al., 2022](https://openreview.net/pdf?id=BfxZAuWOg9)、[Kadir et al., 2023](https://ieeexplore.ieee.org/document/10297629)]，从而能够衡量解释的内在质量。例如，**忠实度**衡量解释与模型行为的一致性，侧重于为目标类别预测选择的特征的重要性。[Qi
    et al., 2020](https://openaccess.thecvf.com/content_CVPRW_2019/papers/Explainable%20AI/Qi_Visualizing_Deep_Networks_by_Optimizing_with_Integrated_Gradients_CVPRW_2019_paper.pdf)展示了一种通过集成梯度进行特征重要性分析的方法，强调生成忠实的模型行为表示的重要性。**稳定性**指的是在类似输入之间解释的一致性。[Ribeiro
    et al., 2016](https://dl.acm.org/doi/10.1145/2939672.2939778)对LIME的研究强调了稳定性在生成可靠解释中的重要性，稳定的解释不会因为细微的输入变化而发生剧烈波动。**保真度**反映了解释与模型决策过程的准确匹配程度。[Doshi-Velez
    & Kim, 2017](https://arxiv.org/pdf/1702.08608)在其可解释机器学习框架中强调了保真度，认为高保真度对可信赖的AI系统至关重要。**明确性**涉及人类理解解释的难易程度。[Alvarez-Melis
    & Jaakkola, 2018](https://proceedings.neurips.cc/paper_files/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf)通过自解释神经网络（SENN）讨论了解释性中的鲁棒性，SENN在追求明确性的同时，也注重稳定性和忠实度。
- en: 'To link the concepts, the correctness property, as described in Table 1, refers
    to the faithfulness of the explanation in relation to the model being explained,
    indicating how truthful the explanation reflects the “true” behavior of the black
    box. This property is distinct from the model’s predictive accuracy, but rather
    descriptive to the XAI method with respect to the model’s functioning [[Naute
    et al., 2023](https://dl.acm.org/doi/10.1145/3583558), [Sokol & Vogt, 2024](https://dl.acm.org/doi/abs/10.1145/3613905.3651047)].
    Ideally, an explanation is “nothing but the truth”, so high correctness is therefore
    desired. The faithfulness via deletion score can be obtained by calculating normalized
    area under the curve representing the difference between two feature importance
    functions: the one built by gradually removing features (starting with the Least
    Relevant First — LeRF) and evaluating the model performance at every step, and
    another one, for which the deletion order is random (Random Order — RaO) [[Won
    et al., 2023](https://arxiv.org/pdf/2303.14608)]. Computing points for both types
    of curves starts with providing the full image to the model and continues with
    a gradual removal of pixels, whose importance, assigned by an attribution method,
    lies below a certain threshold. A higher score implies that the model has a better
    ability to retain important information even when redundant features are deleted
    (Eq. 1).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了联系这些概念，如表 1 所述，正确性属性是指解释与被解释模型之间的忠实度，表示解释反映“真实”黑盒行为的真实性。这一属性不同于模型的预测准确性，而是对
    XAI 方法在模型功能方面的描述性[[Naute et al., 2023](https://dl.acm.org/doi/10.1145/3583558),
    [Sokol & Vogt, 2024](https://dl.acm.org/doi/abs/10.1145/3613905.3651047)]。理想情况下，解释应该是“完全真实的”，因此需要较高的正确性。通过删除得出的忠实度得分可以通过计算曲线下的标准化面积来获得，该曲线表示两种特征重要性函数之间的差异：一种是通过逐渐移除特征（从最不相关的特征开始
    — LeRF）并在每一步评估模型性能，另一种则是删除顺序随机（随机顺序 — RaO）[[Won et al., 2023](https://arxiv.org/pdf/2303.14608)]。计算两种曲线的点数是从将完整图像输入模型开始，接着逐渐移除像素，这些像素的特征重要性低于某个阈值，且由归因方法分配。得分越高，意味着模型在删除冗余特征时，仍能更好地保留重要信息（公式
    1）。
- en: '![](../Images/3836ab2a4e7daab43cf9f74790f28cc3.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3836ab2a4e7daab43cf9f74790f28cc3.png)'
- en: '*Equation 1\. Faithfulness metric computation for feature importance assessment
    via deletion*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*公式 1. 通过删除评估特征重要性的忠实度度量计算*'
- en: 'Another approach for evaluating faithfulness is to compute feature importance
    via insertion, similar to the method described above, but by gradually showing
    the model the most relevant image regions as identified by the attribution method.
    The key idea here: include important features and see what happens. In the demo,
    we will explore both qualitative and quantitative approaches for evaluating model
    explanations.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 评估忠实度的另一种方法是通过插入计算特征重要性，类似于上述方法，但通过逐步展示模型由归因方法识别出的最相关的图像区域。这里的关键思想是：包含重要特征并观察结果。在演示中，我们将探讨评估模型解释的定性和定量方法。
- en: 5\. Feature importance for fine-grained classification
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. 精细分类的特征重要性
- en: In fine-grained classification tasks, such as distinguishing between different
    vehicle types or identifying bird species, small variations in visual appearance
    can significantly affect model predictions. Determining which features are most
    important for the model’s decision-making process can help to shed light on misclassification
    issues, thus allowing to optimize the model on the task. To demonstrate how explainability
    can be effectively applied to leverage understanding on deep learning models for
    vision, we will consider a use case of bird classification. Bird populations are
    important biodiversity indicators, so collecting reliable data of species and
    their interactions across environmental contexts is quite important to ecologists
    [[Atanbori et al., 2016](https://dl.acm.org/doi/10.1016/j.patrec.2015.08.015)].
    In addition, automated bird monitoring systems can also benefit windfarm producers,
    since the construction requires preliminary collision risk assessment and mitigation
    at the design stages [[Croll et al., 2022](https://www.sciencedirect.com/science/article/pii/S0006320722003482)].
    This part will showcase how to apply XAI methods and metrics to enhance model
    explainability in bird species classification (more on the topic can be found
    in the related [article](/bird-by-bird-using-deep-learning-4c0fa81365d7) and [tutorials](https://github.com/slipnitskaya/computer-vision-birds)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在细粒度分类任务中，如区分不同类型的车辆或识别鸟类物种，视觉外观的微小变化可能会显著影响模型的预测结果。确定哪些特征对模型决策过程最为重要，有助于揭示误分类问题，从而能够优化模型的任务表现。为了展示如何有效地应用可解释性方法来加深对深度学习视觉模型的理解，我们将考虑鸟类分类的应用案例。鸟类种群是重要的生物多样性指标，因此收集物种及其在不同环境中的相互作用的可靠数据对生态学家来说至关重要[[Atanbori
    et al., 2016](https://dl.acm.org/doi/10.1016/j.patrec.2015.08.015)]。此外，自动化鸟类监测系统也能为风电场生产商带来好处，因为建设过程中需要在设计阶段进行初步的碰撞风险评估和缓解[[Croll
    et al., 2022](https://www.sciencedirect.com/science/article/pii/S0006320722003482)]。本部分将展示如何应用XAI方法和指标来增强鸟类物种分类模型的可解释性（更多相关内容可以参考[文章](/bird-by-bird-using-deep-learning-4c0fa81365d7)和[教程](https://github.com/slipnitskaya/computer-vision-birds)）。
- en: Figure 8 below presents the feature importance analysis results for fine-grained
    image classification using ResNet-50 pretrained on ImageNet and fine-tuned on
    the Caltech-UCSD Birds-200–2011 dataset. The qualitative assessment of faithfulness
    was conducted for the Guided Grad-CAM method to evaluate the significance of the
    selected features given the model. Quantitative XAI metrics included faithfulness
    via deletion (FTHN), with higher values indicating better faithfulness, alongside
    metrics that reflect the degree of non-robustness and instability, such as maximum
    sensitivity (SENS) and infidelity (INFD), where lower values are preferred. The
    latter metrics are perturbation-based and rely on the assumption that explanations
    should remain consistent with small changes in input data or the model itself
    [[Yeh et al., 2019](https://proceedings.neurips.cc/paper_files/paper/2019/file/a7471fdc77b3435276507cc8f2dc2569-Paper.pdf)].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了使用在ImageNet上预训练并在Caltech-UCSD Birds-200–2011数据集上微调的ResNet-50进行细粒度图像分类的特征重要性分析结果。对Guided
    Grad-CAM方法进行了定性评估，以评估给定模型的选定特征的重要性。定量XAI指标包括通过删除（FTHN）评估的忠实度，值越高表示忠实度越好，同时还有反映非稳健性和不稳定性程度的指标，如最大敏感性（SENS）和不忠实度（INFD），其中较低的值为佳。后者指标基于扰动，依赖于假设解释应随着输入数据或模型本身的微小变化保持一致的前提[[Yeh
    et al., 2019](https://proceedings.neurips.cc/paper_files/paper/2019/file/a7471fdc77b3435276507cc8f2dc2569-Paper.pdf)]。
- en: '![](../Images/329204b9401f7128998f0a56ba6b0305.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/329204b9401f7128998f0a56ba6b0305.png)'
- en: '*Figure 8\. Evaluating explainability metrics for fine-grained image classification
    (Image by author)*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8：评估细粒度图像分类的可解释性指标（图像由作者提供）*'
- en: When evaluating our model on an independent test image of [Northern Cardinal](https://www.allaboutbirds.org/guide/Northern_Cardinal/photo-gallery/63667291),
    we notice that slight changes in the model’s scores during the initial iterations
    are followed by a sharp increase toward the final iteration as the most critical
    features are progressively incorporated (Fig. 8). These results suggest two key
    interpretations regarding the model’s faithfulness with respect to the evaluated
    XAI methods. Firstly, attribution-based interpretability using Guided Grad-CAM
    is faithful to the model, as adding regions identified as redundant (90% of LeRF,
    axis-x) caused minimal changes in the model’s score (less than 0.1 predicted probability
    score). This implies that the model did not rely on these regions when making
    predictions, in contrast to the remaining top 10% of the most relevant features
    identified. Another category — robustness — refers to the model resilience to
    small input variations. Here, we can see that changes in around 90% of the original
    image had little impact on the overall model’s performance, maintaining the target
    probability score despite changes to the majority of pixels, suggesting its stability
    and generalization capabilities for the target class prediction.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当在[北方红雀](https://www.allaboutbirds.org/guide/Northern_Cardinal/photo-gallery/63667291)的独立测试图像上评估我们的模型时，我们注意到在初始迭代过程中模型得分的轻微变化之后，随着最关键特征的逐步加入，最终迭代的得分急剧增加（图8）。这些结果表明，对于评估的XAI方法，模型的忠实性有两个关键解释。首先，使用引导型Grad-CAM的基于归因的可解释性忠实于模型，因为添加被识别为冗余的区域（90%的LeRF，x轴）对模型得分的影响很小（预测概率得分低于0.1）。这意味着模型在做出预测时并没有依赖这些区域，而是依赖于剩余的前10%最相关的特征。另一类——鲁棒性——指的是模型对小的输入变化的抗干扰能力。在这里，我们可以看到，原始图像约90%的变化对模型整体性能几乎没有影响，尽管大部分像素发生变化，模型仍能保持目标概率得分，这表明模型在目标类别预测上的稳定性和泛化能力。
- en: To further assess the robustness of our model, we compute additional metrics,
    such as sensitivity and infidelity [[Yeh et al., 2019](https://proceedings.neurips.cc/paper_files/paper/2019/file/a7471fdc77b3435276507cc8f2dc2569-Paper.pdf)].
    Results indicate that while the model is not overly sensitive to slight perturbations
    in the input (SENS=0.21), the alterations to the top-important regions may potentially
    have an influence on model decisions, in particular, for the top-10% (Fig. 8).
    To perform a more in-depth assessment of the sensitivity of the explanations for
    our model, we can further extend the list of explainability methods, for instance,
    using Integrated Gradients and SHAP [[Lundberg & Lee, 2017](https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html)].
    In addition, to assess model resistance to adversarial attacks, the next steps
    may include quantifying further robustness metrics [[Goodfellow et al., 2015](https://www.semanticscholar.org/reader/bee044c8e8903fb67523c1f8c105ab4718600cdb),
    [Dong et al., 2023](https://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_Benchmarking_Adversarial_Robustness_on_Image_Classification_CVPR_2020_paper.pdf)].
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步评估我们模型的鲁棒性，我们计算了额外的指标，如敏感性和失真[[Yeh et al., 2019](https://proceedings.neurips.cc/paper_files/paper/2019/file/a7471fdc77b3435276507cc8f2dc2569-Paper.pdf)]。结果表明，尽管模型对输入中的轻微扰动不太敏感（SENS=0.21），但对最重要区域的改动可能会对模型决策产生影响，尤其是对于前10%的区域（图8）。为了更深入地评估模型解释的敏感性，我们可以进一步扩展可解释性方法列表，例如使用集成梯度和SHAP[[Lundberg
    & Lee, 2017](https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html)]。此外，为了评估模型对对抗性攻击的抵抗力，下一步可能包括量化更多的鲁棒性指标[[Goodfellow
    et al., 2015](https://www.semanticscholar.org/reader/bee044c8e8903fb67523c1f8c105ab4718600cdb),
    [Dong et al., 2023](https://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_Benchmarking_Adversarial_Robustness_on_Image_Classification_CVPR_2020_paper.pdf)]。
- en: Conclusions
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This article provides a comprehensive overview of scientific literature published
    over past decades encompassing key milestones in deep learning and computer vision
    that laid the foundation of the research in the field of XAI. Reflecting on recent
    technological advances and perspectives in the field, we discussed potential implications
    of XAI in light of emerging AI regulatory frameworks and responsible AI practices,
    anticipating the increased relevance of explainability in the future. Furthermore,
    we examined application domains and explored stakeholders’ groups and their desiderata
    to provide practical suggestions on how XAI can address current challenges and
    needs for creating reliable and trustworthy AI systems. We have also covered fundamental
    concepts and taxonomies related to explainability, commonly used methods and approaches
    used for vision, along with qualitative and quantitative metrics to evaluate post-hoc
    explanations. Finally, to demonstrate how explainability can be applied to leverage
    understanding on deep learning models, the last section presented a case in which
    XAI methods and metrics were effectively applied to a fine-grained classification
    task to identify relevant features affecting model decisions and to perform quantitative
    and qualitative assessment of results to validate quality of the derived explanations
    with respect to model reasoning.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了对过去几十年发表的科学文献的全面概述，涵盖了深度学习和计算机视觉领域的关键里程碑，这些里程碑为XAI（可解释人工智能）领域的研究奠定了基础。回顾这一领域最近的技术进展和前景，我们讨论了在新兴的AI监管框架和负责任的AI实践背景下，XAI可能带来的影响，并预见到可解释性在未来将变得更加重要。此外，我们还考察了XAI的应用领域，探索了各利益相关方的需求，并提出了如何应对当前挑战、满足需求的实践建议，以创造可靠和值得信赖的AI系统。我们还涵盖了与可解释性相关的基本概念和分类法，常用的视觉方法和技术，以及用于评估事后解释的定性和定量指标。最后，为了展示如何利用可解释性加深对深度学习模型的理解，最后一节呈现了一个案例，展示了XAI方法和指标如何有效应用于精细分类任务，以识别影响模型决策的相关特征，并对结果进行定性和定量评估，从而验证所得到的解释的质量，特别是相对于模型推理的合理性。
- en: '**What’s next?**'
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**接下来是什么？**'
- en: In the upcoming article, we will further explore the topic of explainability
    and its practical applications, focusing on how to leverage XAI in design for
    optimizing model performance and reducing classification errors. Interested to
    keep it on? Stay updated on more materials at — [https://github.com/slipnitskaya/computer-vision-birds](https://github.com/slipnitskaya/computer-vision-birds)
    and [https://medium.com/@slipnitskaya](https://medium.com/@slipnitskaya).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的文章中，我们将进一步探讨可解释性的主题及其实际应用，重点讨论如何在设计中利用XAI优化模型性能并减少分类错误。想继续了解吗？请关注更多资料，访问——[https://github.com/slipnitskaya/computer-vision-birds](https://github.com/slipnitskaya/computer-vision-birds)
    和 [https://medium.com/@slipnitskaya](https://medium.com/@slipnitskaya)。
