<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Automate Video Chaptering with LLMs and TF-IDF</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Automate Video Chaptering with LLMs and TF-IDF</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/automate-video-chaptering-with-llms-and-tf-idf-f6569fd4d32b?source=collection_archive---------3-----------------------#2024-09-09">https://towardsdatascience.com/automate-video-chaptering-with-llms-and-tf-idf-f6569fd4d32b?source=collection_archive---------3-----------------------#2024-09-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="5eaf" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Transform raw transcripts into well-structured documents</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://ya-lb.medium.com/?source=post_page---byline--f6569fd4d32b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Yann-Aël Le Borgne" class="l ep by dd de cx" src="../Images/acc1c8b32373d7f345064b89b51869fd.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*I1DOXVG7-91vkO5iX_qjyg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--f6569fd4d32b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://ya-lb.medium.com/?source=post_page---byline--f6569fd4d32b--------------------------------" rel="noopener follow">Yann-Aël Le Borgne</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--f6569fd4d32b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/32d60a6f1a504bcb2a692f7a8846cb0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*V7Oo30P5_V4SePio"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@jakobowens1?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jakob Owens</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="a831" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Video chaptering is the task of segmenting a video into distinct chapters. Besides its use as a navigation aid as seen with YouTube chapters, it is also core to a series of downstream applications ranging from information retrieval (e.g., RAG semantic chunking), to referencing or <a class="af nc" href="https://medium.com/p/d2c1e04e7a7b" rel="noopener">summarization</a>.</p><p id="c5bf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In a recent project, I needed to automate this task and was suprised by the limited options available, especially in the open-source domain. While some professional tools or paid APIs offer such services, I couldn’t find any library or tutorial that provided a sufficiently robust and accurate solution. If you know of any, please share them in the comment!</p><p id="0b9a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And in case you wonder why not simply copy and paste the transcript into a large language model (LLM) and ask for chapter headings, this won’t be effective for two reasons. First, LLMs cannot consistently preserve timestamp information to link them back to chapter titles. Second, LLMs often overlook important sections when dealing with long transcripts.</p><p id="7401" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I therefore ended up designing a custom workflow by relying on LLMs for different language processing subtasks (text formatting, paragraph structuring, chapter segmentation and title generation), and on a <a class="af nc" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">TF-IDF</a> statistics to add timestamps back after the paragraph structuring.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk nz"><img src="../Images/780f882c38449694c48c995a6382887c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Awl0OC_9UcLPBl8d9fYRXw.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The combination of LLMs and TF-IDF allows to efficiently edit and structure a raw transcript while preserving timestamps — See demo on this <a class="af nc" href="https://huggingface.co/spaces/Yannael/video-chaptering" rel="noopener ugc nofollow" target="_blank">HuggingFace space</a></figcaption></figure><p id="cbc1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The resulting workflow turns out to work pretty well, often generating chapters that replicate or enhance YouTube’s suggested ones. The tool additionally allows to export poorly formatted transcripts into well-structured documents, as illustrated on this <a class="af nc" href="https://huggingface.co/spaces/Yannael/video-chaptering" rel="noopener ugc nofollow" target="_blank">HuggingFace space</a>.</p><p id="6207" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This blog post aims at explaining its main steps, which are outlined in the diagram below:</p></div></div><div class="mr"><div class="ab cb"><div class="lm oa ln ob lo oc cf od cg oe ci bh"><figure class="mm mn mo mp mq mr og oh paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk of"><img src="../Images/ec059b1795fb2245cf75e684608ae62c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*SK20_6dCPN0q0zI3EmC0Dw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Proposed workflow for video chaptering, from raw transcript retrieval to structured markdown and Gradio app.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8177" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The key steps in the workflow lie in structuring the transcript in paragraphs (step 2) before grouping the paragraphs into chapters from which a table of contents is derived (step 4). Note that these two steps may rely on different LLMs: A fast and cheap LLM such as LLama 3 8B for the simple task of text editing and paragraph identification, and a more sophisticated LLM such as GPT-4o-mini for the generation of the table of contents. In between, TF-IDF is used to add back timestamp information to the structured paragraphs.</p><p id="71d5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The rest of the post describes each step in more detail.</p><p id="2a67" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Check out the accompanying <a class="af nc" href="https://github.com/Yannael/automatic-video-chaptering" rel="noopener ugc nofollow" target="_blank">Github repository and Colab notebook</a> to explore on your own!</p><h1 id="f03d" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">1) Get the video/audio transcript</h1><p id="f92e" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">Let us use as an example <a class="af nc" href="https://www.youtube.com/watch?v=ErnWZxJovaM" rel="noopener ugc nofollow" target="_blank">the first lecture</a> of the course ‘MIT 6.S191: Introduction to Deep Learning’ (<a class="af nc" href="http://introtodeeplearning.com" rel="noopener ugc nofollow" target="_blank">IntroToDeepLearning.com</a>) by Alexander Amini and Ava Amini<strong class="nf fr"> (</strong><a class="af nc" href="http://introtodeeplearning.com/" rel="noopener ugc nofollow" target="_blank">licensed under the MIT License</a>).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pj"><img src="../Images/5c2f97d5dd2e28cf1b72c2562e1e38cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dS_PfTN2LM9sx9HP5a81_A.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Screenshot of the course YouTube page. Course material is under an MIT licence.</figcaption></figure><p id="c949" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note that chapters are already provided in the video description.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pk"><img src="../Images/01a6fbed9e9f9dff298272404cc8bd7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*O9HQSduQ_LZMZPzYuU61EQ.jpeg"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Chaptering made available in the YouTube description</figcaption></figure><p id="05bd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This provides us with a baseline to qualitatively compare our chaptering later in this post.</p><h2 id="5a35" class="pl oj fq bf ok pm pn po on pp pq pr oq nm ps pt pu nq pv pw px nu py pz qa qb bk"><em class="qc">YouTube transcript API</em></h2><p id="6f85" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">For YouTube videos, an automatically generated transcript is usually made available by YouTube. A convenient way to retrieve that transcript is by calling the <em class="qd">get_transcript</em> method of the Python <em class="qd">youtube_transcript_api</em> library. The method takes the YouTube <em class="qd">video_id </em>library as argument:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="876f" class="qi oj fq qf b bg qj qk l ql qm"># https://www.youtube.com/watch?v=ErnWZxJovaM<br/>video_id = "ErnWZxJovaM" # MIT Introduction to Deep Learning - 2024<br/><br/># Retrieve transcript with the youtube_transcript_api library<br/>from youtube_transcript_api import YouTubeTranscriptApi<br/>transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=["en"])</span></pre><p id="789b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This returns the transcript as a list of text and timestamp key-value pairs:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="49ac" class="qi oj fq qf b bg qj qk l ql qm">[{'text': '[Music]', 'start': 1.17}, <br/>{'text': 'good afternoon everyone and welcome to',  'start': 10.28}, <br/>{'text': 'MIT sus1 191 my name is Alexander amini',  'start': 12.88}, <br/>{'text': "and I'll be one of your instructors for",  'start': 16.84},<br/>...]</span></pre><p id="8d11" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The transcript is however poorly formatted: it lacks punctuation and contains typos (‘MIT sus1 191’ instead of ‘MIT 6.S191', or ‘amini’ instead of ‘Amini’).</p><h2 id="ad1d" class="pl oj fq bf ok pm pn po on pp pq pr oq nm ps pt pu nq pv pw px nu py pz qa qb bk">Speech-to-text with Whisper</h2><p id="e321" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">Alternatively, a speech-to-text library can be used to infer the transcript from a video or audio file. We recommend using <a class="af nc" href="https://github.com/SYSTRAN/faster-whisper" rel="noopener ugc nofollow" target="_blank"><em class="qd">faster-whisper</em></a>, which is a fast implementation of the state-of-the-art open-source <a class="af nc" href="https://github.com/openai/whisper" rel="noopener ugc nofollow" target="_blank"><em class="qd">whisper</em></a> model.</p><p id="a533" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The models come in different size. The most accurate is the ‘large-v3’, which is able to transcribe about 15 minutes of audio per minute on a T4 GPU (available for free on Google Colab).</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="5289" class="qi oj fq qf b bg qj qk l ql qm">from faster_whisper import WhisperModel<br/><br/># Load Whisper model<br/>whisper_model = WhisperModel("large-v3",<br/>                              device="cuda" if torch.cuda.is_available() else "cpu",<br/>                              compute_type="float16",<br/>                            )<br/><br/># Call the Whisper transcribe function on the audio file<br/>initial_prompt = "Use punctuation, like this."<br/>segments, transcript_info = whisper_model.transcribe(audio_file,  initial_prompt=initial_prompt, language="en")</span></pre><p id="a1be" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The result of the transcription is provided as segments which can be easily converted in a list of text and timestamps as with the <em class="qd">youtube_transcript_api</em> library.</p><p id="0a31" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Tip: Whisper may sometimes <a class="af nc" href="https://github.com/openai/whisper/discussions/194" rel="noopener ugc nofollow" target="_blank">not include the punctuation</a>. The<em class="qd"> initial_prompt</em> argument can be used to nudge the model to add punctuation by providing a small sentence containing punctuation.</p><p id="94e4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Below is an excerpt of the transcription of the our video example with whisper large-v3:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="16e7" class="qi oj fq qf b bg qj qk l ql qm">[{'start': 0.0, 'text': ' Good afternoon, everyone, and welcome to MIT Success 191.'},<br/> {'start': 15.28, 'text': " My name is Alexander Amini, and I'll be one of your instructors for the course this year"},<br/> {'start': 19.32, 'duration': 2.08, 'text': ' along with Ava.'}<br/>...]</span></pre><p id="8611" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note that compared to the YouTube transcription, the punctuation is added. Some transcription errors however still remain (‘MIT Success 191’ instead of ‘MIT 6.S191').</p><h1 id="bdde" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">2) Structure the transcript in paragraphs</h1><p id="874c" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">Once a transcript is available, the second stage consists in editing and structuring the transcript in paragraphs.</p><p id="4aef" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Transcript editing refers to changes made to improve readability. This involves, for example, adding punctuation if it is missing, correcting grammatical errors, removing verbal tics, etc.</p><p id="6495" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The structuring in paragraphs also improves readability, and additionnally serves as a preprocessing step for identifying chapters in stage 4, since chapters will be formed by grouping paragraphs together.</p><p id="54f9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Paragraph editing and structuring can be carried out in a single operation, using an LLM. We illustrated below the expected result of this stage:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qn"><img src="../Images/8c5df77d97bd440adef5a65f05b5309f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LdzrYwbAj0D_09Rpd7tnKg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Left: Raw transcript. Right: Edited and structured transcript.</figcaption></figure><p id="b6cc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This task does not require a very sophisticated LLM since it mostly consists in reformulating content. At the time of writing this article, decent results could be obtained with for example GPT-4o-mini or Llama 3 8B, and the following system prompt:</p><blockquote class="qo qp qq"><p id="a92a" class="nd ne qd nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You are a helpful assistant.</p><p id="ab78" class="nd ne qd nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Your task is to improve the user input’s readability: add punctuation if needed and remove verbal tics, and structure the text in paragraphs separated with ‘\n\n’.</p><p id="107f" class="nd ne qd nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Keep the wording as faithful as possible to the original text.</p><p id="af39" class="nd ne qd nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Put your answer within &lt;answer&gt;&lt;/answer&gt; tags.</p></blockquote><p id="df78" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We rely on<a class="af nc" href="https://platform.openai.com/docs/guides/chat-completions" rel="noopener ugc nofollow" target="_blank"> OpenAI compatible chat completion API</a> for LLM calling, with messages having the roles of either ‘system’, ‘user’ or ‘assistant’. The code below illustrates the instantiation of an LLM client with <a class="af nc" href="https://console.groq.com/docs/text-chat" rel="noopener ugc nofollow" target="_blank">Groq</a>, using LLama 3 8B:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="6e69" class="qi oj fq qf b bg qj qk l ql qm"># Connect to Groq with a Groq API key<br/>llm_client = Groq(api_key=api_key)<br/>model = "llama-8b-8192"<br/><br/># Extract text from transcript<br/>transcript_text = ' '.join([s['text'] for s in transcript])<br/><br/># Call LLM<br/>response = client.chat.completions.create(<br/>        messages=[<br/>            {<br/>                "role": "system",<br/>                "content": system_prompt<br/>            },<br/>            {<br/>                "role": "user",<br/>                "content": transcript_text<br/>            }<br/>        ],<br/>        model=model,<br/>        temperature=0,<br/>        seed=42<br/>    )<br/></span></pre><p id="64b7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Given a piece of raw ‘transcript_text’ as input, this returns an edited piece of text within &lt;answer&gt; tags:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="6040" class="qi oj fq qf b bg qj qk l ql qm">response_content=response.choices[0].message.content<br/><br/>print(response_content)<br/>"""<br/>&lt;answer&gt;<br/>Good afternoon, everyone, and welcome to MIT 6.S191. My name is Alexander Amini, and I'll be one of your instructors for the course this year, along with Ava. We're really excited to welcome you to this incredible course.<br/><br/>This is a fast-paced and intense one-week course that we're about to go through together. We'll be covering the foundations of a rapidly changing field, and a field that has been revolutionizing many areas of science, mathematics, physics, and more.<br/><br/>Over the past decade, AI and deep learning have been rapidly advancing and solving problems that we didn't think were solvable in our lifetimes. Today, AI is solving problems beyond human performance, and each year, this lecture is getting harder and harder to teach because it's supposed to cover the foundations of the field.<br/>&lt;/answer&gt;<br/>"""</span></pre><p id="b0cf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let us then extract the edited text from the &lt;answer&gt; tags, divide it into paragraphs, and structure the results as a JSON dictionary consisting of paragraph numbers and pieces of text:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="0d6b" class="qi oj fq qf b bg qj qk l ql qm">import re<br/>pattern = re.compile(r'&lt;answer&gt;(.*?)&lt;/answer&gt;', re.DOTALL)<br/>response_content_edited =  pattern.findall(response_content)<br/>paragraphs = response_content_edited.strip().split('\n\n')<br/>paragraphs_dict = [{'paragraph_number': i, 'paragraph_text': paragraph} for i, paragraph in enumerate(paragraphs)<br/><br/>print(paragraph_dict)<br/><br/>[{'paragraph_number': 0,<br/>  'paragraph_text': "Good afternoon, everyone, and welcome to MIT 6.S191. My name is Alexander Amini, and I'll be one of your instructors for the course this year, along with Ava. We're really excited to welcome you to this incredible course."},<br/> {'paragraph_number': 1,<br/>  'paragraph_text': "This is a fast-paced and intense one-week course that we're about to go through together. We'll be covering the foundations of a rapidly changing field, and a field that has been revolutionizing many areas of science, mathematics, physics, and more."},<br/> {'paragraph_number': 2,<br/>  'paragraph_text': "Over the past decade, AI and deep learning have been rapidly advancing and solving problems that we didn't think were solvable in our lifetimes. Today, AI is solving problems beyond human performance, and each year, this lecture is getting harder and harder to teach because it's supposed to cover the foundations of the field."}]</span></pre><p id="3997" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note that the input should not be too long as the LLM will otherwise ‘forget’ part of the text. For long inputs, the transcript must be split in chunks to improve reliability. We noticed that GPT-4o-mini handles well up to 5000 characters, while Llama 3 8B can only handle up to 1500 characters. The notebook provides the function <em class="qd">transcript_to_paragraphs </em>which takes care of splitting the transcript in chunks.</p><h1 id="5dee" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">3) Infer paragraph timestamps with TF-IDF</h1><p id="17cf" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">The transcript is now structured as a list of edited paragraphs, but the timestamps have been lost in the process.</p><p id="29c5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The third stage consists in adding back timestamps, by inferring which segment in the raw transcript is the closest to each paragraph.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qr"><img src="../Images/61fb2633ad48094b726132474c7d221f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yHkZVEd2KuvX1NSBwsJ9dQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">TF-IDF is used to find which raw transcript segment (right) best matches the beginning of the edited pargagraphs (left).</figcaption></figure><p id="9e84" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We rely for this task on the <a class="af nc" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">TF-IDF metric</a>. TF-IDF stands for <strong class="nf fr">term frequency–inverse document frequency</strong> and is a similarity measure for comparing two pieces of text. The measure works by computing the number of similar words, giving more weight to words which appear less frequently.</p><p id="8c0b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As a preprocessing step, we adjust the transcript segments and paragraph beginnings so that they contain the same number of words. The text pieces should be long enough so that paragraph beginnings can be successfully matched to a unique transcript segment. We find that using 50 words works well in practice.</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="3bb8" class="qi oj fq qf b bg qj qk l ql qm"><br/>num_words = 50<br/><br/>transcript_num_words = transform_text_segments(transcript, num_words=num_words)<br/>paragraphs_start_text = [{"start": p['paragraph_number'], "text": p['paragraph_text']} for p in paragraphs]<br/>paragraphs_num_words = transform_text_segments(paragraphs_start_text, num_words=num_words)<br/></span></pre><p id="b410" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We then rely on the <em class="qd">sklearn</em> library and its<a class="af nc" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> TfidfVectorizer</a> and <a class="af nc" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html#cosine-similarity" rel="noopener ugc nofollow" target="_blank">cosine_similarity</a> function to run TF-IDF and compute similarities between each paragraph beginning and transcript segment. below is an example of code for finding the best match index in the transcript segments for the first paragraph.</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="f3c2" class="qi oj fq qf b bg qj qk l ql qm">from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.metrics.pairwise import cosine_similarity<br/><br/># Paragraph for which to find the timestamp<br/>paragraph_i = 0<br/><br/># Create a TF-IDF vectorizer<br/>vectorizer = TfidfVectorizer().fit_transform(transcript_num_words + paragraphs_num_words)<br/># Get the TF-IDF vectors for the transcript and the excerpt<br/>vectors = vectorizer.toarray()<br/># Extract the TF-IDF vector for the paragraph<br/>paragraph_vector = vectors[len(transcript_num_words) + paragraph_i]<br/><br/># Calculate the cosine similarity between the paragraph vector and each transcript chunk<br/>similarities = cosine_similarity(vectors[:len(transcript_num_words)], paragraph_vector.reshape(1, -1))<br/># Find the index of the most similar chunk<br/>best_match_index = int(np.argmax(similarities))</span></pre><p id="87d3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We wrapped the process in a <em class="qd">add_timestamps_to_paragraphs </em>function, which adds timestamps to paragraphs, together with the matched segment index and text:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="5dfe" class="qi oj fq qf b bg qj qk l ql qm">paragraphs = add_timestamps_to_paragraphs(transcript, paragraphs, num_words=50)<br/><br/>#Example of output for the first paragraph:<br/>print(paragraphs[0])<br/><br/>{'paragraph_number': 0,<br/>  'paragraph_text': "Good afternoon, everyone, and welcome to MIT 6.S191. My name is Alexander Amini, and I'll be one of your instructors for the course this year, along with Ava. We're really excited to welcome you to this incredible course.",<br/>  'matched_index': 1,<br/>  'matched_text': 'good afternoon everyone and welcome to',<br/>  'start_time': 10}</span></pre><p id="bb6c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the example above, the first paragraph (numbered 0) is found to match the transcript segment number 1 that starts at time 10 (in seconds).</p><h1 id="f1c9" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">4) Generate table of contents</h1><p id="f8d1" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">The table of contents is then found by grouping consecutive paragraphs into chapters and identifying meaningful chapter titles. The task is mostly carried out by an LLM, which is instructed to transform an input consisting in a list of JSON paragraphs into an output consisting in a list of JSON chapter titles with the starting paragraph numbers:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="9c18" class="qi oj fq qf b bg qj qk l ql qm">system_prompt_paragraphs_to_toc = """<br/><br/>You are a helpful assistant.<br/><br/>    You are given a transcript of a course in JSON format as a list of paragraphs, each containing 'paragraph_number' and 'paragraph_text' keys.<br/><br/>    Your task is to group consecutive paragraphs in chapters for the course and identify meaningful chapter titles.<br/><br/>    Here are the steps to follow:<br/><br/>1. Read the transcript carefully to understand its general structure and the main topics covered.<br/>2. Look for clues that a new chapter is about to start. This could be a change of topic, a change of time or setting, the introduction of new themes or topics, or the speaker's explicit mention of a new part.<br/>3. For each chapter, keep track of the paragraph number that starts the chapter and identify a meaningful chapter title.<br/>4. Chapters should ideally be equally spaced throughout the transcript, and discuss a specific topic.<br/><br/>    Format your result in JSON, with a list dictionaries for chapters, with 'start_paragraph_number':integer and 'title':string as key:value.<br/>    <br/>    Example: <br/>    {"chapters": <br/>        [{"start_paragraph_number": 0, "title": "Introduction"}, <br/>         {"start_paragraph_number": 10, "title": "Chapter 1"}<br/>        ]<br/>    }<br/>"""<br/></span></pre><p id="83c3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">An important element is to specifically ask for a JSON output, which increases the chances to get a correctly formatted JSON output that can later be loaded back in Python.</p><p id="93e5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">GPT-4o-mini is used for this task, as it is more cost-effective than OpenAI’s GPT-4o and generally provides good results. The instructions are provided through the ‘system’ role, and paragraphs are provided in JSON format through the ‘user’ role.</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="889f" class="qi oj fq qf b bg qj qk l ql qm"># Connect to OpenAI with an OpenAI API key<br/>llm_client_get_toc = OpenAI(api_key=api_key)<br/>model_get_toc = "gpt-4o-mini-2024-07-18"<br/><br/><br/># Dump JSON paragraphs as text<br/>paragraphs_number_text = [{'paragraph_number': p['paragraph_number'], 'paragraph_text': p['paragraph_text']} for p in paragraphs]<br/>paragraphs_json_dump = json.dumps(paragraphs_number_text)<br/><br/># Call LLM<br/>response = client_get_toc.chat.completions.create(<br/>        messages=[<br/>            {<br/>                "role": "system",<br/>                "content": system_prompt_paragraphs_to_toc<br/>            },<br/>            {<br/>                "role": "user",<br/>                "content": paragraphs_json_dump<br/>            }<br/>        ],<br/>        model=model_get_toc,<br/>        temperature=0,<br/>        seed=42<br/>    )</span></pre><p id="bf3b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Et voilà! The call returns the list of chapter titles together with the starting paragraph number in JSON format:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="7659" class="qi oj fq qf b bg qj qk l ql qm">print(response)<br/><br/>{<br/>  "chapters": [<br/>    {<br/>      "start_paragraph_number": 0,<br/>      "title": "Introduction to the Course"<br/>    },<br/>    {<br/>      "start_paragraph_number": 17,<br/>      "title": "Foundations of Intelligence and Deep Learning"<br/>    },<br/>    {<br/>      "start_paragraph_number": 24,<br/>      "title": "Course Structure and Expectations"<br/>    }<br/>....<br/>  ]<br/>}</span></pre><p id="dec4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As in step 2, the LLM may struggle with long inputs and dismiss part of the input. The solution consists again in splitting the input into chunks, which is implemented in the notebook with the <em class="qd">paragraphs_to_toc</em> function and the <em class="qd">chunk_size</em> parameter.</p><h1 id="17dc" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">5) Output structured chapters</h1><p id="9497" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">This last stage combines the paragraphs and the table of contents to create a structured JSON file with chapters, an example of which is provided in the <a class="af nc" href="https://github.com/Yannael/automatic-video-chaptering/blob/master/tmp/ErnWZxJovaM/ErnWZxJovaM.json" rel="noopener ugc nofollow" target="_blank">accompanying Github repository</a>.</p><p id="422a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We illustrate below the resulting chaptering (right), compared to the baseline chaptering that was available from the YouTube description (left):</p></div></div><div class="mr"><div class="ab cb"><div class="lm oa ln ob lo oc cf od cg oe ci bh"><figure class="mm mn mo mp mq mr og oh paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qs"><img src="../Images/3f23d10c1159cfcf70370ae8be3055be.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*uQ7CKIV0NPTZnxNngRGusA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Side-by-side comparison of baseline chaptering from YouTube (left) and ours (right)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="fc89" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The comparison is mostly qualitative as there is no ‘ground truth’. Overall, the approach described in this post identified similar chapters but provides a slightly more refined segmentation of the video. A manual check of both chapterings revealed that the baseline chaptering is off regarding course information, which indeed starts at 9:37 and not at 7:25.</p><p id="3bb6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A couple of other examples of chaptering are given on this <a class="af nc" href="https://huggingface.co/spaces/Yannael/video-chaptering" rel="noopener ugc nofollow" target="_blank">HuggingFace space</a>. The whole workflow is bundled as a Gradio app at the end of the <a class="af nc" href="https://colab.research.google.com/github/Yannael/automatic-video-chaptering/blob/master/video_chapter_generator.ipynb" rel="noopener ugc nofollow" target="_blank">accompanying notebook</a>, making it easier to test it on your own videos.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk nz"><img src="../Images/780f882c38449694c48c995a6382887c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Awl0OC_9UcLPBl8d9fYRXw.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af nc" href="https://colab.research.google.com/github/Yannael/automatic-video-chaptering/blob/master/video_chapter_generator.ipynb" rel="noopener ugc nofollow" target="_blank">Gradio app</a> that bundles the different steps and outputs a well-structured document from a raw transcript</figcaption></figure><h1 id="0a26" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">To go further</h1><ul class=""><li id="b3cf" class="nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny qt qu qv bk"><a class="af nc" href="https://arxiv.org/pdf/2402.17633v1" rel="noopener ugc nofollow" target="_blank">From Text Segmentation to Smart Chaptering:<br/>A Novel Benchmark for Structuring Video Transcriptions</a></li><li id="806b" class="nd ne fq nf b go qw nh ni gr qx nk nl nm qy no np nq qz ns nt nu ra nw nx ny qt qu qv bk"><a class="af nc" href="https://www.notta.ai/en/blog/transcript-summarizer" rel="noopener ugc nofollow" target="_blank">Review of existing solutions for transcript summarizers</a></li><li id="3780" class="nd ne fq nf b go qw nh ni gr qx nk nl nm qy no np nq qz ns nt nu ra nw nx ny qt qu qv bk"><a class="af nc" href="https://paperswithcode.com/task/text-segmentation" rel="noopener ugc nofollow" target="_blank">https://paperswithcode.com/task/text-segmentation</a></li><li id="d61d" class="nd ne fq nf b go qw nh ni gr qx nk nl nm qy no np nq qz ns nt nu ra nw nx ny qt qu qv bk"><a class="af nc" href="https://en.wikipedia.org/wiki/Video_chapter" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Video_chapter</a></li></ul><p id="5dc3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note:</p><ul class=""><li id="b54e" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qt qu qv bk">Unless otherwise noted, all images are by the author</li></ul></div></div></div><div class="ab cb rb rc rd re" role="separator"><span class="rf by bm rg rh ri"/><span class="rf by bm rg rh ri"/><span class="rf by bm rg rh"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="5331" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="qd">Enjoyed this post? Share your thoughts, give it claps, or </em><a class="af nc" href="https://www.linkedin.com/in/yannaelb/" rel="noopener ugc nofollow" target="_blank"><em class="qd">connect with me on LinkedIn</em></a><em class="qd"> .</em></p></div></div></div></div>    
</body>
</html>