- en: Explainable Generic ML Pipeline with MLflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/explainable-generic-ml-pipeline-with-mlflow-2494ca1b3f96?source=collection_archive---------5-----------------------#2024-11-26](https://towardsdatascience.com/explainable-generic-ml-pipeline-with-mlflow-2494ca1b3f96?source=collection_archive---------5-----------------------#2024-11-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An end-to-end demo to wrap a pre-processor and explainer into an algorithm-agnostic
    ML pipeline with `mlflow.pyfunc`
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://menawang.medium.com/?source=post_page---byline--2494ca1b3f96--------------------------------)[![Mena
    Wang, PhD](../Images/eac9fa55026f9fc119bc868439ff311b.png)](https://menawang.medium.com/?source=post_page---byline--2494ca1b3f96--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2494ca1b3f96--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2494ca1b3f96--------------------------------)
    [Mena Wang, PhD](https://menawang.medium.com/?source=post_page---byline--2494ca1b3f96--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2494ca1b3f96--------------------------------)
    ¬∑13 min read¬∑Nov 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/718ec8036048d1449b127442c59434ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Hannah Murrell](https://unsplash.com/@hannahj236?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/person-holding-ball-focus-on-tree-pTfdcT0hxGc?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: '**Intro**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One common challenge in MLOps is the hassle of migrating between various algorithms
    or frameworks. To tackle the challenge, this is my second article on the topic
    of generic model building using `mlflow.pyfunc`.
  prefs: []
  type: TYPE_NORMAL
- en: In my previous article, I offered a beginner-friendly step-by-step demo on creating
    a minimalist algorithm-agnostic model wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/algorithm-agnostic-model-building-with-mlflow-b106a5a29535?source=post_page-----2494ca1b3f96--------------------------------)
    [## Algorithm-Agnostic Model Building with MLflow'
  prefs: []
  type: TYPE_NORMAL
- en: A beginner-friendly step-by-step guide to creating generic ML pipelines using
    mlflow.pyfunc
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/algorithm-agnostic-model-building-with-mlflow-b106a5a29535?source=post_page-----2494ca1b3f96--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'To further our journey, by the end of this article, we will build a much more
    sophisticated ML pipeline with the below functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: This pipeline supports both classification (binary) and regression tasks. It
    works with scikit-learn models and other algorithms that follow the scikit-learn
    interface (i.e., fit, predict/predict_proba).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Incorporating a fully functional `Pre-Processor` that can be fitted on train
    data and then used to transform new data for model consumption. This pre-processor
    can handle both numeric and categorical features and handle missing values with
    various imputation strategies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding an `explainer` to shed light on the model‚Äôs reasoning, which is invaluable
    for model selection, monitoring and implementation. This task can be tricky due
    to the varying implementations of SHAP values across different ML algorithms.
    But, all good, we will address the challenge in this article. üòé
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consistent with the previous article,
  prefs: []
  type: TYPE_NORMAL
- en: You will see how easy it is to switch between different customized pre-processors,
    similar to switching between various ML algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This ML pipeline then encapsulates any customized pipeline elements under the
    hood, yet still offers a unified model representation in `pyfunc` flavour to simplify
    model deployment, redeployment, and downstream scoring.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: üîó All code and config are available [on GitHub](https://github.com/MenaWANG/mlflow-demo/blob/main/pyfunc_pipeline.ipynb).
    üß∞
  prefs: []
  type: TYPE_NORMAL
- en: '**The Pre-Processor (V1)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many machine learning algorithms ‚Äî such as linear models (e.g., linear regression,
    SVM), distance-based models (e.g., KNN, PCA), and gradient-based models (e.g.,
    gradient boosting methods or gradient descent optimization) ‚Äî tend to perform
    better with scaled input features, because scaling prevents features with larger
    ranges from dominating the learning process. Additionally, real-world data often
    contains missing values. Therefore, in this first iteration, we will build a pre-processor
    that can be trained to scale new data and impute missing values, preparing it
    for model consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Once this pre-processor is built, I will then demo how to easily plug it into
    `pyfunc` ML pipeline. Sounds good? Let‚Äôs go. ü§†
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This pre-processor can be fitted on train data and then used to process any
    new data. It will become an element in the ML pipeline below, but of course, we
    can use or test it independently. Let‚Äôs create a synthetic dataset and use the
    pre-processor to transform it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Below are screenshots from {sweetViz} reports before vs after scaling; you can
    see that scaling didn‚Äôt change the underlying shape of each feature‚Äôs distribution
    but simply rescaled and shifted it. BTW, it takes two lines to generate a pretty
    comprehensive EDA report with {sweetViz}, code available in the GitHub repo linked
    above. ü•Ç
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a1b3029a4739afeff34a9d50170c3d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshots from SweetViz reports before vs after preprocessing
  prefs: []
  type: TYPE_NORMAL
- en: '**ML Pipeline with Pre-Processor**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's create an ML pipeline in the `mlflow.pyfunc` flavour that can encapsulate
    this preprocessor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The ML pipeline defined above takes the preprocessor and ML algorithm as parameters.
    Usage example below
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It is as simple as that! üéâ If you want to experiment with another algorithm,
    just swap it like shown below. As a wrapper, it can encapsulate both regression
    and classification algorithms. For the latter, predicted probabilities are returned,
    as shown in the example above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the code chunk below, passing hyperparameters to the algorithms
    is easy, making this ML pipeline a perfect instrument for hyperparameter tuning.
    I will elaborate on this topic in the following articles.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Because this ml pipeline is built in the `mlflow.pyfunc` flavour. We can log
    it with rich metadata saved automatically by `mlflow` for downstream use. When
    deployed, we can feed the metadata as `context` for the model in the `predict`
    function as shown below. More info and demos are available in my previous article,
    which is linked at the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Pre-Processor (V2)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The above pre-processor has worked well so far, but let‚Äôs improve it in two
    ways below and then demonstrate how to swap between pre-processors easily.
  prefs: []
  type: TYPE_NORMAL
- en: Allow users to customize the pre-processing process. For instance, to specify
    the impute strategy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Expand pre-processor capacity to handle categorical features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Easy Switch of Custom Pre-Processors**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There you have it: a new preprocessor that is 1) more customizable and 2) handles
    both numerical and categorical features. Let‚Äôs define an ML pipeline instance
    with it.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs test this new ML pipeline instance with another synthetic dataset containing
    both numerical and categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There you have it‚Äîthe ML pipeline runs smoothly with the new data. As expected,
    however, if we define the ML pipeline with the previous preprocessor and then
    run it on this dataset, we will encounter errors because the previous preprocessor
    was not designed to handle categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The Benefit of An Explainable ML Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Adding an explainer to an ML pipeline can be super helpful in several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Selection**: It helps us select the best model by evaluating the soundness
    of its reasoning. Two algorithms may perform similarly on metrics like AUC or
    precision, but the key features they rely on may differ. Reviewing model reasoning
    with domain experts to discuss which model makes more sense in such scenarios
    is a good idea.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Troubleshooting**: One helpful strategy for model improvement is to analyze
    the reasoning behind mistakes. For example, in classification problems, we can
    identify false positives where the model was most confident (i.e., produced the
    highest predicted possibilities) and investigate what went wrong in the reasoning
    and what key features contributed to the mistakes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model Monitoring**: Besides the typical monitoring elements such as data
    drift and performance metrics, it is informative to monitor model reasoning as
    well. If there is a significant shift in key features that drive the decisions
    made by a model in production, I want to be alerted.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model Implementation**: In some scenarios, supplying model reasoning along
    with model predictions can be highly beneficial to our end users. For example,
    to help a customer service agent best retain a churning customer, we can provide
    the churn score alongside the customer features that contributed to this score.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding An Explainer to the ML Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because our ML pipeline is algorithm agnostic, it is imperative that the explainer
    can also work across algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: SHAP (SHapley Additive exPlanations) values are an excellent choice for our
    purpose because they provide theoretically robust explanations based on game theory.
    They are designed to work consistently across algorithms, including both tree-based
    and non-tree-based models, with some approximations for the latter. Additionally,
    SHAP offers rich visualization capabilities and is widely regarded as an industry
    standard.
  prefs: []
  type: TYPE_NORMAL
- en: In the notebooks below, I have dug into the similarities and differences between
    SHAP implementations for various ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '[SHAP for regressor](https://github.com/MenaWANG/ML_toy_examples/blob/main/explain%20models/shap_basic_regression.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SHAP for XGBoost Classifier](https://github.com/MenaWANG/ML_toy_examples/blob/main/explain%20models/shap_XGB_classification.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SHAP for RandomForest Classifier](https://github.com/MenaWANG/ML_toy_examples/blob/main/explain%20models/shap_basic_RF_classification.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SHAP for LightGBM Classifier](https://github.com/MenaWANG/ML_toy_examples/blob/main/explain%20models/shap_lightgbm_classification.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To create a generic explainer for our ML pipeline, the key differences to address
    are
  prefs: []
  type: TYPE_NORMAL
- en: '***1\. Whether the model is directly supported by*** `***shap.Explainer***`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The model-specific SHAP explainers are significantly more efficient than the
    model-agnostic ones. Therefore, the approach we take here is
  prefs: []
  type: TYPE_NORMAL
- en: first attempts to use the direct SHAP explainer for the model type,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If that fails, falls back to a model-agnostic explainer using the predict function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***2\. The shape of SHAP values***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For binary classification problems, SHAP values can come in two formats/shapes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Format 1**: Only shows impact on positive class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Format 2**: Shows impact on both classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The explainer implementation below always shows the impact on the positive class.
    When the impact on both classes is available in SHAP values, it selects the ones
    on the positive class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please see the code below for the implementation of the approach discussed above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now, the updated ML pipeline instance can create explanatory graphs for you
    in just one line of code. üòé
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/130400a4a9c80519ff731a20d3ede222.png)'
  prefs: []
  type: TYPE_IMG
- en: SHAP plot for global explanation of the model
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b20822a6bb527748b1849d31e4cb83d0.png)'
  prefs: []
  type: TYPE_IMG
- en: SHAP plot for local explanation of any specific case
  prefs: []
  type: TYPE_NORMAL
- en: '**Log and Use the Model**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, you can log a trained ML pipeline using `mlflow` and enjoy all the
    metadata for model deployment and reproducibility. In the screenshot below, you
    can see that in addition to the pickled `pyfunc` model itself, the Python environment,
    metrics, and hyperparameters have all been logged in just a few lines of code
    below. To learn more, please refer to my previous article on `mlflow.pyfunc`,
    which is linked at the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d1ce888009587f800ff1a165eb1fb61a.png)'
  prefs: []
  type: TYPE_IMG
- en: Rich model metadata and artifacts logged with mlflow
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusions & Next Steps**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is it, a generic and explainable ML pipeline that works for both classification
    and regression algorithms. Take the code and extend it to suit your use case.
    ü§ó If you find this useful, please give me a clap üëèü•∞
  prefs: []
  type: TYPE_NORMAL
- en: To further our journey on the `mlflow.pyfunc` series, below are some topics
    I am considering. Feel free to leave a comment and let me know what you would
    like to see. ü•∞
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If instead of *choosing* between off-the-shelf algorithms, one decides to *ensemble*
    multiple algorithms or have highly customized solutions, they can still enjoy
    a generic model representation and seamless migration via `mlflow.pyfunc`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stay tuned and follow me on [Medium](https://menawang.medium.com/). üòÅ
  prefs: []
  type: TYPE_NORMAL
- en: üíº[LinkedIn](https://www.linkedin.com/in/mena-ning-wang/) | üò∫[GitHub](https://github.com/MenaWANG)
    | üïäÔ∏è[Twitter/X](https://x.com/mena_wang)
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are by the author.
  prefs: []
  type: TYPE_NORMAL
