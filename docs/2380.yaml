- en: Can Transformers Solve Everything?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: transformers能解决一切问题吗？
- en: 原文：[https://towardsdatascience.com/can-transformers-solve-everything-0421ebaf9f8e?source=collection_archive---------1-----------------------#2024-10-01](https://towardsdatascience.com/can-transformers-solve-everything-0421ebaf9f8e?source=collection_archive---------1-----------------------#2024-10-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/can-transformers-solve-everything-0421ebaf9f8e?source=collection_archive---------1-----------------------#2024-10-01](https://towardsdatascience.com/can-transformers-solve-everything-0421ebaf9f8e?source=collection_archive---------1-----------------------#2024-10-01)
- en: Looking into the math and the data reveals that transformers are both overused
    and underused.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看数学和数据后发现，transformers既被过度使用，也被低估。
- en: '[](https://medium.com/@crackalamoo?source=post_page---byline--0421ebaf9f8e--------------------------------)[![Harys
    Dalvi](../Images/cf7fa3865063408efd1fd4c0b4b603db.png)](https://medium.com/@crackalamoo?source=post_page---byline--0421ebaf9f8e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0421ebaf9f8e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0421ebaf9f8e--------------------------------)
    [Harys Dalvi](https://medium.com/@crackalamoo?source=post_page---byline--0421ebaf9f8e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@crackalamoo?source=post_page---byline--0421ebaf9f8e--------------------------------)[![Harys
    Dalvi](../Images/cf7fa3865063408efd1fd4c0b4b603db.png)](https://medium.com/@crackalamoo?source=post_page---byline--0421ebaf9f8e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0421ebaf9f8e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0421ebaf9f8e--------------------------------)
    [Harys Dalvi](https://medium.com/@crackalamoo?source=post_page---byline--0421ebaf9f8e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0421ebaf9f8e--------------------------------)
    ·13 min read·Oct 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0421ebaf9f8e--------------------------------)
    ·阅读时长13分钟·2024年10月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Transformers are best known for their applications in natural language processing.
    They were originally designed for translating between languages,[[1](https://arxiv.org/pdf/1706.03762)]
    and are now most famous for their use in large language models like ChatGPT (generative
    pretrained *transformer*).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: transformers最为人知的是它们在自然语言处理中的应用。它们最初是为语言翻译而设计的[[1](https://arxiv.org/pdf/1706.03762)]，现在因其在大型语言模型中的应用而最为著名，例如ChatGPT（生成预训练的*transformer*）。
- en: But since their introduction, transformers have been applied to ever more tasks,
    with great results. These include image recognition,[[2](https://arxiv.org/abs/2010.11929)]
    reinforcement learning,[[3](https://arxiv.org/abs/2106.01345)] and even weather
    prediction.[[4](https://arxiv.org/abs/2312.03876)]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 但是自从它们被引入以来，transformers已经被应用于越来越多的任务，并取得了很好的结果。这些任务包括图像识别[[2](https://arxiv.org/abs/2010.11929)]、强化学习[[3](https://arxiv.org/abs/2106.01345)]，甚至是天气预测[[4](https://arxiv.org/abs/2312.03876)]。
- en: Even the seemingly specific task of language generation with transformers has
    a number of surprises, as we’ve already seen. Large language models have emergent
    properties that feel more intelligent than just predicting the next word. For
    example, they may know various facts about the world, or replicate nuances of
    a person’s style of speech.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是看似特定的任务，如使用transformers进行语言生成，也有许多令人惊讶的地方，正如我们已经看到的那样。大型语言模型具有涌现的属性，表现得比单纯预测下一个词更智能。例如，它们可能知道关于世界的各种事实，或复现一个人讲话风格的细微差别。
- en: The success of transformers has made some people ask the question of whether
    transformers can do everything. If transformers generalize to so many tasks, is
    there any reason *not* to use a transformer?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: transformers的成功让一些人提出了一个问题，那就是transformers是否能解决所有问题。如果transformers能推广到这么多任务，难道就没有理由*不*使用transformer吗？
- en: Clearly, there is still a case for other machine learning models and, as is
    often forgotten these days, non-machine learning models and human intellect. But
    transformers do have a number of unique properties, and have shown incredible
    results so far. There is also a considerable mathematical and empirical basis…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，仍然需要其他机器学习模型，并且如今天常被忽视的那样，非机器学习模型和人类智力也有其存在的意义。但transformers确实具有一些独特的属性，并且迄今为止已经展现出惊人的结果。它们也有相当的数学和经验基础……
