- en: Integrating Text and Images for Smarter Data Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/integrating-text-and-images-for-smarter-data-classification-6a53252d8a73?source=collection_archive---------6-----------------------#2024-11-18](https://towardsdatascience.com/integrating-text-and-images-for-smarter-data-classification-6a53252d8a73?source=collection_archive---------6-----------------------#2024-11-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@CVxTz?source=post_page---byline--6a53252d8a73--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--6a53252d8a73--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6a53252d8a73--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6a53252d8a73--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--6a53252d8a73--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6a53252d8a73--------------------------------)
    ·7 min read·Nov 18, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: A technical walk-through on leveraging multi-modal AI to classify mixed text
    and image data, including detailed instructions, executable code examples, and
    tips for effective implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88c05e548892522a34fec7ecb8d4b602.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Tschernjawski Sergej](https://unsplash.com/@mrt1987?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In AI, one of the most exciting areas of growth is **multimodal learning**,
    where models process and combine different types of data — such as images and
    text — to better understand complex scenarios. This approach is particularly useful
    in real-world applications where information is often split between text and visuals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take e-commerce as an example: a product listing might include an image showing
    what an item looks like and a description providing details about its features.
    To fully classify and understand the product, both sources of information need
    to be considered together. Multimodal large language models (LLMs) like **Gemini
    1.5**, **Llama 3.2, Phi-3 Vision**, and open-source tools such as **LlaVA, DocOwl**
    have been developed specifically to handle these types of inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: Why Multimodal Models Are Important
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Information from images and text can complement each other in ways that single-modality
    systems might miss:'
  prefs: []
  type: TYPE_NORMAL
- en: A product’s description might mention its dimensions or material, which isn’t
    clear from the image alone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, an image might reveal key aspects like style or color that
    text can’t adequately describe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we only process images or text separately, we risk missing critical details.
    Multimodal models address this challenge by combining both sources during processing,
    resulting in more accurate and useful outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: What You’ll Learn in This Tutorial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This tutorial will guide you through creating a pipeline designed to handle
    **image-text classification**. You’ll learn how to process and analyze inputs
    that combine visual and textual elements, achieving results that are more accurate
    than those from text-only systems.
  prefs: []
  type: TYPE_NORMAL
- en: If your project involves text-only classification, you might find my [other
    blog post](https://medium.com/towards-data-science/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605)
    helpful — it focuses specifically on those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605?source=post_page-----6a53252d8a73--------------------------------)
    [## Building a Reliable Text Classification Pipeline with LLMs: A Step-by-Step
    Guide'
  prefs: []
  type: TYPE_NORMAL
- en: Overcoming common challenges in LLM-based text classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605?source=post_page-----6a53252d8a73--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'To successfully build a multimodal image-text classification system, we’ll
    need three essential components. Here’s a breakdown of each element:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. A Reliable LLM Provider
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The backbone of this tutorial is a **hosted LLM as a service**. After experimenting
    with several options, I found that not all LLMs deliver consistent results, especially
    when working with structured outputs. Here’s a summary of my experience:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Groq** and **Fireworks.ai**: These platforms offer multimodal LLMs in a serverless,
    pay-per-token format. While they seem promising, their APIs had issues following
    structured output requests. For example, when sending a query with a predefined
    schema, the returned output didn’t adhere to the expected format, making them
    unreliable for tasks requiring precision. Groq’s Llama 3.2 is still in preview
    so maybe I’ll try them again later. Fireworks.ai don’t typically respond to bug
    reports so I’ll just remove them from my options from now on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gemini 1.5**: After some trial and error, I settled on Gemini 1.5\. It consistently
    returned results in the desired format and has been working very ok so far. Though
    it still has its own weird quirks that you will find if you poke at it long enough
    (like the fact that you can’t use enums that are too large…). We will discuss
    them later in the post. This will be the LLM we use for this tutorial.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2\. The Python Library: LangChain'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To interface with the LLM and handle multimodal inputs, we’ll use the **LangChain**
    library. LangChain is particularly well-suited for this task because it allows
    us to:'
  prefs: []
  type: TYPE_NORMAL
- en: Inject both text and image data as input to the LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defines common abstraction for different LLM as a service providers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define structured output schemas to ensure the results match the format we need.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured outputs are especially important for classification tasks, as they
    involve predefined classes that the output must conform to. LangChain ensures
    this structure is enforced, making it ideal for our use case.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. The Classification Task: Keyword Suggestion for Photography Images'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The task we’ll focus on in this tutorial is **keyword suggestion** for photography-related
    images. This is a **multi-label classification** problem, meaning that:'
  prefs: []
  type: TYPE_NORMAL
- en: Each image can belong to more than one class simultaneously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The list of possible classes is predefined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, an input consisting of an image and its description might be classified
    with keywords like *landscape, sunset,* and *nature*. While multiple keywords
    can apply to a single input, they must be selected from the predefined set of
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step-by-Step Guide: Setting Up Multimodal Image-Text Classification with Gemini
    1.5 and LangChain'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the foundational concepts covered, let’s dive into the implementation.
    This step-by-step guide will walk you through configuring Gemini 1.5, setting
    up LangChain, and building a keyword suggestion system for photography-related
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Obtain Your Gemini API Key'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step is to get your **Gemini API key**, which you can generate in
    [Google AI Studio](https://aistudio.google.com/app/apikey). Once you have your
    key, export it to an environment variable called `GOOGLE_API_KEY`. You can either:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add it to a `.env` file:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Export it directly in your terminal:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: Install and Initialize the Client'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, install the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once installed, initialize the client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Define the Output Schema'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To ensure the LLM produces valid, structured results, we use **Pydantic** to
    define an output schema. This schema acts as a filter, validating that the categories
    returned by the model match our predefined list of acceptable values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Why** `**field_validator**` **Is Needed as a Workaround:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'While defining the schema, we encountered a limitation in Gemini 1.5 (and similar
    LLMs): they do not strictly enforce **enums**. This means that even though we
    provide a fixed set of categories, the model might return values outside this
    set. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Expected: `["landscape", "forest", "mountain"]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returned: `["landscape", "ocean", "sun"]` *(with "ocean" and "sun" being invalid
    categories)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Without handling this, the invalid categories could cause errors or degrade
    the classification’s accuracy. To address this, the `field_validator` method is
    used as a workaround. It acts as a filter, ensuring:'
  prefs: []
  type: TYPE_NORMAL
- en: Only valid categories from `list_classes` are included in the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Invalid or unexpected values are removed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This safeguard ensures the model’s results align with the task’s requirements.
    It is annoying we have to do this but it seems to be a common issue for all LLM
    providers I tested, if you know of one that handles Enums well let me know please.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Bind the Schema to the LLM Client'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, bind the schema to the client for structured output handling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 5: Build the Query and Call the LLM'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Define the prediction function to send image and text inputs to the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To send image data to the Gemini LLM API, we need to encode the image into a
    format the model can process. This is where **base64 encoding** comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is Base64?**'
  prefs: []
  type: TYPE_NORMAL
- en: Base64 is a binary-to-text encoding scheme that converts binary data (like an
    image) into a text format. This is useful when transmitting data that might otherwise
    be incompatible with text-based systems, such as APIs. By encoding the image into
    base64, we can include it as part of the payload when sending data to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: Get Results as Multi-Label Keywords'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, run the classifier and see the results. Let’s test it with an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example Input 1:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Image**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6a7a0ee9460973d78f72f4b816a1fbed.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Calvin Ma](https://unsplash.com/@mkwcalvin?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/classic-red-and-white-bus-parked-beside-road-VaH2X8eHKVg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: '**Description**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: classic red and white bus parked beside road
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Result:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Image + Text**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Text Only**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As shown, when using both text and image inputs, the results are more relevant
    to the actual content. With text-only input, the LLM gave correct but incomplete
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example Input 2:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Image**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/124e22659b64a9cdbe4ec3d8bdcf71bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Tadeusz Lakota](https://unsplash.com/@tadekl?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/black-and-white-coated-dog-bLQFCJDImnc?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: '**Description**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: black and white coated dog
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Result:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image + Text**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Text Only**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multimodal classification, which combines text and image data, provides a way
    to create more contextually aware and effective AI systems. In this tutorial,
    we built a keyword suggestion system using Gemini 1.5 and LangChain, tackling
    key challenges like structured output handling and encoding image data.
  prefs: []
  type: TYPE_NORMAL
- en: By blending text and visual inputs, we demonstrated how this approach can lead
    to more accurate and meaningful classifications than using either modality alone.
    The practical examples highlighted the value of combining data types to better
    capture the full context of a given scenario.
  prefs: []
  type: TYPE_NORMAL
- en: What’s Next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This tutorial focused on text and image classification, but the principles
    can be applied to other multimodal setups. Here are some ideas to explore next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text and Video**: Extend the system to classify or analyze videos by integrating
    video frame sampling along with text inputs, such as subtitles or metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text and PDFs**: Develop classifiers that handle documents with rich content,
    like scientific papers, contracts, or resumes, combining visual layouts with textual
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-World Applications**: Integrate this pipeline into platforms like e-commerce
    sites, educational tools, or social media moderation systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These directions demonstrate the flexibility of multimodal approaches and their
    potential to address diverse real-world challenges. As multimodal AI evolves,
    experimenting with various input combinations will open new possibilities for
    more intelligent and responsive systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Full code: [llmclassifier/llm_multi_modal_classifier.py](https://github.com/CVxTz/llmclassifier/blob/master/llmclassifier/llm_multi_modal_classifier.py)'
  prefs: []
  type: TYPE_NORMAL
