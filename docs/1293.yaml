- en: Language as a Universal Learning Machine
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言作为一种通用学习机器
- en: 原文：[https://towardsdatascience.com/language-as-a-universal-learning-machine-d2c67cb15e5f?source=collection_archive---------7-----------------------#2024-05-23](https://towardsdatascience.com/language-as-a-universal-learning-machine-d2c67cb15e5f?source=collection_archive---------7-----------------------#2024-05-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/language-as-a-universal-learning-machine-d2c67cb15e5f?source=collection_archive---------7-----------------------#2024-05-23](https://towardsdatascience.com/language-as-a-universal-learning-machine-d2c67cb15e5f?source=collection_archive---------7-----------------------#2024-05-23)
- en: 'LANGUAGE PROCESSING IN HUMANS AND COMPUTERS: Part 4'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类与计算机的语言处理：第4部分
- en: Saying is believing. Seeing is hallucinating.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 说出来就是相信，看到就是幻觉。
- en: '[](https://medium.com/@dusko_p?source=post_page---byline--d2c67cb15e5f--------------------------------)[![Dusko
    Pavlovic](../Images/3d242896266291f7adbf6f131fe2e16d.png)](https://medium.com/@dusko_p?source=post_page---byline--d2c67cb15e5f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d2c67cb15e5f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d2c67cb15e5f--------------------------------)
    [Dusko Pavlovic](https://medium.com/@dusko_p?source=post_page---byline--d2c67cb15e5f--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dusko_p?source=post_page---byline--d2c67cb15e5f--------------------------------)[![Dusko
    Pavlovic](../Images/3d242896266291f7adbf6f131fe2e16d.png)](https://medium.com/@dusko_p?source=post_page---byline--d2c67cb15e5f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d2c67cb15e5f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d2c67cb15e5f--------------------------------)
    [Dusko Pavlovic](https://medium.com/@dusko_p?source=post_page---byline--d2c67cb15e5f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d2c67cb15e5f--------------------------------)
    ·40 min read·May 23, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d2c67cb15e5f--------------------------------)
    ·40分钟阅读·2024年5月23日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Machine-learned language models have transformed everyday life: they steer
    us when we study, drive, manage money. They have the potential to transform our
    civilization. But they hallucinate. Their realities are virtual. This 4th part
    of the series on language processing provides a high-level overview of low-level
    details of how the learning machines work. It turns out that, even after they
    become capable of recognizing hallucinations and dreaming safely, as humans tend
    to be, the learning machines will proceed to form broader systems of false beliefs
    and self-confirming theories, as humans tend to do.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习语言模型已经改变了我们的日常生活：它们在我们学习、驾驶、理财时为我们提供指导。它们有潜力改变我们的文明。但它们也会产生幻觉。它们的现实是虚拟的。本系列的第4部分提供了一个关于学习机器如何工作的低层次细节的高级概述。事实证明，即使它们能够像人类一样识别幻觉并安全地做梦，学习机器仍然会像人类一样，形成更广泛的错误信念和自我验证的理论体系。
- en: '[I tried to make this text readable for all. Skipping the math underpinnings
    provided with some claims shouldn’t impact the later claims. Even just the pictures
    at the beginning and at the end are hoped to convey the main message. Suggestions
    for improvements are welcome :)]'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[我尝试让这篇文章对所有人都易于阅读。省略一些数学推导应该不会影响后续的论点。即使仅仅是开始和结尾的图片，也希望能传达主要信息。欢迎提出改进建议：)]'
- en: 'Part 1 was:'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1部分是：
- en: '[Who are chatbots (and what are they to you)?](https://medium.com/towards-data-science/who-are-chatbots-and-what-are-they-to-you-5c77d9201d11)
    Afterthoughts: [Four elephants in a room with chatbots](https://medium.com/towards-data-science/four-elephants-in-the-room-with-chatbots-82c48a823b94)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[聊天机器人是谁（它们对你来说意味着什么）？](https://medium.com/towards-data-science/who-are-chatbots-and-what-are-they-to-you-5c77d9201d11)
    后续思考： [房间里的四只大象与聊天机器人](https://medium.com/towards-data-science/four-elephants-in-the-room-with-chatbots-82c48a823b94)'
- en: 'Part 2 was:'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2部分是：
- en: '[Syntax: The Language Form](https://medium.com/towards-data-science/syntax-the-language-form-612257c4aa5f)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[语法：语言形式](https://medium.com/towards-data-science/syntax-the-language-form-612257c4aa5f)'
- en: 'Part 3 was:'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3部分是：
- en: '[Semantics: The Meaning of Language](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[语义学：语言的意义](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41)'
- en: 'THIS IS Part 4:'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这是第4部分：
- en: '[Language models, celebrities, and steam engines](#3cda)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[语言模型、名人和蒸汽机](#3cda)'
- en: '[Evolution of learning](#63f9)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[学习的演变](#63f9)'
- en: '[2.1\. Learning causes and superstitions](#deb1)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2.1\. 学习的原因和迷信](#deb1)'
- en: '[2.2\. General learning framework](#e77a)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2.2\. 一般学习框架](#e77a)'
- en: '[2.3\. Examples: From pigeons to perceptrons](#0f24)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2.3\. 示例：从鸽子到感知机](#0f24)'
- en: 3\. [Learning functions](#12c0)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. [学习函数](#12c0)
- en: '[3.1\. Why learning is possible](#4211)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3.1\. 为什么学习是可能的](#4211)'
- en: '[3.2\. Decomposing continuous functions: Kolmogorov-Arnold](#ca7b)[⁶](#d246)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3.2\. 分解连续函数：Kolmogorov-Arnold](#ca7b)[⁶](#d246)'
- en: '[3.3\. Wide learning](#e045)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3.3\. 广泛学习](#e045)'
- en: '[3.4\. Approximating continuous functions: Cybenko et al](#ece6)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3.4\. 逼近连续函数：Cybenko等](#ece6)'
- en: '[3.5\. Deep learning](#8395)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3.5\. 深度学习](#8395)'
- en: 4\. [Learning channels and paying attention](#0e5b)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. [学习通道与注意力](#0e5b)
- en: '[4.1 Channeling through concepts](#4f8f)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4.1 通过概念引导](#4f8f)'
- en: '[4.2 Static channel learning: RNN, LSTM…](#b1bf)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4.2 静态通道学习：RNN、LSTM…](#b1bf)'
- en: '[4.3 Dynamic channel learning: Attention, Transformer…](#1d50)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4.3 动态通道学习：注意力、变压器…](#1d50)'
- en: 5\. [Beyond hallucinations](#50f8)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. [超越幻觉](#50f8)
- en: '[5.1\. Parametric learning framework](#16d1)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5.1\. 参数化学习框架](#16d1)'
- en: '[5.2\. Self-learning](#47ec)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5.2\. 自我学习](#47ec)'
- en: '[5.3\. Self-confirming beliefs](#72b2)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5.3\. 自我确认的信念](#72b2)'
- en: '[Attributions](#aa48)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[归属](#aa48)'
- en: '[Notes](http://f974)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记](http://f974)'
- en: 1\. Language models, celebrities, and steam engines
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 语言模型、名人和蒸汽机
- en: Anyone can drive a car. Most people even know what the engine looks like. But
    when you need to fix it, you need to figure out how it works.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 任何人都可以开车。大多数人甚至知道发动机长什么样。但是当你需要修理它时，你需要弄清楚它是如何工作的。
- en: Anyone can chat with a chatbot. Most people know that there is a Large Language
    Model (LLM) under the hood. There are lots and lots and lots of articles describing
    what an LLM looks like. Lots of colorful pictures. Complicated meshes of small
    components, as if both mathematical abstraction and modular programming still
    wait to be invented. YouTube channels with fresh scoops on LLM celebrities. We
    get to know their parts and how they are connected, we know their performance,
    we even see how each of them changes a heat map of inputs to a heat map of outputs.
    One hotter than the other. But do we understand how they work? Experts say that
    they do, but they don’t seem to be able to explain it even to each other, as they
    continue to disagree about pretty much everything.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 任何人都可以和聊天机器人对话。大多数人知道在背后有一个大型语言模型（LLM）。有很多文章描述了LLM是什么样子的。很多五彩斑斓的图片。复杂的小组件网状结构，好像数学抽象和模块化编程还等着被发明一样。YouTube频道上有关于LLM名人的新鲜资讯。我们了解它们的各个部分及其连接方式，知道它们的性能，甚至看到每个部分如何将输入的热图转化为输出的热图，一个比一个热。但是我们真的理解它们是如何工作的么？专家说他们理解，但似乎连彼此之间都无法解释清楚，因为他们在几乎所有问题上都有分歧。
- en: Every child, of course, knows that it can be hard to explain what you just built.
    Our great civilization built lots of stuff that it couldn’t explain. Steam engines
    have been engineered for nearly 2000 years before scientists explained how they
    extract work from heat. There aren’t many steam engines around anymore, but there
    are lots of language engines and a whole industry of scientific explanations how
    they extract sense from references. The leading theory is that Santa Claus descended
    from the mountain and gave us the transformer architecture carved in a stone tablet.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，每个孩子都知道，解释自己刚刚建造的东西可能很困难。我们的伟大文明建造了很多无法解释的东西。蒸汽机已经被工程化近2000年，才有科学家解释它们是如何从热量中提取工作的。如今蒸汽机已经不多见，但有很多语言引擎，还有一个完整的科学解释行业，讲述它们如何从引用中提取意义。领先的理论是，圣诞老人从山上下来，给我们留下了刻在石板上的变压器架构。
- en: '![](../Images/3bcce5a27ad24729bcbacb2d352afb7c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bcce5a27ad24729bcbacb2d352afb7c.png)'
- en: Transformers changed the world, spawned offspring and competitors. . . Just
    like steam engines. Which may be a good thing, since steam engines did not exterminate
    their creators just because the creators didn’t understand them.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器改变了世界，催生了后代和竞争者……就像蒸汽机一样。这可能是件好事，因为蒸汽机并没有因为它们的创造者不理解它们而将它们的创造者消灭。
- en: I wasn’t around in the times of steam engines, but I was around in the times
    of bulky computers, and when the web emerged and everything changed, and when
    the web giants emerged and changed the web. Throughout that time, AI research
    seemed like an effort towards the intelligent design of intelligence. It didn’t
    change anything, because intelligence, like life, is an evolutionary process*,*
    not a product of intelligent design[¹](#858d).But now some friendly learning machines
    and chatbot AIs evolved and everything is changing again. Having survived and
    processed the paradigm shifts of the past, I am trying to figure out the present
    one. Hence this course and these writings. On one hand, I probably stand no chance
    to say anything that hasn’t been said before. Even after a lot of honest work,
    I remain a short-sighted non-expert. On the other hand, there are some powerful
    tools and ideas that evolved in the neighborhood of AI that AI experts don’t seem
    to be aware of. People clump into research communities, focus on the same things,
    and ignore the same things. Looking over the fences, neighbors sometimes understand
    neighbors better than they understand themselves. This sometimes leads to trouble.
    An ongoing temptation. Here is a view over the fence.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我没经历过蒸汽机时代，但我经历过笨重的计算机时代，也亲历了网络的出现以及一切发生变化的时期，还经历了网络巨头的崛起和他们改变网络的过程。在那段时间里，人工智能研究似乎像是智能设计智能的一种努力。它并没有改变什么，因为智能，像生命一样，是一个*进化的过程*，而不是智能设计的产物[¹](#858d)。但现在，一些友好的学习机器和聊天机器人AI已经进化，一切又在发生变化。在经历了过去的范式转变后，我正在试图理解这一当前的转变。因此有了这门课程和这些文字。从一方面讲，我可能没有机会说出任何未曾有人提到过的东西。即使做了很多诚实的工作，我仍然是一个目光短浅的非专家。另一方面，在人工智能的领域，出现了一些强大的工具和思想，AI专家似乎并未意识到这些。人们会聚集在研究社区中，集中关注相同的事情，却忽视了相同的事情。站在围栏旁，邻居有时能比自己更好地理解邻居。这有时会导致麻烦。一种持续的诱惑。这是一个站在围栏外的视角。
- en: 2\. Evolution of learning
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 学习的演化
- en: '[2.1\. Learning causes and superstitions](http://deb1)'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[2.1\. 学习的原因和迷信](http://deb1)'
- en: Spiders are primed to build spider webs. Their engineering skills to weave webs
    are programmed in their genes. They are pretrained builders and even their capability
    to choose and remember a good place for a web is automated.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 蜘蛛天生就会织蜘蛛网。它们编织网的工程技能是通过基因程序化的。它们是预先训练好的建造者，甚至它们选择和记住一个适合织网的好地方的能力也是自动化的。
- en: Dogs and pigeons are primed to seek food. Their capabilities to learn sources
    and actions that bring food are automated. In a famous experiment, physiologist
    Pavlov studied one of the simplest forms of learning, usually called *conditioning*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 狗和鸽子天生就会寻找食物。它们学会寻找带来食物的来源和行为的能力是自动化的。在一项著名的实验中，生理学家巴甫洛夫研究了最简单的学习形式之一，通常被称为*条件反射*。
- en: '![](../Images/8ae6dd8b177d587867dbbe024c5abada.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ae6dd8b177d587867dbbe024c5abada.png)'
- en: If the bell rings whenever the dog is fed, he learns to salivate whenever the
    bell rings.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每次喂狗时铃声响起，他就会学会每当铃声响起时分泌唾液。
- en: Continuing in the same vein, psychologist Skinner showed that pigeons could
    even develop a form of superstition, also by trying to learn where the food comes
    from.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 延续这一思路，心理学家斯金纳展示了鸽子甚至能够发展出一种迷信行为，同样是通过试图学习食物的来源。
- en: '![](../Images/7fca4475cde3c65f6bc8d6df78157582.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fca4475cde3c65f6bc8d6df78157582.png)'
- en: If food arrives while the pigeon is pecking, she learns that pecking conjures
    food
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果食物在鸽子啄食时到来，她就会学会啄食能够引来食物。
- en: Skinner fed pigeons at completely random times, with no correlation with their
    behaviors. About 70% of them developed beliefs that they could conjure food. If
    a pigeon happened to be pecking on the ground, or ruffling feathers just before
    the food arrived, this would make them engage in this action more frequently,
    which increased the chance that the food would arrive while they were performing
    that action. If one of the random associations, say of food and pecking, after
    a while prevails, then it gets promoted into a ritual dance for food. Each time,
    the food eventually arrives and confirms that the ritual works.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 斯金纳在完全随机的时间喂鸽子，行为和喂食没有任何关联。约有70%的鸽子发展出了它们可以召唤食物的信念。如果一只鸽子恰好在食物到来之前在地面啄食，或在食物到达之前整理羽毛，这会使它们更频繁地进行这些动作，从而增加在这些行为发生时食物到来的概率。如果某个随机的联结，比如食物和啄食，在一段时间后占主导地位，那么它就会变成一种寻求食物的仪式舞蹈。每次食物最终都会到来，并确认仪式有效。
- en: Humans are primed to seek causes and predict effects. Like pigeons, they associate
    coinciding events as correlated and develop superstitions, promoting coincidences
    into causal theories. While pigeons end up pecking empty surfaces to conjure grains,
    humans build monumental systems of false beliefs, attributing their fortunes and
    misfortunes, say, to the influence of stars millions of light years away, or to
    their neighbor’s evil eye, or to pretty much anything that can be seen, felt,
    or counted[²](#a74e).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 人类天生倾向于寻找原因并预测结果。像鸽子一样，他们将巧合的事件关联为相关事件，并发展出迷信，将巧合升华为因果理论。当鸽子最终啄空的表面来召唤谷粒时，人类则建立起庞大的虚假信仰体系，归因于星星数百万光年外的影响，或邻居的恶意眼光，或者几乎任何可以看到、感受到或计算的事物[²](#a74e)。
- en: But while our causal beliefs are shared with pigeons, our capabilities to build
    houses and span bridges are not shared with spiders. Unlike spiders, we are not
    primed to build but have to *learn* our engineering skills. *We are primed to
    learn.*
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 但尽管我们的因果信念与鸽子相似，我们建造房屋和跨越桥梁的能力却与蜘蛛不同。与蜘蛛不同，我们不是天生会建造，而是必须*学习*我们的工程技能。*我们天生会学习*。
- en: 2.2\. General learning framework
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2. 一般学习框架
- en: 'A bird’s eye view of the scene of learning looks something like this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 学习场景的鸟瞰图大致如下：
- en: '![](../Images/0c994833ec2d8ac882a394db11a8c84a.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c994833ec2d8ac882a394db11a8c84a.png)'
- en: 'The inputs come from the left. The main characters are:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 输入从左边进入。主要角色是：
- en: a process *F*, the *supervisor* in supervised learning ([Turing called it a
    “teacher”](https://medium.com/p/5c77d9201d11#e1d2)) processing input data *x*
    of type *X* to produce output classes or parameters *y* of type *Y*;
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个过程*F*，在监督学习中是*监督者*（[图灵称之为“老师”](https://medium.com/p/5c77d9201d11#e1d2)），处理类型为*X*的输入数据*x*，生成类型为*Y*的输出类别或参数*y*；
- en: an **a**-indexed family of functions 𝒰(−)**a**, where 𝒰 is a *learning machine*
    or *interpreter* ([Turing called it a “pupil”](https://medium.com/p/5c77d9201d11#2e21))
    and the indices **a** are the *models*, usually expressed as *programs*; lastly,
    there is
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**a**索引的函数族𝒰(−)**a**，其中𝒰是一个*学习机*或*解释器*（[图灵称之为“学生”](https://medium.com/p/5c77d9201d11#2e21)），索引**a**是*模型*，通常表示为*程序*；最后，还有
- en: a function ℒ, usually called the *loss*, comparing the outputs *y* = *F*(*x*)
    with the predictions *ỹ* = 𝒰(*x*)**a** and delivering a real number ℒ(*y*,*ỹ*)
    that measures their difference.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个函数ℒ，通常称为*损失*，比较输出*y* = *F*(*x*)与预测值*ỹ* = 𝒰(*x*)**a**，并给出一个实数ℒ(*y*,*ỹ*)，表示它们之间的差异。
- en: The learner overseeing the learning framework is given a finite set
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习框架的学习者提供一个有限的集合
- en: '![](../Images/68bfd63f4334bc5d85cfac7fe08f1098.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68bfd63f4334bc5d85cfac7fe08f1098.png)'
- en: where the *x*s are samples from a source *X* and the *y*s are the corresponding
    samples from the random variable *Y* = *F*(*X*). The learner’s task is to build
    a model **a** that minimizes the losses
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*x*是来自源*X*的样本，*y*是来自随机变量*Y* = *F*(*X*)的相应样本。学习者的任务是构建一个最小化损失的模型**a**。
- en: '![](../Images/f4652087b372746f1517dc1c3ce14083.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4652087b372746f1517dc1c3ce14083.png)'
- en: where *yi* = *F*(*xi*) and *ỹi* = 𝒰(*xi*)**a** for *i* = 1,2,…,*n*. Since some
    of the losses may increase when the others decrease, the learning algorithm is
    required to minimize the average *guessing risk*
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*yi* = *F*(*xi*)，*ỹi* = 𝒰(*xi*)**a**，对于*i* = 1,2,…,*n*。由于某些损失可能在其他损失减少时增加，因此要求学习算法最小化平均*猜测风险*。
- en: '![](../Images/26df659663e5b994ef42eb2f950d4bd4.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26df659663e5b994ef42eb2f950d4bd4.png)'
- en: where [𝒰(*xi*)**a**] denotes the frequency with which the guesses 𝒰(*xi*)**a**
    are tried. Once a model **a** is found for which the risk is minimal, the function
    *F* is approximated by running the machine 𝒰 on a program implementing the model
    **a** and we write
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中[𝒰(*xi*)**a**]表示猜测𝒰(*xi*)**a**被尝试的频率。一旦找到一个使风险最小化的模型**a**，就通过运行实现模型**a**的程序在机器𝒰上近似函数*F*，并写成
- en: '![](../Images/15346bce3485caea133988d78905e635.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15346bce3485caea133988d78905e635.png)'
- en: '**Potato, potahto, tomato, tomahto.** What are the outcomes of learning? We
    just called the outcome **a** of a round of supervised learning a *model* of the
    supervisor *F*. Since **a** is an attempt to describe *F*, most logicians would
    call it a *theory* of *F*. If the interpretations 𝒰(*X*)**a** describe *F*(*X*)
    truthfully, the logicians would say that *F* is actually a model of the theory
    **a** under the semantical interpretation by 𝒰. So there is a terminological clash
    between the theory of learning, where **a** is a model of *F*, and logic, where
    *F* is a model of **a.** In a further contribution to the confusion, statisticians
    say that **a** is a *hypothesis* about *F*. If a hypothesis or a theory is believed
    to be true, then it is a part of the learner’s *belief state*. In the [final section](#50f8),
    we will arrive at a curious construction illustrating a need for studying the
    *belief logic of machine learning*[³](#0dbf). We stick with calling the learning
    outcomes **a** *models* since that seems to be the common usage. An important
    wrinkle, is, however, that a model **a** of *F* needs to be *executable* in order
    to allow computing the predictions 𝒰(*X*)**a** of the values *F*(*X*). But if
    you think about it, executable models are what we normally call *programs*. In
    summary, the outcome of a learning process is an executable model. The cumulative
    outcome of learning is the learner’s belief state. ***The process of learning
    is the search for learnable programs.***'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**土豆、洋葱、番茄、番茄。** 学习的结果是什么？我们刚才提到的监督学习的一轮结果**a**是监督者*F*的*模型*。由于**a**是试图描述*F*，大多数逻辑学家会称其为*F*的*理论*。如果解释𝒰(*X*)**a**忠实地描述了*F*(*X*)，那么逻辑学家会说*F*实际上是理论**a**在𝒰语义解释下的一个模型。所以在学习理论中，**a**是*F*的模型，而在逻辑中，*F*是**a**的模型，这两者之间存在术语上的冲突。更进一步，统计学家说**a**是关于*F*的*假设*。如果一个假设或理论被认为是真的，那么它就是学习者*信念状态*的一部分。在[最后部分](#50f8)，我们将提出一个有趣的构建，说明研究*机器学习的信念逻辑*的必要性[³](#0dbf)。我们仍然称学习结果**a**为*模型*，因为这似乎是最常见的用法。然而，一个重要的细节是，*F*的模型**a**需要是*可执行的*，以便能够计算出值*F*(*X*)的预测𝒰(*X*)**a**。但如果你仔细想想，可执行模型通常就是我们所称的*程序*。总之，学习过程的结果是一个可执行的模型。学习的累积结果是学习者的信念状态。***学习的过程就是寻找可学习程序的过程。***'
- en: '**All learning is language learning.** In general, the process *F* to be learned
    is given as a channel, which means that the outputs are context-dependent. The
    story from [Sec. 3.2 of the *Semantics* part](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#d3b1)
    applies. The channel inputs *xj* depend on the earlier inputs *xi*, *i* < *j*.
    When there is feedback, *xj* also depends on the earlier outputs *yi*, *i* < *j*.
    To be able to learn *F*’s behavior, the learning machine 𝒰 must also be a channel.
    *Since capturing channel dependencies requires syntactic and semantic references,
    there is a language behind every learner*, whether it is apparent or not. The
    semiotic analyses of the languages of film, music, or images, etc., describe genuine
    syntactic and semantic structures. Different organisms learn in different ways,
    but for humans and their machines, all learning is language learning.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**所有学习都是语言学习。** 一般来说，待学习的过程*F*是通过一个通道给出的，这意味着输出是上下文依赖的。来自[第3.2节 *语义* 部分](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#d3b1)的故事适用。通道输入*xj*依赖于早期的输入*xi*，其中*i*
    < *j*。当有反馈时，*xj*也依赖于早期的输出*yi*，其中*i* < *j*。为了能够学习*F*的行为，学习机器𝒰也必须是一个通道。*由于捕捉通道依赖性需要句法和语义引用，所以每个学习者背后都有一种语言*，无论它是否显而易见。电影、音乐或图像等语言的符号学分析描述了真正的句法和语义结构。不同的生物以不同的方式学习，但对于人类及其机器而言，所有学习都是语言学习。'
- en: '2.3\. Examples: From pigeons to perceptrons'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3\. 示例：从鸽子到感知机
- en: '**Pigeon superstition.** The function *F* that a pigeon learns to predict is
    a source of food. It can be viewed as a channel *[X* ⊢ *Y]*, where the values
    *x*1, *x*2, . . . of type *X* are moments in time and *Y* = *F*(*X*) is a random
    variable, delivering seeds with a fixed probability. Suppose that *Y* = 1 means
    “food” and *Y* = 0 means “no food”. If we take the possible models (programs,
    beliefs) **a** to correspond to the elements of a set of actions available to
    the pigeon, then the pigeon is trying to learn for which actions **a** and at
    which moments *x* to output 𝒰(*x*)**a** = 1 and when to output 0\. The loss ℒ(*y*,
    𝒰(*x*)**a**) = |*y-*𝒰(*x*)**a**| is 0 if the food is delivered just when the pigeon
    takes the action **a**. After a sufficient amount of time, the random output *Y*
    = 1 will almost surely coincide with a prediction 𝒰(*X*)**a** = 1 for some **a**.
    The pigeon will then learn to do **a** more often and increase the chance of such
    coincidences. If one **a** prevails, the pigeon will learn that it causes food.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**鸽子迷信。** 鸽子学习预测的函数*F*是食物的来源。它可以视为一个通道*[X* ⊢ *Y]*，其中类型*X*的值*x*1、*x*2、...是时间点，*Y*
    = *F*(*X*)是一个随机变量，以固定的概率提供种子。假设*Y* = 1表示“食物”，*Y* = 0表示“没有食物”。如果我们将可能的模型（程序、信念）**a**对应于鸽子可用动作集的元素，那么鸽子正在尝试学习在哪些时刻*x*和哪些动作**a**下，输出𝒰(*x*)**a**
    = 1，以及何时输出0。如果食物恰好在鸽子采取动作**a**时送到，则损失ℒ(*y*, 𝒰(*x*)**a**) = |*y-*𝒰(*x*)**a**|为0。经过足够的时间后，随机输出*Y*
    = 1几乎肯定会与某个**a**的预测𝒰(*X*)**a** = 1相符。鸽子随后会学会更频繁地做**a**，从而增加这种巧合发生的机会。如果某个**a**占主导地位，鸽子将学会它是导致食物出现的原因。'
- en: '**Statistical testing.** Science is a family of methods designed to overcome
    superstition and prejudice. The idea is to prevent pigeon-style confirmations
    by systematically testing hypotheses and only accepting significant correlations.
    The mathematical foundations of statistical hypothesis testing were developed
    in the 1920s by Ronald Fisher, and have remained the bread and butter of scientific
    practices. The crucial assumption is that the interpretation 𝒰 for any hypothesis
    **a** is given together with its probability density *p***a**(*x*) = *d*𝒰(*x*)**a**
    . The loss ℒ is then estimated by the length of the description of this probability.
    If the value of *p***a**(*x*) is described by a string of digits, its description
    length is proportional to −log *p***a**(*x*). The guessing risk is thus ℛ(**a**)
    = ∫− log *p***a**(*x*)*d*𝒰(*x*)**a**. Values of this kind are studied in information
    theory as measures of uncertainty. Minimizing ℛ(**a**) thus boils down to choosing
    the hypothesis a that minimizes the uncertainty of sampling 𝒰 for **a**. Fisher
    recommended the learning algorithm that selects the hypothesis with a *maximal
    likelihood*.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**统计检验。** 科学是一系列旨在克服迷信和偏见的方法。其核心思想是通过系统地检验假设，并仅接受显著的相关性，从而防止“鸽子式”的确认偏误。统计假设检验的数学基础由罗纳德·费舍尔（Ronald
    Fisher）在1920年代发展起来，并一直是科学实践中的基础工具。其关键假设是，任何假设**a**的解释𝒰是与其概率密度*p***a**(*x*) = *d*𝒰(*x*)**a**一起给出的。损失ℒ则通过该概率的描述长度来估计。如果*p***a**(*x*)的描述由一串数字表示，则其描述长度与−log
    *p***a**(*x*)成正比。因此，猜测风险为ℛ(**a**) = ∫− log *p***a**(*x*)*d*𝒰(*x*)**a**。这类值在信息论中作为不确定性的度量进行研究。因此，最小化ℛ(**a**)就归结为选择能够最小化采样𝒰的不确定性的假设**a**。费舍尔推荐了一种学习算法，通过它可以选择具有*最大似然*的假设。'
- en: The basic shortcoming of statistical testing is that the densities *p***a**
    must be known. They are presumed to arise from scientists’ minds, together with
    their hypotheses parametrized by **a**. Statistics thus provides a testing service,
    but the actual process of learning the hypotheses **a** is out of scope and left
    to the magic of insight and creativity. While Kolmogorov and his students were
    pondering this problem for decades and eventually solved it, a central part of
    the solution emerged inadvertently, and from an unexpected direction.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 统计检验的基本缺点是，密度*p***a**必须是已知的。它们被假设为来源于科学家的思想，以及由**a**参数化的假设。因此，统计学提供的是检验服务，但实际的假设学习过程**a**超出了统计的范围，留给了直觉和创造力的神奇。在Kolmogorov及其学生们研究这一问题数十年并最终解决它的过程中，解决方案的核心部分无意间出现在一个意想不到的方向。
- en: '**Perceptrons.** In 1943, McCulloch and Pitts proposed a mathematical model
    of the neuron. It boiled down to a state machine, like Turing’s original 1936
    computer, just simpler, since it didn’t have the external memory. In the late
    1950s, Frank Rosenblatt was working on expanding the model of a neuron into a
    model of the brain. It was a very ambitious project.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知机**。1943年，McCulloch 和 Pitts 提出了神经元的数学模型。它简化为一种状态机，就像图灵1936年的原始计算机一样，只是更简单，因为它没有外部存储器。在1950年代末，Frank
    Rosenblatt 正在研究将神经元模型扩展为大脑模型。这是一个非常雄心勃勃的项目。'
- en: '![](../Images/3e804fdec6259980b5176ec417cd51f4.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e804fdec6259980b5176ec417cd51f4.png)'
- en: Illustration from Rosenblatt’s 1958 project report to the Office of Naval Research.
    — Public domain
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Rosenblatt 1958年向海军研究办公室提交的项目报告插图。 — 公共领域
- en: Rosenblatt, however, arrived at a component simpler than the McCulloch-Pitts
    neuron. He called it *perceptron*, to emphasize the difference of his project
    from the “various engineering projects concerned with automatic pattern recognition
    and ‘artificial intelligence’ ”. Nevertheless, the project generated news reports
    with titles like “Frankenstein Monster Designed by Navy Robot That Thinks”, as
    Rosenblatt duly reports in his book[⁴](#04e9).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Rosenblatt 提出了一个比 McCulloch-Pitts 神经元更简单的组件。他称之为 *感知机*，以强调他的项目与“各种工程项目，尤其是自动模式识别和‘人工智能’”的区别。然而，该项目产生了像“海军机器人设计的‘弗兰肯斯坦怪物’，它能思考”这样的新闻报道，正如
    Rosenblatt 在他的书中如实报道的那样[⁴](#04e9)。
- en: '***Mathematical neurons*** were defined as pairs **a** = *(b, ⟨w |)*, where[⁵](#7754)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '***数学神经元*** 被定义为一对 **a** = *(b, ⟨w |)*，其中[⁵](#7754)'
- en: '![](../Images/685ee22d76a5c57265d94dfb4e6f5f5c.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/685ee22d76a5c57265d94dfb4e6f5f5c.png)'
- en: and *b* is a scalar. It is meant to be a very simple program intepreted by the
    interpreter 𝒰. To evaluate **a** = *(b, ⟨w |)* on an input vector input vector
    | *x* ⟩, the interpreter 𝒰 applies the projection ⟨*w* | on | *x* ⟩ to get the
    inner product ⟨*w* | *x*⟩, which measures the length of the projection of either
    of the vectors on the other, and then it outputs the sign of the difference ⟨*w*
    | *x*⟩ − *b:*
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 并且 *b* 是一个标量。它被认为是由解释器 𝒰 解释的非常简单的程序。为了在输入向量 | *x* ⟩ 上评估 **a** = *(b, ⟨w |)*，解释器
    𝒰 对 | *x* ⟩ 进行投影 ⟨*w* |，以获得内积 ⟨*w* | *x*⟩，该内积衡量了任一向量在另一个向量上的投影长度，然后输出差值 ⟨*w* |
    *x*⟩ − *b* 的符号：
- en: '![](../Images/164dc55568f7482bc20ce6b309a98f1f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/164dc55568f7482bc20ce6b309a98f1f.png)'
- en: For a more succinct view, the pair **a** = *(b*, ⟨*w* |) and the input | *x*
    ⟩ are often modified to
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更简洁地表达，通常将配对 **a** = *(b*, ⟨*w* |) 和输入 | *x* ⟩ 修改为
- en: '![](../Images/a574620827c0a2ab52b1486b235983d8.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a574620827c0a2ab52b1486b235983d8.png)'
- en: so that the interpretation of a neuron boils down to
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，神经元的解释就简化为
- en: '![](../Images/1bf3c497036c5ec001d527c51f3e5d30.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bf3c497036c5ec001d527c51f3e5d30.png)'
- en: '***Perceptrons*** are compositions of such neurons. If a neuron is presented
    as a single row vector, then a perceptron is an (*n* + 1)-tuple of row vectors'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '***感知机*** 是由这些神经元组成的。如果一个神经元被表示为一个单一的行向量，那么感知机就是一个 (*n* + 1)-元组，由行向量组成。'
- en: '![](../Images/b0de2483c9e7fd9cd0c13b6a8ae79518.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0de2483c9e7fd9cd0c13b6a8ae79518.png)'
- en: On an input | *x*⟩, the interpretation of a perceptron **a** computes
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入 | *x*⟩ 上，感知机 **a** 的解释计算
- en: '![](../Images/1f7918759774f69c8d77638d766f8aaa.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f7918759774f69c8d77638d766f8aaa.png)'
- en: For a more succinct view, the *n*-tuple of vectors ⟨*w*1 |, . . . , ⟨*wn* |
    can be arranged into the matrix
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更简洁地表达，*n*-元组向量 ⟨*w*1 |, . . . , ⟨*wn* | 可以排列成矩阵
- en: '![](../Images/7d1b7a7d209964290c21c98e9ff51128.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d1b7a7d209964290c21c98e9ff51128.png)'
- en: so that the perceptron **a** = (⟨*v*|, ⟨*w*1 |,…,⟨*wn* |) boils down to **a**
    = (⟨*v*|, *W)* and its interpretation becomes
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，感知机 **a** = (⟨*v*|, ⟨*w*1 |,…,⟨*wn* |) 简化为 **a** = (⟨*v*|, *W)*，其解释变为
- en: '![](../Images/1916147fd646b3e9dd170034b5110296.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1916147fd646b3e9dd170034b5110296.png)'
- en: To summarize in diagrams, here are the two presentations of a neuron on the
    left and the two presentations of a perceptron on the right.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用图示总结，左侧是神经元的两种表示方式，右侧是感知机的两种表示方式。
- en: '![](../Images/3e749bfb2dc23f8474b76caab7d7b485.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e749bfb2dc23f8474b76caab7d7b485.png)'
- en: Rosenblatt’s neuron and perceptron
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Rosenblatt 的神经元和感知机
- en: The first row shows the neuron and the perceptron in the original form, with
    the thresholds *bj*. The second row shows the versions where each *bj* is absorbed
    as the 0-th component of the weight vector ⟨*wj* |.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行显示了原始形式的神经元和感知机，其中包括阈值 *bj*。第二行显示了每个 *bj* 被吸收为权重向量 ⟨*wj* | 的0-项版本。
- en: '**Perceptrons were a breakthrough into machine learning and inductive inference
    as two sides of the same coin.** Statistics provided the formal methods for hypothesis
    testing but left the task of learning and inferring hypotheses to informal methods
    and the magic of creativity. Perceptron training was the first formal method for
    inductive inference. Nowadays, this method looks obvious. The learner initiates
    the weights | *w* ⟩ and the thresholds *b* to arbitrary values, runs the interpreter
    𝒰to generate predictions, compares them with the training data supplied by the
    supervisor *F*, and updates the weights proportionally to the losses ℒ. This didn’t
    seem like a big deal even to Frank Rosenblatt, who wrote that'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知机是机器学习和归纳推理的突破，二者是同一枚硬币的两面。** 统计学提供了假设检验的正式方法，但将学习和推理假设的任务留给了非正式方法和创造力的魔力。感知机训练是归纳推理的第一个正式方法。如今，这种方法看起来理所当然。学习者将权重|
    *w* ⟩ 和阈值 *b* 初始化为任意值，运行解释器𝒰生成预测，将其与由监督者*F*提供的训练数据进行比较，并根据损失ℒ比例更新权重。这甚至对弗兰克·罗森布拉特来说都不算什么大事，他写道：'
- en: the perceptron program [was] not primarily concerned with the invention of devices
    for “artificial intelligence”, but rather with investigating the physical structures
    and neurodynamic principles which underlie “natural intelligence”.
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 感知机程序[并非]主要关注“人工智能”设备的发明，而是研究支撑“自然智能”的物理结构和神经动力学原理。
- en: Rosenblatt laid the stepping stone into machine learning while attempting to
    model the learning process in human brains. Even the very first learning machine
    was not purposefully designed but evolved spontaneously.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 罗森布拉特为机器学习奠定了基础，试图模拟人脑中的学习过程。即使是最初的学习机器也并非有目的地设计，而是自发地演化出来的。
- en: It is often said that airplanes were not built by studying how the birds fly
    and that intelligent machines will not be built by looking inside people’s heads.
    But there is more at hand. Perceptrons opened an alley into **learning as a *universal
    computational process.*** *Machine learning and human learning are particular
    implementations of the universal process of learning*, which is a natural process
    that evolves and diversifies. Machine learning models offer insights into a common
    denominator of all avatars of learning. The pattern of perceptron computation
    will be repeated on each of the models presented in the rest of this note.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 人们常说，飞机的建造并不是通过研究鸟类如何飞行来完成的，智能机器也不会通过观察人脑内部的运作来建造。然而，事情并非如此简单。感知机打开了通向**作为*普遍计算过程*的学习的道路。**
    *机器学习和人类学习是学习这一普遍过程的特殊实现*，它是一个自然过程，具有演化和多样化的特征。机器学习模型提供了对所有学习表现形式的共同点的洞察。感知机计算的模式将在本文接下来介绍的每一个模型中得到重复。
- en: 3\. Learning functions
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 学习函数
- en: 3.1\. Why learning is possible
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1. 为什么学习是可能的
- en: 'To understand why learning is possible, we first consider the special case
    when channel *F* is memoryless and deterministic: an ordinary function.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么学习是可能的，我们首先考虑一个特殊情况，即通道 *F* 是无记忆和确定性的：一个普通的函数。
- en: '**Learnable functions are continuous.** What can be learned about a function
    *F*:*X*⟶*Y* from a finite set of pairs (*x*1,*y*1), (*x*2,*y*2),…,(*xn*,*yn*),
    where *F*(*xi*) = *yi*? Generally nothing. Knowing *F*(*x*) does not tell anything
    about *F*(*x*′), unless *x* and *x*′ are related in some way, and *F* preserves
    their relation. To generalize the observed sample (*x*1, *y*1), . . . , (*xn*,
    *yn*) and predict a classification *F*(*x*′) = *y*′ for an unobserved data item
    *x*′, it is necessary that'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**可学习的函数是连续的。** 从一组有限的对(*x*1,*y*1)、(*x*2,*y*2)、…、(*xn*,*yn*)中，我们能学到关于函数 *F*:*X*⟶*Y*
    的什么？通常什么也学不到。知道 *F*(*x*) 并不能告诉我们 *F*(*x*′) 的任何信息，除非 *x* 和 *x*′ 以某种方式相关，并且 *F*
    保持它们之间的关系。为了将观察到的样本 (*x*1, *y*1)、…、(*xn*, *yn*) 泛化并预测未观察到的数据项 *x*′ 的分类 *F*(*x*′)
    = *y*′，必须满足：'
- en: '*x*′ is related to *x*1,…,*xn*,'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*′与 *x*1,…,*xn* 相关，'
- en: '*y*′ is related to *y*1,…,*yn*, where *yi* = *F*(*xi*) for *i* = 1,…,*n*, and'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*′ 与 *y*1,…,*yn* 相关，其中 *yi* = *F*(*xi*) 对于 *i* = 1,…,*n*，并且'
- en: '*F* preserves the relations.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F* 保持这些关系。'
- en: If the sets of the *x*s and the *y*s in such relationships are viewed as *neighborhoods*,
    then the datatype *X* and the classifier type *Y* become *topological* spaces.
    The neighborhoods form topologies. Don’t worry if you don’t know the formal definition
    of a topology. It is just an abstract way to say that *x* and *x*′ live in the
    same neighborhood. A function *F*:*X*⟶*Y* is *continuous* when it maps neighbors
    to neighbors. And the neighborhoods don’t have to be physical proximities. Two
    words with similar meanings live in a semantical neighborhood. Any kind of relation
    can be expressed in terms of neighborhoods. So if *x*′ is related with *x*1 and
    *x*2, and *F* is continuous, then *y*′ = *F*(*x*′) is related with *y*1 = *F*(*x*1)
    and *y*2 = *F*(*x*2). That allows us to learn from a set of pairs (*x*1, *y*1),
    . . . , (*xn*, *yn*) where *F*(*xi*) = *yi* that *F*(*x*′) = *y*′ also holds.
    Then we can add the pair (*x*′, *y*′) to the list as a prediction. Without the
    neighborhoods and the continuity, we cannot make such predictions. To be learnable
    a function must be continuous.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将这种关系中的 *x* 和 *y* 的集合视为 *邻域*，那么数据类型 *X* 和分类器类型 *Y* 就变成了 *拓扑* 空间。邻域形成拓扑。如果你不熟悉拓扑的正式定义，也不用担心。这只是一个抽象的方式，意味着
    *x* 和 *x*′ 生活在同一个邻域中。当一个函数 *F*:*X* ⟶ *Y* 将邻域映射到邻域时，它是 *连续* 的。而且邻域不一定是物理上的接近。两个含义相似的词汇生活在语义邻域中。任何种类的关系都可以通过邻域来表达。所以，如果
    *x*′ 与 *x*1 和 *x*2 有关系，并且 *F* 是连续的，那么 *y*′ = *F*(*x*′) 也与 *y*1 = *F*(*x*1) 和 *y*2
    = *F*(*x*2) 有关系。这使得我们可以通过一组对 (*x*1, *y*1), . . . , (*xn*, *yn*)，其中 *F*(*xi*) =
    *yi*，来学习 *F*(*x*′) = *y*′ 也成立。然后我们可以将对 (*x*′, *y*′) 添加到列表中，作为一种预测。如果没有邻域和连续性，我们无法做出这样的预测。要能够学习，函数必须是连续的。
- en: 'There are many ways in which this is used, and many details to work out. For
    the moment, just note that *learning is based on associations.* You associate
    a set of names *X* with a set of faces *Y* along a continuous function *F*:*X*
    ⟶*Y.* You remember the face *F*(Allison) by searching through the pairs (*x*1,
    *y*1), . . . , (*xn*, *yn*) where the names *xi* are associated with Allison’s.
    Since *F* is continuous, the faces *yi* = *F*(*xi*) must be associated with Allison’s.
    Therefore, if you find a face of a neighbor of Allison’s name, then you can find
    Allison’s face in the neighborhood of the face of Allison’s neighbor. This is
    how *associative memory* works: as a family of continuous functions. The *key-value
    associations* in databases work similarly. Both in human memory and in databases,
    associative memory is implemented using referential neighborhoods. Functions are
    learnable when they preserve associations. They preserve associations when they
    are continuous.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这在许多方面都有应用，且有许多细节需要解决。暂时只需注意，*学习是基于关联的*。你将一组名字 *X* 与一组面孔 *Y* 关联在一起，通过一个连续函数
    *F*:*X* ⟶ *Y*。你通过在对 (*x*1, *y*1), . . . , (*xn*, *yn*) 中搜索名字 *xi*，来记住面孔 *F*(Allison)。因为
    *F* 是连续的，面孔 *yi* = *F*(*xi*) 必须与 Allison 的面孔有关联。因此，如果你找到一个与 Allison 的名字邻近的面孔，那么你就可以在
    Allison 邻居的面孔的邻域中找到 Allison 的面孔。这就是 *关联记忆* 的工作方式：作为一类连续函数。数据库中的 *键值关联* 工作原理类似。在人类记忆和数据库中，关联记忆是通过参照邻域实现的。当函数保持关联时，它们是可学习的。它们在连续时保持关联。
- en: '**Continuous functions can be partially evaluated and linearly approximated.**
    The Fundamental Theorem of Calculus says, roughly, that the derivative and the
    integral, as operations on functions, are each other’s inverses. The integral
    approximates with arbitrary precision any differentiable function by linear combinations
    of step functions that approximate the derivative of the function. Any differentiable
    function is linearly approximable by piecewise linear functions.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**连续函数可以被部分求值并进行线性近似。** 微积分基本定理大致上说，导数和积分作为对函数的运算是相互逆操作。积分通过将任何可微分函数近似为由阶梯函数线性组合的形式，来以任意精度近似该函数的导数。任何可微分函数都可以通过分段线性函数进行线性近似。'
- en: A function that is just continuous (not differentiable) may not be approximable
    by piecewise linear functions. Yet it turns out that it can always be approximated
    by linear combinations of pieces of a continuous function (not linear or polynomial),
    usually called *actuation.* The approximating linear combinations of this nonlinear
    function are *learnable*. Hence machine learning.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一个仅仅是连续（而不是可微分）的函数可能无法通过分段线性函数进行近似。然而，事实证明它始终可以通过连续函数片段的线性组合进行近似（而不是线性或多项式），通常称为
    *驱动*。这个非线性函数的近似线性组合是 *可学习的*。因此，机器学习得以实现。
- en: On the other hand, the approximability of continuous functions has remained
    one of the big secrets of calculus. The fact that ***all continuous functions
    can be decomposed into sums of single-variable continuous functions***defies most
    people’s intuitions. It says that, as far as computations are concerned, there
    are no genuine multi-dimensional phenomena among continuous functions. All those
    complicated multi-variable functions you may have seen in a vector calculus textbook,
    or encountered in practice if you are an engineer or a scientist — they can all
    be partially evaluated, each variable separately. Which is why they can be learned.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，连续函数的可逼近性仍然是微积分中的一个重大秘密。***所有连续函数都可以分解为单变量连续函数的和***这一事实挑战了大多数人的直觉。它表明，就计算而言，连续函数中没有真正的多维现象。你可能在矢量微积分教材中看到过的，或者如果你是工程师或科学家，在实践中遇到过的那些复杂的多变量函数——它们都可以部分求解，每个变量分别处理。这也是为什么它们可以被学习的原因。
- en: '3.2 Decomposing continuous functions: Kolmogorov-Arnold[⁶](#d246)'
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 连续函数的分解：Kolmogorov-Arnold[⁶](#d246)
- en: '**Hilbert’s 13th Problem.** Back in the year 1900, the famous mathematician
    David Hilbert offered his famous list of 23 mathematical problems for the next
    century. Number 13 on the list was the question if all functions with 3 variables
    can be expressed by composing functions of 2 variables. Hilbert conjectured that
    a specific function, the formula for the solutions of the equation *x*⁷ + *ax³*
    + *bx*² + *cx* + 1 = 0 expressed in terms of the coefficients *a*, *b*, and *c*,
    could not be decomposed into functions of pairs of the coefficients. More than
    half-way through the century, 19-year-old Vladimir Arnold proved that all continuous
    functions with 3 variables can be decomposed into continuous functions with 2
    variables and disproved Hilbert’s conjecture. Next year, Arnold’s thesis advisor
    Andrey Kolmogorov proved a stunning generalization. The theorem has been strengthened
    and simplified ever since. Early simplifications were based on the following embedding
    of the *d*-dimensional cube into the (2*d*+1)-dimensional cube, constructed to
    allow separating the *d* variables in any continuous function.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**希尔伯特的第13个问题。** 1900年，著名数学家大卫·希尔伯特提出了他为下一个世纪设定的23个数学问题，其中第13个问题是，所有具有3个变量的函数是否可以通过组合具有2个变量的函数来表示。希尔伯特猜测，一个特定的函数，表示方程
    *x*⁷ + *ax³* + *bx*² + *cx* + 1 = 0 的解的公式，用系数 *a*、*b* 和 *c* 表示，不能分解为系数对的函数。到世纪过半，19岁的弗拉基米尔·阿诺德证明了所有具有3个变量的连续函数都可以分解为具有2个变量的连续函数，从而推翻了希尔伯特的猜想。次年，阿诺德的论文导师安德烈·科尔莫哥洛夫证明了一个惊人的推广。自那时以来，这一定理一直在不断强化和简化。早期的简化是基于以下将
    *d* 维立方体嵌入到 (2*d*+1) 维立方体中的构造，旨在允许分离任何连续函数中的 *d* 变量。'
- en: '![](../Images/0fa93b3737e5e2706790a8e1dd455902.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0fa93b3737e5e2706790a8e1dd455902.png)'
- en: Together with a fixed (2*d*+1)-dimensional vector ⟨*v* | , the embedding[⁷](#a35f)
    *W*, yields the claimed decomposition.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与一个固定的 (2*d*+1) 维向量 ⟨*v*| 一起，嵌入[⁷](#a35f) *W*，得到了所声称的分解。
- en: '![](../Images/9e71bdeaf3888d4c4fe110fee6cab9ec.png)![](../Images/fe99a93b37f4643fc3153b462fad4fe7.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e71bdeaf3888d4c4fe110fee6cab9ec.png)![](../Images/fe99a93b37f4643fc3153b462fad4fe7.png)'
- en: Kolmogorov-Arnold Decomposition
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Kolmogorov-Arnold 分解
- en: '**Comments and explanations.** Unfolding the decomposition yields'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论与解释。** 展开分解得到：'
- en: '![](../Images/8c888e333ae7118389191320c5882db5.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c888e333ae7118389191320c5882db5.png)'
- en: 'Only 𝜑 depends on *f*, whereas *W* and *v* are given globally, for all functions
    of *d* variables. They are not unique and *W* can be chosen so that ⟨*v*| is a
    vector of 1s, as it is assumed in this unfolded version. The constructions not
    only disproved Hilbert’s conjecture, but still defy most people’s geometric intuitions.
    The reason may be that we tend to think in terms of smooth functions, whereas
    the funcions 𝜓 and 𝜑 are heavily fractal. They are constructed using copies of
    the Devil Staircase or space-filling curves. The geometric interpretation of the
    embedding *W* is that the (2*d*+1)-tuple of 𝜓s draws a curve in the (2*d*+1)-dimensional
    cube and copies of that curve span a homeomorphic image of the *d*-dimensional
    cube:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 只有 𝜑 依赖于 *f*，而 *W* 和 *v* 是全局给定的，适用于所有 *d* 变量的函数。它们不是唯一的，*W* 可以选择使得 ⟨*v*| 是一个全是
    1 的向量，正如在这个展开版本中假设的那样。这些构造不仅推翻了希尔伯特猜想，而且至今仍挑战着大多数人的几何直觉。其原因可能在于我们习惯于用光滑函数来思考，而
    𝜓 和 𝜑 函数则是高度分形的。它们是通过复制恶魔楼梯或空间填充曲线来构造的。*W* 的几何解释是，(2*d*+1) 元组的 𝜓 描绘了一条在 (2*d*+1)
    维立方体中的曲线，这条曲线的复制构成了一个同胚图像，映射到 *d* 维立方体：
- en: '![](../Images/4183261dd51555a36478795ffce8233f.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4183261dd51555a36478795ffce8233f.png)'
- en: This is the first component of *W* in the diagram above. The projections on
    ⟨*w*| of vectors from within the *d*-cube determine the linear combinations of
    copies of 𝜓 whose inverse images iteratively fill the *d*-cube. Kolmogorov’s original
    construction partitioned the mapping *f* along the edges of the *d-*cube and combined
    *d* differentfunctions 𝜑 to represent *f* . Sprecher and Lorentz later noticed
    that additional stretching allows capturing all parts of *f* by a single 𝜑. This
    is possible because the dependency of *f* on each of its *d* variables can be
    approximated with arbitrary precision on a null-subset of its domain, and the
    null-subsets of *[0,1]* can be made disjoint. The upshot is that *the only genuinely
    multi-variable continuous function is the addition*. The multiple inputs for multi-variable
    continuous functions can always be preprocessed in such a way that each input
    is processed separately, by a single-variable function. The output of the original
    multi-variable function is then obtained by adding up the outputs of the single-variable
    components. *Continuous functions are thus partially evaluated, each input separately.*
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这是上图中 *W* 的第一个组成部分。来自 *d* 立方体内的向量在 ⟨*w*| 上的投影决定了 𝜓 的副本线性组合，反向映射通过迭代填充 *d* 立方体。Kolmogorov
    最初的构造将映射 *f* 沿 *d* 立方体的边进行分割，并结合了 *d* 个不同的函数 𝜑 来表示 *f*。Sprecher 和 Lorentz 后来注意到，额外的拉伸可以通过单个
    𝜑 捕捉 *f* 的所有部分。这是可能的，因为 *f* 对其每个 *d* 变量的依赖可以在其定义域的零子集上以任意精度逼近，并且 *[0,1]* 的零子集可以被分割成不相交的部分。结果是，*唯一真正的多变量连续函数是加法*。多变量连续函数的多个输入总是可以通过单变量函数分别预处理，使每个输入都由单独的单变量函数处理。然后通过将单变量组件的输出加起来，获得原始多变量函数的输出。*因此，连续函数是部分求值的，每个输入分别处理*。
- en: The price to be paid is that the single-variable continuous functions that perform
    the preprocessing and the processing are complicated, ineffective, and constructed
    through iterative approximations. For a long time, the iterative fugue of Kolmogorov’s
    proof was viewed as a glimpse from the darkness of a world of complexities beyond
    any our our imagination or utility. Then in the late 1980s, Hecht-Nielsen noticed
    that the Kolmogorov-Arnold decomposition seemed related to the perceptron architecture,
    as the diagrams above also suggest. What is going on?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 付出的代价是，执行预处理和处理的单变量连续函数既复杂又低效，并通过迭代逼近构造而成。长期以来，Kolmogorov 证明中的迭代式 fuga 被认为是从一个超出我们任何想象和实用性的复杂世界中窥见的一线曙光。直到
    1980 年代末，Hecht-Nielsen 注意到 Kolmogorov-Arnold 分解似乎与感知机结构有关，正如上面的图示所暗示的那样。到底发生了什么？
- en: 3.3 Wide learning
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 宽度学习
- en: So far we studieda[***theoretic*** construction](#2f11) providing
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们研究了一个[***理论性的*** 构造](#2f11)，提供
- en: an ***exact*** *representation* of *f* using
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个***精确的*** *表示* *f*，使用
- en: a projection-embedding pair (⟨*v*|*,W)* ***independent*** on *f* and
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个投影-嵌入对（⟨*v*|*,W)* ***独立于*** *f* 和
- en: an ***approximate*** *construction* 𝜑 ***dependent*** on *f.*
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 ***近似的*** *构造* 𝜑 ***依赖于*** *f*。
- en: Now we turn to a [***practical*** construction](#5646) providing
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向一个[***实际的*** 构造](#5646)，提供
- en: an ***approximate*** *representation* of *f* using
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 ***近似的*** *表示* *f*，使用
- en: a projection-embedding pair (⟨*v*|*,W)* ***dependent*** on *f* and
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个投影-嵌入对（⟨*v*|*,W)* ***依赖于*** *f* 和
- en: an ***exact*** *construction σ* ***independent*** on *f.*
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个***精确的*** *构造 σ* ***独立于*** *f*。
- en: 'The step from the Continuous Decomposition above to the Neural Approximation
    below is illustrated by comparing the [diagram](#3071) of the KA representation
    above with the following diagrams of the CHSW representation:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的连续分解到下面的神经逼近的步骤，通过比较上面 KA 表示法的[图示](#3071)与下面 CHSW 表示法的图示来说明：
- en: '![](../Images/96d0dcc21aa912e2e6832016edf44773.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96d0dcc21aa912e2e6832016edf44773.png)'
- en: Neuron with a *σ-activation*
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 带有 *σ 激活函数* 的神经元
- en: Letting *W* vary with *f* allows omitting the deformations 𝜓. Letting ⟨*v*|
    vary with *f* allows replacing 𝜑 with a fixed *activation* function, independent
    on *f*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让 *W* 随 *f* 变化，可以省略变形 𝜓。让 ⟨*v*| 随 *f* 变化，可以用一个固定的 *激活* 函数代替 𝜑，且与 *f* 无关。
- en: '3.4\. Approximating continuous functions: Cybenko et al'
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4\. 逼近连续函数：Cybenko 等人
- en: '**Activation functions.** The Neural Approximation theorem below states that
    any continuous function can be approximated by linear combinations of a fixed
    *activation function σ*. All we need from this function is that it restricts to
    a homeomorphism between two closed real intervals *not representable by a polynomial*.
    The construction can be set up to only use the part that establishes this continuous,
    monotonic bijection of intervals. That part can be conveniently renormalized to
    a *sigmoid*: a homeomorphism of the extended real line and the interval [0,1].
    Early neural networks used the logistic sigmoid, which readily establishes that
    homeomorphism. The hyperbolic tangent and arcus tangent were also used, suitably
    renormalized. Nowadays the function max(0, *x*) is preferred. Its original designation
    as *“Rectified Linear Unit”* got mellowed down to *ReLU*, a nickname shared with
    small pets. The Neural Approximation construction fails if the activation function
    is representable by a polynomial. This obviously precludes all linear functions
    — *but* already a continuous combination of two linear functions works fine, as
    ReLU shows, combining the constant 0 below 0 and the identity above 0.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数。** 以下神经逼近定理表明，任何连续函数都可以通过固定的*激活函数σ*的线性组合来逼近。我们对这个函数的要求是，它在两个封闭实数区间之间限制为一个同胚函数，而这两个区间*不能由多项式表示*。构造过程可以只使用那个部分，该部分建立了这个连续的、单调的区间双射。这个部分可以方便地重新归一化为一个*sigmoid*：一个扩展实数线与区间[0,1]的同胚。早期的神经网络使用了逻辑sigmoid，它可以很容易地建立这个同胚。双曲正切和反正切也曾被使用，并适当重新归一化。如今，函数max(0,
    *x*)更为常见。它最初的命名为*“修正线性单元”*，后来被简称为*ReLU*，这个名字也用来指代小宠物。当激活函数可以通过多项式表示时，神经逼近构造会失败。显然，这排除了所有线性函数——*但是*，两个线性函数的连续组合已经能够很好地工作，正如ReLU所示，结合了0以下的常数0和0以上的恒等函数。'
- en: '![](../Images/fe33d2cf10d69af5ef92d875cddf60a2.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe33d2cf10d69af5ef92d875cddf60a2.png)'
- en: Since both *f* and 𝒰(-)**f** are continuous, the approximation claim is equivalent
    to saying that for every *ε > 0* there is *δ=δ(ε) >0* such that
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 因为*f*和𝒰(-)**f**都是连续的，所以逼近的声明等价于说，对于每一个*ε > 0*，存在*δ=δ(ε) > 0*，使得
- en: '![](../Images/2bf637223a5e666a3694d6b287a0d272.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bf637223a5e666a3694d6b287a0d272.png)'
- en: Neurons with activation functions are thus *universal approximators* of continuous
    functions, in the sense that for every continuous *f* on a cube there is a neuron
    **f**=*(W*,⟨*v*|) such that *f*|*x*⟩≈ 𝒰| *x* ⟩**f**, with arbitrary precision.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 带有激活函数的神经元因此是连续函数的*通用逼近器*，意味着对于每一个定义在立方体上的连续*f*，存在一个神经元**f**=*(W*,⟨*v*|)，使得*f*|*x*⟩≈
    𝒰| *x* ⟩**f**，且具有任意精度。
- en: The proofs of different versions of the Neural Approximation Theorem were published
    by Cybenko and by Harnik-Stinchcombe-White independently, both in 1989\. In the
    meantime, the neural approximations have been widely used, and various other versions,
    views, and overviews have been provided. The overarching insight links the CHSW-approximation
    and the KA-decomposition in a computational framework that seems to have taken
    both beyond the original motivations.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 神经逼近定理的不同版本的证明分别由Cybenko以及Harnik-Stinchcombe-White在1989年独立发布。与此同时，神经逼近方法被广泛应用，并且提供了各种其他版本、视角和概述。总体的见解将CHSW-逼近和KA-分解联系在一个计算框架中，这个框架似乎已经将二者超越了最初的动机。
- en: '**Continuous functions can be approximated because their variables can be separated.**
    In computational terms, this means that continuous functions can be partially
    evaluated. That makes them learnable. Unfolding the CHSW-approximation in parallel
    with the KA-decomposition displays the common pattern:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**连续函数可以被逼近，因为它们的变量可以分离。** 从计算的角度来看，这意味着连续函数可以被部分求值。这使得它们可以被学习。将CHSW-逼近与KA-分解并行展开，展示了共同的模式：'
- en: '![](../Images/8363593ee2f3a1ceade56f979c144bbc.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8363593ee2f3a1ceade56f979c144bbc.png)'
- en: The corresponding diagrams after the statements show the analogy yet again.
    But note the differences. The first difference is that *w* and *v* on the left
    depend on *f* , whereas on the right only 𝜑 depends on it. The second difference
    is that the number of separate variables that allow partial evaluation, for a
    fixed input *d*, is fixed at (2*d* + 1) in the case of decomposition on the right,
    whereas in the case of approximation on the left, *n* = *n*(ε) depends on the
    approximation error ε. This is an important point.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 语句后的相应图示再次显示了类比，但请注意其中的差异。第一个差异是左侧的 *w* 和 *v* 依赖于 *f*，而右侧只有𝜑依赖于 *f*。第二个差异是，对于固定输入
    *d*，右侧分解的情况下，允许部分评估的独立变量数是固定的 (2*d* + 1)，而左侧近似的情况下，*n* = *n*(ε) 依赖于近似误差ε。这是一个重要的点。
- en: The dimension *n* ofthe space on which a given function is approximable by a
    σ-neuron up to a required precision is the ***width*** *of the neuron*. The Neural
    Approximation Theorem says that for any continuous function, there is a wide enough
    neuron that will approximate it up to any required precision. This is the essence
    of **wide learning**. The idea of approximating continuous functions by a linear
    combination of copies of σ is similar to Lebesgue’s idea to approximate an integrable
    function by a linear combination of step functions. In both cases, closer approximations
    are achieved by larger numbers *n* of approximants.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一个给定函数可以通过σ神经元逼近到所需精度的空间维度 *n* 被称为神经元的***宽度***。神经近似定理表明，对于任何连续函数，都存在一个足够宽的神经元，可以将其逼近到任意所需精度。这就是**宽学习**的本质。通过线性组合σ的副本来逼近连续函数的思想，类似于勒贝格（Lebesgue）将可积函数通过阶梯函数的线性组合来逼近的思想。在这两种情况下，通过增加近似者
    *n* 的数量，可以实现更精确的逼近。
- en: '**Wide neural networks.** Everything stated for continuous real functions lifts
    without much ado to continuous *vector* functions. For finite dimensions, they
    are just tuples of continuous real functions. The approximations by σ-neurons
    lift to tuples of σ-neurons, a.k.a. the *single-layer neural networks*. The tupling
    step is the step from left to right in the following diagram.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**宽神经网络。** 对于连续实函数的所有陈述在很少的变化下适用于连续*向量*函数。对于有限维度，它们只是连续实函数的元组。通过σ神经元的逼近可以扩展到σ神经元的元组，也就是*单层神经网络*。元组化步骤是以下图示中从左到右的步骤。'
- en: '![](../Images/b97e862590919d34d3503b7ef4291b10.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b97e862590919d34d3503b7ef4291b10.png)'
- en: A *q*-tuple of neurons (*W*, ⟨*v1*|), (*W*, ⟨*v2|),…,(W, ⟨vq*|), bundled together,
    gives a single-layer neural network a = (*W*,⟨*v1 |,⟨v2* |,…,⟨*vq* |), more succinctly
    written a = (*W*,*V*), where *V* is the matrix with the ⟨*vj*| vectors as rows,
    like before. The Neural Approximation Theorem implies that every continuous vector
    function can be approximated with arbitrary precision by a sufficiantly wide single-layer
    neural network. The term *wide neural network* usually refers to a single-layer
    network. The circuit view in the top row of the last figure is aligned with the
    more abstract view the middle row, with layers of variables enclosed in boxes.
    This will come handy when the networks become deep.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *q* 元组的神经元（*W*，⟨*v1*|），（*W*，⟨*v2*|），...，（*W*，⟨vq*|）组合在一起，给出了一个单层神经网络 a =
    (*W*，⟨*v1*|，⟨v2*|，...，⟨*vq*|），更简洁地写作 a = (*W*，*V*)，其中 *V* 是包含 ⟨*vj*| 向量作为行的矩阵，和之前一样。神经近似定理意味着每个连续向量函数都可以通过一个足够宽的单层神经网络以任意精度进行逼近。术语
    *宽神经网络* 通常指的是单层网络。最后一张图的顶部行的电路视图与中间行的更抽象视图对齐，其中包含了用框围起来的变量层。随着网络变得更深，这将非常有用。
- en: 3.4\. Deep learning
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4. 深度学习
- en: '**Scaling up.** The trouble with wide learning is that there are simple functions
    where separating variables is hard, and the number *n* of variables that need
    to be separated increases exponentially with the dimension *d*. E.g., separating
    variables in function presenting a hemisphere in any dimension is hard. Although
    any continuous real function on a cube is approximable by a wide σ-neuron, and
    any continuous vector function by a single-layer network, the approximations are
    in the worst case intractable. The amounts of training data needed to extrapolate
    predictions also explode exponentially with the width[⁸](#f923).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩展规模。** 宽学习的问题在于，有些简单函数很难分离变量，并且需要分离的变量数 *n* 随着维度 *d* 的增加呈指数增长。例如，在任何维度中，分离表示半球的函数的变量是困难的。尽管任何立方体上的连续实函数都可以通过宽σ神经元进行逼近，并且任何连续向量函数都可以通过单层网络逼近，但在最坏情况下，逼近是无法处理的。用于外推预测的训练数据量也会随着宽度的增加呈指数级爆炸性增长[⁸](#f923)。'
- en: '**Narrowing by deepening.** The general idea of approximating a function *f*
    is to find an algorithm to transform the data'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过加深来缩小范围。** 逼近一个函数 *f* 的一般思路是找到一种算法来转换数据'
- en: '![](../Images/b7fafff38d31a32764e8d668ed255cf3.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7fafff38d31a32764e8d668ed255cf3.png)'
- en: and determine an approximatorwith
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 并确定一个近似器
- en: '![](../Images/e1a557e2c6252919270af0e31bbb4210.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1a557e2c6252919270af0e31bbb4210.png)'
- en: 'for a desired precision ε. The exponential growth of the width *n* of single-layer
    neural networks is thus tempered by descending through layers of deep neural networks,
    which look something like this:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于期望的精度 ε。单层神经网络宽度 *n* 的指数增长因此被通过深度神经网络的层级逐步下降所缓解，结构如下所示：
- en: '![](../Images/389c874787ee044fcb0f146c26f66016.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/389c874787ee044fcb0f146c26f66016.png)'
- en: At each inner layer, composing the input transformation *W* with the output
    transformation *V* of the preceding layer gives a composite *H.*
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一层内部，将输入变换 *W* 与前一层输出变换 *V* 组合起来形成复合 *H*。
- en: '![](../Images/200df926bd76a73ab51b20a0df936c06.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/200df926bd76a73ab51b20a0df936c06.png)'
- en: As a composite of linear operators, *H* is linear itself, and can be trained
    directly, forgetting about the *W*s and the *V*s. Deep neural networks are thus
    programs in the form **a**=(*W*, *H*1, *H*2, . . . , *HL*, *V*).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 作为线性算子的复合，*H* 本身是线性的，可以直接进行训练，忽略 *W* 和 *V*。因此，深度神经网络是形如 **a**=(*W*, *H*1, *H*2,
    . . . , *HL*, *V*) 的程序。
- en: '**Neural networks are learnable programs.** The [general learning process](#5da6)
    can be viewed as the process of program development. To learn a function *F* means
    to converge to a program **a** whose executions 𝒰*(x)***a** approximate *F(x)*.
    Learners are programmers. It is true that the goal of programming is not just
    to approximate a function, but to precisely implement it. Ideally, a program **a**
    for a function *F* should satisfy 𝒰(*x*)**a=***F(x)*. In reality, a program implements
    a function only up to a correctness gauge ℒ(𝒰(*x*)**a,***F(x))*, realized through
    program testing or software assurance methodologies. Programming can be viewed
    as a special case of learning.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络是可学习的程序。** [通用学习过程](#5da6) 可以视为程序开发的过程。学习一个函数 *F* 意味着收敛到一个程序 **a**，其执行
    𝒰*(x)***a** 近似 *F(x)*。学习者是程序员。确实，编程的目标不仅仅是近似一个函数，而是精确地实现它。理想情况下，函数 *F* 的程序 **a**
    应该满足 𝒰(*x*)**a=***F(x)*。在现实中，一个程序实现一个函数，只能做到一定的正确性标准 ℒ(𝒰(*x*)**a,***F(x))*，通过程序测试或软件保证方法来实现。编程可以视为学习的一个特例。'
- en: We have already seen many of the main features of the syntax of neural networks
    as a programming language. A single neuron a = ⟨*w* | is an atomic program expression.
    A single-layer network a = (*W*,*V*) is a single-instruction program. A deep network
    a = (*W*, *H*1, *H*2, . . . , *HL*, *V*) is a general program. Its inner layers
    are the program instructions. For simplicity, the inner layers are often bundled
    under a common name, say **h** = (*H*1,*H*2,…,*HL*). A general neural program
    is in the form **a** = (*W*,**h**,*V*).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了神经网络作为编程语言语法的许多主要特征。单个神经元 a = ⟨*w* | 是一个原子程序表达式。单层网络 a = (*W*,*V*) 是一个单指令程序。深度网络
    a = (*W*, *H*1, *H*2, . . . , *HL*, *V*) 是一个通用程序。其内部层次就是程序指令。为了简便起见，内部层次通常会被归为一个公共名称，例如
    **h** = (*H*1,*H*2,…,*HL*)。一个通用神经程序的形式是 **a** = (*W*,**h**,*V*)。
- en: '![](../Images/fc59c0af992bb2b1ae0724cce727bb24.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc59c0af992bb2b1ae0724cce727bb24.png)'
- en: 4\. Learning channels and paying attention
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 学习通道和关注
- en: '**The trouble with applying function learning to language** is that language
    is context-sensitive: the word “up”, for instance, means one thing in “shut up”
    and another thing in “cheer up”. We talked about this in the [*Beyond sintax*](/syntax-the-language-form-612257c4aa5f#8904)section
    of the [*Syntax* part](/syntax-the-language-form-612257c4aa5f). A function is
    required to assign to each input the same unique output in all contexts. Meaning
    is not a function but a communication channel, assigning to each context a probability
    distribution over the concepts *y*:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**将函数学习应用于语言的难题** 在于语言是上下文敏感的：例如，“up”一词在“shut up”中有一种含义，在“cheer up”中则有另一种含义。我们在
    [*超越语法*](/syntax-the-language-form-612257c4aa5f#8904)部分的 [*语法*](/syntax-the-language-form-612257c4aa5f)
    中讨论过这一点。一个函数要求在所有上下文中为每个输入分配相同的唯一输出。意义不是一个函数，而是一个通信通道，它为每个上下文分配一个关于概念 *y* 的概率分布：'
- en: '![](../Images/80c8793cecf6d6778cf83c2d31d8b07c.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80c8793cecf6d6778cf83c2d31d8b07c.png)'
- en: In the [*Semantics* part](/syntax-the-language-form-612257c4aa5f#4389), we saw
    how concepts are modeled as vectors, usually linear combinations of words. Meaning
    is thus a random variable *Y*, sampled over the concept vectors *y*. There is
    an overview of the channel formalism in the [*Dynamic semantics*](/syntax-the-language-form-612257c4aa5f#912e)section
    of the *Semantics* part. When there is no channel feedback, the context is the
    channel source
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*语义学*部分](/syntax-the-language-form-612257c4aa5f#4389)，我们看到概念是如何被建模为向量的，通常是单词的线性组合。意义因此是一个随机变量
    *Y*，在概念向量 *y* 上进行采样。在[*动态语义学*](/syntax-the-language-form-612257c4aa5f#912e)部分的*语义学*章节中有关于通道形式主义的概述。当没有通道反馈时，上下文就是通道源。
- en: '![](../Images/80787715e7edc3201d2471b68b420146.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80787715e7edc3201d2471b68b420146.png)'
- en: and the channel outputs are sampled according to the probabilities
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 通道输出根据概率进行采样
- en: '![](../Images/0cfd8750c1bda6afbcc2c9598e24e08b.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cfd8750c1bda6afbcc2c9598e24e08b.png)'
- en: 'You can think of the source *X* as a text and of the channel *F* as the process
    of translating the text to a text *Y* in another language:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将源文本 *X* 看作是文本，将通道 *F* 看作是将文本翻译成另一种语言的文本 *Y* 的过程：
- en: '![](../Images/a6a7677c1a3737a1299f59276384b2e4.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6a7677c1a3737a1299f59276384b2e4.png)'
- en: Similar interpretations subsume meaning, syntactic typing, classification, and
    generation under the channel model. The common denominator is the context dependency,
    be it syntactic or semantic, deterministic or stochastic. Semantic references
    can be remote. The meaning of a sentence in a novel may depend on a context from
    800 pages earlier. The meaning that you assign to something that an old friend
    says may be based on a model of their personality from years ago. To make it more
    complicated, remote references and long established channel models may change
    from context to context, whenever new information becomes available.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的解释将意义、句法类型、分类和生成归纳到通道模型下。共同点是上下文依赖性，无论是句法的还是语义的，是确定性的还是随机的。语义引用可能是远程的。一句小说中的话的意义可能依赖于800页前的上下文。你赋予某个老朋友说的话的意义，可能基于多年前的他们的个性模型。更复杂的是，远程引用和长期建立的通道模型可能会随着新信息的到来而在不同上下文中发生变化。
- en: 4.1 Channeling through concepts
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 通过概念进行通道化
- en: 'In different languages, semantical references are mapped to syntactic references
    in different ways. Mapping a Mandarin phrase to a French phrase requires deviating
    from the [syntactic dependency mechanisms](/syntax-the-language-form-612257c4aa5f#d37c)
    of the two languages. Good translators first understand the phrase in one language
    and then express what they understood in the other language. It is a two-stage
    process:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的语言中，语义引用以不同的方式映射到句法引用。将中文短语映射到法语短语需要偏离两种语言的[句法依赖机制](/syntax-the-language-form-612257c4aa5f#d37c)。优秀的翻译者首先理解一种语言中的短语，然后用另一种语言表达他们理解的内容。这是一个两阶段的过程：
- en: '![](../Images/84951a402995cbfd179617dddad2ed5b.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84951a402995cbfd179617dddad2ed5b.png)'
- en: '*E* is a *concept encoding* map, whereas *D* is *concept decoding*. Similar
    pattern came up in the [*Semantics* part](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#aeca),
    as instances of concept mining through *Singular Value Decomposition (SVD).*'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*E* 是 *概念编码* 映射，而 *D* 是 *概念解码*。类似的模式出现在[*语义学*部分](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#aeca)，作为通过
    *奇异值分解 (SVD)* 进行概念挖掘的实例。'
- en: '![](../Images/a17b04e6bf6b57527b95680b6cc989b3.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a17b04e6bf6b57527b95680b6cc989b3.png)'
- en: 'The concepts that are latent in a given data matrix M are mined as its singular
    values *λi*. Now compare the diagrams of [𝜎-neurons](#ef75) and [single-layer
    networks](#e52e) with the corresponding diagram of the SVD:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据矩阵 M 中潜在的概念被挖掘为其奇异值 *λi*。现在比较[𝜎-神经元](#ef75)和[单层网络](#e52e)的图示与 SVD 的相应图示：
- en: '![](../Images/c578eb08bf8e5d7e07cc1eb53e5b57d5.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c578eb08bf8e5d7e07cc1eb53e5b57d5.png)'
- en: 'A neural network approximates a continuous function by separating its variables
    and approximating the impact of each of them by a separate copy of the activation
    function 𝜎. The SVD algorithm decomposes a data matrix through a canonical basis
    eigenspaces, corresponding to the singular values of the matrix, viewed as the
    dominant concepts, spanning the concept space. The eigenspaces in the SVD are
    mutually orthogonal. The action of the data matrix boils down to multiplying each
    of them separately by the corresponding singular value. Both the neural network
    approximation and the SVD mine the latent concepts as the minimally correlated
    subspaces, preferrably orthogonal at each other. The diagrams display the same
    three-step pattern:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经网络通过分离其变量，并通过激活函数𝜎的单独副本来逼近每个变量的影响，从而逼近一个连续函数。SVD算法通过一个规范基特征空间将数据矩阵分解，对应于矩阵的奇异值，视为主导概念，扩展概念空间。SVD中的特征空间是相互正交的。数据矩阵的作用归结为分别用相应的奇异值乘以它们。神经网络逼近和SVD都挖掘潜在概念，作为最小相关的子空间，优选地在每个子空间之间正交。这些图表展示了相同的三步模式：
- en: '![](../Images/b6338e436c878672cb2be82a18fc8ecc.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6338e436c878672cb2be82a18fc8ecc.png)'
- en: '**encoding** of inputs in terms of concepts,'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入的概念编码**，'
- en: '**separate processing** of each concept,'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**各个概念的独立处理**，'
- en: '**decoding** of the concepts into the output terms.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码**概念到输出术语的过程。'
- en: 'The three steps serve different purposes in different ways:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这三步以不同的方式服务于不同的目的：
- en: '![](../Images/ee3005c689e6e1335f5a54171b99090a.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee3005c689e6e1335f5a54171b99090a.png)'
- en: But difference (1) causes differences (2–3). When the function *F* happens to
    be linear and difference (1) disappears, the neural network converges to the SVD
    and differences (2–3) also disappear. Neural networks also mine latent concepts,
    like the SVD. They just learn them from arbitrary continuous functions.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 但差异(1)会导致差异(2-3)。当函数*F*恰好是线性并且差异(1)消失时，神经网络会收敛到SVD，并且差异(2-3)也会消失。神经网络也挖掘潜在概念，就像SVD一样。它们只是从任意连续函数中学习这些概念。
- en: 4.2 Static channel learning
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 静态通道学习
- en: A network of neural networks is static if it processes its inputs by applying
    the same neural network **h** on all channel inputs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经网络是静态的，如果它通过在所有通道输入上应用相同的神经网络**h**来处理其输入。
- en: '***n*-grams of concepts.** As a warmup, suppose that we want to make a static
    network of networks slightly context-sensitive by taking into account at the *j*-th
    step not only *Xj* but also *Xj*−1, for all *j* ≥ 2.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '***n*-grams概念。** 作为热身，假设我们想通过在第*j*步考虑不仅是*Xj*，而且还包括*Xj*−1，使一个静态的神经网络稍微具有上下文敏感性，其中对于所有*j*
    ≥ 2。'
- en: '![](../Images/0d46519e956b5bf86ffadfafa3d67592.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d46519e956b5bf86ffadfafa3d67592.png)'
- en: 2-grams of concepts
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 2-grams概念
- en: The weights *T* are updated in the same way as *W*, by minimizing the losses
    and propagating the updates back from layer to layer. They just add one training
    step per layer. This is not a big deal structurally, but it is a significant slowdown
    computationally. If the inner layers are viewed as latent concept spaces, then
    this architecture can be thought of as a lifting of the idea of 2-grams (capturing
    the dependencies of contests of length 2) from words to concepts. Generalizing
    to *n*-grams for larger *n*s causes further slowdown.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 权重*T*的更新方式与*W*相同，通过最小化损失并从层到层传播更新。它们只是每一层增加一步训练。这在结构上并不重要，但在计算上却是一个显著的减速。如果将内部层视为潜在概念空间，那么该架构可以看作是将2-grams（捕捉长度为2的上下文依赖性）这一思想从词语扩展到概念的提升。将其推广到更大的*n*-grams会导致进一步的减速。
- en: '**Recurrent Neural Networks (RNNs).** The RNNs also apply the same neural network
    on all input tokens and also pass to the *j*-th module not only *Xj* but also
    the information from *Xj*−1 — *b*ut they pass it *after* the previous network
    module was applied to *Xj*−1, not before.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络 (RNNs)**。RNNs也会在所有输入标记上应用相同的神经网络，并且不仅将*Xj*，而且还将来自*Xj*−1的信息传递到*j*-th模块——*但是*它们是在应用前一个网络模块于*Xj*−1之后，而不是之前。'
- en: '![](../Images/44981b49fbda774477e8199703430f31.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44981b49fbda774477e8199703430f31.png)'
- en: RNN idea
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的思路
- en: Note that information from *Xj*−1 is this time forwarded by *S* not only to
    the *j*-th module, but with the output of the *j*-th module by the next copy of
    *S* also to the *(j+1)*-st module, and so on. The information propagation is thus
    in principle unbounded, and not truncated like in the *n*-gram model. The matrices
    *S* that propagate important information further are promoted in training. However,
    the weights assigned to all input entries are all packed in *S*. Propagating longer
    contexts requires exponentially wider network modules. So we are back to square
    one, the problem of width.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*Xj*−1 中的信息这次不仅由 *S* 转发到 *j* 号模块，而且在 *j* 号模块的输出通过 *S* 的下一复制品转发到 *(j+1)* 号模块，依此类推。因此，信息传播原则上是无限的，而不像
    *n*-gram 模型那样被截断。那些进一步传播重要信息的矩阵 *S* 会在训练中得到提升。然而，分配给所有输入条目的权重都被打包在 *S* 中。传播更长的上下文需要指数级扩展的网络模块。因此，我们回到了最初的问题，宽度问题。
- en: '**Long Short-Term Memory (LSTM).** The LSTM networks address the problem of
    the cost of forwarding the context information between the iterations of the same
    neural network module by forwarding the information from the *(j−1)*-th input
    token to the *j*-th module both before it was processed by the *(j−1)*-th module,
    and after. The former makes passing the information from each input more efficient,
    the latter makes the propagation easier.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**长短时记忆（LSTM）**。LSTM 网络通过将 *(j−1)* 号输入标记的信息同时转发到 *j* 号模块（既在 *(j−1)* 号模块处理之前，又在其处理之后）来解决在同一神经网络模块的迭代之间转发上下文信息的成本问题。前者使得从每个输入传递信息更有效，后者使得传播更加容易。'
- en: '![](../Images/67ce3d32737e914c5b323b1b7c70ff64.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67ce3d32737e914c5b323b1b7c70ff64.png)'
- en: LSTM idea
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 思路
- en: The idea of passing around the information at different stages of processing
    is simple enough, but optimizing the benefits is a conundrum, as already the “long
    short” name may be suggesting. The implementation details are many. Different
    activation functions are applied on different mixtures of the same inputs and
    remixed in different ways for the outputs. Expressing the concepts learned from
    the same data in multiple bases requires multiple matrices and provides more opportunities
    for training. Hence for improvements. But further steps require further ideas.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 将信息在不同的处理阶段传递的想法相当简单，但优化其效益却是一个难题，正如“长短”这个名字可能已经暗示的那样。实现细节有很多。不同的激活函数应用于相同输入的不同混合，并以不同的方式重新组合用于输出。从相同数据中学习的概念在多个基中表达需要多个矩阵，并提供更多的训练机会。因此，有了改进的可能。但是，进一步的步骤需要更多的想法。
- en: 4.3 Dynamic channel learning
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 动态通道学习
- en: Just like the function learner, the channel learner seeks to learn how the inputs
    are transformed into the outputs. The difference is that the channel transformations
    are context-dependent. Not only are the outputs always dependent on the input
    contexts, but there may be feedforward dependencies of outputs on outputs, and
    feedback dependencies of inputs on outputs, as discussed in the [channel section](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#d3b1)
    of the [*Semantics* part](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41).
    A dynamic network of neural networks learns a channel by adaptively updating the
    “key” subnetworks **k***,* processing the channel inputs, and the “value” subnetworks
    **v,** delivering the corresponding channel outputs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 就像函数学习者一样，通道学习者也试图学习如何将输入转换为输出。不同之处在于，通道变换是依赖上下文的。输出不仅总是依赖于输入上下文，而且可能存在输出对输出的前馈依赖关系，以及输入对输出的反馈依赖关系，正如在[通道部分](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#d3b1)的[*语义*部分](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41)中讨论的那样。一个动态的神经网络网络通过自适应更新“键”子网络**k***，处理通道输入，以及“值”子网络**v**，提供相应的通道输出，从而学习一个通道。
- en: '**Encoder-Decoder Procedures.** An important programming concept is the idea
    of a *procedure*. While the earliest programs were just sequences of program instructions,
    procedures enabled programmers to invoke within programs not just instructions
    but also entire programs, encapsulated in procedures as generalized instructions.
    Since procedures can be used inside most program control structures, this enabled
    ***programming over programs***, and gave rise to software engineering. The later
    programming paradigms, modular, object-oriented, component and connector-oriented,
    extend this basic idea.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码器-解码器程序**。一个重要的编程概念是*过程*的概念。最早的程序只是程序指令的序列，而过程使得程序员不仅能在程序中调用指令，还能调用整个程序，这些程序被封装在过程内，作为一般化的指令。由于过程可以在大多数程序控制结构中使用，这使得***程序编程程序***成为可能，并催生了软件工程。后来的编程范式，如模块化编程、面向对象编程、组件和连接器导向编程，都扩展了这个基本概念。'
- en: The encoder-decoder architecture is a ***network of neural networks***. If neural
    networks are thought of as programs, it is a program over programs. The encoder-decoder
    architecture **A** = (**e**, **d**) lifts the structure **a** = (*W*, *V*) of
    a wide neural network to a network of networks.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器架构是一个***神经网络的网络***。如果将神经网络看作程序，它就是一个“程序中的程序”。编码器-解码器架构**A** = (**e**,
    **d**) 将宽神经网络**a** = (*W*, *V*)的结构提升为一个网络的网络。
- en: '![](../Images/f8c7a4f420ed913c2fe3f243a8df4626.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8c7a4f420ed913c2fe3f243a8df4626.png)'
- en: Encoder-decoder
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器
- en: The input remixing matrix *W* is replaced by an encoder network **e**, the output
    remixing matrix *V* by a decoder network **d**. Both the wide network **a** and
    its lifting **A** follow the architectural pattern of [concept mining](#4f8f).
    Just like procedural programming allowed lifting control structures from programs
    to software systems, the encoder-decoder architecture allows lifting the concept
    mining structures from neural networks to neural architectures. The problem with
    the basic form of the encoder-decoder architecture as a concept mining framework
    is that a concept space induced by a static dataset is static whereas channels
    are dynamic. To genuinely learn concepts from a channel, a neural network architecture
    needs dynamic components.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 输入重混矩阵*W*被编码器网络**e**替代，输出重混矩阵*V*被解码器网络**d**替代。宽网络**a**及其提升**A**遵循[概念挖掘](#4f8f)的架构模式。就像过程式编程允许将控制结构从程序提升到软件系统一样，编码器-解码器架构也允许将概念挖掘结构从神经网络提升到神经架构。作为概念挖掘框架的编码器-解码器架构的基本形式的问题在于，由静态数据集引起的概念空间是静态的，而信道是动态的。为了真正从信道中学习概念，神经网络架构需要动态组件。
- en: '**Idea of attention.** A natural step towards enabling neural networks to predict
    the outputs of a channel'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力的概念**。使神经网络能够预测信道输出的一个自然步骤'
- en: '![](../Images/9d06990fcbde5f81cc2dfe1f5e034331.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d06990fcbde5f81cc2dfe1f5e034331.png)'
- en: is to generalize the basic 𝜎-neuron from the [CHSW construction](#5646)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 是将基本的 𝜎-神经元从[CHSW构造](#5646)中推广
- en: '![](../Images/9837ae9b75a6e5f0b870d1cf3ae0a1ca.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9837ae9b75a6e5f0b870d1cf3ae0a1ca.png)'
- en: In this format, a component of an Encoder-Decoder procedure output would be
    something like
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种格式中，编码器-解码器程序输出的一个组成部分可能是
- en: '![](../Images/b2cd3b620caf1f205ee938fc9cd86c54.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2cd3b620caf1f205ee938fc9cd86c54.png)'
- en: where
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../Images/0214787a853c0c7be7df4df427a10cf1.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0214787a853c0c7be7df4df427a10cf1.png)'
- en: are basic encoder and decoder matrices from a [channel concept mining framework](#4f8f).
    But now we need to take into account the concepts learned at inner layers of a
    deep network. The impacts on the (*n+1)-*st output value of the input vectors
    |*xj*⟩ are therefore weighed by their projections ⟨*ej* | *xj*⟩ on the input concepts
    ⟨*ej*| *and* the projections ⟨*yn* | *dj*⟩ of the row vector ⟨*yn*| of the previous
    outputs on the output concepts |*dj* ⟩. The relationship between the corresponding
    concepts ⟨*ej*| and |*dj* ⟩ are trained to align the channel inputs and the channel
    outputs. This is the basic idea of the *attention architecture*. It can be drawn
    as a common generalization of the [𝜎-neuron](#ef75) and the [SVD-schema](#46fc),
    with dynamic singular values. (This is an instructive **exercise**.) For string
    outputs, the obvious extension is
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 是来自 [通道概念挖掘框架](#4f8f)的基本编码器和解码器矩阵。但现在我们需要考虑在深度网络的内部层学习到的概念。输入向量 |*xj*⟩ 对 (*n+1)-*st
    输出值的影响因此通过它们在输入概念 ⟨*ej*| 上的投影 ⟨*ej* | *xj*⟩ 以及前一输出的行向量 ⟨*yn*| 在输出概念 |*dj* ⟩ 上的投影
    ⟨*yn* | *dj*⟩ 来加权。相应概念 ⟨*ej*| 和 |*dj* ⟩ 之间的关系被训练以对齐通道输入和通道输出。这就是 *注意力架构* 的基本思想。它可以被绘制为
    [𝜎-神经元](#ef75) 和 [SVD-模式](#46fc) 的常见广义形式，具有动态奇异值。（这是一个有教育意义的 **练习**。）对于字符串输出，显而易见的扩展是
- en: '![](../Images/a9e071205f4d39ec6785272b9402232b.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9e071205f4d39ec6785272b9402232b.png)'
- en: But it is not obvious how to train the matrix *V* whose rows are the output
    mixtures ⟨*vi* |. The issue is solved by approaching the task from a slightly
    different direction.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如何训练矩阵 *V* 的问题并不显而易见，矩阵的行是输出混合体 ⟨*vi* |。这个问题通过从略有不同的方向来解决。
- en: '**Dynamic concept decomposition.** A set of vectors *|1⟩,|2⟩,…,|n⟩* is said
    to span the vector space if every vector |*y*⟩ can be expressed in the form'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态概念分解。** 一组向量 *|1⟩,|2⟩,…,|n⟩* 被称为能够生成向量空间，如果每个向量 |*y*⟩ 都可以表示为以下形式'
- en: '![](../Images/bec39bcf8e4bfd5a0170e95f88c5b9fa.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bec39bcf8e4bfd5a0170e95f88c5b9fa.png)'
- en: The decomposition is unique if and only if the vectors *|1⟩,…,|n⟩* are orthogonal,
    in the sense that ⟨*i*|*j*⟩ = 0 as soon as *i≠j*. If the spanning vectors are
    not orthogonal, but there is an orthogonal set *|c1⟩,|c2⟩,…,|cn⟩*, then there
    is a unique decomposition
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 当且仅当向量 *|1⟩,…,|n⟩* 彼此正交时，分解是唯一的，这意味着当 *i≠j* 时，⟨*i*|*j*⟩ = 0。如果生成向量不是正交的，但存在一个正交集合
    *|c1⟩,|c2⟩,…,|cn⟩*，那么就存在一个唯一的分解。
- en: '![](../Images/1745a488eda1099938cb5ae274429a4f.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1745a488eda1099938cb5ae274429a4f.png)'
- en: As discussed [earlier](#4f8f), concept analysis is the quest for concept bases
    with minimal interferences between the basic concepts. The basic concept vectors
    do not interfere at all when they are mutually orthogonal. If a channelis implemented
    by a neural network, the above concept decomposition becomes
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [前文](#4f8f)讨论的那样，概念分析是寻求具有最小干扰的基本概念基。基本概念向量在相互正交时完全不干扰。如果通道通过神经网络实现，以上的概念分解变为
- en: '![](../Images/01935a41b836071a7c373f248f77b249.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01935a41b836071a7c373f248f77b249.png)'
- en: The first difference is that the activation function σ allows approximating
    nonlinearities. The second is that the components are not projected on the original
    basis vectors ⟨*i*| anymore but on the output mixtures ⟨*vi*|. Lastly and most
    importantly, the concept decomposition was unique because the concept basis *|c1⟩,…,|cn⟩*
    was orthogonal, whereas here the output is projected on the inputs *|x1⟩,…,|xn⟩*,
    which are not orthogonal. But if an orthogonal concept basis *|c1⟩,…,|cn⟩* exists,
    we can play the same trick again, and get a unique concept decomposition
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个不同之处在于激活函数 σ 允许近似非线性。第二个不同之处在于，组件不再投影到原始基向量 ⟨*i*| 上，而是投影到输出混合体 ⟨*vi*| 上。最后也是最重要的一点，概念分解是独特的，因为概念基
    *|c1⟩,…,|cn⟩* 是正交的，而这里的输出投影到输入 *|x1⟩,…,|xn⟩*，它们并不是正交的。但如果存在正交的概念基 *|c1⟩,…,|cn⟩*，我们可以再次使用相同的技巧，得到唯一的概念分解。
- en: '![](../Images/50fd452382b15f00fcf7b2c5a0b7077f.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50fd452382b15f00fcf7b2c5a0b7077f.png)'
- en: What does this abstract decomposition mean for a concrete channel? The projections
    on the concept basis vectors measure their weights in the inputs |*xj*⟩ and in
    the outputs |*yn*⟩. The sum of the products of the projections measures the impact
    of the input |*xj*⟩ on the output |*yn*⟩. This measurement, activated by σ, then
    impacts the *i*-th component of the channel output | *yn* ⟩ according to the projection
    ⟨*vi* | *xj*⟩.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这种抽象分解对于具体通道意味着什么？在概念基向量上的投影衡量了它们在输入 *|xj*⟩ 和输出 *|yn*⟩ 中的权重。投影乘积的和衡量了输入 *|xj*⟩
    对输出 *|yn*⟩ 的影响。这个测量由 σ 激活，然后根据投影 ⟨*vi* | *xj*⟩ 影响通道输出 *|yn*⟩ 的第 *i* 个分量。
- en: The only problem is that the projection of |*yn*⟩ on the right is unknown since
    |*yn*⟩ is what we are trying to predict. What other value can be used to approximate
    the impact of a concept in the output |*yn*⟩? — Two answers have been proposed.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的问题是，右侧的 *|yn*⟩ 投影是未知的，因为 *|yn*⟩ 正是我们试图预测的输出。可以用什么其他值来近似概念在输出 *|yn*⟩ 中的影响呢？—
    已经提出了两种答案。
- en: '**Translator’s attention:** If the channel *F* : *[X* ⊢ *Y]* is a [translation](#d171),
    say from Mandarin to French through concepts *|c*⟩, then the summands ⟨*xj*|*c*⟩⟨*c*|*yn*⟩
    can be thought of as distributing *translator’s attention* over the concepts *|c*⟩,
    latent in the Mandarin input tokens |*xj*⟩ *after* the French output token |*yn*⟩
    is produced. That is the attention that effectively impacts the *(n+1)*-st output,
    and the above decomposition should be updated to'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**翻译者的注意力：** 如果通道 *F*： *[X* ⊢ *Y]* 是[翻译](#d171)，例如通过概念 *|c*⟩ 从中文翻译到法语，那么加法项
    ⟨*xj*|*c*⟩⟨*c*|*yn*⟩ 可以被看作是将 *翻译者的注意力* 分布到中文输入符号 *|xj*⟩ 上，而这种注意力是在法语输出符号 *|yn*⟩
    产生之后潜藏在 *|c*⟩ 上的。这就是实际上影响 *（n+1）* 输出的注意力，而上述分解应更新为'
- en: '![](../Images/72516dceb2c5967cdb28f109a60ebe17.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72516dceb2c5967cdb28f109a60ebe17.png)'
- en: '**Speaker’s self-attention:** If the channel *F*: *[X* ⊢ *Y]* is not a translation
    to another language but a continuation *Y* in the same language, then it is not
    [feedback-free](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#aba3),
    since *Xn*+1 is not independent of *Yn*, but identical to it. To capture the feedback,
    the concept base splits into an encoding base and a decoding base as [above](#5b17),
    expressing the inputs as mixtures of concepts, and the concepts as mixtures of
    outputs. But since each output is now recast as the next input, the *encoder-decoder*
    view morphs into the *key-query-value* view of language production as an ongoing
    stochastic process of database retrieval, with the projections of the inputs to
    the queries modeling a rudimentary “attention span” over the preceding context:'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**说话者的自注意力：** 如果通道 *F*： *[X* ⊢ *Y]* 不是对另一种语言的翻译，而是同一种语言中的延续 *Y*，那么它就不是[无反馈](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#aba3)，因为
    *Xn*+1 并不是独立于 *Yn*，而是与其相同。为了捕捉反馈，概念基被分为编码基和解码基，如[上面](#5b17)所示，将输入表示为概念的混合，而将概念表示为输出的混合。但由于每个输出现在都被重新定义为下一个输入，*编码器-解码器*视角转变为语言生成的*键-查询-值*视角，作为数据库检索的持续随机过程，其中输入的投影到查询上模拟了前文上下文的基本“注意力范围”：'
- en: '![](../Images/a5872daaf3b6d10313b4ffce8fb043fe.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5872daaf3b6d10313b4ffce8fb043fe.png)'
- en: The self-attention modules in the form **a** = (*K*, *Q*, *V*), with
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力模块形式为 **a** = (*K*, *Q*, *V*)，其中
- en: '![](../Images/c8d1926aa0cee155b4691ff40706ba9a.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8d1926aa0cee155b4691ff40706ba9a.png)'
- en: 'are the central components of the *transformer* architecture, which is the
    “T” of the GPTs. They can be viewed as a step towards capturing the general language-production
    channels discussed in the [Semantics part](https://medium.com/p/99b009ccef41#d3b1),
    with the query vectors capturing the basic feedback flows and the value vectors
    capturing the feedforward flows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 是 *transformer* 架构的核心组件，它是 GPTs 的“T”部分。它们可以看作是捕捉[语义部分](https://medium.com/p/99b009ccef41#d3b1)中讨论的通用语言生成通道的一步，其中查询向量捕捉了基本的反馈流，值向量捕捉了前馈流：
- en: '![](../Images/cc442b45cc44e834929a4b9d97bfd248.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc442b45cc44e834929a4b9d97bfd248.png)'
- en: Reconciling the intuitions for attention as a mental process with the database
    terminology for key-query-value may feel awkward at first, yet the time may be
    ripe to expand our intuitions and recognize the same natural processes unfurling
    in our heads and in computers.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 将注意力作为一种心理过程的直觉与数据库术语中的键-查询-值对接起来，最初可能感觉有些尴尬，但或许是时候拓展我们的直觉，意识到这些自然过程在我们的大脑和计算机中是相同的。
- en: 5 Beyond hallucinations
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 超越幻觉
- en: 5.1 Parametric learning framework
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 参数化学习框架
- en: Staring back at the [general learning framework](#e77a), after a while you realize
    that the transformer architecture uncovered a feature that was not visible in
    the [learning diagram](#5da6) there. It uncovered that *models and their programs
    can be parametrized.*
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下[通用学习框架](#e77a)，你会意识到Transformer架构揭示了一个在[学习图](#5da6)中看不到的特性。它揭示了*模型及其程序可以被参数化*。
- en: '![](../Images/4dbff571ec4acb9e2d41c0dffb262fa2.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dbff571ec4acb9e2d41c0dffb262fa2.png)'
- en: 'A learner can train a model **a**(*X*) that captures the dependencies of a
    channel *F* on *n-*input contexts, for any *n*, and leave the *(n+1)*-st input
    *X* as a program parameter. When the *(n+1)*-st input is sampled to *X=*|*x*⟩,
    the model is instantiated to **a**|*x*⟩*.* Interpreting this instance produces
    a prediction of the next channel output:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 学习者可以训练一个模型**a**(*X*)，该模型捕捉了通道*F*对*n*输入上下文的依赖关系，对于任何*n*，并将*(n+1)*-st输入*X*作为程序参数。当*(n+1)*-st输入被采样为*X=*|*x*⟩时，模型实例化为**a**|*x*⟩*。解释该实例会产生对下一个通道输出的预测：
- en: '![](../Images/6864d58788dac8258619861567c89542.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6864d58788dac8258619861567c89542.png)'
- en: Transformers are parametric programs in the form a(*X*) = *(K*, *Q*(*X*),*V)*.
    Parametricity is an important feature of computation, arising from the partial
    interpretability of programs, which propagates to machine learning as the partial
    learnability of models. While the partial evaluation of programs grew from Gödel’s
    Substitution Lemma and Kleene’s Smn Theorem into a practical programming methodology,
    the partial learnability of models seems to have evolved in practice and, as far
    as I can tell, awaits a theory. In the rest of this note, I sketch some preliminary
    ideas[⁹](http://ed33).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer是形式为a(*X*) = *(K*, *Q*(*X*),*V)*的参数化程序。参数化性是计算的重要特征，源自程序的部分可解释性，这一特征传播到机器学习中，表现为模型的部分可学习性。虽然程序的部分评估从哥德尔的替代引理和克莱尼的Smn定理发展为一种实用的编程方法论，但模型的部分可学习性似乎是在实践中逐渐演变的，据我所知，它仍待理论化。在本文的其余部分，我将勾画一些初步的想法[⁹](http://ed33)。
- en: 5.2 Self-learning
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 自我学习
- en: The parametric learning framework in the [above diagram](#e981) captures the
    learning scenarios where learners interact and learn to predict each other’s behaviors.
    This includes not only conversations between different learning machines, or betewen
    different instances of the same machine, but also a self-learning process where
    a learning machine learns to predict its own behaviors modulo a parameter. A framework
    for such self-learning can be obtained by instantiating the supervisor *F* in
    the [parametric learning framework](#16d1) to
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[上图](#e981)中的参数学习框架捕捉了学习者相互作用并学习预测彼此行为的学习场景。这不仅包括不同学习机器之间的对话，或同一机器不同实例之间的对话，还包括一种自我学习过程，在这种过程中，学习机器学习预测其自身行为，按参数进行调整。通过实例化[参数学习框架](#16d1)中的监督者*F*，可以得到这样一种自我学习框架。'
- en: '![](../Images/09e09095ad8f65befca4d21eb0961a38.png)![](../Images/35dbb3bbaebdce7bcdeff653f4328efe.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09e09095ad8f65befca4d21eb0961a38.png)![](../Images/35dbb3bbaebdce7bcdeff653f4328efe.png)'
- en: 'The obtained model **s** of *self* allows the learner to predict its own future
    behaviors, as parametrized by future inputs:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的**s**模型可以让学习者预测其未来行为，预测是通过未来输入参数化的：
- en: '![](../Images/82ac4b6d98c393efbb04919680f7d220.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82ac4b6d98c393efbb04919680f7d220.png)'
- en: This framework also captures the cases of unintended self-learning, where learning
    machines get trained on corpora saturated by their own outputs, due to overproduction
    and overuse, in a process familiar from other industries that tap into natural
    resources.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架还捕捉了无意自我学习的情况，在这些情况下，学习机器因过度生产和过度使用，在充满自身输出的语料库上进行训练，这一过程类似于其他行业在开采自然资源时的情况。
- en: '**Predicting effects of predictions.** Learning the consequences of what we
    do sometimes impacts what we do. *To make e*ff*ective predictions, the learner
    must take into account the e*ff*ects of their predictions.* Parametric learning
    provides a framework for that. The learner’s capability to predict the effects
    of their predictions allows them to steer the predictions in the desired direction.
    This is how intentionally self-fulfilling prophecies, self-validating, or self-invalidating
    theories come about. A particularly interesting and worrying case is presented
    by *adaptive* theories, designed to pass all tests by reinterpreting their predictions.
    Such logical phenomena are ubiquitous in history, culture, and religions[¹⁰](#4f51).
    The learning machines surely evolve such processes faster and more methodically.
    The method to produce them is based on the learner’s model **s**of self.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测预测的效果。** 学习我们行为的后果有时会影响我们自己的行为。*为了做出有效的预测，学习者必须考虑到他们预测的效果。* 参数化学习为此提供了框架。学习者预测自己预测的效果的能力使得他们能够将预测引导到期望的方向。这就是有意的自我实现预言、自我验证或自我否定理论的形成方式。一种特别有趣且令人担忧的情况是*自适应*理论，它们通过重新解释预测来通过所有测试。这类逻辑现象在历史、文化和宗教中无处不在[¹⁰](#4f51)。学习机无疑能更快更有条理地进化出这些过程。生成它们的方法基于学习者关于自我的模型**s**。'
- en: 5.3 Self-confirming beliefs
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 自我确认信念
- en: '**Learning is believing.** A model **a** of a process *F* expresses a *belief*
    held by the learner 𝒰 about *F*. The learner updates the belief as they learn
    more. Learning is belief *updating*.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习就是相信。** 一个过程*F*的模型**a**表示学习者𝒰对*F*的*信念*。学习者在学习更多的过程中更新这个信念。学习就是信念的*更新*。'
- en: '**Beliefs impact their own truth values.** Our beliefs have impacts on what
    we do, and what we do changes some aspects of reality: we change the world by
    moving things around. Since the reality determines whether our beliefs are true
    or false, and our beliefs, through our actions, change some aspects of reality,
    it follows that our beliefs may change their own truth values. Accusing an honest
    person of being a criminal may drive them into crime. Entrusting a poor but honest
    person with a lot of money may transform them into a rich and dishonest person.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**信念影响它们自己的真值。** 我们的信念对我们的行为有影响，而我们的行为又会改变现实的某些方面：我们通过移动物体来改变世界。既然现实决定了我们的信念是真还是假，而我们的信念通过我们的行动改变现实的某些方面，那么我们的信念可能会改变它们自己的真值。指控一个诚实的人是罪犯，可能会推动他走上犯罪道路。将大量金钱交给一个贫穷但诚实的人，可能会将他变成一个富有且不诚实的人。'
- en: '**Making self-confirming predictions.** If *B*ob uses a learning machine 𝒰riel
    to decide what to do, then 𝒰riel can learn a model **b** that will always move
    *B*ob to behave as predicted by **b**. If *B*ob shares 𝒰riel’s beliefs, then those
    beliefs will be confirmed by *B*ob’s actions.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**做出自我确认的预测。** 如果*B*ob使用学习机𝒰riel来决定做什么，那么𝒰riel可以学习一个模型**b**，使得*B*ob的行为总是按照**b**预测的方式进行。如果*B*ob与𝒰riel的信念一致，那么这些信念将通过*B*ob的行为得到确认。'
- en: To spell out the learning process that 𝒰riel can use to construct the self-confirming
    belief **b**, suppose that *B*ob’s behavior is expressed through a channel *B.*
    The assumption that *B*ob uses 𝒰riel to decide what to do can be formalized by
    taking the outputs of the channel to be in the form
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 为了阐明𝒰riel可以用来构建自我确认信念**b**的学习过程，假设*B*ob的行为通过通道*B*来表达。假设*B*ob使用𝒰riel来决定做什么，可以通过将通道的输出形式化来实现：
- en: '![](../Images/bd48e3aa95901ffed3e7df82c3e01bab.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd48e3aa95901ffed3e7df82c3e01bab.png)'
- en: 'meaning that *B*ob consults 𝒰riel and believes that the model **a**(X) explains
    the input *X*. The claim is that 𝒰riel can then find a model **b**(*X*) that will
    cause *B*ob to act as **b**(*X*) predicts:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着*B*ob咨询𝒰riel并且相信模型**a**(X)解释了输入*X*。声明是，𝒰riel可以找到一个模型**b**(*X*)，使得*B*ob按照**b**(*X*)的预测行为：
- en: '![](../Images/e7cd0a0ce2813d07f9ad96161ecbd171.png)![](../Images/f35db90a77e7d7de1097cab618383916.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7cd0a0ce2813d07f9ad96161ecbd171.png)![](../Images/f35db90a77e7d7de1097cab618383916.png)'
- en: 'To learn **b**(X), 𝒰riel first learns a model 𝛽 of *B* instantiated to 𝒰riel’s
    model of self:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习**b**(X)，𝒰riel首先学习一个模型𝛽，它是*B*针对𝒰riel自我模型的实例化：
- en: '![](../Images/ad3792fc40ae36902ef4246d9e62a3ea.png)![](../Images/73689071181dce69471feb1a076f57f8.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad3792fc40ae36902ef4246d9e62a3ea.png)![](../Images/73689071181dce69471feb1a076f57f8.png)'
- en: Processing the 𝛽-explanation of the (2n+1)-th input as the (2n+2)-th input,
    the definition of 𝛽 yields
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 将𝛽解释处理为(2n+1)-th输入作为(2n+2)-th输入，𝛽的定义给出
- en: '![](../Images/94838ff55f41829384def431313e8864.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94838ff55f41829384def431313e8864.png)'
- en: 'The claimed self-confirming model is now defined:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 所声明的自我确认模型现在被定义为：
- en: '![](../Images/ae43a9b27cac920658fadc15cd938652.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae43a9b27cac920658fadc15cd938652.png)'
- en: It satisfies the [claim](#3df3) because
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 它满足[声明](#3df3)，因为
- en: '![](../Images/942cf993cef33dbe6ac1ce71918970d5.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/942cf993cef33dbe6ac1ce71918970d5.png)'
- en: '**From learnable programs to unfalsifiable theories and self-fulfilling prophecies.**
    The insight that learning is like programming opens up a wide range of program
    fixpoint constructions. Applied on learning, such constructions produce models
    that steer their own truth, whether into self-confirmations or paradoxes, along
    the lines of logical completeness or incompleteness proofs. The above construction
    is one of the simplest examples from that range[⁹](#ed33). They prepare models
    and theories that absorb all future evidence, explain away counterexamples, and
    confirm predictions.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**从可学习的程序到无法证伪的理论和自我实现的预言。** 认知学习就像编程的洞察力，开启了广泛的程序不动点构造。在学习的应用下，这些构造产生的模型引导着它们自身的真理，无论是走向自我确认还是悖论，遵循逻辑完备性或不完备性的证明。上述构造就是该范围内最简单的例子之一[⁹](#ed33)。它们准备了吸收所有未来证据、解释反例并确认预测的模型和理论。'
- en: '![](../Images/019ecfb85ebe1c40e697024c75c57cbf.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/019ecfb85ebe1c40e697024c75c57cbf.png)'
- en: Attributions
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归属
- en: The color tableaus were authored by DALL-E, prompted by Dusk-o. The hand-drawn
    diagrams and icons were authored by Dusk-o, prompted in some cases by DALL-E.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这些色彩图案由DALL-E创作，由Dusk-o提出提示。手绘的图表和图标由Dusk-o创作，有时由DALL-E提出提示。
- en: The results presented in this note originate from many publications which would
    normally be listed in a bibliography. But while the bibliographic formats were
    standardized in the pre-web era, the subject of this text is post-web. In the
    meantime, during the web era, we got used to finding all references on the web.
    The students who used this lecture note were asked to find the relevant references
    using the keywords in the text. They needed additional information in a handful
    of places. I added more keywords and notes in these places. If proper references
    turn out to be needed, or if the reference system gets updated for actual use,
    a bibliography will be added.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 本文所呈现的结果来源于多篇文献，通常会在书目中列出。但在互联网时代之前，书目格式已被标准化，而本文的主题则属于后互联网时代。在此期间，我们已习惯在网络上找到所有的参考资料。使用这份讲义的学生被要求通过文中的关键词查找相关参考资料。在一些地方，他们需要更多的信息。我在这些地方添加了更多的关键词和注释。如果以后需要正式的参考文献，或者参考系统需要更新以便实际使用，书目将被添加。
- en: Notes
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注释
- en: ¹Alan Turing explained that machine intelligence could not be achieved through
    intelligent designs, because intelligence itself could not be completely specified,
    as it is its nature to always seek and find new paths. But Turing was also the
    first to realize that the process of computation was not bound by designs and
    specifications either, but could evolve and innovate. He anticipated that machine
    intelligence would evolve with computation. However, three years after Turing’s
    death, the concept of *machine intelligence*, which he thought and wrote about
    for the last 8 years of his life, got renamed to *artificial intelligence,* his
    writings sank into oblivion, and the logical systems designed to capture intelligence
    proliferated.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ¹艾伦·图灵解释说，机器智能无法通过智能设计实现，因为智能本身无法完全定义，因为它的本质是不断寻找和发现新路径。但图灵也是第一个意识到计算过程并不受设计和规范的束缚，而是可以发展和创新的人。他预见到机器智能将随着计算的发展而进化。然而，在图灵去世三年后，他在生命的最后八年里思考和写作的*机器智能*概念被重新命名为*人工智能*，他的著作也被遗忘，而旨在捕捉智能的逻辑系统则得到了广泛应用。
- en: ²Skinner’s explorations of our intellectual kinship with pigeons have interesting
    interpretations in the context of arguments that the concept of causality as such
    is in essence unfounded. From different directions, such arguments have been developed
    by Hume, Russell, Bohr, and many other scientists and philosophers.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ²斯金纳对我们与鸽子在智力上的亲缘关系的探索，在讨论因果性概念本身在本质上是没有根据的这一论点时，具有有趣的解释。从不同的角度，休谟、罗素、波尔以及其他许多科学家和哲学家都提出了这样的论点。
- en: ³Our [paper on bots’ religions](https://arxiv.org/abs/2303.14338) also points
    in this direction.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ³我们的[关于机器人的宗教论文](https://arxiv.org/abs/2303.14338)也指向了这一方向。
- en: ⁴Frank Rosenblatt. *Principles of neurodynamics; perceptrons and the theory
    of brain mechanisms*, volume 55\. Spartan Books, Washington, D.C., 1962.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴弗兰克·罗森布拉特。*神经动力学原理；感知器与大脑机制理论*，第55卷。斯巴达图书出版社，华盛顿特区，1962年。
- en: ⁵Recall that ⟨*w*| is a convenient notation (attributed to Paul Dirac) for the
    row vector *(w*1 *w*2 ···*wd )*, whereas |*w*⟩ is the corresponding column vector.
    Viewed as a linear operator, the row vector ⟨*w* | denotes the projection on the
    column vector |*w*⟩.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵回想一下，⟨*w*|是一个方便的符号（归功于保罗·狄拉克），表示行向量*(w*1 *w*2 ···*wd )*，而|*w*⟩是相应的列向量。从线性算子的角度来看，行向量⟨*w*
    |表示对列向量|*w*⟩的投影。
- en: ⁶Since this note was written, a [paper](https://arxiv.org/abs/2404.19756) proposing
    a new family of neural networks, called the *Kolmogorov-Arnold Networks (KAN)*
    appeared on arxiv. The idea is very natural, as even our derivation of [neural
    approximation](#5646) from [continuous decomposition](#2f11) confirms. Remarkably,
    though, the proposers of the KAN approach make no use of the substantial mathematical
    and computational simplifications and improvements of Kolmogorov’s 1957 construction,
    although they cite some papers with fairly complete reference lists. Since they
    seem to be actively updating the posted reports about their work, the missed opportunities
    for improvement will presumably be taken in the future versions.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ⁶自这篇笔记写成以来，一篇关于提出一种新型神经网络家族的[论文](https://arxiv.org/abs/2404.19756)在arxiv上发表，这种神经网络被称为*Kolmogorov-Arnold
    Networks (KAN)*。这一想法非常自然，甚至我们从[连续分解](#2f11)推导出的[神经近似](#5646)也证实了这一点。值得注意的是，KAN方法的提议者并没有利用科尔莫哥洛夫1957年构造的数学和计算简化与改进，尽管他们引用了一些文献并提供了相对完整的参考文献列表。由于他们似乎在积极更新关于自己工作的报告，因此这些未利用的改进机会预计会在未来版本中得到采纳。
- en: ⁷In the old-style matrix notation the Lorentz-Sprecher embedding is
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ⁷在老式矩阵表示法中，洛伦茨-斯普雷切尔嵌入是
- en: '![](../Images/2b820513ddc9719dfad217e67d74d761.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b820513ddc9719dfad217e67d74d761.png)'
- en: ⁸In their seminal, critical book *“Perceptrons”*, Minsky and Papert proved that
    the coefficients of a perceptron representing a boolean function are always invariant
    under the actions of a group under which the function is invariant itself. Since
    a perceptron therefore cannot tell apart the functions that are equivariant under
    the group actions, this was viewed as a no-go theorem. While the Minsky-Papert
    construction lifts from perceptrons and boolean functions to wide neural networks
    and continuous functions by standard methods, the resulting group invariances
    are nowadays viewed as proofs that the glass of neural approximations is half-full,
    not that it is half-empty.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ⁸在他们的开创性、批判性著作*《感知机》*中，明斯基和帕珀特证明了表示布尔函数的感知机的系数在函数本身不变的群作用下始终是不变的。因此，感知机无法区分在该群作用下等变的函数，这被视为一个禁忌定理。虽然明斯基-帕珀特构造通过标准方法将感知机和布尔函数提升到广泛的神经网络和连续函数，但由此得到的群不变性如今被视为证明神经近似的“杯子是半满的”，而不是“杯子是半空的”。
- en: ⁹The constructions and discussions presented in this section are based on the
    paper [“*From Gödel’s Incompleteness Theorem to the completeness of bot beliefs*”](https://arxiv.org/abs/2303.14338).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ⁹本节中提出的构造和讨论基于论文[“*从哥德尔不完全性定理到机器人信仰的完备性*”](https://arxiv.org/abs/2303.14338)。
- en: ¹⁰Shakespeare’s tragedy of Macbeth is built on a self-fulfilling prophecy. At
    the beginning, the witches predict that Macbeth will become King. To fulfill the
    inevitable, Macbeth kills the King. Even a completely rational Macbeth is forced
    to fulfill the prophecy, or risk that the King will hear of it and kill him to
    prevent it from being fulfilled. An example of a self-fulfilling prophecy from
    current life arises from the task of launching a social networking service. This
    service is only valuable to its users if their friends are also using it. To get
    its first users, the social network must convince them that it already has many
    users, enough to include their friends. Initially, this must be a lie. But if
    many people believe this lie, they will join the network, the network will get
    many users, and the lie will stop being a lie. Examples of adaptive theories include
    the religions that attribute any evidence contrary to their claims to demons or
    to faith testing and temptations.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ¹⁰莎士比亚的悲剧《麦克白》建立在一个自我实现的预言上。起初，女巫预言麦克白将成为国王。为了实现这个不可避免的命运，麦克白杀死了国王。即使是完全理性的麦克白也被迫实现这一预言，否则他就有可能被国王发现并被杀害，以阻止预言成真。从现实生活中来看，一个自我实现的预言的例子可以从启动社交网络服务的任务中找到。这个服务对其用户来说只有在他们的朋友也在使用它时才有价值。为了获得首批用户，社交网络必须说服他们相信它已经有了很多用户，足够多以至于包括他们的朋友。最初，这必须是个谎言。但如果很多人相信这个谎言，他们就会加入网络，网络将获得大量用户，而这个谎言就不再是谎言。适应性理论的例子包括那些将与其主张相悖的证据归因于恶魔或信仰考验和诱惑的宗教。
