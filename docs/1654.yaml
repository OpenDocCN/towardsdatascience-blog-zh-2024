- en: 'LLM Alignment: Reward-Based vs Reward-Free Methods'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM对齐：基于奖励的方法与无奖励的方法
- en: 原文：[https://towardsdatascience.com/llm-alignment-reward-based-vs-reward-free-methods-ef0c0f6e8d88?source=collection_archive---------0-----------------------#2024-07-05](https://towardsdatascience.com/llm-alignment-reward-based-vs-reward-free-methods-ef0c0f6e8d88?source=collection_archive---------0-----------------------#2024-07-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llm-alignment-reward-based-vs-reward-free-methods-ef0c0f6e8d88?source=collection_archive---------0-----------------------#2024-07-05](https://towardsdatascience.com/llm-alignment-reward-based-vs-reward-free-methods-ef0c0f6e8d88?source=collection_archive---------0-----------------------#2024-07-05)
- en: Optimization methods for LLM alignment
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM对齐的优化方法
- en: '[](https://medium.com/@anishdubey?source=post_page---byline--ef0c0f6e8d88--------------------------------)[![Anish
    Dubey](../Images/f85f17fb79718c819b4bd1c9a16338a7.png)](https://medium.com/@anishdubey?source=post_page---byline--ef0c0f6e8d88--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ef0c0f6e8d88--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ef0c0f6e8d88--------------------------------)
    [Anish Dubey](https://medium.com/@anishdubey?source=post_page---byline--ef0c0f6e8d88--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@anishdubey?source=post_page---byline--ef0c0f6e8d88--------------------------------)[![Anish
    Dubey](../Images/f85f17fb79718c819b4bd1c9a16338a7.png)](https://medium.com/@anishdubey?source=post_page---byline--ef0c0f6e8d88--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ef0c0f6e8d88--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ef0c0f6e8d88--------------------------------)
    [Anish Dubey](https://medium.com/@anishdubey?source=post_page---byline--ef0c0f6e8d88--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ef0c0f6e8d88--------------------------------)
    ·10 min read·Jul 5, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ef0c0f6e8d88--------------------------------)
    ·阅读时长 10 分钟·2024年7月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Context
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: Language models have demonstrated remarkable abilities in producing a wide range
    of compelling text based on prompts provided by users. However, defining what
    constitutes “good” text is challenging, as it often depends on personal preferences
    and the specific context. For instance, in storytelling, creativity is key; in
    crafting informative content, accuracy and reliability are crucial; and when generating
    code, ensuring it runs correctly is essential. Hence the *“LLM alignment problem,”*
    which refers to the challenge of ensuring that large language models (LLMs) act
    in ways that are consistent with human values, intentions, and preferences.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型在基于用户提供的提示生成各种引人注目的文本方面展现了显著的能力。然而，定义什么是“好”文本是具有挑战性的，因为它通常依赖于个人偏好和具体上下文。例如，在讲故事时，创造力是关键；在制作信息内容时，准确性和可靠性至关重要；在生成代码时，确保代码正确运行是必要的。因此，*“LLM对齐问题”*，即确保大型语言模型（LLM）按照与人类价值观、意图和偏好一致的方式运作的挑战。
- en: Designing a loss function that captures the diverse qualities we value in text
    — like creativity, accuracy, or executability — is highly complex and often impractical.
    Concepts like these are not differentiable and hence not back-propagated and cannot
    be trained upon with simple next token generation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个能够捕捉我们在文本中重视的多种特质（如创造力、准确性或可执行性）的损失函数是极其复杂的，且往往不切实际。像这些概念并不可微分，因此无法进行反向传播，也不能通过简单的下一词生成进行训练。
- en: Imagine if we could harness human feedback to evaluate the quality of generated
    text or, even better, use that feedback as a guiding loss function to improve
    the model’s performance. This concept is at the heart of Reinforcement Learning
    from Human Feedback (RLHF). By applying reinforcement learning techniques, RLHF
    allows us to fine-tune language models based on direct human feedback, aligning
    the models more closely with nuanced human values and expectations. This approach
    has opened up new possibilities for training language models that are not only
    more responsive but also more aligned with the complexity of human preferences.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们能够利用人类反馈来评估生成文本的质量，或者更好的是，使用这些反馈作为引导损失函数来提升模型的表现。这个概念是“人类反馈强化学习”（RLHF）的核心。通过应用强化学习技术，RLHF使我们能够根据直接的人类反馈微调语言模型，使模型更加符合人类细微的价值观和期望。这种方法为训练不仅更加响应迅速，而且更符合人类偏好复杂性的语言模型开辟了新的可能性。
- en: Below, we will aim to learn more about RLHF via reward-based and then about
    RLHF via reward-free methods.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过基于奖励的方法，进一步了解RLHF，并了解基于无奖励的方法。
- en: What is Reinforcement learning through human feedback (RLHF) via a reward-based
    system?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是通过人类反馈的强化学习（RLHF）和基于奖励的系统？
- en: 'Let’s go through Reinforcement learning through human feedback (RLHF). It consist
    of 3 main stages:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来了解通过人类反馈的强化学习（RLHF）。它由三个主要阶段组成：
- en: Supervised fine tuning
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监督微调
- en: Reward modeling phase
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励建模阶段
- en: RL fine-tuning phase
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RL微调阶段
- en: '**Supervised fine tuning**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**监督微调**'
- en: RLHF is a pre-trained model which is fine tuned already on a high quality data
    set. Its objective is simple i.e. when given an input (prompt), it produces an
    output. The ultimate objective here is to further fine tune this model to produce
    output according to human preference. Hence, let’s call this a ***base model***
    for reference. Currently, this model is a vanilla base model which is not aware
    of any human preference.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF是一个预训练模型，已经在高质量数据集上进行了微调。它的目标很简单，即在给定输入（提示）时，产生一个输出。最终目标是进一步微调该模型，以根据人类的偏好产生输出。因此，让我们称之为***基础模型***以供参考。目前，这个模型是一个标准的基础模型，它对任何人类偏好一无所知。
- en: '**Reward Modelling Phase**'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**奖励建模阶段**'
- en: '*Reward model innovation:* This is where the new innovation begins on how reward
    models are incorporated into RLHF. The idea behind the reward model is that a
    new LLM model, which can be same as the above mentioned base model, will have
    the ability to generate human preference score. The reason it is similar to a
    large language model is because this model also needs to understand the language
    semantics before it can rate if an output is human preferred or not. Since the
    reward is scalar, we add a linear layer on top of LLM to generate a scalar score
    in terms of human preference.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*奖励模型创新*：这是奖励模型如何被融入到RLHF中的新创新开始的地方。奖励模型的背后思想是，一个新的LLM模型（它可以与上述的基础模型相同）将能够生成人的偏好评分。之所以与大型语言模型相似，是因为该模型也需要理解语言语义，才能评估输出是否符合人类偏好。由于奖励是标量，我们在LLM上方添加一个线性层，以生成一个关于人类偏好的标量评分。'
- en: '*Data collection phase*: This is done from the supervised fine tuning stage
    where the base model is asked to generate 2 outputs for a given text. Example:
    For an input token x, two output tokens are generated, y1 and y2 by the base model.
    These outputs are shown to human raters to rate and human preference is recorded
    for each individual output.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据收集阶段*：这个阶段是在监督微调阶段完成的，在该阶段中，基础模型会为给定的文本生成两个输出。示例：对于输入标记x，基础模型生成两个输出标记y1和y2。这些输出会展示给人工评分员进行评分，并记录每个输出的人工偏好。'
- en: '*Training phase:* Once the data sample is collected from the data collection
    phase, the ***reward model*** is trained with the following prompt. *“Given the
    following input: <x>, LLM generated <y> output. Can you rate the performance of
    the output?”.* The model will output r(reward) and we already know the actual
    value of reward r1 from the data collection phase. Now, this can be back-propagated
    with the loss function and the model can be trained. Below is the objective loss
    function which the model optimises for through back-propagation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*训练阶段*：一旦数据收集阶段采集到数据样本，***奖励模型***就会使用以下提示进行训练：*“给定以下输入：<x>，LLM生成了<y>输出。你能评估该输出的表现吗？”*
    模型将输出r（奖励），我们已经知道从数据收集阶段得到的实际奖励值r1。现在，可以通过损失函数进行反向传播，模型可以得到训练。以下是模型通过反向传播优化的目标损失函数：'
- en: '![](../Images/1f126383029386b638a86cd909424bcd.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f126383029386b638a86cd909424bcd.png)'
- en: 'Equation from this paper: [https://arxiv.org/pdf/2305.18290](https://arxiv.org/pdf/2305.18290)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文中的公式：[https://arxiv.org/pdf/2305.18290](https://arxiv.org/pdf/2305.18290)
- en: '*Notation:*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*符号说明：*'
- en: 'rΦ(x, y): a reward model parameterized by Φ which estimates the reward. Parameterized
    means we don’t know the actual value and this needs to be optimized from the above
    equation. This is the reward LLM model itself. Mostly, the LLM parameters are
    frozen here and only few parameters are left to change. Most important layer is
    the linear layer added at the top. This does most of the learning to rate the
    score of output.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rΦ(x, y)：一个由Φ参数化的奖励模型，用于估计奖励。参数化意味着我们不知道实际的值，且需要根据上述方程进行优化。这就是奖励LLM模型本身。通常，LLM的参数在这里被冻结，只有少数参数允许变化。最重要的层是顶部添加的线性层，它负责进行大部分的学习，以评估输出的评分。
- en: 'Ɗ: A dataset of triplets (*x, yw, yl*) where *x*: input, *yw*: the winner output
    and *yl*: the loser output'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ɗ：一个三元组数据集（*x, yw, yl*），其中 *x*：输入，*yw*：赢家输出，*yl*：输家输出
- en: 'σ: the sigmoid function which maps the difference in reward to a probability
    (0–1)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: σ：sigmoid函数，它将奖励差异映射到一个概率值（0–1之间）
- en: ∑(x, y,w yl) ~Ɗ means *x, yw, yl* are all sampled from Ɗ
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ∑(x, y,w yl) ~Ɗ 表示 *x, yw, yl* 都是从 Ɗ 中采样的。
- en: '*Example scenario:* Imagine you’re training a reward model to evaluate responses.
    You have pairs of responses to a given prompt, and human feedback tells you which
    response is better. For context, *x*(“What is the capital of France?”), you have
    *yw*(“The capital of France is Paris.”) as winner and *yl*(“The capital of France
    is Berlin.” ) as loser. The reward model should eventually learn to give higher
    reward for “The capital of France is Paris.” output when compared to “The capital
    of France is Berlin.” output if “What is the capital of France?” input is given.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例场景：* 假设你正在训练一个奖励模型来评估回答。你有一对针对给定提示的回答，并且人类反馈告诉你哪个回答更好。举个例子，*x*（“法国的首都是什么？”），你有
    *yw*（“法国的首都巴黎。”）作为赢家，以及 *yl*（“法国的首都柏林。”）作为输家。当输入是“法国的首都是什么？”时，奖励模型最终应该学会给“法国的首都巴黎”更高的奖励，而不是“法国的首都柏林”。'
- en: '**RL fine-tuning phase**'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**RL 微调阶段**'
- en: '*Reinforcement learning idea:* Now the *base model* and *reward model* are
    trained, the idea is how to leverage reward model score and update base model
    parameters to reflect human preference. ***Since the reward model outputs a scalar
    score and is not differentiable, we cannot use simple back-propogation to update
    the base model param***. Hence, we need other techniques to update the base model.
    This is where reinforcement learning comes which helps the base model to change
    the params through reward model score. This is done through PPO (proximal policy
    optimization). Understanding the core architecture of PPO is not required to grasp
    this concept and hence we will not cover it here but on a high level, the idea
    is that PPO can use scalar score to update base model parameters. Now let’s understand
    how base and reward models are incorporated to make base models learn human preference.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习思路：* 现在，*基本模型*和*奖励模型*都已经训练完成，思路是如何利用奖励模型的得分并更新基本模型参数，以反映人类偏好。***由于奖励模型输出的是标量分数，并且不可微，我们无法使用简单的反向传播来更新基本模型参数***。因此，我们需要其他技术来更新基本模型。这就是强化学习的作用，它通过奖励模型的得分帮助基本模型改变参数。这是通过PPO（近端策略优化）完成的。理解PPO的核心架构并不是理解这个概念所必需的，因此我们不会在这里讲解，但从高层次来说，PPO的思路是可以使用标量得分来更新基本模型参数。现在让我们了解基本模型和奖励模型如何结合起来，使得基本模型学习人类偏好。'
- en: '*RL fine-tuning idea:* In reinforcement learning, we have action, space and
    rewards. The idea is to come up with a policy which any action agent can take
    in the space which maximizes the reward. This becomes quite complicated but in
    a simplified sense, π is the policy which is our base LLM model only. *Πref* means
    the base model and *ΠӨ* means a different LLM optimal model which we are trying
    to generate. We need to find *ΠӨ* (the base model’s neural network weights will
    be fine-tuned) which gives human-preferred output. It’s just that we don’t know
    ΠӨ and the idea is to find this optimal model.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*RL 微调思路：* 在强化学习中，我们有动作、空间和奖励。思路是提出一个策略，让任何动作代理都可以在该空间内采取，从而最大化奖励。这个过程比较复杂，但简化来看，π是我们的基本LLM模型。*Πref*表示基本模型，*ΠӨ*表示我们正在尝试生成的不同的LLM最优模型。我们需要找到
    *ΠӨ*（即基本模型的神经网络权重将被微调），从而输出人类更喜欢的结果。问题是我们不知道ΠӨ，而目标是找到这个最优模型。'
- en: '*RL training and feedback loop phase:* An input x is given to 2 policy models,
    *Πref* (baseline model) and *ΠӨ* (optimal model which we are trying to generate).
    Initially both models are kept the same. Input x to two models individually will
    give two outputs correspondingly. The output from ΠӨ model is also fed to reward
    model (input: x, output: y; as discussed above) and asked to output the reward
    score which is rΦ(x, y). Now we have 3 things, output from the baseline model,
    output from the optimal model, and a reward score from the optimal model. There
    are 2 things we are optimizing here, one is to *maximize the reward* because eventually
    we want the model to be as close as human preference and another is to *minimize
    the divergence from baseline model*. Maximizing the reward is easy since it is
    already a scalar quantity but how do we minimize the divergence of baseline and
    optimal model. Here we use *“Kullback–Leibler divergence”* which estimates the
    difference between 2 continuous probability distributions. Let’s take a deeper
    look into the objective loss function'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习训练与反馈循环阶段*：输入x被提供给两个策略模型，*Πref*（基准模型）和*ΠӨ*（我们试图生成的最优模型）。最初，两个模型是相同的。将输入x分别传入这两个模型会分别产生两个输出。ΠӨ模型的输出也会输入到奖励模型中（输入：x，输出：y；如上所述），并要求输出奖励分数，即rΦ(x,
    y)。现在我们有三个内容：基准模型的输出、最优模型的输出和最优模型的奖励分数。这里有两个优化目标，一个是*最大化奖励*，因为最终我们希望模型与人类偏好尽可能接近，另一个是*最小化与基准模型的差异*。最大化奖励很容易，因为它本身就是一个标量值，但如何最小化基准模型和最优模型之间的差异呢？这里我们使用*“Kullback–Leibler散度”*，它估计两个连续概率分布之间的差异。让我们更深入地了解目标损失函数。'
- en: '![](../Images/62b0ece843cfa43a76a0035f21c15b45.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62b0ece843cfa43a76a0035f21c15b45.png)'
- en: 'Equation from this paper: [https://arxiv.org/pdf/2305.18290](https://arxiv.org/pdf/2305.18290)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 来自本文的方程：[https://arxiv.org/pdf/2305.18290](https://arxiv.org/pdf/2305.18290)
- en: '*Notation*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*符号表示法*：'
- en: 'rΦ(x, y): a scalar value for an input x and output y (from optimal model).
    To be explicit, output from the optimal model is fed into the reward model.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rΦ(x, y)：表示输入x和输出y的标量值（来自最优模型）。为了明确起见，最优模型的输出会被输入到奖励模型中。
- en: 'Dkl (ΠӨ (y | x) || Πref (y | x)): This computes the *Kullback–Leibler divergence*
    between 2 probability distributions*.* Each token from each model is a probability
    distribution. KL estimates how far the distribution is from each other.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dkl (ΠӨ (y | x) || Πref (y | x))：计算两个概率分布之间的*Kullback–Leibler散度*。每个模型中的每个标记都是一个概率分布。KL估计两个分布之间的差异程度。
- en: 'β : Hyperparameter which is used to determine how important it is to have optimal
    model close to baseline model.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: β：超参数，用于确定最优模型与基准模型接近的重要性。
- en: '*Example scenario:* Imagine you are asking (“What is the capital of France?”),
    Πref (baseline model) says: “The capital of France is Berlin.” and ΠӨ (optimal
    model) “There are 3 capitals, Paris, Versailles, and Lyon, but Paris is considered
    as the official capital”. Now rΦ(“x: What is the capital…”, “y: There are 3 capital..”)
    should give low score as it is less human-preferred and Kullback–Leibler divergence
    of (ΠӨ (y | x) || Πref (y | x)) should be high as well since the probability distribution
    space differs for both individual output. Hence the loss will be high from both
    terms. We do not want the model to only optimize for reward but also stay closer
    to the baseline model and hence both the terms are used to optimize the reward.
    In the next iteration with learning let’s say, ΠӨ (optimal model) says “The capital
    of France is Delhi”, in this case model learned to stay closer to Πref (baseline
    model) and output the format closer to baseline model but the reward component
    will still be lower. Hopefully, in the third iteration ΠӨ (optimal model) should
    be able to learn and output “The capital of France is Paris” with higher reward
    and model output aligning closely with baseline model.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例场景*：假设你问：“法国的首都是什么？”，Πref（基准模型）回答：“法国的首都是柏林。”而ΠӨ（最优模型）回答：“法国有三个首都，巴黎、凡尔赛和里昂，但巴黎被视为官方首都。”现在rΦ(“x:
    法国的首都是什么…”, “y: 法国有三个首都…”)应该给出较低的分数，因为它不太符合人类的偏好，且Kullback–Leibler散度（ΠӨ (y | x)
    || Πref (y | x)）也应该较高，因为两个模型的输出概率分布空间存在差异。因此，这两个项的损失都会较高。我们不希望模型仅仅优化奖励，还希望它能够保持接近基准模型，因此这两个项都用于优化奖励。在接下来的学习迭代中，假设ΠӨ（最优模型）回答“法国的首都是德里”，在这种情况下，模型学习到保持接近Πref（基准模型）并输出更接近基准模型格式的内容，但奖励部分仍然较低。希望在第三次迭代中，ΠӨ（最优模型）能够学习并输出“法国的首都是巴黎”，并获得更高的奖励，同时模型输出与基准模型紧密对齐。'
- en: The below diagram helps illustrate the logic. I will also highly recommend to
    go through [RLHF link](https://huggingface.co/blog/rlhf) from hugging face.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示有助于说明逻辑。我还强烈推荐浏览[RLHF链接](https://huggingface.co/blog/rlhf)，该链接来自Hugging
    Face。
- en: '![](../Images/751a0f87e0b251e69b17e3e65bad459e.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/751a0f87e0b251e69b17e3e65bad459e.png)'
- en: Image by author, inspired by [https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片，灵感来自[https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf)
- en: What is Reinforcement learning through human feedback (RLHF) via reward-free
    method ?
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是通过无奖励方法的基于人类反馈的强化学习（RLHF）？
- en: 'With RLHF using a reward-based method in mind, let’s move to the reward-free
    method. According to the paper: *“our key insight is to leverage an analytical
    mapping from reward functions to optimal policies, which enables us to transform
    a loss function over reward functions into a loss function over policies. This
    change-of-variables approach avoids fitting an explicit, standalone reward model,
    while still optimizing under existing models of human preferences”.* Very complicated
    to understand, but let’s try to break this down in simple phases in the next section.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑使用基于奖励的方法的RLHF之后，我们来讨论无奖励方法。根据论文中的描述：*“我们的关键见解是利用从奖励函数到最优策略的分析映射，这使我们能够将奖励函数上的损失函数转换为策略上的损失函数。这种变量变换方法避免了拟合显式的独立奖励模型，同时仍然在现有的人类偏好模型下进行优化。”*
    非常复杂，理解起来有难度，但我们将在下一部分尝试将其分解为简单的阶段。
- en: '*Reward-free method’s key idea:* In RLHF, a separate new reward model is trained
    which is expensive and costly to maintain. Is there any mechanism to avoid training
    a new reward model and use the existing base model to achieve a new optimal model?
    This is exactly what reward-free method does i.e. it avoids training a new reward
    model and in turn changes the equation in such a way that there is no reward model
    term in the loss function of DPO (Direct preference optimization). One way to
    think about this is that we need to reach optimal model policy(ΠӨ) from base model
    (Πref). It can be reached ***either through optimizing the reward function space
    which helps build a proxy to reach optimal model policy or directly learning a
    mapping function from reward to policy and in turn optimize for policy itself***.
    This is exactly what the authors have tried by removing the reward function component
    in loss function and substitute it directly by model policy parameter. This is
    what the author meant when they say *“leverage an analytical mapping from reward
    function to optimal policies …. into a loss function over policies”.* This is
    the core innovation of the paper.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*无奖励方法的关键思想：* 在RLHF中，训练一个单独的新的奖励模型既昂贵又难以维护。有没有什么机制可以避免训练新的奖励模型，而使用现有的基础模型来达到新的最优模型？这正是无奖励方法的做法，即它避免了训练新的奖励模型，并通过改变方程式，使得DPO（直接偏好优化）的损失函数中不再包含奖励模型项。可以这样理解，我们需要从基础模型（Πref）到达最优模型策略（ΠӨ）。可以通过***优化奖励函数空间来帮助建立代理模型，从而达到最优模型策略，或直接学习从奖励到策略的映射函数，并进而优化策略本身***。这正是作者们通过移除损失函数中的奖励函数组件，并直接用模型策略参数替代它所尝试的做法。这就是作者们所说的*“利用奖励函数到最优策略的分析映射...到损失函数上”*的核心创新。'
- en: '*DPO training and feedback loop phase:* Using Πref (baseline model), input
    x is given and asked to produce 2 outputs (y1 and y2). All x, y1 and y2 are used
    by human raters to decide winning yw and losing yl. Offline data set is collected
    with triplet information <x, yw and yl>. With this information, we know what the
    winning (human preferred) and losing (human not preferred) answers are. Now, the
    same input x is given to 2 policy (models) Πref (baseline model) and ΠӨ (optimal
    model). Initially both models are kept the same for training purposes. Input x
    to two models individually will give two outputs correspondingly. We compute how
    far the output is from winning and losing answers from both reference and optimal
    model through *“Kullback–Leibler divergence”*. Let’s take a deeper look into the
    objective loss function'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*DPO训练与反馈回路阶段：* 使用Πref（基线模型），输入x并要求产生2个输出（y1和y2）。所有x、y1和y2都会被人工评分员用来决定胜出的yw和失败的yl。离线数据集收集了三元组信息<x,
    yw和yl>。通过这些信息，我们知道胜出（人类偏好）和失败（人类不偏好）的答案是什么。现在，相同的输入x被传入2个模型（Πref基线模型和ΠӨ最优模型）。最初，为了训练目的，两个模型保持相同。将输入x分别输入这两个模型，得到相应的两个输出。我们通过*“Kullback-Leibler散度”*计算输出与参考模型和最优模型的胜出与失败答案之间的差距。让我们深入研究目标损失函数。'
- en: '*Equation*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*公式*'
- en: '![](../Images/62aa446dfe029c31daee49c2f31cb3fb.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62aa446dfe029c31daee49c2f31cb3fb.png)'
- en: Equation from [https://arxiv.org/pdf/2305.18290](https://arxiv.org/pdf/2305.18290)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [https://arxiv.org/pdf/2305.18290](https://arxiv.org/pdf/2305.18290) 的方程
- en: ΠӨ (yw | x) -> Given x(input), how far is the corresponding output of the model
    say youtput from the winning output yw. Output youtput and yw are probability
    distributions and differences among both will be computed through “*Kullback–Leibler
    divergence*”. This will be a scalar value. Also this is computed for both models
    with different combinations of Πref (yw | x), Πref (yl | x), ΠӨ (yw | x) and ΠӨ
    (yl | x).
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ΠӨ (yw | x) -> 给定输入x，模型的对应输出youtput与获胜输出yw之间的差距是多少。输出youtput和yw是概率分布，二者之间的差异将通过“*Kullback–Leibler散度*”来计算。这将是一个标量值。此外，这也会针对不同组合的Πref
    (yw | x)、Πref (yl | x)、ΠӨ (yw | x)和ΠӨ (yl | x)进行计算。
- en: 'β : Hyperparameter which is used to determine how important it is to have optimal
    model close to baseline model.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: β ：超参数，用于确定将最佳模型与基线模型接近的重要性。
- en: '![](../Images/830c15cf030cab6633ac5e530ebb7464.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/830c15cf030cab6633ac5e530ebb7464.png)'
- en: Image by author, inspired by [https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者，灵感来自 [https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf)
- en: Conclusion
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Naturally, the question comes down to which one is better, RLHF through reward-based
    method using PPO or reward-free method using DPO. There is no right answer to
    this question. A recent paper compares *“Is DPO superior to PPO for LLM alignment”*
    (paper [link](https://arxiv.org/pdf/2404.10719)) and concludes that PPO is generally
    better than DPO and that DPO suffers more heavily from out-of-distribution data.
    “Out-of-distribution” data means the human preference data is different from the
    baseline trained data. This can happen if base model training is done on some
    dataset while preference output is done for some other dataset.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然地，问题变成了哪种方法更好，基于奖励的RLHF方法使用PPO，还是无奖励的方法使用DPO。这个问题没有确切的答案。最近一篇论文比较了*“DPO是否优于PPO用于大语言模型对齐”*（论文
    [链接](https://arxiv.org/pdf/2404.10719)）并得出结论，PPO通常优于DPO，而且DPO在处理超出分布的数据时更为困难。“超出分布”数据指的是人类偏好数据与基线训练数据不同。如果基础模型的训练是在某些数据集上进行，而偏好输出是在另一个数据集上进行，这种情况就可能发生。
- en: Overall, the research is still out on which one is better while we have seen
    companies like OpenAI, Anthropic, Meta leverage both RLHF via PPO and DPO as a
    tool for LLM alignment.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总的来说，关于哪种方法更好，研究仍在进行中，但我们已经看到像OpenAI、Anthropic、Meta等公司将RLHF通过PPO和DPO两种工具用于大语言模型的对齐。
- en: Reference
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Direct Preference Optimization: Your Language Model is Secretly a Reward Model:
    [https://arxiv.org/pdf/2305.18290](https://arxiv.org/pdf/2305.18290)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接偏好优化：你的语言模型实际上是一个奖励模型：[https://arxiv.org/pdf/2305.18290](https://arxiv.org/pdf/2305.18290)
- en: Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study [https://arxiv.org/pdf/2404.10719](https://arxiv.org/pdf/2404.10719)
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DPO是否优于PPO用于大语言模型对齐？一项综合研究 [https://arxiv.org/pdf/2404.10719](https://arxiv.org/pdf/2404.10719)
- en: Hugging face RLHF article [https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf)
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging face RLHF文章 [https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf)
