<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Geographic Position Encoders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Geographic Position Encoders</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/geographic-position-encoders-59dafebf6f2d?source=collection_archive---------10-----------------------#2024-10-01">https://towardsdatascience.com/geographic-position-encoders-59dafebf6f2d?source=collection_archive---------10-----------------------#2024-10-01</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="66da" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Understanding modern techniques for encoding geographic coordinates in a neural network</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@crastoru?source=post_page---byline--59dafebf6f2d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ruth Crasto" class="l ep by dd de cx" src="../Images/5deaf13d4a79273e3f2986793aecc123.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Qwdm47olKjE5cty9XqHS6w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--59dafebf6f2d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@crastoru?source=post_page---byline--59dafebf6f2d--------------------------------" rel="noopener follow">Ruth Crasto</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--59dafebf6f2d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 1, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/5d581953d4a0e7b66067889fdc282b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OylUR3M_nIom9REh"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@chuttersnap?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">CHUTTERSNAP</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="b997" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">An inductive bias in machine learning is a constraint on a model given some prior knowledge of the target task. As humans, we can recognize a bird whether it’s flying in the sky or perched in a tree. Moreover, we don’t need to examine every cloud or take in the entirety of the tree to know that we are looking at a bird and not something else. These biases in the vision process are encoded in convolution layers via two properties:</p><ul class=""><li id="3d3b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk"><strong class="ne fr">Weight sharing</strong>: the same kernel weights are re-used along an input channel’s full width and height.</li><li id="291b" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk"><strong class="ne fr">Locality</strong>: the kernel has a much smaller width and height than the input.</li></ul><p id="bd51" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can also encode inductive biases in our choice of input features to the model, which can be interpreted as a constraint on the model itself. Designing input features for a neural network involves a trade-off between expressiveness and inductive bias. On one hand, we want to allow the model the flexibility to learn patterns beyond what we humans can detect and encode. On the other hand, a model without any inductive biases will struggle to learn anything meaningful at all.</p><p id="ce68" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this article, we will explore the inductive biases that go into designing effective position encoders for geographic coordinates. Position on Earth can be a useful input to a wide range of prediction tasks, including image classification. As we will see, using latitude and longitude directly as input features is under-constraining and ultimately will make it harder for the model to learn anything meaningful. Instead, it is more common to encode prior knowledge about latitude and longitude in a nonparametric re-mapping that we call a positional encoder.</p><h1 id="85f8" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Introduction: Position Encoders in Transformers</h1><p id="8127" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">To motivate the importance of choosing effective position encoder more broadly, let’s first examine the well-known position encoder in the transformer model. We start with the notion that the representation of a token input to an attention block should include some information about its position in the sequence it belongs to. The question is then: how should we encode the position index (0, 1, 2…) into a vector?</p><p id="565a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Assume we have a position-independent token embedding. One possible approach is to add or concatenate the index value directly to this embedding vector. Here is why this doesn’t work well:</p><ol class=""><li id="eb63" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ph nz oa bk">The similarity (dot product) between two embeddings — after their position has been encoded — should be independent of the total number of tokens in the sequence. The two last tokens of a sequence should record the same similarity whether the sequence is 5 or 50 words long.</li><li id="46d3" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ph nz oa bk">The similarity between two tokens should not depend on the absolute value of their positions, but only the relative distance between them. Even if the encoded indices were normalized to the range [0, 1], two adjacent tokens at positions 1 and 2 would record a lower similarity than the same two tokens later in the sequence.</li></ol><p id="b0f4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The original “Attention is All You Need” paper <a class="af nb" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">[1]</a> proposes instead to encode the position index <em class="pi">pos </em>into a discrete “snapshot” of <em class="pi">k </em>different sinusoids, where <em class="pi">k </em>is the dimension of the token embeddings. These snapshots are computed as follows:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pj"><img src="../Images/a9556872f6edc6eda258a8d68f95decf.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*59lniHSZp3nnA5H51PnI-Q.png"/></div></figure><p id="7621" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">where <em class="pi">i = </em>1, 2, …, <em class="pi">k </em>/ 2. The resulting <em class="pi">k</em>-dimensional position embedding is then added elementwise to the corresponding token embedding.</p><p id="efe8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The intuition behind this encoding is that the more snapshots are out of phase for any two embeddings, the further apart are their corresponding positions. The absolute value of two different positions will not influence how out of phase their snapshots are. Moreover, since the range of any sinusoid is the interval [-1, 1], the magnitude of the positional embeddings will not grow with sequence length.</p><p id="8567" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I won’t go into more detail on this particular position encoder since there are several excellent blog posts that do so (see <a class="af nb" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" rel="noopener ugc nofollow" target="_blank">[2]</a>). Hopefully, you can now see why it is important, in general, to think carefully about how position should be encoded.</p><h1 id="5901" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Geographic Position Encoders</h1><p id="dcb8" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Let’s now turn to encoders for geographic position. We want to train a neural network to predict some variable of interest given a position on the surface of the Earth. How should we encode a position (λ, ϕ) in spherical coordinates — i.e. a longitude/latitude pair — into a vector that can be used as an input to our network?</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pk"><img src="../Images/1b247402a9bb764ff5a8cfafb2529c4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*UbV4kyi-yOsnEdnsT9iuXw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">By Peter Mercator, <a class="af nb" href="https://commons.wikimedia.org/w/index.php?curid=12226167" rel="noopener ugc nofollow" target="_blank">Public Domain</a>.</figcaption></figure><h2 id="1600" class="pl oh fq bf oi pm pn po ol pp pq pr oo nl ps pt pu np pv pw px nt py pz qa qb bk">Simple approach</h2><p id="42bb" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">One possible approach would be to use latitude and longitude values directly as inputs. In this case our input feature space would be the rectangle [-<em class="pi">π, π</em>] × [0<em class="pi">, π</em>], which I will refer to as lat/lon space. As with position encoders for transformers, this simple approach unfortunately has its limitations:</p><ol class=""><li id="41bc" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ph nz oa bk">Notice that as you move towards the poles, the distance on the surface of the Earth covered by 1 unit of longitude (λ) decreases. Lat/lon space does not preserve distances on the surface of the Earth.</li><li id="06b8" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ph nz oa bk">Notice that the position on Earth corresponding to coordinates (λ, ϕ) should be identical to the position corresponding to (λ + 2<em class="pi">π, </em>ϕ). But in lat/lon space, these two coordinates are very far apart. Lat/lon space does not preserve periodicity: the way spherical coordinates wrap around the surface of the Earth.</li></ol><p id="ad9e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To learn anything meaningful directly from inputs in lat/long space, a neural network must learn how to encode these properties about the curvature of the Earth’s surface on its own — a challenging task. How can we instead design a position encoder that already encodes these inductive biases? Let’s explore some early approaches to this problem and how they have evolved over time.</p><h1 id="cbb1" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Early Position Encoders</h1><h2 id="b1aa" class="pl oh fq bf oi pm pn po ol pp pq pr oo nl ps pt pu np pv pw px nt py pz qa qb bk">Discretization-based (2015)</h2><p id="ffe6" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">The first paper to propose featurizing geographic coordinates for use as input to a convolutional neural network is called “Improving Image Classification with Location Context” <a class="af nb" href="https://arxiv.org/pdf/1505.03873" rel="noopener ugc nofollow" target="_blank">[3]</a>. Published in 2015, this work proposes and evaluates many different featurization approaches with the goal of training better classification models for geo-tagged images.</p><p id="1434" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The idea behind each of their approaches is to directly encode a position on Earth into a set of numerical features that can be computed from auxiliary data sources. Some examples include:</p><ul class=""><li id="5091" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Dividing the U.S. into evenly spaced grids in lat/lon space and using a one-hot encoding to encode a given location into a vector based on which grid it falls into.</li><li id="5c41" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Looking up the U.S ZIP code that corresponds to a given location, then retrieving demographic data about this ZIP code from ACS (American Community Survey) related to age, sex, race, living conditions, and more. This is made into a numerical vector using one-hot encodings.</li><li id="3500" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">For a chosen set of Instagram hashtags, counting how many hashtags are recorded at different distances from a given location and concatenating these counts into a vector.</li><li id="2cd3" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Retrieving color-coded maps from Google Maps for various features such as precipitation, land cover, congressional district, and concatenating the numerical color values from each into a vector.</li></ul><p id="5f3d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that these positional encodings are not continuous and do not preserve distances on the surface of the Earth. In the first example, two nearby locations that fall into different grids will be equally distant in feature space as two locations from opposite sides of the country. Moreover, these features mostly rely on the availability of auxiliary data sources and must be carefully hand-crafted, requiring a specific choice of hashtags, map features, survey data, etc. These approaches do not generalize well to arbitrary locations on Earth.</p><h2 id="c3d7" class="pl oh fq bf oi pm pn po ol pp pq pr oo nl ps pt pu np pv pw px nt py pz qa qb bk">WRAP (2019)</h2><p id="9043" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">In 2019, a paper titled “Presence-Only Geographical Priors for Fine-Grained Image Classification” <a class="af nb" href="https://arxiv.org/pdf/1906.05272" rel="noopener ugc nofollow" target="_blank">[4]</a> took an important step towards the geographic position encoders commonly used today. Similar to the work from the previous section, this paper studies how to use geographic coordinates for improving image classification models.</p><p id="4ccf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The key idea behind their position encoder is to leverage the periodicity of sine and cosine functions to encode the way geographic coordinates wrap around the surface of the Earth. Given latitude and longitude (λ, ϕ), both normalized to the range [-1, 1], the WRAP position encoder is defined as:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qc"><img src="../Images/920ea42f1458310b33c5bf742712d519.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*DoXZgI0kuFaYO2ck--NUxg.png"/></div></figure><p id="0f34" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Unlike the approaches in the previous section, WRAP is continuous and easily computed for any position on Earth. The paper then shows empirically that training a fully-connected network on top of these features and combining them with latent image features can lead to improved performance on fine-grained image classification benchmarks.</p><h1 id="0839" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">The Double Fourier Sphere Method</h1><p id="4d81" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">The WRAP encoder appears simple, but it successfully encodes a key inductive bias about geographic position while remaining expressive and flexible. In order to see why this choice of position encoder is so powerful, we need to understand the Double Fourier Sphere (DFS) method <a class="af nb" href="https://en.wikipedia.org/wiki/Double_Fourier_sphere_method" rel="noopener ugc nofollow" target="_blank">[5]</a>.</p><p id="a8ee" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">DFS is a method of transforming any real-valued function <em class="pi">f </em>(<em class="pi">x</em>, <em class="pi">y</em>, <em class="pi">z</em>) defined on the surface of a unit sphere into a 2<em class="pi">π</em>-periodic function defined on a rectangle [-<em class="pi">π</em>, <em class="pi">π</em>] × [-<em class="pi">π</em>, <em class="pi">π</em>]. At a high level, DFS consists of two steps:</p><ol class=""><li id="d282" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ph nz oa bk">Re-parametrize the function <em class="pi">f </em>(<em class="pi">x</em>, <em class="pi">y</em>, <em class="pi">z</em>) using spherical coordinates, where (λ, ϕ) ∈ [-<em class="pi">π</em>, <em class="pi">π</em>] × [0<em class="pi">, π</em>]</li></ol><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qd"><img src="../Images/61081c492c27cdcfaa8390d55d660dfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*I4gbCHQ9Gc4fTLsjkkrIRg.png"/></div></figure><p id="a4f5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. Define a new piece-wise function over the rectangle [-<em class="pi">π</em>, <em class="pi">π</em>] × [-<em class="pi">π</em>, <em class="pi">π</em>] based on the re-parametrized <em class="pi">f </em>(essentially “doubling it over”).</p><p id="926f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Notice that the DFS re-parametrization of the Earth’s surface (step 1.) preserves the properties we discussed earlier. For one, as ϕ tends to 0 or ± <em class="pi">π </em>(the Earth’s poles), the distance between two points (λ, ϕ) and (λ’, ϕ) after re-parametrization decreases. Moreover, the re-parametrization is periodic and smooth.</p><h2 id="3018" class="pl oh fq bf oi pm pn po ol pp pq pr oo nl ps pt pu np pv pw px nt py pz qa qb bk">Fourier Theorem</h2><p id="12e4" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">It is a fact that any continuous, periodic, real-valued function can be represented as a weighted sum of sines and cosines. This is called the Fourier Theorem, and this weighted sum representation is called a Fourier series. It turns out that any DFS-transformed function can be represented with a finite set of sines and cosines. They are known as <strong class="ne fr">DFS basis functions</strong>, listed below:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qe"><img src="../Images/17444feab234a748fcd0872fe182820b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*C4u5HOQ8Tp3E7WmqDZH9zw.png"/></div></figure><p id="5a10" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here, ∪ denotes union of sets, and <em class="pi">S </em>is a collection of scales (i.e. frequencies) for the sinusoids.</p><h2 id="afd6" class="pl oh fq bf oi pm pn po ol pp pq pr oo nl ps pt pu np pv pw px nt py pz qa qb bk">DFS-Based Position Encoders</h2><p id="2634" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Notice that the set of DFS basis functions includes the four terms in the WRAP position encoder. “Sphere2Vec” <a class="af nb" href="https://arxiv.org/pdf/2201.10489" rel="noopener ugc nofollow" target="_blank">[6]</a> is the earliest publication to observe this, proposing a unified view of position encoders based on DFS. In fact, with this generalization in mind, we can construct a geographic position encoder by choosing any subset of the DFS basis functions — WRAP is just one such choice. Take a look at <a class="af nb" href="https://arxiv.org/pdf/2310.06743v2" rel="noopener ugc nofollow" target="_blank">[7]</a> for a comprehensive overview of various DFS-based position encoders.</p><h2 id="16c1" class="pl oh fq bf oi pm pn po ol pp pq pr oo nl ps pt pu np pv pw px nt py pz qa qb bk">Why are DFS-based encoders so powerful?</h2><p id="f363" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Consider what happens when a linear layer is trained on top of a DFS-based position encoder: each output element of the network is a weighted sum of the chosen DFS basis functions. Hence, the network can be interpreted as a <strong class="ne fr">learned Fourier series</strong>.<strong class="ne fr"> </strong>Since virtually any function defined on the surface of a sphere can be transformed using the DFS method, it follows that a linear layer trained on top of DFS basis functions is powerful enough to encode arbitrary functions on the sphere! This is akin to the universal approximation theorem for multilayer perceptrons.</p><p id="9a7b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In practice, only a small subset of the DFS basis functions is used for the position encoder and a fully-connected network is trained on top of these. The composition of a non-parametric position encoder with a neural network is commonly referred to as a <strong class="ne fr">location encoder</strong>:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qf"><img src="../Images/315f844ae0997de2f4526c4cbc711b3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D48gnx3ETxbpTN-8t9JsLg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">A depiction of a geographic location encoder. Image by author.</figcaption></figure><h1 id="a631" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Geographic Location Encoders Today</h1><p id="0942" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">As we have seen, a DFS-based position encoder can effectively encode inductive biases we have about the curvature of the Earth’s surface. One limitation of DFS-based encoders is that they assume a rectangular domain [-<em class="pi">π</em>, <em class="pi">π</em>] × [-<em class="pi">π</em>, <em class="pi">π</em>]. While this is mostly fine since the DFS re-parametrization already accounts for how distances get warped closer to the poles, this assumption breaks down at the poles themselves (ϕ = 0, ± <em class="pi">π</em>), which are lines in the rectangular domain that collapse to singular points on the Earth’s surface.</p><p id="bf09" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A different set of basis functions called spherical harmonics have recently emerged as an alternative. Spherical harmonics are basis functions that are natively defined on the surface of the sphere as opposed to a rectangle. They have been shown to exhibit fewer artifacts around the Earth’s poles compared to DFS-based encoders <a class="af nb" href="https://arxiv.org/pdf/2310.06743v2" rel="noopener ugc nofollow" target="_blank">[7]</a>. Notably, spherical harmonics are the basis functions used in the SatCLIP location encoder <a class="af nb" href="https://arxiv.org/pdf/2311.17179" rel="noopener ugc nofollow" target="_blank">[8]</a>, a recent foundation model for geographic coordinates trained in the style of CLIP.</p><p id="e664" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Though geographic position encoders began with discrete, hand-crafted features in the 2010s, these do not easily generalize to arbitrary locations and require domain-specific metadata such as land cover and demographic data. Today, geographic coordinates are much more commonly used as neural network inputs because simple yet meaningful and expressive ways of encoding them have emerged. With the rise of web-scale datasets which are often geo-tagged, the potential for using geographic coordinates as inputs for prediction tasks is now immense.</p></div></div></div><div class="ab cb qg qh qi qj" role="separator"><span class="qk by bm ql qm qn"/><span class="qk by bm ql qm qn"/><span class="qk by bm ql qm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="d918" class="og oh fq bf oi oj qo gq ol om qp gt oo op qq or os ot qr ov ow ox qs oz pa pb bk">References</h1><p id="bc80" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser &amp; I. Polosukhin, <a class="af nb" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a> (2017), 31st Conference on Neural Information Processing Systems</p><p id="7c8d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[2] A Kazemnejad, <a class="af nb" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" rel="noopener ugc nofollow" target="_blank">Transformer Architecture: The Positional Encoding</a> (2019), Amirhossein Kazemnejad’s Blog</p><p id="2766" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[3] K. Tang, M. Paluri, L. Fei-Fei, R. Fergus, L. Bourdev, <a class="af nb" href="https://arxiv.org/pdf/1505.03873" rel="noopener ugc nofollow" target="_blank">Improving Image Classification with Location Context</a> (2015)</p><p id="ad86" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[4] O. Mac Aodha, E. Cole, P. Perona, <a class="af nb" href="https://arxiv.org/pdf/1906.05272" rel="noopener ugc nofollow" target="_blank">Presence-Only Geographical Priors for Fine-Grained Image Classification</a> (2019)</p><p id="5295" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[5] <a class="af nb" href="https://en.wikipedia.org/wiki/Double_Fourier_sphere_method" rel="noopener ugc nofollow" target="_blank">Double Fourier Sphere Method</a>, Wikipedia</p><p id="ef58" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[6] G. Mai, Y. Xuan, W. Zuo, K. Janowicz, N. Lao <a class="af nb" href="https://arxiv.org/pdf/2201.10489" rel="noopener ugc nofollow" target="_blank">Sphere2Vec: Multi-Scale Representation Learning over a Spherical Surface for Geospatial Predictions</a> (2022)</p><p id="c1c9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[7] M. Rußwurm, K. Klemmer, E. Rolf, R. Zbinden, D. Tuia, <a class="af nb" href="https://arxiv.org/pdf/2310.06743v2" rel="noopener ugc nofollow" target="_blank">Geographic Location Encoding with Spherical Harmonics and Sinusoidal Representation Network</a> (2024), ICLR 2024</p><p id="0f07" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[8] K. Klemmer, E. Rolf, C. Robinson, L. Mackey, M. Rußwurm, <a class="af nb" href="https://arxiv.org/pdf/2311.17179" rel="noopener ugc nofollow" target="_blank">SatCLIP: Global, General-Purpose Location Embeddings with Satellite Imagery</a> (2024)</p></div></div></div></div>    
</body>
</html>