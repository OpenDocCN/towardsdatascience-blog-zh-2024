<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Practical Guide to Contrastive Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Practical Guide to Contrastive Learning</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-practical-guide-to-contrastive-learning-26e912c0362f?source=collection_archive---------1-----------------------#2024-07-30">https://towardsdatascience.com/a-practical-guide-to-contrastive-learning-26e912c0362f?source=collection_archive---------1-----------------------#2024-07-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e967" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How to build your very first SimSiam model with FashionMNIST</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://mengliuz.medium.com/?source=post_page---byline--26e912c0362f--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mengliu Zhao" class="l ep by dd de cx" src="../Images/0b950a0785fa065db3319ed5be4a91de.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*siAyGzGqa7K3xsa639R_2w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--26e912c0362f--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://mengliuz.medium.com/?source=post_page---byline--26e912c0362f--------------------------------" rel="noopener follow">Mengliu Zhao</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--26e912c0362f--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="aa39" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Contrastive learning has many use cases these days. From NLP and computer vision to recommendation systems, contrastive learning can be used to learn underlying data representations without any explicit labels, which can then be used for downstream classification, detection, similarity search, etc.</p><p id="517a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">There are many online resources to help the audience understand the basic ideas of contrastive learning so that I won’t add one more blog post repeating the information. Instead, I will show you how to convert your supervised learning problem into a contrastive learning problem in this article. Specifically, I will start with a basic classification model for the <a class="af ne" href="https://github.com/zalandoresearch/fashion-mnist/tree/master" rel="noopener ugc nofollow" target="_blank">FashionMNIST</a> (<a class="af ne" href="https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank">MIT licence</a>). Then, I will proceed to an advanced problem with limited training labels (e.g., reducing the full training set of 60,000 labels to 1,000). I will introduce <a class="af ne" href="https://arxiv.org/pdf/2011.10566" rel="noopener ugc nofollow" target="_blank">SimSiam</a>, a state-of-the-art method for contrastive learning, and show step-by-step instructions on modifying the original linear layers in the SimSiam style. Ultimately, I’ll show the results — SimSiam could improve the F1 score by 15% with a very basic configuration.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/8640abb31d4b5da8c81b4a240542a64b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sE9_CrD8svoUkXsWuxewyA.jpeg"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Image source: <a class="af ne" href="https://pxhere.com/en/photo/395408" rel="noopener ugc nofollow" target="_blank">https://pxhere.com/en/photo/395408</a></figcaption></figure></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="1182" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Now, let’s start. First, we’ll load in the FashionMNIST dataset. A custom FashionMNIST class is used to obtain a subset of the training set named the finetune_dataset. The source code for the customer FashionMNIST class will be given at the end of this article.</p><pre class="ni nj nk nl nm og oh oi bp oj bb bk"><span id="8169" class="ok ol fq oh b bg om on l oo op">import matplotlib.pyplot as plt<br/><br/>import torchvision.transforms as transforms<br/><br/>from FashionMNIST import FashionMNIST<br/><br/>train_dataset = FashionMNIST("./FashionMNIST", <br/>                             train=True, <br/>                             transform=transforms.ToTensor(), <br/>                             download=True,<br/>                             )<br/>test_dataset = FashionMNIST("./FashionMNIST", <br/>                            train=False, <br/>                            transform=transforms.ToTensor(), <br/>                            download=True,<br/>                            )<br/>finetune_dataset = FashionMNIST("./FashionMNIST", <br/>                                train=True, <br/>                                transform=transforms.ToTensor(), <br/>                                download=True, <br/>                                first_k=1000,<br/>                                )<br/><br/># Create a subplot with 4x4 grid<br/>fig, axs = plt.subplots(4, 4, figsize=(8, 8))<br/><br/># Loop through each subplot and plot an image<br/>for i in range(4):<br/>    for j in range(4):<br/>        image, label = train_dataset[i * 4 + j]  # Get image and label<br/>        image_numpy = image.numpy().squeeze()    # Convert image tensor to numpy array<br/>        axs[i, j].imshow(image_numpy, cmap='gray')  # Plot the image<br/>        axs[i, j].axis('off')  # Turn off axis<br/>        axs[i, j].set_title(f"Label: {label}")  # Set title with label<br/><br/>plt.tight_layout()  # Adjust layout<br/>plt.show()  # Show plot</span></pre><p id="671a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The code will show a grid of images from the train_dataset</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng oq"><img src="../Images/8b4d3bbce67a51d76de2166c7d47e17c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tRfVL1dVKgez4RfFxx6T9Q.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">First 16 images from the FashionMNIST training set. Image by author.</figcaption></figure><p id="9c3d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Next, we’ll define the supervised classification model. The architecture contains a backbone of convolutional layers and an MLP head of two linear layers. This will set a consistent baseline for the following experiments, as SimSiam will only replace the MLP head for contrastive learning purposes.</p><pre class="ni nj nk nl nm og oh oi bp oj bb bk"><span id="23ab" class="ok ol fq oh b bg om on l oo op">import torch.nn as nn<br/><br/>class supervised_classification(nn.Module):<br/>    <br/>    def __init__(self):<br/>        super(supervised_classification, self).__init__()<br/>        <br/>        self.backbone = nn.Sequential(<br/>                                nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),<br/>                                nn.ReLU(),<br/>                                nn.BatchNorm2d(32),<br/>                                nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),<br/>                                nn.ReLU(),<br/>                                nn.BatchNorm2d(64),<br/>                                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),<br/>                                nn.ReLU(),<br/>                                nn.BatchNorm2d(128),<br/>        )<br/>        <br/>        self.fc = nn.Sequential(<br/>                                nn.Linear(128*4*4, 32),<br/>                                nn.ReLU(),<br/>                                nn.Linear(32, 10),<br/>        )<br/>    <br/>    def forward(self, x):<br/>        x = self.backbone(x).view(-1, 128 * 4 * 4)<br/>        <br/>        return self.fc(x)</span></pre><p id="ce25" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We’ll train the model for 10 epochs:</p><pre class="ni nj nk nl nm og oh oi bp oj bb bk"><span id="8509" class="ok ol fq oh b bg om on l oo op">import tqdm<br/><br/>import torch<br/>import torch.optim as optim<br/>from torch.utils.data import DataLoader<br/><br/>import wandb<br/><br/>wandb_config = {<br/>    "learning_rate": 0.001,<br/>    "architecture": "fashion mnist classification full training",<br/>    "dataset": "FashionMNIST",<br/>    "epochs": 10,<br/>    "batch_size": 64,<br/>    }<br/><br/>wandb.init(<br/>    # set the wandb project where this run will be logged<br/>    project="supervised_classification",<br/>    # track hyperparameters and run metadata<br/>    config=wandb_config,<br/>)<br/><br/># Initialize model and optimizer<br/>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<br/><br/>supervised = supervised_classification()<br/><br/>optimizer = optim.SGD(supervised.parameters(), <br/>                      lr=wandb_config["learning_rate"], <br/>                      momentum=0.9, <br/>                      weight_decay=1e-5,<br/>                      )<br/><br/>train_dataloader = DataLoader(train_dataset, <br/>                              batch_size=wandb_config["batch_size"], <br/>                              shuffle=True,<br/>                              )<br/><br/># Training loop<br/>loss_fun = nn.CrossEntropyLoss()<br/>for epoch in range(wandb_config["epochs"]):<br/>    supervised.train()<br/>    <br/>    train_loss = 0<br/>    for batch_idx, (image, target) in enumerate(tqdm.tqdm(train_dataloader, total=len(train_dataloader))):<br/>        optimizer.zero_grad()<br/><br/>        prediction = supervised(image)<br/>                                <br/>        loss = loss_fun(prediction, target)<br/>        loss.backward()<br/>        optimizer.step()<br/>                <br/>        wandb.log({"training loss": loss})<br/>        <br/>torch.save(supervised.state_dict(), "weights/fully_supervised.pt")</span></pre><p id="f94a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Using the classification_report from the scikit-learn package, we’ll get the following results:</p><pre class="ni nj nk nl nm og oh oi bp oj bb bk"><span id="3cbb" class="ok ol fq oh b bg om on l oo op">from sklearn.metrics import classification_report<br/><br/>supervised = supervised_classification()<br/>                         <br/>supervised.load_state_dict(torch.load("weights/fully_supervised.pt"))<br/>supervised.eval()<br/>supervised.to(device)<br/><br/>target_list = []<br/>prediction_list = []<br/>for batch_idx, (image, target) in enumerate(tqdm.tqdm(test_dataloader, total=len(test_dataloader))):<br/>    with torch.no_grad():<br/>        prediction = supervised(image.to(device))<br/>    <br/>    prediction_list.extend(torch.argmax(prediction, dim=1).detach().cpu().numpy())<br/>    target_list.extend(target.detach().cpu().numpy())<br/><br/>print(classification_report(target_list, prediction_list))<br/><br/># Create a subplot with 4x4 grid<br/>fig, axs = plt.subplots(4, 4, figsize=(8, 8))<br/><br/># Loop through each subplot and plot an image<br/>for i in range(4):<br/>    for j in range(4):<br/>        image, label = test_dataset[i * 4 + j]  # Get image and label<br/>        image_numpy = image.numpy().squeeze()    # Convert image tensor to numpy array<br/>        prediction = supervised(torch.unsqueeze(image, dim=0).to(device))<br/>        prediction = torch.argmax(prediction, dim=1).detach().cpu().numpy()<br/>        axs[i, j].imshow(image_numpy, cmap='gray')  # Plot the image<br/>        axs[i, j].axis('off')  # Turn off axis<br/>        axs[i, j].set_title(f"Label: {label}, Pred: {prediction}")  # Set title with label<br/><br/>plt.tight_layout()  # Adjust layout<br/>plt.show()  # Show plot</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng or"><img src="../Images/b99775d536be606ab36bba78f5014a32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*ItKSuPRRKkRtffgzhclf3w.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Classification results of the fully supervised model. Image by author.</figcaption></figure></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="faa1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Now, let’s think about <strong class="mk fr">a new problem</strong>. What should we do if we’re given a limited subset of the training set labels, e.g., only 1000 images out of the total 60,000 images are annotated? The natural idea is to simply train the model on the limited annotated dataset. So without changing the backbone, we let the model train on the limited subset for 100 epochs (we increase the epochs to have a fair comparison to our SimSiam training):</p><pre class="ni nj nk nl nm og oh oi bp oj bb bk"><span id="bd03" class="ok ol fq oh b bg om on l oo op">import tqdm<br/><br/>import torch<br/>import torch.optim as optim<br/>from torch.utils.data import DataLoader<br/><br/><br/>import wandb<br/><br/>wandb_config = {<br/>    "learning_rate": 0.001,<br/>    "architecture": "fashion mnist classification full training on finetune set",<br/>    "dataset": "FashionMNIST",<br/>    "epochs": 100,<br/>    "batch_size": 64,<br/>    }<br/><br/>wandb.init(<br/>    # set the wandb project where this run will be logged<br/>    project="supervised_classification",<br/>    # track hyperparameters and run metadata<br/>    config=wandb_config,<br/>)<br/><br/># Initialize model and optimizer<br/>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<br/><br/>supervised = supervised_classification()<br/><br/>optimizer = optim.SGD(supervised.parameters(), <br/>                      lr=wandb_config["learning_rate"], <br/>                      momentum=0.9, <br/>                      weight_decay=1e-5,<br/>                      )<br/><br/>finetune_dataloader = DataLoader(finetune_dataset, <br/>                                 batch_size=wandb_config["batch_size"], <br/>                                 shuffle=True,<br/>                                 )<br/><br/># Training loop<br/>loss_fun = nn.CrossEntropyLoss()<br/>for epoch in range(wandb_config["epochs"]):<br/>    supervised.train()<br/>    <br/>    train_loss = 0<br/>    for batch_idx, (image, target) in enumerate(tqdm.tqdm(finetune_dataloader, total=len(finetune_dataloader))):<br/>        optimizer.zero_grad()<br/><br/>        prediction = supervised(image)<br/>                                <br/>        loss = loss_fun(prediction, target)<br/>        loss.backward()<br/>        optimizer.step()<br/>                <br/>        wandb.log({"training loss": loss})<br/>        <br/>torch.save(supervised.state_dict(), "weights/fully_supervised_finetunedataset.pt")</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng os"><img src="../Images/17e7139fe4c8f3e1fff111b4c66fbfc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*3KbsLnJ8rpiiGcs13vdgvQ.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Fully supervised training loss on the limited training set. Image by author.</figcaption></figure><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng ot"><img src="../Images/4a1010578e8405e600400d717408e350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*udzesfmOcia08f9taOiKyQ.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Quantitative evaluation results on the testing set. Note the performance drops more than 25% by reducing the training size. Image by author.</figcaption></figure></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="3ec3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Now it’s time for some <strong class="mk fr">contrastive learning</strong>. To mitigate the issue of insufficient annotation labels and fully utilize the large quantity of unlabelled data, contrastive learning could be used to effectively help the backbone learn the data representations without a specific task. The backbone could be frozen for a given downstream task and only train a shallow network on a limited annotated dataset to achieve satisfactory results.</p><p id="13e1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The most commonly used contrastive learning approaches include SimCLR, SimSiam, and MOCO (see my <a class="af ne" href="https://medium.com/towards-data-science/from-moco-v1-to-v3-towards-building-a-dynamic-dictionary-for-self-supervised-learning-part-1-745dc3b4e861" rel="noopener">previous article on MOCO</a>). Here, we compare SimCLR and SimSiam.</p><p id="a8ae" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">SimCLR </strong>calculates over positive and negative pairs within the data batch, which requires hard negative mining, NT-Xent loss (which extends the cosine similarity loss over a batch) and a large batch size. SimCLR also requires the LARS optimizer to accommodate a large batch size.</p><p id="b6fa" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">SimSiam, </strong>however, uses a Siamese architecture, which avoids using negative pairs and further avoids the need for large batch sizes. The differences between SimSiam and SimCLR are given in the table below.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng ou"><img src="../Images/543adb5cca61405edac9477bfbeac9f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HGV5hcd8bhkMa5PiupN3jg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Comparison between SimCLR and SimSiam. Image by author.</figcaption></figure><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng ov"><img src="../Images/a9008b9698d06ee3fddd458255e54136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*L-7z34l5UCkGCO0zqiK8jw.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">The SimSiam architecture. Image source: <a class="af ne" href="https://arxiv.org/pdf/2011.10566" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2011.10566</a></figcaption></figure><p id="6b08" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We can see from the figure above that the SimSiam architecture only contains two parts: the encoder/backbone and the predictor. During training time, the gradient propagation of the Siamese part is stopped, and the cosine similarity is calculated between the outputs of the predictors and the backbone.</p><p id="2ea2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">So, how do we implement this architecture in reality? Continuing on the supervised classification design, we <strong class="mk fr">keep the backbone the same and only modify the MLP layer</strong>. In the supervised learning architecture, the MLP outputs a 10-element vector indicating the probabilities of the 10 classes. But for SimSiam, the purpose is not to perform “classification” but to learn the “representation,” so we need the output to be of the same dimension as the backbone output for loss calculation. And the negative_cosine_similarity is given below:</p><pre class="ni nj nk nl nm og oh oi bp oj bb bk"><span id="289c" class="ok ol fq oh b bg om on l oo op">import torch.nn as nn<br/>import matplotlib.pyplot as plt<br/><br/><br/>class SimSiam(nn.Module):<br/>    <br/>    def __init__(self):<br/>        <br/>        super(SimSiam, self).__init__()<br/>        <br/>        self.backbone = nn.Sequential(<br/>                                nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),<br/>                                nn.ReLU(),<br/>                                nn.BatchNorm2d(32),<br/>                                nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),<br/>                                nn.ReLU(),<br/>                                nn.BatchNorm2d(64),<br/>                                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),<br/>                                nn.ReLU(),<br/>                                nn.BatchNorm2d(128),<br/>        )<br/>        <br/>        self.prediction_mlp = nn.Sequential(nn.Linear(128*4*4, 64),<br/>                               nn.BatchNorm1d(64),<br/>                               nn.ReLU(),<br/>                               nn.Linear(64, 128*4*4),<br/>        )<br/><br/>    def forward(self, x):<br/>        x = self.backbone(x)<br/>        <br/>        x = x.view(-1, 128 * 4 * 4)<br/>        pred_output = self.prediction_mlp(x)<br/>        return x, pred_output<br/>    <br/>cos = nn.CosineSimilarity(dim=1, eps=1e-6)<br/>def negative_cosine_similarity_stopgradient(pred, proj):<br/>    return -cos(pred, proj.detach()).mean()</span></pre><p id="a948" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The pseudo-code for training the SimSiam is given in the original paper below:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng ow"><img src="../Images/168babbce2573a8be4068e0476bb70fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lxf9twaUuVkofiLB1ir2fg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Training pseudo-code for SimSiam. Source: <a class="af ne" href="https://arxiv.org/pdf/2011.10566" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2011.10566</a></figcaption></figure><p id="0015" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">And we convert it into real training code:</p><pre class="ni nj nk nl nm og oh oi bp oj bb bk"><span id="91dd" class="ok ol fq oh b bg om on l oo op">import tqdm<br/><br/>import torch<br/>import torch.optim as optim<br/>from torch.utils.data import DataLoader<br/>from torchvision.transforms import RandAugment<br/><br/>import wandb<br/><br/>wandb_config = {<br/>    "learning_rate": 0.0001,<br/>    "architecture": "simsiam",<br/>    "dataset": "FashionMNIST",<br/>    "epochs": 100,<br/>    "batch_size": 256,<br/>    }<br/><br/>wandb.init(<br/>    # set the wandb project where this run will be logged<br/>    project="simsiam",<br/>    # track hyperparameters and run metadata<br/>    config=wandb_config,<br/>)<br/><br/># Initialize model and optimizer<br/>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<br/><br/>simsiam = SimSiam()<br/><br/>random_augmenter = RandAugment(num_ops=5)<br/><br/>optimizer = optim.SGD(simsiam.parameters(), <br/>                      lr=wandb_config["learning_rate"], <br/>                      momentum=0.9, <br/>                      weight_decay=1e-5,<br/>                      )<br/><br/>train_dataloader = DataLoader(train_dataset, batch_size=wandb_config["batch_size"], shuffle=True)<br/><br/># Training loop<br/>for epoch in range(wandb_config["epochs"]):<br/>    simsiam.train()<br/>    <br/>    print(f"Epoch {epoch}")<br/>    train_loss = 0<br/>    for batch_idx, (image, _) in enumerate(tqdm.tqdm(train_dataloader, total=len(train_dataloader))):<br/>        optimizer.zero_grad()<br/>        <br/>        aug1, aug2 = random_augmenter((image*255).to(dtype=torch.uint8)).to(dtype=torch.float32) / 255.0, \<br/>                        random_augmenter((image*255).to(dtype=torch.uint8)).to(dtype=torch.float32) / 255.0<br/>                        <br/>        proj1, pred1 = simsiam(aug1)<br/>        proj2, pred2 = simsiam(aug2)<br/>                                <br/>        loss = negative_cosine_similarity_stopgradient(pred1, proj2) / 2 + negative_cosine_similarity_stopgradient(pred2, proj1) / 2<br/>        loss.backward()<br/>        optimizer.step()<br/>                <br/>        wandb.log({"training loss": loss})<br/>        <br/>    if (epoch+1) % 10 == 0:<br/>        torch.save(simsiam.state_dict(), f"weights/simsiam_epoch{epoch+1}.pt")</span></pre><p id="4b81" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We trained for 100 epochs as a fair comparison to the limited supervised training; the training loss is shown below. Note: Due to its Siamese design, SimSiam could be very sensitive to hyperparameters like learning rate and MLP hidden layers. The original SimSiam paper provides a detailed configuration for the ResNet50 backbone. For the ViT-based backbone, we recommend reading the <a class="af ne" href="https://arxiv.org/abs/2104.02057" rel="noopener ugc nofollow" target="_blank">MOCO v3 paper</a>, which adopts the SimSiam model in a momentum update scheme.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng ox"><img src="../Images/650a9d50562c4074180b497046a8f065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*vD4uVdKPnMF3-Is5nBcAxw.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Training loss for SimSiam. Image by author.</figcaption></figure><p id="217b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Then, we run the trained SimSiam on the testing set and visualize the representations using UMAP reduction:</p><pre class="ni nj nk nl nm og oh oi bp oj bb bk"><span id="113f" class="ok ol fq oh b bg om on l oo op">import tqdm<br/>import numpy as np<br/><br/>import torch<br/><br/>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<br/><br/>simsiam = SimSiam()                      <br/><br/>test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)<br/>simsiam.load_state_dict(torch.load("weights/simsiam_epoch100.pt"))<br/><br/>simsiam.eval()<br/>simsiam.to(device)<br/><br/>features = []<br/>labels = []<br/>for batch_idx, (image, target) in enumerate(tqdm.tqdm(test_dataloader, total=len(test_dataloader))):<br/>    <br/>    with torch.no_grad():<br/>    <br/>        proj, pred = simsiam(image.to(device))<br/>        <br/>    features.extend(np.squeeze(pred.detach().cpu().numpy()).tolist())<br/>    labels.extend(target.detach().cpu().numpy().tolist())<br/><br/>import plotly.express as px<br/>import umap.umap_ as umap<br/><br/>reducer = umap.UMAP(n_components=3, n_neighbors=10, metric="cosine")<br/>projections = reducer.fit_transform(np.array(features))<br/><br/>px.scatter(projections, x=0, y=1,<br/>    color=labels, labels={'color': 'Fashion MNIST Labels'}<br/>)</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng oy"><img src="../Images/34435a04bcac905eee507c19a059ffbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c1S8UbSrupYXYtl3ARkbkA.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">The UMAP of the SimSiam representation over the testing set. Image by author.</figcaption></figure><p id="2303" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">It’s interesting to see that there are two small islands in the reduced-dimension map above: class 5, 7, 8, and some 9. If we pull out the FashionMNIST class list, we know that these classes correspond to footwear such as “Sandal,” “Sneaker,” “Bag,” and “Ankle boot.” The big purple cluster corresponds to clothing classes like “T-shirt/top,” “Trousers,” “Pullover,” “Dress,” “Coat,” and “Shirt.” The SimSiam demonstrates learning a meaningful representation in the vision domain.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="92c6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Now that we have the correct representations, how can they benefit our classification problem? We simply load the trained SimSiam backbone into our classification model. However, instead of fine-tuning the whole architecture in the limited training set, we fine-tuned the linear layers and froze the backbone because we didn’t want to corrupt the representation already learned.</p><pre class="ni nj nk nl nm og oh oi bp oj bb bk"><span id="5001" class="ok ol fq oh b bg om on l oo op">import tqdm<br/><br/>import torch<br/>import torch.optim as optim<br/>from torch.utils.data import DataLoader<br/><br/>import wandb<br/><br/>wandb_config = {<br/>    "learning_rate": 0.001,<br/>    "architecture": "supervised learning with simsiam backbone",<br/>    "dataset": "FashionMNIST",<br/>    "epochs": 100,<br/>    "batch_size": 64,<br/>    }<br/>wandb.init(<br/>    # set the wandb project where this run will be logged<br/>    project="simsiam-finetune",<br/>    # track hyperparameters and run metadata<br/>    config=wandb_config,<br/>)<br/><br/># Initialize model and optimizer<br/>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<br/><br/>supervised = supervised_classification()<br/>                         <br/>model_dict = supervised.state_dict()<br/>simsiam_dict = {k: v for k, v in model_dict.items() if k in torch.load("simsiam.pt")}<br/>supervised.load_state_dict(simsiam_dict, strict=False)<br/><br/>finetune_dataloader = DataLoader(finetune_dataset, batch_size=32, shuffle=True)<br/><br/>for param in supervised.backbone.parameters():<br/>    param.requires_grad = False<br/>parameters = [para for para in supervised.parameters() if para.requires_grad]<br/>optimizer = optim.SGD(parameters, <br/>                      lr=wandb_config["learning_rate"], <br/>                      momentum=0.9, <br/>                      weight_decay=1e-5,<br/>                      )<br/><br/># Training loop<br/>for epoch in range(wandb_config["epochs"]):<br/>    supervised.train()<br/>    <br/>    train_loss = 0<br/>    for batch_idx, (image, target) in enumerate(tqdm.tqdm(finetune_dataloader)):<br/>        optimizer.zero_grad()<br/>        <br/>        prediction = supervised(image)<br/>        <br/>        loss = nn.CrossEntropyLoss()(prediction, target)<br/>        loss.backward()<br/>        optimizer.step()<br/>                <br/>        wandb.log({"training loss": loss})<br/>        <br/>torch.save(supervised.state_dict(), "weights/supervised_with_simsiam.pt")</span></pre><p id="bedf" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Here is the evaluation result of the SimSiam-pre-trained classification model. The average F1 score is increased by 15% compared to the supervised-only method.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng oz"><img src="../Images/b58599f74751115b271a349a81cf8209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*VQDRmE4KUVKNEyqFqDHmBg.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">The classification scores of the SimSiam model fine-tune on the limited set. Image by author.</figcaption></figure></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="dc25" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Summary. We showcase a simple but intuitive example, using FashionMNIST for contrastive learning. By using SimSiam for backbone pre-training and only fine-tuning the linear layers on the limited training set (which contains only 2% of the labels of the full training set), we increased the average F1 score by 15% over the fully supervised learning method. The trained weights, the notebook, and the customized FashionMNIST dataset class are all included in this <a class="af ne" href="https://github.com/adoskk/MachineLearningBasics/tree/main/unsupervised_learning/simsiam" rel="noopener ugc nofollow" target="_blank">GitHub repository</a>.</p><p id="8cf7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Give it a try!</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="85b6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">References:</strong></p><ul class=""><li id="656a" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pa pb pc bk">Chen et al., Exploring simple siamese representation learning. CVPR 2021.</li><li id="9a4f" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pa pb pc bk">Chen et al., A simple framework for contrastive learning of visual representations. ICML 2020.</li><li id="2629" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pa pb pc bk">Chen et al., An Empirical Study of Training Self-Supervised Vision Transformers. ICCV 2021.</li><li id="6736" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pa pb pc bk">Xiao et al., Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. arXiv preprint 2017. Github: <a class="af ne" href="https://github.com/zalandoresearch/fashion-mnist" rel="noopener ugc nofollow" target="_blank">https://github.com/zalandoresearch/fashion-mnist</a></li></ul></div></div></div></div>    
</body>
</html>