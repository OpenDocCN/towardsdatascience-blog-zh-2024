<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Case-Study: Multilingual LLM for Questionnaire Summarization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Case-Study: Multilingual LLM for Questionnaire Summarization</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/case-study-multilingual-llm-for-questionnaire-summarization-edf7acdcb37c?source=collection_archive---------5-----------------------#2024-07-30">https://towardsdatascience.com/case-study-multilingual-llm-for-questionnaire-summarization-edf7acdcb37c?source=collection_archive---------5-----------------------#2024-07-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="55a6" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">An LLM Approach to Summarizing Students’ Responses for Open-ended Questionnaires</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sria.louis?source=post_page---byline--edf7acdcb37c--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sria Louis" class="l ep by dd de cx" src="../Images/d65b17e9d4ace7e0222118abc70f3954.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*E7QMAOyM0WcVeQSPJ87KGw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--edf7acdcb37c--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@sria.louis?source=post_page---byline--edf7acdcb37c--------------------------------" rel="noopener follow">Sria Louis</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--edf7acdcb37c--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/cbbea877f5f00d69ee043faac8a55599.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hIR6LdKW70FwBD5T8R5cMQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Illustration: Or Livneh</figcaption></figure><p id="1af3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><a class="af nx" href="https://madrasafree.com/" rel="noopener ugc nofollow" target="_blank">Madrasa (מדרסה in Hebrew)</a> is an Israeli NGO dedicated to teaching Arabic to Hebrew speakers. Recently, while learning Arabic, I discovered that the NGO has unique data and that the organization might benefit from a thorough analysis. A friend and I joined the NGO as volunteers, and we were asked to work on the summarization task described below.</p><p id="78ad" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">What makes this summarization task so interesting is the unique mix of documents in three languages — Hebrew, Arabic, and English — while also dealing with the imprecise <a class="af nx" href="https://en.wikipedia.org/wiki/Transcription_(linguistics)" rel="noopener ugc nofollow" target="_blank">transcriptions</a> among them.</p><p id="c54a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">A word on privacy: The data may include PII and therefore cannot be published at this time. If you believe you can contribute, please contact us.</p><h2 id="b3ac" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk">Context of the Problem</h2><p id="82b0" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">As part of its language courses, Madrasa distributes questionnaires to students, which include both quantitative questions requiring numeric responses and open-ended questions where students provide answers in natural language.</p><p id="81fb" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In this blog post, we will concentrate on the open-ended natural language responses.</p><h2 id="1a08" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk">The Problem</h2><p id="6c33" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">The primary challenge is managing and extracting insights from a substantial volume of responses to open-ended questions. Specifically, the difficulties include:</p><p id="da1f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Multilingual Responses</strong>: Student responses are primarily in Hebrew but also include Arabic and English, creating a complex multilingual dataset. Additionally, since transliteration is commonly used in Spoken Arabic courses, we found that students sometimes answered questions using both transliteration and Arabic script. We were surprised to see that some students even transliterated Hebrew and Arabic into Latin letters.</p><p id="528c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Nuanced Sentiments</strong>: The responses vary widely in sentiment and tone, including humor, suggestions, gratitude, and personal reflections.</p><p id="7bc5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Diverse Topics</strong>: Students touch on a wide range of subjects, from praising teachers to reporting technical issues with the website and app, to personal aspirations.</p><h2 id="cf18" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk"><strong class="al">The Data</strong></h2><p id="e73b" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">There are couple of courses. Each course includes three questionnaires administered at the beginning, middle, and end of the course. Each questionnaire contains a few open-ended questions.</p><p id="f154" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The tables below provides examples of two questions along with a curated selection of student responses.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj oy"><img src="../Images/f88ecea943d4381a5ff4241b367c0428.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c9Mo4T8oWDoYYUt4xLf-VA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Example of a Question and Student Responses. LEFT: Original question and student responses. RIGHT: Translation into English for the blog post reader. Note the mix of languages, including Arabic-to-Hebrew transliteration, the variety of topics even within and the same sentences, and the different language registers. . Credit: Sria Louis / Madarsa</figcaption></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj oz"><img src="../Images/7feb7527f2a36be8c1ca11edf55634f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IwixuAdzCnhpyThm643aZA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Example of a question and student responses. LEFT: Original question and student responses. RIGHT: Translation into English for the blog post reader. Note the mix of languages and transliterations, including both English-to-Hebrew and Hebrew-to-English. Credit: Sria Louis / Madarsa</figcaption></figure><p id="0f1d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">There are tens of thousands of student responses for each question, and after splitting into sentences (as described below), there can be up to around 100,000 sentences per column. This volume is manageable, allowing us to work locally.</p><p id="1e82" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Our goal is to summarize student opinions on various topics for each course, questionnaire, and open-ended question. We aim to capture the “main opinions” of the students while ensuring that “niche opinions” or “valuable insights” provided by individual students are not overlooked.</p><h2 id="fc83" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk">The Solution</h2><p id="ee1a" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">To tackle challenges mention above, we implemented a multi-step natural language processing (NLP) solution.</p><p id="4a1b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The process pipeline involves:</p><ol class=""><li id="8e23" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pa pb pc bk">Sentence Tokenization (using NLTK Sentence Tokenizer)</li><li id="a18d" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pa pb pc bk">Topic Modeling (using BERTopic)</li><li id="26ae" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pa pb pc bk">Topic representation (using BERTopic + LLM)</li><li id="6504" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pa pb pc bk">Batch summarizing (LLM with mini-batch fitting into the context-size)</li><li id="9f9f" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pa pb pc bk">Re-summarizing the batches to create a final comprehensive summary.</li></ol><p id="f779" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Sentence Tokenization:</strong> We use NLTK to divide student responses into individual sentences. This process is crucial because student inputs often cover multiple topics within a single response. For example, a student might write, “The teacher used day-to-day examples. The games on the app were very good.” Here, each sentence addresses a different aspect of their experience. While sentence tokenization sometimes results in the loss of context due to cross-references between sentences, it generally enhances the overall analysis by breaking down responses into more manageable and topic-specific units. This approach has proven to significantly improve the end results.</p><blockquote class="pi pj pk"><p id="71d2" class="nb nc pl nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">NLTK’s Sentence Tokenizer (<code class="cx pm pn po pp b"><a class="af nx" href="https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html" rel="noopener ugc nofollow" target="_blank">nltk.tokenize.sent_tokenize</a></code>) splits documents into sentences using linguistics rules and models to identify sentence boundaries. The default English model worked well for our use case.</p></blockquote><p id="afe6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Topic Modeling with BERTopic:</strong> We utilized BERTopic to model the topics of the tokenized sentences, identify underlying themes, and assign a topic to each sentence. This step is crucial before summarization for several reasons. First, the variety of topics within the student responses is too vast to be handled effectively without topic modeling. By splitting the students’ answers into topics, we can manage and batch the data more efficiently, leading to improved performance during analysis. Additionally, topic modeling ensures that niche topics, mentioned by only a few students, do not get overshadowed by mainstream topics during the summarization process.</p><blockquote class="pi pj pk"><p id="b372" class="nb nc pl nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><a class="af nx" href="https://maartengr.github.io/BERTopic/index.html" rel="noopener ugc nofollow" target="_blank">BERTopic</a> is an elegant topic-modeling tool that embeds documents into vectors, clusters them, and models each cluster’s representation. Its key advantage is modularity, which we utilize for Hebrew embeddings and hyperparameter tuning.</p></blockquote><p id="ec40" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The BERTopic configuration was meticulously designed to address the multilingual nature of the data and the specific nuances of the responses, thereby enhancing the accuracy and relevance of the topic assignment.</p><p id="8d7a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Specifically, note that we used a <a class="af nx" href="https://huggingface.co/imvladikon/sentence-transformers-alephbert" rel="noopener ugc nofollow" target="_blank">Hebrew Sentence-embedding model</a>. We did consider using an embedding on word-level, but the sentence-embedding proved to be capturing the needed information.</p><p id="a145" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For dimension-reduction and clustering we used BERTopic standard models <a class="af nx" href="https://umap-learn.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">UMAP</a> and HDBSCAN, respectively, and with some hyper-parameter fine tuning the results satisfied us.</p><blockquote class="pi pj pk"><p id="3a3d" class="nb nc pl nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><a class="af nx" href="https://youtu.be/dGsxd67IFiU?si=CrAHWrXgLnq6-3ul" rel="noopener ugc nofollow" target="_blank">Here’s a fantastic talk on HDBSCAN</a> by John Healy, one of the authors. It’s not just very educational; the speaker is really funny and witty! Definitely worth a watch :)</p></blockquote><p id="e5ab" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">BERTopic has excellent documentation and a supportive community, so I’ll share a code snippet to show how easy it is to use with advanced models. More importantly, we want to emphasize some hyperparameter choices designed to achieve high cluster granularity and allow smaller topics. Remember that our goal is not only to summarize the “mainstream” ideas that most students agree upon but also to highlight nuanced perspectives and rarer students’ suggestions. This approach comes with the trade-off of slower processing and the risk of having too many topics, but managing ~40 topics is still feasible.</p><ul class=""><li id="8e27" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pq pb pc bk"><strong class="nd fr">UMAP dimension reduction</strong>: higher-than-standard number of components and small number of UMAP-neighbors.</li><li id="b37c" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pq pb pc bk"><strong class="nd fr">HDBSCAN clustering</strong>: min_sample = 2 for high sensitivity, while min_cluster_size = 7 allows very small clusters.</li><li id="4852" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pq pb pc bk"><strong class="nd fr">BERTopic</strong>: nr_topics = 40.</li></ul><pre class="ml mm mn mo mp pr pp ps bp pt bb bk"><span id="f090" class="pu nz fq pp b bg pv pw l px py">from bertopic import BERTopic<br/>from umap import UMAP <br/>from hdbscan import HDBSCAN<br/>from sentence_transformers import SentenceTransformer <br/>from bertopic.vectorizers import ClassTfidfTransformer<br/><br/>topic_size_ = 7<br/><br/># Sentence Embedding in Hebrew (works well also on English)<br/>sent_emb_model = "imvladikon/sentence-transformers-alephbert"<br/>sentence_model = SentenceTransformer(sent_emb_model)<br/><br/># Initialize UMAP model for dimensionality reduction to improve BERTopic<br/>umap_model = UMAP(n_components=128, n_neighbors=4, min_dist=0.0)<br/><br/># Initialize HDBSCAN model for BERTopic clustering<br/>hdbscan_model = HDBSCAN(min_cluster_size = topic_size_, <br/>                        gen_min_span_tree=True, <br/>                        prediction_data=True, <br/>                        min_samples=2)<br/><br/># class-based TF-IDF vectorization for topic representation prior to clustering<br/>ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)<br/><br/># Initialize MaximalMarginalRelevance for enhancing topic representation<br/>representation_model = MaximalMarginalRelevance(diversity=0.1)<br/><br/># Configuration for BERTopic<br/>bert_config = {<br/>    'embedding_model': sentence_model,  <br/>    'top_n_words': 20,  # Number of top words to represent each topic<br/>    'min_topic_size': topic_size_,  <br/>    'nr_topics': 40, <br/>    'low_memory': False, <br/>    'calculate_probabilities': False, <br/>    'umap_model': umap_model, <br/>    'hdbscan_model': hdbscan_model, <br/>    'ctfidf_model': ctfidf_model, <br/>    'representation_model': representation_model<br/>}<br/><br/># Initialize BERTopic model with the specified configuration<br/>topic_model = BERTopic(**bert_config)</span></pre><h2 id="f662" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk">Topic Representation &amp; Summarization</h2><p id="89a0" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">For the next two parts — topic representation and topic summarization — we used chat-based LLMs, carefully crafting system and user prompts. The straightforward approach involved setting the system prompt to define the tasks of keyword extraction and summarization, and using the user prompt to input a lengthy list of documents, constrained only by context limits.</p><p id="da11" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Before diving deeper, let’s discuss the choice of chat-based LLMs and the infrastructure used. For a rapid proof of concept and development cycle, we opted for Ollama, known for its easy setup and minimal friction. we encountered some challenges switching models on Google Colab, so we decided to work locally on my M3 laptop. Ollama utilizes the Mac iGPU efficiently and proved adequate for my needs.</p><p id="ea85" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Initially, we tested various multilingual models, including LLaMA2, <a class="af nx" href="https://ai.meta.com/blog/meta-llama-3/" rel="noopener ugc nofollow" target="_blank">LLaMA3</a> and LLaMA3.1. However, a new version of the Dicta 2.0 model was released recently, which outperformed the others right away. Dicta 2.0 not only delivered better semantic results but also featured improved Hebrew tokenization (~one token per Hebrew character), allowing for longer context lengths and therefore larger batch processing without quality loss.</p><blockquote class="pi pj pk"><p id="412b" class="nb nc pl nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><a class="af nx" href="https://dicta.org.il/dicta-lm" rel="noopener ugc nofollow" target="_blank">Dicta</a> is an LLM, bilingual (Hebrew/English), fine-tuned on Mistral-7B-v0.1. and is available on <a class="af nx" href="https://huggingface.co/collections/dicta-il/dicta-lm-20-collection-661bbda397df671e4a430c27" rel="noopener ugc nofollow" target="_blank">Hugging Face</a>.</p></blockquote><p id="d129" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Topic Representation:</strong> This crucial step in topic modeling involves defining and describing topics through representative keywords or phrases, capturing the essence of each topic. The aim is to create clear, concise descriptions to understand the content associated with each topic. While BERTopic offers effective tools for topic representation, we found it easier to use external LLMs for this purpose. This approach allowed for more flexible experimentation, such as keyword prompt engineering, providing greater control over topic description and refinement.</p><ul class=""><li id="ca18" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pq pb pc bk">System Prompt:</li></ul><blockquote class="pi pj pk"><p id="6e04" class="nb nc pl nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“תפקידך למצוא עד חמש מילות מפתח של הטקסט ולהחזירן מופרדות בסימון נקודה. הקפד שכל מילה נבחרת תהיה מהטקסט הנתון ושהמילים תהיינה שונות אחת מן השניה. החזר לכל היותר חמש מילים שונות, בעברית, בשורה אחת קצרה, ללא אף מילה נוספת לפני או אחרי, ללא מספור וללא מעבר שורה וללא הסבר נוסף.”</p></blockquote><ul class=""><li id="7d9f" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pq pb pc bk">User prompt was simply keywords and representative sentences returned by BERTopic default representation model (c-tf-idf).</li></ul><p id="a0f3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Batch Summarization with LLM Models:</strong> For each topic, we employed an LLM to summarize student responses. Due to the large volume of data, responses were processed in batches, with each batch summarized individually before aggregating these summaries into a final comprehensive overview.</p><ul class=""><li id="9273" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pq pb pc bk">System Prompt:</li></ul><blockquote class="pi pj pk"><p id="8c0c" class="nb nc pl nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“המטרה שלך היא לתרגם לעברית ואז לסכם בעברית. הקלט הוא תשובות התלמידים לגבי השאלה הבאה [&lt;X&gt;]. סכם בפסקה אחת בדיוק עם לכל היותר 10 משפטים. הקפד לוודא שהתשובה מבוססת רק על הדעות שניתנו. מבחינה דקדוקית, נסח את הסיכום בגוף ראשון יחיד, כאילו אתה אחד הסטודנטים. כתוב את הסיכום בעברית בלבד, ללא תוספות לפני או אחרי הסיכום”</p></blockquote><p id="719a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[&lt;X&gt;] above is the string of the question, that we are trying to summarize.</p><ul class=""><li id="c28f" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pq pb pc bk">User prompt was was a batch of students’ response (as in the example above)</li></ul><p id="ba96" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Note that we required translation to Hebrew before summarization. Without this specification, the model occasionally responded in English or Arabic if the input contained a mix of languages.</p><p id="5d06" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[Interestingly, Dicta 2.0 was able to converse in Arabic as well. This is surprising because Dicta 2.0 was not trained on Arabic (according to its release post, it was trained on 50% English and 50% Hebrew), and its base model, Mistral, was not specifically trained on Arabic either.]</p><p id="4545" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Re-group the Batches:</strong> The non-trivial final step involved re-summarizing the aggregated batches to produce a single cohesive summary per topic per question. This required meticulous prompt engineering to ensure relevant insights from each batch were accurately captured and effectively presented. By refining prompts, we guided the LLM to focus on key points, resulting in a comprehensive and insightful summary.</p><p id="9530" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This multi-step approach allowed us to effectively manage the multilingual and nuanced dataset, extract significant insights, and provide actionable recommendations to enhance the educational experience at מדרסה (Madrasa).</p><h2 id="9d07" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk">Evaluation</h2><p id="6ad3" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">Evaluating the summarization task typically involves manual scoring of the summary’s quality by humans. In our case, the task includes not only summarization but also business insights. Therefore, we require a summary that captures not only the average student’s response but also the edge cases and rare or radical insights from a small number of students.</p><p id="2520" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To address these needs, we split the evaluation into the six steps mentioned and assess them manually with a business-oriented approach. If you have a more rigorous method for holistic evaluation of such a project, we would love to hear your ideas :)</p><h1 id="79fa" class="pz nz fq bf oa qa qb gq oe qc qd gt oi qe qf qg qh qi qj qk ql qm qn qo qp qq bk">Results — example</h1><p id="144e" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">For instance, let’s look at one question from a questionnaire in the middle of a beginners’ course. The students were asked: “אנא שתף אותנו בהצעות לשיפור הקורס” (in English: “Please share with us suggestions for improving the course”).</p><p id="ad88" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Most students responded with positive feedback, but some provided specific suggestions. The variety of suggestions is vast, and using clustering (topic modeling) and summarization, we can derive impactful insights for the NGO’s management team.</p><p id="0e55" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is a plot of the topic clusters, presented using BERTopic visualization tools.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qr"><img src="../Images/4ff235d05a9f39828963f9ef2c5bf5f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4pyyS8yFdeb4sdNuGGnDHQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Hierarchical Clustering: For visualization purposes, we present a set of 10 topics. However, in some cases our analysis included experimentation with tens of topics. Credit: Sria Louis / Madrasa.</figcaption></figure><p id="3ac8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">And finally, below are seven topics (out of 40) summarizing the students’ responses to the above question. Each topic includes its keywords (generated by the keyword prompt), three representative responses from the cluster (selected using Representation Model), and the final summarization.</p><p id="ea48" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Bottom line, note the variety of topics and the insightful summaries.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qs"><img src="../Images/aa14eb3d8ebaca7422d86685dc12eeba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8gCPyyn6bwSbp_mpTE3mkA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Some of the topics: keywords, representing sentences and summaries. Credit: Sria Louis / Madrasa</figcaption></figure><h2 id="964b" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk">What next?</h2><p id="f6a3" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">We have six steps in mind:</p><ol class=""><li id="1fe7" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pa pb pc bk"><strong class="nd fr">Optimization</strong>: Experimenting with different architectures and hyperparameters.</li><li id="839f" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pa pb pc bk"><strong class="nd fr">Robustness</strong>: Understanding and addressing unexpected sensitivity to certain hyperparameters.</li><li id="7275" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pa pb pc bk"><strong class="nd fr">Hallucinations</strong>: Tackling hallucinations, particularly in small clusters/topics where the number of input sentences is limited, causing the model to generate ‘imaginary’ information.</li><li id="a4ea" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pa pb pc bk"><strong class="nd fr">Enriching Summarizations</strong>: Using chain-of-thought techniques.</li><li id="4a7d" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pa pb pc bk"><strong class="nd fr">Enriching Topic Modeling</strong>: Adding sentiment analysis before clustering. For example, if in a specific topic 95% of the responses were positive but 5% were very negative, it might be helpful to cluster based on both the topic and the sentiment in the sentence. This might help the summarizer avoid converging to the mean.</li><li id="cbeb" class="nb nc fq nd b go pd nf ng gr pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pa pb pc bk"><strong class="nd fr">Enhancing User Experience</strong>: Implementing RAG or LLM-explainability techniques. For instance, given a specific non-trivial insight, we want the user to click on the insight and trace back to the exact student’s response that led to the insight.</li></ol></div></div></div><div class="ab cb qt qu qv qw" role="separator"><span class="qx by bm qy qz ra"/><span class="qx by bm qy qz ra"/><span class="qx by bm qy qz"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="0fef" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">If you’re an LLM expert and would like to share your insights, we’d love to learn from you. Do you have suggestions for better models or approaches we should use? Ping us!</p><p id="494e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><a class="af nx" href="mailto:sria.louis@gmail.com" rel="noopener ugc nofollow" target="_blank">sria.louis@gmail.com</a></p><p id="d942" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Want to learn more about Madarsa? <a class="af nx" href="https://madrasafree.com/" rel="noopener ugc nofollow" target="_blank">https://madrasafree.com/</a></p><p id="70aa" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Code can be found on the <a class="af nx" href="https://github.com/gitLouis/madarse-summarization" rel="noopener ugc nofollow" target="_blank">Project GitHub reop</a>.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rb"><img src="../Images/62500ce8f7182ccfc9e9822e71ec64bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IO9wypPAsCWhuQUb3oAuTA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Illustration: Or Livneh</figcaption></figure><p id="0b3a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Keywords: NLP, Topic Modeling, LLM, Hebrew, Sentence Embedding , BERTopic, llama, NLTK, Dicta 2.0, Summarization, madrasa</p></div></div></div></div>    
</body>
</html>