# 多模态大语言模型与苹果的MM1

> 原文：[https://towardsdatascience.com/multimodal-large-language-models-apples-mm1-c1e94d87a161?source=collection_archive---------0-----------------------#2024-04-13](https://towardsdatascience.com/multimodal-large-language-models-apples-mm1-c1e94d87a161?source=collection_archive---------0-----------------------#2024-04-13)

## 本博客文章将深入介绍苹果“MM1：多模态LLM预训练的方式、分析与洞察”论文背后的架构和研究发现。

[](https://medium.com/@mgunton7?source=post_page---byline--c1e94d87a161--------------------------------)[![Matthew Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--c1e94d87a161--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c1e94d87a161--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c1e94d87a161--------------------------------) [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--c1e94d87a161--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c1e94d87a161--------------------------------) ·9分钟阅读·2024年4月13日

--

![](../Images/4b89777cfb3161fa6ce57a05fdb4277c.png)

图片由作者生成，使用DALL-E

抽象化是计算机科学中最重要的概念之一，具有极其深远的影响。从简化的角度来看，抽象化是将某种方法应用于多个不同情况的能力。例如，如果你在工厂中创建了一种根据大小成功分类苹果的方法，那么你也可以将这一解决方案抽象化，用于同样的方式对橙子或桃子进行分类。因此，通过抽象化，一种非常强大的解决方案能够深刻影响世界的多个领域。

虽然大语言模型在处理文本输入时具有出色的推理能力，但最近我们已经能够将它们的输入抽象化，使它们能够通过图像和声音进行推理。

以下博客文章深入探讨了苹果MM1论文中的架构剖析及其在构建多模态大语言模型（MLLM）时的研究成果。

# 抽象化LLM输入

大语言模型背后的架构可以追溯到2017年发表的论文《Attention is All You Need》，其中引入了Transformer架构。

本文展示了如何将人类语言转化为神经网络处理的标记（在该论文中，转化为不同语言）。

![](../Images/8537829bb3d08c52694fbb294fe530d1.png)

图1来自[《Attention is All You Need》](https://arxiv.org/pdf/1706.03762.pdf)

如你从图像中看到的，我们在早期进行了一次转换，将输入转换为标记（嵌入部分）。然而，并没有固有的理由认为只有文本数据才能映射为标记。因此，研究领域开始尝试将其他类型的数据映射为标记。

# MM1架构基础

![](../Images/effb93869b2820023bfd10695c290fa0.png)

来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的图3部分

苹果的模型有三个关键组件：视觉转换器（ViT）图像编码器、视觉-语言连接器和大型语言模型。假设你已经对LLM有一个不错的了解，知道它是如何工作的，那么我们就深入了解图像编码器和VL连接器。

# 图像编码器与视觉连接器

从抽象的角度看，我们可以将文本和图像视为不同种类的输入，但为了实现这一点，我们需要接受一个事实，即可能需要以不同的方式处理它们，以便将其转化为标记。目前，我们有两个不同的系统帮助我们将图像转换为LLM可以推理的标记：图像编码器和连接器。

首先，图像编码器负责将我们的图像转换为转换器模型可以理解的标记表示。

第二，连接器是将来自视觉编码器的数据转换成直接传递给大型语言模型的数据的部分。由于图像编码器返回的是标记，你可能会想，为什么我们还需要连接器。其背后的想法似乎是图像编码器在其标记中提供了过多的信息，因此为了在优化推理的同时减少成本，我们希望有选择地传递信息。

下面的图像展示了我们在此工作中的数据流。

![](../Images/d88a4a539cf61aeecfdaea062e1ce7a2.png)

来自[“Honeybee: Locality-enhanced Projector for Multimodal LLM”](https://arxiv.org/pdf/2312.06742.pdf)的图2

# 消融实验

机器学习中的消融研究围绕去除和修改模型的某些部分，以观察它们如何影响整体性能。苹果的研究聚焦于图像编码器的不同训练方式、VL连接器的不同投影器以及不同的预训练数据。

让我们深入探讨主要的发现。

# 图像编码器消融实验

对于图像编码器，他们在CLIP和AIM模型、图像分辨率大小以及模型训练所用的数据集之间进行了变化。下面的图表展示了每个消融实验的结果。

![](../Images/72efc54bb2d9ecd0a6ef4975725f6171.png)

来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的表1

让我们逐一介绍上述的主要部分，并解释它们是什么。

**CLIP**代表对比语言图像预训练，旨在通过为需要被视作文本的事物提供名称，帮助模型学习视觉概念。如下面的图像所示，它将图像与文本编码配对，以便模型最终能够将视觉令牌（在下面的图像中表示为I）与文本令牌T连接起来。这种方法称为对比训练。

![](../Images/2ee9a0733e16658d9ed133ce1a2ddda0.png)

图1来自[《从自然语言监督学习可迁移的视觉模型》](https://arxiv.org/pdf/2103.00020.pdf)

**AIM**代表自回归图像模型，它通过重构损失优化算法进行训练。这里的目标是查看变换器是否能够重建（恢复）给定的图像。

![](../Images/2df9b1a9690d422713ce7d372da6f989.png)

图2来自[《可扩展的大型自回归图像模型预训练》](https://arxiv.org/pdf/2401.08541.pdf)

**图像分辨率**在这里指的是输入到变换器中的像素数量。例如，378 x 378的图像分辨率意味着我们将传入一个该大小的矩阵，然后将其转换为模型将要训练的嵌入。训练数据分为（DFN-2B）、（DFN-5B）、（DFN-5B + VeCap）和（ImageText-400M）四组。

作者发现图像分辨率最为重要，其次是模型大小，然后是训练数据内容。具体来说，他们发现图像分辨率越高，模型在零-shot和少-shot提示下的表现越好。由于训练和运行具有更高图像分辨率要求的模型需要更多的计算资源，这表明对于视觉变换器而言，计算资源仍然至关重要。

# VL连接器消融实验

对于VL连接器，他们测试了图像使用64或144个令牌，图像分辨率使用224、336和378进行测试，并在几种架构之间做了选择。下面我将简要介绍这些架构。

**平均池化**正如其名称所示，是取所有令牌的平均值，然后对这个平均值进行线性投影，使得网格为8x8或12x12。

**注意力池化**假设图像令牌应被视为来自与文本令牌完全不同的群体。这里我们调整每个图像输入的令牌数量，在论文中称之为k可学习查询。研究人员仅考虑了k为64或144的情况。

**卷积映射**是Honeybee中的一种方法，使用ResNet动态决定从图像传递给LLM多少令牌。这在C-Abstractor模块中实现。

![](../Images/4848e1619cc1b54fc72ebf7f388afa54.png)

图4来自[该论文](https://arxiv.org/pdf/2403.09611.pdf)

从上面的内容可以看出，不同的架构实际上对性能的影响很小。正如人们可能猜到的，分辨率更高的图像以及传递的 token 更多，提升了所有连接器的性能，但提升幅度并不显著。

这一发现表明，我们要么尚未找到一种显著更好的方式来将图像编码器与大语言模型连接起来，要么这一领域根本就不是优秀模型能够显著区分的地方。

# 预训练数据消融

![](../Images/a7eba075bb1027a4aff1a184921b0107.png)

来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的表2

在这里，作者使用了四种不同的数据类型：带有描述的图像、合成描述的图像、交织的图像-文本数据，以及仅文本数据。他们总结出了四个经验教训，并用图表总结了性能变化。

![](../Images/817a2d9f60801091eb142e89d99b205a.png)

来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的图5a

**第一**，交织数据有助于少样本和仅文本性能，而带描述的数据有助于零样本性能。研究人员通过改变交织数据的比例，以下的图表展示了结果。正如你所看到的，使用交织数据训练的模型在少样本提示上表现明显优于使用完全文本或完全图像的模型。

![](../Images/4ac3c1384e53bdf08a5a445bbb9a4fd8.png)

来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的图5b

**第二**，仅文本数据有助于少样本推理。在此背景下，仅文本数据意味着训练数据包含了图像示例和仅文本示例。这么做是为了确保模型能够理解人类语言和图像。比较仅描述与描述加文本的效果，除了0-shot推理外，所有任务都显示了明显的提升，但对于除TextCore测试以外的所有任务，交织数据的表现优于交织加文本。

![](../Images/edf1887d66e248908c526bf475ee57b9.png)

来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的图5c

**第三**，如果你能正确地将图像和文本混合在一起，就能获得非常强的性能。上面的图表展示了不同的图像与文本数据混合比例与仅文本数据的对比。由于目标是拥有一个多模态模型，因此他们从未测试过没有任何图像数据的性能表现。这里的作者指出，91/9的比例产生了最 consistently 的优秀结果。

![](../Images/212adb561e42c3bbe4a6878cf7b81c6a.png)

来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的图5d

**第四**，合成数据有助于少样本学习。VeCap代表视觉增强描述，它是一种创建描述的方法，确保这些描述能够准确表达图像中的关键视觉信息。相反，想象一个描述，可能解释了照片背后的意义，但并没有解释照片中的任何元素。如果你的数据抓取工具找到了描述不准确的图片，你通常会做这种处理。

作者在此得出结论，VeCap在少样本推理中提供了“非平凡”的提升，但在质量上提升较小。这引发了关于VeCap性价比的问题。

# 结果

通过他们的消融实验结果，作者创建了两种形式的变换器：专家混合（Mixture-of-Expert）和常规形式。两个模型的编码器处理的是378 x 378的图像，且仅用DFN-5B数据集进行了预训练。它们的数据包含45%的带字幕数据，45%的交错数据和10%的仅文本数据（近似图像与文本数据的91:9比例）。VL连接器有144个令牌，他们选择了一个C摘要器，尽管他们指出这是一个相对随意的选择。对于大语言模型本身，他们创建了一个3B、7B和30B参数的模型（其中MoE模型仅支持到7B）。下图显示了这些模型的表现。

![](../Images/2e8811a3e70ecaa26b03e606a7725e08.png)

来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的表格4

有趣的是，这个30B参数的模型与其他拥有数十亿更多参数的模型（如LLaVA-NeXT-34B等）表现相当，暗示参数大小与性能之间可能存在某种量子关系。

# 结束语

多模态大语言模型是这一领域令人兴奋的一个重要部分。随着我们找到更好的方法将不同类型的数据转化为令牌，我们可能会解锁这些变换器的更多应用。展望未来，考虑如何将除文本描述以外的其他感官输入（如声音、气味甚至触觉）结合进来并不不合理。数据质量很可能会变得愈加宝贵。

由于作者得出结论认为不同的语言连接器没有显著差异，因此很有趣的是，研究是否应该集中于图像编码器，还是我们只是尚未找到真正突破性的方式来使用VL连接器。

除了这篇特定的论文外，一个大问题是这些多模态大语言模型（MLLMs）在基准测试之外的表现如何。随着大语言模型（LLMs）的普及，常见的批评之一是围绕基准测试进行的比较。通常这些基准测试使用一致的数据集进行比较，允许某个模型通过过拟合来获得更好的表现，即使这种过拟合并非故意。像[LLM Arena from lmsys](https://chat.lmsys.org/)这样的使用ELO棋类评级算法的方法，可能会提供一个更真实的模型表现比较。

总结来说，随着更多输入能够与大语言模型相连接，可以预期它们的应用领域将会增加。只有时间才能告诉我们如何让这项技术变得更加有用。

[1] McKinzie, B., 等人. [“MM1：多模态大语言模型预训练方法、分析与洞察”（2024）](https://arxiv.org/pdf/2403.09611.pdf)，arXiv

[2] Cha, J., 等人. [“Honeybee：多模态大语言模型的局部增强投影器”](https://arxiv.org/pdf/2312.06742.pdf)（2023），arXiv

[3] Antoniadis, P. 等人. [“机器学习：什么是消融研究？”](https://www.baeldung.com/cs/ml-ablation-study) (2024), arXiv

[4] Radford, A. 等人. [“从自然语言监督中学习可迁移的视觉模型”](https://arxiv.org/pdf/2103.00020.pdf) (2021), arXiv

[5] El-Nouby, Al. 等人. [“大规模预训练自回归图像模型”](https://arxiv.org/pdf/2401.08541.pdf) (2024), arXiv

[6] Vaswani, A. 等人. “[注意力即一切](https://arxiv.org/pdf/1706.03762.pdf)” (2017), arXiv

[7] Lai, Z. 等人. [“VeCLIP：通过视觉增强标题改进CLIP训练”](https://arxiv.org/abs/2310.07699) (2023), arXiv
