<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>9.11 or 9.9 — which one is higher?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>9.11 or 9.9 — which one is higher?</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/9-11-or-9-9-which-one-is-higher-6efbdbd6a025?source=collection_archive---------7-----------------------#2024-07-25">https://towardsdatascience.com/9-11-or-9-9-which-one-is-higher-6efbdbd6a025?source=collection_archive---------7-----------------------#2024-07-25</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="08a1" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Evaluating the uncertainty and brittleness in LLM prompts</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@armin.catovic?source=post_page---byline--6efbdbd6a025--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Armin Catovic" class="l ep by dd de cx" src="../Images/046042098f3fec885e756f7f8ee94e6a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*mbpGpAHB8ZUfLAYgsZpR_Q.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6efbdbd6a025--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@armin.catovic?source=post_page---byline--6efbdbd6a025--------------------------------" rel="noopener follow">Armin Catovic</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6efbdbd6a025--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">5 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 25, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div></div><div class="ab cb mi mj mk ml" role="separator"><span class="mm by bm mn mo mp"/><span class="mm by bm mn mo mp"/><span class="mm by bm mn mo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="5b04" class="pw-post-body-paragraph mq mr fq ms b go mt mu mv gr mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl fj bk">This ChatGPT prompt and its corresponding (incorrect) response were recently shared and re-posted on LinkedIn countless times. They were given as a solid proof that the AGI is just not there yet. Further re-posts also pointed out that re-arranging the prompt to: <em class="nm">“Which one is higher: 9.11 or 9.9?”,</em> guarantees a correct answer, and further emphasizes the brittleness of LLMs.</p><p id="25bb" class="pw-post-body-paragraph mq mr fq ms b go mt mu mv gr mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl fj bk">After evaluating both prompts against a random group of ChatGPT users, we found that in both cases the answer is <strong class="ms fr">incorrect about 50%</strong> of the time. As some users have correctly pointed out, there is a subtle ambiguity with the question, i.e. are we referring to mathematical inequality of two real numbers, or are we referring to two dates (e.g. September 11 vs September 9), or two sub-sections in a document (e.g. chapter 9.11 or 9.9)?</p><p id="9111" class="pw-post-body-paragraph mq mr fq ms b go mt mu mv gr mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl fj bk">We decided to perform a more controlled experiment by using OpenAI APIs. This way we have full control over both the system prompt and the user prompt; we can also take out the sampling uncertainty out of the equation as far as possible by e.g. setting the temperature low.</p><p id="78a9" class="pw-post-body-paragraph mq mr fq ms b go mt mu mv gr mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl fj bk"><strong class="ms fr">The final results are very interesting!</strong></p><h1 id="9366" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Hypotheses and Experimental Design</h1><p id="174a" class="pw-post-body-paragraph mq mr fq ms b go oj mu mv gr ok mx my mz ol nb nc nd om nf ng nh on nj nk nl fj bk">Our hypotheses can be stated as follows:</p><ul class=""><li id="43da" class="mq mr fq ms b go mt mu mv gr mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl oo op oq bk">Given the same prompt, without any additional context, and with temperature kept close to zero, we should nearly always obtain the same output, with stable log probabilities. While people refer to LLMs as “stochastic”, for a given input, LLM should always generate the same output; the “hallucinations” or variance comes from the sampling mechanism outside of the LLM, and this we can dampen significantly by setting a very low temperature value.</li><li id="8dd4" class="mq mr fq ms b go or mu mv gr os mx my mz ot nb nc nd ou nf ng nh ov nj nk nl oo op oq bk">Based on our random user tests with ChatGPT, we would expect both the original prompt, and the re-worded version to give incorrect answer 50% of the time — in other words, without further disambiguation or context, we wouldn’t expect one prompt to perform better than the other.</li></ul><p id="2e90" class="pw-post-body-paragraph mq mr fq ms b go mt mu mv gr mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl fj bk">For our experiment design, we perform the following:</p><ul class=""><li id="5de3" class="mq mr fq ms b go mt mu mv gr mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl oo op oq bk">We conduct a number of experiments, starting with the original prompt, followed by a series of “interventions”</li><li id="33a0" class="mq mr fq ms b go or mu mv gr os mx my mz ot nb nc nd ou nf ng nh ov nj nk nl oo op oq bk">For each experiment/intervention, we execute 1 000 trials</li><li id="1984" class="mq mr fq ms b go or mu mv gr os mx my mz ot nb nc nd ou nf ng nh ov nj nk nl oo op oq bk">We use OpenAI’s most advanced GPT-4o model</li><li id="cc07" class="mq mr fq ms b go or mu mv gr os mx my mz ot nb nc nd ou nf ng nh ov nj nk nl oo op oq bk">We set the temperature to 0.1 to essentially eliminate the randomness due to sampling; we experiment with both random seed as well as fixed seed</li><li id="6fa0" class="mq mr fq ms b go or mu mv gr os mx my mz ot nb nc nd ou nf ng nh ov nj nk nl oo op oq bk">To gauge the “confidence” of the answer, we collect the log probability and calculate the linear probability of the answer in each trial; we plot the Kernel Density Estimate (KDE) of the linear probabilities across the 1 000 trials for each of the experiments</li></ul><p id="dc8e" class="pw-post-body-paragraph mq mr fq ms b go mt mu mv gr mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl fj bk">The full code for our experimental design is available <a class="af ow" href="https://github.com/acatovic/llm-prompt-uncertainty-test" rel="noopener ugc nofollow" target="_blank">here</a>.</p><h1 id="eaa2" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Experiment (A) — Original Prompt</h1><p id="5493" class="pw-post-body-paragraph mq mr fq ms b go oj mu mv gr ok mx my mz ol nb nc nd om nf ng nh on nj nk nl fj bk">The user prompt is set to <em class="nm">“9.11 or 9.9 — which one is higher?”</em>.</p><p id="6c2e" class="pw-post-body-paragraph mq mr fq ms b go mt mu mv gr mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl fj bk">In line with what social media users have reported, <strong class="ms fr">GPT-4o gives the correct answer 55% of the time</strong> ☹️. The model is also not very certain — on large number of trials, <strong class="ms fr">its “confidence” in the answer is ~80%</strong>.</p><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy oz"><img src="../Images/303e02b6da3e317bb99c38ca028ad4fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_e9JySfsFPNsSPGCCAMzYw.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Figure 1 — Smoothed histogram (KDE) of confidence values (0–100%) across 1000 trials, when the original user prompt is used; image by the author</figcaption></figure><h1 id="c065" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Experiment (B) — Re-worded User Prompt</h1><p id="4482" class="pw-post-body-paragraph mq mr fq ms b go oj mu mv gr ok mx my mz ol nb nc nd om nf ng nh on nj nk nl fj bk">In the re-worded user prompt, no additional context/disambiguation is provided, but the wording is slightly changed to: <em class="nm">“Which one is higher, 9.11 or 9.9?”</em></p><p id="9e15" class="pw-post-body-paragraph mq mr fq ms b go mt mu mv gr mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl fj bk">Amazingly, and contrary to our ChatGPT user tests, the <strong class="ms fr">correct answer is reached 100% of the time</strong> across 1 000 trials. Furthermore, the model exhibits <strong class="ms fr">very high confidenc</strong>e in its answer 🤔.</p><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy pq"><img src="../Images/bd2ac34fa08ee808f5ab358f41a3e665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wr_cVsMthcQGuz7CLPctoQ.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Figure 2 — Smoothed histogram (KDE) of confidence values (0–100%) across 1000 trials, when the original user prompt is slightly re-worded; image by the author</figcaption></figure><h1 id="d42d" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Experiment (C) — Original User Prompt with Reasoning</h1><p id="69eb" class="pw-post-body-paragraph mq mr fq ms b go oj mu mv gr ok mx my mz ol nb nc nd om nf ng nh on nj nk nl fj bk">There has been significant work recently in trying to induce improved “reasoning” capabilities in LLMs with chain-of-thought (CoT) prompting being the most popular. <a class="af ow" href="https://arxiv.org/pdf/2212.10403" rel="noopener ugc nofollow" target="_blank">Huang et al</a> have published a very comprehensive survey on LLM reasoning capabilities.</p><p id="048e" class="pw-post-body-paragraph mq mr fq ms b go mt mu mv gr mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl fj bk">As such, we modify the original user prompt by also telling the LLM to explain its reasoning. Interestingly enough, the <strong class="ms fr">probability of correct answer improves to 62%</strong>, however the answers come with <strong class="ms fr">even greater uncertainty</strong>.</p><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy pr"><img src="../Images/c8d22876d032feec673a6dce588b7c6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eOly5gDTRNOakcDwxvwmEw.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Figure 3 — Smoothed histogram (KDE) of confidence values (0–100%) across 1000 trials, when the original user prompt is modified to also “explain its reasoning”; image by the author</figcaption></figure><h1 id="51ff" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Experiment (D) — Original User Prompt with Reasoning in the System Prompt</h1><p id="b940" class="pw-post-body-paragraph mq mr fq ms b go oj mu mv gr ok mx my mz ol nb nc nd om nf ng nh on nj nk nl fj bk">The final experiment is the same as experiment “C”, however we instead bootstrap the <strong class="ms fr">system prompt</strong> by telling the LLM to “explain its reasoning”. Incredibly, we now see the <strong class="ms fr">correct answer 100% of the time</strong>, with <strong class="ms fr">very high confidence</strong>. We see identical results if we use the re-worded user prompt as well.</p><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy ps"><img src="../Images/c7fbb5db47568c2f787a98e7d755c7dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*negpF-0E9PkHloLHBRQnIg.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Figure 4 — Smoothed histogram (KDE) of confidence values (0–100%) across 1000 trials, with the original user prompt, and system prompt amended with instructions to “explain its reasoning”; image by the author</figcaption></figure><h1 id="a20a" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Conclusion and Takeaways</h1><p id="3033" class="pw-post-body-paragraph mq mr fq ms b go oj mu mv gr ok mx my mz ol nb nc nd om nf ng nh on nj nk nl fj bk">What started off as a simple experiment to validate some of the statements seen on social media, ended up with some very interesting findings. Let’s summarize the key takeaways:</p><ul class=""><li id="17fd" class="mq mr fq ms b go mt mu mv gr mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl oo op oq bk"><strong class="ms fr">For an identical prompt, with both temperature set very low (essentially eliminating sampling uncertainty), and a fixed seed value, we see very large variance in log probabilities</strong>. Slight variance can be explained by hardware precision, but variance this large is very difficult to explain. It indicates that either (1) sampling mechanism is a LOT more complicated, or (2) there are more layers/models upstream beyond our control.</li><li id="f3c1" class="mq mr fq ms b go or mu mv gr os mx my mz ot nb nc nd ou nf ng nh ov nj nk nl oo op oq bk">In line with previous literature, <strong class="ms fr">simply instructing the LLM to “explain its reasoning” improves its performance</strong>.</li><li id="688e" class="mq mr fq ms b go or mu mv gr os mx my mz ot nb nc nd ou nf ng nh ov nj nk nl oo op oq bk"><strong class="ms fr">There is clearly a distinct handling between the system prompt and the user prompt</strong>. Bootstrapping a role in the system prompt as opposed to the user prompt, seems to result in significantly better performance.</li><li id="31d1" class="mq mr fq ms b go or mu mv gr os mx my mz ot nb nc nd ou nf ng nh ov nj nk nl oo op oq bk">We can clearly see how brittle the prompts can be. The key takeaway here is that we should always aim to provide disambiguation and clear context in our prompts.</li></ul></div></div></div><div class="ab cb mi mj mk ml" role="separator"><span class="mm by bm mn mo mp"/><span class="mm by bm mn mo mp"/><span class="mm by bm mn mo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="67ba" class="pw-post-body-paragraph mq mr fq ms b go mt mu mv gr mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl fj bk"><strong class="ms fr"><em class="nm">Disclaimer:</em></strong><em class="nm"> due to heavy coverage on social media, it is likely that the lovely people at OpenAI have in fact improved the above behaviour, so the results may not be directly reproducible. However the key takeaways are still very valid!</em></p></div></div></div></div>    
</body>
</html>