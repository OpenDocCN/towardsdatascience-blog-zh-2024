<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Visualizing Gradient Descent Parameters in Torch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Visualizing Gradient Descent Parameters in Torch</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visualizing-gradient-descent-parameters-in-torch-332a63d1e5c5?source=collection_archive---------0-----------------------#2024-02-26">https://towardsdatascience.com/visualizing-gradient-descent-parameters-in-torch-332a63d1e5c5?source=collection_archive---------0-----------------------#2024-02-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="790b" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Prying behind the interface to see the effects of SGD parameters on your model training</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://thepgb.medium.com/?source=post_page---byline--332a63d1e5c5--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="P.G. Baumstarck" class="l ep by dd de cx" src="../Images/b23cb187c99cc30201ad8028afca72ed.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*-UjKX3Wn7Au14jF-OitMHw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--332a63d1e5c5--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://thepgb.medium.com/?source=post_page---byline--332a63d1e5c5--------------------------------" rel="noopener follow">P.G. Baumstarck</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--332a63d1e5c5--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">13</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="5e29" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Behind the simple interfaces of modern machine learning frameworks lie large amounts of complexity. With so many dials and knobs exposed to us, we could easily fall into cargo cult programming if we don’t understand what’s going on underneath. Consider the many parameters of Torch’s <a class="af nf" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html" rel="noopener ugc nofollow" target="_blank">stochastic gradient descent (SGD) optimizer</a>:</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="e6b3" class="np nq fq nm b bg nr ns l nt nu">def torch.optim.SGD(<br/>  params, lr=0.001, momentum=0, dampening=0,<br/>  weight_decay=0, nesterov=False, *, maximize=False,<br/>  foreach=None, differentiable=False):<br/>  # Implements stochastic gradient descent (optionally with momentum).<br/>  # ...</span></pre><p id="6cb9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Besides the familiar learning rate <code class="cx nv nw nx nm b">lr</code> and <code class="cx nv nw nx nm b">momentum</code> parameters, there are several other that have stark effects on neural network training. In this article we’ll visualize the effects of these parameters on a simple ML objective with a variety of loss functions.</p><h1 id="e14d" class="ny nq fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Toy Problem</h1><p id="5077" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">To start we construct a toy problem of performing linear regression over a set of points. To make it interesting we’re going to use a quadratic function plus noise so that the neural network will have to make trade-offs—and we’ll also get to observe more of the impact of the loss functions:</p><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div class="oy oz pa"><img src="../Images/46819efbc1942caad0ed06096226be7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*veHPKs8xxSyvnDNCSQq3zw.png"/></div></figure><p id="2ea4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We start off just using <code class="cx nv nw nx nm b">numpy</code> and <code class="cx nv nw nx nm b">matplotlib</code> to visualization our data—no <code class="cx nv nw nx nm b">torch</code> required yet:</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="f0d7" class="np nq fq nm b bg nr ns l nt nu">import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/>np.random.seed(20240215)<br/>n = 50<br/>x = np.array(np.random.randn(n), dtype=np.float32)<br/>y = np.array(<br/>  0.75 * x**2 + 1.0 * x + 2.0 + 0.3 * np.random.randn(n),<br/>  dtype=np.float32)<br/><br/>plt.scatter(x, y, facecolors='none', edgecolors='b')<br/>plt.scatter(x, y, c='r')<br/>plt.show()</span></pre><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div role="button" tabindex="0" class="pe pf ed pg bh ph"><div class="oy oz pd"><img src="../Images/fc9a458f92c166abbba3805523a44f56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mxbyrtN1gLhXxKaqW2bONQ.png"/></div></div><figcaption class="pi pj pk oy oz pl pm bf b bg z dx">Figure 1. Toy problem set of points.</figcaption></figure><p id="3e04" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next we’ll break out the <code class="cx nv nw nx nm b">torch</code> and introduce a simple training loop for a single-neuron network. To get consistent results when we vary the loss function, we’ll start our training from the same set of parameters each time with the neuron’s first “guess” being the equation <code class="cx nv nw nx nm b">y = 6*x — 3</code> (which we effect via the neuron’s <code class="cx nv nw nx nm b">weight</code> and <code class="cx nv nw nx nm b">bias</code> parameters):</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="bce1" class="np nq fq nm b bg nr ns l nt nu">import torch<br/><br/>model = torch.nn.Linear(1, 1)<br/>model.weight.data.fill_(6.0)<br/>model.bias.data.fill_(-3.0)<br/><br/>loss_fn = torch.nn.MSELoss()<br/>learning_rate = 0.1<br/>epochs = 100<br/>optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)<br/><br/>for epoch in range(epochs):<br/>  inputs = torch.from_numpy(x).requires_grad_().reshape(-1, 1)<br/>  labels = torch.from_numpy(y).reshape(-1, 1)<br/><br/>  optimizer.zero_grad()<br/>  outputs = model(inputs)<br/>  loss = loss_fn(outputs, labels)<br/>  loss.backward()<br/>  optimizer.step()<br/>  print('epoch {}, loss {}'.format(epoch, loss.item()))</span></pre><p id="648a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Running this gives us text output that shows us the loss is decreasing, eventually down to a minimum, as expected:</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="b66b" class="np nq fq nm b bg nr ns l nt nu">epoch 0, loss 53.078269958496094<br/>epoch 1, loss 34.7295036315918<br/>epoch 2, loss 22.891206741333008<br/>epoch 3, loss 15.226042747497559<br/>epoch 4, loss 10.242652893066406<br/>epoch 5, loss 6.987757682800293<br/>epoch 6, loss 4.85075569152832<br/>epoch 7, loss 3.4395809173583984<br/>epoch 8, loss 2.501774787902832<br/>epoch 9, loss 1.8742430210113525<br/>...<br/>epoch 97, loss 0.4994412660598755<br/>epoch 98, loss 0.4994412362575531<br/>epoch 99, loss 0.4994412660598755</span></pre><p id="da54" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To visualize our fit, we take the learned bias and weight out of our neuron and plot the fit against the points:</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="611f" class="np nq fq nm b bg nr ns l nt nu">weight = model.weight.item()<br/>bias = model.bias.item()<br/>plt.scatter(x, y, facecolors='none', edgecolors='b')<br/>plt.plot(<br/>  [x.min(), x.max()],<br/>  [weight * x.min() + bias, weight * x.max() + bias],<br/>  c='r')<br/>plt.show()</span></pre><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div role="button" tabindex="0" class="pe pf ed pg bh ph"><div class="oy oz pn"><img src="../Images/b45c711099dd52f177b3ff13e8098502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PW5SsGJ3Ma4mY4xrVdFkTw.png"/></div></div><figcaption class="pi pj pk oy oz pl pm bf b bg z dx">Figure 2. L2-learned linear boundary on toy problem.</figcaption></figure><h1 id="0170" class="ny nq fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Visualizing the Loss Function</h1><p id="0a49" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">The above seems a reasonable fit, but so far everything has been handled by high-level Torch functions like <code class="cx nv nw nx nm b">optimizer.zero_grad()</code>, <code class="cx nv nw nx nm b">loss.backward()</code>, and <code class="cx nv nw nx nm b">optimizer.step()</code>. To understand where we’re going next, we’ll need to visualize the journey our model is taking through the loss function. To visualize the loss, we’ll sample it in a grid of 101-by-101 points, then plot it using <code class="cx nv nw nx nm b">imshow</code>:</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="628e" class="np nq fq nm b bg nr ns l nt nu">def get_loss_map(loss_fn, x, y):<br/>  """Maps the loss function on a 100-by-100 grid between (-5, -5) and (8, 8)."""<br/>  losses = [[0.0] * 101 for _ in range(101)]<br/>  x = torch.from_numpy(x)<br/>  y = torch.from_numpy(y)<br/>  for wi in range(101):<br/>    for wb in range(101):<br/>      w = -5.0 + 13.0 * wi / 100.0<br/>      b = -5.0 + 13.0 * wb / 100.0<br/>      ywb = x * w + b<br/>      losses[wi][wb] = loss_fn(ywb, y).item()<br/><br/>  return list(reversed(losses))  # Because y will be reversed.<br/><br/>import pylab<br/><br/>loss_fn = torch.nn.MSELoss()<br/>losses = get_loss_map(loss_fn, x, y)<br/>cm = pylab.get_cmap('terrain')<br/><br/>fig, ax = plt.subplots()<br/>plt.xlabel('Bias')<br/>plt.ylabel('Weight')<br/>i = ax.imshow(losses, cmap=cm, interpolation='nearest', extent=[-5, 8, -5, 8])<br/>fig.colorbar(i)<br/>plt.show()</span></pre><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div role="button" tabindex="0" class="pe pf ed pg bh ph"><div class="oy oz po"><img src="../Images/f827e4cc908fde0c3706cb7893924698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N2XI9kysEt5wI1hz2IIW5g.png"/></div></div><figcaption class="pi pj pk oy oz pl pm bf b bg z dx">Figure 3. L2 loss function on toy problem.</figcaption></figure><p id="e37e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now we can capture the model parameters while running gradient descent to show us how the optimizer is performing:</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="7694" class="np nq fq nm b bg nr ns l nt nu">model = torch.nn.Linear(1, 1)<br/>...<br/>models = [[model.weight.item(), model.bias.item()]]<br/>for epoch in range(epochs):<br/>  ...<br/>  print('epoch {}, loss {}'.format(epoch, loss.item()))<br/>  models.append([model.weight.item(), model.bias.item()])<br/><br/># Plot model parameters against the loss map.<br/>cm = pylab.get_cmap('terrain')<br/>fig, ax = plt.subplots()<br/>plt.xlabel('Bias')<br/>plt.ylabel('Weight')<br/>i = ax.imshow(losses, cmap=cm, interpolation='nearest', extent=[-5, 8, -5, 8])<br/><br/>model_weights, model_biases = zip(*models)<br/>ax.scatter(model_biases, model_weights, c='r', marker='+')<br/>ax.plot(model_biases, model_weights, c='r')<br/><br/>fig.colorbar(i)<br/>plt.show()</span></pre><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div role="button" tabindex="0" class="pe pf ed pg bh ph"><div class="oy oz pp"><img src="../Images/9d08ab225a9b8cba02f255503b700ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AWRFSOMb2c3IuWR4mPMkQg.png"/></div></div><figcaption class="pi pj pk oy oz pl pm bf b bg z dx">Figure 4. Visualized gradient descent down loss function.</figcaption></figure><p id="89c7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">From inspection this looks exactly as it should: the model starts off at our force-initialized parameters of <code class="cx nv nw nx nm b">(-3, 6)</code>, it takes progressively smaller steps in the direction of the gradient, and it eventually bottoms-out in the global minimum.</p><h1 id="0a1c" class="ny nq fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Visualizing the Other Parameters</h1><h2 id="4145" class="pq nq fq bf nz pr ps pt oc pu pv pw of ms px py pz mw qa qb qc na qd qe qf qg bk">Loss Function</h2><p id="1d7e" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Now we’ll start examining the effects of the other parameters on gradient descent. First is the loss function, for which we used the standard L2 loss:</p><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div role="button" tabindex="0" class="pe pf ed pg bh ph"><div class="oy oz qh"><img src="../Images/fb8c557e293869de99c1499dbcf614e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1jKrH5Rf-IFVDrqicGSS-g.png"/></div></div><figcaption class="pi pj pk oy oz pl pm bf b bg z dx">L2 loss (<code class="cx nv nw nx nm b">torch.nn.MSELoss) accumulates the squared error. Source: <a class="af nf" href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html" rel="noopener ugc nofollow" target="_blank">link</a>. Screen capture by author.</code></figcaption></figure><p id="d964" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But there are several other loss functions we could use:</p><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div role="button" tabindex="0" class="pe pf ed pg bh ph"><div class="oy oz qi"><img src="../Images/4bddd81b3c6d0dedc452902c6366a2f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lgD-3oGdTtBaf-PqqsiPtw.png"/></div></div><figcaption class="pi pj pk oy oz pl pm bf b bg z dx">L1 loss (torch.nn.L1Loss) <code class="cx nv nw nx nm b">accumulates absolute</code> errors. <code class="cx nv nw nx nm b">Source: <a class="af nf" href="https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html" rel="noopener ugc nofollow" target="_blank">link</a>. Screen capture by author.</code></figcaption></figure><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div role="button" tabindex="0" class="pe pf ed pg bh ph"><div class="oy oz qj"><img src="../Images/3a64709da24887fc575dd61e2d012c98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_RJTLoufNBzFgW-Q5G-2pQ.png"/></div></div><figcaption class="pi pj pk oy oz pl pm bf b bg z dx">Huber loss (torch.nn.HuberLoss) uses L2 for small errors and L1 for large. <code class="cx nv nw nx nm b">Source: <a class="af nf" href="https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html" rel="noopener ugc nofollow" target="_blank">link</a>. Screen capture by author.</code></figcaption></figure><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div role="button" tabindex="0" class="pe pf ed pg bh ph"><div class="oy oz qk"><img src="../Images/a52df315f2839329b503ba215290e53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rnK-VdZeQtNpfUHShDIUoA.png"/></div></div><figcaption class="pi pj pk oy oz pl pm bf b bg z dx">Smooth L1 loss (torch.nn.SmoothL1Loss) is roughly equivalent to Huber loss with an extra beta parameter. <code class="cx nv nw nx nm b">Source: <a class="af nf" href="https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html" rel="noopener ugc nofollow" target="_blank">link</a>. Screen capture by author.</code></figcaption></figure><p id="ccc0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We wrap everything we’ve done so far in a loop to try out all the loss functions and plot them together:</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="cee1" class="np nq fq nm b bg nr ns l nt nu">def multi_plot(lr=0.1, epochs=100, momentum=0, weight_decay=0, dampening=0, nesterov=False):<br/>  fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)<br/>  for loss_fn, title, ax in [<br/>    (torch.nn.MSELoss(), 'MSELoss', ax1),<br/>    (torch.nn.L1Loss(), 'L1Loss', ax2),<br/>    (torch.nn.HuberLoss(), 'HuberLoss', ax3),<br/>    (torch.nn.SmoothL1Loss(), 'SmoothL1Loss', ax4),<br/>  ]:<br/>    losses = get_loss_map(loss_fn, x, y)<br/>    model, models = learn(<br/>      loss_fn, x, y, lr=lr, epochs=epochs, momentum=momentum,<br/>      weight_decay=weight_decay, dampening=dampening, nesterov=nesterov)<br/><br/>    cm = pylab.get_cmap('terrain')<br/>    i = ax.imshow(losses, cmap=cm, interpolation='nearest', extent=[-5, 8, -5, 8])<br/>    ax.title.set_text(title)<br/>    loss_w, loss_b = zip(*models)<br/>    ax.scatter(loss_b, loss_w, c='r', marker='+')<br/>    ax.plot(loss_b, loss_w, c='r')<br/><br/>  plt.show()<br/><br/>multi_plot(lr=0.1, epochs=100)</span></pre><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div role="button" tabindex="0" class="pe pf ed pg bh ph"><div class="oy oz ql"><img src="../Images/2a7f3f2a63ba5652ce65510a4a010915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x6pKNbi-mznjKWFYrUDgfw.png"/></div></div><figcaption class="pi pj pk oy oz pl pm bf b bg z dx">Figure 5. Visualized gradient descent down all loss functions.</figcaption></figure><p id="7e9f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here we can see the interesting contours of the non-L2 loss functions. While the L2 loss function is smooth and exhibits large values up to 100, the other loss functions have much smaller values as they reflect only the absolute errors. But the L2 loss’s steeper gradient means the optimizer makes a quicker approach to the global minimum, as evidenced by the greater spacing between its early points. Meanwhile the L1 losses all display much more gradual approaches to their minima.</p><h2 id="dd7b" class="pq nq fq bf nz pr ps pt oc pu pv pw of ms px py pz mw qa qb qc na qd qe qf qg bk">Momentum</h2><p id="2140" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">The next most interesting parameter is the momentum, which dictates how much of the last step’s gradient to add in to the current gradient update going froward. Normally very small values of momentum are sufficient, but for the sake of visualization we’re going to set it to the crazy value of 0.9—kids, do NOT try this at home:</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="835f" class="np nq fq nm b bg nr ns l nt nu">multi_plot(lr=0.1, epochs=100, momentum=0.9)</span></pre><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div role="button" tabindex="0" class="pe pf ed pg bh ph"><div class="oy oz qm"><img src="../Images/3f5d67a6c7da5fe4e6be615b5e5eba6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dLui2AFzt7D0pafuk3UukA.png"/></div></div><figcaption class="pi pj pk oy oz pl pm bf b bg z dx">Figure 6. Visualized gradient descent down all loss functions with high momentum.</figcaption></figure><p id="a90d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Thanks to the outrageous momentum value, we can clearly see its effect on the optimizer: it overshoots the global minimum and has to swerve sloppily back around. This effect is most pronounced in the L2 loss, whose steep gradients carry it clean over the minimum and bring it very close to diverging.</p><h2 id="669d" class="pq nq fq bf nz pr ps pt oc pu pv pw of ms px py pz mw qa qb qc na qd qe qf qg bk">Nesterov Momentum</h2><p id="91cb" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Nesterov momentum is an interesting tweak on momentum. Normal momentum adds in some of the gradient from the last step to the gradient for the current step, giving us the scenario in figure 7(a) below. But if we already know where the gradient from the last step is going to carry us, then Nesterov momentum instead calculates the current gradient by looking ahead to where that will be, giving us the scenario in figure 7(b) below:</p><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div role="button" tabindex="0" class="pe pf ed pg bh ph"><div class="oy oz qn"><img src="../Images/1fef60334367760ff9aedcab24d46ab5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1M6hPwsTt3tn7cAlQfEDDA.png"/></div></div><figcaption class="pi pj pk oy oz pl pm bf b bg z dx">Figure 7. (a) Momentum vs. (b) Nesterov momentum.</figcaption></figure><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="a768" class="np nq fq nm b bg nr ns l nt nu">multi_plot(lr=0.1, epochs=100, momentum=0.9, nesterov=True)</span></pre><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div role="button" tabindex="0" class="pe pf ed pg bh ph"><div class="oy oz qo"><img src="../Images/af2be22828b89deacf074725c7de16cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EfsWULAh46qk79wOAFiO6w.png"/></div></div><figcaption class="pi pj pk oy oz pl pm bf b bg z dx">Figure 8. Visualized gradient descent down all loss functions with high Nesterov momentum.</figcaption></figure><p id="21b0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When viewed graphically, we can see that Nesterov momentum has cut down the overshooting we observed with plain momentum. Especially in the L2 case, since our momentum carried us clear over the global minimum, using Nesterov to lookahead where we were going to land allowed us to mix in countervailing gradients from the opposite side of the objective function, in effect course-correcting earlier.</p><h2 id="b2bc" class="pq nq fq bf nz pr ps pt oc pu pv pw of ms px py pz mw qa qb qc na qd qe qf qg bk">Weight Decay</h2><p id="63db" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Next weight decay adds a regularizing L2 penalty on the values of the parameters (the weight and bias of our linear network):</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="a298" class="np nq fq nm b bg nr ns l nt nu">multi_plot(lr=0.1, epochs=100, momentum=0.9, nesterov=True, weight_decay=2.0)</span></pre><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div role="button" tabindex="0" class="pe pf ed pg bh ph"><div class="oy oz qp"><img src="../Images/0ac8ddd706e0e1814dd328b503652295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hhvl2A6vfYW6ay1h9_QgeQ.png"/></div></div><figcaption class="pi pj pk oy oz pl pm bf b bg z dx">Figure 9. Visualized gradient descent down all loss functions with high Nesterov momentum and weight decay.</figcaption></figure><p id="8038" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In all cases, the regularizing factor has pulled the solutions away from their rightful global minima and closer to the origin (0, 0). The effect is least pronounced with the L2 loss, however, since the loss values are large enough to offset the L2 penalties on the weights.</p><h2 id="27ec" class="pq nq fq bf nz pr ps pt oc pu pv pw of ms px py pz mw qa qb qc na qd qe qf qg bk">Dampening</h2><p id="2e8a" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Finally we have dampening, which discounts the momentum by the dampening factor. Using a dampening factor of 0.8 we see how it effectively moderates the momentum path through the loss function.</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="69da" class="np nq fq nm b bg nr ns l nt nu">multi_plot(lr=0.1, epochs=100, momentum=0.9, dampening=0.8)</span></pre><figure class="ng nh ni nj nk pb oy oz paragraph-image"><div role="button" tabindex="0" class="pe pf ed pg bh ph"><div class="oy oz qq"><img src="../Images/d81b751b1cbc2d155e7eec6e8757802c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LG9A86iZGgEsPAGALXJ2kQ.png"/></div></div><figcaption class="pi pj pk oy oz pl pm bf b bg z dx">Figure 10. Visualized gradient descent down all loss functions with high momentum and high dampening.</figcaption></figure><p id="056a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Unless otherwise noted, all images are by the author.</p><h1 id="4509" class="ny nq fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">References</h1><ul class=""><li id="17e8" class="mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne qr qs qt bk"><a class="af nf" href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html</a></li><li id="7c7f" class="mj mk fq ml b go qu mn mo gr qv mq mr ms qw mu mv mw qx my mz na qy nc nd ne qr qs qt bk"><a class="af nf" href="https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html</a></li><li id="433e" class="mj mk fq ml b go qu mn mo gr qv mq mr ms qw mu mv mw qx my mz na qy nc nd ne qr qs qt bk"><a class="af nf" href="https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html</a></li><li id="1170" class="mj mk fq ml b go qu mn mo gr qv mq mr ms qw mu mv mw qx my mz na qy nc nd ne qr qs qt bk"><a class="af nf" href="https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html</a></li><li id="93ca" class="mj mk fq ml b go qu mn mo gr qv mq mr ms qw mu mv mw qx my mz na qy nc nd ne qr qs qt bk"><a class="af nf" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/generated/torch.optim.SGD.html</a></li></ul><h2 id="58e1" class="pq nq fq bf nz pr ps pt oc pu pv pw of ms px py pz mw qa qb qc na qd qe qf qg bk">See Also</h2><ul class=""><li id="8224" class="mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne qr qs qt bk"><a class="af nf" rel="noopener" target="_blank" href="/extending-context-length-in-large-language-models-74e59201b51f">https://towardsdatascience.com/extending-context-length-in-large-language-models-74e59201b51f</a></li><li id="e641" class="mj mk fq ml b go qu mn mo gr qv mq mr ms qw mu mv mw qx my mz na qy nc nd ne qr qs qt bk">Code available at: <a class="af nf" href="https://github.com/pbaumstarck/scaling-invention/blob/main/code/torch_loss.py" rel="noopener ugc nofollow" target="_blank">https://github.com/pbaumstarck/scaling-invention/blob/main/code/torch_loss.py</a></li><li id="1122" class="mj mk fq ml b go qu mn mo gr qv mq mr ms qw mu mv mw qx my mz na qy nc nd ne qr qs qt bk"><a class="af nf" href="https://github.com/tomgoldstein/loss-landscape" rel="noopener ugc nofollow" target="_blank">https://github.com/tomgoldstein/loss-landscape</a></li><li id="83ea" class="mj mk fq ml b go qu mn mo gr qv mq mr ms qw mu mv mw qx my mz na qy nc nd ne qr qs qt bk"><a class="af nf" href="https://neptune.ai/blog/pytorch-loss-functions" rel="noopener ugc nofollow" target="_blank">https://neptune.ai/blog/pytorch-loss-functions</a></li></ul></div></div></div></div>    
</body>
</html>