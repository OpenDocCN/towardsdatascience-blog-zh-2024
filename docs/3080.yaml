- en: 'How Neural Networks Learn: A Probabilistic Viewpoint'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-neural-networks-learn-a-probabilistic-viewpoint-0f6a78dc58e2?source=collection_archive---------1-----------------------#2024-12-26](https://towardsdatascience.com/how-neural-networks-learn-a-probabilistic-viewpoint-0f6a78dc58e2?source=collection_archive---------1-----------------------#2024-12-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding loss functions for training neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bilalhsp?source=post_page---byline--0f6a78dc58e2--------------------------------)[![Bilal
    Ahmed](../Images/840f8aedf34c06a410f35290443769e2.png)](https://medium.com/@bilalhsp?source=post_page---byline--0f6a78dc58e2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0f6a78dc58e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0f6a78dc58e2--------------------------------)
    [Bilal Ahmed](https://medium.com/@bilalhsp?source=post_page---byline--0f6a78dc58e2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0f6a78dc58e2--------------------------------)
    ·8 min read·Dec 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is very hands-on, and everyone charts their own path. There
    isn’t a standard set of courses to follow, as was traditionally the case. There’s
    no ‘Machine Learning 101,’ so to speak. However, this sometimes leaves gaps in
    understanding. If you’re like me, these gaps can feel uncomfortable. For instance,
    I used to be bothered by things we do casually, like the choice of a loss function.
    I admit that some practices are learned through heuristics and experience, but
    most concepts are rooted in solid mathematical foundations. Of course, not everyone
    has the time or motivation to dive deeply into those foundations — unless you’re
    a researcher.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have attempted to present some basic ideas on how to approach a machine learning
    problem. Understanding this background will help practitioners feel more confident
    in their design choices. The concepts I covered include:'
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying the difference in probability distributions using cross-entropy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A probabilistic view of neural network models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deriving and understanding the loss functions for different applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In information theory, entropy is a measure of the uncertainty associated with
    the values of a random variable. In other words, it is used to quantify the spread
    of distribution. The narrower the distribution the lower the entropy and vice
    versa. Mathematically, entropy of distribution ***p(x)*** is defined as;
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33dd85ca764eeece3a21440ee7905d46.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is common to use log with the base 2 and in that case entropy is measured
    in bits. The figure below compares two distributions: the blue one with high entropy
    and the orange one with low entropy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/964baafb5e977e35c85107b5f4900b32.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualization examples of distributions having high and low entropy — created
    by the author using Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also measure entropy between two distributions. For example, consider
    the case where we have observed some data having the distribution ***p(x)*** and
    a distribution ***q(x)*** that could potentially serve as a model for the observed
    data. In that case we can compute cross-entropy ***Hpq​(X)*** between data distribution
    ***p(x)*** and the model distribution ***q(x)***. Mathematically cross-entropy
    is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4fa58dee8edcc1c39b14f48a872a761a.png)'
  prefs: []
  type: TYPE_IMG
- en: Using cross entropy we can compare different models and the one with lowest
    cross entropy is better fit to the data. This is depicted in the contrived example
    in the following figure. We have two candidate models and we want to decide which
    one is better model for the observed data. As we can see the model whose distribution
    exactly matches that of the data has lower cross entropy than the model that is
    slightly off.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0969895a26721b15b064252d92294c46.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of cross entropy of data distribution p(x) with two candidate models.
    (a) candidate model exactly matches data distribution and has low cross entropy.
    (b) candidate model does not match the data distribution hence it has high cross
    entropy — created by the author using Python.
  prefs: []
  type: TYPE_NORMAL
- en: There is another way to state the same thing. As the model distribution deviates
    from the data distribution cross entropy increases. While trying to fit a model
    to the data i.e. training a machine learning model, we are interested in minimizing
    this deviation. This increase in cross entropy due to deviation from the data
    distribution is defined as relative entropy commonly known as **Kullback-Leibler
    Divergence** of simply **KL-Divergence.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b6f13e6fb4bf7a5a1edcc559307df54.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, we can quantify the divergence between two probability distributions
    using cross-entropy or KL-Divergence. To train a model we can adjust the parameters
    of the model such that they minimize the cross-entropy or KL-Divergence. Note
    that minimizing cross-entropy or KL-Divergence achieves the same solution. KL-Divergence
    has a better interpretation as its minimum is zero, that will be the case when
    the model exactly matches the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important consideration is how do we pick the model distribution? This
    is dictated by two things: the problem we are trying to solve and our preferred
    approach to solving the problem. Let’s take the example of a classification problem
    where we have ***(X, Y)*** pairs of data, with ***X*** representing the input
    features and ***Y*** representing the true class labels. We want to train a model
    to correctly classify the inputs. There are two ways we can approach this problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Discriminative vs Generative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The generative approach refers to modeling the joint distribution ***p(X,Y)***
    such that it learns the data-generating process, hence the name ‘generative’.
    In the example under discussion, the model learns the prior distribution of class
    labels ***p(Y)*** and for given class label ***Y***, it learns to generate features
    ***X*** using ***p(X|Y)***.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5eb90240b9184995d34273875592b62f.png)'
  prefs: []
  type: TYPE_IMG
- en: It should be clear that the learned model is capable of generating new data
    ***(X,Y)***. However, what might be less obvious is that it can also be used to
    classify the given features ***X*** using Bayes’ Rule, though this may not always
    be feasible depending on the model’s complexity. Suffice it to say that using
    this for a task like classification might not be a good idea, so we should instead
    take the direct approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8c29ddac205624e27902f56d5dcc526.png)'
  prefs: []
  type: TYPE_IMG
- en: Discriminative vs generative approach of modelling — created by the author using
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Discriminative approach refers to modelling the relationship between input
    features ***X*** and output labels ***Y*** directly i.e. modelling the conditional
    distribution ***p(Y|X)***. The model thus learnt need not capture the details
    of features ***X*** but only the class discriminatory aspects of it. As we saw
    earlier, it is possible to learn the parameters of the model by minimizing the
    cross-entropy between observed data and model distribution. The cross-entropy
    for a discriminative model can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16acd7e725501a5072a2743d1d0c700d.png)'
  prefs: []
  type: TYPE_IMG
- en: Where the right most sum is the sample average and it approximates the expectation
    w.r.t data distribution. Since our learning rule is to minimize the cross-entropy,
    we can call it our general loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a033e75b7e65062de1656ecb8e087c7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Goal of learning (training the model) is to minimize this loss function. Mathematically,
    we can write the same statement as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3beee4f01aff22571a56f8d67802dea.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s now consider specific examples of discriminative models and apply the
    general loss function to each example.
  prefs: []
  type: TYPE_NORMAL
- en: Binary Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the name suggests, the class label ***Y*** for this kind of problem is either
    ***0*** or ***1***. That could be the case for a face detector, or a cat vs dog
    classifier or a model that predicts the presence or absence of a disease. How
    do we model a binary random variable? That’s right — it’s a Bernoulli random variable.
    The probability distribution for a Bernoulli variable can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea45a2b2ec777baffd917d5767c3efd4.png)'
  prefs: []
  type: TYPE_IMG
- en: where ***π*** is the probability of getting ***1*** i.e. ***p(Y=1) = π***.
  prefs: []
  type: TYPE_NORMAL
- en: Since we want to model ***p(Y|X)***, let’s make ***π*** a function of ***X***
    i.e. output of our model ***π(X)*** depends on input features ***X***. In other
    words, our model takes in features ***X*** and predicts the probability of ***Y***=1\.
    Please note that in order to get a valid probability at the output of the model,
    it has to be constrained to be a number between ***0*** and ***1***. This is achieved
    by applying a sigmoid non-linearity at the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f67b498697edf028bc2c24b22c827b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To simplify, let’s rewrite this explicitly in terms of true label and predicted
    label as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa4841469e25d666a92da564ccebeb56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can write the general loss function for this specific conditional distribution
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5216ae575375531b227f467ceb2049be.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the commonly referred to as binary cross entropy (BCE) loss.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For a multi-class problem, the goal is to predict a category from ***C*** classes
    for each input feature ***X***.In this case we can model the output ***Y*** as
    a categorical random variable, a random variable that takes on a state c out of
    all possible ***C*** states. As an example of categorical random variable, think
    of a six-faced die that can take on one of six possible states with each roll.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37005c1eb91a4b7bd720e337b07a66d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see the above expression as easy extension of the case of binary random
    variable to a random variable having multiple categories. We can model the conditional
    distribution ***p(Y|X)*** by making ***λ***’s as function of input features ***X***.
    Based on this, let’s we write the conditional categorical distribution of ***Y***
    in terms of predicted probabilities as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e29155a0355083486acf8a0c0e05db1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using this conditional model distribution we can write the loss function using
    the general loss function derived earlier in terms of cross-entropy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ad3ae465bf243324e79a6d733c009de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is referred to as Cross-Entropy loss in PyTorch. The thing to note here
    is that I have written this in terms of predicted probability of each class. In
    order to have a valid probability distribution over all ***C*** classes, a softmax
    non-linearity is applied at the output of the model. Softmax function is written
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ac6a4e9220c2e17d64eb467e6b72096.png)'
  prefs: []
  type: TYPE_IMG
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider the case of data ***(X, Y)*** where ***X*** represents the input features
    and ***Y*** represents output that can take on any real number value. Since ***Y***
    is real valued, we can model the its distribution using a Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01010a2acfb87ce81341cc905424a55d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Again, since we are interested in modelling the conditional distribution ***p(Y|X).***
    We can capture the dependence on ***X*** by making the conditional mean of ***Y***
    a function of ***X***. For simplicity, we set variance equal to 1\. The conditional
    distribution can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d630333a23272a40e4824a00d0c5841.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now write our general loss function for this conditional model distribution
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/003a6bd679d934b65064b8b13cefe42d.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the famous MSE loss for training the regression model. Note that the
    constant factor is irrelevant here as we are only interest in finding the location
    of minima and can be dropped.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this short article, I introduced the concepts of entropy, cross-entropy,
    and KL-Divergence. These concepts are essential for computing similarities (or
    divergences) between distributions. By using these ideas, along with a probabilistic
    interpretation of the model, we can define the general loss function, also referred
    to as the objective function. Training the model, or ‘learning,’ then boils down
    to minimizing the loss with respect to the model’s parameters. This optimization
    is typically carried out using gradient descent, which is mostly handled by deep
    learning frameworks like PyTorch. Hope this helps — happy learning!
  prefs: []
  type: TYPE_NORMAL
