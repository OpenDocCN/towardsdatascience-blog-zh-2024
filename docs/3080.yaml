- en: 'How Neural Networks Learn: A Probabilistic Viewpoint'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络如何学习：一种概率视角
- en: 原文：[https://towardsdatascience.com/how-neural-networks-learn-a-probabilistic-viewpoint-0f6a78dc58e2?source=collection_archive---------1-----------------------#2024-12-26](https://towardsdatascience.com/how-neural-networks-learn-a-probabilistic-viewpoint-0f6a78dc58e2?source=collection_archive---------1-----------------------#2024-12-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-neural-networks-learn-a-probabilistic-viewpoint-0f6a78dc58e2?source=collection_archive---------1-----------------------#2024-12-26](https://towardsdatascience.com/how-neural-networks-learn-a-probabilistic-viewpoint-0f6a78dc58e2?source=collection_archive---------1-----------------------#2024-12-26)
- en: Understanding loss functions for training neural networks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解训练神经网络的损失函数
- en: '[](https://medium.com/@bilalhsp?source=post_page---byline--0f6a78dc58e2--------------------------------)[![Bilal
    Ahmed](../Images/840f8aedf34c06a410f35290443769e2.png)](https://medium.com/@bilalhsp?source=post_page---byline--0f6a78dc58e2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0f6a78dc58e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0f6a78dc58e2--------------------------------)
    [Bilal Ahmed](https://medium.com/@bilalhsp?source=post_page---byline--0f6a78dc58e2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bilalhsp?source=post_page---byline--0f6a78dc58e2--------------------------------)[![Bilal
    Ahmed](../Images/840f8aedf34c06a410f35290443769e2.png)](https://medium.com/@bilalhsp?source=post_page---byline--0f6a78dc58e2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0f6a78dc58e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0f6a78dc58e2--------------------------------)
    [Bilal Ahmed](https://medium.com/@bilalhsp?source=post_page---byline--0f6a78dc58e2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0f6a78dc58e2--------------------------------)
    ·8 min read·Dec 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0f6a78dc58e2--------------------------------)
    ·阅读时间 8 分钟·2024年12月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Machine learning is very hands-on, and everyone charts their own path. There
    isn’t a standard set of courses to follow, as was traditionally the case. There’s
    no ‘Machine Learning 101,’ so to speak. However, this sometimes leaves gaps in
    understanding. If you’re like me, these gaps can feel uncomfortable. For instance,
    I used to be bothered by things we do casually, like the choice of a loss function.
    I admit that some practices are learned through heuristics and experience, but
    most concepts are rooted in solid mathematical foundations. Of course, not everyone
    has the time or motivation to dive deeply into those foundations — unless you’re
    a researcher.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是非常实践性的，每个人都在探索自己的路径。它不像传统课程那样有一套标准的学习路径，也没有所谓的“机器学习 101”课程。然而，这也常常会导致理解上的空白。如果你像我一样，这些空白会让人感觉不舒服。例如，我曾经对我们随便做的一些事情感到困惑，比如选择损失函数。我承认，有些做法是通过启发式和经验学到的，但大多数概念都有坚实的数学基础。当然，并不是每个人都有时间或动机深入研究这些基础知识——除非你是一个研究人员。
- en: 'I have attempted to present some basic ideas on how to approach a machine learning
    problem. Understanding this background will help practitioners feel more confident
    in their design choices. The concepts I covered include:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我试图呈现一些关于如何解决机器学习问题的基本思路。理解这些背景知识将帮助实践者在设计选择上更加自信。我所覆盖的概念包括：
- en: Quantifying the difference in probability distributions using cross-entropy.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉熵量化概率分布之间的差异。
- en: A probabilistic view of neural network models.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络模型的概率视角。
- en: Deriving and understanding the loss functions for different applications.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推导并理解不同应用的损失函数。
- en: Entropy
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵
- en: In information theory, entropy is a measure of the uncertainty associated with
    the values of a random variable. In other words, it is used to quantify the spread
    of distribution. The narrower the distribution the lower the entropy and vice
    versa. Mathematically, entropy of distribution ***p(x)*** is defined as;
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息理论中，熵是衡量随机变量取值的不确定性。换句话说，它用于量化分布的扩散程度。分布越狭窄，熵越低，反之亦然。从数学角度来看，分布 ***p(x)***
    的熵定义为：
- en: '![](../Images/33dd85ca764eeece3a21440ee7905d46.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33dd85ca764eeece3a21440ee7905d46.png)'
- en: 'It is common to use log with the base 2 and in that case entropy is measured
    in bits. The figure below compares two distributions: the blue one with high entropy
    and the orange one with low entropy.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用以 2 为底的对数，在这种情况下，熵以比特为单位进行度量。下图比较了两种分布：蓝色的分布具有较高的熵，而橙色的分布具有较低的熵。
- en: '![](../Images/964baafb5e977e35c85107b5f4900b32.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/964baafb5e977e35c85107b5f4900b32.png)'
- en: Visualization examples of distributions having high and low entropy — created
    by the author using Python.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 分布具有高熵和低熵的可视化示例 — 该图由作者使用Python创建。
- en: 'We can also measure entropy between two distributions. For example, consider
    the case where we have observed some data having the distribution ***p(x)*** and
    a distribution ***q(x)*** that could potentially serve as a model for the observed
    data. In that case we can compute cross-entropy ***Hpq​(X)*** between data distribution
    ***p(x)*** and the model distribution ***q(x)***. Mathematically cross-entropy
    is written as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以测量两个分布之间的熵。例如，假设我们观察到某些数据具有分布***p(x)***，并且有一个分布***q(x)***，它可能作为观察数据的模型。在这种情况下，我们可以计算数据分布***p(x)***与模型分布***q(x)***之间的交叉熵***Hpq​(X)***。从数学上讲，交叉熵可以表示为：
- en: '![](../Images/4fa58dee8edcc1c39b14f48a872a761a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fa58dee8edcc1c39b14f48a872a761a.png)'
- en: Using cross entropy we can compare different models and the one with lowest
    cross entropy is better fit to the data. This is depicted in the contrived example
    in the following figure. We have two candidate models and we want to decide which
    one is better model for the observed data. As we can see the model whose distribution
    exactly matches that of the data has lower cross entropy than the model that is
    slightly off.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用交叉熵，我们可以比较不同的模型，交叉熵最小的模型更适合数据。以下图所示，我们有两个候选模型，想要决定哪个模型更适合观察到的数据。正如我们所看到的，与数据分布完全匹配的模型的交叉熵低于略有偏差的模型。
- en: '![](../Images/0969895a26721b15b064252d92294c46.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0969895a26721b15b064252d92294c46.png)'
- en: Comparison of cross entropy of data distribution p(x) with two candidate models.
    (a) candidate model exactly matches data distribution and has low cross entropy.
    (b) candidate model does not match the data distribution hence it has high cross
    entropy — created by the author using Python.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 比较数据分布p(x)与两个候选模型的交叉熵。（a）候选模型完全匹配数据分布，交叉熵较低。（b）候选模型与数据分布不匹配，因此交叉熵较高 — 该图由作者使用Python创建。
- en: There is another way to state the same thing. As the model distribution deviates
    from the data distribution cross entropy increases. While trying to fit a model
    to the data i.e. training a machine learning model, we are interested in minimizing
    this deviation. This increase in cross entropy due to deviation from the data
    distribution is defined as relative entropy commonly known as **Kullback-Leibler
    Divergence** of simply **KL-Divergence.**
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种表述相同内容的方式是：当模型分布偏离数据分布时，交叉熵增加。在尝试将模型拟合到数据时，即训练一个机器学习模型时，我们的目标是最小化这种偏差。由于偏离数据分布而导致的交叉熵增加被定义为相对熵，通常被称为**KL散度**，或简称为**KL-Divergence**。
- en: '![](../Images/2b6f13e6fb4bf7a5a1edcc559307df54.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b6f13e6fb4bf7a5a1edcc559307df54.png)'
- en: Hence, we can quantify the divergence between two probability distributions
    using cross-entropy or KL-Divergence. To train a model we can adjust the parameters
    of the model such that they minimize the cross-entropy or KL-Divergence. Note
    that minimizing cross-entropy or KL-Divergence achieves the same solution. KL-Divergence
    has a better interpretation as its minimum is zero, that will be the case when
    the model exactly matches the data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用交叉熵或KL散度来量化两个概率分布之间的差异。为了训练一个模型，我们可以调整模型的参数，使其最小化交叉熵或KL散度。请注意，最小化交叉熵或KL散度会得到相同的解决方案。KL散度有一个更好的解释，因为其最小值为零，这种情况发生在模型与数据完全匹配时。
- en: 'Another important consideration is how do we pick the model distribution? This
    is dictated by two things: the problem we are trying to solve and our preferred
    approach to solving the problem. Let’s take the example of a classification problem
    where we have ***(X, Y)*** pairs of data, with ***X*** representing the input
    features and ***Y*** representing the true class labels. We want to train a model
    to correctly classify the inputs. There are two ways we can approach this problem.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的考虑因素是我们如何选择模型分布？这一点由两个因素决定：我们要解决的问题和我们偏好的解决问题的方式。假设我们有一个分类问题，其中有***(X,
    Y)***数据对，***X***表示输入特征，***Y***表示真实的类别标签。我们的目标是训练一个模型来正确分类输入数据。我们可以通过两种方式来解决这个问题。
- en: Discriminative vs Generative
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别模型与生成模型
- en: The generative approach refers to modeling the joint distribution ***p(X,Y)***
    such that it learns the data-generating process, hence the name ‘generative’.
    In the example under discussion, the model learns the prior distribution of class
    labels ***p(Y)*** and for given class label ***Y***, it learns to generate features
    ***X*** using ***p(X|Y)***.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 生成方法是指建模联合分布***p(X,Y)***，使其学习数据生成过程，因此得名“生成式”。在当前讨论的示例中，模型学习类别标签的先验分布***p(Y)***，并且在给定类别标签***Y***的情况下，学习使用***p(X|Y)***生成特征***X***。
- en: '![](../Images/5eb90240b9184995d34273875592b62f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5eb90240b9184995d34273875592b62f.png)'
- en: It should be clear that the learned model is capable of generating new data
    ***(X,Y)***. However, what might be less obvious is that it can also be used to
    classify the given features ***X*** using Bayes’ Rule, though this may not always
    be feasible depending on the model’s complexity. Suffice it to say that using
    this for a task like classification might not be a good idea, so we should instead
    take the direct approach.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 应该清楚的是，学习到的模型能够生成新的数据***(X,Y)***。然而，可能不太明显的是，它也可以使用贝叶斯定理对给定的特征***X***进行分类，尽管根据模型的复杂性，这可能并不总是可行。可以说，使用这种方法进行分类任务可能不是一个好主意，因此我们应该采取直接的方法。
- en: '![](../Images/a8c29ddac205624e27902f56d5dcc526.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8c29ddac205624e27902f56d5dcc526.png)'
- en: Discriminative vs generative approach of modelling — created by the author using
    Python.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 判别式与生成式建模方法 — 作者使用Python创建。
- en: 'Discriminative approach refers to modelling the relationship between input
    features ***X*** and output labels ***Y*** directly i.e. modelling the conditional
    distribution ***p(Y|X)***. The model thus learnt need not capture the details
    of features ***X*** but only the class discriminatory aspects of it. As we saw
    earlier, it is possible to learn the parameters of the model by minimizing the
    cross-entropy between observed data and model distribution. The cross-entropy
    for a discriminative model can be written as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 判别方法是指直接建模输入特征***X***和输出标签***Y***之间的关系，即建模条件分布***p(Y|X)***。因此，学习到的模型不必捕捉特征***X***的所有细节，而只需关注其类别区分特征。正如我们之前所看到的，可以通过最小化观测数据和模型分布之间的交叉熵来学习模型的参数。判别模型的交叉熵可以写成如下形式：
- en: '![](../Images/16acd7e725501a5072a2743d1d0c700d.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16acd7e725501a5072a2743d1d0c700d.png)'
- en: Where the right most sum is the sample average and it approximates the expectation
    w.r.t data distribution. Since our learning rule is to minimize the cross-entropy,
    we can call it our general loss function.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最右侧的求和是样本平均值，它近似于数据分布的期望。由于我们的学习规则是最小化交叉熵，我们可以将其称为我们的通用损失函数。
- en: '![](../Images/a033e75b7e65062de1656ecb8e087c7d.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a033e75b7e65062de1656ecb8e087c7d.png)'
- en: 'Goal of learning (training the model) is to minimize this loss function. Mathematically,
    we can write the same statement as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的目标（训练模型）是最小化这个损失函数。从数学上讲，我们可以将同样的陈述写成如下形式：
- en: '![](../Images/e3beee4f01aff22571a56f8d67802dea.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3beee4f01aff22571a56f8d67802dea.png)'
- en: Let’s now consider specific examples of discriminative models and apply the
    general loss function to each example.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一些具体的判别模型示例，并将通用损失函数应用到每个示例中。
- en: Binary Classification
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二分类
- en: 'As the name suggests, the class label ***Y*** for this kind of problem is either
    ***0*** or ***1***. That could be the case for a face detector, or a cat vs dog
    classifier or a model that predicts the presence or absence of a disease. How
    do we model a binary random variable? That’s right — it’s a Bernoulli random variable.
    The probability distribution for a Bernoulli variable can be written as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名称所示，这种问题的类别标签***Y***要么是***0***，要么是***1***。这可能适用于人脸检测器、猫狗分类器或预测疾病是否存在的模型。我们如何建模二元随机变量？没错——它是一个伯努利随机变量。伯努利变量的概率分布可以写成如下形式：
- en: '![](../Images/ea45a2b2ec777baffd917d5767c3efd4.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea45a2b2ec777baffd917d5767c3efd4.png)'
- en: where ***π*** is the probability of getting ***1*** i.e. ***p(Y=1) = π***.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中***π***是获取***1***的概率，即***p(Y=1) = π***。
- en: Since we want to model ***p(Y|X)***, let’s make ***π*** a function of ***X***
    i.e. output of our model ***π(X)*** depends on input features ***X***. In other
    words, our model takes in features ***X*** and predicts the probability of ***Y***=1\.
    Please note that in order to get a valid probability at the output of the model,
    it has to be constrained to be a number between ***0*** and ***1***. This is achieved
    by applying a sigmoid non-linearity at the output.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想要建模***p(Y|X)***，让我们将***π***表示为***X***的函数，即模型的输出***π(X)***依赖于输入特征***X***。换句话说，我们的模型接受特征***X***并预测***Y***=1的概率。请注意，为了在模型输出端获得有效的概率，它必须被限制在***0***和***1***之间。这是通过在输出端应用sigmoid非线性函数来实现的。
- en: '![](../Images/2f67b498697edf028bc2c24b22c827b5.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f67b498697edf028bc2c24b22c827b5.png)'
- en: 'To simplify, let’s rewrite this explicitly in terms of true label and predicted
    label as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们将其明确重写为真实标签和预测标签的形式，如下所示：
- en: '![](../Images/fa4841469e25d666a92da564ccebeb56.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa4841469e25d666a92da564ccebeb56.png)'
- en: 'We can write the general loss function for this specific conditional distribution
    as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此特定条件分布的一般损失函数写成如下形式：
- en: '![](../Images/5216ae575375531b227f467ceb2049be.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5216ae575375531b227f467ceb2049be.png)'
- en: This is the commonly referred to as binary cross entropy (BCE) loss.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被称为二元交叉熵（BCE）损失。
- en: Multi-class Classification
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类分类
- en: For a multi-class problem, the goal is to predict a category from ***C*** classes
    for each input feature ***X***.In this case we can model the output ***Y*** as
    a categorical random variable, a random variable that takes on a state c out of
    all possible ***C*** states. As an example of categorical random variable, think
    of a six-faced die that can take on one of six possible states with each roll.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个多类问题，目标是为每个输入特征***X***从***C***个类别中预测一个类别。在这种情况下，我们可以将输出***Y***建模为一个类别随机变量，一个从所有可能的***C***个状态中选择一个状态的随机变量。作为类别随机变量的例子，可以想象一个六面骰子，它可以在每次掷骰时取六个可能状态之一。
- en: '![](../Images/37005c1eb91a4b7bd720e337b07a66d2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37005c1eb91a4b7bd720e337b07a66d2.png)'
- en: 'We can see the above expression as easy extension of the case of binary random
    variable to a random variable having multiple categories. We can model the conditional
    distribution ***p(Y|X)*** by making ***λ***’s as function of input features ***X***.
    Based on this, let’s we write the conditional categorical distribution of ***Y***
    in terms of predicted probabilities as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将上述表达式看作是将二元随机变量的情况扩展到具有多个类别的随机变量。我们可以通过将***λ***表示为输入特征***X***的函数来建模条件分布***p(Y|X)***。基于此，让我们将条件类别分布***Y***用预测概率的形式表示如下：
- en: '![](../Images/6e29155a0355083486acf8a0c0e05db1.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e29155a0355083486acf8a0c0e05db1.png)'
- en: 'Using this conditional model distribution we can write the loss function using
    the general loss function derived earlier in terms of cross-entropy as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个条件模型分布，我们可以利用之前推导出的交叉熵的通用损失函数写出损失函数，如下所示：
- en: '![](../Images/6ad3ae465bf243324e79a6d733c009de.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ad3ae465bf243324e79a6d733c009de.png)'
- en: 'This is referred to as Cross-Entropy loss in PyTorch. The thing to note here
    is that I have written this in terms of predicted probability of each class. In
    order to have a valid probability distribution over all ***C*** classes, a softmax
    non-linearity is applied at the output of the model. Softmax function is written
    as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这在PyTorch中被称为交叉熵损失。需要注意的是，我是根据每个类别的预测概率来写这个公式的。为了在所有***C***个类别中得到一个有效的概率分布，我们在模型的输出端应用了softmax非线性函数。softmax函数写作如下：
- en: '![](../Images/4ac6a4e9220c2e17d64eb467e6b72096.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ac6a4e9220c2e17d64eb467e6b72096.png)'
- en: Regression
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归
- en: Consider the case of data ***(X, Y)*** where ***X*** represents the input features
    and ***Y*** represents output that can take on any real number value. Since ***Y***
    is real valued, we can model the its distribution using a Gaussian distribution.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑数据***(X, Y)***的情况，其中***X***表示输入特征，***Y***表示可以取任意实数值的输出。由于***Y***是实值的，我们可以使用高斯分布来建模其分布。
- en: '![](../Images/01010a2acfb87ce81341cc905424a55d.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01010a2acfb87ce81341cc905424a55d.png)'
- en: 'Again, since we are interested in modelling the conditional distribution ***p(Y|X).***
    We can capture the dependence on ***X*** by making the conditional mean of ***Y***
    a function of ***X***. For simplicity, we set variance equal to 1\. The conditional
    distribution can be written as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，由于我们对建模条件分布***p(Y|X)***感兴趣。我们可以通过将***Y***的条件均值表示为***X***的函数来捕捉对***X***的依赖关系。为了简化，我们将方差设为1。条件分布可以写成如下形式：
- en: '![](../Images/0d630333a23272a40e4824a00d0c5841.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d630333a23272a40e4824a00d0c5841.png)'
- en: 'We can now write our general loss function for this conditional model distribution
    as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将这个条件模型分布的通用损失函数写成如下形式：
- en: '![](../Images/003a6bd679d934b65064b8b13cefe42d.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/003a6bd679d934b65064b8b13cefe42d.png)'
- en: This is the famous MSE loss for training the regression model. Note that the
    constant factor is irrelevant here as we are only interest in finding the location
    of minima and can be dropped.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于训练回归模型的著名MSE损失。请注意，这里的常数因子是无关紧要的，因为我们只关注最小值的位置，可以忽略不计。
- en: Summary
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this short article, I introduced the concepts of entropy, cross-entropy,
    and KL-Divergence. These concepts are essential for computing similarities (or
    divergences) between distributions. By using these ideas, along with a probabilistic
    interpretation of the model, we can define the general loss function, also referred
    to as the objective function. Training the model, or ‘learning,’ then boils down
    to minimizing the loss with respect to the model’s parameters. This optimization
    is typically carried out using gradient descent, which is mostly handled by deep
    learning frameworks like PyTorch. Hope this helps — happy learning!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇简短的文章中，我介绍了熵、交叉熵和KL散度的概念。这些概念对于计算分布之间的相似性（或散度）至关重要。通过使用这些思想，以及模型的概率解释，我们可以定义通用损失函数，也称为目标函数。训练模型或“学习”则归结为最小化与模型参数相关的损失。这个优化过程通常通过梯度下降来完成，大多数深度学习框架如PyTorch会处理这个过程。希望这对你有帮助——祝你学习愉快！
