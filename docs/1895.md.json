["```py\n{\n  \"messages\": [\n    {\n      \"role\": \"system\", \n      \"content\": \"<SYSTEM PROMPT>\"\n    }, \n    {\n      \"role\": \"user\", \n      \"content\": \"<INPUT PROMPT>\"\n    }, \n    {\n      \"role\": \"assistant\", \n      \"content\": \"<EXPECTED OUTPUT>\"\n    }\n  ]\n} \n```", "```py\n{\n  \"messages\": [\n    {\n      \"role\": \"system\", \n      \"content\": \"You are an assistant that replies with HXL tags and attributes\"\n    }, \n    {\n      \"role\": \"user\", \n      \"content\": \"What are the HXL tags and attributes for a column with these details? \n                    resource_name='admin1-summaries-earthquake.csv'; \n                    dataset_description='The dataset contains earthquake data for various \n                                         administrative regions in Afghanistan, \n                                         including country name, admin1 name, latitude, \n                                         longitude, aggregation type, indicator name, \n                                         and indicator value. The data includes maximum \n                                         earthquake values recorded in different regions, \n                                         with corresponding latitude and longitude coordinates. \n                                         The dataset provides insights into the seismic \n                                         activity in different administrative areas of \n                                         Afghanistan.'; \n                   column_name:'indicator'; \n                   examples: ['earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake']\"\n      }, \n      {\n        \"role\": \"assistant\", \n        \"content\": \"#indicator+name\"\n      }\n  ]\n}\n```", "```py\ndef fine_tune_model(train_file, model_name=\"gpt-4o-mini\"):\n    \"\"\"\n    Fine-tune an OpenAI model using training data.\n\n    Args:\n        prompt_file (str): The file containing the prompts to use for fine-tuning.\n        model_name (str): The name of the model to fine-tune. Default is \"davinci-002\".\n\n    Returns:\n        str: The ID of the fine-tuned model.\n    \"\"\"\n\n    # Upload file to OpenAI for fine-tuning\n    file = client.files.create(\n        file=open(train_file, \"rb\"),\n        purpose=\"fine-tune\"\n    )\n    file_id = file.id\n    print(f\"Uploaded training file with ID: {file_id}\")\n\n    # Start the fine-tuning job\n    ft = client.fine_tuning.jobs.create(\n        training_file=file_id,\n        model=model_name\n    )\n    ft_id = ft.id\n    print(f\"Fine-tuning job started with ID: {ft_id}\")\n\n    # Monitor the status of the fine-tuning job\n    ft_result = client.fine_tuning.jobs.retrieve(ft_id)\n    while ft_result.status != 'succeeded':\n        print(f\"Current status: {ft_result.status}\")\n        time.sleep(120)  # Wait for 60 seconds before checking again\n        ft_result = client.fine_tuning.jobs.retrieve(ft_id)\n        if 'failed' in ft_result.status.lower():\n            sys.exit()\n\n    print(f\"Fine-tuning job {ft_id} succeeded!\")\n\n    # Retrieve the fine-tuned model\n    fine_tuned_model = ft_result.fine_tuned_model\n    print(f\"Fine-tuned model: {fine_tuned_model}\")\n\n    return fine_tuned_model\n\nmodel = fine_tune_model(\"hxl_chat_prompts_train.jsonl\", model_name=\"gpt-4o-mini-2024-07-18\")\n```", "```py\nUploaded training file with ID: file-XXXXXXXXXXXXXXX\nFine-tuning job started with ID: ftjob-XXXXXXXXXXXXXXX\nCurrent status: validating_files\nCurrent status: validating_files\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nCurrent status: running\nFine-tuning job ftjob-XXXXXXXXXXXXXXX succeeded!\nFine-tuned model: ft:gpt-4o-mini-2024-07-18::XXXXXXX\n```", "```py\ndef make_chat_predictions(prompts, model, temperature=0.1, max_tokens=13):\n    \"\"\"\n    Generate chat predictions based on given prompts using the OpenAI chat model.\n\n    Args:\n        prompts (list): A list of prompts, where each prompt is a dictionary containing a list of messages.\n                        Each message in the list has a 'role' (either 'system', 'user', or 'assistant') and 'content'.\n        model (str): The name or ID of the OpenAI chat model to use for predictions.\n        temperature (float, optional): Controls the randomness of the predictions. Higher values (e.g., 0.5) make the\n                                       output more random, while lower values (e.g., 0.1) make it more deterministic.\n                                       Defaults to 0.1.\n        max_tokens (int, optional): The maximum number of tokens in the predicted response. Defaults to 13.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the results of the chat predictions. Each row in the DataFrame\n                          corresponds to a prompt and includes the prompt messages, the actual message, and the\n                          predicted message.\n\n    \"\"\"\n    results = []\n    for p in prompts:\n        actual = p[\"messages\"][-1][\"content\"]\n        p[\"messages\"] = p[\"messages\"][0:2]\n        completion = client.chat.completions.create(\n            model=model,\n            messages=p[\"messages\"],\n            temperature=temperature,\n            max_tokens=max_tokens\n        )\n        predicted = completion.choices[0].message.content\n        predicted = filter_for_schema(predicted)\n\n        res = {\n            \"prompt\": p[\"messages\"],\n            \"actual\": actual,\n            \"predicted\": predicted\n        }\n\n        print(f\"Predicted: {predicted}; Actual: {actual}\")\n\n        results.append(res)\n\n    results = pd.DataFrame(results)\n\n    return results\n\ndef filter_for_schema(text):\n    \"\"\"\n    Filters the input text to extract approved HXL schema tokens.\n\n    Args:\n        text (str): The input text to be filtered.\n\n    Returns:\n        str: The filtered text containing only approved HXL schema tokens.\n    \"\"\"\n\n    if \" \" in text:\n        text = text.replace(\" \",\"\")\n\n    tokens_raw = text.split(\"+\")\n    tokens = [tokens_raw[0]]\n    for t in tokens_raw[1:]:\n        tokens.append(f\"+{t}\")\n\n    filtered = []\n    for t in tokens:\n        if t in APPROVED_HXL_SCHEMA:\n            if t not in filtered:\n                filtered.append(t)\n    filtered = \"\".join(filtered)\n\n    if len(filtered) > 0 and filtered[0] != '#':\n        filtered = \"\"\n\n    return filtered\n\ndef output_prediction_metrics(results, prediction_field=\"predicted\", actual_field=\"actual\"):\n    \"\"\"\n    Prints out model performance report for HXL tag prediction. Metrics are for\n    just predicting tags, as well as predicting tags and attributes.\n\n    Parameters\n    ----------\n    results : dataframe\n        Dataframe of results\n    prediction_field : str\n        Field name of element with prediction. Handy for comparing raw and post-processed predictions.\n    actual_field: str\n        Field name of the actual result for comparison with prediction\n    \"\"\"\n    y_test = []\n    y_pred = []\n    y_justtag_test = []\n    y_justtag_pred = []\n    for index, r in results.iterrows():\n        if actual_field not in r and predicted_field not in r:\n            print(\"Provided results do not contain expected values.\")\n            sys.exit()\n        y_pred.append(r[prediction_field])\n        y_test.append(r[actual_field])\n        actual_tag = r[actual_field].split(\"+\")[0]\n        predicted_tag = r[prediction_field].split(\"+\")[0]\n        y_justtag_test.append(actual_tag)\n        y_justtag_pred.append(predicted_tag)\n\n    print(f\"LLM results for {prediction_field}, {len(results)} predictions ...\")\n    print(\"\\nJust HXL tags ...\\n\")\n    print(f\"Accuracy: {round(accuracy_score(y_justtag_test, y_justtag_pred),2)}\")\n    print(\n        f\"Precision: {round(precision_score(y_justtag_test, y_justtag_pred, average='weighted', zero_division=0),2)}\"\n    )\n    print(\n        f\"Recall: {round(recall_score(y_justtag_test, y_justtag_pred, average='weighted', zero_division=0),2)}\"\n    )\n    print(\n        f\"F1: {round(f1_score(y_justtag_test, y_justtag_pred, average='weighted', zero_division=0),2)}\"\n    )\n\n    print(f\"\\nTags and attributes with {prediction_field} ...\\n\")\n    print(f\"Accuracy: {round(accuracy_score(y_test, y_pred),2)}\")\n    print(\n        f\"Precision: {round(precision_score(y_test, y_pred, average='weighted', zero_division=0),2)}\"\n    )\n    print(\n        f\"Recall: {round(recall_score(y_test, y_pred, average='weighted', zero_division=0),2)}\"\n    )\n    print(\n        f\"F1: {round(f1_score(y_test, y_pred, average='weighted', zero_division=0),2)}\"\n    )\n\n    return\n\nwith open(TEST_FILE) as f:\n    X_test = [json.loads(line) for line in f]\n\nresults = make_chat_predictions(X_test, model)\n\noutput_prediction_metrics(results)\n\nprint(\"Done\")\n```", "```py\nLLM results for predicted, 458 predictions ...\n\nJust HXL tags ...\n\nAccuracy: 0.83\nPrecision: 0.85\nRecall: 0.83\nF1: 0.82\n\nTags and attributes with predicted ...\n\nAccuracy: 0.61\nPrecision: 0.6\nRecall: 0.61\nF1: 0.57 \n```", "```py\n Just HXL tags ...\n\nAccuracy: 0.88\nPrecision: 0.88\nRecall: 0.88\nF1: 0.88\n\nTags and attributes with predicted ...\n\nAccuracy: 0.66\nPrecision: 0.71\nRecall: 0.66\nF1: 0.66\n```", "```py\ndef generate_hxl_standard_prompt(local_data_file):\n  \"\"\"\n  Generate a standard prompt for predicting Humanitarian Markup Language (HXL) tags and attributes.\n\n  Args:\n    local_data_file (str): The path to the local data file containing core hashtags and attributes.\n\n  Returns:\n    str: The generated HXL standard prompt.\n\n  \"\"\"\n\n  core_hashtags = pd.read_excel(local_data_file, sheet_name='Core hashtags')\n  core_hashtags = core_hashtags.loc[core_hashtags[\"Release status\"] == \"Released\"]\n  core_hashtags = core_hashtags[[\"Hashtag\", \"Hashtag long description\", \"Sample HXL\"]]\n\n  core_attributes = pd.read_excel(local_data_file, sheet_name='Core attributes')\n  core_attributes = core_attributes.loc[core_attributes[\"Status\"] == \"Released\"]\n  core_attributes = core_attributes[[\"Attribute\", \"Attribute long description\", \"Suggested hashtags (selected)\"]]\n\n  print(core_hashtags.shape)\n  print(core_attributes.shape)\n\n  core_hashtags = core_hashtags.to_dict(orient='records')\n  core_attributes = core_attributes.to_dict(orient='records')\n\n  hxl_prompt= f\"\"\"\n  You are an AI assistant that predicts Humanitarian Markup Language (HXL) tags and attributes for columns of data where the HXL standard is defined as follows:\n\n  CORE HASHTAGS:\n\n  {json.dumps(core_hashtags,indent=4)}\n\n  CORE ATTRIBUTES:\n\n  {json.dumps(core_attributes, indent=4)}\n\n  Key points:\n\n  - ALWAYS predict hash tags\n  - NEVER predict a tag which is not a valid core hashtag\n  - NEVER start with a core hashtag, you must always start with a core hashtag\n  - Always try and predict an attribute if possible\n  - Do not use attribute +code if the data examples are human readable names\n\n  You must return your result as a JSON record with the fields 'predicted' and 'reasoning', each is of type string.\n\n  \"\"\"\n\n  print(len(hxl_prompt.split(\" \")))\n  print(hxl_prompt)\n  return hxl_prompt\n```", "```py\nYou are an AI assistant that predicts Humanitarian Markup Language (HXL) tags and attributes for columns of data where the HXL standard is defined as follows:\n\n  CORE HASHTAGS:\n\n  [\n    {\n        \"Hashtag\": \"#access\",\n        \"Hashtag long description\": \"Accessiblity and constraints on access to a market, distribution point, facility, etc.\",\n        \"Sample HXL\": \"#access +type\"\n    },\n    {\n        \"Hashtag\": \"#activity\",\n        \"Hashtag long description\": \"A programme, project, or other activity. This hashtag applies to all levels; use the attributes +activity, +project, or +programme to distinguish different hierarchical levels.\",\n        \"Sample HXL\": \"#activity +project\"\n    },\n    {\n        \"Hashtag\": \"#adm1\",\n        \"Hashtag long description\": \"Top-level subnational administrative area (e.g. a governorate in Syria).\",\n        \"Sample HXL\": \"#adm1 +code\"\n    },\n    {\n        \"Hashtag\": \"#adm2\",\n        \"Hashtag long description\": \"Second-level subnational administrative area (e.g. a subdivision in Bangladesh).\",\n        \"Sample HXL\": \"#adm2 +name\"\n    },\n    {\n        \"Hashtag\": \"#adm3\",\n        \"Hashtag long description\": \"Third-level subnational administrative area (e.g. a subdistrict in Afghanistan).\",\n        \"Sample HXL\": \"#adm3 +code\"\n    },\n    {\n        \"Hashtag\": \"#adm4\",\n        \"Hashtag long description\": \"Fourth-level subnational administrative area (e.g. a barangay in the Philippines).\",\n        \"Sample HXL\": \"#adm4 +name\"\n    },\n    {\n        \"Hashtag\": \"#adm5\",\n        \"Hashtag long description\": \"Fifth-level subnational administrative area (e.g. a ward of a city).\",\n        \"Sample HXL\": \"#adm5 +code\"\n    },\n    {\n        \"Hashtag\": \"#affected\",\n        \"Hashtag long description\": \"Number of people or households affected by an emergency. Subset of #population; superset of #inneed.\",\n        \"Sample HXL\": \"#affected +f +children\"\n    },\n    {\n        \"Hashtag\": \"#beneficiary\",\n        \"Hashtag long description\": \"General (non-numeric) information about a person or group meant to benefit from aid activities, e.g. \\\"lactating women\\\".\",\n        \"Sample HXL\": \"#beneficiary +name\"\n    },\n    {\n        \"Hashtag\": \"#capacity\",\n        \"Hashtag long description\": \"The response capacity of the entity being described (e.g. \\\"25 beds\\\").\",\n        \"Sample HXL\": \"#capacity +num\"\n    },\n\n... Truncated for brevity\n\n    },\n    {\n        \"Hashtag\": \"#targeted\",\n        \"Hashtag long description\": \"Number of people or households targeted for humanitarian assistance. Subset of #inneed; superset of #reached.\",\n        \"Sample HXL\": \"#targeted +f +adult\"\n    },\n    {\n        \"Hashtag\": \"#value\",\n        \"Hashtag long description\": \"A monetary value, such as the price of goods in a market, a project budget, or the amount of cash transferred to beneficiaries. May be used together with #currency in financial or cash data.\",\n        \"Sample HXL\": \"#value +transfer\"\n    }\n]\n\n  CORE ATTRIBUTES:\n\n  [\n    {\n        \"Attribute\": \"+abducted\",\n        \"Attribute long description\": \"Hashtag refers to people who have been abducted.\",\n        \"Suggested hashtags (selected)\": \"#affected, #inneed, #targeted, #reached\"\n    },\n    {\n        \"Attribute\": \"+activity\",\n        \"Attribute long description\": \"The implementers classify this activity as an \\\"activity\\\" proper  (may imply different hierarchical levels in different contexts).\",\n        \"Suggested hashtags (selected)\": \"#activity\"\n    },\n    {\n        \"Attribute\": \"+adolescents\",\n        \"Attribute long description\": \"Adolescents, loosely defined (precise age range varies); may overlap +children and +adult.  You can optionally create custom attributes in addition to this to add precise age ranges, e.g. \\\"+adolescents +age12_17\\\".\",\n        \"Suggested hashtags (selected)\": \"#affected, #inneed, #targeted, #reached, #population\"\n    },\n    {\n        \"Attribute\": \"+adults\",\n        \"Attribute long description\": \"Adults, loosely defined (precise age range varies); may overlap +adolescents and +elderly. You can optionally create custom attributes in addition to this to add precise age ranges, e.g. \\\"+adults +age18_64\\\".\",\n        \"Suggested hashtags (selected)\": \"#affected, #inneed, #targeted, #reached, #population\"\n    },\n    {\n        \"Attribute\": \"+approved\",\n        \"Attribute long description\": \"Date or time when something was approved.\",\n        \"Suggested hashtags (selected)\": \"#date\"\n    },\n    {\n        \"Attribute\": \"+bounds\",\n        \"Attribute long description\": \"Boundary data (e.g. inline GeoJSON).\",\n        \"Suggested hashtags (selected)\": \"#geo\"\n    },\n    {\n        \"Attribute\": \"+budget\",\n        \"Attribute long description\": \"Used with #value to indicate that the amount is planned/approved/budgeted rather than actually spent.\",\n        \"Suggested hashtags (selected)\": \"#value\"\n    },\n    {\n        \"Attribute\": \"+canceled\",\n        \"Attribute long description\": \"Date or time when something (e.g. an #activity) was canceled.\",\n        \"Suggested hashtags (selected)\": \"#date\"\n    },\n    {\n        \"Attribute\": \"+children\",\n        \"Attribute long description\": \"The associated hashtag applies to non-adults, loosely defined (precise age range varies; may overlap +infants and +adolescents). You can optionally create custom attributes in addition to this to add precise age ranges, e.g. \\\"+children +age3_11\\\".\",\n        \"Suggested hashtags (selected)\": \"#affected, #inneed, #targeted, #reached, #population\"\n    },\n    {\n        \"Attribute\": \"+cluster\",\n        \"Attribute long description\": \"Identifies a sector as a formal IASC humanitarian cluster.\",\n        \"Suggested hashtags (selected)\": \"#sector\"\n    },\n    {\n        \"Attribute\": \"+code\",\n        \"Attribute long description\": \"A unique, machine-readable code.\",\n        \"Suggested hashtags (selected)\": \"#region, #country, #adm1, #adm2, #adm3, #adm4, #adm5, #loc, #beneficiary, #activity, #org, #sector, #subsector, #indicator, #output, #crisis, #cause, #impact, #severity, #service, #need, #currency, #item, #need, #service, #channel, #modality, #event, #group, #status\"\n    },\n    {\n        \"Attribute\": \"+converted\",\n        \"Attribute long description\": \"Date or time used for converting a monetary value to another currency.\",\n        \"Suggested hashtags (selected)\": \"#date\"\n    },\n    {\n        \"Attribute\": \"+coord\",\n        \"Attribute long description\": \"Geodetic coordinates (lat+lon together).\",\n        \"Suggested hashtags (selected)\": \"#geo\"\n    },\n    {\n        \"Attribute\": \"+dest\",\n        \"Attribute long description\": \"Place of destination (intended or actual).\",\n        \"Suggested hashtags (selected)\": \"#region, #country, #adm1, #adm2, #adm3, #adm4, #adm5, #loc\"\n    },\n    {\n        \"Attribute\": \"+displaced\",\n        \"Attribute long description\": \"Displaced people or households. Refers to all types of displacement: use +idps or +refugees to be more specific.\",\n        \"Suggested hashtags (selected)\": \"#affected, #inneed, #targeted, #reached, #population\"\n    },\n    {\n        \"Attribute\": \"+elderly\",\n        \"Attribute long description\": \"Elderly people, loosely defined (precise age range varies). May overlap +adults. You can optionally create custom attributes in addition to this to add precise age ranges, e.g. \\\"+elderly +age65plus\\\".\",\n        \"Suggested hashtags (selected)\": \"#affected, #inneed, #targeted, #reached, #population\"\n    },\n\n... Truncated for brevity\n\n    {\n        \"Attribute\": \"+url\",\n        \"Attribute long description\": \"The data consists of web links related to the main hashtag (e.g. for an #org, #service, #activity, #loc, etc).\",\n        \"Suggested hashtags (selected)\": \"#contact, #org, #activity, #service, #meta\"\n    },\n    {\n        \"Attribute\": \"+used\",\n        \"Attribute long description\": \"Refers to a #service, #item, etc. that affected people have actually consumed or otherwise taken advantage of.\",\n        \"Suggested hashtags (selected)\": \"#service, #item\"\n    }\n]\n\n  Key points:\n\n  - ALWAYS predict hash tags\n  - NEVER predict a tag which is not a valid core hashtag\n  - NEVER start with a core hashtag, you must always start with a core hashtag\n  - Always try and predict an attribute if possible\n\n  You must return your result as a JSON record with the fields 'predicted' and 'reasoning', each is of type string.\n```", "```py\nWhat are the HXL tags and attributes for a column with these details? resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC Appeals Data for South Sudan8.csv'; \n   dataset_description='The dataset contains information on various \n                        appeals and events related to South Sudan, \n                        including details such as the type of appeal, \n                        status, sector, amount requested and funded, \n                        start and end dates, as well as country-specific \n                        information like country code, region, and average \n                        household size. The data includes appeals for \n                        different crises such as floods, population \n                        movements, cholera outbreaks, and Ebola preparedness, \n                        with details on beneficiaries and confirmation needs. \n                        The dataset also includes metadata such as IDs, \n                        names, and translation modules for countries and regions.'; \n   column_name:'aid'; \n   examples: ['18401', '17770', '17721', '16858', '15268', '15113', '14826', '14230', '12788', '9286', '8561']\n```", "```py\ndef call_gpt(prompt, system_prompt, model, temperature, top_p, max_tokens):\n    \"\"\"\n    Calls the GPT model to generate a response based on the given prompt and system prompt.\n\n    Args:\n        prompt (str): The user's input prompt.\n        system_prompt (str): The system's input prompt.\n        model (str): The name or ID of the GPT model to use.\n        temperature (float): Controls the randomness of the generated output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more deterministic.\n        top_p (float): Controls the diversity of the generated output. Higher values (e.g., 0.8) make the output more diverse, while lower values (e.g., 0.2) make it more focused.\n        max_tokens (int): The maximum number of tokens to generate in the response.\n\n    Returns:\n        dict or None: The generated response as a dictionary object, or None if an error occurred during generation.\n    \"\"\"\n    response = client.chat.completions.create(\n        model=model,\n        messages= [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        max_tokens=2000,\n        temperature=temperature,\n        top_p=top_p,\n        frequency_penalty=0,\n        presence_penalty=0,\n        stop=None,\n        stream=False,\n        response_format={ \"type\": \"json_object\" }\n    )\n\n    result = response.choices[0].message.content\n    result = result.replace(\"```", "```py\",\"\")\n    try:\n        result = json.loads(result)\n        result[\"predicted\"] = result[\"predicted\"].replace(\" \",\"\")\n    except:\n        print(result)\n        result = None\n    return result\n\ndef make_prompt_predictions(prompts, model, temperature=0.1, top_p=0.1, \\\n                            max_tokens=2000, debug=False, actual_field=\"actual\"):\n    \"\"\"\n    Generate predictions for a given set of prompts using the specified model.\n\n    Args:\n        prompts (pandas.DataFrame): A DataFrame containing the prompts to generate predictions for.\n        model (str): The name of the model to use for prediction.\n        temperature (float, optional): The temperature parameter for the model's sampling. Defaults to 0.1.\n        top_p (float, optional): The top-p parameter for the model's sampling. Defaults to 0.1.\n        max_tokens (int, optional): The maximum number of tokens to generate for each prompt. Defaults to 2000.\n        debug (bool, optional): Whether to print debug information during prediction. Defaults to False.\n        actual_field (str, optional): The name of the column in the prompts DataFrame that contains the actual values. Defaults to \"actual\".\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the results of the predictions, including the prompt, actual value, predicted value, and reasoning.\n\n    \"\"\"\n\n    num_prompts = len(prompts)\n    print(f\"Number of prompts: {num_prompts}\")\n\n    results = []\n    for index, p in prompts.iterrows():\n\n        if index % 50 == 0:\n            print(f\"{index/num_prompts*100:.2f}% complete\")\n\n        prompt = p[\"prompt\"]\n        prompt = ast.literal_eval(prompt)\n        prompt = prompt[1][\"content\"]\n        actual = p[actual_field]\n\n        result = call_gpt(prompt, hxl_prompt, model, temperature, top_p, max_tokens)\n\n        if result is None:\n            print(\"    !!!!! No LLM result\")\n            predicted = \"\"\n            reasoning = \"\"\n        else:\n            predicted = result[\"predicted\"]\n            reasoning = result[\"reasoning\"]\n\n        if debug is True:\n            print(f\"Actual: {actual}; Predicted: {predicted}; Reasoning: {reasoning}\")\n\n        results.append({\n            \"prompt\": prompt,\n            \"actual\": actual,\n            \"predicted\": predicted,\n            \"reasoning\": reasoning\n        })\n\n    results = pd.DataFrame(results)\n\n    print(f\"\\n\\n===================== {model} Results =========================\\n\\n\")\n    output_prediction_metrics(results)\n    print(f\"\\n\\n=================================================================\")\n\n    results[\"match\"] = results['predicted'] == results['actual']\n    results.to_excel(f\"{LOCAL_DATA_DIR}/hxl-metadata-prompting-only-prediction-{model}-results.xlsx\", index=False)\n\n    return results\n\nfor model in [\"gpt-4o-mini\",\"gpt-4o\"]:\n  print(f\"Model: {model}\")\n  results = make_prompt_predictions(X_test, model, temperature=0.1, top_p=0.1, max_tokens=2000)\n```", "```py\n===================== gpt-4o-mini Results =========================\n\nLLM results for predicted, 458 predictions ...\n\nJust HXL tags ...\n\nAccuracy: 0.77\nPrecision: 0.83\nRecall: 0.77\nF1: 0.77\n\nTags and attributes with predicted ...\n\nAccuracy: 0.53\nPrecision: 0.54\nRecall: 0.53\nF1: 0.5\n\n===================== gpt-4o Results =========================\n\nLLM results for predicted, 458 predictions ...\n\nJust HXL tags ...\n\nAccuracy: 0.86\nPrecision: 0.86\nRecall: 0.86\nF1: 0.85\n\nTags and attributes with predicted ...\n\nAccuracy: 0.71\nPrecision: 0.7\nRecall: 0.71\nF1: 0.69\n\n=================================================================\n```", "```py\nJust HXL tags ...\n\nAccuracy: 0.83\nPrecision: 0.85\nRecall: 0.83\nF1: 0.82\n\nTags and attributes with predicted ...\n\nAccuracy: 0.61\nPrecision: 0.6\nRecall: 0.61\nF1: 0.57\n```", "```py\ndf = pd.read_excel(f\"{LOCAL_DATA_DIR}/hxl-metadata-prompting-only-prediction-gpt-4o-results.xlsx\")\n\nbreaks = df[df[\"match\"]==False]\nprint(breaks.shape)\n\nfor index, row in breaks.iterrows():\n  print(\"\\n======================================== \")\n  pprint.pp(f\"\\nPrompt: {row['prompt']}\")\n  print()\n  print(f\"Actual\", row[\"actual\"])\n  print(f\"Predicted\", row[\"predicted\"])\n  print()\n  pprint.pp(f'Reasoning: \\n{row[\"reasoning\"]}')\n```", "```py\n'\\n'\n 'Prompt: What are the HXL tags and attributes for a column with these '\n 'details? '\n \"resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC \"\n \"Appeals Data for South Sudan8.csv'; dataset_description='The dataset \"\n 'contains information on various appeals and events related to South Sudan, '\n 'including details such as the type of appeal, status, sector, amount '\n 'requested and funded, start and end dates, as well as country-specific '\n 'information like country code, region, and average household size. The data '\n 'includes appeals for different crises such as floods, population movements, '\n 'cholera outbreaks, and Ebola preparedness, with details on beneficiaries and '\n 'confirmation needs. The dataset also includes metadata such as IDs, names, '\n \"and translation modules for countries and regions.'; column_name:'dtype.id'; \"\n \"examples: ['12', '5', '1', '1', '12', '12', '1', '6', '1', '1', '7']\")\n\nActual #cause+id\nPredicted #meta+id\n\n('Reasoning: \\n'\n \"The column 'dtype.id' contains numeric identifiers (e.g., '12', '5', '1') \"\n 'which are likely to be internal identifiers for data records. According to '\n 'the HXL standard, the appropriate hashtag for internal identifiers is '\n \"'#meta' with the attribute '+id'.\")\n\n======================================== \n('\\n'\n 'Prompt: What are the HXL tags and attributes for a column with these '\n 'details? '\n \"resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC \"\n \"Appeals Data for South Sudan8.csv'; dataset_description='The dataset \"\n 'contains information on various appeals and events related to South Sudan, '\n 'including details such as the type of appeal, status, sector, amount '\n 'requested and funded, start and end dates, as well as country-specific '\n 'information like country code, region, and average household size. The data '\n 'includes appeals for different crises such as floods, population movements, '\n 'cholera outbreaks, and Ebola preparedness, with details on beneficiaries and '\n 'confirmation needs. The dataset also includes metadata such as IDs, names, '\n \"and translation modules for countries and regions.'; \"\n \"column_name:'dtype.name'; examples: ['Flood', 'Population Movement', \"\n \"'Epidemic', 'Epidemic', 'Flood', 'Flood', 'Epidemic', 'Complex Emergency', \"\n \"'Epidemic', 'Epidemic', 'Civil Unrest']\")\n\nActual #cause+name\nPredicted #event+type\n\n('Reasoning: \\n'\n \"The examples provided in the column ('Flood', 'Population Movement', \"\n \"'Epidemic', 'Complex Emergency', 'Civil Unrest') describe different types of \"\n 'events or incidents within a crisis or emergency. According to the HXL '\n 'standard, the appropriate hashtag for this type of data is #event, and the '\n 'attribute +type is used to specify the type or category of the event.')\n\n======================================== \n('\\n'\n 'Prompt: What are the HXL tags and attributes for a column with these '\n 'details? '\n \"resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC \"\n \"Appeals Data for South Sudan8.csv'; dataset_description='The dataset \"\n 'contains information on various appeals and events related to South Sudan, '\n 'including details such as the type of appeal, status, sector, amount '\n 'requested and funded, start and end dates, as well as country-specific '\n 'information like country code, region, and average household size. The data '\n 'includes appeals for different crises such as floods, population movements, '\n 'cholera outbreaks, and Ebola preparedness, with details on beneficiaries and '\n 'confirmation needs. The dataset also includes metadata such as IDs, names, '\n \"and translation modules for countries and regions.'; \"\n \"column_name:'status_display'; examples: ['Active', 'Active', 'Closed', \"\n \"'Closed', 'Closed', 'Closed', 'Closed', 'Closed', 'Closed', 'Closed', \"\n \"'Closed']\")\n\nActual #status+name\nPredicted #status+code\n\n('Reasoning: \\n'\n \"The column 'status_display' contains values such as 'Active' and 'Closed', \"\n 'which describe the status of appeals or events. The appropriate HXL hashtag '\n 'for project or activity status is #status. Since the values are categorical '\n 'and represent different statuses, the attribute +code is suitable to '\n 'indicate these status codes.')\n\n======================================== \n('\\n'\n 'Prompt: What are the HXL tags and attributes for a column with these '\n 'details? '\n \"resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC \"\n \"Appeals Data for South Sudan8.csv'; dataset_description='The dataset \"\n 'contains information on various appeals and events related to South Sudan, '\n 'including details such as the type of appeal, status, sector, amount '\n 'requested and funded, start and end dates, as well as country-specific '\n 'information like country code, region, and average household size. The data '\n 'includes appeals for different crises such as floods, population movements, '\n 'cholera outbreaks, and Ebola preparedness, with details on beneficiaries and '\n 'confirmation needs. The dataset also includes metadata such as IDs, names, '\n \"and translation modules for countries and regions.'; \"\n \"column_name:'region.id'; examples: ['0', '0', '0', '0', '0', '0', '0', '0', \"\n \"'0', '0', '0']\")\n\nActual #adm1+code\nPredicted #region+id\n\n('Reasoning: \\n'\n \"The column 'region.id' contains numeric identifiers for regions, which \"\n 'aligns with the HXL tag #region and the attribute +id. The examples provided '\n 'are all numeric, indicating that these are likely unique identifiers for '\n 'regions.')\n\n======================================== \n```", "```py\n def num_tokens_from_string(string: str, encoding_name: str) -> int:\n    \"\"\"\n    Returns the number of tokens in a text string using toktoken.\n    See: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n\n    Args:\n        string (str): The text string to count the tokens for.\n        encoding_name (str): The name of the encoding to use.\n\n    Returns:\n        num_tokens: The number of tokens in the text string.\n\n    \"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\ndef calc_costs(data, model, method=\"prompting\"):\n  \"\"\"\n  Calculate token costs for a given dataset, method and model.\n  Note: Only for inference costs, not fine-tuning\n\n  Args:\n    data (pandas.DataFrame): The data to get the tokens for.\n    method (str, optional): The method to use. Defaults to \"prompting\".\n    model (str): The model to use, eg \"gpt-4o-mini\"\n\n  Returns:\n    input_tokens: The number of input tokens.\n    output_tokens: The number of output tokens.\n\n  \"\"\"\n  # See https://openai.com/api/pricing/\n  price = {\n      \"gpt-4o-mini\": {\n          \"input\": 0.150,\n          \"output\": 0.600\n      },\n      \"gpt-4o\": {\n          \"input\": 5.00,\n          \"output\": 15.00\n      }\n  }\n  input_tokens = 0\n  output_tokens = 0\n  for index, p in data.iterrows():\n      prompt = p[\"prompt\"]\n      prompt = ast.literal_eval(prompt)\n      input = prompt[1][\"content\"] \n      # If prompting, we must include system prompt\n      if method == \"prompting\":\n        input += \" \" + hxl_prompt\n      output = p[\"Corrected actual\"]\n      input_tokens += num_tokens_from_string(str(input), \"cl100k_base\")\n      output_tokens += num_tokens_from_string(str(output), \"cl100k_base\") \n\n  input_cost = input_tokens / 1000000 * price[model][\"input\"]\n  output_cost = output_tokens / 1000000 * price[model][\"output\"]\n\n  print(f\"\\nFor {data.shape[0]} table columns where we predicted HXL tags ...\")\n  print(f\"{method} prediction with model {model}, {input_tokens} input tokens = ${input_cost}\")\n  print(f\"Fine-tuning prediction GPT-4o-mini {output_tokens} output tokens = ${output_cost}\\n\")\n\nhxl_prompt = generate_hxl_standard_prompt(HXL_SCHEMA_LOCAL_FILE, debug=False)\nX_test2 = pd.read_excel(f\"{LOCAL_DATA_DIR}/hxl-metadata-fine-tune-prediction-results-review.xlsx\", sheet_name=0)\n\ncalc_costs(X_test2, method=\"fine-tuning\", model=\"gpt-4o-mini\")\ncalc_costs(X_test2, method=\"prompting\", model=\"gpt-4o-mini\")\ncalc_costs(X_test2, method=\"prompting\", model=\"gpt-4o\")\n```", "```py\nFor 458 table columns where we predicted HXL tags ...\nfine-tuning prediction with model gpt-4o-mini, 99738 input tokens = $0.014960699999999999\nFine-tuning prediction GPT-4o-mini 2001 output tokens = $0.0012006\n\nFor 458 table columns where we predicted HXL tags ...\nprompting prediction with model gpt-4o-mini, 2688812 input tokens = $0.4033218\nFine-tuning prediction GPT-4o-mini 2001 output tokens = $0.0012006\n\nFor 458 table columns where we predicted HXL tags ...\nprompting prediction with model gpt-4o, 2688812 input tokens = $13.44406\nFine-tuning prediction GPT-4o-mini 2001 output tokens = $0.030015000000000003\n```", "```py\n# See HDX for this file: https://data.humdata.org/dataset/sudan-acled-conflict-data\nDATAFILE_URL=\"https://data.humdata.org/dataset/5efad450-8b15-4867-b7b3-8a25b455eed8/resource/3352a0d8-2996-4e70-b618-3be58699be7f/download/sudan_hrp_civilian_targeting_events_and_fatalities_by_month-year_as-of-25jul2024.xlsx\"\nlocal_data_file = f\"{LOCAL_DATA_DIR}/{DATAFILE_URL.split('/')[-1]}\"\n\n# Save data file locally \nurllib.request.urlretrieve(DATAFILE_URL, local_data_file)\n\n# Read it to get a dataframe\ndf = pd.read_excel(local_data_file, sheet_name=1)\n```", "```py\nfrom hxl_utils import HXLUtils\n\nhxl_utils = HXLUtils(LOCAL_DATA_DIR, model=\"gpt-4o\")\ndata = hxl_utils.add_hxl(df,\"sudan_hrp_civilian_targeting_events_and_fatalities_by_month-year_as-of-25jul2024.xlsx\")\n\nprint(\"\\n\\nAFTER: \\n\\n\")\ndisplay(data)\n```", "```py\nhxl_utils = HXLUtils(LOCAL_DATA_DIR, model=\"gpt-4o-mini\")\ndata = hxl_utils.add_hxl(df,\"sudan_hrp_civilian_targeting_events_and_fatalities_by_month-year_as-of-25jul2024.xlsx\")\n```"]