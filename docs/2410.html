<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Exploring How the New OpenAI Realtime API Simplifies Voice Agent Flows</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Exploring How the New OpenAI Realtime API Simplifies Voice Agent Flows</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-how-the-new-openai-realtime-api-simplifies-voice-agent-flows-7b136ef8483d?source=collection_archive---------7-----------------------#2024-10-03">https://towardsdatascience.com/exploring-how-the-new-openai-realtime-api-simplifies-voice-agent-flows-7b136ef8483d?source=collection_archive---------7-----------------------#2024-10-03</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1f14" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Setting up a Voice Agent using Twilio and the OpenAI Realtime API</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ssmaameri?source=post_page---byline--7b136ef8483d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sami Maameri" class="l ep by dd de cx" src="../Images/9e9892fe7d3cc53ad1c4d165145878ef.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*idWYMwlRbOHeOx70A-XzMQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--7b136ef8483d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@ssmaameri?source=post_page---byline--7b136ef8483d--------------------------------" rel="noopener follow">Sami Maameri</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--7b136ef8483d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 3, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">5</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mj mk ml mm mn mo"><div class="mp io l ed"><div class="mq mr l"/></div></figure><h1 id="1828" class="ms mt fq bf mu mv mw gq mx my mz gt na nb nc nd ne nf ng nh ni nj nk nl nm nn bk">Introduction</h1><p id="c4f1" class="pw-post-body-paragraph no np fq nq b go nr ns nt gr nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">At the recent OpenAI Dev Day on October 1st, 2024, OpenAI’s biggest release was the reveal of their Realtime API:</p><blockquote class="ok"><p id="855b" class="ol om fq bf on oo op oq or os ot oj dx">“Today, we’re introducing a public beta of the Realtime API, enabling all paid developers to build low-latency, multimodal experiences in their apps.</p><p id="1e2d" class="ol om fq bf on oo op oq or os ot oj dx">Similar to ChatGPT’s Advanced Voice Mode, the Realtime API supports natural speech-to-speech conversations using the <a class="af ou" href="https://platform.openai.com/docs/guides/text-to-speech" rel="noopener ugc nofollow" target="_blank">six preset voices</a> already supported in the API.”</p><p id="26f3" class="ol om fq bf on oo op oq or os ot oj dx">(source: OpenAI website)</p></blockquote><p id="e84a" class="pw-post-body-paragraph no np fq nq b go ov ns nt gr ow nv nw nx ox nz oa ob oy od oe of oz oh oi oj fj bk">As per their message, some of its key benefits include low latency, and its speech to speech capabilities. Let’s see how that plays out in practice in terms of building out voice AI agents.</p><p id="7364" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">It also has an interruption handling feature, so that the realtime stream will stop sending audio if it detects you are trying to speak over it, a useful feature for sure when building voice agents.</p><h1 id="d616" class="ms mt fq bf mu mv mw gq mx my mz gt na nb nc nd ne nf ng nh ni nj nk nl nm nn bk">Contents</h1><p id="af38" class="pw-post-body-paragraph no np fq nq b go nr ns nt gr nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">In this article we will:</p><ul class=""><li id="6f10" class="no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj pf pg ph bk">Compare what a phone voice agent flow might have looked like before the Realtime API, and what it looks like now,</li><li id="3f4f" class="no np fq nq b go pi ns nt gr pj nv nw nx pk nz oa ob pl od oe of pm oh oi oj pf pg ph bk">Review a GitHub project from Twilio that sets up a voice agent using the new Realtime API, so we can see what the implementation looks like in practice, and get an idea how the websockets and connections are setup for such an application,</li><li id="7122" class="no np fq nq b go pi ns nt gr pj nv nw nx pk nz oa ob pl od oe of pm oh oi oj pf pg ph bk">Quickly review the React demo project from OpenAI that uses the Realtime API,</li><li id="a6a4" class="no np fq nq b go pi ns nt gr pj nv nw nx pk nz oa ob pl od oe of pm oh oi oj pf pg ph bk">Compare the pricing of these various options.</li></ul><h1 id="3ffd" class="ms mt fq bf mu mv mw gq mx my mz gt na nb nc nd ne nf ng nh ni nj nk nl nm nn bk">Voice Agent Flows</h1><h2 id="a432" class="pn mt fq bf mu po pp pq mx pr ps pt na nx pu pv pw ob px py pz of qa qb qc qd bk">Before the OpenAI Realtime API</h2><p id="ad73" class="pw-post-body-paragraph no np fq nq b go nr ns nt gr nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">To get a phone voice agent service working, there are some key services we require</p><ul class=""><li id="f731" class="no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj pf pg ph bk">Speech to Text ( e.g Deepgram),</li><li id="14ef" class="no np fq nq b go pi ns nt gr pj nv nw nx pk nz oa ob pl od oe of pm oh oi oj pf pg ph bk">LLM/Agent ( e.g OpenAI),</li><li id="336a" class="no np fq nq b go pi ns nt gr pj nv nw nx pk nz oa ob pl od oe of pm oh oi oj pf pg ph bk">Text to Speech (e.g ElevenLabs).</li></ul><p id="4dcd" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">These services are illustrated in the diagram below</p><figure class="mj mk ml mm mn mo qe qf paragraph-image"><div role="button" tabindex="0" class="qh qi ed qj bh qk"><div class="qe qf qg"><img src="../Images/8dd1f33ba5808328d984fbd791914750.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WNcCVONSMfJy0J8JH1KG0Q.png"/></div></div><figcaption class="qm qn qo qe qf qp qq bf b bg z dx">(source <a class="af ou" href="https://github.com/twilio-labs/call-gpt" rel="noopener ugc nofollow" target="_blank">https://github.com/twilio-labs/call-gpt</a>, MIT license)</figcaption></figure><p id="f4e2" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">That of course means integration with a number of services, and separate API requests for each parts.</p><p id="ad1c" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">The new OpenAI Realtime API allows us to bundle all of those together into a single request, hence the term, speech to speech.</p><h2 id="9982" class="pn mt fq bf mu po pp pq mx pr ps pt na nx pu pv pw ob px py pz of qa qb qc qd bk">After the OpenAI Realtime API</h2><p id="42e9" class="pw-post-body-paragraph no np fq nq b go nr ns nt gr nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">This is what the flow diagram would look like for a similar new flow using the new OpenAI Realtime API.</p><figure class="mj mk ml mm mn mo qe qf paragraph-image"><div role="button" tabindex="0" class="qh qi ed qj bh qk"><div class="qe qf qr"><img src="../Images/b46168112d4e112560181c037971de44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YC2HagTFrju0bf-AhrCfjw.png"/></div></div></figure><p id="0308" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">Obviously this is a much simpler flow. What is happening is we are just passing the speech/audio from the phone call directly to the OpenAI Realtime API. No need for a speech to text intermediary service.</p><p id="4098" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">And on the response side, the Realtime API is again providing an audio stream as the response, which we can send right back to Twilio (i.e to the phone call response). So again, no need for an extra text to speech service, as it is all taken care of by the OpenAI Realtime API.</p><h1 id="0ca2" class="ms mt fq bf mu mv mw gq mx my mz gt na nb nc nd ne nf ng nh ni nj nk nl nm nn bk">Source code review for a Twilio and Realtime API voice agent</h1><p id="55fd" class="pw-post-body-paragraph no np fq nq b go nr ns nt gr nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">Let’s look at some code samples for this. Twilio has provided a great github repository example for setting up this Twilio and OpenAI Realtime API flow. You can find it here:</p><div class="qs qt qu qv qw qx"><a href="https://github.com/twilio-samples/speech-assistant-openai-realtime-api-node?source=post_page-----7b136ef8483d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qy ab ig"><div class="qz ab co cb ra rb"><h2 class="bf fr hw z io rc iq ir rd it iv fp bk">GitHub - twilio-samples/speech-assistant-openai-realtime-api-node</h2><div class="re l"><h3 class="bf b hw z io rc iq ir rd it iv dx">Contribute to twilio-samples/speech-assistant-openai-realtime-api-node development by creating an account on GitHub.</h3></div><div class="rf l"><p class="bf b dy z io rc iq ir rd it iv dx">github.com</p></div></div><div class="rg l"><div class="rh l ri rj rk rg rl lr qx"/></div></div></a></div><p id="7e04" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">Here are some excerpts from key parts of the code related to setting up</p><ul class=""><li id="0a73" class="no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj pf pg ph bk">the websockets connection from Twilio to our application, so that we can receive audio from the caller, and send audio back,</li><li id="1572" class="no np fq nq b go pi ns nt gr pj nv nw nx pk nz oa ob pl od oe of pm oh oi oj pf pg ph bk">and the websockets connection to the OpenAI Realtime API from our application.</li></ul><p id="d3fb" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">I have added some comments in the source code below to try and explain what is going on, expecially regarding the websocket connection between Twilio and our applicaion, and the websocket connection from our application to OpenAI. The triple dots (…) refere to sections of the source code that have been removed for brevity, since they are not critical to understanding the core features of how the flow works.</p><pre class="mj mk ml mm mn rm rn ro bp rp bb bk"><span id="89c7" class="rq mt fq rn b bg rr rs l rt ru">// On receiving a phone call, Twilio forwards the incoming call request to<br/>// a webhook we specify, which is this endpoint here. This allows us to <br/>// create programatic voice applications, for example using an AI agent<br/>// to handle the phone call<br/>// <br/>// So, here we are providing an initial response to the call, and creating<br/>// a websocket (called a MediaStream in Twilio, more on that below) to receive<br/>// any future audio that comes into the call<br/>fastify.all('/incoming', async (request, reply) =&gt; {<br/>    const twimlResponse = `&lt;?xml version="1.0" encoding="UTF-8"?&gt;<br/>                          &lt;Response&gt;<br/>                              &lt;Say&gt;Please wait while we connect your call to the A. I. voice assistant, powered by Twilio and the Open-A.I. Realtime API&lt;/Say&gt;<br/>                              &lt;Pause length="1"/&gt;<br/>                              &lt;Say&gt;O.K. you can start talking!&lt;/Say&gt;<br/>                              &lt;Connect&gt;<br/>                                  &lt;Stream url="wss://${request.headers.host}/media-stream" /&gt;<br/>                              &lt;/Connect&gt;<br/>                          &lt;/Response&gt;`;<br/><br/>    reply.type('text/xml').send(twimlResponse);<br/>});<br/><br/>fastify.register(async (fastify) =&gt; {<br/><br/>    // Here we are connecting our application to the websocket media stream we<br/>    // setup above. That means all audio that comes though the phone will come<br/>    // to this websocket connection we have setup here<br/>    fastify.get('/media-stream', { websocket: true }, (connection, req) =&gt; {<br/>        console.log('Client connected');<br/><br/>        // Now, we are creating websocket connection to the OpenAI Realtime API<br/>        // This is the second leg of the flow diagram above<br/>        const openAiWs = new WebSocket('wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01', {<br/>            headers: {<br/>                Authorization: `Bearer ${OPENAI_API_KEY}`,<br/>                "OpenAI-Beta": "realtime=v1"<br/>            }<br/>        });<br/><br/>        ...<br/><br/>        // Here we are setting up the listener on the OpenAI Realtime API <br/>        // websockets connection. We are specifying how we would like it to<br/>        // handle any incoming audio streams that have come back from the<br/>        // Realtime API.<br/>        openAiWs.on('message', (data) =&gt; {<br/>            try {<br/>                const response = JSON.parse(data);<br/><br/>                ...<br/><br/>        // This response type indicates an LLM responce from the Realtime API<br/>        // So we want to forward this response back to the Twilio Mediat Stream<br/>        // websockets connection, which the caller will hear as a response on<br/>        // on the phone<br/>                if (response.type === 'response.audio.delta' &amp;&amp; response.delta) {<br/>                    const audioDelta = {<br/>                        event: 'media',<br/>                        streamSid: streamSid,<br/>                        media: { payload: Buffer.from(response.delta, 'base64').toString('base64') }<br/>                    };<br/>         // This is the actual part we are sending it back to the Twilio<br/>         // MediaStream websockets connection. Notice how we are sending the<br/>         // response back directly. No need for text to speech conversion from<br/>         // the OpenAI response. The OpenAI Realtime API already provides the<br/>         // response as an audio stream (i.e speech to speech)<br/>                    connection.send(JSON.stringify(audioDelta));<br/>                }<br/>            } catch (error) {<br/>                console.error('Error processing OpenAI message:', error, 'Raw message:', data);<br/>            }<br/>        });<br/><br/>        // This parts specifies how we handle incoming messages to the Twilio<br/>        // MediaStream websockets connection i.e how we handle audio that comes<br/>        // into the phone from the caller<br/>        connection.on('message', (message) =&gt; {<br/>            try {<br/>                const data = JSON.parse(message);<br/><br/>                switch (data.event) {<br/>        // This case ('media') is that state for when there is audio data <br/>        // available on the Twilio MediaStream from the caller<br/>                    case 'media':<br/>                        // we first check out OpenAI Realtime API websockets<br/>                        // connection is open <br/>                        if (openAiWs.readyState === WebSocket.OPEN) {<br/>                            const audioAppend = {<br/>                                type: 'input_audio_buffer.append',<br/>                                audio: data.media.payload<br/>                            };<br/>                        // and then forward the audio stream data to the<br/>                        // Realtime API. Again, notice how we are sending the<br/>                        // audio stream directly, not speech to text converstion<br/>                        // as would have been required previously<br/>                            openAiWs.send(JSON.stringify(audioAppend));<br/>                        }<br/>                        break;<br/><br/>                  ...<br/>                }<br/>            } catch (error) {<br/>                console.error('Error parsing message:', error, 'Message:', message);<br/>            }<br/>        });<br/><br/>...<br/><br/>fastify.listen({ port: PORT }, (err) =&gt; {<br/>    if (err) {<br/>        console.error(err);<br/>        process.exit(1);<br/>    }<br/>    console.log(`Server is listening on port ${PORT}`);<br/>});</span></pre><p id="28c6" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">So, that is how the new OpenAI Realtime API flow plays out in practice.</p><p id="4589" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">Regarding the Twilio MediaStreams, you can read more about them <a class="af ou" href="https://www.twilio.com/docs/voice/media-streams" rel="noopener ugc nofollow" target="_blank">here</a>. They are a way to setup a websockets connection between a call to a Twilio phone number and your application. This allows streaming of audio from the call to and from you application, allowing you to build programmable voice applications over the phone.</p><p id="9c49" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">To get to the code above running, you will need to setup a Twilio number and ngrok also. You can check out my other article over here for help setting those up.</p><div class="qs qt qu qv qw qx"><a href="https://levelup.gitconnected.com/ai-voice-agent-with-twilio-express-and-openai-96e19c1e8035?source=post_page-----7b136ef8483d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qy ab ig"><div class="qz ab co cb ra rb"><h2 class="bf fr hw z io rc iq ir rd it iv fp bk">AI Voice Agent with Twilio, Express and OpenAI</h2><div class="re l"><h3 class="bf b hw z io rc iq ir rd it iv dx">Let’s get ChatGPT over the phone</h3></div><div class="rf l"><p class="bf b dy z io rc iq ir rd it iv dx">levelup.gitconnected.com</p></div></div><div class="rg l"><div class="rv l ri rj rk rg rl lr qx"/></div></div></a></div><p id="94d9" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">Since access to the OpenAI Realtime API has just been rolled, not everyone may have access just yet. I intially was not able to access it. Running the application worked, but as soon as it tries to connect to the OpenAI Realtime API I got a 403 error. So in case you see the same issue, it could be related to not having access yet also.</p><figure class="mj mk ml mm mn mo qe qf paragraph-image"><div role="button" tabindex="0" class="qh qi ed qj bh qk"><div class="qe qf rw"><img src="../Images/ef320cbbdec8e19721b97fe4e4cb01b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AzfzWNssQ9LH6ufnYlFAAw.png"/></div></div></figure><h1 id="088c" class="ms mt fq bf mu mv mw gq mx my mz gt na nb nc nd ne nf ng nh ni nj nk nl nm nn bk">React OpenAI Realtime API Demo</h1><p id="e84e" class="pw-post-body-paragraph no np fq nq b go nr ns nt gr nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">OpenAI have also provided a great demo for testing out their Realtime API in the browser using a React app. I tested this out myself, and was very impressed with the speed of response from the voice agent coming from the Realtime API. The response is instant, there is no latency, and makes for a great user experience. I was definitley impressed when testing it out.</p><p id="59c5" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">Sharing a link to the source code here. It has intructions in the README.md for how to get setup</p><div class="qs qt qu qv qw qx"><a href="https://github.com/openai/openai-realtime-console?source=post_page-----7b136ef8483d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qy ab ig"><div class="qz ab co cb ra rb"><h2 class="bf fr hw z io rc iq ir rd it iv fp bk">GitHub - openai/openai-realtime-console: React app for inspecting, building and debugging with the…</h2><div class="re l"><h3 class="bf b hw z io rc iq ir rd it iv dx">React app for inspecting, building and debugging with the Realtime API - openai/openai-realtime-console</h3></div><div class="rf l"><p class="bf b dy z io rc iq ir rd it iv dx">github.com</p></div></div><div class="rg l"><div class="rx l ri rj rk rg rl lr qx"/></div></div></a></div><p id="3d31" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">This is a picture of what the application looks like once you get it running on local</p><figure class="mj mk ml mm mn mo qe qf paragraph-image"><div role="button" tabindex="0" class="qh qi ed qj bh qk"><div class="qe qf ry"><img src="../Images/7c66d5839aa9c80f702df45586fc6195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hVHkMHn08F7rNz9ym2oMCA.png"/></div></div><figcaption class="qm qn qo qe qf qp qq bf b bg z dx">(source <a class="af ou" href="https://github.com/openai/openai-realtime-console" rel="noopener ugc nofollow" target="_blank">https://github.com/openai/openai-realtime-console</a>, MIT license)</figcaption></figure><h1 id="616d" class="ms mt fq bf mu mv mw gq mx my mz gt na nb nc nd ne nf ng nh ni nj nk nl nm nn bk">Pricing</h1><p id="3bba" class="pw-post-body-paragraph no np fq nq b go nr ns nt gr nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">Let’s compare the cost the of using the OpenAI Realtime API versus a more conventional approach using Deepagram for speech to text (STT) and text to speech (TTS) and using OpenAI GPT-4o for the LLM part.</p><p id="3f61" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">Comparison using the prices from their websites shows that for a 1 minute conversation, with the caller speaking half the time, and the AI agent speaking the other half, the cost per minute using Deepgram and GPT-4o would be $0.0117/minute, whereas using the OpenAI Realtime API would be $0.15/minute.</p><p id="d0b1" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">That means using the OpenAI Realtime API would be just over 10x the price per minute.</p><figure class="mj mk ml mm mn mo qe qf paragraph-image"><div class="qe qf rz"><img src="../Images/27e7d126e7f638d7166645bb22c3464c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*pkpVxr3evvBqM8odN1Zujg.png"/></div></figure><p id="775c" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">It does sound like a fair amount more expensive, though we should balance that with some of the benefits the OpenAI Realtime API could provide, including</p><ul class=""><li id="3278" class="no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj pf pg ph bk">reduced latencies, crucial for having a good voice experience,</li><li id="2aee" class="no np fq nq b go pi ns nt gr pj nv nw nx pk nz oa ob pl od oe of pm oh oi oj pf pg ph bk">ease of setup due to fewer moving parts,</li><li id="6625" class="no np fq nq b go pi ns nt gr pj nv nw nx pk nz oa ob pl od oe of pm oh oi oj pf pg ph bk">conversation interruption handling provided out of the box.</li></ul><p id="4883" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">Also, please do be aware that prices can change over time, so the prices you find at the time of reading this article, may not be the same as those reflected above.</p><h1 id="dde3" class="ms mt fq bf mu mv mw gq mx my mz gt na nb nc nd ne nf ng nh ni nj nk nl nm nn bk">Conclusion</h1><p id="e574" class="pw-post-body-paragraph no np fq nq b go nr ns nt gr nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">Hope that was helpful! What do you think of the new OpenAI Realtime API? Think you will be using it in any upcoming projects?</p><p id="2ace" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">While we are here, are there any other tutorials or articles around voice agents andvoice AI you would be interested in? I am deep diving into that field a bit just now, so would be happy to look into anything people find interesting.</p><p id="26ce" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk">Happy hacking!</p></div></div></div><div class="ab cb sa sb sc sd" role="separator"><span class="se by bm sf sg sh"/><span class="se by bm sf sg sh"/><span class="se by bm sf sg"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b642" class="pw-post-body-paragraph no np fq nq b go pa ns nt gr pb nv nw nx pc nz oa ob pd od oe of pe oh oi oj fj bk"><em class="si">All image provided are by the author, unless stated otherwise</em></p></div></div></div></div>    
</body>
</html>