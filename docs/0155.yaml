- en: Leveraging Graphs to Advance Chain-of-Thought Reasoning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/leveraging-graphs-to-advance-chain-of-thought-reasoning-77022a0e1413?source=collection_archive---------11-----------------------#2024-01-16](https://towardsdatascience.com/leveraging-graphs-to-advance-chain-of-thought-reasoning-77022a0e1413?source=collection_archive---------11-----------------------#2024-01-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alcarazanthony1?source=post_page---byline--77022a0e1413--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page---byline--77022a0e1413--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--77022a0e1413--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--77022a0e1413--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page---byline--77022a0e1413--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--77022a0e1413--------------------------------)
    ·5 min read·Jan 16, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial intelligence software was used to enhance the grammar, flow, and
    readability of this article’s text.*'
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-thought (CoT) prompting has rapidly emerged as a technique to substantially
    improve the reasoning capabilities of large language models. By demonstrating
    step-by-step reasoning chains, CoT allows models like GPT-4 to solve multi-step
    problems — from mathematics to commonsense puzzles. The key insight is that by
    learning from contextual examples, models can acquire complex logicial skills
    without resorting to task-specific fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: However, a key limitation hampering wider applicability of CoT prompting is
    the reliance on hand-designed demonstrations. Crafting high-quality reasoning
    chains with coherent logical flow requires substantial human effort and expertise.
    To unlock the full potential, we need methods to automatically generate quality
    CoT demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: Recent work has sought to address this through retrieval and generative approaches.
    But resulting chains often suffer from incoherence, gaps, and grounding errors.
    Capturing the fluid, conceptual flow of reasoning chains in textual sequences
    has proven difficult. We propose instead representing reasoning structure with
    specialized graphs to advance CoT prompting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, this article identifies two complementary graph-powered techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Modeling CoT demonstrations as directed graphs to capture flow and analyze structure
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
