<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Understanding Techniques for Solving GenAI Challenges</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Understanding Techniques for Solving GenAI Challenges</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-techniques-for-solving-genai-challenges-83a7ad4650bd?source=collection_archive---------5-----------------------#2024-06-20">https://towardsdatascience.com/understanding-techniques-for-solving-genai-challenges-83a7ad4650bd?source=collection_archive---------5-----------------------#2024-06-20</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="d51b" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><em class="hd">Dive into model pre-training, fine-tuning, RAG, prompt engineering, and more!</em></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@tula.masterman?source=post_page---byline--83a7ad4650bd--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Tula Masterman" class="l ep by dd de cx" src="../Images/c36b3740befd5dfdb8719dc6596f1a99.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*Fn6lzAzI489IDlnO-QI8_A.jpeg"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--83a7ad4650bd--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@tula.masterman?source=post_page---byline--83a7ad4650bd--------------------------------" rel="noopener follow">Tula Masterman</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--83a7ad4650bd--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 20, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk ld le ab q ee lf lg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lc"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lb lc">1</span></p></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lh k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al li an ao ap ie lj lk ll" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lm cn"><div class="l ae"><div class="ab cb"><div class="ln lo lp lq lr ls ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml mm"><img src="../Images/bff37c9c3e8b1d57d88922cd6e0dedca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*odzvHfLnWQ3xUtGw"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Source: Author &amp; GPT4o. Image is designed to show a language model learning and developing its brain!</figcaption></figure><h1 id="d668" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Introduction</h1><p id="8edc" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Generative AI adoption is rapidly increasing for both individuals and businesses. A recent <a class="af ov" href="https://www.gartner.com/en/newsroom/press-releases/2024-05-07-gartner-survey-finds-generative-ai-is-now-the-most-frequently-deployed-ai-solution-in-organizations" rel="noopener ugc nofollow" target="_blank">Gartner study</a> found that GenAI solutions are the number one AI solution used by organizations, with most companies leveraging GenAI features built into existing tools like Microsoft 365 Copilot. In my experience, most businesses are looking for some sort of “private ChatGPT” they can use to get more value from their unique organizational data. Company goals vary from finding information in particular documents, generating reports based on tabular data, and summarizing content, to finding all the projects related to some domain, and much more.</p><p id="15a1" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">This article explores various approaches to solve these problems, outlining the pros, cons, and applications of each. My goal is to provide guidance on when to consider different approaches and how to combine them for the best outcomes, covering everything from the most complex and expensive approaches like pre-training to the simplest, most cost-effective techniques like prompt engineering.</p><p id="bb76" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">The sequence of the article is intended to build from the foundational concepts for model training (pre-training, continued pre-training, and fine tuning) to the more commonly understood techniques (RAG and prompt engineering) for interacting with existing models.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml pb"><img src="../Images/ffd3959aaf3e515bd7c43b58844fdc8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZCAF1aNP9lhlnseUrNBdgA.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Image by author</figcaption></figure><h1 id="8790" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Background</h1><p id="1960" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">There is no one-size fits all approach to tackling GenAI problems. Most use cases require a combination of techniques to achieve successful outcomes. Typically, organizations start with a model like GPT-4, Llama3 70b Instruct, or DBRX Instruct which have been pretrained on trillions of tokens to perform next token prediction, then fine-tuned for a particular task, like instruction or chat. Instruction based models are trained and optimized to follow specific directions given in the prompt while chat based models are trained and optimized to handle conversational formats over multiple turns, maintaining context and coherence throughout the conversation.</p><p id="1f71" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Using existing models allows organizations to take advantage of the significant time and financial investments made by companies like OpenAI, Meta, and Databricks to curate datasets, create innovative architectures, and train and evaluate their models.</p><p id="7b80" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Although not every company will need to pre-train or instruction fine-tune their models, anyone using a Large Language Model (LLM) benefits from the groundwork laid by these industry leaders. This foundation allows other companies to address their unique challenges without starting from scratch.</p><p id="21ce" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">In the following sections, we’ll explore pre-training, fine-tuning (both instruction fine-tuning, and continued pre-training), Retrieval Augmented Generation (RAG), fine-tuning embeddings for RAG, and prompt engineering, discussing how and when each of these approaches should be used or considered.</p><h2 id="2d92" class="pc ne fq bf nf pd pe pf ni pg ph pi nl oi pj pk pl om pm pn po oq pp pq pr ps bk">Setting the Baseline with Pre-Training</h2><p id="af90" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><strong class="ob fr">Overview</strong>: Pre-Training a model creates a foundation which will be used as a base for all downstream tasks. This process includes defining the architecture for the model, curating a massive dataset (generally trillions of tokens), training the model, and evaluating its performance. In the context of LLMs and SLMs, the pre-training phase is used to inject knowledge into the model, enabling it to predict the next word or token in a sequence. For instance, in the sentence “the cat sat on the ___”, the model learns to predict “mat”.</p><p id="5f7b" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Companies like OpenAI have invested heavily in the pre-training phase for their GPT models, but since models like GPT-3.5, GPT-4, and GPT-4o are closed source it is not possible to use the underlying architecture and pre-train the model on a different dataset with different parameters. However, with resources like Mosaic AI’s pre-training API it is possible to pre-train open source models like DBRX.</p><p id="ad8c" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Pros</strong>:</p><ul class=""><li id="7458" class="nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou pt pu pv bk"><strong class="ob fr">Complete control</strong>: The benefit of pre-training a model is that you’d have complete control over the entire process to create the model. You can tailor the architecture, dataset, and training parameters to your needs and test it with evaluations representative of your domain instead of a focusing primarily on common benchmarks.</li><li id="52c6" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Inherent domain specific knowledge</strong>: By curating a dataset focused on a particular domain, the model can develop a deeper understanding of that domain compared to a general purpose model.</li></ul><p id="7506" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Cons</strong>:</p><ul class=""><li id="ae77" class="nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou pt pu pv bk"><strong class="ob fr">Most expensive option</strong>: Pre-training requires an extreme amount of computational power (many, many GPUs) which means the cost of pre-training is typically in the millions to tens or hundreds of millions of dollars and often takes weeks to complete the training.</li><li id="573b" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Knowledge cutoffs</strong>: The final model is also completed at a certain point in time, so it will have no inherent understanding of real time information unless augmented by techniques like RAG or function-calling.</li><li id="e295" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Advanced requirements</strong>: This approach requires the most data and the most advanced expertise to achieve meaningful results.</li></ul><p id="1f95" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Applications</strong>: Generally, pre-training your own model is <strong class="ob fr">only necessary if none of the other approaches are sufficient for your use case</strong>. For example, if you wanted to train a model to understand a new language it has no previous exposure to, you may consider pre-training it then fine-tuning it for your intended use.</p><p id="44f1" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Once the base training is complete, the models typically need to be fine-tuned so that they can perform tasks effectively. When you see a model labeled as a chat or instruct model, that indicates the base model has been fine-tuned for either of those purposes. Nearly any model you interact with today has been fine-tuned for one of these purposes so that end users can interact with the model efficiently.</p><p id="9e85" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Given the incredible cost and intensive process required to pre-train a model, most organizations decide to leverage existing models in their GenAI use cases. To get started with pretraining, check out <a class="af ov" href="https://docs.mosaicml.com/projects/mcli/en/latest/pretraining/pretraining.html" rel="noopener ugc nofollow" target="_blank">Mosaic AI’s pretraining API</a>, this allows you to pretrain a Databricks DBRX model with different parameter sizes.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml qb"><img src="../Images/2d91cac4da86a88bc7ae611d90af3628.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3UuQ2Y_nsq2jLhTr6Ev4pQ.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Image by author. Overview of LLM and SLM pre-training.</figcaption></figure><h2 id="4d04" class="pc ne fq bf nf pd pe pf ni pg ph pi nl oi pj pk pl om pm pn po oq pp pq pr ps bk">Adding Knowledge with Continued Pre-Training (CPT)</h2><p id="63f1" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><strong class="ob fr">Overview: </strong>CPT is a type of fine-tuning that allows extends the knowledge of an existing model rather than training the entire model from scratch. The output of a model that’s gone through CPT will still predict the next token. In general it’s recommended that you use CPT then Instruction Fine-Tuning (IFT) this way you can extend the model’s knowledge first, then tune it to a particular task like following instructions or chat. If done in the reverse order, the model may forget instructions that it learned during the IFT phase.</p><p id="d7fb" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Pros</strong>:</p><ul class=""><li id="0384" class="nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou pt pu pv bk"><strong class="ob fr">No need for labeled training data</strong>: CPT does not require labeled training data. This is great if you have a lot of domain-specific or new information you want to teach the model in general. Since the output is still focused on next token prediction, the output from CPT is helpful if you want a text-completion model.</li><li id="e65b" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Faster and more cost effective than pre-training</strong>: CPT can be completed in hours or days using less GPUs than pre-training making it faster and cheaper!</li></ul><p id="da27" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Cons</strong>:</p><ul class=""><li id="4c89" class="nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou pt pu pv bk"><strong class="ob fr">Still comparatively expensive</strong>: CPT is significantly cheaper than pre-training, but can still be expensive and cost tens of thousands of dollars to train a model depending on the volume of data and number of GPUs required.</li><li id="529e" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Requires curated evaluations</strong>: Additionally, you will need to create your own evaluations to make sure the model is performing well in the new domain you are teaching it.</li><li id="f5bc" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Typically requires subsequent IFT</strong>: For most use cases, you would still need to perform IFT on the model once CPT finishes so that the final model can properly respond to questions or chats. This ultimately increases the time and cost until you have a model ready for use.</li></ul><p id="17c3" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Applications</strong>: For industries with <strong class="ob fr">highly domain specific content like healthcare or legal, CPT may be a great option for introducing new topics</strong> to the model. With tools like <a class="af ov" href="https://docs.mosaicml.com/projects/mcli/en/latest/finetuning/finetuning.html" rel="noopener ugc nofollow" target="_blank">Mosaic AI’s Fine-Tuning API</a> you can easily get started with CPT, all you need is a series of text files you want to use for training. For the CPT process, all the text files will be concatenated with a separation token between each of the documents, Mosaic handles the complexity behind the scenes for how these files get fed to the model for training.</p><p id="e6f2" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">As an example, let’s say we used CPT with a series of text files about responsible AI and AI policies. If I prompt the model to “Tell me three principles important to Responsible AI”, I would likely get a response with a high probability to follow the sentence I prompted like “I need to understand the key Responsible AI principles so I can train an effective model”. Although this response is related to my prompt, it does not directly answer the question. This demonstrates the need for IFT to refine the models instruction following capabilities.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml qc"><img src="../Images/3711cba668b5294676a57c2a5cc54cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rknz9t0B-R2XoFF2DBR2Cg.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Image by author inspired by <a class="af ov" href="https://arxiv.org/pdf/2402.01364" rel="noopener ugc nofollow" target="_blank">Continual Learning for Large Language Models: A Survey</a></figcaption></figure><h2 id="5454" class="pc ne fq bf nf pd pe pf ni pg ph pi nl oi pj pk pl om pm pn po oq pp pq pr ps bk">Tailoring Responses with Instruction Fine-Tuning (IFT)</h2><p id="b7ab" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><strong class="ob fr">Overview: </strong>IFT is used to teach a model how to perform a particular task. It typically requires thousands of examples and can be used for a specific purpose such as improving question answering, extracting key information, or adopting a certain tone.</p><p id="4375" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Pros</strong>:</p><ul class=""><li id="d39d" class="nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou pt pu pv bk"><strong class="ob fr">Speed and cost-effectiveness</strong>: IFT takes significantly less time to complete, this type of training can be achieved in minutes making it not only faster, but much cheaper compared to pre-training or CPT.</li><li id="90f0" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Task-specific customization</strong>: This is a great method to get tailored results out of the model by guiding it to respond in a particular tone, classify documents, revise certain documents, and more.</li></ul><p id="39a2" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Cons</strong>:</p><ul class=""><li id="ec8b" class="nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou pt pu pv bk"><strong class="ob fr">Requires labeled dataset</strong>: IFT needs labeled data to teach the model how it should behave. While there are many open-source datasets available, it may take time to properly create and label a dataset for your unique use case.</li><li id="d491" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Potential decrease in general capabilities</strong>: Introducing new skills through IFT may reduce the model’s performance on general tasks. If you are concerned about maintaining the model’s ability to generalize, you may want to include examples of general skills in your training and evaluation set this way you can measure performance on the general tasks as well as the new skill(s) you are teaching.</li></ul><p id="6c8f" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Applications</strong>: IFT helps the model <strong class="ob fr">perform particular tasks like question answering </strong>much better. Using the prompt “Tell me three principles important to Responsible AI”, a model that had undergone IFT would likely respond with an answer to the question like “Responsible AI is critical for ensuring the ethical use of models grounded in core principles like transparency, fairness, and privacy. Following responsible AI principles helps align the solution with broader societal values and ethical standards”. This response is more useful to the end user compared to a response that may come from a CPT or PT model only since it addresses the question directly.</p><p id="222a" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><em class="qd">Note that there are a variety of fine-tuning approaches and techniques designed to improve the overall model performance and reduce both time and cost associated with training.</em></p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div class="mk ml qe"><img src="../Images/dd9e755d6ab5ba83cf8ba9a63559b189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*Q1dSuU-bRGwuRrv5bBn24Q.png"/></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Image by author. Inspired by <a class="af ov" href="https://arxiv.org/pdf/2402.01364" rel="noopener ugc nofollow" target="_blank">Continual Learning for Large Language Models: A Survey</a></figcaption></figure><h2 id="6eaa" class="pc ne fq bf nf pd pe pf ni pg ph pi nl oi pj pk pl om pm pn po oq pp pq pr ps bk">Finding real-time or private information with Retrieval Augmented Generation (RAG)</h2><p id="bbad" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><strong class="ob fr">Overview: </strong>RAG enables language models to answer questions using information outside of their training data. In the RAG process, a user query triggers a retrieval of relevant information from a vector index, which is then integrated into a new prompt along with the original query to generate a response. This technique is one of the most common techniques used today due to its effectiveness and simplicity.</p><p id="18fa" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Pros</strong>:</p><ul class=""><li id="2073" class="nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou pt pu pv bk"><strong class="ob fr">Access to real-time information &amp; information beyond training data</strong>: RAG allows models to utilize query information from diverse and constantly updated sources like the internet or internal document datastores. Anything that can be stored in a vector index or retrieved via a plugin/tool, can be used in the RAG process.</li><li id="57b4" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Ease of implementation</strong>: RAG does not require custom training making it both cost-effective and straightforward to get started. It’s also a very well documented and researched area with many articles providing insights on how to improve responses from RAG systems.</li><li id="00a4" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Traceability and citations</strong>: All generated responses can include citations for which documents were used to answer the query making it easy for the user to verify the information and understand how the response was generated. Since you know exactly what information got sent to the model to answer the question, it’s easy to provide a traceable answers to the end user, and if needed the end user can look at the referenced documents for more information. In comparison, if you are querying a model directly, it’s difficult to know how it answered that question or what references were used to generate the response.</li></ul><p id="4821" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Cons</strong>:</p><ul class=""><li id="99f4" class="nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou pt pu pv bk"><strong class="ob fr">Context window limitations</strong>: The first major problem is the context windows of different models, some models like GPT-4 and 4o have 128k token context window, while the Llama-3 series is still only 8k tokens. With smaller context windows, you cannot pass as much information to the model to answer the question. As a result, it becomes more important to have robust chunking and chunk re-ranking techniques in place so you can retreive the right context and use this to respond to the user correctly.</li><li id="1854" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">The “Lost in the Middle Problem”</strong>: Even with longer context windows, there is a common “lost in the middle problem” where models tend to pay more attention to information at the beginning or end of the prompt, meaning that if the answer to the question lies in the middle of the context, the model may still answer incorrectly even when presented with all the information needed to answer the question. Similarly, the models might mix up information they’ve retrieved and answer the question only partially correct. For example, I’ve seen when asking a model to find information about two companies and return their point of view on AI, the model has on occasion mixed up the companies policies.</li><li id="f209" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Top K Retrieval Challenge</strong>: In typical RAG pipelines, only the top K documents (or chunks of text) related to the query are retrieved and sent to the model for a final response. This pattern yields better results when looking for specific details in a document corpus, but often fails to correctly answer exhaustive search based questions. For example, the prompt “give me all of the documents related to responsible AI” would need additional logic to keep searching through the vector index for all responsible AI documents instead of stopping after returning the first top K related chunks.</li><li id="f699" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Overly similar documents</strong>: If the vector index contains documents that are all semantically similar, it might be difficult for the model to retrieve the exact document relevant to the task. This is particularly true in specialized domains or domains with uniform language. This may not be a problem in a vector index where the content of all the documents is diverse, however, if you’re using RAG against an index on medical documents where all the language is very similar and not something a typical embedding model would be trained on, it might be harder to find the documents / answers you’re looking for.</li></ul><p id="6622" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Applications</strong>: Any use case involving <strong class="ob fr">question and answering over a set of documents will typically involve RAG</strong>. It’s a very practical way to get started with Generative AI and requires no additional model training. The emerging concept of AI Agents also tend to have at least one tool for RAG. Many agent implementations will have RAG based tools for different data sources. For example, an internal support agent might have access to an HR tool and IT support tool. In this set-up there could be a RAG component for both the HR and IT documents, each tool could have the same pipeline running behind the scenes, the only difference would be the document dataset.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml qf"><img src="../Images/d6de63bdf1ca0335007f71ee4341b209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rAKljiKAtaFDWo1zCCWCCg.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Image by author. Overview of the RAG process.</figcaption></figure><h2 id="9f08" class="pc ne fq bf nf pd pe pf ni pg ph pi nl oi pj pk pl om pm pn po oq pp pq pr ps bk">Improving the R in RAG by Fine-Tunning Embeddings</h2><p id="272f" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><strong class="ob fr">Overview: </strong>Fine-Tuning Embeddings can improve the retrieval component of RAG. The goal of fine-tuning embeddings is to push the vector embeddings further apart in the vector space, making them more different from one another and therefore easier to find the documents most relevant to the question.</p><p id="0920" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Pros</strong>:</p><ul class=""><li id="ae68" class="nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou pt pu pv bk"><strong class="ob fr">Generally cost-effective</strong>: Fine-tuning embeddings is comparatively inexpensive when considering other training methods.</li><li id="ddd7" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Domain-specific customization</strong>: This method can be a great option for distinguishing text in domains that the underlying embedding model was not as exposed to during training. For example, highly specific legal or health care documents may benefit from fine-tuning embeddings for those corpuses in their RAG pipeline.</li></ul><p id="d15e" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Cons</strong>:</p><ul class=""><li id="4a92" class="nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou pt pu pv bk"><strong class="ob fr">Requires labeled data &amp; often re-training</strong>: A labeled dataset is needed to fine-tune an embedding model. Additionally, you may need to continuously re-train the embedding model as you add additional information to your index.</li><li id="c03f" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Additional maintenance across indexes: </strong>Depending on how many data sources you’re querying you also might have to keep track of multiple sets of embedding models and their related data sources. It’s important to remember that whatever embedding model was used to embed the corpus of documents must be the same model used to embed the query when it’s time to retrieve relevant information. If you are querying against multiple indexes, each embedded using a different embedding model, then you’ll need to make sure that your models match at the time of retrieval.</li></ul><p id="0c48" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Applications</strong>: Fine-tuning embeddings is a <strong class="ob fr">great option if the traditional RAG approach is not effective because the documents in your index are too similar to one another</strong>. By fine-tuning the embeddings you can teach the model to differentiate better between domain specific concepts and improve your RAG results.</p><h2 id="05f3" class="pc ne fq bf nf pd pe pf ni pg ph pi nl oi pj pk pl om pm pn po oq pp pq pr ps bk">Talking to Models with Prompt Engineering</h2><p id="bb17" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><strong class="ob fr">Overview: </strong>Prompt engineering is the most common way to interact with Generative AI models, it is merely sending a message to the model that’s designed to get the output you want. It can be as simple as “Tell me a story about a German Shepherd” or it can be incredibly complicated with particular details regarding what you’d like the model to do.</p><p id="769b" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Pros</strong>:</p><ul class=""><li id="339d" class="nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou pt pu pv bk"><strong class="ob fr">Immediate results</strong>: Experimenting with different prompts can be done anytime you have access to a language model and results are returned in seconds (or less)! As soon as the idea hits, you can begin working on refining a prompt until the model gives the desired response.</li><li id="e3e6" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">High performance on general tasks</strong>: Prompt engineering alone works great for generic tasks that do not require any retrieval of business specific information or real-time information.</li><li id="1270" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Compatibility with other techniques</strong>: It will work with models that have been pre-trained, continuously pre-trained, or fine-tuned, and it can be used in conjunction with RAG making it the most used and versatile of the approaches.</li></ul><p id="4b06" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Cons</strong>:</p><ul class=""><li id="3787" class="nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou pt pu pv bk"><strong class="ob fr">Limited capability on its own</strong>: Prompt engineering alone is typically not enough to get the model to perform how you want. In most cases, people want the model to interact with some external data whether it’s a document database, API call, or SQL table, all of which will need to combine prompt engineering with RAG or other specialized tool calling.</li><li id="ac09" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><strong class="ob fr">Precision challenges</strong>: Writing the perfect prompt can be challenging and often requires a lot of tweaking until the model performs as intended. The prompt that works great with one model might fail miserably with another, requiring lots of iterations and experimentation across many models and prompt variations.</li></ul><p id="9fff" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><strong class="ob fr">Applications</strong>: Prompt Engineering will be <strong class="ob fr">used in combination with all of the aforementioned techniques to produce the intended response</strong>. There are many different techniques for prompt engineering to help steer the model in the right direction. For more information on these techniques I recommend this <a class="af ov" href="https://developer.microsoft.com/en-us/reactor/events/22001/" rel="noopener ugc nofollow" target="_blank">Prompt Engineering Guide from Microsoft</a> they give a variety of examples from Chain-of-Thought prompting and beyond.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml qg"><img src="../Images/4d0ece3e50b7decdc6a47529331eb595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tfpe9v6n8DcpLmZFx1ggpA.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Image by author. Overview of Prompt Engineering Process.</figcaption></figure><h1 id="da1a" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Conclusion</h1><p id="2de1" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Generative AI technology is changing and improving everyday. Most applications will require leveraging a variety of the techniques described in this article. Getting started with existing language models that have been fine-tuned for instruction or chat capabilities and focusing on prompt engineering and RAG is a great place to start! From here finding more tailored use cases that require fine-tuning/instruction fine-tuning can provide even greater benefits.</p><p id="e195" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Looking ahead, AI agents offer a promising way to take advantage of the latest advancements in both closed and open-source models that have been pre-trained on tons of public data and fine-tuned for chat/instruction following. When given the right tools, these agents can perform incredible tasks on your behalf from information retrieval with RAG to helping plan company events or vacations.</p><p id="1fb6" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Additionally, we can expect to see a proliferation of more domain specific models as organizations with lots of specialized data begin pre-training their own models. For instance, companies like <a class="af ov" href="https://www.harvey.ai/" rel="noopener ugc nofollow" target="_blank">Harvey</a> are already developing tailored AI solutions that can handle the unique demands of the legal industry. This trend will likely continue, leading to highly specialized models that deliver even more precise and relevant results in various fields.</p><p id="ac8e" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">By combining the strengths of different AI techniques and leveraging the power of AI agents and domain-specific models, organizations can unlock the full potential of Generative AI.</p><h2 id="bd31" class="pc ne fq bf nf pd pe pf ni pg ph pi nl oi pj pk pl om pm pn po oq pp pq pr ps bk">Additional References</h2><ul class=""><li id="854f" class="nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou pt pu pv bk"><a class="af ov" href="https://www.gartner.com/en/newsroom/press-releases/2024-05-07-gartner-survey-finds-generative-ai-is-now-the-most-frequently-deployed-ai-solution-in-organizations" rel="noopener ugc nofollow" target="_blank">Gartner Survey Finds Generative AI Is Now the Most Frequently Deployed AI Solution in Organizations</a></li><li id="06ae" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><a class="af ov" href="https://arxiv.org/abs/2308.10792v5" rel="noopener ugc nofollow" target="_blank">Instruction Tuning for Large Language Models: A Survey</a></li><li id="2aca" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><a class="af ov" href="https://arxiv.org/pdf/2308.04014" rel="noopener ugc nofollow" target="_blank">Continual Pre-Training of Large Language Models: How to (re)warm your model?</a></li><li id="e727" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><a class="af ov" href="https://arxiv.org/abs/2402.01364" rel="noopener ugc nofollow" target="_blank">Continual Learning for Large Language Models: A Survey</a></li><li id="cd2d" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><a class="af ov" href="https://arxiv.org/abs/2307.03172" rel="noopener ugc nofollow" target="_blank">Lost in the Middle: How Language Models Use Long Contexts</a></li><li id="2560" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><a class="af ov" href="https://docs.mosaicml.com/projects/mcli/en/latest/finetuning/finetuning.html" rel="noopener ugc nofollow" target="_blank">Mosaic AI Fine-Tuning Documentation</a></li><li id="077b" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><a class="af ov" href="https://developer.microsoft.com/en-us/reactor/events/22001/" rel="noopener ugc nofollow" target="_blank">Prompt Engineering Guide / Recording from Microsoft</a></li><li id="30a2" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><a class="af ov" href="https://docs.anthropic.com/en/docs/prompt-engineering" rel="noopener ugc nofollow" target="_blank">Prompt Engineering Guide from Anthropic</a></li><li id="2556" class="nz oa fq ob b go pw od oe gr px og oh oi py ok ol om pz oo op oq qa os ot ou pt pu pv bk"><a class="af ov" href="https://www.cudocompute.com/blog/what-is-the-cost-of-training-large-language-models" rel="noopener ugc nofollow" target="_blank">What is the Cost of Training Large Language Models from Cudo Compute</a></li></ul><p id="de71" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk"><em class="qd">Interested in discussing further or collaborating? Reach out on </em><a class="af ov" href="https://www.linkedin.com/in/tula-masterman/" rel="noopener ugc nofollow" target="_blank"><em class="qd">LinkedIn</em></a><em class="qd">!</em></p></div></div></div></div>    
</body>
</html>