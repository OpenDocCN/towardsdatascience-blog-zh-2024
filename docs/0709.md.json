["```py\nP( Yn ∣ X0, X1, ... Xn-1, θ )\n```", "```py\nclass KohonenSOM():\n    \"\"\"\n    The code is developed based on the following article:\n    http://www.ai-junkie.com/ann/som/som1.html\n\n    The vector and matrix operations are developed using PyTorch Tensors.\n    \"\"\"\n    def __init__( ... )\n    ...\n    def find_topk_best_matching_units( self, data_points : torch.Tensor, topk : int = 1 ) -> List[ List[ int ] ] :\n        if len( data_points.size() ) == 1:\n            #batching \n            data_points = data_points.view( 1, data_points.shape[0] )\n\n        topk = int( topk )\n\n        distances = self.dist_evaluator( data_points, self.lattice_node_weights )\n\n        topk_best_matching_unit_indexes = torch.topk( distances, topk, dim=1, largest=False ).indices\n        topk_best_matching_units = []\n\n        for i in range( data_points.shape[0] ):\n            best_matching_unit_indexes = topk_best_matching_unit_indexes[i]\n            best_matching_units = [ self.lattice_coordinates[ bmu_index.item() ].tolist() for bmu_index in best_matching_unit_indexes ]\n            topk_best_matching_units.append( best_matching_units )\n\n        return topk_best_matching_units\n```", "```py\nclass SOMBasedVectorIndexer():\n    ...\n\n    def train_n_gen_indexes( \n                                self, input_vectors : torch.Tensor, \n                                train_epochs : int = 100 \n                           ):\n        if self.generated_indexes:\n            print( \"WARNING: Indexes were already generated. Ignoring the request...\" )\n            return\n\n        self.som.train( input_vectors, train_epochs )\n\n        topk_bmu_indexes = self.som.find_topk_best_matching_units( input_vectors, topk = self.topk_bmu_for_indexing )\n\n        for idx in tqdm( range( len( topk_bmu_indexes ) ), desc=\"SOM-Based Indexed Vectors\"  ):\n            bmu_indexes = topk_bmu_indexes[ idx ]\n\n            for bmu_index in bmu_indexes:\n                bmu_index_key = tuple( bmu_index )\n                idx_set = self.som_node_idx_map.get( bmu_index_key, set() )\n                idx_set.add( idx )\n                self.som_node_idx_map[ bmu_index_key ] = idx_set\n\n        self.generated_indexes = True\n```", "```py\nimport openai\nfrom openai.embeddings_utils import get_embedding\n...\nfrom vector_encoder_parent import VectorEncoder\n...\n\nclass OpenAIEmbeddingsVectorEncoder( VectorEncoder ):\n    def __init__( ... )\n    ...\n    def encode_batch( self, list_of_text : List[ str ] ) -> torch.Tensor :\n        if list_of_text == None or len( list_of_text ) == 0:\n            raise ValueError( \"ERROR: Required list_of_text is None or empty\" )\n\n        list_of_text = [ str( text ) for text in list_of_text ]\n\n        openai.api_key = self.openai_key\n        response = openai.Embedding.create(\n                                            input = list_of_text,\n                                            engine = self.vector_encoder_id\n                                          )\n\n        embeddings = [ data[\"embedding\"] for data in response[\"data\"] ] \n        vectors = torch.tensor( embeddings, dtype=torch.float )\n        return vectors\n```", "```py\nimport requests\nimport pandas as pd\nfrom dateutil.parser import parse\n...\nclass WikiEventsDataSource():\n    ...\n    def fetch_n_prepare_data( self ):\n        if self.fetched:\n            print( \"WARNING: Wiki events for the specified years already fetched. Ignoring the request...\" )\n            return\n\n        main_df = pd.DataFrame()\n\n        for year in self.event_years_to_fetch:\n            wiki_api_params = {\n                                \"action\": \"query\", \n                                \"prop\": \"extracts\",\n                                \"exlimit\": 1,\n                                \"titles\": year,\n                                \"explaintext\": 1,\n                                \"formatversion\": 2,\n                                \"format\": \"json\"\n                              }\n\n            response = requests.get( \"https://en.wikipedia.org/w/api.php\", params=wiki_api_params )\n            response_dict = response.json()\n\n            df = pd.DataFrame()\n            df[ \"text\" ] = response_dict[\"query\"][\"pages\"][0][\"extract\"].split(\"\\n\")\n            df = self.__clean_df__( df, year )\n\n            main_df = pd.concat( [ main_df, df ] )\n\n        self.df = main_df.reset_index(drop=True)\n        self.fetched = True\n```", "```py\n...\nfrom vector_encoder_parent import VectorEncoder\nfrom vector_indexer import SOMBasedVectorIndexer\n\nclass SOM_Based_RAG_Util():\n    ...\n    def load_n_vectorize_data( self, data_source ):\n        if self.data_loaded_n_vectorized:\n            print( \"WARNING: Data already loaded and vectorized. Ignoring the request...\" )\n            return\n\n        data_source.fetch_n_prepare_data()\n        self.df = data_source.get_data()\n\n        vectors = None\n\n        for i in tqdm( range(0, len(self.df), self.vectorize_batch_size ), desc=\"Vectorized Data Batch\" ):\n            list_of_text = self.df.iloc[ i:i+self.vectorize_batch_size ][\"text\"].tolist()\n            batch_encoded_vectors = self.vector_encoder.encode_batch( list_of_text )\n\n            if vectors == None:\n                vectors = batch_encoded_vectors\n            else:\n                vectors = torch.cat( [ vectors, batch_encoded_vectors], dim=0 )\n\n        self.vectors = vectors.to( self.device )\n        self.data_loaded_n_vectorized = True\n```", "```py\ndef train_n_index_data_vectors( self, train_epochs : int = 100  ):\n        if not self.data_loaded_n_vectorized:\n            raise ValueError( \"ERROR: Data not loaded and vectorized.\" )\n\n        if self.data_vectors_indexed:\n            print( \"WARNING: Data vectors already indexed. Ignoring the request...\" )\n            return\n\n        self.vector_indexer.train_n_gen_indexes( self.vectors, train_epochs )\n        self.data_vectors_indexed = True\n```", "```py\ndef find_semantically_similar_data( self, query: str, sim_evaluator = None, sim_threshold : float = 0.8  ):\n        if not self.data_vectors_indexed:\n            raise ValueError( \"ERROR: Data vectors not indexed.\" )\n\n        if query == None or len( query.strip() ) == 0:\n            raise ValueError( \"ERROR: Required query text is not specified.\" )\n\n        sim_threshold = float( sim_threshold )\n\n        if sim_evaluator == None:\n            sim_evaluator = nn.CosineSimilarity(dim=0, eps=1e-6)\n\n        query_vector = self.vector_encoder.encode( query )\n        query_vector = query_vector.view( self.vector_encoder.get_encoded_vector_dimensions() )\n        query_vector = query_vector.to( self.device )\n\n        nearest_indexes = self.vector_indexer.find_nearest_indexes( query_vector )\n        nearest_indexes = nearest_indexes[0]\n\n        sim_scores = []\n\n        for idx in nearest_indexes:\n            data_vector = self.vectors[ idx ]\n            data_vector = data_vector.view( self.vector_encoder.get_encoded_vector_dimensions() )\n\n            sim_score = sim_evaluator( query_vector, data_vector )\n\n            if sim_score >= sim_threshold:\n                sim_score_tuple = (idx, sim_score.item() )\n                sim_scores.append( sim_score_tuple )\n\n        sim_scores.sort( key = lambda x: x[1], reverse=True )\n\n        semantically_similar_data = [ \n                                        { \n                                            'text': self.df[ 'text' ][ idx ],\n                                            'sim_score' : sim_score\n                                        } for idx, sim_score in sim_scores\n                                    ]\n\n        return semantically_similar_data\n```", "```py\nfrom abc import ABC, abstractmethod\nimport torch\nimport math\n\nclass QuestionAnswerChatBot( ABC ):\n    ...\n    def find_answer_to_question( self, question : str, sim_threshold = 0.68, max_new_tokens : int = 5 ):\n        if question == None or len( question.strip() ) == 0:\n            raise ValueError( \"ERROR: Required question is not specified\" )\n\n        sim_threshold = float( sim_threshold )\n        max_new_tokens = int( max_new_tokens )\n\n        qa_instruction = self.get_qa_instruction( question, sim_threshold = sim_threshold )\n\n        answer_text = self.__get_answer_text__( qa_instruction, max_new_tokens = max_new_tokens )\n        answer_text = self.__clean_answer_text__( qa_instruction, answer_text )\n\n        return answer_text\n    ...\n    def __qa_template__( self ):\n        qa_template = \"\"\"Context: \n\n    {}\n\n    ---\n\n    Question: {}\n    Answer:\"\"\"\n        return qa_template\n```", "```py\nimport openai\nimport tiktoken\nfrom qa_chatbot import QuestionAnswerChatBot\n\nclass OpenAIQuestionAnswerChatBot( QuestionAnswerChatBot ):\n    ...\n    def __get_answer_text__( self, qa_instruction : str, max_new_tokens : int = 5 ) -> str :\n        openai.api_key = self.openai_key\n\n        basic_answer = openai.Completion.create(\n                                                    model = self.openai_model_name,\n                                                    prompt = qa_instruction, \n\n                                               )\n\n        answer_text = basic_answer[ \"choices\" ][0][ \"text\" ]\n        return answer_text\n\n    def __token_count__( self, text : str ):    \n        return len( self.tokenizer.encode( text ) )\n```", "```py\nsample_questions = [\n                        \"Who won the 2022 soccer world cup?\",\n                        \"When did Sweden join NATO?\",\n                        \"Who joined NATO in 2023?\",\n                        \"Who joined NATO in 2024?\",\n                        \"Which is the 31st member of NATO?\",\n                        \"Which is the 32nd member of NATO?\",\n                        \"Who won the Cricket World Cup in 2023?\",\n                        \"Who defeated India in Cricket World Cup final in 2023?\",\n                        \"Name the former prime minister of Japan that was assassinated in 2022?\",\n                        \"When did Chandrayaan-3 land near the south pole of the Moon?\",\n                        \"Where did Chandrayaan-3 land on the Moon?\",\n                        \"Who acquired Twitter in 2022?\",\n                        \"Who owns Twitter?\",\n                        \"Who acquired Activision Blizzard in 2023?\"\n                   ]\n```", "```py\nopenai_vector_encoder_id = \"text-embedding-ada-002\"\nopenai_encoded_vector_dimensions = 1536\nopenai_tokenizer_name = \"cl100k_base\" \nopenai_model_name = \"gpt-3.5-turbo-instruct\"\n\nvector_encoder = OpenAIEmbeddingsVectorEncoder( openai_encoded_vector_dimensions, openai_vector_encoder_id, openai_key )\n\nevent_years_to_fetch = [ 2022, 2023, 2024 ]\ndata_source = WikiEventsDataSource( event_years_to_fetch  )\n...\nsom_driven_rag_util = SOM_Based_RAG_Util( \n                                            vector_encoder = vector_encoder,\n                                            som_lattice_height = 20,\n                                            som_lattice_width = 30,\n                                            learning_rate = 0.3,\n                                            topk_bmu_for_indexing = 10,\n                                            device = device\n                                        )\n...\nopenai_chatbot = OpenAIQuestionAnswerChatBot( \n                                                vector_db_util = som_driven_rag_util,\n                                                openai_tokenizer_name = openai_tokenizer_name,\n                                                openai_model_name = openai_model_name,\n                                                openai_key = openai_key,\n                                                question_input_max_token_count = 100,\n                                                context_trim_percent = 0.1,\n                                                device = device\n                                            )\n```"]