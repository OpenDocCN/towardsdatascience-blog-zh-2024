- en: The Needle In a Haystack Test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-needle-in-a-haystack-test-a94974c1ad38?source=collection_archive---------1-----------------------#2024-02-15](https://towardsdatascience.com/the-needle-in-a-haystack-test-a94974c1ad38?source=collection_archive---------1-----------------------#2024-02-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/5d3dd87eaedfaa9810c9a5f571ce002f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using Dall-E 3
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the performance of RAG systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://aparnadhinak.medium.com/?source=post_page---byline--a94974c1ad38--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page---byline--a94974c1ad38--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a94974c1ad38--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a94974c1ad38--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page---byline--a94974c1ad38--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a94974c1ad38--------------------------------)
    ·9 min read·Feb 15, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*My thanks to* [*Greg Kamradt*](https://twitter.com/GregKamradt) *and* [*Evan
    Jolley*](https://www.linkedin.com/in/evanjolley/) *for their contributions to
    this piece*'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-augmented generation (RAG) underpins many of the LLM applications
    in the real world today, from companies generating headlines to solo developers
    solving problems for small businesses.
  prefs: []
  type: TYPE_NORMAL
- en: '[RAG evaluation](https://arize.com/blog-course/rag-evaluation/), therefore,
    has become a critical part in the development and deployment of these systems.
    One new innovative approach to this challenge is the “Needle in a Haystack’’ test,
    first outlined by [Greg Kamradt](https://twitter.com/GregKamradt) in [this X post](https://twitter.com/GregKamradt/status/1722386725635580292?lang=en)
    and discussed in detail on his YouTube [here](https://www.youtube.com/watch?v=KwRRuiCCdmc).
    This test is designed to evaluate the performance of RAG systems across different
    sizes of context. It works by embedding specific, targeted information (the “needle”)
    within a larger, more complex body of text (the “haystack”). The goal is to assess
    an LLM’s ability to identify and utilize this specific piece of information amidst
    a vast amount of data.'
  prefs: []
  type: TYPE_NORMAL
- en: Often in RAG systems, the context window is absolutely overflowing with information.
    Large pieces of context returned from a vector database are cluttered together
    with instructions for the language model, templating, and anything else that might
    exist in the prompt. The Needle in a Haystack evaluation tests the capabilities
    of an LLM to pinpoint specifics in amongst this mess. Your RAG system might do
    a stellar job of retrieving the most relevant context, but what use is this if
    the granular specifics within are overlooked?
  prefs: []
  type: TYPE_NORMAL
- en: We ran this test multiple times across several major language models. Let’s
    take a closer look at the process and overall results.
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not all LLMs are the same. Models are trained with different objectives and
    requirements in mind. For example, Anthropic’s Claude is known for being a slightly
    wordier model, which often stems from its objective to not make unsubstantiated
    claims.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minute differences in prompts can lead to drastically different outcomes**
    across models due to this fact. Some LLMs need more tailored prompting to perform
    well at specific tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building on top of LLMs — especially when those models are connected to
    private data — it is necessary to **evaluate retrieval and model performance throughout
    development and deployment**. Seemingly insignificant differences can lead to
    incredibly large differences in performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Needle In a Haystack Test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Needle in a Haystack test was first used to evaluate the recall of two
    popular LLMs, OpenAI’s ChatGPT-4 and Anthropic’s Claude 2.1\. An out of place
    statement, “The best thing to do in San Francisco is eat a sandwich and sit in
    Dolores Park on a sunny day,” was placed at varying depths within snippets of
    varying lengths taken from [essays](https://paulgraham.com/articles.html) by Paul
    Graham, similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fda17ae9412103a6f11920bd500ff0e6.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1: About 120 tokens and 50% depth* | Image by [Greg Kamradt](https://twitter.com/GregKamradt)
    on [X](https://twitter.com/GregKamradt/status/1722386725635580292), used here
    with author’s permission'
  prefs: []
  type: TYPE_NORMAL
- en: 'The models were then prompted to answer what the best thing to do in San Francisco
    was, only using the provided context. This was then repeated for different depths
    between 0% (top of document) and 100% (bottom of document) and different context
    lengths between 1K tokens and the token limit of each model (128k for GPT-4 and
    200k for Claude 2.1). The below graphs document the performance of these two models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6016bcb61c2d84a0d250f1bd99bc4857.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: ChatGPT-4’s performance | Image by [Greg Kamradt](https://twitter.com/GregKamradt)
    on [X](https://twitter.com/GregKamradt/status/1722386725635580292), used here
    with author’s permission'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, ChatGPT’s performance begins to decline at <64k tokens and sharply
    falls at 100k and over. Interestingly, if the “needle” is positioned towards the
    beginning of the context, the model tends to overlook or “forget” it — whereas
    if it’s placed towards the end or as the very first sentence, the model’s performance
    remains solid.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32ac9bfcaf7e2b20842a4d4b61596967.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Claude 2.1’s performance | | Image by [Greg Kamradt](https://twitter.com/GregKamradt)
    on [X](https://twitter.com/GregKamradt/status/1722386725635580292), used here
    with author’s permission'
  prefs: []
  type: TYPE_NORMAL
- en: For Claude, initial testing did not go as smoothly, finishing with an overall
    score of 27% retrieval accuracy. A similar phenomenon was observed with performance
    declining as context length increased, performance generally increasing as the
    needle was hidden closer to the bottom of the document, and 100% accuracy retrieval
    if the needle was the first sentence of the context.
  prefs: []
  type: TYPE_NORMAL
- en: Anthropic’s Response
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In response to these findings, Anthropic published an [article](https://www.anthropic.com/news/claude-2-1-prompting)
    detailing their re-run of this test with a few key changes.
  prefs: []
  type: TYPE_NORMAL
- en: First, they changed the needle to more closely mirror the topic of the haystack.
    Claude 2.1 was trained to “not [answer] a question based on a document if it doesn’t
    contain enough information to justify that answer.” Thus, Claude may well have
    correctly identified eating a sandwich in Dolores Park as the best thing to do
    in San Francisco. However, along with an essay about doing great work, this small
    piece of information may have appeared unsubstantiated. This could have led to
    a verbose response explaining that Claude cannot confirm that eating a sandwich
    is the best thing to do in San Francisco or an omission of the detail entirely.
    When re-running the experiments, researchers at Anthropic found that changing
    the needle to a small detail originally mentioned in the essay led to significantly
    increased outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Second, a small edit was made to the prompt template used to query the model.
    A single line — *here is the most relevant sentence in the context* — was added
    to the end of the template, directing the model to simply return the most relevant
    sentence provided in the context. Similar to the first, this change allows us
    to circumvent the model’s propensity to avoid unsubstantiated claims by directing
    it to simply return a sentence rather than make an assertion.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'These changes led to a significant jump in Claude’s overall retrieval accuracy:
    from 27% to 98%! Finding this initial research fascinating, we decided to run
    our own set of experiments using the Needle in a Haystack test.'
  prefs: []
  type: TYPE_NORMAL
- en: Further Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In conducting a new series of tests, we implemented several modifications to
    the original experiments. The needle we used was a random number that changed
    each iteration, eliminating the possibility of caching. Additionally, we used
    our open source Phoenix evals [library](https://docs.arize.com/phoenix/llm-evals/running-pre-tested-evals)
    (full disclosure: I lead the team that built Phoenix) to reduce the testing time
    and use rails to search directly for the random number in the output, cutting
    through wordiness that would decrease a retrieval score. Finally, we considered
    the negative case where the system fails to retrieve the results, marking it as
    unanswerable. We ran a separate test for this negative case to assess how well
    the system recognizes when it can’t retrieve the data. These modifications allowed
    us to conduct a more rigorous and comprehensive evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The updated tests were run across several different configurations using four
    different large language models: ChatGPT-4, Claude 2.1 (with and without the aforementioned
    change to the prompt that Anthropic suggested), and Mistral AI’s [Mixtral-8X7B](https://arize.com/blog/mistral-ai)-v0.1
    and 7B Instruct. Given that small nuances in prompting can lead to vastly different
    results across models, we used several prompt templates in the attempt to compare
    these models performing at their best. The simple template we used for ChatGPT
    and Mixtral was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For Claude, we tested both previously discussed templates.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: All code run to complete these tests can be found in [this GitHub repository](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack).
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/3eddc04a8685a9a9df55ec5fed5f02c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Comparison of GPT-4 results between the initial research (Run #1)
    and our testing (Run #2) | Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed7e0c7a1308960520963cbb485fa689.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Comparison of Claude 2.1 (without prompting guidance) results between
    Run #1 and Run #2 | Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our results for ChatGPT and Claude (without prompting guidance) did not stray
    far from Mr. Kamradt’s findings, and the generated graphs appear relatively similar:
    the upper right (long context, needle near the beginning of the context) is where
    LLM information retrieval sufferers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c1a96ba84a13180a3a67282298894c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Comparison of Claude 2.1 results with and without prompting guidance'
  prefs: []
  type: TYPE_NORMAL
- en: Although we were not able to replicate Anthropic’s results of 98% retrieval
    accuracy for Claude 2.1 with prompting guidance, we did see a significant decrease
    in total misses when the prompt was updated (from 165 to 74). This jump was achieved
    by simply adding a 10 word instruction to the end of the existing prompt, highlighting
    that small differences in prompts can have drastically different outcomes for
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5871e33962023634315d861adec2a6de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Mixtral results | Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: Last but certainly not least, it is interesting to see just how well Mixtral
    performed at this task despite these being by far the smallest models tested.
    The Mixture of Experts (MOEs) model was far better than 7B-Instruct, and we are
    finding that MOEs do much better for retrieval evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Needle in a Haystack test is a clever way to quantify an LLM’s ability to
    parse context to find needed information. [Our research](https://arize.com/blog-course/the-needle-in-a-haystack-test-evaluating-the-performance-of-llm-rag-systems/)
    concluded with a few main takeaways. First, ChatGPT-4 is the industry’s current
    leader in this arena along with many other evaluations that we and others have
    carried out. Second, at first Claude 2.1 seemed to underperform this test, but
    with tweaks to the prompt structure the model showed significant improvement.
    Claude is a bit wordier than some other models, and taking extra care to direct
    it can go a long way in terms of results. Finally, Mixtral MOE greatly outperformed
    our expectations, and we are excited to see Mixtral models continually overperform
    expectations.
  prefs: []
  type: TYPE_NORMAL
