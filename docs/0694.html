<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>CausalLM Part 2: Fine-Tuning a Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>CausalLM Part 2: Fine-Tuning a Model</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/causallm-part-2-finetuning-a-model-3fdb4d9bd936?source=collection_archive---------13-----------------------#2024-03-14">https://towardsdatascience.com/causallm-part-2-finetuning-a-model-3fdb4d9bd936?source=collection_archive---------13-----------------------#2024-03-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="6222" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">3 ways to fine-tune a CausalLM model on chat data</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://tlebryk.medium.com/?source=post_page---byline--3fdb4d9bd936--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Theo Lebryk" class="l ep by dd de cx" src="../Images/c2e0d606f4a99831fad5575f59848544.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*FQIW0ZdFzdQBfDWgWzsh1Q.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3fdb4d9bd936--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://tlebryk.medium.com/?source=post_page---byline--3fdb4d9bd936--------------------------------" rel="noopener follow">Theo Lebryk</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3fdb4d9bd936--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/e96c0450be2b342ca68c996282211eae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gAEjQVVfPRoOCG37"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">In this tutorial, we’ll be fine-tuning a CausalLM model to do simple translation. Photo by <a class="af nc" href="https://unsplash.com/@ventanamedia?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Rob Wilson</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="dd58" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the<a class="af nc" rel="noopener" target="_blank" href="/training-causallm-models-part-1-what-actually-is-causallm-6c3efb2490ec"> last post</a>, we talked about what CausalLM is and how Hugging Face expects data to be formatted. In this post, we’re going to walk through an abridged notebook with three ways to format the data to fine-tune a model. The first is a straightforward approach building on the intuition from the previous post simply copying input_ids into labels. The second approach utilizes masking to learn select parts of the text. The third approach uses a separate library, <a class="af nc" href="https://huggingface.co/docs/trl/main/en/index" rel="noopener ugc nofollow" target="_blank">TRL</a>, so that we don’t have to manually mask the data.</p><p id="5dbd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I’ll leave out some function definitions to keep it readable, so it’s best to reference <a class="af nc" href="https://github.com/tlebryk/CausalLM/blob/main/finetune_casuallm.ipynb" rel="noopener ugc nofollow" target="_blank">the full noteboo</a>k to get all the code.</p><h2 id="a3ae" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Fine-tuning with labels copied from input ids</h2><p id="4953" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">We’re going to be using <a class="af nc" href="https://huggingface.co/bigscience/bloom-560m" rel="noopener ugc nofollow" target="_blank">Bloom-560m</a>, a multilingual model which is small enough that we can fine-tune it on a standard laptop.</p><pre class="mm mn mo mp mq oz pa pb bp pc bb bk"><span id="facb" class="pd oa fq pa b bg pe pf l pg ph">model_name = "bigscience/bloom-560m"<br/>tokenizer = AutoTokenizer.from_pretrained(<br/>    model_name, trust_remote_code=True, padding_side="right"<br/>)  # padding side should be right for CausalLM models<br/># overfit to 5 made up examples<br/>str1 = '\n\n### Human: How do you say "dog" in Spanish?\n\n### Assistant: perro'<br/>str2 = '\n\n### Human: How do you say "water" in Spanish?\n\n### Assistant: agua'<br/>str3 = '\n\n### Human: How do you say "hello" in Spanish?\n\n### Assistant: hola'<br/>str4 = '\n\n### Human: How do you say "tree" in Spanish?\n\n### Assistant: árbol'<br/>str5 = '\n\n### Human: How do you say "mother" in Spanish?\n\n### Assistant: madre'<br/>train_data = {<br/>    "text": [str1, str2, str3, str4, str5],<br/>}<br/>dataset_text = Dataset.from_dict(train_data)<br/><br/># to test if we learn how to generate an unknown word. <br/>holdout_str = (<br/>    '\n\n### Human: How do you say "day" in Spanish?\n\n### Assistant:&lt;s&gt;'  # día<br/>)<br/>device = "cuda" if torch.cuda.is_available() else "cpu"<br/>holdout_input = tokenizer(holdout_str, return_tensors="pt").to(device)</span></pre><p id="833b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s start by doing some preprocessing. We’re going to add some special tokens, namely “end of sequence” (eos) and “beginning of sequence“ (bos). These special tokens can be helpful for the model to know when it’s supposed to start and stop generating text.</p><pre class="mm mn mo mp mq oz pa pb bp pc bb bk"><span id="e0ca" class="pd oa fq pa b bg pe pf l pg ph">INSTRUCTION_TEMPLATE_BASE = "\n\n### Human:"<br/>RESPONSE_TEMPLATE_BASE = "\n\n### Assistant:"<br/>def add_special_tokens(<br/>    example: Dict,<br/>    tokenizer: PreTrainedTokenizerBase,<br/>) -&gt; Dict:<br/>    # add eos_token before human text and bos_token before assistant text<br/>    example["text"] = (<br/>        example["text"]<br/>        .replace(<br/>            INSTRUCTION_TEMPLATE_BASE, tokenizer.eos_token + INSTRUCTION_TEMPLATE_BASE<br/>        )<br/>        .replace(RESPONSE_TEMPLATE_BASE, RESPONSE_TEMPLATE_BASE + tokenizer.bos_token)<br/>    )<br/>    if not example["text"].endswith(tokenizer.eos_token):<br/>        example["text"] += tokenizer.eos_token<br/>    # Remove leading EOS tokens<br/>    while example["text"].startswith(tokenizer.eos_token):<br/>        example["text"] = example["text"][len(tokenizer.eos_token) :]<br/>    return example<br/><br/>dataset_text = dataset_text.map(lambda x: add_special_tokens(x, tokenizer))<br/>print(f"{dataset_text=}")<br/>print(f"{dataset_text[0]=}")<br/>&gt;&gt;&gt; dataset_text=Dataset({<br/>    features: ['text'],<br/>    num_rows: 5<br/>})<br/>&gt;&gt;&gt; dataset_text[0]={'text': '\n\n### Human: How do you say "dog" in Spanish?\n\n### Assistant:&lt;s&gt; perro&lt;/s&gt;'}</span></pre><p id="fb0b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, we’re going to do what we learned last session: create an input with a labels key copied from input_ids.</p><pre class="mm mn mo mp mq oz pa pb bp pc bb bk"><span id="4837" class="pd oa fq pa b bg pe pf l pg ph"># tokenize the text<br/>dataset = dataset_text.map(<br/>    lambda example: tokenizer(example["text"]), batched=True, remove_columns=["text"]<br/>)<br/># copy the input_ids to labels<br/>dataset = dataset.map(lambda x: {"labels": x["input_ids"]}, batched=True)<br/>print(f"{dataset=}")<br/>print(f"{dataset[0]['input_ids']=}")<br/>print(f"{dataset[0]['labels']=}")<br/>&gt;&gt;&gt; dataset=Dataset({<br/>    features: ['input_ids', 'attention_mask', 'labels'],<br/>    num_rows: 5<br/>})<br/>&gt;&gt;&gt; dataset[0]['input_ids']=[603, 105311, 22256, 29, 7535, 727, 1152, 5894, 20587, 744, 5, 361, 49063, 7076, 105311, 143005, 29, 1, 82208, 2]<br/>&gt;&gt;&gt; dataset[0]['labels']=[603, 105311, 22256, 29, 7535, 727, 1152, 5894, 20587, 744, 5, 361, 49063, 7076, 105311, 143005, 29, 1, 82208, 2]</span></pre><p id="96a5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To start, labels and input_ids are identical. Let’s see what happens when we train a model like that.</p><pre class="mm mn mo mp mq oz pa pb bp pc bb bk"><span id="a06d" class="pd oa fq pa b bg pe pf l pg ph"># training code inspired by<br/>#https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html<br/>model = load_model(model_name)<br/>output_dir = "./results"<br/># How many times to iterate over the entire dataset<br/>num_train_epochs = 15<br/># We're not aligning the sequence length (ie padding or truncating)<br/># so batch training won't work for our toy example.<br/>per_device_train_batch_size = 1<br/><br/><br/>training_arguments = TrainingArguments(<br/>    output_dir=output_dir,<br/>    num_train_epochs=num_train_epochs,<br/>    per_device_train_batch_size=per_device_train_batch_size,<br/>    seed=1,<br/>)<br/>trainer = Trainer(<br/>    model=model,<br/>    train_dataset=dataset,<br/>    args=training_arguments,<br/>)<br/>training1 = trainer.train()<br/><br/><br/># Sample generate prediction on holdout set<br/>“\n\n### Human: How do you say "good" in Spanish?\n\n### Assistant:”<br/># the correct output is “bueno&lt;/s&gt;”<br/><br/><br/>sample_generate(model, tokenizer, holdout_inputs, max_new_tokens=5)<br/>&gt;&gt;&gt; ‘&lt;/s&gt;’</span></pre><p id="afe2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After 15 epochs, we’re still kind of confused. We output ‘&lt;/s&gt;’ which is close but we really want to output “bueno&lt;/s&gt;”. Let’s learn another 15 epochs.</p><pre class="mm mn mo mp mq oz pa pb bp pc bb bk"><span id="97e8" class="pd oa fq pa b bg pe pf l pg ph">trainer.train()<br/>sample_generate(model, tokenizer, holdout_input, max_new_tokens=5)<br/>&gt;&gt;&gt; bueno &lt;/s&gt;</span></pre><p id="00b0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After 30 epochs we learned what we were supposed to!</p><p id="2308" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s simulate what happens in training by iteratively predicting the prompt one token at a time, based on the previous tokens.</p><pre class="mm mn mo mp mq oz pa pb bp pc bb bk"><span id="e50a" class="pd oa fq pa b bg pe pf l pg ph">print_iterative_generate(model, tokenizer, inputs)<br/>&gt;&gt;&gt;<br/>#<br/>: How do you say "how morning in Spanish?<br/><br/><br/>### Assistant: gu buenopu</span></pre><p id="cb9d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">That’s pretty close to the actual prompt, as we expected. But the task is translation, so we don’t really care about being able to predict the user prompt. Is there a way to learn just the response part?</p><h2 id="bf5a" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Masked approach</h2><p id="558b" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Hugging Face allows you to only learn to predict certain tokens by “masking” the tokens you don’t care about in “labels.” This is different from the attention mask, which hides <em class="pi">previous </em>tokens we use to generate a new token. Masking the labels hides the token you’re supposed to output at a certain index from the loss function. Note the wording: Hugging Face has it implemented such that during training, we still generate predictions for that masked token. However, because we hide the true label to compare the predictions with, we don’t directly learn how to improve on that prediction.</p><p id="473f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We create the “mask” by flipping those tokens to -100 in the labels key.</p><pre class="mm mn mo mp mq oz pa pb bp pc bb bk"><span id="efbb" class="pd oa fq pa b bg pe pf l pg ph">def create_special_mask(example: Dict) -&gt; Dict:<br/>    """Mask human text and keep assistant text as it is.<br/><br/>    Args:<br/>        example (Dict): Result of tokenizing some text<br/><br/>    Returns:<br/>        Dict: The dict with the label masked<br/>    """<br/>    # setting a token to -100 is how we "mask" a token<br/>    # and tell the model to ignore it when calculating the loss<br/>    mask_token_id = -100<br/>    # assume we always start with a human text<br/>    human_text = True<br/>    for idx, tok_id in enumerate(example["labels"]):<br/>        if human_text:<br/>            # mask all human text up until and including the bos token<br/>            example["labels"][idx] = mask_token_id<br/>            if tok_id == tokenizer.bos_token_id:<br/>                human_text = False<br/>        elif not human_text and tok_id == tokenizer.eos_token_id:<br/>            # don’t mask the eos token, but the next token will be human text to mask<br/>            human_text = True<br/>        elif not human_text:<br/>            # leave example['labels'] text as it is when assistant text<br/>            continue<br/>    return example<br/><br/><br/>dataset_masked = dataset.map(create_special_mask)<br/># convert dataset from lists to torch tensors<br/>dataset_masked.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])<br/>print(f"{dataset_masked[0]["labels"]=}")<br/><br/>&gt;&gt;&gt; dataset[0]["labels"]=tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 82208,     2])</span></pre><pre class="pj oz pa pb bp pc bb bk"><span id="b230" class="pd oa fq pa b bg pe pf l pg ph">model = load_model(model_name)<br/>trainer = Trainer(<br/>    model=model,<br/>    train_dataset=dataset_masked,<br/>    args=training_arguments,<br/>)<br/><br/>training2 = trainer.train()<br/><br/>print(f"{training2.metrics['train_runtime']=}")<br/>print(f"{training1.metrics['train_runtime'] =}")<br/>print(<br/>    f"{100*round((training1.metrics['train_runtime']  - training2.metrics['train_runtime']) / training1.metrics['train_runtime'] , 2)}%"<br/>)<br/><br/><br/>&gt;&gt;&gt; training2.metrics['train_runtime']=61.7164<br/>&gt;&gt;&gt; training1.metrics['train_runtime'] =70.8013<br/>&gt;&gt;&gt; 13.0%</span></pre><p id="49cb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">First off, we were faster this time by more than 10%. Presumably, the fact that we have fewer loss calculations makes things a bit quicker.</p><p id="b1ab" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I wouldn’t bank on the speed up being this large — our example is pretty lopsided with much more human text than generated text. But when training times are in the hours, every little percentage is helpful.</p><p id="6422" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The big question: did we learn the task?</p><pre class="mm mn mo mp mq oz pa pb bp pc bb bk"><span id="55e3" class="pd oa fq pa b bg pe pf l pg ph">sample_generate(model, tokenizer, holdout_input, max_new_tokens=5)<br/>&gt;&gt;&gt; bueno &lt;/s&gt;</span></pre><p id="bafa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This time we only need 15 epochs to learn the task. Let’s go back to how things are under the hood during training</p><pre class="mm mn mo mp mq oz pa pb bp pc bb bk"><span id="dd9d" class="pd oa fq pa b bg pe pf l pg ph">print_iterative_generate(model, tokenizer, inputs)<br/>&gt;&gt;&gt;#include<br/> code<br/> to I get "we" in English?<br/>A: Spanish: How bueno</span></pre><p id="b478" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Iteratively predicting the prompt leads to non-sense compared with our first training approach. This checks out: we masked the prompt during training and therefore don’t learn how to predict anything up until our real target: the assistant response.</p><h2 id="5620" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Using TRL’s supervised fine-tuning trainer</h2><p id="dc50" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Hugging Face semi-recently rolled out a TRL (transformer reinforcement learning) library to add end-to-end support for the LLM training process. One feature is supervised fine-tuning. Using the DataCollatorForCompletionOnlyLM and SFTTrainer classes, we can create the labels like we did with <em class="pi">create_special_mask</em> with just a few configs.</p><pre class="mm mn mo mp mq oz pa pb bp pc bb bk"><span id="93db" class="pd oa fq pa b bg pe pf l pg ph">model = load_model(model_name)<br/><br/># a hugging face function to do the copying of labels for you.<br/># using the instruction and response templates will mask everything between the instruction template and the start of the response_template<br/>collator = DataCollatorForCompletionOnlyLM(<br/>    instruction_template=tokenizer.eos_token,<br/>    response_template=tokenizer.bos_token,<br/>    tokenizer=tokenizer,<br/>)<br/><br/>trainersft = SFTTrainer(<br/>    model,<br/>    train_dataset=dataset_text,<br/>    dataset_text_field="text",<br/>    data_collator=collator,<br/>    args=training_arguments,<br/>    tokenizer=tokenizer,<br/>)<br/>sftrain = trainersft.train()<br/><br/>sample_generate(model, tokenizer, holdout_input, max_new_tokens=5)<br/>&gt;&gt;&gt; ' bueno&lt;/s&gt;'</span></pre><p id="55e9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Success! If you dig deeper, training actually took longer using SFT. This might be credited to the fact that we have to tokenize at training time rather than as a preprocessing step in the masked approach. However, this approach gives us free batching (you’d need to tweak the tokenization process to use the masked approach to batch properly), which should make things faster in the long run.</p><p id="1947" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The full notebook explores a few other things like training off multi-turn chats and using special_tokens to indicate human vs chat text.</p><p id="5539" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Obviously, this example is a bit basic. However, hopefully you can start to see the power of using CausalLM: You can imagine taking interactions from a large, reliable model, and using the techniques above to fine-tune a smaller model on the large model’s outputs. This is called knowledge distillation.</p><p id="9636" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If we’ve learned anything over the last couple years of LLMs, it’s that we can do some surprisingly intelligent things just by training on next token prediction. Causal language models are designed to do just that. Even if the Hugging Face class is a bit confusing at first, once you’re used to it, you have a very powerful interface to train your own generative models.</p></div></div></div></div>    
</body>
</html>