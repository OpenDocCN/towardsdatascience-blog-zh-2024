# 设计大型语言模型（LLMs）与用户体验之间的关系

> 原文：[https://towardsdatascience.com/designing-the-relationship-between-llms-and-user-experience-6720f8fee998?source=collection_archive---------6-----------------------#2024-04-19](https://towardsdatascience.com/designing-the-relationship-between-llms-and-user-experience-6720f8fee998?source=collection_archive---------6-----------------------#2024-04-19)

[](https://medium.com/@janna.lipenkova_52659?source=post_page---byline--6720f8fee998--------------------------------)[![Dr. Janna Lipenkova](../Images/112fe9a8c5936869243f2a43fde6dfee.png)](https://medium.com/@janna.lipenkova_52659?source=post_page---byline--6720f8fee998--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6720f8fee998--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6720f8fee998--------------------------------) [Dr. Janna Lipenkova](https://medium.com/@janna.lipenkova_52659?source=post_page---byline--6720f8fee998--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6720f8fee998--------------------------------) ·13分钟阅读·2024年4月19日

--

![](../Images/e1bb3550165ffc5ffb08c3d0cbd76f53.png)

不久前，我在Medium上写了一篇文章，[选择适合您NLP用例的语言模型](https://medium.com/towards-data-science/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929)。文章重点介绍了大型语言模型（LLMs）的基本工作原理——尽管它相当受欢迎，但我现在意识到它其实并没有提供关于如何选择LLMs的很多信息。我在LLM学习之初写了这篇文章，当时我认为LLMs的技术细节——它们的内部结构和训练历史——能够自行说明问题，从而帮助AI产品开发者在特定场景中自信地选择LLMs。

从那时起，我将LLMs集成到多个AI产品中。这让我发现了LLMs的技术构成如何决定产品最终体验的具体方式。它也加强了我的信念——产品经理和设计师需要对LLM的“幕后”运作有深入了解。LLM接口不同于传统的图形接口。后者通过以隐含的方式展示产品功能，提供给用户一个（希望是清晰的）心理模型。另一方面，LLM接口将自由文本作为主要的互动形式，提供了更多的灵活性。同时，它们也“隐藏”了底层模型的能力和局限性，让用户自行探索和发现。因此，一个简单的文本框或聊天窗口可以接收无限数量的意图和输入，并能显示出许多不同的输出。

![](../Images/18bdd4b643ac7011cb6e01a66a1d4b65.png)

图1 显示了一个简单的聊天窗口，可以接收无限数量的输入（图片来自vectorstock.com，由作者购买授权）

这些交互成功的责任不仅仅在工程方面——实际上，大部分责任应该由产品经理和设计者来承担。在本文中，我们将阐述LLM与用户体验之间的关系，并探讨你可以用来提升产品体验的两个通用要素：

1.  **功能**，即LLM执行的任务，例如对话、问答和情感分析

1.  **质量**，即LLM执行任务的质量，包括正确性和一致性等客观标准，也包括语气和风格等主观标准

*(注意：这两项内容是任何LLM应用的组成部分。除了这两项，大多数应用还会有一系列更为个性化的标准需要满足，例如延迟、隐私和安全性，本文不予讨论。)*

因此，正如彼得·德鲁克所说，这涉及到“做正确的事”（功能）和“做对的事”（质量）。如今，我们知道，LLM永远不可能做到100%的正确。作为构建者，你可以从两个方向来接近理想的体验：

+   一方面，你需要追求**工程卓越**，在选择、微调和评估LLM时做出正确的决策。

+   另一方面，你需要通过引导你的**用户**朝着LLM所涵盖的目标迈进，管理他们的期望，并在出现问题时触发相应的处理流程。

在本文中，我们将重点关注工程部分。与人类用户的理想合作关系将在未来的文章中讨论。首先，我将简要介绍工程过程中的步骤——LLM选择、适配和评估——这些步骤直接决定最终的体验。然后，我们将讨论两个要素——功能和质量——并提供一些指南，帮助你在这两个维度上优化LLM的工作，从而提升产品的表现。

范围说明：在本文中，我们将讨论独立LLM的使用。许多原则和指南同样适用于用于RAG（检索增强生成）和代理系统中的LLM。若需更详细地了解这些扩展LLM场景中的用户体验，请参阅我的书籍[《AI产品开发的艺术》](https://www.manning.com/books/the-art-of-ai-product-development)。

# LLM工程过程

接下来，我们将重点讨论LLM选择、适配和评估这三个步骤。让我们逐一探讨这些步骤：

1.  **LLM选择**涉及确定你的部署选项（特别是开源与商业LLM）并选择一个其训练数据和预训练目标与目标功能对齐的LLM。此外，你能选择的模型越强大（无论是参数规模还是训练数据的数量），它实现高质量的可能性就越大。

1.  **LLM适应**通过上下文学习或微调，给你提供了弥合用户意图与模型原始预训练目标之间差距的机会。此外，你还可以通过将你希望模型采用的风格和语气融入微调数据中，来调整模型的质量。

1.  **LLM评估**涉及在模型生命周期中持续评估模型。因此，它不是流程结束时的最终步骤，而是一个持续的活动，随着你收集到更多关于模型的洞察和数据，它会逐渐变得更加具体。

以下图总结了该过程：

![](../Images/565cda173a35980b5c3e99bcad928b5f.png)

图2 LLM用户体验工程

在实际应用中，这三个阶段会有所重叠，且各阶段之间可能会反复往返。通常，模型选择更像是“一次性的大决定”。当然，你可以在后续过程中从一个模型切换到另一个，甚至应当在新的、更合适的模型出现在市场时这样做。然而，这些更改是昂贵的，因为它们会影响所有下游工作。在发现阶段之后，你通常不希望频繁进行这些更改。另一方面，LLM适应和评估是高度迭代的。这些活动应伴随持续的发现过程，你将通过这些过程更多地了解模型的行为和用户的需求。最后，所有三个活动都应嵌入到一个稳固的LLMOps流水线中，这将使你能够以最小的工程摩擦整合新的洞察和数据。

现在，让我们进入图表的第二列，确定LLM的功能范围，并了解它如何在该过程中三个阶段中得到塑造。

# 功能：响应用户意图

你可能会想知道，为什么我们要讨论LLM的“功能”？毕竟，LLM不是那种能够神奇地完成我们能想到的任何语言任务的多才多艺的全能型工具吗？实际上，它们确实如此，正如论文[Language Models Are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)中著名描述的那样。LLM可以从仅仅几个例子中学习新能力。有时，它们的能力甚至会在正常训练中“突然出现”——并且——希望是偶然被发现的。这是因为语言建模的任务既多才多艺又充满挑战——作为副作用，它使得LLM具备了执行许多其他相关任务的能力。

然而，LLM的预训练目标是**根据过去词语的上下文生成下一个词**（好吧，这是一个简化的说法——在自编码中，LLM可以双向工作[3]）。这就是一个预训练的LLM在被提示后会坚持做的事情，动机来自一个虚构的“奖励”。在大多数情况下，这个目标与一个用户使用你的产品进行聊天、寻求问题的答案，或是将一段德文翻译成意大利文之间有着相当大的差距。Emily Bender和Alexander Koller的标志性论文[《迈向自然语言理解：在数据时代的意义、形式与理解》](https://aclanthology.org/2020.acl-main.463.pdf)甚至认为语言模型通常无法恢复沟通意图，因此注定只能处理不完整的意义表示。

因此，夸耀在科学研究中令人惊叹的LLM能力并在高度控制的基准和测试场景中展示它们是一回事。而将LLM推出给一群具有不同AI技能和目的——其中一些可能是有害的——匿名用户则是另一回事。这一点尤其正确，一旦你明白你的产品不仅继承了LLM的能力，还继承了它的弱点和风险，并且你（而非第三方提供商）需要对其行为负责。

实际上，我们已经学到，在将LLM集成到产品中时，最好识别并隔离离散的功能模块。这些功能大致对应于用户使用你的产品时的不同意图。例如，它可能是：

+   进行对话

+   检索信息

+   寻求特定情境的建议

+   寻找灵感

这些能力通常可以进一步分解成更细化、甚至是可复用的能力。例如，“进行对话”可以分解为：

+   提供有信息量和相关性的对话回复

+   维护过去交互的记忆（而不是每次都从头开始）

+   展示一致的个性

采用这种更加离散的方法来处理LLM的能力，能为你带来以下优势：

+   ML工程师和数据科学家可以更好地专注于他们的工程活动（见图2），聚焦于目标功能。

+   关于你的产品的沟通变得精准和具体，有助于管理用户期望，并维护信任、诚信和信誉。

+   在用户界面中，你可以使用一系列设计模式，如提示模板和占位符，以提高用户意图与模型功能对齐的机会。

# 确保正确功能的指南

让我们总结一些实用的指南，确保LLM在你的产品中做对了事：

+   在LLM选择过程中，确保你了解模型的基本预训练目标。预训练目标有三种基本类型（自编码、自动回归、序列到序列），它们每种都会影响模型的行为。

+   许多LLM也已经通过更高级的目标进行预训练，例如对话或执行明确指令（指令微调）。选择一个已经为你的任务做好准备的模型，可以让你高效地起步，减少你在下游适配和微调中所需的工作量，从而实现满意的质量。

+   通过上下文学习或微调进行LLM适配，可以为你提供一个机会，弥合原始预训练目标和你希望服务的用户意图之间的差距。

![](../Images/7dbefddedca2b9f8e0073900caaff6a4.png)

图3 LLM适配缩小了预训练目标和用户意图之间的差距

+   在初期发现阶段，你可以通过上下文学习来收集初步的使用数据，并加深对相关用户意图及其分布的理解。

+   在大多数情况下，长远来看，上下文学习（提示微调）是不可持续的——它本身并不高效。随着时间的推移，你可以利用新数据和学习成果作为基础，对模型的权重进行微调。

+   在模型评估过程中，确保应用特定任务的度量标准。例如，Text2SQL LLM（参见[这篇文章](https://medium.com/towards-data-science/enabling-the-data-driven-organisation-with-text2sql-f8e07089dd0c)）可以通过执行准确率和测试集准确率等指标进行评估，而摘要生成可以通过基于相似度的指标进行评估。

这些只是我们在整合LLM时学到的课程的简短片段。我即将出版的书籍《[人工智能产品开发的艺术](https://www.manning.com/books/the-art-of-ai-product-development)》将深入探讨每一条指南，并提供大量的示例。关于预训练目标和过程的技术细节，你可以参考[这篇文章](https://medium.com/towards-data-science/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929)。

好的，现在你已经了解了用户使用你的产品时的意图，并“激发”了你的模型来响应这些意图。你甚至可能已经将LLM投入到实际环境中，希望它能够启动数据飞轮。现在，如果你想保持现有用户并吸引新用户，你需要迅速提升第二个关键要素——质量。

# 实现高质量

在LLM的背景下，质量可以分解为客观和主观两个组成部分。客观部分告诉你何时以及为何出现问题（即LLM犯了明显的错误）。主观部分则更为微妙和情感化，反映了与特定用户群体的契合度。

# 目标质量标准

使用语言进行交流对人类来说是自然而然的。语言从我们生命的开始就深深扎根在我们的大脑中，我们很难想象从零开始学习语言需要付出多少努力。即便是我们在学习外语时所遇到的挑战，也无法与 LLM 的训练相比。LLM 从一张空白的纸开始，而我们学习语言的过程则建立在一个极其丰富的世界知识基础和语言运作的基本理解之上。

在使用 LLM 时，我们应该时刻保持意识，注意到许多可能出现的错误：

+   LLM 可能会犯语言错误。

+   LLM 可能在连贯性、逻辑性和一致性方面表现欠佳。

+   LLM 可能缺乏足够的世界知识，从而导致错误的陈述和幻觉。

这些缺点可能很快就会成为你产品的致命问题——输出质量是 LLM 产品用户体验的核心决定因素。例如，ChatGPT 在“公开”成功的一个主要因素是它确实能够在各个领域生成正确、流畅且相对连贯的文本。早期版本的 LLM 无法实现这一目标质量。如今大多数用于生产的预训练 LLM 确实具备生成语言的能力。然而，它们在连贯性、一致性和世界知识等标准上的表现可能非常不稳定且不一致。为了实现你期望的体验，明确优先排序这些要求，并据此选择和调整 LLM 是非常重要的。

# 主观质量标准

在更为微妙的主观领域，您需要了解并监控用户对产品的感受。他们在使用产品时是否感到愉快、信任，并进入心流状态？还是他们带着沮丧、低效和不对劲的感觉离开？这些都与个人的文化、价值观和风格息息相关。如果你正在为初级开发人员构建一个副驾驶，你显然不希望它使用高级管理人员的语言，反之亦然。

举个例子，假设你是一个产品营销人员，你花了很多时间和一个工程师一起反复修改一个帮助你进行内容生成的 LLM。某一天，你和团队中的 UX 设计师聊天，向他炫耀你的新 AI 助手。你的同事不理解为何要付出这么多努力。他平时使用 ChatGPT 来帮助创建和评估 UX 调查，并对结果非常满意。你反驳道——ChatGPT 输出的内容对于你的故事叙述和写作任务来说过于通用且单调。事实上，刚开始使用时，你也曾遇到过相当尴尬的情况，因为有一段时间，读者开始识别出 ChatGPT 特有的风格。那是你职业生涯中的一个滑铁卢，之后你决定需要一个更复杂的工具。

在这个讨论中没有对错之分。ChatGPT适合处理那些直接的、事实性的任务，在这些任务中风格并不那么重要。相反，作为市场营销人员，你需要一个能够帮助你打造高质量、具有说服力的沟通内容的助手，这些内容能够讲述你客户的语言，并且反映你公司独特的DNA。

这些主观的细微差别最终可能决定一个LLM是否有用——即使它的输出最终需要重写，还是能够定义为“足够好”，让用户开始使用它并提供合适的微调数据。LLM掌握的圣杯是个性化——即通过高效的微调或提示调优，使LLM适应任何与模型有一定时间互动的用户的个人偏好。如果你刚刚开始LLM之旅，这些细节可能看起来还很遥远——但最终，它们将帮助你达到一个阶段，让你的LLM通过以精确的方式和风格回应用户的需求，带来用户满意度和大规模采用，并把你的竞争对手甩在身后。

# 指南

以下是我们在管理LLM质量方面的建议：

+   留意不同种类的反馈。对质量的追求是持续的和迭代的——你从一些数据点和对产品质量含义的粗略理解开始。随着时间的推移，你会逐渐丰富更多细节，学习哪些杠杆可以用来改善你的LLM。

+   在模型选择过程中，你仍然有很多发现要做——从“眼球”判断开始，并通过不同的输入测试不同的LLM（最好由多个团队成员来做）。

+   你的工程师们也会评估学术基准和与模型一起发布的评估结果。然而，请记住，这些只是模型在你特定产品中的表现的粗略指标。

+   一开始，完美主义不是答案。你的模型应该足够好，以吸引那些能够开始为其提供相关数据以进行微调和评估的用户。

+   将你的团队和用户聚集起来，进行定性的LLM输出讨论。当他们用语言来判断和辩论什么是对的，什么是错的时，你可以逐渐揭示出他们的客观和情感需求。

+   确保建立一个完善的LLMOps流程，这样你可以顺利地集成新数据，减少工程上的摩擦。

+   不要停下来——在后期阶段，你可以将焦点转向细微差别和个性化，这也将帮助你加强竞争优势。

# 总结：承担责任

预训练的大语言模型非常方便——它们让AI变得对每个人都可访问，减轻了训练大型初始模型所需的巨额工程、计算和基础设施开支。一旦发布，它们可以立即使用，我们可以将其惊人的功能融入到我们的产品中。然而，当在产品中使用第三方模型时，你不仅继承了它的强大功能，还继承了它可能失败的多种方式。当事情出错时，为了保持产品的完整性，你最不希望做的事情就是将责任归咎于外部模型提供商、你的工程师，或者——更糟糕——你的用户。

因此，在使用大语言模型（LLMs）时，你不仅应该了解模型的来源（训练数据和过程）的透明度，还要建立因果理解，了解其技术构成如何影响你的产品所提供的体验。这将帮助你在开始阶段找到启动强大数据飞轮和在产品逐渐走向卓越时持续优化和差异化大语言模型之间的敏感平衡。

# 参考文献

[1] Janna Lipenkova（2022年）。[《为你的NLP用例选择合适的语言模型》](https://medium.com/towards-data-science/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929)，Medium。

[2] Tom B. Brown 等（2020年）。[《语言模型是少量示例学习者》](https://arxiv.org/abs/2005.14165)。

[3] Jacob Devlin 等（2018年）。[《BERT：用于语言理解的深度双向Transformer预训练》](https://arxiv.org/abs/1810.04805)。

[4] Emily M. Bender 和 Alexander Koller（2020年）。[《迈向自然语言理解：在数据时代的意义、形式和理解》](https://aclanthology.org/2020.acl-main.463.pdf)。

[5] Janna Lipenkova（即将出版）。[《AI产品开发艺术》](https://www.manning.com/books/the-art-of-ai-product-development)，Manning出版公司。

注：所有图片均由作者提供，除非另有说明。
