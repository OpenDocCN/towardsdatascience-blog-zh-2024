- en: Exploring mergekit for Model Merge, AutoEval for Model Evaluation, and DPO for
    Model Fine-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/exploring-mergekit-for-model-merge-and-autoeval-for-model-evaluation-c681766fd1f3?source=collection_archive---------5-----------------------#2024-01-19](https://towardsdatascience.com/exploring-mergekit-for-model-merge-and-autoeval-for-model-evaluation-c681766fd1f3?source=collection_archive---------5-----------------------#2024-01-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: My observations from experimenting with model merge, evaluation, and two model
    fine-tuning techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@wenqiglantz?source=post_page---byline--c681766fd1f3--------------------------------)[![Wenqi
    Glantz](../Images/65b518863e01aaa48ecc6b8ac6d1be60.png)](https://medium.com/@wenqiglantz?source=post_page---byline--c681766fd1f3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c681766fd1f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c681766fd1f3--------------------------------)
    [Wenqi Glantz](https://medium.com/@wenqiglantz?source=post_page---byline--c681766fd1f3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c681766fd1f3--------------------------------)
    ·14 min read·Jan 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fbfcc3ab489becdd52f76fbc342fc138.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by DALL-E 3 by the author
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue our learning journey of [Maxime Labonne](https://medium.com/u/dc89da634938?source=post_page---user_mention--c681766fd1f3--------------------------------)’s
    [llm-course](https://github.com/mlabonne/llm-course), which is pure gold for the
    community. This time, we will focus on model merge, evaluation, and fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Maxime has a great article titled [Merge Large Language Models with mergekit](https://medium.com/towards-data-science/merge-large-language-models-with-mergekit-2118fb392b54).
    I highly recommend you check it out first. We will not repeat the steps he has
    already laid out in his article, but we will explore some details I came across
    that might be helpful to you.
  prefs: []
  type: TYPE_NORMAL
- en: High-Level Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to experiment with model merge, model evaluation, and model fine-tuning
    in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Using [LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing),
    we merge two models from the Hugging Face hub, `mistralai/Mistral-7B-Instruct-v0.2`
    and `jan-hq/trinity-v1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run AutoEval on the base model `mistralai/Mistral-7B-Instruct-v0.2`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run AutoEval on the merged model `MistralTrinity-7b-slerp`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised fine-tune the merged model with QLoRA. Run AutoEval on the fine-tuned
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
