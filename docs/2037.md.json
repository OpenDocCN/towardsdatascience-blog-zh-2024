["```py\npip install transformers[torch] datasets[audio] audiomentations\n```", "```py\nDataset({\n    features: ['audio', 'labels'],\n    num_rows: 1234\n})\n```", "```py\nfrom datasets import load_dataset\n\nesc50 = load_dataset(\"ashraq/esc50\", split=\"train\")\n```", "```py\nfrom datasets import Dataset, Audio, ClassLabel, Features\n\n# Define class labels\nclass_labels = ClassLabel(names=[\"bang\", \"dog_bark\"])\n# Define features with audio and label columns\nfeatures = Features({\n    \"audio\": Audio(),  # Define the audio feature\n    \"labels\": class_labels  # Assign the class labels\n})\n# Construct the dataset from a dictionary\ndataset = Dataset.from_dict({\n    \"audio\": [\"/audio/fold1/7061-6-0-0.wav\", \"/audio/fold1/7383-3-0-0.wav\"],\n    \"labels\": [0, 1],  # Corresponding labels for the audio files\n}, features=features)\n```", "```py\nprint(dataset[0])\n```", "```py\n{'audio': {'path': '/audio/fold1/7061-6-0-0.wav',\n  'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n         1.52587891e-05, 3.05175781e-05, 0.00000000e+00]),\n  'sampling_rate': 44100},\n 'labels': 0}\n```", "```py\nimport numpy as np\nfrom datasets import Audio, ClassLabel\n\n# get target value - class name mappings\ndf = esc50.select_columns([\"target\", \"category\"]).to_pandas()\nclass_names = df.iloc[np.unique(df[\"target\"], return_index=True)[1]][\"category\"].to_list()\n# cast target and audio column\nesc50 = esc50.cast_column(\"target\", ClassLabel(names=class_names))\nesc50 = esc50.cast_column(\"audio\", Audio(sampling_rate=16000))\n# rename the target feature\nesc50 = esc50.rename_column(\"target\", \"labels\")\nnum_labels = len(np.unique(esc50[\"labels\"]))\n```", "```py\nfrom transformers import ASTFeatureExtractor\n\n# we define which pretrained model we want to use and instantiate a feature extractor\npretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\nfeature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n# we save model input name and sampling rate for later use\nmodel_input_name = feature_extractor.model_input_names[0]  # key -> 'input_values'\nSAMPLING_RATE = feature_extractor.sampling_rate\n```", "```py\n# calculate values for normalization\nfeature_extractor.do_normalize = False  # we set normalization to False in order to calculate the mean + std of the dataset\nmean = []\nstd = []\n\n# we use the transformation w/o augmentation on the training dataset to calculate the mean + std\ndataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\nfor i, (audio_input, labels) in enumerate(dataset[\"train\"]):\n    cur_mean = torch.mean(dataset[\"train\"][i][audio_input])\n    cur_std = torch.std(dataset[\"train\"][i][audio_input])\n    mean.append(cur_mean)\n    std.append(cur_std)\nfeature_extractor.mean = np.mean(mean)\nfeature_extractor.std = np.mean(std)\nfeature_extractor.do_normalize = True\n```", "```py\ndef preprocess_audio(batch):\n    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n    # inputs are spectrograms as torch.tensors now\n    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n\n    output_batch = {model_input_name: inputs.get(model_input_name), \"labels\": list(batch[\"labels\"])}\n    return output_batch\n\n# Apply the transformation to the dataset\ndataset = dataset.rename_column(\"audio\", \"input_values\")  # rename audio column\ndataset.set_transform(preprocess_audio, output_all_columns=False)\n```", "```py\n{'input_values': tensor([[-1.2776, -1.2776, -1.2776,  ..., -1.2776, -1.2776, -1.2776],\n         [-1.2776, -1.2776, -1.2776,  ..., -1.2776, -1.2776, -1.2776],\n         [-1.2776, -1.2776, -1.2776,  ..., -1.2776, -1.2776, -1.2776],\n         ...,\n         [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670],\n         [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670],\n         [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670]]),\n 'label': 0}\n```", "```py\n# split training data\nif \"test\" not in dataset:\n    dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=0, stratify_by_column=\"labels\")\n```", "```py\nfrom audiomentations import Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift\n\naudio_augmentations = Compose([\n    AddGaussianSNR(min_snr_db=10, max_snr_db=20),\n    Gain(min_gain_db=-6, max_gain_db=6),\n    GainTransition(min_gain_db=-6, max_gain_db=6, min_duration=0.01, max_duration=0.3, duration_unit=\"fraction\"),\n    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.5),\n    TimeStretch(min_rate=0.8, max_rate=1.2),\n    PitchShift(min_semitones=-4, max_semitones=4),\n], p=0.8, shuffle=True)\n```", "```py\ndef preprocess_audio_with_transforms(batch):\n    # we apply augmentations on each waveform\n    wavs = [audio_augmentations(audio[\"array\"], sample_rate=SAMPLING_RATE) for audio in batch[\"input_values\"]]\n    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n\n    output_batch = {model_input_name: inputs.get(model_input_name), \"labels\": list(batch[\"labels\"])}\n    return output_batch\n\n# Cast the audio column to the appropriate feature type and rename it\ndataset = dataset.cast_column(\"input_values\", Audio(sampling_rate=feature_extractor.sampling_rate))\n```", "```py\n# with augmentations on the training set\ndataset[\"train\"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)\n# w/o augmentations on the test set\ndataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)\n```", "```py\nfrom transformers import ASTConfig, ASTForAudioClassification\n\n# Load configuration from the pretrained model\nconfig = ASTConfig.from_pretrained(pretrained_model)\n# Update configuration with the number of labels in our dataset\nconfig.num_labels = num_labels\nconfig.label2id = label2id\nconfig.id2label = {v: k for k, v in label2id.items()}\n# Initialize the model with the updated configuration\nmodel = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)\nmodel.init_weights()\n```", "```py\nSome weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n```", "```py\nfrom transformers import TrainingArguments\n\n# Configure training run with TrainingArguments class\ntraining_args = TrainingArguments(\n    output_dir=\"./runs/ast_classifier\",\n    logging_dir=\"./logs/ast_classifier\",\n    report_to=\"tensorboard\",\n    learning_rate=5e-5,  # Learning rate\n    push_to_hub=False,\n    num_train_epochs=10,  # Number of epochs\n    per_device_train_batch_size=8,  # Batch size per device\n    eval_strategy=\"epoch\",  # Evaluation strategy\n    save_strategy=\"epoch\",\n    eval_steps=1,\n    save_steps=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    logging_strategy=\"steps\",\n    logging_steps=20,\n)\n```", "```py\nimport evaluate\nimport numpy as np\n\naccuracy = evaluate.load(\"accuracy\")\nrecall = evaluate.load(\"recall\")\nprecision = evaluate.load(\"precision\")\nf1 = evaluate.load(\"f1\")\nAVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n\ndef compute_metrics(eval_pred):\n    logits = eval_pred.predictions\n    predictions = np.argmax(logits, axis=1)\n    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n    return metrics\n```", "```py\nfrom transformers import Trainer\n\n# Setup the trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    compute_metrics=compute_metrics,  # Use the metrics function from above\n)\n```", "```py\ntrainer.train()\n```", "```py\ntensorboard --logdir=\"./logs\"\n```", "```py\npip install renumics-spotlight\n```", "```py\nfrom renumics import spotlight\n\nspotlight.show(esc50, dtype={\"audio\": spotlight.Audio})\n```"]