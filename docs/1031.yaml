- en: Differential Privacy and Federated Learning for Medical Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/differential-privacy-and-federated-learning-for-medical-data-0f2437d6ece9?source=collection_archive---------7-----------------------#2024-04-23](https://towardsdatascience.com/differential-privacy-and-federated-learning-for-medical-data-0f2437d6ece9?source=collection_archive---------7-----------------------#2024-04-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical assessment of Differential Privacy & Federated Learning in the medical
    context.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@eric.boernert?source=post_page---byline--0f2437d6ece9--------------------------------)[![Eric
    Boernert](../Images/3038e5e8f0cd468ec69794caecf32bba.png)](https://medium.com/@eric.boernert?source=post_page---byline--0f2437d6ece9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0f2437d6ece9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0f2437d6ece9--------------------------------)
    [Eric Boernert](https://medium.com/@eric.boernert?source=post_page---byline--0f2437d6ece9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0f2437d6ece9--------------------------------)
    ·10 min read·Apr 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8e948c4524bfa196d71cadef58061a5.png)'
  prefs: []
  type: TYPE_IMG
- en: (Bing AI generated image, original, full ownership)
  prefs: []
  type: TYPE_NORMAL
- en: Sensitive data cries out for more protection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The need for data privacy seems to be generally at ease nowadays in the era
    of large language models trained on everything from the public internet, regardless
    of [actual intellectual property](https://futurism.com/video-openai-cto-sora-training-data)
    which their respective [company leaders openly admit](https://www.theguardian.com/technology/2024/jan/08/ai-tools-chatgpt-copyrighted-material-openai).
  prefs: []
  type: TYPE_NORMAL
- en: But there’s a much more sensitive parallel universe when it comes to patients’
    data, our health records, which are undoubtedly much more sensitive and in need
    of [protection](https://www.youtube.com/watch?v=4-7jSoINyq4).
  prefs: []
  type: TYPE_NORMAL
- en: Also the regulations are getting stronger all over the world, the trend is unanimously
    towards more stricter data protection regulations, including AI.
  prefs: []
  type: TYPE_NORMAL
- en: There are obvious ethical reasons which we don’t have to explain, but from the
    enterprise levels regulatory and legal reasons that require pharmaceutical companies,
    labs and hospitals to use state of the art technologies to protect data privacy
    of patients.
  prefs: []
  type: TYPE_NORMAL
- en: Federated paradigm is here to help
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Federated analytics and learning are great options to be able to analyze data
    and train models on patients’ data without accessing any raw data.
  prefs: []
  type: TYPE_NORMAL
- en: In case of federated analytics it means, for instance, we can get correlation
    between blood glucose and patients BMI without accessing any raw data that could
    lead to patients re-identification.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of machine learning, let’s use the example of diagnostics, where
    models are trained on patients’ images to detect malignant changes in their tissues
    and detect early stages of cancer, for instance. This is literally a life saving
    application of machine learning. Models are trained locally at the hospital level
    using local images and labels assigned by professional radiologists, then there’s
    aggregation which combines all those local models into a single more generalized
    model. The process repeats for tens or hundreds of rounds to improve the performance
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2adfedf9f70a43b17a534cd0d78e5a60.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig. 1\. Federated learning in action, sharing model updates, not data.*'
  prefs: []
  type: TYPE_NORMAL
- en: The reward for each individual hospital is that they will benefit from a better
    trained model able to detect disease in future patients with higher probability.
    It’s a win-win situation for everyone, especially patients.
  prefs: []
  type: TYPE_NORMAL
- en: Of course there’s a variety of federated network topologies and model aggregation
    strategies, but for the sake of this article we tried to focus on the typical
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Trust building with the help of technology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s believed that [vast amounts of clinical data are not being used](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6376961/)
    due to a (justified) [reluctance of data owners to share their data](https://bmcpublichealth.biomedcentral.com/articles/10.1186/1471-2458-14-1144)
    with partners.
  prefs: []
  type: TYPE_NORMAL
- en: Federated learning is a key strategy to build that trust backed up by the technology,
    not only on contracts and faith in ethics of particular employees and partners
    of the organizations forming consortia.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, the data remains at the source, never leaves the hospital, and
    is not being centralized in a single, potentially vulnerable location. Federated
    approach means there aren’t any external copies of the data that may be hard to
    remove after the research is completed.
  prefs: []
  type: TYPE_NORMAL
- en: The technology blocks access to raw data because of multiple techniques that
    follow defense in depth principle. Each of them is minimizing the risk of data
    exposure and patient re-identification by tens or thousands of times. Everything
    to make it economically unviable to discover nor reconstruct raw level data.
  prefs: []
  type: TYPE_NORMAL
- en: Data is minimized first to expose only the necessary properties to machine learning
    agents running locally, PII data is stripped, and we also use anonymization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Then local nodes protect local data against the so-called too curious data scientist
    threat by allowing only the code and operations accepted by local data owners
    to run against their data. For instance model training code deployed locally at
    the hospital as a package is allowed or not by the local data owners. Remote data
    scientists cannot just send any code to remote nodes as that would allow them
    for instance to return raw level data. This requires a new, decentralized way
    of thinking to embrace different mindset and technologies for permission management,
    an interesting topic for another time.
  prefs: []
  type: TYPE_NORMAL
- en: Are models private enough?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assuming all those layers of protection are in place there’s still concern related
    to the safety of model weights themselves.
  prefs: []
  type: TYPE_NORMAL
- en: There’s growing concern in the AI community about machine learning models as
    the super compression of the data, not as black-boxy as previously considered,
    and revealing more information about the underlying data than previously thought.
  prefs: []
  type: TYPE_NORMAL
- en: And that means that with enough skills, time, effort and powerful hardware a
    motivated adversary can try to reconstruct the original data, or at least prove
    with high probability that a given patient was in the group that was used to train
    the model (Membership Inference Attack (MIA)) . Other [types of attacks](https://arxiv.org/pdf/2211.14952.pdf)
    possible such as extraction, reconstruction and evasion.
  prefs: []
  type: TYPE_NORMAL
- en: To make things even worse, the progress of generative AI that we all admire
    and benefit from, delivers new, more effective techniques for image reconstruction
    ([for example, lung scan of the patients](https://arxiv.org/abs/2202.06924)).
    The same ideas that are used by all of us to generate images on demand can be
    used by adversaries to reconstruct original images from MRI/CT scan machines.
    Other modalities of data such as [tabular data](https://arxiv.org/pdf/2208.08114.pdf),
    text, sound and video can now be reconstructed using gen AI.
  prefs: []
  type: TYPE_NORMAL
- en: Differential Privacy to the rescue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Differential privacy (DP) algorithms promise that we exchange some of the model’s
    accuracy for much improved resilience against inference attacks. This is another
    privacy-utility trade-off that is worth considering.
  prefs: []
  type: TYPE_NORMAL
- en: '[Differential privacy](https://blog.openmined.org/differential-privacy-using-pydp/)
    means in practice we add a very special type of noise and clipping, that in return
    will result in a [very good ratio of privacy gains versus accuracy loss](https://privacytools.seas.harvard.edu/files/privacytools/files/nissim_et_al_-_differential_privacy_primer_for_non-technical_audiences_1.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: It can be as easy as least effective Gaussian noise but nowadays we embrace
    the development of much more sophisticated algorithms such as Sparse Vector Technique
    (SVT), Opacus library as practical implementation of differentially private stochastic
    gradient descent (DP-SGD), plus venerable Laplacian noise based libraries (i.e.
    PyDP).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e66c98be78968db9ff7da231d8c762aa.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig. 2\. On device differential privacy that we all use all the time.*'
  prefs: []
  type: TYPE_NORMAL
- en: And, by the way, we all benefit from this technique without even realizing that
    it even exists, and it is happening right now. Our telemetry data from mobile
    devices ([Apple iOS](https://machinelearning.apple.com/research/learning-with-privacy-at-scale),
    [Google Android](https://developers.googleblog.com/2021/01/how-were-helping-developers-with-differential-privacy.html?m=1))
    and desktop OSes ([Microsoft Windows](https://blogs.microsoft.com/on-the-issues/2020/06/24/differential-privacy-harvard-opendp/))
    is using differential privacy and federated learning algorithms to train models
    without sending raw data from our devices. And it’s been around for years now.
  prefs: []
  type: TYPE_NORMAL
- en: Now, there’s growing adoption for other use cases including our favorite siloed
    federated learning case, with relatively few participants with large amounts of
    data in on-purpose established consortia of different organizations and companies.
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy is not specific to federated learning. However, there are
    different strategies of applying DP in federated learning scenarios as well as
    different selection of algorithms. Different algorithms which work better for
    federated learning setups, different for local data privacy (LDP) and centralized
    data processing.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of federated learning we anticipate a drop in model accuracy
    after applying differential privacy, but still (and to some extent hopefully)
    expect the model to perform better than local models without federated aggregation.
    So the federated model should still retain its advantage despite added noise and
    clipping (DP).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33bb671540c98ab5748bd857a92d1618.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig. 3\. What we can expect based on known papers and our experiences.*'
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy can be applied as early as at the source data (Local Differential
    Privacy (LDP)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4215a9e94305aef91a8b158d3654068c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig. 4, different places where DP can be applied to improve data protection*'
  prefs: []
  type: TYPE_NORMAL
- en: There are also cases of federated learning within a network of partners who
    have all data access rights and are less concerned about data protection levels
    so there might be no DP at all.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand when the model is going to be shared with the outside world
    or sold commercially it might be a good idea to apply DP for the global model
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: Practical experimentation results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At Roche’s Federated Open Science team, [NVIDIA Flare](https://developer.nvidia.com/flare)
    is our tool of choice for federated learning as the most mature open source federated
    framework on the market. We also collaborate with the NVIDIA team on [future development
    of NVIDIA Flare](https://developer.nvidia.com/blog/turning-machine-learning-to-federated-learning-in-minutes-with-nvidia-flare-2-4/)
    and are glad to help to improve an already great solution for federated learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We tested three different DP algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Opacus](https://github.com/pytorch/opacus)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SVT](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/filters/svt_privacy.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gaussian noise](https://github.com/NVIDIA/NVFlare/blob/main/research/quantifying-data-leakage/src/nvflare_gradinv/filters/gaussian_privacy.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We applied differential privacy for the models using different strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: Every federated learning round
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only the first round (of federated training)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each Nth round (of federated training)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'for three different cases (datasets and algorithms):'
  prefs: []
  type: TYPE_NORMAL
- en: FLamby Tiny IXI dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breast density classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higgs classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, we tried three dimensions of algorithm, strategy and dataset (case).
  prefs: []
  type: TYPE_NORMAL
- en: The results are conforming with our expectations of model accuracy degradation
    that was larger with lower privacy budgets (as expected).
  prefs: []
  type: TYPE_NORMAL
- en: FLamby Tiny IXI dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '(Dataset source: [https://owkin.github.io/FLamby/fed_ixi.html](https://owkin.github.io/FLamby/fed_ixi.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a878aa9570490b92fdc7705b1e450ca0.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig. 5\. Models performance without DP*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1fa7a6c6e5a7c5e925df2ca556067c5.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig. 6\. Models performance with DP on first round*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae7a6c918def2d29b01335393c43045b.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig. 7\. SVT applied every second round (with decreasing threshold)*'
  prefs: []
  type: TYPE_NORMAL
- en: We observe significant improvement of accuracy with SVT applied on the first
    round compared to SVT filter applied to every round.
  prefs: []
  type: TYPE_NORMAL
- en: Breast density case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: (Dataset source [Breast Density Classification using MONAI | Kaggle](https://www.kaggle.com/code/theoviel/breast-density-classification-using-monai))
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d79ec0cd119de736d026df28b2de5c4.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig. 8\. Models performance without DP*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84ba2dad1d9e2ab32f33e37d46b71413.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig. 9\. DP applied to the first round*'
  prefs: []
  type: TYPE_NORMAL
- en: We observe a mediocre accuracy loss after applying a Gaussian noise filter.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset was the most troublesome and sensitive to DP (major accuracy loss,
    unpredictability).
  prefs: []
  type: TYPE_NORMAL
- en: Higgs classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: (Dataset source [HIGGS — UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/280/higgs))
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b52ce7bd21ec84c112083f3575e297da.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig. 10\. Models performance with percentile value 95.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59fc53bec323a0e2af6a500807da9f63.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig. 11\. Percentile value 50.*'
  prefs: []
  type: TYPE_NORMAL
- en: We observe minor, acceptable accuracy loss related to DP.
  prefs: []
  type: TYPE_NORMAL
- en: Lessons learned
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Important lesson learned is that differential privacy outcomes are very sensitive
    to parameters of a given DP algorithm and it’s hard to tune it to avoid total
    collapse of model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we experienced some kind of anxiety, based on the impression of not really
    really knowing how much privacy protection we have gained for what price. We only
    saw the “cost” side (accuracy degradation).
  prefs: []
  type: TYPE_NORMAL
- en: We had to rely to a major extent on known literature, that says and demonstrated,
    that even small amounts of DP-noise are helping to secure data.
  prefs: []
  type: TYPE_NORMAL
- en: As engineers, we’d like to see some type of automatic measure that would prove
    how much privacy we gained for how much accuracy lost, and maybe even some kind
    of AutoDP tuning. Seems to be far, far away from the current state of technology
    and knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Then we applied privacy meters to see if there’s a visible difference between
    models without DP versus models with DP and we observed changes in the curve,
    but it’s really hard to quantify how much we gained.
  prefs: []
  type: TYPE_NORMAL
- en: Some algorithms didn’t work at all, some required many attempts to tune them
    properly to deliver viable results. There was no clear guidance on how to tune
    different parameters for particular dataset and ML models.
  prefs: []
  type: TYPE_NORMAL
- en: So our current opinion is that DP for FL is hard, but totally feasible. It requires
    a lot of iterations, and trial and error loops to achieve acceptable results while
    believing in privacy improvements of orders of magnitude based on the trust in
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The future
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Federated learning is a great option to improve patients’ outcomes and treatment
    efficacy because of the improved ML models while preserving patients’ data.
  prefs: []
  type: TYPE_NORMAL
- en: But data protection never comes without any price and differential privacy for
    federated learning is a perfect example of that trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: It’s great to see improvements in algorithms of differential privacy for federated
    learning scenarios to minimize the impact on accuracy while maximizing resilience
    of models against inference attacks.
  prefs: []
  type: TYPE_NORMAL
- en: As with all trade-offs the decisions have to be made balancing usefulness of
    models for practical applications against the risks of data leakage and reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s where our expectation for privacy meters are growing to know more
    precisely what we are selling and we are “buying”, what the exchange ratio is.
  prefs: []
  type: TYPE_NORMAL
- en: The landscape is dynamic, with better tools available for both those who want
    to better protect their data and those who are motivated to violate those rules
    and expose sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: We also invite other federated minds to build upon and contribute to the collective
    effort of advancing patient data privacy for Federated Learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The author would like to thank [Jacek Chmiel](https://www.linkedin.com/in/jacekchmiel/)
    for significant impact on the blog post itself, as well as the people who helped
    develop these ideas and bring them to practice: [Jacek Chmiel](https://www.linkedin.com/in/jacekchmiel/),
    [Lukasz Antczak](https://www.linkedin.com/in/%C5%82ukasz-antczak-a3475910b/),
    Grzegory Gajda and the Federated Open Science team at Roche.'
  prefs: []
  type: TYPE_NORMAL
- en: '*All images in this article were created by the authors.*'
  prefs: []
  type: TYPE_NORMAL
