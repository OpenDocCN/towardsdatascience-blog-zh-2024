<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>6 Step Framework to Manage Reputational & Ethical Risks of Generative AI in Your Product</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>6 Step Framework to Manage Reputational & Ethical Risks of Generative AI in Your Product</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/6-step-framework-to-manage-ethical-risks-of-generative-ai-in-your-product-7ed46783d282?source=collection_archive---------7-----------------------#2024-02-19">https://towardsdatascience.com/6-step-framework-to-manage-ethical-risks-of-generative-ai-in-your-product-7ed46783d282?source=collection_archive---------7-----------------------#2024-02-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="0e50" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A pragmatic guide to understand AI risks and build trust with users through responsible AI development</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sarthakh330?source=post_page---byline--7ed46783d282--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sarthak Handa" class="l ep by dd de cx" src="../Images/0c75ba0f085fdb22a221705450047c40.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*gQdzjTlAAyqFByyRYi1vMg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--7ed46783d282--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@sarthakh330?source=post_page---byline--7ed46783d282--------------------------------" rel="noopener follow">Sarthak Handa</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--7ed46783d282--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">6</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/a0471e3425a437220dcefcbd1449eb78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*89vR3UolpF0xfLz-AXH_BQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Source: Dalle-3</figcaption></figure><p id="0cb9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the fast-paced technology landscape, product teams feel the relentless pressure to rush innovative AI offerings to market. However, prioritizing speed over ethical and safety considerations can severely backfire. When AI systems breach social norms and values, organizations bear the risk of facing long-lasting reputational damage and losing trust with users.</p><p id="2c5d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Companies are caught in a real-world “<em class="ny">prisoner’s dilemma</em>” of the AI era — while collective adherence to ethical development is ideal, it’s often more tempting for companies to secure a first-mover advantage by compromising on these crucial standards. This tension is exemplified by the infamous <strong class="ne fr">Facebook-Cambridge Analytica</strong> scandal, where taking ethical shortcuts led to grave, long-lasting reputational damage. As AI becomes more entrenched in our daily lives, the stakes of this ethical gamble only heightens, with each player in the industry weighing immediate gains against the longer-term value of trust and responsibility.</p><p id="cc2a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With the rise in AI adoption across industries since 2018, high-profile incidents of ethical violations have sharply increased. Mapping out these risks and mitigation strategies is now pivotal. This article probes the key source of AI risks, examines a few high-profile fallouts, and lays out a 6 steps framework for building products that counter these threats.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="001e" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Identifying the Risks of AI</h1><h2 id="6b91" class="pd oi fq bf oj pe pf pg om ph pi pj op nl pk pl pm np pn po pp nt pq pr ps pt bk"><strong class="al"><em class="pu">1. Privacy Intrusion &amp; Consent Issue</em></strong></h2><p id="3f43" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk"><strong class="ne fr">Risk:</strong> Arguably the most prevalent AI pitfall is the infringement of privacy and copyright laws. This entails two distinct types of failures: failure to secure consent for the use of data, and using the data for purposes beyond the scope for which the consent was given.</p><p id="9ce8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Example: A</strong>rtists and writers have filed class action lawsuits against <strong class="ne fr">OpenAI</strong> and <strong class="ne fr">MidJourney</strong> for training models with their work without consent. Similarly, <strong class="ne fr">Getty Images</strong> is suing <strong class="ne fr">Stability AI</strong> for using its data for model training. Privacy breaches can also occur even when consent is given but the data is used for unintended purposes. For example, <strong class="ne fr">Google DeepMind</strong> used the data of 1.6 million patients at the <strong class="ne fr">Royal London NHS Foundation Trust</strong> to build a new healthcare application. Although there was an implied consent that the data could be used to improve patients’ health, the Trust and DeepMind did not clearly communicate to patients that their information is being used to build an app.</p><div class="qa qb qc qd qe qf"><a href="https://www.reuters.com/legal/litigation/artists-take-new-shot-stability-midjourney-updated-copyright-lawsuit-2023-11-30/?source=post_page-----7ed46783d282--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Artists take new shot at Stability, Midjourney in updated copyright lawsuit</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">A group of visual artists has filed an amended copyright lawsuit against Stability AI, Midjourney and other companies…</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">www.reuters.com</p></div></div><div class="qo l"><div class="qp l qq qr qs qo qt lr qf"/></div></div></a></div><div class="qa qb qc qd qe qf"><a href="https://www.wired.co.uk/article/google-deepmind-nhs-health-data?source=post_page-----7ed46783d282--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Why Google consuming DeepMind Health is scaring privacy experts</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">DeepMind Health has run into data protection problems before and it being moved fully into Google raises more…</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">www.wired.co.uk</p></div></div><div class="qo l"><div class="qu l qq qr qs qo qt lr qf"/></div></div></a></div><h2 id="c21d" class="pd oi fq bf oj pe pf pg om ph pi pj op nl pk pl pm np pn po pp nt pq pr ps pt bk"><strong class="al"><em class="pu">2. Algorithmic Bias</em></strong></h2><p id="9979" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk"><strong class="ne fr">Risk</strong>: This risk involves AI systems making biased predictions, which systematically disadvantages or excludes certain group of people based on characteristics like race, gender, or socio-economic background. Such biases can have significant societal impact, especially when the AI is used to make critical decisions impacting the lives of the individuals.</p><p id="2774" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Example:</strong> There was backlash against Apple in 2019, when allegations surfaced that men received higher credit limits for <strong class="ne fr">Apple Credit Cards</strong> than women, despite women having better credit scores in some cases. Other notable instances include AI-driven recruitment software and criminal justice applications, such as the <strong class="ne fr">COMPAS</strong> tool, which have been criticized for displaying racial and gender bias.</p><div class="qa qb qc qd qe qf"><a href="https://www.cnn.com/2019/11/12/business/apple-card-gender-bias/index.html?source=post_page-----7ed46783d282--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Apple Card is accused of gender bias. Here's how that can happen | CNN Business</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Some Apple Card customers say the credit card's issuer, Goldman Sachs, is giving women far lower credit limits, even if…</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">www.cnn.com</p></div></div><div class="qo l"><div class="qv l qq qr qs qo qt lr qf"/></div></div></a></div><div class="qa qb qc qd qe qf"><a href="https://www.nytimes.com/2017/10/26/opinion/algorithm-compas-sentencing-bias.html?source=post_page-----7ed46783d282--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Opinion | When an Algorithm Helps Send You to Prison (Published 2017)</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Giving a computer program responsibility over sentences doesn't eliminate bias.</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">www.nytimes.com</p></div></div><div class="qo l"><div class="qw l qq qr qs qo qt lr qf"/></div></div></a></div><h2 id="1268" class="pd oi fq bf oj pe pf pg om ph pi pj op nl pk pl pm np pn po pp nt pq pr ps pt bk"><strong class="al"><em class="pu">3. Accuracy Risk &amp; Explainability Gap</em></strong></h2><p id="3b4d" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk"><strong class="ne fr">Risk: </strong>Significant risks arise when AI systems that are used for making high-stakes decisions provide inaccurate results or fail to offer a clear rationale for their output. The ‘black box’ nature of the AI makes it difficult for users to understand and verify its results, which obscures accountability and leads to a loss of trust.</p><p id="abde" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Example:</strong> <strong class="ne fr">IBM Watson for Oncology</strong> was designed to provide clinicians with personalized recommendations for cancer treatment. However, reports surfaced that Watson gave unsafe and incorrect treatment suggestions, which led to a loss of trust in technology and damaged IBM’s reputation.</p><div class="qa qb qc qd qe qf"><a href="https://www.statnews.com/2018/07/25/ibm-watson-recommended-unsafe-incorrect-treatments/?source=post_page-----7ed46783d282--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">IBM's Watson supercomputer recommended 'unsafe and incorrect' cancer treatments, internal documents…</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Slide decks presented last summer by an IBM Watson Health executive largely blame the problems on the training of…</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">www.statnews.com</p></div></div><div class="qo l"><div class="qx l qq qr qs qo qt lr qf"/></div></div></a></div><h2 id="83c1" class="pd oi fq bf oj pe pf pg om ph pi pj op nl pk pl pm np pn po pp nt pq pr ps pt bk"><strong class="al"><em class="pu">4. Security Risk</em></strong></h2><p id="c2ff" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk"><strong class="ne fr">Risk</strong>: A significant societal risks emerge from the use of AI to create deepfakes, which involves generating highly convincing images or video that makes it appear as though individuals are saying or doing things they never actually did. Deepfakes have been used to create deceptive and realistic media that can be used for committing fraud, spreading misinformation, or damaging reputations. Moreover, AI can be weaponized for cyberattacks and social engineering, like tailoring phishing campaigns, which can introduce extensive security vulnerabilities.</p><p id="a1fe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Example</strong>: In the political domain, use of deepfakes to fabricate speeches or actions could sway public opinion during critical events like elections. During the <strong class="ne fr">Russia-Ukraine conflict</strong>, a deepfake showing Ukrainian president appearing to tell his soldiers to lay down arms and surrender was circulated, an act that could have demoralized troops and provided time-sensitive tactical advantage to Russia. In the cybersecurity realm, AI-powered voice imitation was used to impersonate a CEO’s voice convincingly, leading to a fraudulent transfer of funds.</p><div class="qa qb qc qd qe qf"><a href="https://www.npr.org/2022/03/16/1087062648/deepfake-video-zelenskyy-experts-war-manipulation-ukraine-russia?source=post_page-----7ed46783d282--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Deepfake video of Zelenskyy could be 'tip of the iceberg' in info war, experts warn</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">A fake video of the Ukrainian president claiming defeat spread on social media on Wednesday.</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">www.npr.org</p></div></div><div class="qo l"><div class="qy l qq qr qs qo qt lr qf"/></div></div></a></div><div class="qa qb qc qd qe qf"><a href="https://www.trendmicro.com/vinfo/mx/security/news/cyber-attacks/unusual-ceo-fraud-via-deepfake-audio-steals-us-243-000-from-u-k-company?source=post_page-----7ed46783d282--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Unusual CEO Fraud via Deepfake Audio Steals US$243,000 From UK Company</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">An unusual case of CEO fraud used a deepfake audio, an artificial intelligence (AI)-generated audio, and was reported…</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">www.trendmicro.com</p></div></div><div class="qo l"><div class="qz l qq qr qs qo qt lr qf"/></div></div></a></div></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0b7d" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk"><strong class="al">6 Step Framework for Mitigating AI Risks</strong></h1><p id="b627" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk">Managing AI risks requires a thoughtful approach throughout the entire product life-cycle. Below is a six step framework, organized by different stages of AI development, that organizations can adopt to ensure the responsible use of AI technology in their products.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ra"><img src="../Images/ef48c5b206b4ec5173e6f452dc24bc0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SbYWRuoO7ZEwTtgawuX58Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Source: Author</figcaption></figure><h2 id="64ac" class="pd oi fq bf oj pe pf pg om ph pi pj op nl pk pl pm np pn po pp nt pq pr ps pt bk"><strong class="al"><em class="pu">1. Pre-Development: Ethical Groundwork and Design Principles</em></strong></h2><p id="1d4d" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk">Before a single line of code is written, product teams should lay out the groundwork. Prioritize early engagement with a broad set of stakeholders, including users, technical experts, ethicists, legal professionals, and members of communities who may be impacted by the AI application. The goal is to identify both the overt and subtle risks associated with the product’s use case. Use these insights to chalk out the set of ethical guidelines and product capabilities that needs to be embedded into the product prior to its launch to preemptively address the identified risks.</p><h2 id="02f8" class="pd oi fq bf oj pe pf pg om ph pi pj op nl pk pl pm np pn po pp nt pq pr ps pt bk"><strong class="al"><em class="pu">2. Development: Data Consent, Integrity, Diversity</em></strong></h2><p id="77f1" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk">Data is the bedrock of AI and also the most significant source of AI risks. It is critical to ensure that all data procured for model training are ethically sourced and comes with consent for its intended use. For example, <strong class="ne fr">Adobe</strong> trained its image generation model (<strong class="ne fr">Firefly</strong>) with proprietary data which allows it to provide legal protection to users against copyright lawsuits.</p><p id="dfca" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Further, Personally Identifiable Information (PII) should be removed from sensitive datasets used for training models to prevent potential harm. Access to such datasets should be appropriately gated and tracked to protect privacy. It’s equally important to ensure that the datasets represent the diversity of user base and the breadth of usage scenarios to mitigate bias and fairness risks. Companies like <strong class="ne fr">Runway</strong> have trained their text-to-image models with synthetic datasets containing AI-generated images of people from different ethnicities, genders, professions, and ages to ensure that their AI models exhibit diversity in the content they create.</p><div class="qa qb qc qd qe qf"><a href="https://www.nasdaq.com/articles/how-adobes-copyright-protection-will-make-it-an-ai-leader?source=post_page-----7ed46783d282--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">How Adobe's Copyright Protection Will Make it an AI Leader</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Artificial intelligence (AI) is fundamentally reshaping many industries. AI will have many benefits, but it's also…</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">www.nasdaq.com</p></div></div><div class="qo l"><div class="rb l qq qr qs qo qt lr qf"/></div></div></a></div><div class="qa qb qc qd qe qf"><a href="https://research.runwayml.com/publications/mitigating-stereotypical-biases-in-text-to-image-generative-systems?source=post_page-----7ed46783d282--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Mitigating stereotypical biases in text to image generative systems | Runway Research</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Reimagining creativity with artificial intelligence.</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">research.runwayml.com</p></div></div><div class="qo l"><div class="rc l qq qr qs qo qt lr qf"/></div></div></a></div><h2 id="4c82" class="pd oi fq bf oj pe pf pg om ph pi pj op nl pk pl pm np pn po pp nt pq pr ps pt bk"><strong class="al"><em class="pu">3. Development: Robustness Testing and Implementing Guardrails</em></strong></h2><p id="c266" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk">The testing phase is pivotal in determining AI’s readiness for a public release. This involves comparing AI’s output against the curated set of verified results. An effective testing uses:</p><ul class=""><li id="aed3" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx rd re rf bk"><strong class="ne fr">Performance Metrics </strong>aligned with user objectives and business values,</li><li id="8a61" class="nc nd fq ne b go rg ng nh gr rh nj nk nl ri nn no np rj nr ns nt rk nv nw nx rd re rf bk"><strong class="ne fr">Evaluation Data</strong> representing users from different demographics and covering a range of usage scenarios, including edge-cases</li></ul><p id="d86e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In addition to performance testing, it is also critical to implement guardrails that prevents AI from producing harmful results. For instance, <strong class="ne fr">ImageFX</strong>, <strong class="ne fr">Google</strong>’s Image generation service, proactively blocks users from generating content that could be deemed inappropriate or used to spread misinformation. Similarly, <strong class="ne fr">Anthropic</strong> has proactively set guardrails and measures to avoid misuse of its AI services in 2024 elections.</p><div class="qa qb qc qd qe qf"><a href="https://blog.google/technology/ai/google-imagen-2/?source=post_page-----7ed46783d282--------------------------------#:~:text=Our%20responsible%20approach%20to%20building%20Imagen%202" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">New and better ways to create images with Imagen 2</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">We're rolling out Imagen 2, a major update to our image generation technology. Try it out today in Bard, Image FX…</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">blog.google</p></div></div><div class="qo l"><div class="rl l qq qr qs qo qt lr qf"/></div></div></a></div><div class="qa qb qc qd qe qf"><a href="https://www.linkedin.com/posts/anthropicresearch_preparing-for-global-elections-in-2024-activity-7164707718656643073-VcaW?utm_source=share&amp;utm_medium=member_desktop&amp;source=post_page-----7ed46783d282--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Anthropic on LinkedIn: Preparing for global elections in 2024</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">With high-stakes elections taking place around the world in 2024, we're shedding light on some of the work that our…</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">www.linkedin.com</p></div></div><div class="qo l"><div class="rm l qq qr qs qo qt lr qf"/></div></div></a></div><h2 id="59d9" class="pd oi fq bf oj pe pf pg om ph pi pj op nl pk pl pm np pn po pp nt pq pr ps pt bk"><strong class="al"><em class="pu">4. Development: Explainability &amp; Empowerment</em></strong></h2><p id="0cf4" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk">In critical industry use cases where building trust is pivotal, it’s important for the AI to enable humans in an assistive role. This can be achieved by:</p><ul class=""><li id="68bc" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx rd re rf bk">Providing citations for the sources of the AI’s insights.</li><li id="75cc" class="nc nd fq ne b go rg ng nh gr rh nj nk nl ri nn no np rj nr ns nt rk nv nw nx rd re rf bk">Highlighting the uncertainty or confidence-level of the AI’s prediction.</li><li id="0f9c" class="nc nd fq ne b go rg ng nh gr rh nj nk nl ri nn no np rj nr ns nt rk nv nw nx rd re rf bk">Offering users the option to opt-out of using the AI.</li><li id="9729" class="nc nd fq ne b go rg ng nh gr rh nj nk nl ri nn no np rj nr ns nt rk nv nw nx rd re rf bk">Creating application workflows that ensure human oversight and prevent some tasks from being fully automated.</li></ul><h2 id="2a11" class="pd oi fq bf oj pe pf pg om ph pi pj op nl pk pl pm np pn po pp nt pq pr ps pt bk"><strong class="al"><em class="pu">5. Deployment: Progressive Roll Out &amp; Transparency</em></strong></h2><p id="f9b6" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk">As you transition the AI systems from development to real-world deployment, adopting a phased roll-out strategy is crucial for assessing risks and gathering feedback in a controlled setting. It’s also important to clearly communicate the AI’s intended use case, capabilities, and limitations to users and stakeholders. Transparency at this stage helps manage expectations and mitigates reputational risks associated with unexpected failures of the AI system.</p><p id="e0ac" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">OpenAI</strong>, for example, demonstrated this approach with <strong class="ne fr">Sora</strong>, its latest text-to-video service, by initially making the service available to only a select group of red teamers and creative professionals. It has been upfront about Sora’s capabilities as well as its current limitations, such as challenges in generating video involving complex physical interactions. This level of disclosure ensures users understand where the technology excels and where it might fail, thereby managing expectations, earning users’ trust, and facilitating responsible adoption of the AI technology.</p><div class="qa qb qc qd qe qf"><a href="https://openai.com/sora?source=post_page-----7ed46783d282--------------------------------#safety" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Sora: Creating video from text</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Sora is an AI model that can create realistic and imaginative scenes from text instructions. Read technical report All…</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">openai.com</p></div></div><div class="qo l"><div class="rn l qq qr qs qo qt lr qf"/></div></div></a></div><h2 id="5024" class="pd oi fq bf oj pe pf pg om ph pi pj op nl pk pl pm np pn po pp nt pq pr ps pt bk"><strong class="al"><em class="pu">6. Deployment: Monitoring, Feedback, and Adaptation</em></strong></h2><p id="f840" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk">After an AI system goes live, the work isn’t over. Now comes the task of keeping a close watch on how the AI behaves in the wild and tuning it based on what you find. Create an ongoing mechanism to track performance drifts and continually test and train the model on fresh data to avoid degradation in the AI performance. Make it easy for users to flag issues and use these insights to adapt AI and constantly update guardrails to meet high ethical standards. This will ensure that the AI systems remain reliable, trustworthy, and in step with the dynamic world they operate in.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="b8c1" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk"><strong class="al">Conclusion:</strong></h1><p id="a0eb" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk">As AI becomes further entrenched in our lives, managing ethical risks proactively is no longer optional — it is imperative. By embedding ethical considerations in every step of bringing AI products to life, companies not only mitigate risks but also build foundational trust with their users. The era of “<em class="ny">move fast and break things</em>” cannot be applied when dealing with technologies that are exponentially more powerful than anything we have seen before. There are no shortcuts when it comes to managing risks that can have far-reaching societal impacts.</p><p id="3e55" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">AI product builders have a duty to society to move more intentionally and purposefully, making trust their true North Star. The future success and continued progress of AI hinges on getting the ethics right today.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8bae" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">Thanks for reading! If these insights resonate with you or spark new thoughts, let’s continue the conversation. Share your perspectives in the comments below or connect with me on </em><a class="af ro" href="https://www.linkedin.com/" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr"><em class="ny">LinkedIn</em></strong></a><em class="ny">.</em></p></div></div></div></div>    
</body>
</html>