- en: 'Text Embeddings: Comprehensive Guide'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/text-embeddings-comprehensive-guide-afd97fce8fb5?source=collection_archive---------0-----------------------#2024-02-13](https://towardsdatascience.com/text-embeddings-comprehensive-guide-afd97fce8fb5?source=collection_archive---------0-----------------------#2024-02-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Evolution, visualisation, and applications of text embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://miptgirl.medium.com/?source=post_page---byline--afd97fce8fb5--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page---byline--afd97fce8fb5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--afd97fce8fb5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--afd97fce8fb5--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page---byline--afd97fce8fb5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--afd97fce8fb5--------------------------------)
    ·20 min read·Feb 13, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/edce20eff0ac79af415ddaee3b40cfc3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by DALL-E 3
  prefs: []
  type: TYPE_NORMAL
- en: As human beings, we can read and understand texts (at least some of them). Computers
    in opposite “think in numbers”, so they can’t automatically grasp the meaning
    of words and sentences. If we want computers to understand the natural language,
    we need to convert this information into the format that computers can work with
    — vectors of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: People learned how to convert texts into machine-understandable format many
    years ago (one of the first versions was [ASCII](https://en.wikipedia.org/wiki/ASCII)).
    Such an approach helps render and transfer texts but doesn’t encode the meaning
    of the words. At that time, the standard search technique was a keyword search
    when you were just looking for all the documents that contained specific words
    or N-grams.
  prefs: []
  type: TYPE_NORMAL
- en: Then, after decades, embeddings have emerged. We can calculate embeddings for
    words, sentences, and even images. Embeddings are also vectors of numbers, but
    they can capture the meaning. So, you can use them to do a semantic search and
    even work with documents in different languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I would like to dive deeper into the embedding topic and discuss
    all the details:'
  prefs: []
  type: TYPE_NORMAL
- en: what preceded the embeddings and how they evolved,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how to calculate embeddings using OpenAI tools,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how to define whether sentences are close to each other,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how to visualise embeddings,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the most exciting part is how you could use embeddings in practice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s move on and learn about the evolution of embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start our journey with a brief tour into the history of text representations.
  prefs: []
  type: TYPE_NORMAL
- en: Bag of Words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most basic approach to converting texts into vectors is a bag of words.
    Let’s look at one of the famous quotes of Richard P. Feynman*“We are lucky to
    live in an age in which we are still making discoveries”.* We will use it to illustrate
    a bag of words approach.
  prefs: []
  type: TYPE_NORMAL
- en: The first step to get a bag of words vector is to split the text into words
    (tokens) and then reduce words to their base forms. For example, *“running”* will
    transform into *“run”*. This process is called stemming. We can use the NLTK Python
    package for it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have a list of base forms of all our words. The next step is to calculate
    their frequencies to create a vector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Actually, if we wanted to convert our text into a vector, we would have to take
    into account not only the words we have in the text but the whole vocabulary.
    Let’s assume we also have *“i”*, *“you”* and *”study”* in our vocabulary and let’s
    create a vector from Feynman’s quote.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/352726337fbddb935f18fb369b489d69.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: This approach is quite basic, and it doesn’t take into account the semantic
    meaning of the words, so the sentences *“the girl is studying data science”* and
    *“the young woman is learning AI and ML”* won’t be close to each other.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A slightly improved version of the bag of the words approach is [**TF-IDF**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
    (*Term Frequency — Inverse Document Frequency*). It’s the multiplication of two
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48834842d5c59c0eb25c7ba84ffa02da.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Term Frequency** shows the frequency of the word in the document. The most
    common way to calculate it is to divide the raw count of the term in this document
    (like in the bag of words) by the total number of terms (words) in the document.
    However, there are many other approaches like just raw count, boolean “frequencies”,
    and different approaches to normalisation. You can learn more about different
    approaches on [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/4b88c942246dd26a9376e31a1c7eb065.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Inverse Document Frequency** denotes how much information the word provides.
    For example, the words *“a”* or *“that”* don’t give you any additional information
    about the document’s topic. In contrast, words like *“ChatGPT”* or *“bioinformatics”*
    can help you define the domain (but not for this sentence). It’s calculated as
    the logarithm of the ratio of the total number of documents to those containing
    the word. The closer IDF is to 0 — the more common the word is and the less information
    it provides.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/a07031a5a4933e2fb90ac8384d62f5a8.png)'
  prefs: []
  type: TYPE_IMG
- en: So, in the end, we will get vectors where common words (like *“I”* or *“you”*)
    will have low weights, while rare words that occur in the document multiple times
    will have higher weights. This strategy will give a bit better results, but it
    still can’t capture semantic meaning.
  prefs: []
  type: TYPE_NORMAL
- en: The other challenge with this approach is that it produces pretty sparse vectors.
    The length of the vectors is equal to the corpus size. There are about 470K unique
    words in English ([source](https://en.wikipedia.org/wiki/List_of_dictionaries_by_number_of_words)),
    so we will have huge vectors. Since the sentence won’t have more than 50 unique
    words, 99.99% of the values in vectors will be 0, not encoding any info. Looking
    at this, scientists started to think about dense vector representation.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most famous approaches to dense representation is word2vec, proposed
    by Google in 2013 in the paper [“Efficient Estimation of Word Representations
    in Vector Space”](https://arxiv.org/abs/1301.3781) by Mikolov et al.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two different word2vec approaches mentioned in the paper: Continuous
    Bag of Words (when we predict the word based on the surrounding words) and Skip-gram
    (the opposite task — when we predict context based on the word).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a55f2b25a828530cca95fcce2c9901fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure from the paper by Mikolov et al. 2013 | [source](https://arxiv.org/pdf/1301.3781.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'The high-level idea of dense vector representation is to train two models:
    encoder and decoder. For example, in the case of skip-gram, we might pass the
    word *“christmas”* to the encoder. Then, the encoder will produce a vector that
    we pass to the decoder expecting to get the words *“merry”*, *“to”*, and *“you”*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eaa26d1a1c68799c60dfe915ea18569b.png)'
  prefs: []
  type: TYPE_IMG
- en: Scheme by author
  prefs: []
  type: TYPE_NORMAL
- en: This model started to take into account the meaning of the words since it’s
    trained on the context of the words. However, it ignores morphology (information
    we can get from the word parts, for example, that “*-less”* means the lack of
    something). This drawback was addressed later by looking at subword skip-grams
    in [GloVe](https://www-nlp.stanford.edu/pubs/glove.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Also, word2vec was capable of working only with words, but we would like to
    encode whole sentences. So, let’s move on to the next evolutional step with transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers and Sentence Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next evolution was related to the transformers approach introduced in the
    [“Attention Is All You Need”](https://arxiv.org/abs/1706.03762) paper by Vaswani
    et al. Transformers were able to produce information-reach dense vectors and become
    the dominant technology for modern language models.
  prefs: []
  type: TYPE_NORMAL
- en: I won’t cover the details of the transformers’ architecture since it’s not so
    relevant to our topic and would take a lot of time. If you’re interested in learning
    more, there are a lot of materials about transformers, for example, [“Transformers,
    Explained”](https://daleonai.com/transformers-explained) or [“The Illustrated
    Transformer”](https://jalammar.github.io/illustrated-transformer/).
  prefs: []
  type: TYPE_NORMAL
- en: Transformers allow you to use the same “core” model and fine-tune it for different
    use cases without retraining the core model (which takes a lot of time and is
    quite costly). It led to the rise of pre-trained models. One of the first popular
    models was BERT (Bidirectional Encoder Representations from Transformers) by Google
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: Internally, BERT still operates on a token level similar to word2vec, but we
    still want to get sentence embeddings. So, the naive approach could be to take
    an average of all tokens’ vectors. Unfortunately, this approach doesn’t show good
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: This problem was solved in 2019 when [Sentence-BERT](https://arxiv.org/abs/1908.10084)
    was released. It outperformed all previous approaches to semantic textual similarity
    tasks and allowed the calculation of sentence embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: It’s a huge topic so we won’t be able to cover it all in this article. So, if
    you’re really interested, you can learn more about the sentence embeddings in
    [this article](https://www.pinecone.io/learn/series/nlp/sentence-embeddings/).
  prefs: []
  type: TYPE_NORMAL
- en: We’ve briefly covered the evolution of embeddings and got a high-level understanding
    of the theory. Now, it’s time to move on to practice and lear how to calculate
    embeddings using OpenAI tools.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we will be using OpenAI embeddings. We will try a new model
    `text-embedding-3-small` that was [released](https://openai.com/blog/new-embedding-models-and-api-updates)
    just recently. The new model shows better performance compared to `text-embedding-ada-002`:'
  prefs: []
  type: TYPE_NORMAL
- en: The average score on a widely used multi-language retrieval ([MIRACL](https://github.com/project-miracl/miracl))
    benchmark has risen from 31.4% to 44.0%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The average performance on a frequently used benchmark for English tasks ([MTEB](https://github.com/embeddings-benchmark/mteb))
    has also improved, rising from 61.0% to 62.3%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI also released a new larger model `text-embedding-3-large`. Now, it’s
    their best performing embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: As a data source, we will be working with a small sample of [Stack Exchange
    Data Dump](https://archive.org/details/stackexchange) — an anonymised dump of
    all user-contributed content on the [Stack Exchange network](https://stackexchange.com/).
    I’ve selected a bunch of topics that look interesting to me and sample 100 questions
    from each of them. Topics range from Generative AI to coffee or bicycles so that
    we will see quite a wide variety of topics.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to calculate embeddings for all our Stack Exchange questions.
    It’s worth doing it once and storing results locally (in a file or vector storage).
    We can generate embeddings using the OpenAI Python package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As a result, we got a 1536-dimension vector of float numbers. We can now repeat
    it for all our data and start analysing the values.
  prefs: []
  type: TYPE_NORMAL
- en: The primary question you might have is how close the sentences are to each other
    by meaning. To uncover answers, let’s discuss the concept of distance between
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Distance between vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embeddings are actually vectors. So, if we want to understand how close two
    sentences are to each other, we can calculate the distance between vectors. A
    smaller distance would be equivalent to a closer semantic meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different metrics can be used to measure the distance between two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance (L2),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manhattant distance (L1),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dot product,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine distance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss them. As a simple example, we will be using two 2D vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Euclidean distance (L2)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most standard way to define distance between two points (or vectors) is
    Euclidean distance or L2 norm. This metric is the most commonly used in day-to-day
    life, for example, when we are talking about the distance between 2 towns.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a visual representation and formula for L2 distance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4d3008368443f6d56ba82b20c182b3a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We can calculate this metric using vanilla Python or leveraging the numpy function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Manhattant distance (L1)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The other commonly used distance is the L1 norm or Manhattan distance. This
    distance was called after the island of Manhattan (New York). This island has
    a grid layout of streets, and the shortest routes between two points in Manhattan
    will be L1 distance since you need to follow the grid.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4e01e943ce4e010819197e8f2110ce5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We can also implement it from scratch or use the numpy function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Dot product
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way to look at the distance between vectors is to calculate a dot or
    scalar product. Here’s a formula and we can easily implement it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3e514aa4acf640b217c3c129beb49f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This metric is a bit tricky to interpret. On the one hand, it shows you whether
    vectors are pointing in one direction. On the other hand, the results highly depend
    on the magnitudes of the vectors. For example, let’s calculate the dot products
    between two pairs of vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(1, 1)` vs `(1, 1)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(1, 1)` vs `(10, 10)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In both cases, vectors are collinear, but the dot product is ten times bigger
    in the second case: 2 vs 20.'
  prefs: []
  type: TYPE_NORMAL
- en: Cosine similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quite often, cosine similarity is used. Cosine similarity is a dot product normalised
    by vectors’ magnitudes (or normes).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a44f78804a4d51ccf45138b17cc279c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We can either calculate everything ourselves (as previously) or use the function
    from sklearn.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The function `cosine_similarity` expects 2D arrays. That’s why we need to reshape
    the numpy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s talk a bit about the physical meaning of this metric. Cosine similarity
    is equal to the cosine between two vectors. The closer the vectors are, the higher
    the metric value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f20da9af06b65db763be0113f19108f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We can even calculate the exact angle between our vectors in degrees. We get
    results around 30 degrees, and it looks pretty reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: What metric to use?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve discussed different ways to calculate the distance between two vectors,
    and you might start thinking about which one to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use any distance to compare the embeddings you have. For example, I
    calculated the average distances between the different clusters. Both L2 distance
    and cosine similarity show us similar pictures:'
  prefs: []
  type: TYPE_NORMAL
- en: Objects within a cluster are closer to each other than to other clusters. It’s
    a bit tricky to interpret our results since for L2 distance, closer means lower
    distance, while for cosine similarity — the metric is higher for closer objects.
    Don’t get confused.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can spot that some topics are really close to each other, for example, *“politics”*
    and *“economics”* or *“ai”* and *“datascience”*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/c2b2f26c939f419047e52a0c1c7a1fac.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4dc3fdc10fb1dc9616d7ff13a18d177e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for NLP tasks, the best practice is usually to use cosine similarity.
    Some reasons behind it:'
  prefs: []
  type: TYPE_NORMAL
- en: Cosine similarity is between -1 and 1, while L1 and L2 are unbounded, so it’s
    easier to interpret.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the practical perspective, it’s more effective to calculate dot products
    than square roots for Euclidean distance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine similarity is less affected by the curse of dimensionality (we will talk
    about it in a second).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI embeddings are already normed, so dot product and cosine similarity are
    equal in this case.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You might spot in the results above that the difference between inter- and
    intra-cluster distances is not so big. The root cause is the high dimensionality
    of our vectors. This effect is called “the curse of dimensionality”: the higher
    the dimension, the narrower the distribution of distances between vectors. You
    can learn more details about it in [this article](https://towardsai.net/p/l/why-should-euclidean-distance-not-be-the-default-distance-measure).'
  prefs: []
  type: TYPE_NORMAL
- en: I would like to briefly show you how it works so that you get some intuition.
    I calculated a distribution of OpenAI embedding values and generated sets of 300
    vectors with different dimensionalities. Then, I calculated the distances between
    all the vectors and draw a histogram. You can easily see that the increase in
    vector dimensionality makes the distribution narrower.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fbfdf3c34e03b2ae519e305d5703bfef.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: We’ve learned how to measure the similarities between the embeddings. With that
    we’ve finished with a theoretical part and moving to more practical part (visualisations
    and practical applications). Let’s start with visualisations since it’s always
    better to see your data first.
  prefs: []
  type: TYPE_NORMAL
- en: Visualising embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The best way to understand the data is to visualise it. Unfortunately, embeddings
    have 1536 dimensions, so it’s pretty challenging to look at the data. However,
    there’s a way: we could use dimensionality reduction techniques to project vectors
    in two-dimensional space.'
  prefs: []
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most basic dimensionality reduction technique is [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)
    (Principal Component Analysis). Let’s try to use it.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to convert our embeddings into a 2D numpy array to pass it to
    sklearn.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Then, we need to initialise a PCA model with `n_components = 2` (because we
    want to create a 2D visualisation), train the model on the whole data and predict
    new values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As a result, we got a matrix with just two features for each question, so we
    could easily visualise it on a scatter plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a0683ee9c8cfe52995691ef7887777cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We can see that questions from each topic are pretty close to each other, which
    is good. However, all the clusters are mixed, so there’s room for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA is a linear algorithm, while most of the relations are non-linear in real
    life. So, we may not be able to separate the clusters because of non-linearity.
    Let’s try to use a non-linear algorithm [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)
    and see whether it will be able to show better results.
  prefs: []
  type: TYPE_NORMAL
- en: The code is almost identical. I just used the t-SNE model instead of PCA.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The t-SNE result looks way better. Most of the clusters are separated except
    *“genai”*, *“datascience”* and *“ai”.* However, it’s pretty expected — I doubt
    I could separate these topics myself.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4f6590a648a74f2c411e17a989b4959.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at this visualisation, we see that embeddings are pretty good at encoding
    semantic meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Also, you can make a projection to three-dimensional space and visualise it.
    I’m not sure whether it would be practical, but it can be insightful and engaging
    to play with the data in 3D.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/aa256e26b30e0c95d5caf111543890ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Barcodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The way to understand the embeddings is to visualise a couple of them as bar
    codes and see the correlations. I picked three examples of embeddings: two are
    closest to each other, and the other is the farthest example in our dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/89478c10950950c9aee5c8cfe304754c.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: It’s not easy to see whether vectors are close to each other in our case because
    of high dimensionality. However, I still like this visualisation. It might be
    helpful in some cases, so I am sharing this idea with you.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve learned how to visualise embeddings and have no doubts left about their
    ability to grasp the meaning of the text. Now, it’s time to move on to the most
    interesting and fascinating part and discuss how you can leverage embeddings in
    practice.
  prefs: []
  type: TYPE_NORMAL
- en: Practical applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, embeddings’ primary goal is not to encode texts as vectors of numbers
    or visualise them just for the sake of it. We can benefit a lot from our ability
    to capture the texts’ meanings. Let’s go through a bunch of more practical examples.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with clustering. Clustering is an unsupervised learning technique
    that allows you to split your data into groups without any initial labels. Clustering
    can help you understand the internal structural patterns in your data.
  prefs: []
  type: TYPE_NORMAL
- en: We will use one of the most basic clustering algorithms — [K-means](https://scikit-learn.org/stable/modules/clustering.html#k-means).
    For the K-means algorithm, we need to specify the number of clusters. We can define
    the optimal number of clusters using [silhouette scores](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try k (number of clusters) between 2 and 50\. For each k, we will train
    a model and calculate silhouette scores. The higher silhouette score — the better
    clustering we got.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In our case, the silhouette score reaches a maximum when `k = 11`. So, let’s
    use this number of clusters for our final model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b555cc018bfc29bf8ecea55f54da8f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: Let’s visualise the clusters using t-SNE for dimensionality reduction as we
    already did before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Visually, we can see that the algorithm was able to define clusters quite well
    — they are separated pretty well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/994324e79d3962523b499f92912d38c2.png)'
  prefs: []
  type: TYPE_IMG
- en: We have factual topic labels, so we can even assess how good clusterisation
    is. Let’s look at the topics’ mixture for each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/58c3df19b731f65894997ea8dddad3d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In most cases, clusterisation worked perfectly. For example, cluster 5 contains
    almost only questions about bicycles, while cluster 6 is about coffee. However,
    it wasn’t able to distinguish close topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“ai”*, *“genai”* and *“datascience”* are all in one cluster,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the same store with *“economics”* and *“politics”*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used only embeddings as the features in this example, but if you have any
    additional information (for example, age, gender or country of the user who asked
    the question), you can include it in the model, too.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use embeddings for classification or regression tasks. For example, you
    can do it to predict customer reviews’ sentiment (classification) or NPS score
    (regression).
  prefs: []
  type: TYPE_NORMAL
- en: Since classification and regression are supervised learning, you will need to
    have labels. Luckily, we know the topics for our questions and can fit a model
    to predict them.
  prefs: []
  type: TYPE_NORMAL
- en: I will use a Random Forest Classifier. If you need a quick refresher about Random
    Forests, you can find it [here](https://medium.com/towards-data-science/interpreting-random-forests-638bca8b49ea).
    To assess the classification model’s performance correctly, we will split our
    dataset into train and test sets (80% vs 20%). Then, we can train our model on
    a train set and measure the quality on a test set (questions that the model hasn’t
    seen before).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: To estimate the model’s performance, let’s calculate a confusion matrix. In
    an ideal situation, all non-diagonal elements should be 0.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a401de02b5f824451d9efc6db62491d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see similar results to clusterisation: some topics are easy to classify,
    and accuracy is 100%, for example, *“bicycles”* or *“travel”*, while some others
    are difficult to distinguish (especially *“ai”*).'
  prefs: []
  type: TYPE_NORMAL
- en: However, we achieved 91.8% overall accuracy, which is quite good.
  prefs: []
  type: TYPE_NORMAL
- en: Finding anomalies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also use embedding to find anomalies in our data. For example, at the
    t-SNE graph, we saw that some questions are pretty far from their clusters, for
    instance, for the *“travel”* topic. Let’s look at this theme and try to find anomalies.
    We will use [the Isolation Forest algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)
    for it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: So, here we are. We’ve found the most uncommon comment for the travel topic
    ([source](https://travel.stackexchange.com/questions/150735/is-it-safe-to-drink-the-water-from-the-fountains-found-all-over-the-older-parts)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Since it talks about water, the embedding of this comment is close to the coffee
    topic where people also discuss water to pour coffee. So, the embedding representation
    is quite reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: We could find it on our t-SNE visualisation and see that it’s actually close
    to the *coffee* cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26e6b84079ede41be03aff139e1c984e.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: RAG — Retrieval Augmented Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the recently increased popularity of LLMs, embeddings have been broadly
    used in RAG use cases.
  prefs: []
  type: TYPE_NORMAL
- en: We need Retrieval Augmented Generation when we have a lot of documents (for
    example, all the questions from Stack Exchange), and we can’t pass them all to
    an LLM because
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have limits on the context size (right now, it’s 128K for GPT-4 Turbo).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We pay for tokens, so it’s more expensive to pass all the information all the
    time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs show worse performance with a bigger context. You can check [Needle In
    A Haystack — Pressure Testing LLMs](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
    to learn more details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To be able to work with an extensive knowledge base, we can leverage the RAG
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute embeddings for all the documents and store them in vector storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we get a user request, we can calculate its embedding and retrieve relevant
    documents from the storage for this request.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass only relevant documents to LLM to get a final answer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn more about RAG, don’t hesitate to read my article with much more details
    [here.](/rag-how-to-talk-to-your-data-eaf5469b83b0)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we’ve discussed text embeddings in much detail. Hopefully,
    now you have a complete and deep understanding of this topic. Here’s a quick recap
    of our journey:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we went through the evolution of approaches to work with texts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we discussed how to understand whether texts have similar meanings to
    each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After that, we saw different approaches to text embedding visualisation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we tried to use embeddings as features in different practical tasks
    such as clustering, classification, anomaly detection and RAG.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thank you a lot for reading this article. If you have any follow-up questions
    or comments, please leave them in the comments section.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I used a dataset from [Stack Exchange Data Dump](https://archive.org/details/stackexchange),
    which is available under [the Creative Commons license](https://creativecommons.org/licenses/by-sa/4.0/).
  prefs: []
  type: TYPE_NORMAL
- en: 'This article was inspired by the following courses:'
  prefs: []
  type: TYPE_NORMAL
- en: “[Understanding and Applying Text Embeddings”](https://www.deeplearning.ai/short-courses/google-cloud-vertex-ai/)
    by DeepLearning.AI in collaboration with Google Cloud,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Vector Databases: From Embeddings to Applications”](https://learn.deeplearning.ai/vector-databases-embeddings-applications/lesson/1/introduction)
    by DeepLearning.AI in collaboration with Weaviate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
