- en: The History of Convolutional Neural Networks for Image Classification (1989
    - Today)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络在图像分类中的历史（1989年至今）
- en: 原文：[https://towardsdatascience.com/the-history-of-convolutional-neural-networks-for-image-classification-1989-today-5ea8a5c5fe20?source=collection_archive---------5-----------------------#2024-06-28](https://towardsdatascience.com/the-history-of-convolutional-neural-networks-for-image-classification-1989-today-5ea8a5c5fe20?source=collection_archive---------5-----------------------#2024-06-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-history-of-convolutional-neural-networks-for-image-classification-1989-today-5ea8a5c5fe20?source=collection_archive---------5-----------------------#2024-06-28](https://towardsdatascience.com/the-history-of-convolutional-neural-networks-for-image-classification-1989-today-5ea8a5c5fe20?source=collection_archive---------5-----------------------#2024-06-28)
- en: A visual tour of the greatest innovations in Deep Learning and Computer Vision.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习和计算机视觉领域最伟大创新的视觉之旅。
- en: '[](https://medium.com/@neural.avb?source=post_page---byline--5ea8a5c5fe20--------------------------------)[![Avishek
    Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--5ea8a5c5fe20--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5ea8a5c5fe20--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5ea8a5c5fe20--------------------------------)
    [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--5ea8a5c5fe20--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@neural.avb?source=post_page---byline--5ea8a5c5fe20--------------------------------)[![Avishek
    Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--5ea8a5c5fe20--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5ea8a5c5fe20--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5ea8a5c5fe20--------------------------------)
    [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--5ea8a5c5fe20--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5ea8a5c5fe20--------------------------------)
    ·15 min read·Jun 28, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5ea8a5c5fe20--------------------------------)
    ·阅读时间 15 分钟 ·2024年6月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Before CNNs, the standard way to train a neural network to classify images was
    to flatten it into a list of pixels and pass it through a feed-forward neural
    network to output the image’s class. The problem with flattening the image is
    that the essential spatial information in the image is discarded.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络出现之前，训练神经网络对图像进行分类的标准方法是将图像展平成像素列表，并通过前馈神经网络来输出图像的类别。展平图像的问题在于，它丢失了图像中至关重要的空间信息。
- en: In 1989, Yann LeCun and team introduced Convolutional Neural Networks — the
    backbone of Computer Vision research for the last 15 years! Unlike feedforward
    networks, CNNs preserve the 2D nature of images and are capable of processing
    information spatially!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 1989年，Yann LeCun 和团队推出了卷积神经网络——过去15年来计算机视觉研究的基石！与前馈网络不同，卷积神经网络保持了图像的二维特性，并能够空间处理信息！
- en: In this article, we are going to go through the history of CNNs specifically
    for Image Classification tasks — starting from those early research years in the
    90's to the golden era of the mid-2010s when many of the most genius Deep Learning
    architectures ever were conceived, and finally discuss the latest trends in CNN
    research now as they compete with attention and vision-transformers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将回顾卷积神经网络（CNN）在图像分类任务中的历史——从90年代的早期研究开始，到2010年代中期的黄金时代，那时许多最杰出的深度学习架构应运而生，最后讨论当前卷积神经网络研究的最新趋势，它们与注意力机制和视觉变换器展开竞争。
- en: '*Check out the* [*YouTube video*](https://youtu.be/N_PocrMHWbw) *that explains
    all the concepts in this article visually with animations. Unless otherwise specified,
    all the images and illustrations used in this article are generated by myself
    during creating the video version.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*查看此* [*YouTube 视频*](https://youtu.be/N_PocrMHWbw) *，视频通过动画形象地解释了本文中的所有概念。除非另有说明，本文中使用的所有图像和插图均由我在制作视频版本时自行生成。*'
- en: '![](../Images/6f788fc5c6f6bbd352b7b138f5198dd1.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f788fc5c6f6bbd352b7b138f5198dd1.png)'
- en: The papers we will be discussing today!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我们将讨论的论文！
- en: '**The Basics of Convolutional Neural Networks**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**卷积神经网络的基础**'
- en: At the heart of a CNN is the convolution operation. We scan the filter across
    the image and calculate the dot product of the filter with the image at each overlapping
    location. **This resulting output is called a feature map and it captures how
    much and where the filter pattern is present in the image.**
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的核心是卷积操作。我们扫描滤波器穿过图像，并在每个重叠位置计算滤波器与图像的点积。**这个输出结果称为特征图，它捕捉了滤波器模式在图像中出现的程度和位置。**
- en: '![](../Images/6a88bb9a19c81a4788e265c2bdfb7177.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a88bb9a19c81a4788e265c2bdfb7177.png)'
- en: How Convolution works — The kernel slides over the input image and calculates
    the overlap (dot-product) at each location — outputting a feature map in the end!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是如何工作的 — 卷积核在输入图像上滑动，并在每个位置计算重叠部分（点积）— 最终输出一个特征图！
- en: In a convolution layer, we train multiple filters that extract different feature
    maps from the input image. When we stack multiple convolutional layers in sequence
    with some non-linearity, we get a convolutional neural network (CNN).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积层中，我们训练多个滤波器，从输入图像中提取不同的特征图。当我们将多个卷积层按顺序堆叠，并加入非线性激活函数时，就得到了卷积神经网络（CNN）。
- en: So each convolution layer simultaneously does 2 things —
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，每一层卷积层同时完成两项任务 —
- en: 1\. **spatial filtering** with the convolution operation between images and
    kernels, and
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. **空间滤波**，即图像和卷积核之间的卷积操作，和
- en: 2\. **combining the multiple input channels** and output a new set of channels.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **结合多个输入通道**并输出一组新的通道。
- en: 90 percent of the research in CNNs has been to modify or to improve just these
    two things.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: CNN研究的90％集中在修改或改进这两项内容。
- en: '![](../Images/7a53ab3d99f44347e87950b7143ae2f8.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a53ab3d99f44347e87950b7143ae2f8.png)'
- en: The two main things CNN do
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的两大核心任务
- en: The 1989 Paper
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1989年论文
- en: '[This 1989 paper](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf) taught
    us how to train non-linear CNNs from scratch using backpropagation. They input
    16x16 grayscale images of handwritten digits, and pass through two convolutional
    layers with 12 filters of size 5x5\. The filters also move with a stride of 2
    during scanning. Strided-convolution is useful for downsampling the input image.
    After the conv layers, the output maps are flattened and passed through two fully
    connected networks to output the probabilities for the 10 digits. Using the softmax
    cross-entropy loss, the network is optimized to predict the correct labels for
    the handwritten digits. After each layer, the tanh nonlinearity is also used —
    allowing the learned feature maps to be more complex and expressive. With just
    9760 parameters, this was a very small network compared to today’s networks which
    contain hundreds of millions of parameters.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[这篇1989年论文](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf)教我们如何通过反向传播从头开始训练非线性CNN。它们输入16x16的灰度手写数字图像，并通过两层卷积层，每层有12个5x5的滤波器。滤波器在扫描过程中还以步长2进行移动。步长卷积对于降采样输入图像非常有用。经过卷积层后，输出的特征图会被展平并传递到两个全连接网络，输出10个数字的概率。通过softmax交叉熵损失函数，网络被优化以预测手写数字的正确标签。每一层后，还使用了tanh非线性激活函数——使得学习到的特征图更加复杂和富有表现力。这个网络仅有9760个参数，相比今天那些包含数亿个参数的网络，它是一个非常小的网络。'
- en: '![](../Images/5f6ce141fce07f95a9408e58d031bf95.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f6ce141fce07f95a9408e58d031bf95.png)'
- en: The OG CNN architecture from 1989
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 1989年的原始CNN架构
- en: '**Inductive Bias**'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**偏向性归纳**'
- en: Inductive Bias is a concept in Machine Learning where we deliberately introduce
    specific rules and limitations into the learning process to move our models away
    from generalizations and steer more toward solutions that follow our human-like
    understanding.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 偏向性归纳（Inductive Bias）是机器学习中的一个概念，指的是我们故意在学习过程中引入特定的规则和限制，以使我们的模型避免过度泛化，更加接近符合人类理解的解决方案。
- en: When humans classify images, we also do spatial filtering *to look for common
    patterns to form multiple representations* and then *combine them together to
    form our predictions*. The CNN architecture is designed to replicate just that.
    In feedforward networks, each pixel is treated like it’s own isolated feature
    as each neuron in the layers connects with all the pixels — in CNNs there is more
    parameter-sharing because the same filter scans the entire image. Inductive biases
    make CNNs less data-hungry too because they get local pattern recognition for
    free due to the network design but feedforward networks need to spend their training
    cycles learning about it from scratch.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当人类进行图像分类时，我们也会进行空间过滤*以寻找共同的模式，形成多个表示*，然后*将它们结合起来做出预测*。CNN架构正是为了复制这一过程而设计的。在前馈网络中，每个像素都被视为独立的特征，因为层中的每个神经元都与所有像素相连——而在CNN中，由于相同的滤波器扫描整个图像，因此参数共享更多。归纳偏置使得CNN在数据需求上也更为节省，因为它们通过网络设计能够免费获得局部模式识别，而前馈网络则需要从头开始通过训练周期学习这些模式。
- en: '**Le-Net 5 (1998)**'
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Le-Net 5（1998年）**'
- en: '![](../Images/d625a2448a44d6150887d3d8a598512d.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d625a2448a44d6150887d3d8a598512d.png)'
- en: 'Lenet-5 architecture (Credit: [Le-Net-5 paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf))'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Lenet-5架构（来源：[Le-Net-5论文](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)）
- en: In 1998, Yann LeCun and team published the [Le-Net 5](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)
    — a deeper and larger 7-layer CNN model network. They also use Max Pooling which
    downsamples the image by grabbing the maximum values from a 2x2 sliding window.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 1998年，Yann LeCun及其团队发布了[Le-Net 5](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)——一个更深、更大的7层CNN模型网络。他们还使用了最大池化（Max
    Pooling），通过从2x2滑动窗口中提取最大值来对图像进行下采样。
- en: '![](../Images/59cfe79ef5bedf3c1b0dd5b33ae17f44.png)![](../Images/1f847d9fc9f1eab0131f8b91092d006d.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59cfe79ef5bedf3c1b0dd5b33ae17f44.png)![](../Images/1f847d9fc9f1eab0131f8b91092d006d.png)'
- en: How Max Pooling works (LEFT) and how Local Receptive Fields increase as CNNs
    add more layers (RIGHT)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Max Pooling如何工作（左侧）以及随着CNN增加更多层，局部感受野如何增大（右侧）
- en: Local Receptive Field
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局部感受野
- en: Notice when you train a 3x3 conv layer, each neuron is connected to a 3x3 region
    in the original image — this is the neuron’s local receptive field — the region
    of the image where this neuron extracts patterns from.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当你训练一个3x3卷积层时，每个神经元都与原始图像中的一个3x3区域相连——这就是神经元的局部感受野——该神经元从中提取模式的图像区域。
- en: When we pass this feature map through another 3x3 layer , the new feature map
    indirectly creates a receptive field of a larger 5x5 region from the original
    image. Additionally, when we downsample the image through max-pooling or strided-convolution,
    the receptive field also increases — making deeper layers access the input image
    more and more globally.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过另一个3x3层传递这个特征图时，新的特征图会间接地创建一个更大的5x5区域的感受野，来源于原始图像。此外，当我们通过最大池化或步幅卷积对图像进行下采样时，感受野也会增加——使得更深层的网络能够更加全局地访问输入图像。
- en: For this reason, earlier layers in a CNN can only pick low-level details like
    specific edges or corners, and the latter layers pick up more spread-out global-level
    patterns.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，CNN中的早期层只能捕捉到低级细节，如特定的边缘或角落，而后续层则捕捉到更为广泛的全局级别的模式。
- en: The Draught (1998–2012)
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: The Draught (1998–2012)
- en: As impressive Le-Net-5 was, researchers in the early 2000s still deemed neural
    networks to be computationally very expensive and data hungry to train. There
    was also problems with overfitting — where a complex neural network will just
    memorize the entire training dataset and fail to generalize on new unseen datasets.
    The researchers instead focused on traditional machine learning algorithms like
    support vector machines that were showing much better performance on the smaller
    datasets of the time with much less computational demands.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Le-Net-5非常令人印象深刻，但2000年代初期的研究人员仍然认为神经网络在计算上非常昂贵且训练数据需求量大。另一个问题是过拟合——复杂的神经网络可能只是记住整个训练数据集，而无法对新的未见数据集进行泛化。研究人员因此转向传统的机器学习算法，如支持向量机，这些算法在当时较小的数据集上表现出了更好的性能，且计算需求远低于神经网络。
- en: '**ImageNet Dataset (2009)**'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ImageNet数据集（2009年）**'
- en: The [ImageNet dataset](https://www.image-net.org/index.php) was open-sourced
    in 2009 — it contained 3.2 million annotated images at the time covering over
    1000 different classes. Today it has over 14 million images and over 20,000 annotated
    different classes. Every year from 2010 to 2017 we got this massive competition
    called the [ILSVRC](https://www.image-net.org/challenges/LSVRC/) where different
    research groups will publish models to beat the benchmarks on a subset of the
    ImageNet dataset. In 2010 and 2011, traditional ML methods like Support Vector
    Machines were winning — but starting from 2012 it was all about CNNs. The metric
    used to rank different networks was generally the top-5 error rate — measuring
    the percentage of times that the true class label was not in the top 5 classes
    predicted by the network.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImageNet数据集](https://www.image-net.org/index.php)在2009年开源——当时包含了320万张标注图像，覆盖了1000多个不同类别。如今，它已经拥有超过1400万张图像和超过2万个标注的不同类别。从2010年到2017年，每年都会举行一个叫做[ILSVRC](https://www.image-net.org/challenges/LSVRC/)的大型比赛，研究小组们会发布模型，以打破ImageNet数据集子集上的基准。在2010年和2011年，传统的机器学习方法如支持向量机（SVM）获胜——但从2012年开始，比赛的焦点就转向了卷积神经网络（CNN）。排名不同网络的指标通常是top-5错误率——即衡量真实类别标签未能出现在网络预测的前5个类别中的比例。'
- en: '**AlexNet (2012)**'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**AlexNet (2012)**'
- en: '[AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf),
    introduced by Dr. Geoffrey Hinton and his team was the winner of ILSVRC 2012 with
    a top-5 test set error of 17%. Here are the three main contributions from AlexNet.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)，由Geoffrey
    Hinton博士及其团队提出，是2012年ILSVRC的冠军，测试集的top-5错误率为17%。以下是AlexNet的三大主要贡献。'
- en: 1\. Multi-scaled Kernels
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 多尺度卷积核
- en: AlexNet trained on 224x224 RGB images and used multiple kernel sizes in the
    network — an 11x11, a 5x5, and a 3x3 kernel. Models like Le-Net 5 only used 5x5
    kernels. Larger kernels are more computationally expensive because they train
    more weights, but also capture more global patterns from the image. Because of
    these large kernels, AlexNet had over 60 million trainable parameters. All that
    complexity can however lead to overfitting.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet在224x224的RGB图像上进行训练，并在网络中使用了多种卷积核大小——分别是11x11、5x5和3x3卷积核。像Le-Net 5这样的模型只使用了5x5卷积核。较大的卷积核计算开销更大，因为它们训练了更多的权重，但也能从图像中捕捉到更多的全局模式。由于这些大卷积核，AlexNet拥有超过6000万个可训练参数。然而，所有这些复杂性可能会导致过拟合。
- en: '![](../Images/3dbfe9a516bc0af4053955d3cbd0ae81.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3dbfe9a516bc0af4053955d3cbd0ae81.png)'
- en: AlexNet starts with larger kernels (11x11) and reduces the size (to 5x5 and
    3x3) for deeper layers (Image by the author)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet从较大的卷积核（11x11）开始，并为更深的层次减少卷积核大小（至5x5和3x3）（图片来源：作者）
- en: 2\. Dropout
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. Dropout
- en: To alleviate overfitting, AlexNet used a regularization technique called Dropout.
    During training, a fraction of the neurons in each layer is turned to zero. This
    prevents the network from being too reliant on specific neurons or groups of neurons
    for generating a prediction and instead encourages all the neurons to learn general
    meaningful features useful for classification.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解过拟合，AlexNet采用了一种叫做Dropout的正则化技术。在训练过程中，每一层的部分神经元会被设为零。这可以防止网络过度依赖某些特定的神经元或神经元组来进行预测，从而鼓励所有神经元学习对分类有用的通用有意义特征。
- en: 3\. RELU
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. RELU
- en: Alexnet also replaced tanh nonlinearity with ReLU. RELU is an activation function
    that turns negative values to zero and keeps positive values as-is. The tanh function
    tends to saturate for deep networks because the gradients get low when the value
    of x goes too high or too low making optimization slow. RELU offers a steady gradient
    signal to train the network about 6 times faster than tanH.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet还将tanh非线性函数替换为ReLU。ReLU是一种激活函数，它将负值变为零，保留正值不变。由于当x值过高或过低时，tanh函数的梯度会变得很小，导致优化过程变慢，因此它通常会在深度网络中饱和。相比之下，ReLU提供了稳定的梯度信号，使得训练速度比tanh快约6倍。
- en: '![](../Images/46bebdc63f8f31a04b7b17236f3a5c49.png)![](../Images/9853cded8ce78082f5567793b0d22c30.png)![](../Images/40d276fedfc70960ef639c45f76d221a.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46bebdc63f8f31a04b7b17236f3a5c49.png)![](../Images/9853cded8ce78082f5567793b0d22c30.png)![](../Images/40d276fedfc70960ef639c45f76d221a.png)'
- en: 'RELU, TANH, and How much difference RELU makes (Image credits: Middle: [Artificial
    Intelligence for Big Data](https://learning.oreilly.com/library/view/artificial-intelligence-for/9781788472173/),
    Right: [Alex-Net paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf))'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'RELU、TANH，以及RELU带来的差异有多大（图片来源：中间：[大数据中的人工智能](https://learning.oreilly.com/library/view/artificial-intelligence-for/9781788472173/)，右侧：[Alex-Net论文](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)）  '
- en: AlexNet also introduced the concept of Local Response Normalization and strategies
    for distributed CNN training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 'AlexNet还引入了局部响应归一化（Local Response Normalization）以及分布式CNN训练策略。  '
- en: '**GoogleNet / Inception (2014)**'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**GoogleNet / Inception（2014）**  '
- en: In 2014, GoogleNet paper got an ImageNet top-5 error rate of 6.67%. The core
    component of GoogLeNet was the **inception module**. Each inception module consists
    of parallel convolutional layers with different filter sizes (1x1, 3x3, 5x5) and
    max-pooling layers. Inception applies these kernels to the same input and then
    concats them, combining both low-level and medium-level features.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '2014年，GoogleNet的论文在ImageNet上取得了6.67%的Top-5错误率。GoogLeNet的核心组件是**Inception模块**。每个Inception模块由不同滤波器大小（1x1、3x3、5x5）和最大池化层的并行卷积层组成。Inception将这些卷积核应用于相同的输入，然后将它们连接起来，结合了低级特征和中级特征。  '
- en: '![](../Images/997501a2e4dd36f4519c2d85b202386a.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/997501a2e4dd36f4519c2d85b202386a.png)  '
- en: An Inception Module
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '一个Inception模块  '
- en: 1x1 Convolution
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '1x1卷积  '
- en: They also use 1x1 convolutional layer. Each 1x1 kernel first scales the input
    channels and then combines them. 1x1 kernels multiply each pixel with a fixed
    value — which is why it is also called pointwise convolutions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '它们还使用了1x1卷积层。每个1x1卷积核首先对输入通道进行缩放，然后将它们结合在一起。1x1卷积核通过与固定值相乘每个像素——这也是它被称为点卷积的原因。  '
- en: While larger kernels like 3x3 and 5x5 kernels do both spatial filtering and
    channel combination, 1x1 kernels are only good for channel mixing, and it does
    so very efficiently with a lower number of weights. For example, A 3-by-4 grid
    of 1x1 convolution layers trains only *(1x1 x 3x4 =)* 12 weights — but if it were
    3x3 kernels — we would train *(3x3 x 3x4 =)* 108 weights.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然像3x3和5x5这样的更大卷积核同时进行空间过滤和通道组合，但1x1卷积核仅适用于通道混合，而且在权重数量较少的情况下非常高效。例如，一个3x4的1x1卷积层网格只需训练*(1x1
    x 3x4 =)* 12个权重——但是如果使用3x3卷积核，则需要训练*(3x3 x 3x4 =)* 108个权重。  '
- en: '![](../Images/6f234f8f634cf640af57b563c47cb50d.png)![](../Images/65f6501cf00fd0cbb64593fff021f896.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f234f8f634cf640af57b563c47cb50d.png)![](../Images/65f6501cf00fd0cbb64593fff021f896.png)  '
- en: 1x1 kernels versus larger kernels (LEFT) and Dimensionality reduction with 1x1
    kernels (RIGHT)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '1x1卷积核与更大卷积核（左）和使用1x1卷积核的降维（右）  '
- en: Dimensionality Reduction
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '降维  '
- en: GoogleNet uses 1x1 conv layers as a dimensionality reduction method to reduce
    the number of channels before running spatial filtering with the 3x3 and 5x5 convolutions
    on these lower dimensional feature maps. This helps them to cut down on the number
    of trainable weights compared to AlexNet.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 'GoogleNet使用1x1卷积层作为降维方法，在对这些低维特征图进行3x3和5x5卷积的空间过滤之前，先降低通道数量。这有助于减少可训练权重的数量，相比于AlexNet，它大大减少了权重数量。  '
- en: '**VGGNet (2014)**'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**VGGNet（2014）**  '
- en: '[The VGG Network](https://arxiv.org/abs/1409.1556) claims that we do not need
    larger kernels like 5x5 or 7x7 networks and all we need are 3x3 kernels. 2 layer
    3x3 convolutional layer has the same receptive field of the image that a single
    5x5 layer does. Three 3x3 layers have the same receptive field that a single 7x7
    layer does.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[VGG网络](https://arxiv.org/abs/1409.1556)指出，我们不需要像5x5或7x7这样的更大卷积核，所需的只是3x3卷积核。两个3x3卷积层的感受野与单个5x5卷积层相同。三个3x3卷积层的感受野与单个7x7卷积层相同。  '
- en: '**Deep 3x3 Convolution Layers capture the same receptive field as larger kernels
    but with fewer parameters!**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度3x3卷积层捕捉到的感受野与更大卷积核相同，但参数更少！**'
- en: One 5x5 filter trains 25 weights — while two 3x3 filters train 18 weights. Similarly
    one 7x7 trains 49 weights, while 3 3x3 trains just 27\. Training with deep 3x3
    convolution layers became the standard for a long time in CNN architectures.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '一个5x5滤波器训练25个权重——而两个3x3滤波器只需训练18个权重。类似地，一个7x7滤波器训练49个权重，而三个3x3卷积层只需训练27个权重。使用深度的3x3卷积层长期以来成为了CNN架构的标准做法。  '
- en: '![](../Images/cfed290a6af3ba3dee066dc01e5f4633.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cfed290a6af3ba3dee066dc01e5f4633.png)  '
- en: '**Batch Normalization (2015)**'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**批量归一化（2015）**  '
- en: Deep neural networks can suffer from a problem known as **“Internal Covariate
    Shift”** during training. Since the earlier layers of the network are constantly
    training, the latter layers need to continuously adapt to the constantly shifting
    input distribution it receive from the previous layers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络在训练过程中可能会遇到一个问题，称为**“内部协方差偏移”**。由于网络的早期层在不断训练，后续层需要持续适应从前面层接收到的不断变化的输入分布。
- en: '[Batch Normalization](https://arxiv.org/pdf/1502.03167) aims to counteract
    this problem by normalizing the inputs of each layer to have zero mean and unit
    standard deviation during training. A batch normalization or BN layer can be applied
    after any convolution layer. During training it subtracts the mean of the feature
    map along the minibatch dimension and divides it by the standard deviation. This
    means that each layer will now see a more stationary unit gaussian distribution
    during training.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[批归一化](https://arxiv.org/pdf/1502.03167)旨在通过在训练过程中将每层的输入归一化为零均值和单位标准差，从而抵消这个问题。批归一化或
    BN 层可以应用于任何卷积层之后。在训练过程中，它会减去特征图在小批量维度上的均值，并除以标准差。这意味着每一层在训练过程中将看到一个更加平稳的单位高斯分布。'
- en: Advantages of Batch Norm
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批归一化的优势
- en: converge around 14 times faster
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收敛速度约为原来的 14 倍
- en: let us use higher learning rates, and
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用更高的学习率，并且
- en: makes the network robust to the initial weights of the network.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使网络对初始权重具有鲁棒性。
- en: '**ResNets (2016)**'
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ResNets (2016)**'
- en: Deep Networks struggle to do Identity Mapping
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度网络在做恒等映射时遇到困难
- en: Imagine you have a shallow neural network that has great accuracy on a classification
    task. Turns out that if we added 100 new convolution layers on top of this network,
    the training accuracy of the model could go down!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你有一个浅层神经网络，它在分类任务上表现出色。结果发现，如果我们在这个网络上加上 100 个新的卷积层，模型的训练准确性可能会下降！
- en: This is quite counter-intuitive because all these new layers need to do is copy
    the output of the shallow network at each layer — and at least be able to match
    the original accuracy. In reality, deep networks can be notoriously difficult
    to train because gradients can saturate or become unstable when backpropagating
    through many layers. With Relu and batch norm, we were able to train 22-layer
    deep CNNs at this point — the good folks at Microsoft introduced [ResNets](https://arxiv.org/abs/1512.03385)
    in 2015 which allowed us to stably train 150 layered CNNs. What did they do?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当反直觉，因为这些新层只需要做的就是复制浅层网络每层的输出——至少能够匹配原始的准确性。实际上，深度网络在训练时往往非常难以调优，因为在通过许多层反向传播时，梯度可能会饱和或变得不稳定。在
    Relu 和批归一化的帮助下，我们当时能够训练 22 层深的 CNN —— 微软的优秀团队在 2015 年推出了[ResNets](https://arxiv.org/abs/1512.03385)，使我们能够稳定地训练
    150 层的 CNN。他们是如何做到的？
- en: Residual learning
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 残差学习
- en: The input passes through one or more CNN layers as usual, but at the end, the
    original input is added back to the final output. These blocks are called residual
    blocks because they don’t need to learn the final output feature maps in the traditional
    sense — but they are just the residual features that must be added to the input
    to get the final feature maps. **If the weights in the middle layers were to turn
    themselves to ZERO, then the residual block would just return the identity function
    — meaning it would be able to easily copy the input X.**
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 输入通常会通过一个或多个 CNN 层，但在最后，原始输入会被加回到最终输出。这些块被称为残差块，因为它们不需要像传统意义上那样学习最终的输出特征图——它们只是必须加到输入上的残差特征，以便得到最终的特征图。**如果中间层的权重变为零，那么残差块就会返回恒等函数——意味着它将能够轻松复制输入
    X。**
- en: '![](../Images/b67371d5f87b5198e3e1a03c3a62ef15.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b67371d5f87b5198e3e1a03c3a62ef15.png)'
- en: Residual Networks
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 残差网络
- en: Easy Gradient Flow
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单的梯度流
- en: During backpropagation gradients can directly flow through these shortcut paths
    to reach the earlier layers of the model faster, helping to prevent gradient vanishing
    issues. ResNet stacks many of these blocks together to form really deep networks
    without any loss of accuracy!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播过程中，梯度可以通过这些快捷路径直接流向模型的早期层，从而更快地到达，有助于防止梯度消失问题。ResNet 将许多这样的块堆叠在一起，形成真正深的网络，而不会损失准确性！
- en: '![](../Images/55b293233faca9c8f22ed808e1dd8c4f.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55b293233faca9c8f22ed808e1dd8c4f.png)'
- en: '[From the ResNet paper](https://arxiv.org/abs/1512.03385)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[来自 ResNet 论文](https://arxiv.org/abs/1512.03385)'
- en: And with this remarkable improvement, ResNets managed to train a 152-layered
    model that got a top-5 error rate that shattered all previous records!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 凭借这一显著的改进，ResNet成功训练了一个152层的模型，创造了打破所有纪录的top-5错误率！
- en: '**DenseNet (2017)**'
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**DenseNet（2017）**'
- en: '![](../Images/ef9fa851e3acd1c08106018c2f39b471.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef9fa851e3acd1c08106018c2f39b471.png)'
- en: '[Dense-Nets](https://arxiv.org/abs/1608.06993) also add shortcut paths connecting
    earlier layers with the latter layers in the network. A DenseNet block trains
    a series of convolution layers, and the output of every layer is concatenated
    with the feature maps of every previous layer in the block before passing to the
    next layer. Each layer adds only a small number of new feature maps to the “collective
    knowledge” of the network as the image flows through the network. DenseNets have
    an improved flow of information and gradients throughout the network because each
    layer has direct access to the gradients from the loss function.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dense-Nets](https://arxiv.org/abs/1608.06993) 也增加了连接早期层与后期层的捷径路径。DenseNet模块训练了一系列卷积层，每一层的输出都会与模块中每个先前层的特征图拼接，之后再传递给下一层。每一层仅向网络的“集体知识”中添加少量的新特征图，随着图像在网络中的流动，DenseNet的网络信息流和梯度流得到了改善，因为每一层可以直接访问来自损失函数的梯度。'
- en: '![](../Images/164f7f2c97a282693b45bcb96e2b21d7.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/164f7f2c97a282693b45bcb96e2b21d7.png)'
- en: Dense Nets
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Dense Nets
- en: Squeeze and Excitation Network (2017)
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 压缩与激励网络（2017）
- en: '[SEN-NET](https://arxiv.org/abs/1709.01507) was the final winner of the ILSVRC
    competition, which introduced the Squeeze and Excitation Layer into CNNs. The
    SE block is designed to explicitly model the dependencies between all the channels
    of a feature map. In normal CNNs, each channel of a feature map is computed independently
    of each other; SEN-Net applies a self-attention-like method to make each channel
    of a feature map contextually aware of the global properties of the input image.
    SEN-Net won the final ILVSRC of 2017, and one of the 154-layered SenNet + ResNet
    models got a ridiculous top-5 error rate of 4.47%.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[SEN-NET](https://arxiv.org/abs/1709.01507) 是ILSVRC比赛的最终获胜者，提出了将压缩与激励层引入到卷积神经网络（CNN）中。SE模块旨在显式地建模特征图中所有通道之间的依赖关系。在普通的CNN中，特征图的每个通道是相互独立计算的；而SEN-Net采用类似自注意力的方法，使得特征图中的每个通道能够感知输入图像的全局特性。SEN-Net赢得了2017年ILSVRC比赛的最终胜利，且其中一个包含154层的SenNet
    + ResNet模型，创下了令人惊讶的top-5错误率4.47%。'
- en: '![](../Images/6b871751ded64eef0b9db11f9795ad28.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b871751ded64eef0b9db11f9795ad28.png)'
- en: '[SEN-NET](https://arxiv.org/abs/1709.01507)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[SEN-NET](https://arxiv.org/abs/1709.01507)'
- en: Squeeze Operation
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 压缩操作
- en: The squeeze operation compresses the spatial dimensions of the input feature
    map into a channel descriptor using global average pooling. Since each channel
    contains neurons that capture local properties of the image, the squeeze operation
    accumulates global information about each channel.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩操作通过全局平均池化将输入特征图的空间维度压缩成一个通道描述符。由于每个通道包含捕捉图像局部特性的神经元，压缩操作将关于每个通道的全局信息进行积累。
- en: Excitation Operation
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激励操作
- en: The excitation operation rescales the input feature maps by channel-wise multiplication
    with the channel descriptors obtained from the squeeze operation. This effectively
    propagates global-level information to each channel — contextualizing each channel
    with the rest of the channels in the feature map.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 激励操作通过与来自压缩操作获得的通道描述符按通道进行乘法，重新调整输入特征图的尺度。这有效地将全局级别的信息传播到每个通道——使每个通道能够理解特征图中其他通道的上下文。
- en: '![](../Images/c37db341a2da8c50a202b6e36a3c93a6.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c37db341a2da8c50a202b6e36a3c93a6.png)'
- en: '[Squeeze and Excitation Block](https://arxiv.org/abs/1709.01507)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[压缩与激励模块](https://arxiv.org/abs/1709.01507)'
- en: '**MobileNet (2017)**'
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**MobileNet（2017）**'
- en: Convolution layers do two things –1) *filtering spatial information* and 2)
    *combining them channel-wise*. The MobileNet paper uses **Depthwise Separable
    Convolution**,a technique that separates these two operations into two different
    layers — Depthwise Convolution for filtering and pointwise convolution for channel
    combination.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层有两个功能——1）*过滤空间信息*，2）*按通道合并信息*。MobileNet 论文使用了**深度可分离卷积**，这是一种将这两种操作分开到两个不同层次的技术——使用深度卷积进行过滤，使用逐点卷积进行通道合并。
- en: Depthwise Convolution
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度卷积
- en: '![](../Images/1b885d58c9a73b3b1a51592e2d9e1966.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b885d58c9a73b3b1a51592e2d9e1966.png)'
- en: Depthwise Separable Convolution
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 深度可分离卷积
- en: Given an input set of feature maps with M channels, first, they use depthwise
    convolution layers that train M 3x3 convolutional kernels. Unlike normal convolution
    layers that perform convolution on all feature maps, depthwise convolution layers
    train filters that perform convolution on just one feature map each. Secondly,
    they use 1x1 pointwise convolution filters to mix all these feature maps. **Separating
    the filtering and combining steps like this drastically reduces the number of
    weights**, making it super lightweight while still retaining the performance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个具有M个通道的特征图集，首先，它们使用深度卷积层训练M个3x3卷积核。与普通卷积层对所有特征图进行卷积不同，深度卷积层训练的滤波器仅对每个特征图单独进行卷积。其次，它们使用1x1逐点卷积滤波器来混合所有这些特征图。**像这样分开滤波和组合步骤极大地减少了权重数量**，使得网络变得非常轻量化，同时仍保持性能。
- en: '![](../Images/66344afbabc2e1473b31832405fdda81.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66344afbabc2e1473b31832405fdda81.png)'
- en: Why Depthwise Separable Layers reduce training weights
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么深度可分离层减少训练权重
- en: '**MobileNetV2 (2019)**'
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**MobileNetV2 (2019)**'
- en: 'In 2018, MobileNetV2 improved the MobileNet architecture by introducing two
    more innovations: *Linear Bottlenecks* and *Inverted residuals*.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，MobileNetV2通过引入两个创新——*线性瓶颈*（Linear Bottlenecks）和*倒残差*（Inverted residuals）——改进了MobileNet架构。
- en: Linear Bottlenecks
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性瓶颈
- en: MobileNetV2 uses 1x1 pointwise convolution for dimensionality reduction, followed
    by depthwise convolution layers for spatial filtering, and another 1x1 pointwise
    convolution layer to expand the channels back. **These bottlenecks don’t pass
    through RELU and are instead kept linear.** RELU zeros out all the negative values
    that came out of the dimensionality reduction step — and this can cause the network
    to lose valuable information especially if a bulk of this lower dimensional subspace
    was negative. Linear layers prevent the loss of excessive information during this
    bottleneck.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNetV2使用1x1逐点卷积进行降维，然后是深度卷积层进行空间滤波，接着再通过一个1x1逐点卷积层扩展通道回原来的维度。**这些瓶颈不经过RELU，而是保持线性。**
    RELU会将降维步骤中产生的所有负值归零，这可能导致网络丢失重要信息，尤其是在这些低维空间大部分为负值时。线性层防止了在这一瓶颈过程中丢失过多信息。
- en: '![](../Images/8950a4c49cce3be0d309a0c1db0f9c82.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8950a4c49cce3be0d309a0c1db0f9c82.png)'
- en: The width of each feature map is intended to show the relative channel dimensions.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特征图的宽度旨在显示相对的通道维度。
- en: Inverted Residuals
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 倒残差（Inverted Residuals）
- en: The second innovation is called Inverted Residuals. Generally, residual connections
    occur between layers with the highest channels, but the authors add shortcuts
    between the bottlenecks layers. The bottleneck captures the relevant information
    within a low-dimensional latent space, and the free flow of information and gradient
    between these layers is the most crucial.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个创新被称为倒残差。通常，残差连接发生在具有最高通道数的层之间，但作者在瓶颈层之间添加了捷径。瓶颈层捕捉了低维潜在空间中的相关信息，而这些层之间信息和梯度的自由流动是至关重要的。
- en: '![](../Images/c2e4c4eae0d3d37c24af81e463c38f2b.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2e4c4eae0d3d37c24af81e463c38f2b.png)'
- en: '**Vision Transformers (2020)**'
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**视觉变换器 (2020)**'
- en: Vision Transformers or ViTs established that transformers can indeed beat state-of-the-art
    CNNs in Image Classification. Transformers and Attention mechanisms provide a
    highly parallelizable, scalable, and general architecture for modeling sequences.
    Neural Attention is a whole different area of Deep Learning, which we won’t get
    into this article, but feel free to learn more in this Youtube video.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉变换器（Vision Transformers，简称ViTs）证明了变换器在图像分类任务中确实能够超越最先进的卷积神经网络（CNN）。变换器和注意力机制提供了一种高度可并行化、可扩展的通用架构，用于建模序列。神经注意力是深度学习中的一个完全不同的领域，本文不会涉及，但你可以在这个YouTube视频中了解更多。
- en: ViTs use Patch Embeddings and Self-Attention
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViTs使用图块嵌入和自注意力
- en: The input image is first divided into a sequence of fixed-size patches. Each
    patch is independently embedded into a fixed-size vector either through a CNN
    or passing through a linear layer. These patch embeddings and their positional
    encodings are then inputted as a sequence of tokens into a self-attention-based
    transformer encoder. Self-attention models the relationships between all the patches,
    and outputs new updated patch embeddings that are contextually aware of the entire
    image.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像首先被分割成一系列固定大小的图块。每个图块都通过卷积神经网络（CNN）或经过一个线性层独立地嵌入成一个固定大小的向量。然后，这些图块嵌入和它们的位置编码作为一个令牌序列输入到基于自注意力的变换器编码器中。自注意力模型会建模所有图块之间的关系，并输出新的更新后的图块嵌入，这些嵌入能够理解整个图像的上下文。
- en: '![](../Images/fc228d2bbe95cf7993f257be0bc535bb.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc228d2bbe95cf7993f257be0bc535bb.png)'
- en: Vision Transformers. Each self-attention layer further contextualizes each patch
    embedding with the global context of the image
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉变换器。每个自注意力层通过图像的全局上下文进一步对每个块的嵌入进行上下文化。
- en: Inductive Bias vs Generality
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归纳偏置与普适性
- en: Where CNNs introduce several inductive biases about images, Transformers do
    the opposite — No localization, no sliding kernels — they rely on generality and
    raw computing to model the relationships between all the patches of the image.
    The Self-Attention layers allow global connectivity between all patches of the
    image irrespective of how far they are spatially. Inductive biases are great on
    smaller datasets, but the promise of Transformers is on massive training datasets,
    a general framework is going to eventually beat out the inductive biases offered
    by CNNs.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当CNN引入了多个关于图像的归纳偏置时，变换器则相反——没有局部化，没有滑动卷积核——它们依赖于普适性和原始计算来建模图像中所有块之间的关系。自注意力层允许图像中所有块之间的全局连接，无论它们在空间上相距多远。归纳偏置在较小的数据集上表现良好，但变换器的优势在于大规模训练数据集，最终，一个通用的框架将战胜CNN所提供的归纳偏置。
- en: '![](../Images/6a450fb97b290d4419671ac36c27f673.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a450fb97b290d4419671ac36c27f673.png)'
- en: Convolution Layers vs Self-Attention Layers
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层与自注意力层
- en: '**ConvNext —** A ConvNet for the 2020s **(2022)**'
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ConvNext —** 2020年代的卷积网络 **(2022)**'
- en: A great choice to include in this article would be Swin Transformers, but that
    is a topic for a different day! Since this is a CNN article, let’s focus on one
    last CNN paper.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中加入Swin Transformers是个很好的选择，但那是另一个话题！因为这是关于CNN的文章，让我们集中讨论最后一篇CNN相关的论文。
- en: '![](../Images/3a0f9822f909de4a7b52dfe04861be4e.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a0f9822f909de4a7b52dfe04861be4e.png)'
- en: Patchifying Images like VITs
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 像VITs一样对图像进行分块
- en: The input of ConvNext follows the patching strategy inspired by Vision Transformers.
    A 4x4 convolution kernel with a stride of 4 creates a downsampled image which
    is inputted into the rest of the network.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ConvNext的输入遵循一种受视觉转换器启发的图像分块策略。一个4x4的卷积核，步幅为4，创建了一个下采样的图像，并输入到网络的其余部分。
- en: Depthwise Separable Convolution
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度可分离卷积
- en: Inspired by MobileNet, ConvNext uses depthwise separable convolution layers.
    The authors also hypothesize depthwise convolution is similar to the weighted
    sum operation in self-attention, which operates on a per-channel basis by only
    mixing information in the spatial dimension. Also the 1x1 pointwise convolutions
    are similar to the channel mixing steps in Self-Attention.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 受MobileNet启发，ConvNext使用深度可分离卷积层。作者还假设深度卷积类似于自注意力中的加权和操作，这种操作在每个通道上进行，仅在空间维度上混合信息。此外，1x1的逐点卷积类似于自注意力中的通道混合步骤。
- en: Larger Kernel Sizes
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更大的卷积核尺寸
- en: While ConvNets have been using 3x3 kernels ever since VGG, ConvNext proposes
    larger 7x7 filters to capture a wider spatial context, trying to come close to
    the fully global context that ViTs capture, while retaining the localization spirits
    of CNNs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 自VGG以来，CNN一直在使用3x3的卷积核，而ConvNext提出了更大的7x7滤波器，以捕捉更广泛的空间上下文，尽量接近ViTs所捕捉的完全全局上下文，同时保留CNN的本地化特性。
- en: There are also some other tweaks, like using MobileNetV2-inspired inverted bottlenecks,
    the GELU activations, layer norms instead of batch norms, and more that shape
    up the rest of the ConvNext architecture.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他的改进，比如使用受MobileNetV2启发的反向瓶颈、GELU激活函数、使用层归一化而非批量归一化等，这些都塑造了ConvNext架构的其余部分。
- en: Scalability
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展性
- en: ConvNext are more computationally efficient way with the depthwise separable
    convolutions and is more scalable than transformers on high-resolution images
    — this is because Self-Attention scales quadratically with sequence length and
    Convolution doesn’t.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ConvNext通过深度可分离卷积提供了更高的计算效率，并且在高分辨率图像上比变换器更具可扩展性——这是因为自注意力在序列长度上按平方级别扩展，而卷积则不受此限制。
- en: '![](../Images/273120ccbef227363f48781e3fc84edf.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/273120ccbef227363f48781e3fc84edf.png)'
- en: Final Thoughts!
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终思考！
- en: The history of CNNs teaches us so much about Deep Learning, Inductive Bias,
    and the nature of computation itself. It’ll be interesting to see what wins out
    in the end — the inductive biases of ConvNets or the Generality of Transformers.
    Do check out the companion YouTube video for a visual tour of this article, and
    the individual papers as listed below.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）的历史教会了我们很多关于深度学习、归纳偏置以及计算本质的知识。最终谁会胜出是很有趣的——是卷积网络的归纳偏置，还是变换器的普适性。一定要查看配套的YouTube视频，以便直观地了解这篇文章及下列列出的相关论文。
- en: References
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'CNN with Backprop (1989): [http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 带反向传播的卷积神经网络（CNN with Backprop）（1989年）：[http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf)
- en: 'LeNet-5: [http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet-5：[http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)
- en: AlexNet:[https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet：[https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- en: 'GoogleNet: [https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: GoogleNet：[https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)
- en: 'VGG: [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: VGG：[https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)
- en: 'Batch Norm: [https://arxiv.org/pdf/1502.03167](https://arxiv.org/pdf/1502.03167)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化（Batch Norm）：[https://arxiv.org/pdf/1502.03167](https://arxiv.org/pdf/1502.03167)
- en: 'ResNet: [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet：[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)
- en: 'DenseNet: [https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet：[https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)
- en: 'MobileNet: [https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet：[https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)
- en: 'MobileNet-V2: [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet-V2：[https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)
- en: 'Vision Transformers: [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉变换器（Vision Transformers）：[https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)
- en: 'ConvNext: [https://arxiv.org/abs/2201.03545](https://arxiv.org/abs/2201.03545)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ConvNext：[https://arxiv.org/abs/2201.03545](https://arxiv.org/abs/2201.03545)
- en: 'Squeeze-and-Excitation Network: [https://arxiv.org/abs/1709.01507](https://arxiv.org/abs/1709.01507)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 挤压与激励网络（Squeeze-and-Excitation Network）：[https://arxiv.org/abs/1709.01507](https://arxiv.org/abs/1709.01507)
- en: 'Swin Transformers: [https://arxiv.org/abs/2103.14030](https://arxiv.org/abs/2103.14030)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Swin Transformers：[https://arxiv.org/abs/2103.14030](https://arxiv.org/abs/2103.14030)
