- en: The History of Convolutional Neural Networks for Image Classification (1989
    - Today)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-history-of-convolutional-neural-networks-for-image-classification-1989-today-5ea8a5c5fe20?source=collection_archive---------5-----------------------#2024-06-28](https://towardsdatascience.com/the-history-of-convolutional-neural-networks-for-image-classification-1989-today-5ea8a5c5fe20?source=collection_archive---------5-----------------------#2024-06-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A visual tour of the greatest innovations in Deep Learning and Computer Vision.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@neural.avb?source=post_page---byline--5ea8a5c5fe20--------------------------------)[![Avishek
    Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--5ea8a5c5fe20--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5ea8a5c5fe20--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5ea8a5c5fe20--------------------------------)
    [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--5ea8a5c5fe20--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5ea8a5c5fe20--------------------------------)
    ·15 min read·Jun 28, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Before CNNs, the standard way to train a neural network to classify images was
    to flatten it into a list of pixels and pass it through a feed-forward neural
    network to output the image’s class. The problem with flattening the image is
    that the essential spatial information in the image is discarded.
  prefs: []
  type: TYPE_NORMAL
- en: In 1989, Yann LeCun and team introduced Convolutional Neural Networks — the
    backbone of Computer Vision research for the last 15 years! Unlike feedforward
    networks, CNNs preserve the 2D nature of images and are capable of processing
    information spatially!
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we are going to go through the history of CNNs specifically
    for Image Classification tasks — starting from those early research years in the
    90's to the golden era of the mid-2010s when many of the most genius Deep Learning
    architectures ever were conceived, and finally discuss the latest trends in CNN
    research now as they compete with attention and vision-transformers.
  prefs: []
  type: TYPE_NORMAL
- en: '*Check out the* [*YouTube video*](https://youtu.be/N_PocrMHWbw) *that explains
    all the concepts in this article visually with animations. Unless otherwise specified,
    all the images and illustrations used in this article are generated by myself
    during creating the video version.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f788fc5c6f6bbd352b7b138f5198dd1.png)'
  prefs: []
  type: TYPE_IMG
- en: The papers we will be discussing today!
  prefs: []
  type: TYPE_NORMAL
- en: '**The Basics of Convolutional Neural Networks**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the heart of a CNN is the convolution operation. We scan the filter across
    the image and calculate the dot product of the filter with the image at each overlapping
    location. **This resulting output is called a feature map and it captures how
    much and where the filter pattern is present in the image.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a88bb9a19c81a4788e265c2bdfb7177.png)'
  prefs: []
  type: TYPE_IMG
- en: How Convolution works — The kernel slides over the input image and calculates
    the overlap (dot-product) at each location — outputting a feature map in the end!
  prefs: []
  type: TYPE_NORMAL
- en: In a convolution layer, we train multiple filters that extract different feature
    maps from the input image. When we stack multiple convolutional layers in sequence
    with some non-linearity, we get a convolutional neural network (CNN).
  prefs: []
  type: TYPE_NORMAL
- en: So each convolution layer simultaneously does 2 things —
  prefs: []
  type: TYPE_NORMAL
- en: 1\. **spatial filtering** with the convolution operation between images and
    kernels, and
  prefs: []
  type: TYPE_NORMAL
- en: 2\. **combining the multiple input channels** and output a new set of channels.
  prefs: []
  type: TYPE_NORMAL
- en: 90 percent of the research in CNNs has been to modify or to improve just these
    two things.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a53ab3d99f44347e87950b7143ae2f8.png)'
  prefs: []
  type: TYPE_IMG
- en: The two main things CNN do
  prefs: []
  type: TYPE_NORMAL
- en: The 1989 Paper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[This 1989 paper](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf) taught
    us how to train non-linear CNNs from scratch using backpropagation. They input
    16x16 grayscale images of handwritten digits, and pass through two convolutional
    layers with 12 filters of size 5x5\. The filters also move with a stride of 2
    during scanning. Strided-convolution is useful for downsampling the input image.
    After the conv layers, the output maps are flattened and passed through two fully
    connected networks to output the probabilities for the 10 digits. Using the softmax
    cross-entropy loss, the network is optimized to predict the correct labels for
    the handwritten digits. After each layer, the tanh nonlinearity is also used —
    allowing the learned feature maps to be more complex and expressive. With just
    9760 parameters, this was a very small network compared to today’s networks which
    contain hundreds of millions of parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f6ce141fce07f95a9408e58d031bf95.png)'
  prefs: []
  type: TYPE_IMG
- en: The OG CNN architecture from 1989
  prefs: []
  type: TYPE_NORMAL
- en: '**Inductive Bias**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inductive Bias is a concept in Machine Learning where we deliberately introduce
    specific rules and limitations into the learning process to move our models away
    from generalizations and steer more toward solutions that follow our human-like
    understanding.
  prefs: []
  type: TYPE_NORMAL
- en: When humans classify images, we also do spatial filtering *to look for common
    patterns to form multiple representations* and then *combine them together to
    form our predictions*. The CNN architecture is designed to replicate just that.
    In feedforward networks, each pixel is treated like it’s own isolated feature
    as each neuron in the layers connects with all the pixels — in CNNs there is more
    parameter-sharing because the same filter scans the entire image. Inductive biases
    make CNNs less data-hungry too because they get local pattern recognition for
    free due to the network design but feedforward networks need to spend their training
    cycles learning about it from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: '**Le-Net 5 (1998)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/d625a2448a44d6150887d3d8a598512d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Lenet-5 architecture (Credit: [Le-Net-5 paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: In 1998, Yann LeCun and team published the [Le-Net 5](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)
    — a deeper and larger 7-layer CNN model network. They also use Max Pooling which
    downsamples the image by grabbing the maximum values from a 2x2 sliding window.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59cfe79ef5bedf3c1b0dd5b33ae17f44.png)![](../Images/1f847d9fc9f1eab0131f8b91092d006d.png)'
  prefs: []
  type: TYPE_IMG
- en: How Max Pooling works (LEFT) and how Local Receptive Fields increase as CNNs
    add more layers (RIGHT)
  prefs: []
  type: TYPE_NORMAL
- en: Local Receptive Field
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Notice when you train a 3x3 conv layer, each neuron is connected to a 3x3 region
    in the original image — this is the neuron’s local receptive field — the region
    of the image where this neuron extracts patterns from.
  prefs: []
  type: TYPE_NORMAL
- en: When we pass this feature map through another 3x3 layer , the new feature map
    indirectly creates a receptive field of a larger 5x5 region from the original
    image. Additionally, when we downsample the image through max-pooling or strided-convolution,
    the receptive field also increases — making deeper layers access the input image
    more and more globally.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, earlier layers in a CNN can only pick low-level details like
    specific edges or corners, and the latter layers pick up more spread-out global-level
    patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The Draught (1998–2012)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As impressive Le-Net-5 was, researchers in the early 2000s still deemed neural
    networks to be computationally very expensive and data hungry to train. There
    was also problems with overfitting — where a complex neural network will just
    memorize the entire training dataset and fail to generalize on new unseen datasets.
    The researchers instead focused on traditional machine learning algorithms like
    support vector machines that were showing much better performance on the smaller
    datasets of the time with much less computational demands.
  prefs: []
  type: TYPE_NORMAL
- en: '**ImageNet Dataset (2009)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [ImageNet dataset](https://www.image-net.org/index.php) was open-sourced
    in 2009 — it contained 3.2 million annotated images at the time covering over
    1000 different classes. Today it has over 14 million images and over 20,000 annotated
    different classes. Every year from 2010 to 2017 we got this massive competition
    called the [ILSVRC](https://www.image-net.org/challenges/LSVRC/) where different
    research groups will publish models to beat the benchmarks on a subset of the
    ImageNet dataset. In 2010 and 2011, traditional ML methods like Support Vector
    Machines were winning — but starting from 2012 it was all about CNNs. The metric
    used to rank different networks was generally the top-5 error rate — measuring
    the percentage of times that the true class label was not in the top 5 classes
    predicted by the network.
  prefs: []
  type: TYPE_NORMAL
- en: '**AlexNet (2012)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf),
    introduced by Dr. Geoffrey Hinton and his team was the winner of ILSVRC 2012 with
    a top-5 test set error of 17%. Here are the three main contributions from AlexNet.'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Multi-scaled Kernels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AlexNet trained on 224x224 RGB images and used multiple kernel sizes in the
    network — an 11x11, a 5x5, and a 3x3 kernel. Models like Le-Net 5 only used 5x5
    kernels. Larger kernels are more computationally expensive because they train
    more weights, but also capture more global patterns from the image. Because of
    these large kernels, AlexNet had over 60 million trainable parameters. All that
    complexity can however lead to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3dbfe9a516bc0af4053955d3cbd0ae81.png)'
  prefs: []
  type: TYPE_IMG
- en: AlexNet starts with larger kernels (11x11) and reduces the size (to 5x5 and
    3x3) for deeper layers (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Dropout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To alleviate overfitting, AlexNet used a regularization technique called Dropout.
    During training, a fraction of the neurons in each layer is turned to zero. This
    prevents the network from being too reliant on specific neurons or groups of neurons
    for generating a prediction and instead encourages all the neurons to learn general
    meaningful features useful for classification.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. RELU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alexnet also replaced tanh nonlinearity with ReLU. RELU is an activation function
    that turns negative values to zero and keeps positive values as-is. The tanh function
    tends to saturate for deep networks because the gradients get low when the value
    of x goes too high or too low making optimization slow. RELU offers a steady gradient
    signal to train the network about 6 times faster than tanH.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46bebdc63f8f31a04b7b17236f3a5c49.png)![](../Images/9853cded8ce78082f5567793b0d22c30.png)![](../Images/40d276fedfc70960ef639c45f76d221a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'RELU, TANH, and How much difference RELU makes (Image credits: Middle: [Artificial
    Intelligence for Big Data](https://learning.oreilly.com/library/view/artificial-intelligence-for/9781788472173/),
    Right: [Alex-Net paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet also introduced the concept of Local Response Normalization and strategies
    for distributed CNN training.
  prefs: []
  type: TYPE_NORMAL
- en: '**GoogleNet / Inception (2014)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2014, GoogleNet paper got an ImageNet top-5 error rate of 6.67%. The core
    component of GoogLeNet was the **inception module**. Each inception module consists
    of parallel convolutional layers with different filter sizes (1x1, 3x3, 5x5) and
    max-pooling layers. Inception applies these kernels to the same input and then
    concats them, combining both low-level and medium-level features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/997501a2e4dd36f4519c2d85b202386a.png)'
  prefs: []
  type: TYPE_IMG
- en: An Inception Module
  prefs: []
  type: TYPE_NORMAL
- en: 1x1 Convolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: They also use 1x1 convolutional layer. Each 1x1 kernel first scales the input
    channels and then combines them. 1x1 kernels multiply each pixel with a fixed
    value — which is why it is also called pointwise convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: While larger kernels like 3x3 and 5x5 kernels do both spatial filtering and
    channel combination, 1x1 kernels are only good for channel mixing, and it does
    so very efficiently with a lower number of weights. For example, A 3-by-4 grid
    of 1x1 convolution layers trains only *(1x1 x 3x4 =)* 12 weights — but if it were
    3x3 kernels — we would train *(3x3 x 3x4 =)* 108 weights.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f234f8f634cf640af57b563c47cb50d.png)![](../Images/65f6501cf00fd0cbb64593fff021f896.png)'
  prefs: []
  type: TYPE_IMG
- en: 1x1 kernels versus larger kernels (LEFT) and Dimensionality reduction with 1x1
    kernels (RIGHT)
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GoogleNet uses 1x1 conv layers as a dimensionality reduction method to reduce
    the number of channels before running spatial filtering with the 3x3 and 5x5 convolutions
    on these lower dimensional feature maps. This helps them to cut down on the number
    of trainable weights compared to AlexNet.
  prefs: []
  type: TYPE_NORMAL
- en: '**VGGNet (2014)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[The VGG Network](https://arxiv.org/abs/1409.1556) claims that we do not need
    larger kernels like 5x5 or 7x7 networks and all we need are 3x3 kernels. 2 layer
    3x3 convolutional layer has the same receptive field of the image that a single
    5x5 layer does. Three 3x3 layers have the same receptive field that a single 7x7
    layer does.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep 3x3 Convolution Layers capture the same receptive field as larger kernels
    but with fewer parameters!**'
  prefs: []
  type: TYPE_NORMAL
- en: One 5x5 filter trains 25 weights — while two 3x3 filters train 18 weights. Similarly
    one 7x7 trains 49 weights, while 3 3x3 trains just 27\. Training with deep 3x3
    convolution layers became the standard for a long time in CNN architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cfed290a6af3ba3dee066dc01e5f4633.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Batch Normalization (2015)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep neural networks can suffer from a problem known as **“Internal Covariate
    Shift”** during training. Since the earlier layers of the network are constantly
    training, the latter layers need to continuously adapt to the constantly shifting
    input distribution it receive from the previous layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[Batch Normalization](https://arxiv.org/pdf/1502.03167) aims to counteract
    this problem by normalizing the inputs of each layer to have zero mean and unit
    standard deviation during training. A batch normalization or BN layer can be applied
    after any convolution layer. During training it subtracts the mean of the feature
    map along the minibatch dimension and divides it by the standard deviation. This
    means that each layer will now see a more stationary unit gaussian distribution
    during training.'
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of Batch Norm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: converge around 14 times faster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: let us use higher learning rates, and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: makes the network robust to the initial weights of the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**ResNets (2016)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Networks struggle to do Identity Mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you have a shallow neural network that has great accuracy on a classification
    task. Turns out that if we added 100 new convolution layers on top of this network,
    the training accuracy of the model could go down!
  prefs: []
  type: TYPE_NORMAL
- en: This is quite counter-intuitive because all these new layers need to do is copy
    the output of the shallow network at each layer — and at least be able to match
    the original accuracy. In reality, deep networks can be notoriously difficult
    to train because gradients can saturate or become unstable when backpropagating
    through many layers. With Relu and batch norm, we were able to train 22-layer
    deep CNNs at this point — the good folks at Microsoft introduced [ResNets](https://arxiv.org/abs/1512.03385)
    in 2015 which allowed us to stably train 150 layered CNNs. What did they do?
  prefs: []
  type: TYPE_NORMAL
- en: Residual learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The input passes through one or more CNN layers as usual, but at the end, the
    original input is added back to the final output. These blocks are called residual
    blocks because they don’t need to learn the final output feature maps in the traditional
    sense — but they are just the residual features that must be added to the input
    to get the final feature maps. **If the weights in the middle layers were to turn
    themselves to ZERO, then the residual block would just return the identity function
    — meaning it would be able to easily copy the input X.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b67371d5f87b5198e3e1a03c3a62ef15.png)'
  prefs: []
  type: TYPE_IMG
- en: Residual Networks
  prefs: []
  type: TYPE_NORMAL
- en: Easy Gradient Flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During backpropagation gradients can directly flow through these shortcut paths
    to reach the earlier layers of the model faster, helping to prevent gradient vanishing
    issues. ResNet stacks many of these blocks together to form really deep networks
    without any loss of accuracy!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55b293233faca9c8f22ed808e1dd8c4f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[From the ResNet paper](https://arxiv.org/abs/1512.03385)'
  prefs: []
  type: TYPE_NORMAL
- en: And with this remarkable improvement, ResNets managed to train a 152-layered
    model that got a top-5 error rate that shattered all previous records!
  prefs: []
  type: TYPE_NORMAL
- en: '**DenseNet (2017)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/ef9fa851e3acd1c08106018c2f39b471.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Dense-Nets](https://arxiv.org/abs/1608.06993) also add shortcut paths connecting
    earlier layers with the latter layers in the network. A DenseNet block trains
    a series of convolution layers, and the output of every layer is concatenated
    with the feature maps of every previous layer in the block before passing to the
    next layer. Each layer adds only a small number of new feature maps to the “collective
    knowledge” of the network as the image flows through the network. DenseNets have
    an improved flow of information and gradients throughout the network because each
    layer has direct access to the gradients from the loss function.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/164f7f2c97a282693b45bcb96e2b21d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Dense Nets
  prefs: []
  type: TYPE_NORMAL
- en: Squeeze and Excitation Network (2017)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[SEN-NET](https://arxiv.org/abs/1709.01507) was the final winner of the ILSVRC
    competition, which introduced the Squeeze and Excitation Layer into CNNs. The
    SE block is designed to explicitly model the dependencies between all the channels
    of a feature map. In normal CNNs, each channel of a feature map is computed independently
    of each other; SEN-Net applies a self-attention-like method to make each channel
    of a feature map contextually aware of the global properties of the input image.
    SEN-Net won the final ILVSRC of 2017, and one of the 154-layered SenNet + ResNet
    models got a ridiculous top-5 error rate of 4.47%.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b871751ded64eef0b9db11f9795ad28.png)'
  prefs: []
  type: TYPE_IMG
- en: '[SEN-NET](https://arxiv.org/abs/1709.01507)'
  prefs: []
  type: TYPE_NORMAL
- en: Squeeze Operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The squeeze operation compresses the spatial dimensions of the input feature
    map into a channel descriptor using global average pooling. Since each channel
    contains neurons that capture local properties of the image, the squeeze operation
    accumulates global information about each channel.
  prefs: []
  type: TYPE_NORMAL
- en: Excitation Operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The excitation operation rescales the input feature maps by channel-wise multiplication
    with the channel descriptors obtained from the squeeze operation. This effectively
    propagates global-level information to each channel — contextualizing each channel
    with the rest of the channels in the feature map.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c37db341a2da8c50a202b6e36a3c93a6.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Squeeze and Excitation Block](https://arxiv.org/abs/1709.01507)'
  prefs: []
  type: TYPE_NORMAL
- en: '**MobileNet (2017)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolution layers do two things –1) *filtering spatial information* and 2)
    *combining them channel-wise*. The MobileNet paper uses **Depthwise Separable
    Convolution**,a technique that separates these two operations into two different
    layers — Depthwise Convolution for filtering and pointwise convolution for channel
    combination.
  prefs: []
  type: TYPE_NORMAL
- en: Depthwise Convolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/1b885d58c9a73b3b1a51592e2d9e1966.png)'
  prefs: []
  type: TYPE_IMG
- en: Depthwise Separable Convolution
  prefs: []
  type: TYPE_NORMAL
- en: Given an input set of feature maps with M channels, first, they use depthwise
    convolution layers that train M 3x3 convolutional kernels. Unlike normal convolution
    layers that perform convolution on all feature maps, depthwise convolution layers
    train filters that perform convolution on just one feature map each. Secondly,
    they use 1x1 pointwise convolution filters to mix all these feature maps. **Separating
    the filtering and combining steps like this drastically reduces the number of
    weights**, making it super lightweight while still retaining the performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66344afbabc2e1473b31832405fdda81.png)'
  prefs: []
  type: TYPE_IMG
- en: Why Depthwise Separable Layers reduce training weights
  prefs: []
  type: TYPE_NORMAL
- en: '**MobileNetV2 (2019)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 2018, MobileNetV2 improved the MobileNet architecture by introducing two
    more innovations: *Linear Bottlenecks* and *Inverted residuals*.'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Bottlenecks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MobileNetV2 uses 1x1 pointwise convolution for dimensionality reduction, followed
    by depthwise convolution layers for spatial filtering, and another 1x1 pointwise
    convolution layer to expand the channels back. **These bottlenecks don’t pass
    through RELU and are instead kept linear.** RELU zeros out all the negative values
    that came out of the dimensionality reduction step — and this can cause the network
    to lose valuable information especially if a bulk of this lower dimensional subspace
    was negative. Linear layers prevent the loss of excessive information during this
    bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8950a4c49cce3be0d309a0c1db0f9c82.png)'
  prefs: []
  type: TYPE_IMG
- en: The width of each feature map is intended to show the relative channel dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Inverted Residuals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second innovation is called Inverted Residuals. Generally, residual connections
    occur between layers with the highest channels, but the authors add shortcuts
    between the bottlenecks layers. The bottleneck captures the relevant information
    within a low-dimensional latent space, and the free flow of information and gradient
    between these layers is the most crucial.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2e4c4eae0d3d37c24af81e463c38f2b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Vision Transformers (2020)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vision Transformers or ViTs established that transformers can indeed beat state-of-the-art
    CNNs in Image Classification. Transformers and Attention mechanisms provide a
    highly parallelizable, scalable, and general architecture for modeling sequences.
    Neural Attention is a whole different area of Deep Learning, which we won’t get
    into this article, but feel free to learn more in this Youtube video.
  prefs: []
  type: TYPE_NORMAL
- en: ViTs use Patch Embeddings and Self-Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The input image is first divided into a sequence of fixed-size patches. Each
    patch is independently embedded into a fixed-size vector either through a CNN
    or passing through a linear layer. These patch embeddings and their positional
    encodings are then inputted as a sequence of tokens into a self-attention-based
    transformer encoder. Self-attention models the relationships between all the patches,
    and outputs new updated patch embeddings that are contextually aware of the entire
    image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc228d2bbe95cf7993f257be0bc535bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Vision Transformers. Each self-attention layer further contextualizes each patch
    embedding with the global context of the image
  prefs: []
  type: TYPE_NORMAL
- en: Inductive Bias vs Generality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Where CNNs introduce several inductive biases about images, Transformers do
    the opposite — No localization, no sliding kernels — they rely on generality and
    raw computing to model the relationships between all the patches of the image.
    The Self-Attention layers allow global connectivity between all patches of the
    image irrespective of how far they are spatially. Inductive biases are great on
    smaller datasets, but the promise of Transformers is on massive training datasets,
    a general framework is going to eventually beat out the inductive biases offered
    by CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a450fb97b290d4419671ac36c27f673.png)'
  prefs: []
  type: TYPE_IMG
- en: Convolution Layers vs Self-Attention Layers
  prefs: []
  type: TYPE_NORMAL
- en: '**ConvNext —** A ConvNet for the 2020s **(2022)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A great choice to include in this article would be Swin Transformers, but that
    is a topic for a different day! Since this is a CNN article, let’s focus on one
    last CNN paper.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a0f9822f909de4a7b52dfe04861be4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Patchifying Images like VITs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The input of ConvNext follows the patching strategy inspired by Vision Transformers.
    A 4x4 convolution kernel with a stride of 4 creates a downsampled image which
    is inputted into the rest of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Depthwise Separable Convolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inspired by MobileNet, ConvNext uses depthwise separable convolution layers.
    The authors also hypothesize depthwise convolution is similar to the weighted
    sum operation in self-attention, which operates on a per-channel basis by only
    mixing information in the spatial dimension. Also the 1x1 pointwise convolutions
    are similar to the channel mixing steps in Self-Attention.
  prefs: []
  type: TYPE_NORMAL
- en: Larger Kernel Sizes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While ConvNets have been using 3x3 kernels ever since VGG, ConvNext proposes
    larger 7x7 filters to capture a wider spatial context, trying to come close to
    the fully global context that ViTs capture, while retaining the localization spirits
    of CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: There are also some other tweaks, like using MobileNetV2-inspired inverted bottlenecks,
    the GELU activations, layer norms instead of batch norms, and more that shape
    up the rest of the ConvNext architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ConvNext are more computationally efficient way with the depthwise separable
    convolutions and is more scalable than transformers on high-resolution images
    — this is because Self-Attention scales quadratically with sequence length and
    Convolution doesn’t.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/273120ccbef227363f48781e3fc84edf.png)'
  prefs: []
  type: TYPE_IMG
- en: Final Thoughts!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The history of CNNs teaches us so much about Deep Learning, Inductive Bias,
    and the nature of computation itself. It’ll be interesting to see what wins out
    in the end — the inductive biases of ConvNets or the Generality of Transformers.
    Do check out the companion YouTube video for a visual tour of this article, and
    the individual papers as listed below.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CNN with Backprop (1989): [http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'LeNet-5: [http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet:[https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'GoogleNet: [https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)'
  prefs: []
  type: TYPE_NORMAL
- en: 'VGG: [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch Norm: [https://arxiv.org/pdf/1502.03167](https://arxiv.org/pdf/1502.03167)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ResNet: [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)'
  prefs: []
  type: TYPE_NORMAL
- en: 'DenseNet: [https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)'
  prefs: []
  type: TYPE_NORMAL
- en: 'MobileNet: [https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)'
  prefs: []
  type: TYPE_NORMAL
- en: 'MobileNet-V2: [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Vision Transformers: [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ConvNext: [https://arxiv.org/abs/2201.03545](https://arxiv.org/abs/2201.03545)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Squeeze-and-Excitation Network: [https://arxiv.org/abs/1709.01507](https://arxiv.org/abs/1709.01507)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Swin Transformers: [https://arxiv.org/abs/2103.14030](https://arxiv.org/abs/2103.14030)'
  prefs: []
  type: TYPE_NORMAL
