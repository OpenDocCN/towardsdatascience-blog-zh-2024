<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>An Intuitive View on Mutual Information</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>An Intuitive View on Mutual Information</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intuitive-view-on-mutual-information-db0655535f84?source=collection_archive---------6-----------------------#2024-03-13">https://towardsdatascience.com/an-intuitive-view-on-mutual-information-db0655535f84?source=collection_archive---------6-----------------------#2024-03-13</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="76cf" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Layman’s guide to appreciating the concept of association</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@markchangg?source=post_page---byline--db0655535f84--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mark Chang" class="l ep by dd de cx" src="../Images/149afb7a61e175743da6c36f05bc6318.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*YXu3bio_AwfAJfRAjHcylA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--db0655535f84--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@markchangg?source=post_page---byline--db0655535f84--------------------------------" rel="noopener follow">Mark Chang</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--db0655535f84--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 13, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/119a293dadb7e3a97d7b8666de767d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vgR9sI0txYKdR7Ca"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@benwhitephotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ben White</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="44d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Recently I’ve been working on a project that aims to screen <strong class="ne fr">pairs of variables</strong> in the stock market, and <strong class="ne fr">see how they show enough correlation potential</strong> for us to deep-dive and research further.</p><p id="d7cb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Throughout my research, I’ve chanced upon many different methodologies; from the humble Spearman’s/Pearson linear correlation, to the more advanced non-linear methods using Time-Delay Embeddings and even Machine Learning techniques.</p><p id="6577" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And that was when I chanced upon this robust and probabilistic concept known as <strong class="ne fr">Mutual Information </strong>that helps one to measure the level of association/dependence between two variables. This serves as a good Step 0 tool for model development or associative studies.</p><p id="577c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Scouring the web to understand further, I’ve realized that while there were excellent mathematical and statistical explanations, <strong class="ne fr">there weren’t many traces of intuitive insights on how and why Mutual Information works</strong>.</p><p id="d501" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And therefore, here we are,</p><blockquote class="ny"><p id="a1a9" class="nz oa fq bf ob oc od oe of og oh nx dx">Welcome to my attempt at helping you break down and appreciate this statistical concept!</p></blockquote></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0b22" class="oq or fq bf os ot ou gq ov ow ox gt oy oz pa pb pc pd pe pf pg ph pi pj pk pl bk"><strong class="al">The Textbook Definition</strong></h1><blockquote class="pm pn po"><p id="56c9" class="nc nd pp ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Mutual Information is a measure of how much “information” you can get of one variable by observing another variable</p></blockquote><p id="daf2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I’m sure you’ve seen the above statement, or variants of it, throughout your own research. But what exactly is this “information” that they are talking about? How does it tell me that two variables are associated/dependent?</p><p id="640e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The definition becomes even more daunting when you look at the formulation:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pq"><img src="../Images/78bbaebed9d998ae8de882ead9c34c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*ASFds3qQhB8odoW6LVaN9g.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Formula for Mutual Information for Discrete Observations</figcaption></figure><p id="4003" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Fret not! Let’s get into breaking down this concept into digestible chunks using a case study.</p></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><blockquote class="ny"><p id="99d1" class="nz oa fq bf ob oc od oe of og oh nx dx">“Do people really use umbrellas only when it rains?”</p></blockquote><figure class="ps pt pu pv pw mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pr"><img src="../Images/23262ac9ac78fbd879c736b941f5b590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JVduYc6KSZPK7oyq"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@dayongzhang?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">zhang dayong</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="9de0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">said Bob, your drunk friend during a night out of festivities.</p><p id="95ff" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">He insists that people carry umbrella only when they feel like it, and not because they need it to shelter them from the rain.</p><p id="d749" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You think that that statement is ludicrous! It challenged every observation you made growing up; every notion of logic within your bones.</p><p id="1e95" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You decided to stalk Bob and observe him over the next 5 days during his vacation in tropical Singapore. You want to see if he really walks the talk and lives true to his bodacious claims.</p><p id="7781" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You decide to do so using the concept of <strong class="ne fr">Mutual Information</strong>.</p></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="7bae" class="oq or fq bf os ot ou gq ov ow ox gt oy oz pa pb pc pd pe pf pg ph pi pj pk pl bk">Bob vs Mutual Information</h1><p id="f892" class="pw-post-body-paragraph nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx fj bk">We can break down the Mutual Information formula into the following parts:</p><h2 id="c112" class="qc or fq bf os qd qe qf ov qg qh qi oy nl qj qk ql np qm qn qo nt qp qq qr qs bk"><strong class="al">The <em class="qt">x, X</em> and <em class="qt">y, Y</em></strong></h2><p id="7f14" class="pw-post-body-paragraph nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx fj bk"><em class="pp">x</em> and <em class="pp">y</em> are the individual observations/values that we see in our data. <em class="pp">X</em> and <em class="pp">Y </em>are just the set of these individual values. A good example would be as follows:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qu"><img src="../Images/a4f6e7a5ead25c261663a5081fbb6a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CI05LBYYsHGO6PYYqb56OQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Discrete/Binary observation of umbrella-wielding and weather</figcaption></figure><p id="9916" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And assuming we have 5 days of observations of Bob in this exact sequence:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qv"><img src="../Images/a5f8f1a57846c491c3a4f19ac00c51ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*geJaw-WU8r0lv1DAegnDzA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Discrete/Binary observation of umbrella-wielding and weather over 5 days</figcaption></figure><h2 id="0073" class="qc or fq bf os qd qe qf ov qg qh qi oy nl qj qk ql np qm qn qo nt qp qq qr qs bk"><strong class="al">Individual/Marginal Probability</strong></h2><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qw"><img src="../Images/383c1c8c07fb4779e277865282fe9c24.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*H3-TqOzA5viRG1odN8SoIw.png"/></div></figure><p id="d945" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These are just the simple probability of observing a particular <em class="pp">x</em> or <em class="pp">y</em> in their respective sets of possible <em class="pp">X</em> and <em class="pp">Y</em> values.</p><p id="71a9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Take <em class="pp">x = 1</em> as an example: the probability is simply <em class="pp">0.4</em> (Bob carried an umbrella 2 out of 5 days of his vacation).</p><h2 id="807d" class="qc or fq bf os qd qe qf ov qg qh qi oy nl qj qk ql np qm qn qo nt qp qq qr qs bk"><strong class="al">Joint Probability</strong></h2><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qx"><img src="../Images/cc6114a1cdc620ee8e98e97fbdbe1177.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/format:webp/1*zhyOkUvCF4N-5fRJu1QbGA.png"/></div></figure><p id="4d97" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is the probability of observing a particular <em class="pp">x </em>and <em class="pp">y</em> from the joint probability of (<em class="pp">X, Y)</em>. The joint probability (<em class="pp">X, Y) </em>is simply just the set of paired observations. We pair them up according to their index.</p><p id="5d34" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In our case with Bob, we pair the observations up based on which day they occurred.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qy"><img src="../Images/a881e1dbacfa805bf74e041ddc5a140b.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*IRnmkhfNwlGaKNBYMn4_0g.png"/></div></figure><p id="3526" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You may be tempted to jump to a conclusion after looking at the pairs:</p><blockquote class="pm pn po"><p id="8327" class="nc nd pp ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since there are equal-value pairs occurring 80% of the time, it clearly means that people carry umbrellas BECAUSE it is raining!</p></blockquote><p id="3c8a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Well I’m here to play the devil’s advocate and say that that may just be a freakish coincidence:</p><p id="e0a1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If the chance of rain is very low in Singapore, and, independently, the likelihood of Bob carrying umbrella is also equally low (because he hates holding extra stuff), can you see that the odds of having <em class="pp">(0,0)</em> paired observations will be very high <strong class="ne fr">naturally</strong>?</p><blockquote class="ny"><p id="522f" class="nz oa fq bf ob oc od oe of og oh nx dx">So what can we do to prove that these paired observations are not by coincidence?</p></blockquote><h2 id="3760" class="qc or fq bf os qd qz qf ov qg ra qi oy nl rb qk ql np rc qn qo nt rd qq qr qs bk"><strong class="al">Joint Versus Individual Probabilities</strong></h2><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj re"><img src="../Images/5be386e3384c40df59a7e23635daadee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nx_wLJQs-xX_Yn0m2fMUNw.png"/></div></div></figure><p id="d594" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can take the ratio of both probabilities to give us a clue on the <strong class="ne fr">“extent of coincidence”</strong>.</p><p id="ed0b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the denominator, we take the product of both individual probabilities of a particular <em class="pp">x</em> and particular <em class="pp">y </em>occurring. Why did we do so?</p><p id="d117" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Peering into the humble coin toss</strong></p><p id="183c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Recall the first lesson you took in statistics class: calculating the probability of getting 2 heads in 2 tosses of a fair coin.</p><ul class=""><li id="2502" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx rf rg rh bk">1st Toss [ <em class="pp">p(x</em>) ]: There’s a 50% chance of getting heads</li><li id="5207" class="nc nd fq ne b go ri ng nh gr rj nj nk nl rk nn no np rl nr ns nt rm nv nw nx rf rg rh bk">2nd Toss [ <em class="pp">p(y</em>) ]: There’s still a 50% chance of getting heads, since the outcome is <strong class="ne fr">independent </strong>of what happened in the 1st toss</li><li id="0cc4" class="nc nd fq ne b go ri ng nh gr rj nj nk nl rk nn no np rl nr ns nt rm nv nw nx rf rg rh bk">The above 2 tosses make up your individual probabilities</li><li id="632a" class="nc nd fq ne b go ri ng nh gr rj nj nk nl rk nn no np rl nr ns nt rm nv nw nx rf rg rh bk">Therefore, the <strong class="ne fr">theoretical</strong> probability of getting both heads in 2 independent tosses is <em class="pp">0.5 </em>* 0.5 <em class="pp">= 0.25 </em>( <a class="af nb" href="#0073" rel="noopener ugc nofollow"><em class="pp">p(x).p(y)</em></a><em class="pp"> </em>)</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rn"><img src="../Images/8499b8db31a00ac5b3cb3b6f52277fbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2xNX40sQ4fldjXcr_8jcUw.png"/></div></div></figure><p id="1013" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And if you actually do maybe 100 sets of that double-coin-toss experiment, you’ll likely see that you get the <em class="pp">(heads, heads)</em> result 25% of the time. The 100 sets of experiment is actually your <a class="af nb" href="#807d" rel="noopener ugc nofollow">(<em class="pp">X, Y) </em>joint probability set</a>!</p><p id="e516" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Hence, when you take the ratio of joint versus combined-individual probabilities, you get a value of <em class="pp">1.</em></p><p id="d398" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is actually the real <strong class="ne fr">expectation for independent events</strong>: the joint probability of a specific pair of values occurring is exactly equal to the product of their individual probabilities! Just like what you were taught in fundamental statistics.</p><p id="4dd7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now imagine that your 100-set experiment yielded <em class="pp">(heads, heads)</em> 90% of the time. <strong class="ne fr">Surely that can’t be a coincidence…</strong></p><p id="676e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You expected 25% since you know that they are independent events, yet what was observed is an extreme skew of this expectation.</p><p id="97e3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To put this qualitative feeling into numbers, the ratio of probabilities is now a whopping <em class="pp">3.6 (0.9 / 0.25)</em>, essentially 3.6x more frequent than we expected.</p><p id="9a44" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As such, we start to think that <strong class="ne fr">maybe the coin tosses were</strong> <strong class="ne fr">not independent. </strong>Maybe the result of the 1st toss might actually have some unexplained effect on the 2nd toss. <strong class="ne fr">Maybe</strong> <strong class="ne fr">there is some level of association/dependence between 1st and 2nd toss</strong>.</p><blockquote class="ny"><p id="4bc0" class="nz oa fq bf ob oc od oe of og oh nx dx">That is what <strong class="al">Mutual Information tries to tells </strong>us!</p></blockquote><h2 id="776f" class="qc or fq bf os qd qz qf ov qg ra qi oy nl rb qk ql np rc qn qo nt rd qq qr qs bk">Expected Value of Observations</h2><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ro"><img src="../Images/25e9be68a91424b51e5245c580c56a0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NhDuFPne8J-XGe66JBhCuQ.png"/></div></div></figure><p id="99f9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For us to be fair to Bob, we should not just look at the times where his claims are wrong, i.e. calculate the ratio of probabilities of <em class="pp">(0,0)</em> and <em class="pp">(1,1)</em>.</p><p id="c0a9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We should also calculate the ratio of probabilities for when his claims are correct, i.e. <em class="pp">(0,1)</em> and <em class="pp">(1,0).</em></p><p id="bb6f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Thereafter, we can <strong class="ne fr">aggregate all 4 scenarios </strong>in an expected value method, which just means “taking the average”: aggregate up all ratio of probabilities for each observed pair in (<em class="pp">X, Y)</em>, then divide it by the number of observations.</p><p id="4936" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">That is the purpose of these two summation terms. For continuous variables like my stock market example, we will then use integrals instead.</p><h2 id="4f9f" class="qc or fq bf os qd qe qf ov qg qh qi oy nl qj qk ql np qm qn qo nt qp qq qr qs bk">Logarithm of Ratios</h2><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rp"><img src="../Images/a273e4b9b4ea16ede7fbcbbd37c09dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*72feCN3Jo8002q82wS66pQ.png"/></div></div></figure><p id="84b5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Similar to how we calculate the probability of getting 2 consecutive heads for the coin toss, we are also now calculating the additional probability of seeing the 5 pairs that we observed.</p><p id="9cb5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For the coin toss, we calculate by <strong class="ne fr">multiplying</strong> the probabilities of each toss. For Bob, it’s the same: the <strong class="ne fr">probabilities have multiplicative effect</strong> on each other to give us the sequence that we observed in the joint set.</p><p id="3e66" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With logarithms, we<strong class="ne fr"> turn multiplicative effects into additive</strong> ones:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rq"><img src="../Images/e40d4d458dde7c2f139bf9b2e10df7b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Og1HTlO7nZ95TlaRWFTkww.png"/></div></div></figure><p id="302a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Converting the ratio of probabilities to their logarithmic variants, we can now simply just calculate the expected value as <a class="af nb" href="#776f" rel="noopener ugc nofollow">described above</a> using <strong class="ne fr">summation of their logarithms</strong>.</p><p id="1e70" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Feel free to use log-base 2, <em class="pp">e</em>, or 10, it does not matter for the purposes of this article.</p><h2 id="b460" class="qc or fq bf os qd qe qf ov qg qh qi oy nl qj qk ql np qm qn qo nt qp qq qr qs bk">Putting It All Together</h2><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pq"><img src="../Images/78bbaebed9d998ae8de882ead9c34c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*ASFds3qQhB8odoW6LVaN9g.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Formula for Mutual Information for Discrete Observations</figcaption></figure><p id="279d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s now prove Bob wrong by calculating the Mutual Information. I will use log-base <em class="pp">e</em> (natural logarithm) for my calculations:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rr"><img src="../Images/c07fbe0d1451d05fa071d7d2555cf854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SvID0Qp_qw5r8n3zeMFOvw.png"/></div></div></figure><p id="7038" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So what does the value of <strong class="ne fr"><em class="pp">0.223</em></strong><em class="pp"> </em>tell us?</p><p id="c70e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s first assume Bob is right, and that the use of umbrellas are <strong class="ne fr">independent</strong> from presence of rain:</p><ul class=""><li id="9fa2" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx rf rg rh bk">We know that the joint probability will exactly equal the product of the individual probabilities.</li><li id="24be" class="nc nd fq ne b go ri ng nh gr rj nj nk nl rk nn no np rl nr ns nt rm nv nw nx rf rg rh bk">Therefore, for every <em class="pp">x</em> and <em class="pp">y</em> permutation, the ratio of probabilities <em class="pp">= 1</em>.</li><li id="ae3a" class="nc nd fq ne b go ri ng nh gr rj nj nk nl rk nn no np rl nr ns nt rm nv nw nx rf rg rh bk">Taking the logarithm, that equates to 0.</li><li id="aaa5" class="nc nd fq ne b go ri ng nh gr rj nj nk nl rk nn no np rl nr ns nt rm nv nw nx rf rg rh bk">Thus, the expected value of all permutations (i.e. Mutual Information) is therefore <strong class="ne fr"><em class="pp">0</em></strong>.</li></ul><p id="51b1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But since the Mutual Information score that we calculated is <strong class="ne fr">non-zero</strong>, we can therefore prove to Bob that he is wrong!</p></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="7a2d" class="oq or fq bf os ot ou gq ov ow ox gt oy oz pa pb pc pd pe pf pg ph pi pj pk pl bk">Beyond Linear Correlation</h1><p id="fda9" class="pw-post-body-paragraph nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx fj bk">Because Mutual Information is a probabilistic measure of association/dependence, it can work for <strong class="ne fr">non-linear correlation</strong> studies as well!</p><p id="7829" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Take for example two variables <em class="pp">X</em> and <em class="pp">Y:</em></p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rs"><img src="../Images/70abc9232eefbc36d9c44dcb55bc713f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P50Ln5RP7llMD5yGAfh2cQ.png"/></div></div></figure><p id="290a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Calculating their Mutual Information score, Spearman’s correlation score, and plotting, we get the following:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qu"><img src="../Images/4c73cfb29d55e9f31e7b62c88a438361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oXMp9TrUhbV4Q71Ms7rZ6g.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Y vs X: Y is a deterministic, non-linear scaling of X</figcaption></figure><p id="b7dc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Relying on Spearman’s correlation alone, we would think that these 2 variables have nothing to do with each other, but we know for a fact that they are deterministically related (based on my formula above)!</p><p id="023a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The non-zero Mutual Information score hints us to look deeper, <strong class="ne fr">albeit not giving us the explicit form of relation</strong>.</p><p id="1d41" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is also robust enough to work on strictly linear correlations:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rn"><img src="../Images/0f8a77486dbd30f736b568c22677154e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4YRyw-DUQ1_IvSi7Rg8ATg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Y vs X: Y is a deterministic, linear translation of X</figcaption></figure><p id="b3be" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, if you are ever unsure what kind of correlation you are expecting going into an X-vs-Y analysis, you can try out Mutual Information as a step zero!</p></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="956a" class="oq or fq bf os ot ou gq ov ow ox gt oy oz pa pb pc pd pe pf pg ph pi pj pk pl bk">My “Layman” Definition of Mutual Information</h1><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rt"><img src="../Images/792fe745d9888281b96376019e26e4a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JMdfZhVJpgQXIR4Y"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@picturesbyalbert?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Albert</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5d4d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With the <a class="af nb" href="#7bae" rel="noopener ugc nofollow">above examples and breakdown</a>, I hope I managed to help you guys get an intuitive understanding what Mutual Information is and how it works.</p><p id="0b4a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If it helps you further, I prefer to summarize Mutual Information as follows:</p><blockquote class="ny"><p id="e7c4" class="nz oa fq bf ob oc od oe of og oh nx dx">Mutual Information gives us the additional probability of x and y happening at the same time due to other factors above just their chance of co-occurring.</p></blockquote><p id="d768" class="pw-post-body-paragraph nc nd fq ne b go ru ng nh gr rv nj nk nl rw nn no np rx nr ns nt ry nv nw nx fj bk">Mutual Information is very useful in areas such as Feature Selection before building your Machine Learning models, and even text association analyses when used with text embeddings. Therefore, it is paramount that we truly know how it works before adopting it for its myriad of uses.</p><p id="9407" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With your newfound intuition and understanding, I believe you will be able to find other pockets of opportunities to apply this versatile concept as I will with my stock market ventures!</p></div></div></div></div>    
</body>
</html>