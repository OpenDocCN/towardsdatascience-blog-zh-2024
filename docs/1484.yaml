- en: Sparse Autoencoders, Additive Decision Trees, and Other Emerging Topics in AI
    Interpretability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/sparse-autoencoders-additive-decision-trees-and-other-emerging-topics-in-ai-interpretability-9ab1eaf14d3e?source=collection_archive---------6-----------------------#2024-06-13](https://towardsdatascience.com/sparse-autoencoders-additive-decision-trees-and-other-emerging-topics-in-ai-interpretability-9ab1eaf14d3e?source=collection_archive---------6-----------------------#2024-06-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://towardsdatascience.medium.com/?source=post_page---byline--9ab1eaf14d3e--------------------------------)[![TDS
    Editors](../Images/4b2d1beaf4f6dcf024ffa6535de3b794.png)](https://towardsdatascience.medium.com/?source=post_page---byline--9ab1eaf14d3e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9ab1eaf14d3e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9ab1eaf14d3e--------------------------------)
    [TDS Editors](https://towardsdatascience.medium.com/?source=post_page---byline--9ab1eaf14d3e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9ab1eaf14d3e--------------------------------)
    ·Sent as a [Newsletter](/newsletter?source=post_page---byline--9ab1eaf14d3e--------------------------------)
    ·4 min read·Jun 13, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Feeling inspired to write your first TDS post?* [*We’re always open to contributions
    from new authors*](http://bit.ly/write-for-tds)*.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As LLMs get bigger and AI applications more powerful, the quest to better understand
    their inner workings becomes harder — and more acute. Conversations around the
    risks of black-box models aren’t exactly new, but as the footprint of AI-powered
    tools continues to grow, and as hallucinations and other suboptimal outputs make
    their way into browsers and UIs with alarming frequency, it’s more important than
    ever for practitioners (and end users) to resist the temptation to accept AI-generated
    content at face value.
  prefs: []
  type: TYPE_NORMAL
- en: Our lineup of weekly highlights digs deep into the problem of model interpretability
    and explainability in the age of widespread LLM use. From detailed analyses of
    an influential new paper to hands-on experiments with other recent techniques,
    we hope you take some time to explore this ever-crucial topic.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Deep Dive into Anthropic’s Sparse Autoencoders by Hand**](/deep-dive-into-anthropics-sparse-autoencoders-by-hand-️-eebe0ef59709)Within
    a few short weeks, Anthropic’s “Scaling Monosemanticity” paper has attracted a
    lot of attention within the XAI community. [Srijanie Dey, PhD](https://medium.com/u/d60d06fe8655?source=post_page---user_mention--9ab1eaf14d3e--------------------------------)
    presents a beginner-friendly primer for anyone interested in the researchers’
    claims and goals, and in how they came up with an “innovative approach to understanding
    how different components in a neural network interact with one another and what
    role each component plays.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Interpretable Features in Large Language Models**](/interpretable-features-in-large-language-models-377fb25c72eb)For
    a high-level, well-illustrated explainer on the “Scaling Monosemanticity” paper’s
    theoretical underpinnings, we highly recommend [Jeremi Nuer](https://medium.com/u/7ce320f77bc9?source=post_page---user_mention--9ab1eaf14d3e--------------------------------)’s
    debut TDS article—you’ll leave it with a firm grasp of the researchers’ thinking
    and of this work’s stakes for future model development: “as improvements plateau
    and it becomes more difficult to scale LLMs, it will be important to truly understand
    how they work if we want to make the next leap in performance.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**The Meaning of Explainability for AI**](/the-meaning-of-explainability-for-ai-d8ae809c97fa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking a few helpful steps back from specific models and the technical challenges
    they create in their wake, [Stephanie Kirmer](https://medium.com/u/a8dc77209ef3?source=post_page---user_mention--9ab1eaf14d3e--------------------------------)
    gets “a bit philosophical” in her article about the limits of interpretability;
    attempts to illuminate those black-box models might never achieve full transparency,
    she argues, but are still important for ML researchers and developers to invest
    in.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/cb75693400faf7951b95020c81f00e96.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Joanna Kosinska](https://unsplash.com/@joannakosinska?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[**Additive Decision Trees**](/additive-decision-trees-85f2feda2223)In his
    recent work, [W Brett Kennedy](https://medium.com/u/5176dd5e0bcf?source=post_page---user_mention--9ab1eaf14d3e--------------------------------)
    has been focusing on interpretable predictive models, unpacking their underlying
    math and showing how they work in practice. His recent deep dive on additive decision
    trees is a powerful and thorough introduction to such a model, showing how it
    aims to supplement the limited available options for interpretable classification
    and regression models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Deep Dive on Accumulated Local Effect Plots (ALEs) with Python**](/deep-dive-on-accumulated-local-effect-plots-ales-with-python-0fc9698ed0ee)To
    round out our selection, we’re thrilled to share [Conor O''Sullivan](https://medium.com/u/4ae48256fb37?source=post_page---user_mention--9ab1eaf14d3e--------------------------------)’s
    hands-on exploration of accumulated local effect plots (ALEs): an older, but dependable
    method for providing clear interpretations even in the presence of multicollinearity
    in your model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in digging into some other topics this week? From quantization to
    Pokémon optimization strategies, we’ve got you covered!
  prefs: []
  type: TYPE_NORMAL
- en: In a fascinating project walkthrough, [Parvathy Krishnan](https://medium.com/u/102000f20d44?source=post_page---user_mention--9ab1eaf14d3e--------------------------------),
    Joaquim Gromicho, and Kai Kaiser show [how they’ve combined several geospatial
    datasets and some Python](/an-open-data-driven-approach-to-optimising-healthcare-facility-locations-using-python-397b3ce38185)
    to optimize the process of selecting healthcare-facility locations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn [how weight quantization works and how to apply it](/optimizing-deep-learning-models-with-weight-quantization-c786ffc6d6c1)
    in real-world deep learning workflows — [Chien Vu](https://medium.com/u/f2928e8b6c04?source=post_page---user_mention--9ab1eaf14d3e--------------------------------)’s
    tutorial is both thorough and accessible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The knapsack problem is a classic optimization challenge; [Maria Mouschoutzi,
    PhD](https://medium.com/u/dce3cb684eae?source=post_page---user_mention--9ab1eaf14d3e--------------------------------)
    approaches it with a fun new twist, showing how to create the most powerful Pokémon
    team [with the aid of modeling and PuLP, a Python optimization framework](/how-many-pokémon-fit-84f812c0387e).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Squeezing the most value out of RAG systems continues to be a top priority for
    many ML professionals. [Leonie Monigatti](https://medium.com/u/3a38da70d8dc?source=post_page---user_mention--9ab1eaf14d3e--------------------------------)
    takes a close look at [potential solutions for measuring context relevance](/the-challenges-of-retrieving-and-evaluating-relevant-context-for-rag-e362f6eaed34).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After more than a decade as a data leader at tech giants and high-growth startups,
    [Torsten Walbaum](https://medium.com/u/4e291ce6380c?source=post_page---user_mention--9ab1eaf14d3e--------------------------------)
    offers the [insights he’s accumulated around a fundamental question](/the-ultimate-guide-to-making-sense-of-data-aaa121db1119):
    how do we make sense of data?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysts might not often think of themselves as programmers, but [there’s
    still a lot of room for cross-disciplinary learning](/from-code-to-insights-software-engineering-best-practices-for-data-analysts-0dd6a2aaadfc)—as
    [Mariya Mansurova](https://medium.com/u/15a29a4fc6ad?source=post_page---user_mention--9ab1eaf14d3e--------------------------------)
    demonstrates in a data-focused roundup of software-engineering best practices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thank you for supporting the work of our authors! We love publishing articles
    from new authors, so if you’ve recently written an interesting project walkthrough,
    tutorial, or theoretical reflection on any of our core topics, don’t hesitate
    to [share it with us](http://bit.ly/write-for-tds).
  prefs: []
  type: TYPE_NORMAL
- en: Until the next Variable,
  prefs: []
  type: TYPE_NORMAL
- en: TDS Team
  prefs: []
  type: TYPE_NORMAL
