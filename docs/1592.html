<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Understanding Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Understanding Transformers</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-transformers-3344d16c8c36?source=collection_archive---------0-----------------------#2024-06-27">https://towardsdatascience.com/understanding-transformers-3344d16c8c36?source=collection_archive---------0-----------------------#2024-06-27</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="b578" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A straightforward breakdown of “Attention is All You Need”¹</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@aveekgoswami?source=post_page---byline--3344d16c8c36--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Aveek Goswami" class="l ep by dd de cx" src="../Images/605b68f373d08d4d82223f9478417177.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*KlK4WB4ztvTZQWUdbH7sEg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3344d16c8c36--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@aveekgoswami?source=post_page---byline--3344d16c8c36--------------------------------" rel="noopener follow">Aveek Goswami</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3344d16c8c36--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 27, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="1e76" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The transformer came out in 2017. There have been many, many articles explaining how it works, but I often find them either going too deep into the math or too shallow on the details. I end up spending as much time googling (or chatGPT-ing) as I do reading, which isn’t the best approach to understanding a topic. That brought me to writing this article, where I attempt to explain the most revolutionary aspects of the transformer while keeping it succinct and simple for anyone to read.</p><blockquote class="nf ng nh"><p id="4d7f" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This article assumes a general understanding of machine learning principles.</p></blockquote><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk nl"><img src="../Images/7e1f9078391c08b14a89cb789a4dcdd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T2Xjr37p_1w7f-34n0WBDg.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Tranformers, transforming. Image source: DALL-E (we’re learning about gen ai anyway)</figcaption></figure><h2 id="fba5" class="oc od fq bf oe of og oh oi oj ok ol om ms on oo op mw oq or os na ot ou ov ow bk">The ideas behind the Transformer led us to the era of Generative AI</h2><p id="a2b3" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">Transformers represented a new architecture of <strong class="ml fr">sequence transduction models. </strong>A sequence model is a type of model that transforms an input sequence to an output sequence. This input sequence can be of various data types, such as characters, words, tokens, bytes, numbers, phonemes (speech recognition), and may also be multimodal¹.</p><p id="bbd8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Before transformers, sequence models were largely based on recurrent neural networks (RNNs), long short-term memory (LSTM), gated recurrent units (GRUs) and convolutional neural networks (CNNs). They often contained some form of an attention mechanism to account for the context provided by items in various positions of a sequence.</p><h1 id="4e9e" class="pc od fq bf oe pd pe gq oi pf pg gt om ph pi pj pk pl pm pn po pp pq pr ps pt bk">The downsides of previous models</h1><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pu"><img src="../Images/89222b340d0651d0fe49a577cfff4f47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MBuTFwRJ8PwHKDluAUqhRw.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">RNN Illustration. Image source: <a class="af pv" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">Christopher Olah</a></figcaption></figure><ul class=""><li id="d33e" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pw px py bk"><strong class="ml fr">RNNs</strong>: The model tackles the data <strong class="ml fr">sequentially</strong>, so anything learned from the previous computation is accounted for in the next computation². However, its sequential nature causes a few problems: the model struggles to account for long-term dependencies for longer sequences (known as <strong class="ml fr">vanishing or exploding gradients</strong>), and <strong class="ml fr">prevents parallel processing</strong> of the input sequence as you cannot train on different chunks of the input at the same time (batching) because you will lose context of the previous chunks. This makes it more computationally expensive to train.</li></ul><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pz"><img src="../Images/b831a80d471f6266880cfbe8635facde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4uAyfopSl8LJk90ENzPLzg.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">LSTM and GRU overview. Image source: <a class="af pv" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">Christopher Olah</a></figcaption></figure><ul class=""><li id="6be8" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pw px py bk"><strong class="ml fr">LSTM and GRUs</strong>: Made use of <strong class="ml fr">gating mechanisms </strong>to preserve long-term dependencies³. The model has a <em class="ni">cell state</em> which contains the relevant information from the whole sequence. The cell state changes through gates such as the <strong class="ml fr">forget, input, output gates (LSTM)</strong><em class="ni">, </em>and<em class="ni"> </em><strong class="ml fr">update, reset gates</strong><em class="ni"> </em><strong class="ml fr">(GRU)</strong><em class="ni">. </em>These gates decide, at each sequential iteration, how much information from the previous state should be kept, how much information from the new update should be added, and then which part of the new cell state should be kept overall. While this improves the vanishing gradient issue, the models still <strong class="ml fr">work sequentially</strong> and hence <strong class="ml fr">train slowly</strong> due to limited parallelisation, especially when sequences get longer.</li><li id="8332" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne pw px py bk"><strong class="ml fr">CNNs</strong>: Process data in a more parallel fashion, but still technically operates sequentially. They are <strong class="ml fr">effective in capturing local patterns</strong> but <strong class="ml fr">struggle with long-term dependencies</strong> due to the way in which convolution works. The number of operations to capture relationships between two input positions <strong class="ml fr">increases with distance</strong> between the positions.</li></ul><p id="495e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Hence, introducing the <strong class="ml fr">Transformer</strong>, which relies <strong class="ml fr">entirely on the attention mechanism</strong> and does away with the recurrence and convolutions. Attention is what the model uses to focus on different parts of the input sequence at each step of generating an output. The Transformer was the first model to use attention without sequential processing, <strong class="ml fr">allowing for parallelisation</strong> and hence <strong class="ml fr">faster training without losing long-term dependencies</strong>. It also performs a <strong class="ml fr">constant number of operations</strong> between input positions, regardless of how far apart they are.</p><h1 id="66b1" class="pc od fq bf oe pd pe gq oi pf pg gt om ph pi pj pk pl pm pn po pp pq pr ps pt bk"><strong class="al">Walking through the Transformer model architecture</strong></h1><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qf"><img src="../Images/05d6f25e062e4c69745db402de2c99cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u3XcQI3nszKKCpfrtU1Xrg.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Transformer architecture. Image source: <a class="af pv" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention is All You Need</a></figcaption></figure><p id="c88d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The important features of the transformer are: <strong class="ml fr">tokenisation</strong>, the <strong class="ml fr">embedding layer, </strong>the <strong class="ml fr">attention mechanism, </strong>the<strong class="ml fr"> encoder </strong>and the<strong class="ml fr"> decoder. </strong>Let’s imagine an input sequence in french: “<em class="ni">Je suis etudiant”</em><strong class="ml fr"><em class="ni"> </em></strong>and a target output sequence in English “<em class="ni">I am a student”</em> (I am blatantly copying from this <a class="af pv" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">link</a>, which explains the process very descriptively)</p><h2 id="1421" class="oc od fq bf oe of og oh oi oj ok ol om ms on oo op mw oq or os na ot ou ov ow bk"><strong class="al">Tokenisation</strong></h2><p id="90e3" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">The input sequence of words is converted into tokens of 3–4 characters long</p><h2 id="6a75" class="oc od fq bf oe of og oh oi oj ok ol om ms on oo op mw oq or os na ot ou ov ow bk"><strong class="al">Embeddings</strong></h2><p id="9941" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">The input and output sequence are mapped to a sequence of continuous representations, <strong class="ml fr">z</strong>, which represents the <strong class="ml fr">input and output embeddings.</strong> Each token will be represented by an embedding to capture some kind of <strong class="ml fr">meaning</strong>, which helps in <strong class="ml fr">computing its relationship</strong> to other tokens; this embedding will be represented as a vector. To create these embeddings, we use the <strong class="ml fr">vocabulary</strong> of the training dataset, which contains every unique output token that is being used to train the model. We then determine an appropriate embedding dimension, which corresponds to the size of the vector representation for each token; higher embedding dimensions will better capture more complex / diverse / intricate meanings and relationships. The dimensions of the embedding matrix, for vocabulary size V and embedding dimension D, hence becomes V x D, making it a <strong class="ml fr">high-dimensional vector</strong>.</p><p id="de61" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">At initialisation, these embeddings can be initialised randomly and more accurate embeddings are <strong class="ml fr">learned during the training process</strong>. The embedding matrix is then updated during training.</p><p id="ab1c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Positional encodings</strong> are added to these embeddings because the transformer does not have a built-in sense of the order of tokens.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div class="nj nk qg"><img src="../Images/4a8f5359678476367201129a60f79a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*oQPfFrzu2E590NrFkELbSA.png"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Computing attention scores for the token “it”. As you can see, the model is paying large attention to the tokens “The” and “Animal”. Image source: <a class="af pv" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">Jay Alammar</a></figcaption></figure><h2 id="505a" class="oc od fq bf oe of og oh oi oj ok ol om ms on oo op mw oq or os na ot ou ov ow bk"><strong class="al">Attention mechanism</strong></h2><p id="cd13" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">Self-attention is the mechanism where each token in a sequence <strong class="ml fr">computes attention scores with every other token</strong> in a sequence to <strong class="ml fr">understand relationships</strong> between all tokens regardless of distance from each other. I’m going to avoid too much math in this article, but you can read up <a class="af pv" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">here</a> about the different matrices formed to compute attention scores and hence capture relationships between each token and every other token.</p><p id="483b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">These attention scores result in a <strong class="ml fr">new set of representations⁴</strong> for each token which is then used in the next layer of processing. During training, the <strong class="ml fr">weight matrices are updated through back-propagation</strong>, so the model can better account for relationships between tokens.</p><p id="45de" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Multi-head attention is just an extension of self-attention. Different attention scores are computed, the results are concatenated and transformed and the resulting representation enhances the model’s ability to <strong class="ml fr">capture various complex relationships between tokens</strong>.</p><h2 id="9e93" class="oc od fq bf oe of og oh oi oj ok ol om ms on oo op mw oq or os na ot ou ov ow bk"><strong class="al">Encoder</strong></h2><p id="4c0d" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">Input embeddings (built from the input sequence) with positional encodings are fed into the encoder. The encoder is 6 layers, with each layer containing 2 sub-layers: <strong class="ml fr">multi-head attention</strong> and <strong class="ml fr">feed forward networks</strong>. There is also a residual connection which leads to the output of each layer being LayerNorm(x+Sublayer(x)) as shown. The output of the encoder is a sequence of vectors which are <strong class="ml fr">contextualised representations</strong> of the inputs after accounting for attention scores. These are then fed to the decoder.</p><h2 id="e331" class="oc od fq bf oe of og oh oi oj ok ol om ms on oo op mw oq or os na ot ou ov ow bk"><strong class="al">Decoder</strong></h2><p id="209c" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">Output embeddings (generated from the target output sequence) with positional encodings are fed into the decoder. The decoder also contains 6 layers, and there are two differences from the encoder.</p><p id="3727" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First, the output embeddings go through <strong class="ml fr">masked</strong> <strong class="ml fr">multi-head attention</strong>, which means that the embeddings from subsequent positions in the sequence are ignored when computing the attention scores. This is because when we generate the current token (in position i), we should <strong class="ml fr">ignore all output tokens at positions after i</strong>. Moreover, the output embeddings are offset to the right by one position, so that the predicted token at position i only depends on outputs at positions less than it.</p><p id="1230" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For example, let’s say the input was “<em class="ni">je suis étudiant à l’école” </em>and target output is “<em class="ni">i am a student in school”. </em>When predicting the token for <em class="ni">student</em>, the encoder takes embeddings for <em class="ni">“je suis etudiant” </em>while the decoder conceals the tokens after “a” so that the prediction of <em class="ni">student</em> only considers the previous tokens in the sentence, namely “I am a”. This trains the model to predict tokens sequentially. Of course, the tokens “<em class="ni">in school”</em> provide added context for the model’s prediction, but we are training the model to capture this context from the <strong class="ml fr">input token,“</strong><em class="ni">etudiant</em><strong class="ml fr">” </strong>and<strong class="ml fr"> subsequent input tokens</strong>, “<em class="ni">à l’école”.</em></p><p id="cbfe" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">How is the decoder getting this context? Well that brings us to the second difference: The second multi-head attention layer in the decoder takes in the <strong class="ml fr">contextualised representations of the inputs before being passed into the feed-forward network</strong>, to ensure that the output representations capture the full context of the input tokens and prior outputs. This gives us a sequence of vectors corresponding to each target token, which are <strong class="ml fr">contextualised</strong> <strong class="ml fr">target representations.</strong></p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qh"><img src="../Images/159b57da25747e495fea64d731ba15e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Uz-Dshd0Nxw3zPbmq5Y1w.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Image source: <a class="af pv" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">Jay Alammar</a></figcaption></figure><h2 id="7438" class="oc od fq bf oe of og oh oi oj ok ol om ms on oo op mw oq or os na ot ou ov ow bk"><strong class="al">The prediction using the Linear and Softmax layers</strong></h2><p id="bf09" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">Now, we want to use those contextualised target representations to figure out what the next token is. Using the contextualised target representations from the decoder, the linear layer projects the sequence of vectors into a much larger <strong class="ml fr">logits vector</strong> which is the same length as our model’s vocabulary, let’s say of length L. The linear layer contains a weight matrix which, when multiplied with the decoder outputs and added with a bias vector, produces a logits vector of size 1 x L. Each cell is the score of a unique token, and the softmax layer than normalises this vector so that the entire vector sums to one; each cell now <strong class="ml fr">represents the probabilities of each token</strong>. The highest probability token is chosen, and voila! we have our predicted token.</p><h2 id="7702" class="oc od fq bf oe of og oh oi oj ok ol om ms on oo op mw oq or os na ot ou ov ow bk"><strong class="al">Training the model</strong></h2><p id="5c74" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">Next, we compare the predicted token probabilities to the actual token probabilites (which will just be logits vector of 0 for every token except for the target token, which has probability 1.0). We calculate an appropriate <strong class="ml fr">loss function</strong> for each token prediction and average this loss over the entire target sequence. We then <strong class="ml fr">back-propagate </strong>this loss over all the model’s parameters to calculate appropriate gradients, and use an appropriate optimisation algorithm to <strong class="ml fr">update the model parameters</strong>. Hence, for the classic transformer architecture, this leads to updates of</p><ol class=""><li id="cce5" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qi px py bk">The embedding matrix</li><li id="6763" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne qi px py bk">The different matrices used to compute attention scores</li><li id="e096" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne qi px py bk">The matrices associated with the feed-forward neural networks</li><li id="8e71" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne qi px py bk">The linear matrix used to make the logits vector</li></ol><blockquote class="nf ng nh"><p id="ac3c" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Matrices in 2–4 are weight matrices, and there are additional bias terms associated with each output which are also updated during training.</p><p id="8bef" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Note: </strong>The linear matrix and embedding matrix are often transposes of each other. This is the case for the Attention is All You Need paper; the technique is called “weight-tying”. The number of parameters to train are thus reduced.</p></blockquote><p id="55a1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This represents <strong class="ml fr">one epoch</strong> of training. Training comprises multiple epochs, with the number depending on the size of the datasets, size of the models, and the model’s task.</p><h1 id="369a" class="pc od fq bf oe pd pe gq oi pf pg gt om ph pi pj pk pl pm pn po pp pq pr ps pt bk"><strong class="al">Going back to what makes Transformers so good</strong></h1><p id="d865" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">As we mentioned earlier, the problems with the RNNs, CNNs, LSTMs and more include the lack of parallel processing, their sequential architecture, and inadequate capturing of long-term dependencies. The transformer architecture above solves these problems as…</p><ol class=""><li id="e25d" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qi px py bk">The Attention mechanism allows<strong class="ml fr"> </strong>the <strong class="ml fr">entire sequence to be processed in parallel rather than sequentially</strong>. With self-attention, each token in the input sequence attends to every other token in the input sequence (of that mini batch, explained next). This captures all relationships at the same time, rather than in a sequential manner.</li><li id="1a40" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne qi px py bk">Mini-batching of input within each epoch allows <strong class="ml fr">parallel processing, faster training, </strong>and <strong class="ml fr">easier scalability of the model</strong>. In a large text full of examples, mini-batches represent a smaller collection of these examples. The examples in the dataset are shuffled before being put into mini-batches, and reshuffled at the beginning of each epoch. Each mini-batch is passed into the model at the same time.</li><li id="84d8" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne qi px py bk">By using positional encodings and batch processing, the order of tokens in a sequence is accounted for. Distances between tokens are also <strong class="ml fr">accounted for equally regardless of how far they are</strong>, and the mini-batch processing further ensures this.</li></ol><h2 id="1557" class="oc od fq bf oe of og oh oi oj ok ol om ms on oo op mw oq or os na ot ou ov ow bk">As shown in the paper, the results were fantastic.</h2><p id="7f8d" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">Welcome to the world of transformers.</p><h1 id="9926" class="pc od fq bf oe pd pe gq oi pf pg gt om ph pi pj pk pl pm pn po pp pq pr ps pt bk">A quick bit on GPT Architecture</h1><p id="e00d" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">The transformer architecture was introduced by the researcher Ashish Vaswani in 2017 while he was working at Google Brain. The Generative Pre-trained Transformer (GPT) was introduced by OpenAI in 2018. The primary difference is that GPT’s do not contain an encoder stack in their architecture. The encoder-decoder makeup is useful when were directly converting one sequence into another sequence. The GPT was designed to focus on generative capabilities, and it did away with the encoder while keeping the rest of the components similar.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qj"><img src="../Images/7207b4ada7af6ca14f8ad89dafe24fbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EXwsU2VI5-st0iTTQiloqA.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Image source: <a class="af pv" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">Improving Language Understanding by Generative Pre-Training</a></figcaption></figure><p id="dc4d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The GPT model is pre-trained on a large corpus of text, unsupervised, to learn relationships between all words and tokens⁵. After fine-tuning for various use cases (such as a general purpose chatbot), they have proven to be extremely effective in generative tasks.</p><h2 id="e4fc" class="oc od fq bf oe of og oh oi oj ok ol om ms on oo op mw oq or os na ot ou ov ow bk">Example</h2><p id="1cf6" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">When you ask it a question, the steps for prediction are largely the same as a regular transformer. If you ask it the question “How does GPT predict responses”, these words are tokenised, embeddings generated, attention scores computed, probabilities of the next word are calculated, and a token is chosen to be the next predicted token. For example, the model might generate the response step by step, starting with “GPT predicts responses by…” and continuing based on probabilities until it forms a complete, coherent response. (<em class="ni">guess what, that last sentence was from chatGPT)</em>.</p></div></div></div><div class="ab cb qk ql qm qn" role="separator"><span class="qo by bm qp qq qr"/><span class="qo by bm qp qq qr"/><span class="qo by bm qp qq"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="073d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I hope all this was easy enough to understand. If it wasn’t, then maybe it’s somebody else’s turn to have a go at explaining transformers.</p><p id="a7e0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Feel free to share your thoughts and connect with me if this article was interesting to you!</p><ul class=""><li id="9af1" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pw px py bk">LinkedIn: <a class="af pv" href="https://www.linkedin.com/in/aveekg00/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/aveekg00/</a></li><li id="032f" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne pw px py bk">Website: <a class="af pv" href="http://aveek.info" rel="noopener ugc nofollow" target="_blank">aveek.info</a></li></ul><h2 id="0f66" class="oc od fq bf oe of og oh oi oj ok ol om ms on oo op mw oq or os na ot ou ov ow bk">References:</h2><ol class=""><li id="145d" class="mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne qi px py bk"><a class="af pv" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1706.03762</a></li><li id="46d7" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne qi px py bk"><a class="af pv" href="https://deeplearningmath.org/sequence-models" rel="noopener ugc nofollow" target="_blank">https://deeplearningmath.org/sequence-models</a></li><li id="7513" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne qi px py bk"><a class="af pv" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li><li id="f07c" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne qi px py bk"><a class="af pv" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-transformer/</a></li><li id="2dab" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne qi px py bk"><a class="af pv" href="https://openai.com/index/language-unsupervised/" rel="noopener ugc nofollow" target="_blank">https://openai.com/index/language-unsupervised/</a></li></ol><h2 id="fcec" class="oc od fq bf oe of og oh oi oj ok ol om ms on oo op mw oq or os na ot ou ov ow bk">Other great articles to refer to:</h2><ul class=""><li id="ae06" class="mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne pw px py bk"><a class="af pv" href="https://lilianweng.github.io/posts/2018-06-24-attention/" rel="noopener ugc nofollow" target="_blank">https://lilianweng.github.io/posts/2018-06-24-attention/</a></li><li id="7911" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne pw px py bk"><a class="af pv" href="https://bastings.github.io/annotated_encoder_decoder/" rel="noopener ugc nofollow" target="_blank">https://bastings.github.io/annotated_encoder_decoder/</a></li><li id="062a" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne pw px py bk"><a class="af pv" href="https://nlp.seas.harvard.edu/annotated-transformer/#prelims" rel="noopener ugc nofollow" target="_blank">https://nlp.seas.harvard.edu/annotated-transformer/#prelims</a></li></ul></div></div></div></div>    
</body>
</html>