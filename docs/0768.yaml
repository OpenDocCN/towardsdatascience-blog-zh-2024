- en: How to Interpret GPT2-Small
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何解释GPT2-Small
- en: 原文：[https://towardsdatascience.com/how-to-interpret-gpt2-small-76e0536a588a?source=collection_archive---------8-----------------------#2024-03-22](https://towardsdatascience.com/how-to-interpret-gpt2-small-76e0536a588a?source=collection_archive---------8-----------------------#2024-03-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-interpret-gpt2-small-76e0536a588a?source=collection_archive---------8-----------------------#2024-03-22](https://towardsdatascience.com/how-to-interpret-gpt2-small-76e0536a588a?source=collection_archive---------8-----------------------#2024-03-22)
- en: Mechanistic Interpretability on prediction of repeated tokens
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测重复令牌的机制可解释性
- en: '[](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--76e0536a588a--------------------------------)[![Shuyang
    Xiang](../Images/36a5fd18fd9b7b88cb41094f09b83882.png)](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--76e0536a588a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--76e0536a588a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--76e0536a588a--------------------------------)
    [Shuyang Xiang](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--76e0536a588a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--76e0536a588a--------------------------------)[![Shuyang
    Xiang](../Images/36a5fd18fd9b7b88cb41094f09b83882.png)](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--76e0536a588a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--76e0536a588a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--76e0536a588a--------------------------------)
    [Shuyang Xiang](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--76e0536a588a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--76e0536a588a--------------------------------)
    ·7 min read·Mar 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--76e0536a588a--------------------------------)
    ·阅读时长7分钟·2024年3月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The development of large-scale language models, especially ChatGPT, has left
    those who have experimented with it, myself included, astonished by its remarkable
    linguistic prowess and its ability to accomplish diverse tasks. However, many
    researchers, including myself, while marveling at its capabilities, also find
    themselves perplexed. Despite knowing the model’s architecture and the specific
    values of its weights, we still struggle to comprehend why a particular sequence
    of inputs leads to a specific sequence of outputs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模语言模型的发展，特别是ChatGPT，令那些曾经试验过它的人，包括我自己，感到惊讶于它卓越的语言能力和执行各种任务的能力。然而，许多研究人员，包括我自己，虽然对它的能力感到惊叹，但也发现自己感到困惑。尽管我们了解模型的架构以及权重的具体数值，但我们仍然难以理解为何某一特定输入序列会导致特定的输出序列。
- en: 'In this blog post, I will attempt to demystify GPT2-small using mechanistic
    interpretability on a simple case: the prediction of repeated tokens.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我将尝试通过机制可解释性来揭开GPT2-small的神秘面纱，研究一个简单的案例：重复令牌的预测。
- en: Mechanistic Interpretability
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机制可解释性
- en: Traditional mathematical tools for explaining machine learning models aren’t
    entirely suitable for language models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的数学工具在解释机器学习模型时，并不完全适用于语言模型。
- en: Consider SHAP, a helpful tool for explaining machine learning models. It’s proficient
    at determining which feature significantly influenced the prediction of a good
    quality wine. However, it’s important to remember that language models make predictions
    at the token level, while SHAP values are mostly computed at the feature level,
    making them potentially unfit for tokens.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以SHAP为例，它是一个有助于解释机器学习模型的工具。它擅长确定哪些特征显著影响了优质葡萄酒的预测。然而，重要的是要记住，语言模型是在令牌级别进行预测的，而SHAP值通常是在特征级别计算的，这使得它们可能不适用于令牌。
- en: Moreover, Language Models (LLMs) have numerous parameters and inputs, creating
    a high-dimensional space. Computing SHAP values is costly even in low-dimensional
    spaces, and even more so in the high-dimensional space of LLMs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，语言模型（LLM）有大量的参数和输入，形成了一个高维空间。即便在低维空间中，计算SHAP值也非常昂贵，而在LLM的高维空间中，这一成本则更为庞大。
- en: Despite tolerating the high computational costs, the explanations provided by
    SHAP can be superficial. For instance, knowing that the term “potter” most influenced
    the output prediction due to the earlier mention of “Harry” doesn’t provide much
    insight. It leaves us uncertain about the part of the model or the specific mechanism
    responsible for such a prediction.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管容忍了高昂的计算成本，SHAP提供的解释可能显得肤浅。例如，知道“potter”这个词由于之前提到“Harry”而最影响输出预测，但这并没有提供太多的洞见。这让我们无法确定模型的哪一部分或具体机制对这种预测负责。
- en: Mechanistic Interpretability offers a different approach. It doesn’t just identify
    important features or inputs for a model’s predictions. Instead, it sheds light
    on the underlying mechanisms or reasoning processes, helping us understand how
    a model makes its predictions or decisions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机制可解释性提供了一种不同的方法。它不仅仅识别模型预测的重要特征或输入。相反，它揭示了模型的底层机制或推理过程，帮助我们理解模型是如何做出预测或决策的。
- en: Prediction of repeated tokens by GPT2-Small
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT2-Small对重复标记的预测
- en: 'We will be using GPT2-small for a simple task: predicting a sequence of repeated
    tokens. The library we will use is [TransformerLens](https://neelnanda-io.github.io/TransformerLens/index.html),
    which is designed for [mechanistic interpretability](https://distill.pub/2020/circuits/zoom-in/)
    of GPT-2 style language models.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用GPT2-small进行一个简单的任务：预测一系列重复的标记。我们将使用的库是[TransformerLens](https://neelnanda-io.github.io/TransformerLens/index.html)，该库旨在提供GPT-2风格语言模型的[机制可解释性](https://distill.pub/2020/circuits/zoom-in/)。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We use the code above to load the GPT2-Small model and predict tokens on a sequence
    generated by a specific function. This sequence includes two identical token sequences,
    followed by the bos_token. An example would be “ABCDABCD” + bos_token when the
    seq_len is 3\. For clarity, we refer to the sequence from the beginning to the
    seq_len as the first half, and the remaining sequence, excluding the bos_token,
    as the second half.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用上面的代码加载GPT2-Small模型，并对由特定函数生成的序列进行标记预测。该序列包含两个相同的标记序列，后面跟着bos_token。例如，当seq_len为3时，序列为“ABCDABCD”
    + bos_token。为清晰起见，我们将从序列开始到seq_len的部分称为前半部分，将剩余部分（不包括bos_token）称为后半部分。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When we allow the model to run on the generated token, we find an interesting
    observation: the model performs significantly better on the second half of the
    sequence than on the first half. This is measured by the log probabilities on
    the correct tokens. To be precise, the performance on the first half is -13.898,
    while the performance on the second half is -0.644.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们允许模型在生成的标记上运行时，发现一个有趣的现象：模型在序列的后半部分表现明显优于前半部分。这是通过正确标记的对数概率来衡量的。具体来说，前半部分的性能为-13.898，而后半部分的性能为-0.644。
- en: '![](../Images/0aecbb799f225fbde4fee8e95c671884.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0aecbb799f225fbde4fee8e95c671884.png)'
- en: 'Image for author: Log probs on correct tokens'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片：正确标记的对数概率
- en: We can also calculate prediction accuracy, defined as the ratio of correctly
    predicted tokens (those identical to the generated tokens) to the total number
    of tokens. The accuracy for the first half sequence is 0.0, which is unsurprising
    since we’re working with random tokens that lack actual meaning. Meanwhile, the
    accuracy for the second half is 0.93, significantly outperforming the first half.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以计算预测准确率，定义为正确预测的标记（与生成的标记相同的标记）与总标记数的比率。前半部分序列的准确率为0.0，这不足为奇，因为我们使用的是没有实际意义的随机标记。与此同时，后半部分的准确率为0.93，明显优于前半部分。
- en: Induction Circuits
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归纳电路
- en: Finding induction head
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找归纳头
- en: The observation above might be explained by the existence of an induction circuit.
    This is a circuit that scans the sequence for prior instances of the current token,
    identifies the token that followed it previously, and predicts that the same sequence
    will repeat. For instance, if it encounters an ‘A’, it scans for the previous
    ‘A’ or a token very similar to ‘A’ in the embedding space, identifies the subsequent
    token ‘B’, and then predicts the next token after ‘A’ to be ‘B’ or a token very
    similar to ‘B’ in the embedding space.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 上述观察可能是由归纳电路的存在所解释的。归纳电路扫描序列中的当前标记的前序实例，识别出它之前跟随的标记，并预测相同的序列将会重复。例如，如果它遇到一个‘A’，它会扫描嵌入空间中与‘A’相似的前一个‘A’或标记，识别出随后的标记‘B’，然后预测‘A’之后的标记将是‘B’或与‘B’非常相似的标记。
- en: '![](../Images/ee98d87987179fb90146a1571297455d.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee98d87987179fb90146a1571297455d.png)'
- en: 'Image by author: Induction circuit'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片：归纳电路
- en: 'This prediction process can be broken down into two steps:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预测过程可以分为两个步骤：
- en: Identify the previous same (or similar) token. Every token in the second half
    of the sequence should “pay attention” to the token ‘seq_len’ places before it.
    For instance, the ‘A’ at position 4 should pay attention to the ‘A’ at position
    1 if ‘seq_len’ is 3\. We can call the attention head performing this task the
    “**induction head**.”
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别之前相同（或相似）的 token。序列后半部分的每个 token 应该“关注”前面 `seq_len` 位置的 token。例如，如果 `seq_len`
    为3，那么位置为4的 'A' 应该关注位置为1的 'A'。我们可以将执行此任务的注意力头称为“**引导头**”。
- en: Identify the following token ‘B’. This is the process of copying information
    from the previous token (e.g., ‘A’) into the next token (e.g., ‘B’). This information
    will be used to “reproduce” ‘B’ when ‘A’ appears again. We can call the attention
    head performing this task the “**previous token head**.”
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别下一个 token ‘B’。这是从前一个 token（例如，‘A’）复制信息到下一个 token（例如，‘B’）的过程。当 'A' 再次出现时，这些信息将用于“重现”‘B’。我们可以将执行此任务的注意力头称为“**前一个
    token 头**”。
- en: These two heads constitute a complete induction circuit. Note that sometimes
    the term “induction head” is also used to describe the entire “induction circuit.”
    For more introduction of induction circuit, I highly recommend the article [In-context
    learning and induction head](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)
    which is a master piece!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个头构成了一个完整的引导回路。请注意，有时“引导头”一词也用来描述整个“引导回路”。关于引导回路的更多介绍，我强烈推荐阅读这篇文章 [In-context
    learning and induction head](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)，这是一篇杰作！
- en: Now, let’s identify the attention head and previous head in GPT2-small.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在 GPT2-small 中识别注意力头和前一个头。
- en: The following code is used to find the induction head. First, we run the model
    with 30 batches. Then, we calculate the mean value of the diagonal with an offset
    of seq_len in the attention pattern matrix. This method lets us measure the degree
    of attention the current token gives to the one that appears seq_len beforehand.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于查找引导头。首先，我们使用30个批次运行模型。然后，我们计算带有`seq_len`偏移量的注意力模式矩阵对角线的平均值。此方法使我们能够衡量当前
    token 对 `seq_len` 之前出现的 token 的关注程度。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, let’s examine the induction scores. We’ll notice that some heads, such
    as the one on layer 5 and head 5, have a high induction score of 0.91.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查引导得分。我们会注意到一些头，如第5层第5头，具有高达0.91的引导得分。
- en: '![](../Images/8a8ae188ca0dd0f8b9fd56e99b55491c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a8ae188ca0dd0f8b9fd56e99b55491c.png)'
- en: 'Image by author: Induction head scores'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：引导头得分
- en: We can also display the attention pattern of this head. You will notice a clear
    diagonal line up to an offset of seq_len.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以显示该头的注意力模式。你会注意到有一条清晰的对角线，直到 `seq_len` 的偏移量。
- en: '![](../Images/cdf95b0714b8e8577791fd751fc7c31a.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdf95b0714b8e8577791fd751fc7c31a.png)'
- en: 'Image by author: layer 5, head 5 attention pattern'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：第5层，第5头的注意力模式
- en: Similarly, we can identify the preceding token head. For instance, layer 4 head
    11 demonstrates a strong pattern for the previous token.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以识别前一个 token 头。例如，第4层第11头展示了对前一个 token 的强烈模式。
- en: '![](../Images/59d5f94ef7520b604a4296d27da7c44e.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59d5f94ef7520b604a4296d27da7c44e.png)'
- en: 'Image by author: previous token head scores'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：前一个 token 头得分
- en: How do MLP layers attribute?
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLP 层如何归因？
- en: 'Let’s consider this question: do MLP layers count? We know that GPT2-Small
    contains both attention and MLP layers. To investigate this, I propose using an
    ablation technique.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑这个问题：MLP 层有影响吗？我们知道 GPT2-Small 包含了注意力和 MLP 层。为了解决这个问题，我提出使用消融技术。
- en: Ablation, as the name implies, systematically removes certain model components
    and observes how performance changes as a result.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 消融，顾名思义，是系统地去除模型中的某些组件，并观察性能变化的过程。
- en: We will replace the output of the MLP layers in the second half of the sequence
    with those from the first half, and observe how this affects the final loss function.
    We will compute the difference between the loss after replacing the MLP layer
    outputs and the original loss of the second half sequence using the following
    code.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用序列前半部分的输出替换序列后半部分的 MLP 层输出，并观察这如何影响最终的损失函数。我们将使用以下代码计算替换 MLP 层输出后的损失与原始后半序列损失之间的差异。
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We arrive at a surprising result: aside from the first token, the ablation
    does not produce a significant logit difference. This suggests that the MLP layers
    may not have a significant contribution in the case of repeated tokens.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得出了一个令人惊讶的结果：除了第一个标记外，消融实验并没有产生显著的logit差异。这表明，在重复标记的情况下，MLP层可能没有显著的贡献。
- en: '![](../Images/29f50de7de46432cb97d79853870184d.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29f50de7de46432cb97d79853870184d.png)'
- en: 'Image by author: loss different before and after ablation of mlp layers'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：消融前后MLP层的损失差异
- en: '**One induction circuit**'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**一个归纳电路**'
- en: 'Given that the MLP layers don’t significantly contribute to the final prediction,
    we can manually construct an induction circuit using the head of layer 5, head
    5, and the head of layer 4, head 11\. Recall that these are the induction head
    and the previous token head. We do it by the following code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于MLP层对最终预测没有显著贡献，我们可以手动构建一个归纳电路，使用第5层的头部5和第4层的头部11。回想一下，这些是归纳头和前一个标记头。我们通过以下代码来实现：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Computing the top 1 accuracy of this circuit yields a value of 0.2283\. This
    is quite good for a circuit constructed by only two heads!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 计算该电路的top 1准确率得到了0.2283的值。这对于一个仅由两个头部构建的电路来说相当不错！
- en: For detailed implementation, please check my [*notebook*](https://colab.research.google.com/drive/1_Qx67oPB2ZNeKa1ANYhFa9tkKGSZhX6a#scrollTo=ss1yF3e8PNYD).
    And many thanks to Neel Nanda who developed the wonderful [TransformerLen](https://neelnanda-io.github.io/TransformerLens/index.html)
    as a great tool for Mechanistic Interpretability of LLMs!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细的实现，请查看我的[*notebook*](https://colab.research.google.com/drive/1_Qx67oPB2ZNeKa1ANYhFa9tkKGSZhX6a#scrollTo=ss1yF3e8PNYD)。非常感谢Neel
    Nanda，他开发了这个精彩的[TransformerLen](https://neelnanda-io.github.io/TransformerLens/index.html)，这是一个用于大语言模型机制可解释性的伟大工具！
