- en: How to Interpret GPT2-Small
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-interpret-gpt2-small-76e0536a588a?source=collection_archive---------8-----------------------#2024-03-22](https://towardsdatascience.com/how-to-interpret-gpt2-small-76e0536a588a?source=collection_archive---------8-----------------------#2024-03-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mechanistic Interpretability on prediction of repeated tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--76e0536a588a--------------------------------)[![Shuyang
    Xiang](../Images/36a5fd18fd9b7b88cb41094f09b83882.png)](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--76e0536a588a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--76e0536a588a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--76e0536a588a--------------------------------)
    [Shuyang Xiang](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--76e0536a588a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--76e0536a588a--------------------------------)
    ·7 min read·Mar 22, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: The development of large-scale language models, especially ChatGPT, has left
    those who have experimented with it, myself included, astonished by its remarkable
    linguistic prowess and its ability to accomplish diverse tasks. However, many
    researchers, including myself, while marveling at its capabilities, also find
    themselves perplexed. Despite knowing the model’s architecture and the specific
    values of its weights, we still struggle to comprehend why a particular sequence
    of inputs leads to a specific sequence of outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this blog post, I will attempt to demystify GPT2-small using mechanistic
    interpretability on a simple case: the prediction of repeated tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: Mechanistic Interpretability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional mathematical tools for explaining machine learning models aren’t
    entirely suitable for language models.
  prefs: []
  type: TYPE_NORMAL
- en: Consider SHAP, a helpful tool for explaining machine learning models. It’s proficient
    at determining which feature significantly influenced the prediction of a good
    quality wine. However, it’s important to remember that language models make predictions
    at the token level, while SHAP values are mostly computed at the feature level,
    making them potentially unfit for tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, Language Models (LLMs) have numerous parameters and inputs, creating
    a high-dimensional space. Computing SHAP values is costly even in low-dimensional
    spaces, and even more so in the high-dimensional space of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Despite tolerating the high computational costs, the explanations provided by
    SHAP can be superficial. For instance, knowing that the term “potter” most influenced
    the output prediction due to the earlier mention of “Harry” doesn’t provide much
    insight. It leaves us uncertain about the part of the model or the specific mechanism
    responsible for such a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Mechanistic Interpretability offers a different approach. It doesn’t just identify
    important features or inputs for a model’s predictions. Instead, it sheds light
    on the underlying mechanisms or reasoning processes, helping us understand how
    a model makes its predictions or decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction of repeated tokens by GPT2-Small
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using GPT2-small for a simple task: predicting a sequence of repeated
    tokens. The library we will use is [TransformerLens](https://neelnanda-io.github.io/TransformerLens/index.html),
    which is designed for [mechanistic interpretability](https://distill.pub/2020/circuits/zoom-in/)
    of GPT-2 style language models.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We use the code above to load the GPT2-Small model and predict tokens on a sequence
    generated by a specific function. This sequence includes two identical token sequences,
    followed by the bos_token. An example would be “ABCDABCD” + bos_token when the
    seq_len is 3\. For clarity, we refer to the sequence from the beginning to the
    seq_len as the first half, and the remaining sequence, excluding the bos_token,
    as the second half.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When we allow the model to run on the generated token, we find an interesting
    observation: the model performs significantly better on the second half of the
    sequence than on the first half. This is measured by the log probabilities on
    the correct tokens. To be precise, the performance on the first half is -13.898,
    while the performance on the second half is -0.644.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0aecbb799f225fbde4fee8e95c671884.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image for author: Log probs on correct tokens'
  prefs: []
  type: TYPE_NORMAL
- en: We can also calculate prediction accuracy, defined as the ratio of correctly
    predicted tokens (those identical to the generated tokens) to the total number
    of tokens. The accuracy for the first half sequence is 0.0, which is unsurprising
    since we’re working with random tokens that lack actual meaning. Meanwhile, the
    accuracy for the second half is 0.93, significantly outperforming the first half.
  prefs: []
  type: TYPE_NORMAL
- en: Induction Circuits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finding induction head
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The observation above might be explained by the existence of an induction circuit.
    This is a circuit that scans the sequence for prior instances of the current token,
    identifies the token that followed it previously, and predicts that the same sequence
    will repeat. For instance, if it encounters an ‘A’, it scans for the previous
    ‘A’ or a token very similar to ‘A’ in the embedding space, identifies the subsequent
    token ‘B’, and then predicts the next token after ‘A’ to be ‘B’ or a token very
    similar to ‘B’ in the embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee98d87987179fb90146a1571297455d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by author: Induction circuit'
  prefs: []
  type: TYPE_NORMAL
- en: 'This prediction process can be broken down into two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the previous same (or similar) token. Every token in the second half
    of the sequence should “pay attention” to the token ‘seq_len’ places before it.
    For instance, the ‘A’ at position 4 should pay attention to the ‘A’ at position
    1 if ‘seq_len’ is 3\. We can call the attention head performing this task the
    “**induction head**.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify the following token ‘B’. This is the process of copying information
    from the previous token (e.g., ‘A’) into the next token (e.g., ‘B’). This information
    will be used to “reproduce” ‘B’ when ‘A’ appears again. We can call the attention
    head performing this task the “**previous token head**.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These two heads constitute a complete induction circuit. Note that sometimes
    the term “induction head” is also used to describe the entire “induction circuit.”
    For more introduction of induction circuit, I highly recommend the article [In-context
    learning and induction head](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)
    which is a master piece!
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s identify the attention head and previous head in GPT2-small.
  prefs: []
  type: TYPE_NORMAL
- en: The following code is used to find the induction head. First, we run the model
    with 30 batches. Then, we calculate the mean value of the diagonal with an offset
    of seq_len in the attention pattern matrix. This method lets us measure the degree
    of attention the current token gives to the one that appears seq_len beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s examine the induction scores. We’ll notice that some heads, such
    as the one on layer 5 and head 5, have a high induction score of 0.91.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a8ae188ca0dd0f8b9fd56e99b55491c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by author: Induction head scores'
  prefs: []
  type: TYPE_NORMAL
- en: We can also display the attention pattern of this head. You will notice a clear
    diagonal line up to an offset of seq_len.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cdf95b0714b8e8577791fd751fc7c31a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by author: layer 5, head 5 attention pattern'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we can identify the preceding token head. For instance, layer 4 head
    11 demonstrates a strong pattern for the previous token.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59d5f94ef7520b604a4296d27da7c44e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by author: previous token head scores'
  prefs: []
  type: TYPE_NORMAL
- en: How do MLP layers attribute?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s consider this question: do MLP layers count? We know that GPT2-Small
    contains both attention and MLP layers. To investigate this, I propose using an
    ablation technique.'
  prefs: []
  type: TYPE_NORMAL
- en: Ablation, as the name implies, systematically removes certain model components
    and observes how performance changes as a result.
  prefs: []
  type: TYPE_NORMAL
- en: We will replace the output of the MLP layers in the second half of the sequence
    with those from the first half, and observe how this affects the final loss function.
    We will compute the difference between the loss after replacing the MLP layer
    outputs and the original loss of the second half sequence using the following
    code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We arrive at a surprising result: aside from the first token, the ablation
    does not produce a significant logit difference. This suggests that the MLP layers
    may not have a significant contribution in the case of repeated tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29f50de7de46432cb97d79853870184d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by author: loss different before and after ablation of mlp layers'
  prefs: []
  type: TYPE_NORMAL
- en: '**One induction circuit**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given that the MLP layers don’t significantly contribute to the final prediction,
    we can manually construct an induction circuit using the head of layer 5, head
    5, and the head of layer 4, head 11\. Recall that these are the induction head
    and the previous token head. We do it by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Computing the top 1 accuracy of this circuit yields a value of 0.2283\. This
    is quite good for a circuit constructed by only two heads!
  prefs: []
  type: TYPE_NORMAL
- en: For detailed implementation, please check my [*notebook*](https://colab.research.google.com/drive/1_Qx67oPB2ZNeKa1ANYhFa9tkKGSZhX6a#scrollTo=ss1yF3e8PNYD).
    And many thanks to Neel Nanda who developed the wonderful [TransformerLen](https://neelnanda-io.github.io/TransformerLens/index.html)
    as a great tool for Mechanistic Interpretability of LLMs!
  prefs: []
  type: TYPE_NORMAL
