- en: Optimizing Transformer Models for Variable-Length Input Sequences
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对变长输入序列优化Transformer模型
- en: 原文：[https://towardsdatascience.com/optimizing-transformer-models-for-variable-length-input-sequences-19fb88fddf71?source=collection_archive---------3-----------------------#2024-11-26](https://towardsdatascience.com/optimizing-transformer-models-for-variable-length-input-sequences-19fb88fddf71?source=collection_archive---------3-----------------------#2024-11-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/optimizing-transformer-models-for-variable-length-input-sequences-19fb88fddf71?source=collection_archive---------3-----------------------#2024-11-26](https://towardsdatascience.com/optimizing-transformer-models-for-variable-length-input-sequences-19fb88fddf71?source=collection_archive---------3-----------------------#2024-11-26)
- en: How PyTorch NestedTensors, FlashAttention2, and xFormers can Boost Performance
    and Reduce AI Costs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch NestedTensors、FlashAttention2 和 xFormers 如何提升性能并降低AI成本
- en: '[](https://chaimrand.medium.com/?source=post_page---byline--19fb88fddf71--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--19fb88fddf71--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--19fb88fddf71--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--19fb88fddf71--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--19fb88fddf71--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page---byline--19fb88fddf71--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--19fb88fddf71--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--19fb88fddf71--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--19fb88fddf71--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--19fb88fddf71--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--19fb88fddf71--------------------------------)
    ·14 min read·Nov 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--19fb88fddf71--------------------------------)
    ·阅读时间：14分钟·2024年11月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0a9af14b8f889cbba8fa1fe74fb680aa.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a9af14b8f889cbba8fa1fe74fb680aa.png)'
- en: Photo by [Tanja Zöllner](https://unsplash.com/@tanjazoellner?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Tanja Zöllner](https://unsplash.com/@tanjazoellner?utm_source=medium&utm_medium=referral)
    于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: As generative AI (genAI) models grow in both popularity and scale, so do the
    computational demands and costs associated with their training and deployment.
    Optimizing these models is crucial for enhancing their runtime performance and
    reducing their operational expenses. At the heart of modern genAI systems is the
    Transformer architecture and its attention mechanism, which is notably compute-intensive.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着生成型AI（genAI）模型的普及和规模不断扩大，与其训练和部署相关的计算需求和成本也在增加。优化这些模型对于提升其运行时性能和降低运营成本至关重要。现代生成型AI系统的核心是Transformer架构及其注意力机制，而该机制通常计算密集型。
- en: In a [previous post](/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6),
    we demonstrated how using optimized attention kernels can significantly accelerate
    the performance of Transformer models. In this post, we continue our exploration
    by addressing the challenge of variable-length input sequences — an inherent property
    of real-world data, including documents, code, time-series, and more.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在[上一篇文章](/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6)中，我们展示了如何通过优化注意力核显著加速Transformer模型的性能。在本文中，我们继续探讨如何解决变长输入序列的挑战——这是现实世界数据的固有特性，包括文档、代码、时间序列等。
- en: The Challenge of Batching Variable-Length Input
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理变长输入的挑战
- en: In a typical deep learning workload, individual samples are grouped into batches
    before being copied to the GPU and fed to the AI model. Batching improves computational
    efficiency and often aids model convergence during training. Usually, batching
    involves [stacking](https://pytorch.org/docs/stable/generated/torch.stack.html)
    all of the sample tensors along a new dimension — the *batch* dimension. However,
    [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html) requires
    all tensors to have the same shape, which is not the case with variable-length
    sequences.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的深度学习工作负载中，单个样本会在被复制到GPU并馈送给AI模型之前，被分成多个批次。批处理可以提高计算效率，并且通常有助于模型在训练中的收敛。通常，批处理涉及沿着一个新的维度—*批次*维度—将所有样本张量进行[堆叠](https://pytorch.org/docs/stable/generated/torch.stack.html)。然而，[torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)要求所有张量具有相同的形状，而这对于可变长度的序列并不适用。
- en: Padding and its Inefficiencies
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充及其低效性
- en: The traditional way to address this challenge is to pad the input sequences
    to a fixed length and then perform [stacking](https://pytorch.org/docs/stable/generated/torch.stack.html).
    This solution requires appropriate masking within the model so that the output
    is not affected by the irrelevant tensor elements. In the case of attention layers,
    a padding mask indicates which tokens are padding and should not be attended to
    (e.g., see [PyTorch MultiheadAttention](https://github.com/pytorch/pytorch/blob/v2.5.1/torch/nn/modules/activation.py#L1139)).
    However, padding can waste considerable GPU resources, increasing costs and slowing
    development. This is especially true for large-scale AI models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个挑战的传统方法是将输入序列填充到一个固定长度，然后执行[堆叠](https://pytorch.org/docs/stable/generated/torch.stack.html)。这个解决方案需要在模型内部进行适当的掩码处理，以确保输出不受无关张量元素的影响。在注意力层的情况下，填充掩码表示哪些标记是填充的，不应该被关注（例如，见[PyTorch
    MultiheadAttention](https://github.com/pytorch/pytorch/blob/v2.5.1/torch/nn/modules/activation.py#L1139)）。然而，填充可能会浪费大量的GPU资源，增加成本并减慢开发进程。对于大规模AI模型来说，这一点尤其如此。
- en: Don’t Pad, Concatenate
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不填充，拼接
- en: One way to avoid padding is to [concatenate](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat)
    sequences along an existing dimension instead of [stacking](https://pytorch.org/docs/stable/generated/torch.stack.html)
    them along a new dimension. Contrary to [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html),
    [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat)
    allows inputs of different shapes. The output of concatenation is a single sequence
    whose length equals the sum of the lengths of the individual sequences. For this
    solution to work, our single sequence would need to be supplemented by an attention
    mask that would ensure that each token attends only to other tokens in the same
    original sequence, in a process sometimes referred to as [document masking](https://pytorch.org/blog/flexattention/#document-maskingjagged-sequences).
    Denoting the sum of the lengths of all of the individual sequences by *N* and
    adopting [”big O” notation](https://en.wikipedia.org/wiki/Big_O_notation), the
    size of this mask would need to be *O(N²)*, as would the compute complexity of
    a naive attention layer (which applies the mask only after calculating the attention
    scores), making this solution highly inefficient.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 避免填充的一种方法是将序列沿着现有的维度进行[拼接](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat)，而不是将它们沿着新维度进行[堆叠](https://pytorch.org/docs/stable/generated/torch.stack.html)。与[torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)不同，[torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat)允许形状不同的输入。拼接的输出是一个单一的序列，其长度等于所有单个序列的长度之和。为了使这个解决方案有效，我们的单一序列需要通过一个注意力掩码进行补充，以确保每个标记只关注同一原始序列中的其他标记，这个过程有时被称为[文档掩码](https://pytorch.org/blog/flexattention/#document-maskingjagged-sequences)。设所有单个序列的长度之和为*N*，并采用[“大O”符号](https://en.wikipedia.org/wiki/Big_O_notation)，该掩码的大小需要是*O(N²)*，就像一个朴素的注意力层的计算复杂度一样（该层只在计算完注意力分数后才应用掩码），使得这个解决方案极其低效。
- en: Attention Layer Optimization
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力层优化
- en: 'The solution to this problem comes in the form of specialized attention layers.
    Contrary to the standard attention layer that performs the full set of *O(N²)
    attention scores* only to mask out the irrelevant ones, these optimized attention
    kernels are designed to calculate only the *scores* that matter. In this post
    we will explore several solutions, each with their own distinct characteristics.
    These include:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 解决此问题的方案是专门化的注意力层。与标准的注意力层需要执行完整的 *O(N²) 注意力分数*，然后屏蔽掉无关的部分不同，这些优化过的注意力内核只计算
    *重要的分数*。在本文中，我们将探讨几种不同的解决方案，每种都有其独特的特点。这些方案包括：
- en: '[PyTorch''s SDPA (Scaled Dot Product Attention) with NestedTensors](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#nestedtensor-and-dense-tensor-support),'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 的 SDPA（缩放点积注意力）与 NestedTensors](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#nestedtensor-and-dense-tensor-support)，'
- en: '[FlashAttention2](https://github.com/Dao-AILab/flash-attention), and'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FlashAttention2](https://github.com/Dao-AILab/flash-attention)，以及'
- en: '[xFormers'' memory-efficient attention](https://facebookresearch.github.io/xformers/components/ops.html).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[xFormers 的内存高效注意力](https://facebookresearch.github.io/xformers/components/ops.html)。'
- en: Integration into Existing HuggingFace Models
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成到现有的 HuggingFace 模型中
- en: For teams working with pre-trained models, transitioning to these optimizations
    might seem challenging. We will demonstrate how [HuggingFace’s](https://huggingface.co/)
    APIs simplify this process, enabling developers to integrate these techniques
    with minimal code changes and effort.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用预训练模型的团队来说，转向这些优化可能看起来具有挑战性。我们将展示 [HuggingFace](https://huggingface.co/)
    的 API 如何简化这一过程，使开发者能够以最小的代码修改和努力将这些技术集成进来。
- en: '**Disclaimers**'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**免责声明**'
- en: Please do not interpret our use of any platforms, libraries, or optimization
    techniques as an endorsement for their use. The best options for you will depend
    greatly on the specifics of your own use-case.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请不要将我们使用任何平台、库或优化技术的做法解读为对其使用的推荐。适合您的最佳选择将很大程度上依赖于您具体的应用场景。
- en: Some of the APIs discussed here are in prototype or beta stages and may change
    in the future.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此处讨论的部分 API 处于原型或测试阶段，未来可能会有所变化。
- en: The code examples provided are for demonstrative purposes only. We make no claims
    regarding their accuracy, optimality, or robustness.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供的代码示例仅用于演示目的。我们不对其准确性、最优性或稳健性做出任何声明。
- en: Special thanks to [Yitzhak Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/)
    and [Peleg Nahaliel](https://www.linkedin.com/in/peleg-nahaliel-b304a61a5/?originalSubdomain=il)
    for their contributions to this post.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢 [Yitzhak Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/) 和 [Peleg
    Nahaliel](https://www.linkedin.com/in/peleg-nahaliel-b304a61a5/?originalSubdomain=il)
    对本文的贡献。
- en: Toy LLM Model
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩具 LLM 模型
- en: To facilitate our discussion we will define a simple generative model (partially
    inspired by the [GPT](https://en.wikipedia.org/wiki/GPT) model defined [here](https://github.com/karpathy/nanoGPT/tree/master)).
    For a more comprehensive guide on building language models, please see one of
    the many excellent tutorials available online (e.g., [here](https://www.youtube.com/watch?v=kCc8FmEb1nY)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便讨论，我们将定义一个简单的生成模型（部分灵感来自于 [GPT](https://en.wikipedia.org/wiki/GPT) 模型，详见
    [这里](https://github.com/karpathy/nanoGPT/tree/master)）。有关构建语言模型的更全面指南，请参阅在线的诸多优秀教程之一（例如，[这里](https://www.youtube.com/watch?v=kCc8FmEb1nY)）。
- en: Transformer Block
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 模块
- en: We begin by constructing a basic Transformer block, specifically designed to
    facilitate experimentation with different attention mechanisms and optimizations.
    While our block performs the same computation as standard Transformer blocks,
    we make slight modifications to the usual choice of operators in order to support
    the possibility of PyTorch [NestedTensor](https://pytorch.org/docs/stable/nested.html#supported-operations)
    inputs (as described [here](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#causal-self-attention)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先构建一个基本的 Transformer 模块，特别设计用于便于实验不同的注意力机制和优化方法。尽管我们的模块执行与标准 Transformer
    模块相同的计算，但我们对常规操作符的选择做了些微调整，以支持 PyTorch [NestedTensor](https://pytorch.org/docs/stable/nested.html#supported-operations)
    输入（如 [这里](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#causal-self-attention)
    所述）。
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Transformer Decoder Model
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 解码器模型
- en: Building on our programmable Transformer block, we construct a typical Transformer
    decoder model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们可编程的 Transformer 模块，我们构建了一个典型的 Transformer 解码器模型。
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Variable Length Sequence Input
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可变长度序列输入
- en: Next, we create a dataset containing sequences of variable lengths, where each
    sequence is made up of randomly generated tokens. For simplicity, we (arbitrarily)
    select a fixed distribution for the sequence lengths. In real-world scenarios,
    the distribution of sequence lengths typically reflects the nature of the data,
    such as the length of documents or audio segments. Note, that the distribution
    of lengths directly affects the computational inefficiencies caused by padding.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含可变长度序列的数据集，其中每个序列由随机生成的令牌组成。为简便起见，我们（任意）选择了一个固定的序列长度分布。在实际场景中，序列长度的分布通常反映了数据的性质，例如文档或音频片段的长度。需要注意的是，长度分布直接影响由填充引起的计算低效。
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Training/Evaluation Loop
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练/评估循环
- en: Lastly, we implement a *main* function that performs training/evaluation on
    input sequences of varying length.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实现了一个 *main* 函数，用于在可变长度的输入序列上执行训练/评估。
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: PyTorch SDPA with Padding
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带填充的 PyTorch SDPA
- en: For our baseline experiments, we configure our Transformer block to utilize
    PyTorch’s [SDPA](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html)
    mechanism. In our experiments, we run both training and evaluation, both with
    and without [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html).
    These were run on an [NVIDIA H100](https://www.nvidia.com/en-eu/data-center/h100/)
    with [CUDA 12.4](https://developer.nvidia.com/cuda-toolkit) and [PyTorch](https://pytorch.org/)
    2.5.1
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的基准实验中，我们配置了 Transformer 块，以使用 PyTorch 的 [SDPA](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html)
    机制。在我们的实验中，我们分别进行了训练和评估，分别使用和不使用 [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)。这些实验在
    [NVIDIA H100](https://www.nvidia.com/en-eu/data-center/h100/) 上运行，使用 [CUDA 12.4](https://developer.nvidia.com/cuda-toolkit)
    和 [PyTorch](https://pytorch.org/) 2.5.1。
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Performance Results:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 性能结果：
- en: '**Evaluation**: 132 milliseconds (ms) without torch.compile, 130 ms with torch.compile'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：没有 torch.compile 时为 132 毫秒 (ms)，使用 torch.compile 时为 130 毫秒'
- en: '**Training**: 342 ms without torch.compile, 299 ms with torch.compile'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练**：没有 torch.compile 时为 342 毫秒，使用 torch.compile 时为 299 毫秒'
- en: Optimizing for Variable Length Input
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化可变长度输入
- en: In this section, we will explore several optimization techniques for handling
    variable-length input sequences in Transformer models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨几种优化技术，用于处理 Transformer 模型中的可变长度输入序列。
- en: Padding Optimization
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充优化
- en: Our first optimization relates not to the attention kernel but to our padding
    mechanism. Rather than padding the sequences in each batch to a constant length,
    we pad to the length of the longest sequence in the batch. The following block
    of code consists of our revised collation function and updated experiments.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个优化并不是针对注意力内核，而是针对我们的填充机制。我们不再将每批次中的序列填充到一个固定长度，而是将它们填充到批次中最长序列的长度。以下代码块展示了我们修改后的拼接函数和更新的实验。
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Padding to the longest sequence in each batch results in a slight performance
    acceleration:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 将每批次中最长期列的长度作为填充目标会略微加速性能：
- en: '**Evaluation**: 129 ms without torch.compile, 116 ms with torch.compile'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：没有 torch.compile 时为 129 毫秒，使用 torch.compile 时为 116 毫秒'
- en: '**Training**: 337 ms without torch.compile, 294 ms with torch.compile'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练**：没有 torch.compile 时为 337 毫秒，使用 torch.compile 时为 294 毫秒'
- en: SDPA with PyTorch NestedTensors
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PyTorch NestedTensors 的 SDPA
- en: Next, we take advantage of the built-in support for [PyTorch NestedTensors](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#nestedtensor-and-dense-tensor-support)
    in SDPA in evaluation mode. Currently a prototype feature, [PyTorch NestedTensors](https://pytorch.org/tutorials/prototype/nestedtensor.html)
    allows for grouping together tensors of varying length. These are sometimes referred
    to as *jagged* or *ragged* tensors. In the code block below, we define a collation
    function for grouping our sequences into NestedTensors. We also define an *indices*
    entry so that we can properly calculate the [positional embeddings](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们利用 [PyTorch NestedTensors](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#nestedtensor-and-dense-tensor-support)
    在评估模式下对 SDPA 的内置支持。目前这是一个原型功能，[PyTorch NestedTensors](https://pytorch.org/tutorials/prototype/nestedtensor.html)
    支持将不同长度的张量进行分组，这些张量有时被称为 *锯齿状* 或 *不规则* 张量。在下面的代码块中，我们定义了一个拼接函数，将我们的序列组合成 NestedTensors。我们还定义了一个
    *indices* 条目，以便我们能够正确计算 [位置嵌入](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)。
- en: PyTorch NestedTensors are supported by a [limited number of PyTorch ops](https://pytorch.org/tutorials/prototype/nestedtensor.html#nested-tensor-operations).
    Working around these limitations can require some creativity. For example, addition
    between NestedTensors is only supported when they share precisely the same “jagged”
    shape. In the code below we use a workaround to ensure that the *indices* entry
    shares the same shape as the model inputs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的NestedTensors支持由[有限数量的PyTorch操作](https://pytorch.org/tutorials/prototype/nestedtensor.html#nested-tensor-operations)提供。解决这些限制可能需要一些创造性。例如，只有在NestedTensors具有完全相同的“锯齿”形状时，才能支持它们之间的加法。在下面的代码中，我们使用一种变通方法，确保*indices*条目与模型输入共享相同的形状。
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Although, with torch.compile, the NestedTensor optimization results in a step
    time of 131 ms, similar to our baseline result, in compiled mode the step time
    drops to 42 ms for an impressive ~3x improvement.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用torch.compile时，NestedTensor优化导致的步长时间为131毫秒，接近我们的基线结果，但在编译模式下，步长时间降至42毫秒，取得了令人印象深刻的约3倍的提升。
- en: FlashAttention2
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlashAttention2
- en: In our [previous post](/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6)
    we demonstrated the use of [FlashAttention](https://github.com/Dao-AILab/flash-attention)
    and its impact on the performance of a transformer model. In this post we demonstrate
    the use of [flash_attn_varlen_func](https://github.com/Dao-AILab/flash-attention/blob/v2.7.0/hopper/flash_attn_interface.py#L429)
    from [flash-attn (2.7.0)](https://pypi.org/project/flash-attn/), an API designed
    for use with variable-sized inputs. To use this function, we concatenate all of
    the sequences in the batch into a single sequence. We also create a *cu_seqlens*
    tensor that points to the indices within the concatenated tensor where each of
    the individual sequences start. The code block below includes our collation function
    followed by evaluation and training experiments. Note, that [flash_attn_varlen_func](https://github.com/Dao-AILab/flash-attention/blob/v2.7.0/hopper/flash_attn_interface.py#L429)
    does not support torch.compile (at the time of this writing).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们[之前的文章](/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6)中，我们演示了使用[FlashAttention](https://github.com/Dao-AILab/flash-attention)及其对Transformer模型性能的影响。在本文中，我们演示了使用[flash_attn_varlen_func](https://github.com/Dao-AILab/flash-attention/blob/v2.7.0/hopper/flash_attn_interface.py#L429)来自[flash-attn
    (2.7.0)](https://pypi.org/project/flash-attn/)，这是一个专为可变大小输入设计的API。使用此功能时，我们将批次中的所有序列拼接成一个单一的序列。我们还创建了一个*cu_seqlens*张量，它指向拼接张量中每个单独序列开始的位置。下面的代码块包括我们的整理函数，接着是评估和训练实验。请注意，[flash_attn_varlen_func](https://github.com/Dao-AILab/flash-attention/blob/v2.7.0/hopper/flash_attn_interface.py#L429)目前不支持torch.compile（截至本文撰写时）。
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The impact of this optimization is dramatic, 51 ms for evaluation and 160 ms
    for training, amounting to 2.6x and 2.1x performance boosts compared to our baseline
    experiment.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这一优化的影响是显著的，评估时间为51毫秒，训练时间为160毫秒，分别比我们的基线实验提高了2.6倍和2.1倍的性能。
- en: XFormers Memory Efficient Attention
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XFormers内存高效注意力
- en: In our previous post we demonstrated the use of the [memory_efficient_attention](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)
    operator from [xFormers (0.0.28)](https://pypi.org/project/xformers/). Here we
    demonstrate the use of [BlockDiagonalMask](https://facebookresearch.github.io/xformers/_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalMask),
    which is specifically designed for input sequences of arbitrary length. The required
    collation function appears in the code block below followed by the evaluation
    and training experiments. Note, that torch.compile failed in training mode.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的文章中，我们演示了使用[xFormers (0.0.28)](https://pypi.org/project/xformers/)中的[memory_efficient_attention](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)操作。在这里，我们演示了使用[BlockDiagonalMask](https://facebookresearch.github.io/xformers/_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalMask)，该操作专为任意长度的输入序列设计。所需的整理函数出现在下面的代码块中，接着是评估和训练实验。请注意，在训练模式下，torch.compile失败。
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The resultant step time were 50 ms and 159 ms for evaluation and training without
    torch.compile. Evaluation with torch.compile resulted in a step time of 42 ms.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有torch.compile的情况下，评估和训练的步长时间分别为50毫秒和159毫秒。使用torch.compile进行评估时，步长时间为42毫秒。
- en: Results
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: The table below summarizes the results of our optimization methods.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了我们优化方法的结果。
- en: '![](../Images/2797be369d1201a6cbfde979b8581934.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2797be369d1201a6cbfde979b8581934.png)'
- en: Step time results for different optimization methods (lower is better) — by
    Author
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 不同优化方法的步长时间结果（越低越好）—— 作者
- en: The best performer for our toy model is [xFormer’s memory_efficient_attention](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)
    which delivered a ~3x performance for evaluation and ~2x performance for training.
    We caution against deriving any conclusions from these results as the performance
    impact of different attention functions can vary significantly depending on the
    specific model and use case.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们玩具模型的最佳表现是[xFormer的memory_efficient_attention](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)，它在评估时提供了约3倍的性能提升，在训练时提供了约2倍的性能提升。我们提醒不要根据这些结果得出任何结论，因为不同的注意力函数的性能影响可能会根据特定的模型和用例有显著的变化。
- en: Optimizing a HuggingFace Model for Variable-Length Input
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为可变长度输入优化HuggingFace模型
- en: The tools and techniques described above are easy to implement when creating
    a model from scratch. However, these days it is not uncommon for ML developers
    to adopt existing (pretrained) models and finetune them for their use case. While
    the optimizations we have described can be integrated without changing the set
    of model weights and without altering the model behavior, it is not entirely clear
    what the best way to do this is. In an ideal world, our ML framework would allow
    us to program the use of an attention mechanism that is optimized for variable-length
    inputs. In this section, we demonstrate how to optimize HuggingFace models for
    variable-length inputs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 上述描述的工具和技术在从头创建模型时很容易实现。然而，现如今，ML开发人员采用现有的（预训练的）模型并对其进行微调以适应其用例并不罕见。虽然我们描述的优化可以在不改变模型权重集和不改变模型行为的情况下集成，但目前尚不完全清楚如何做才是最好的方法。在理想情况下，我们的ML框架将允许我们编程使用针对可变长度输入优化的注意力机制。在本节中，我们演示了如何为可变长度输入优化HuggingFace模型。
- en: A Toy HuggingFace Model - GPT2LMHeadModel
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 玩具HuggingFace模型 - GPT2LMHeadModel
- en: To facilitate the discussion, we create a toy example in which we train a HuggingFace
    [GPT2LMHead](https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/gpt2#transformers.GPT2LMHeadModel)
    model on variable-length sequences. This requires adapting our random dataset
    and data-padding collation function according to HuggingFace's input specifications.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于讨论，我们创建了一个玩具示例，在其中我们训练了一个HuggingFace的[GPT2LMHead](https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/gpt2#transformers.GPT2LMHeadModel)模型，处理可变长度的序列。这需要根据HuggingFace的输入规范调整我们的随机数据集和数据填充整理函数。
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Training Function
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练函数
- en: Our training function instantiates a [GPT2LMHeadModel](https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/gpt2#transformers.GPT2LMHeadModel)
    based on the requested [GPT2Config](https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/gpt2#transformers.GPT2Config)
    and proceeds to train it on our variable-length sequences.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练函数实例化了一个基于请求的[GPT2LMHeadModel](https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/gpt2#transformers.GPT2LMHeadModel)的[GPT2Config](https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/gpt2#transformers.GPT2Config)，并在我们的可变长度序列上进行训练。
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: SDPA with Padding
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带填充的SDPA
- en: In the callback below we call our training function with the default sequence-padding
    collator.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的回调中，我们使用默认的序列填充整理器调用我们的训练函数。
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The resultant step times are 815 ms without torch.compile and 440 ms with torch.compile.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的步骤时间在未使用torch.compile时为815毫秒，使用torch.compile时为440毫秒。
- en: FlashAttention2
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlashAttention2
- en: 'We now take advantage of HuggingFace’s [built-in support for FlashAttention2](https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/gpt2#using-flash-attention-2),
    by setting the *attn_implementation* parameter to “flash_attention_2”. Behind
    the scenes, HuggingFace will [*unpad*](https://github.com/huggingface/transformers/blob/v4.46.3/src/transformers/modeling_flash_attention_utils.py#L246)
    the padded data input and then pass them to the optimized [flash_attn_varlen_func](https://github.com/Dao-AILab/flash-attention/blob/v2.7.0/hopper/flash_attn_interface.py#L429)
    function we saw above:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在利用HuggingFace的[内置支持FlashAttention2](https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/gpt2#using-flash-attention-2)，通过将*attn_implementation*参数设置为“flash_attention_2”。在后台，HuggingFace将[*取消填充*](https://github.com/huggingface/transformers/blob/v4.46.3/src/transformers/modeling_flash_attention_utils.py#L246)填充的数据输入，然后将其传递给我们上面看到的优化过的[flash_attn_varlen_func](https://github.com/Dao-AILab/flash-attention/blob/v2.7.0/hopper/flash_attn_interface.py#L429)函数：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The resultant time step is 620 ms, amounting to a 30% boost (in uncompiled mode)
    with just a simple flick of a switch.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的时间步长为620毫秒，相较于未编译模式提高了30%（仅通过一个简单的开关切换）。
- en: FlashAttention2 with Unpadded Input
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlashAttention2与未填充输入
- en: 'Of course, padding the sequences in the collation function only to have them
    unpadded, hardly seems sensible. In a recent [update to HuggingFace](https://huggingface.co/blog/packing-with-FA2),
    support was added for passing in concatenated (unpadded) sequences to a select
    number of models. Unfortunately, (as of the time of this writing) our GPT2 model
    did not make the cut. However, adding support requires just five small line additions
    to [modeling_gpt2.py](https://github.com/huggingface/transformers/blob/v4.46.3/src/transformers/models/gpt2/modeling_gpt2.py)
    in order to propagate the sequence [*position_ids*](https://github.com/huggingface/transformers/blob/v4.46.3/src/transformers/models/gpt2/modeling_gpt2.py#L985)to
    the [flash-attention kernel](https://github.com/huggingface/transformers/blob/v4.46.3/src/transformers/models/gpt2/modeling_gpt2.py#L436).
    The full *patch* appears in the block below:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在合并函数中填充序列，结果却又将其解填充，这似乎毫无意义。在最近的[HuggingFace更新](https://huggingface.co/blog/packing-with-FA2)中，已增加了对将连接（未填充）序列传递给选定模型的支持。不幸的是（截至本文写作时），我们的GPT2模型未包括在内。然而，添加支持只需要在[modeling_gpt2.py](https://github.com/huggingface/transformers/blob/v4.46.3/src/transformers/models/gpt2/modeling_gpt2.py)中增加五行代码，即可将序列[*position_ids*](https://github.com/huggingface/transformers/blob/v4.46.3/src/transformers/models/gpt2/modeling_gpt2.py#L985)传播到[flash-attention内核](https://github.com/huggingface/transformers/blob/v4.46.3/src/transformers/models/gpt2/modeling_gpt2.py#L436)。完整的*补丁*如下所示：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We define a collate function that concatenates our sequences and train our hugging
    face model on unpadded sequences. (Also see the built-in [DataCollatorWithFlattening](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithFlattening)
    utility.)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个合并函数，将我们的序列连接在一起，并在未填充的序列上训练我们的HuggingFace模型。（另请参见内置的[DataCollatorWithFlattening](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithFlattening)工具。）
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The resulting step time is 323 ms, 90% faster than running flash-attention on
    the padded input.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的步骤时间为323毫秒，比在填充输入上运行flash-attention快了90%。
- en: Results
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: The results of our HuggingFace experiments are summarized below.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的HuggingFace实验结果总结如下。
- en: '![](../Images/a52cc54ef95a4e7ba82fb1f706350d64.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a52cc54ef95a4e7ba82fb1f706350d64.png)'
- en: Step time results for different optimization methods (lower is better) — by
    Author
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 不同优化方法的步骤时间结果（越低越好）——作者
- en: With little effort, we were able to boost our runtime performance by 2.5x when
    compared to the uncompiled baseline experiment, and by 36% when compared to the
    compiled version.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一点小努力，我们将运行时性能提升了2.5倍，相比未编译的基线实验，提升了36%，相较于编译版本。
- en: In this section, we demonstrated how the HuggingFace APIs allow us to leverage
    the optimized kernels in FlashAttention2, significantly boosting the training
    performance of existing models on sequences of varying length.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了HuggingFace APIs如何让我们利用FlashAttention2中的优化内核，显著提升现有模型在不同长度序列上的训练性能。
- en: Summary
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As AI models continue to grow in both popularity and complexity, optimizing
    their performance has become essential for reducing runtime and costs. This is
    especially true for compute-intensive components like attention layers. In this
    post, we have continued our exploration of attention layer optimization, and demonstrated
    new tools and techniques for enhancing Transformer model performance. For more
    insights on AI model optimization, be sure to check out the [first post](/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6)
    in this series as well as our [many other posts](https://chaimrand.medium.com/)
    on this topic.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI模型在流行度和复杂度上的不断增长，优化它们的性能已成为减少运行时间和成本的关键，尤其是对于像注意力层这样的计算密集型组件。在这篇文章中，我们继续探索注意力层的优化，并展示了提升Transformer模型性能的新工具和技术。欲了解更多关于AI模型优化的见解，请务必查看本系列的[第一篇文章](/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6)以及我们在这个话题上的[其他多篇文章](https://chaimrand.medium.com/)。
