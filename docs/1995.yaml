- en: How to Use Explainable AI Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-use-explainable-ai-tools-64749f68088d?source=collection_archive---------7-----------------------#2024-08-15](https://towardsdatascience.com/how-to-use-explainable-ai-tools-64749f68088d?source=collection_archive---------7-----------------------#2024-08-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[AI Pitfalls Digest](https://medium.com/@pedram-ataee/list/ai-pitfalls-digest-881a26c7eec5)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep Dive into Feature Importance, Partial Dependence Plot, and Sub-population
    Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pedram-ataee.medium.com/?source=post_page---byline--64749f68088d--------------------------------)[![Pedram
    Ataee, PhD](../Images/f4fb1ce6d5543f24e56cdf83630844b2.png)](https://pedram-ataee.medium.com/?source=post_page---byline--64749f68088d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--64749f68088d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--64749f68088d--------------------------------)
    [Pedram Ataee, PhD](https://pedram-ataee.medium.com/?source=post_page---byline--64749f68088d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--64749f68088d--------------------------------)
    ·7 min read·Aug 15, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22af3c18fc0d81ec941cbdfdfad2e0a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Artem Sapegin](https://unsplash.com/@sapegin?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'The AI community has introduced various concepts and tools to interpret AI
    model outcomes, including feature importance, partial dependence plots, and sub-population
    analysis. The Explainable AI (XAI) tools are crucial in building **trust** among
    end-users and regulators, identifying and mitigating **bias,** and improving overall
    model **performance**. They are built to answer the main question of all users:
    “Why did the model make a specific prediction for an instance or a group of instances?”'
  prefs: []
  type: TYPE_NORMAL
- en: While the XAI tools are invaluable in identifying bias and building trust, they
    are highly susceptible to misuse.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://shap.readthedocs.io/en/latest/index.html?source=post_page-----64749f68088d--------------------------------)
    [## Welcome to the SHAP documentation - SHAP latest documentation'
  prefs: []
  type: TYPE_NORMAL
- en: SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain
    the output of any machine learning model…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: shap.readthedocs.io](https://shap.readthedocs.io/en/latest/index.html?source=post_page-----64749f68088d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: For instance, most feature importance methods assume that features are independent.
    As a result, including highly correlated features in the analysis can lead to
    **unreliable outcomes**. Moreover, different approaches for calculating the global
    importance of features, such as using the “mean…
  prefs: []
  type: TYPE_NORMAL
