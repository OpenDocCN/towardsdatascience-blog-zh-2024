<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Exploring Music Transcription with Multi-Modal Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Exploring Music Transcription with Multi-Modal Language Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-music-transcription-with-multi-modal-language-models-af352105db56?source=collection_archive---------1-----------------------#2024-11-17">https://towardsdatascience.com/exploring-music-transcription-with-multi-modal-language-models-af352105db56?source=collection_archive---------1-----------------------#2024-11-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e307" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Using Qwen2-Audio to transcribe music into sheet music</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@jon.flynn2?source=post_page---byline--af352105db56--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jon Flynn" class="l ep by dd de cx" src="../Images/492cef280f4ea0b002e5d00ad2e083a5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*16AI0ZxosqDanJ22tGkA_Q.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--af352105db56--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@jon.flynn2?source=post_page---byline--af352105db56--------------------------------" rel="noopener follow">Jon Flynn</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--af352105db56--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">17 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">5</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/1c1e9d05246384c1d3a88590ce3d7033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xu062EKI7BoyXDh7pWG1mQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure><p id="2cdb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Automatic music transcription is the process of converting audio files like MP3 and WAV into sheet music, guitar tablature, and any format a musician may want to learn a song on their instrument.</p><p id="5a31" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We’ll go over the best current tools for doing this, which happen to be deep learning-based, and a novel approach for it.</p><h1 id="04e2" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Current state of the art</h1><p id="201d" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The current state-of-the-art for this task comes from <a class="af oz" href="https://magenta.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">Magenta</a>, an open-source research project developed by the now defunct (as of April 2023) Google Brain Team.</p><p id="37d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">They released a paper <a class="af oz" href="https://arxiv.org/abs/2107.09142" rel="noopener ugc nofollow" target="_blank">Sequence-to-Sequence Piano Transcription with Transformers</a> in 2021 which used a T5-inspired transformer model (similar to <a class="af oz" href="https://huggingface.co/google-t5/t5-small" rel="noopener ugc nofollow" target="_blank">“t5-small”</a>) with 54 million parameters and the <a class="af oz" href="https://magenta.tensorflow.org/datasets/maestro" rel="noopener ugc nofollow" target="_blank">Maestro dataset</a>, achieving great results. The problem is approached as a sequence-to-sequence task using an encoder-decoder Transformer architecture. The encoder processes mel spectrogram frames as input and produces embeddings, while the decoder uses these embeddings via cross-attention to autoregressively generate a sequence of MIDI-like tokens. Their vocabulary consisted of four types of tokens:</p><ul class=""><li id="ed18" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">Note tokens (128 values for MIDI pitches)</li><li id="b1db" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Velocity tokens (128 values including zero for note-off)</li><li id="9e20" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Time tokens (6,000 values in 10ms bins for absolute timing)</li><li id="86f4" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">EOS token (to mark sequence end)</li></ul><p id="78d5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">See the image below for a visualisation of the architecture and an example sequence of their custom MIDI tokens:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pi"><img src="../Images/67b417bfc9720edd402c1c002ca058f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3sWmpLsv7KElnzmY2sI6Jw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 1. from <a class="af oz" href="https://arxiv.org/abs/2107.09142" rel="noopener ugc nofollow" target="_blank">Sequence-to-Sequence Piano Transcription with Transformers</a> paper</figcaption></figure><blockquote class="pj pk pl"><p id="8c06" class="nc nd pm ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Our model is a generic encoder-decoder Transformer architecture where each input position contains a single spectrogram frame and each output position contains an event from our MIDI-like vocabulary. Outputs tokens are autoregressively sampled from the decoder, at each step taking the token with maximum probability.</p></blockquote><p id="3788" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In 2022, they released a paper, <a class="af oz" href="https://arxiv.org/abs/2111.03017" rel="noopener ugc nofollow" target="_blank">MT3: Multi-Task Multitrack Music Transcription</a>. This experiment used the same approach as the last one but added additional instrument tokens to represent the different instruments. Again, they used a similar T5 model and achieved great performance against many of the datasets trained on, notably <a class="af oz" href="http://www.slakh.com/" rel="noopener ugc nofollow" target="_blank">Slakh</a>, Maestro and <a class="af oz" href="https://www.kaggle.com/datasets/imsparsh/musicnet-dataset" rel="noopener ugc nofollow" target="_blank">MusicNet</a>.</p><p id="ff25" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af oz" href="https://arxiv.org/abs/2403.10024" rel="noopener ugc nofollow" target="_blank">MR-MT3</a> was released the following year as a slight improvement to MT3.</p><h1 id="bc37" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Why use language models and not continue with these SOTA models?</h1><h2 id="66de" class="pn nz fq bf oa po pp pq od pr ps pt og nl pu pv pw np px py pz nt qa qb qc qd bk"><strong class="al">Compute/GPU resources</strong></h2><p id="69ee" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Huge resources were needed to train this from scratch, despite being much smaller in size compared to even the smallest language models. The 2021 paper noted:</p><blockquote class="pj pk pl"><p id="e5ed" class="nc nd pm ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">“We trained all models on 32 TPUv3 cores, resulting in a per-core batch size of 8. Based on validation set results, overfitting did not seem to be a problem, so we allowed training to progress for 400K steps, which took about 2.5 days for our baseline models.”</p></blockquote><p id="b7a2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The MT3 paper doesn’t provide as specific details on training, stating they train for 1 million steps.</p><h2 id="dc2b" class="pn nz fq bf oa po pp pq od pr ps pt og nl pu pv pw np px py pz nt qa qb qc qd bk">Other limitations</h2><p id="af20" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">These models have some inherent limitations in their output flexibility. While language models typically have large vocabularies (often 30,000+ tokens) that are extensively pre-trained on diverse natural language data, MT3 and similar music transcription models use a much smaller, specialised token vocabulary (only a few thousand tokens) focused solely on musical events. This specialisation means that adding new tokens, such as for new instruments or playing techniques like palm muting on guitars or pizzicato on violins, is likely not easy — it requires significant retraining to integrate these new tokens effectively with the existing vocabulary, and often requires substantial training data demonstrating these techniques. This differs from large language models which can often describe such musical nuances in natural language without modification, as they’ve encountered these concepts during their broad pre-training.</p><h2 id="ba4f" class="pn nz fq bf oa po pp pq od pr ps pt og nl pu pv pw np px py pz nt qa qb qc qd bk">Transfer learning and zero-shot</h2><p id="3f52" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">We can leverage transfer learning from large open-source pre-trained audio and language models. Examples of music generation models include <a class="af oz" href="https://openai.com/index/jukebox/" rel="noopener ugc nofollow" target="_blank">OpenAI’s Jukebox</a> and <a class="af oz" href="https://audiocraft.metademolab.com/musicgen.html" rel="noopener ugc nofollow" target="_blank">Meta’s MusicGen</a>.</p><h1 id="bd78" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Modern multi-modal model architecture</h1><p id="9f03" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk"><a class="af oz" href="https://openai.com/index/hello-gpt-4o/" rel="noopener ugc nofollow" target="_blank">GPT-4o</a> is designed to handle text, audio and images “natively”. Although OpenAI has not released the technical details on this, it’s assumed that some weights in the network will process all modalities. It’s possible that the model uses a decoder-only architecture like language only GPT models without the need for encoder components to convert different modalities to a dense representation first. This design allows the model to seamlessly process and interpret inputs like text and images together, potentially offering performance benefits both computationally and in terms of model understanding.</p><p id="1df4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Many multi-modal models take a simpler approach reminiscent of the encoder-decoder architecture: they combine two pre-trained models — an encoder for the specific input modality (like <a class="af oz" href="https://huggingface.co/docs/transformers/main/en/model_doc/vit" rel="noopener ugc nofollow" target="_blank">ViT</a> for vision or an audio encoder for sound) and a Large Language Model (such as LLaMA, Gemma, or Qwen). These models are connected through projection layers that align their representations in a shared latent space, often using just a single linear layer. These projection layers learn to convert the encoder’s output into a format that matches the LLM’s expected input dimensions and characteristics. The projection creates new embeddings/tokens from the input modality that can then be injected into the LLM’s input sequence. <a class="af oz" href="https://llava-vl.github.io/" rel="noopener ugc nofollow" target="_blank">LLaVA</a> is a prime example of this architecture for vision-language tasks, while <a class="af oz" href="https://research.atspotify.com/2023/10/llark-a-multimodal-foundation-model-for-music/" rel="noopener ugc nofollow" target="_blank">Spotify’s Llark</a> and <a class="af oz" href="https://github.com/QwenLM/Qwen2-Audio" rel="noopener ugc nofollow" target="_blank">Qwen-Audio</a> apply the same principle using audio encoders instead of vision encoders.</p><p id="0982" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here’s some pseudocode on how the models are stitched together:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="0585" class="qi nz fq qf b bg qj qk l ql qm"># Extract features from final layer of audio encoder<br/># Shape: [batch_size, audio_seq_len, encoder_dim=1024]<br/>audio_features = audio_model(audio_input)<br/>    <br/># Project audio features to match LLM's embedding dimension<br/># Shape: [batch_size, audio_seq_len, llm_embed_dim=4096]<br/>audio_embeddings = projection_layer(audio_features)<br/>    <br/># Get text embeddings from LLM's embedding layer<br/># Shape: [batch_size, text_seq_len, llm_embed_dim=4096]<br/>text_embeddings = llm.embed_text(text_input)<br/>    <br/># Concatenate along sequence length dimension<br/># Shape: [batch_size, audio_seq_len + text_seq_len, llm_embed_dim=4096]<br/>combined_input = concatenate([audio_embeddings, text_embeddings], dim=1)<br/>    <br/># Feed them into the LLM as normal for generation<br/>output = llm(combined_input)</span></pre><h1 id="6da3" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk"><strong class="al">Spotify Llark and Qwen2-Audio</strong></h1><h2 id="5bc8" class="pn nz fq bf oa po pp pq od pr ps pt og nl pu pv pw np px py pz nt qa qb qc qd bk">Overview of architecture</h2><p id="b4e3" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Llark uses <a class="af oz" href="https://research.atspotify.com/2023/10/llark-a-multimodal-foundation-model-for-music/" rel="noopener ugc nofollow" target="_blank">OpenAI’s Jukebox</a> and Qwen2-Audio uses <a class="af oz" href="https://openai.com/index/whisper/" rel="noopener ugc nofollow" target="_blank">OpenAI’s Whisper</a> for the audio towers. Jukebox is a music generation model but it can also take in audio clips as input and outputs a continuation of the audio clip. Whisper is used for transcribing voice to text.</p><p id="810f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Given their purpose, the choice of audio module is clear: Llark specialises in music analysis, while Qwen2Audio primarily focuses on responding to voice instructions with some basic audio and music analysis capabilities.</p><p id="a06e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Determining the optimal source for extracting embeddings from large pre-trained models involves research and experimentation. Additionally, deciding whether to fine-tune the entire module or freeze parts of it is a crucial design choice. For instance, LlaVa’s training strategy involves freezing the vision tower and focusing on fine-tuning the projection layer and language model. We’ll go over this aspect of each model below.</p><h2 id="c74f" class="pn nz fq bf oa po pp pq od pr ps pt og nl pu pv pw np px py pz nt qa qb qc qd bk">Llark: why Jukebox? Are these embeddings the best as of September 2024?</h2><p id="45ca" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Determining the optimal location to extract embeddings from large models typically requires extensive probing. This involves testing various activations or extracted layers of the model on different classification tasks through a process of trial and error. For music generation models, this could include tasks like genre recognition, instrument detection, emotion detection, as well as analysis of harmonic structures and temporal patterns. Many commercial embedding models (lik<a class="af oz" href="https://openai.com/index/new-embedding-models-and-api-updates/" rel="noopener ugc nofollow" target="_blank">e OpenAI’s embedding models</a>) are trained specifically for embedding generation with specialised architectures and training objectives, rather than being fine-tuned versions of existing language models.</p><p id="c839" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The two largest publicly available music generation and music continuation (i.e.: able to take in audio as input) models are Jukebox and MusicGen. MusicGen is newer and faster, and therefore seemed like it would be the obvious choice to me. However, according to <a class="af oz" href="https://www.merl.com/publications/docs/TR2024-032.pdf" rel="noopener ugc nofollow" target="_blank">this paper on probing MusicGen</a>, embeddings extracted from Jukebox appear to outperform MusicGen on average in classification tasks. The findings from this paper led to the authors of Llark using the following approach for extracting embeddings:</p><ol class=""><li id="d9a4" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qn pb pc bk">Embeddings are derived from the output of the 36th layer of the Jukebox encoder following the approach described in <a class="af oz" href="https://arxiv.org/abs/2107.05677" rel="noopener ugc nofollow" target="_blank">Castellon et al. (2021)</a></li><li id="a99e" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qn pb pc bk">Original Jukebox encoding:<br/> * 4800-dimensional vectors at 345Hz<br/> * For a 25s clip: over 4.14 * 10⁷ floating-point values</li><li id="0a31" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qn pb pc bk">The authors use a downsampling approach: Mean-pooling within 100ms frames, resulting in:<br/> * Downsampled frequency: 10Hz<br/> * Embedding size: 1.2 × 10⁶ for a 25s audio clip. That means a 2D array with shape [240, 4800].<br/> * Retains temporal information (unlike Castellon et al. who average over the time dimension)</li></ol><p id="2679" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">(The downsampled embedding size is approximately 6x larger than CLIP ViT-L14 models used in many multimodal vision models)</p><h2 id="7f1b" class="pn nz fq bf oa po pp pq od pr ps pt og nl pu pv pw np px py pz nt qa qb qc qd bk">Qwen2Audio: Whisper</h2><p id="91ff" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The embedding extraction for Qwen2Audio isn’t mentioned in detail in the paper. Whisper is an encoder-decoder architecture where the encoder generates deeply learned representations of the audio and the decoder decodes the representations to text (the transcription). In Qwen2Audio, it appears they extract embeddings from the final layer of Whisper’s encoder, although they don’t mention whether they freeze it during training.</p><h2 id="128c" class="pn nz fq bf oa po pp pq od pr ps pt og nl pu pv pw np px py pz nt qa qb qc qd bk"><strong class="al">Pre-trained weights, training data and datasets</strong></h2><p id="179a" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Unfortunately Spotify has not provided any datasets or their trained model weights to the public, noting:</p><blockquote class="pj pk pl"><p id="9549" class="nc nd pm ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">“With respect to inputs: the inputs to our model are public, open-source, Creative Commons-licensed audio and associated annotations. However, each individual audio file can have its own, potentially more restrictive license. Many of the audio files include “no derivatives” licenses. We encourage users of the datasets to familiarize themselves with the restrictions of these licenses; in order to honor such licenses, we do not release any derivatives from the training data in this paper (including query- response pairs or trained model weights).”</p></blockquote><p id="d957" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">They used the following datasets:</p><ul class=""><li id="ddb3" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">MusicCaps (Agostinelli et al., 2023)</li><li id="b72b" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">YouTube8M-MusicTextClips (McKee et al., 2023)</li><li id="5b02" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">MusicNet (Thickstun et al., 2017)</li><li id="2f68" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">FMA (Defferrard et al., 2017)</li><li id="83a1" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">MTG-Jamendo (Bogdanov et al., 2019)</li><li id="8c98" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">MagnaTagATune (Law et al., 2009)</li></ul><p id="2828" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Llark details it’s training data generation process in the following extract:</p><blockquote class="pj pk pl"><p id="282c" class="nc nd pm ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">“We use variants of ChatGPT to extract the instruction- tuning data for all experiments. However, the exact language model used varies by dataset. We select the OpenAI model as follows: We use GPT-4 for all reasoning tasks. We found that GPT-4 was much more adept at following the complex instructions in the Reasoning task family. For datasets with more than 25k samples, we limit Reasoning data to a random subsample of 25k tracks.”</p></blockquote><p id="6ae2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This results in Q&amp;A data like this:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qo"><img src="../Images/aeb50a9a892d9ed3b78415702057fc99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7kSfblXgRIXxW0HcAWXfLQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="qp">Example text inputs and outputs from LLark, for the provided audio.</em></figcaption></figure><p id="aee2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The datasets used for training Qwen2Audio are not shared either, but the trained model is widely available and also is implemented in the <code class="cx qq qr qs qf b">transformers</code> library:</p><ul class=""><li id="368d" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk"><a class="af oz" href="https://github.com/QwenLM/Qwen2-Audio" rel="noopener ugc nofollow" target="_blank">Qwen2-Audio official github repo</a></li><li id="4f10" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk"><a class="af oz" href="https://huggingface.co/docs/transformers/main/en/model_doc/qwen2_audio" rel="noopener ugc nofollow" target="_blank">Qwen2-Audio transformers docs</a></li></ul><p id="6bd1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For this project, fine-tuning off a pre-trained Llark model would have been optimal, given it’s reportedly good performance against the evaluation benchmarks Spotify stated in the paper.</p><p id="6a72" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, given they didn’t release the weights for it, it’s unfeasible to start training a model like this from scratch without a fair bit of expertise and money. Spotify trained it on:</p><blockquote class="pj pk pl"><p id="79fa" class="nc nd pm ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Our model is trained on 4 80GB NVIDIA A100 GPUs. Training takes approximately 54 hours.</p></blockquote><p id="d01f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This would cost around $700 using a provider like LambdaLabs.</p><p id="aced" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Because of the above, I went with Qwen. However, Qwen2-Audio doesn’t perform that well across basic music tasks like tempo and instrument detection. I detail this below in the evaluation section. This means that the model is probably not large enough or pre-trained enough to achieve this task, but my hope is I could at least set a starting point and framework for fine-tuning on this task in the future. As Alibaba state in their Qwen2-Audio <a class="af oz" href="https://qwenlm.github.io/blog/qwen2-audio/" rel="noopener ugc nofollow" target="_blank">blog post</a>:</p><blockquote class="pj pk pl"><p id="e845" class="nc nd pm ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We also plan to build larger Qwen2-Audio models to explore the scaling laws of audio language models.</p></blockquote><p id="9ae9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For my own learning though, I did have a go at re-creating the model using <code class="cx qq qr qs qf b">torch</code> and pre-trained models with the <code class="cx qq qr qs qf b">transformers</code> library.</p><p id="29f9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I also created datasets for Q&amp;A data and embeddings. I generated short form Q&amp;A data for the URMP dataset, e.g.: “What is the tempo of this track”, “What instruments are playing in this audio”.</p><p id="2dab" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af oz" href="https://colab.research.google.com/drive/1jdR5w-XlJQFog47ZJ36ckEVMW0F5qIpl" rel="noopener ugc nofollow" target="_blank">Here’s a notebook</a> for running Jukebox in a Colab environment to take advantage of the cheap T4 GPU’s. I uploaded both Q&amp;A and embeddings datasets to HuggingFace <a class="af oz" href="https://huggingface.co/jonflynn" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="0dea" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af oz" href="https://colab.research.google.com/drive/1_V5B9ZrwrKtom-N4r-Om3mqlXKPacUBh#scrollTo=72Gv5raTIPqi" rel="noopener ugc nofollow" target="_blank">Here’s a notebook</a> with Llark replicated.</p><h1 id="58c1" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Training data for music transcription</h1><h2 id="db7d" class="pn nz fq bf oa po pp pq od pr ps pt og nl pu pv pw np px py pz nt qa qb qc qd bk">Transcription format</h2><p id="e1f1" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">I chose <a class="af oz" href="https://trillian.mit.edu/~jc/music/doc/ABC.html" rel="noopener ugc nofollow" target="_blank">ABC music notation</a> as the output format that the language model is expected to transcribe the music in. Here’s an example of it:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="51fc" class="qi nz fq qf b bg qj qk l ql qm">X:1<br/>M:4/4<br/>L:1/16<br/>K:none<br/>Q:67<br/><br/>V:1 name="Electric Bass (finger)"<br/>%%octave-default C4<br/>GAA^2E3A2&lt;A^2 | D^D^2E2A2A^4 A^2E2 | A2A^4A^2E2 A2A^4 | A^2E2A2A^4A^2E2A2 |<br/>A^4 A^2E2 A2A^4A^2 E2 | A2A^4 |<br/><br/>V:2 name="Bright Acoustic Piano"<br/>%%octave-default C5<br/>[E3C3][E3C3][E3C3] [E3C3][A^,2E2A^2] | [E3A^3][E3A^3][E3A^3][E3A^3][E3A^3] |<br/>[E3A^3][E3A^3][E3A^3] [E3A^3][E3A^3] | [E3A^3][E3A^3][E3A^3][E3A^3][E3A^3] |<br/>[E3A^3][E3A^3][E3A^3] [E3A^3][E3A^3] | [E3A^3] |<br/><br/>V:3 name="Electric Guitar (jazz)"<br/>%%octave-default C5<br/>E'3C'3A^4E'3C'3 | A^4E'3 C'3A^4E'3C'3 | A^4 E'3C'3A^4 E'3C'3 | A^4E'3C'3A^4E'3C'3 |<br/>A^4E'3C'3 A^4E'3C'3 | A^4 |</span></pre><p id="9717" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this notation we have the time signature and tempo defined at the top denoted by ‘M’ and ‘Q’. The ‘L’ indicates the default note length of the notation, in this case a sixteenth note, which is the norm. We then define each instrument and the default octave they should adhere to when writing the notes for each of them. Here’s a summary of the key syntactical points for writing notes in ABC music notation:</p><ul class=""><li id="8650" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">Notes are represented by letters A-G, with lowercase letters indicating higher octaves</li><li id="389b" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Sharps are denoted by ^ before the note, flats by _</li><li id="ee27" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Natural signs are represented by =</li><li id="9268" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Note length is indicated by numbers after the note (C2 is twice as long as C)</li><li id="e1d4" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Dotted notes use a . after the note (C. is a dotted quarter note)</li><li id="fa50" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Rests are represented by z, with numbers for duration (z2 is a half rest)</li><li id="c747" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Chords are enclosed in square brackets [CEG]</li><li id="e1a6" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Ties are shown with a hyphen -</li><li id="23bd" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Bar lines are represented by |</li><li id="f4d2" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Broken rhythms use &gt; or &lt; between notes (C&gt;D means dotted-C eighth note followed by D sixteenth note)</li></ul><h2 id="a97b" class="pn nz fq bf oa po pp pq od pr ps pt og nl pu pv pw np px py pz nt qa qb qc qd bk">Why ABC?</h2><p id="049c" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The reasons for choosing this notation are:</p><ol class=""><li id="1048" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qn pb pc bk">It’s a minimalist format for writing music</li><li id="d6bf" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qn pb pc bk">It’s widely used and popular; language models already have good comprehension of ABC notation due to extensive pre-training on it.</li><li id="1542" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qn pb pc bk">It’s flexible and can easily be extended to include tempo changes, time signature changes, additional playing styles like mentioned above, etc…</li></ol><p id="0e3a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I converted the MIDI files provided by the datasets to ABC notation using <a class="af oz" href="https://github.com/sshlien/abcmidi" rel="noopener ugc nofollow" target="_blank">this library</a>. A notebook for creating the datasets is <a class="af oz" href="https://colab.research.google.com/drive/1CdQ_PUjhCvCR2VjGt3ya1hNowPrr0Xun" rel="noopener ugc nofollow" target="_blank">here</a>.</p><h1 id="5c8a" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Evaluation</h1><p id="abd7" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">To evaluate both the original model and each stage of fine-tuning I performed thereafter, I randomly selected 30 samples of varying complexity from the URMP dataset and ran the model three times on each sample, manually examining all responses.</p><p id="b0e1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Through manual testing, I found the optimal decoding parameters to be a temperature of 0.7 and a top_p of 1.2. The maximum number of tokens to return was capped at 2048. Adjusting the max seemed to have little difference on performance.</p><p id="913f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The original model performed poorly on this evaluation set. While it occasionally predicted the tempo and instruments correctly, it mostly failed to do so. A text file with the evaluation results is available <a class="af oz" href="https://drive.google.com/file/d/1-0XgJDOhnj1kbffeHcQutgZ1td59WQjI/view?usp=drive_link" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="e728" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Given this starting point, it’s unlikely that we’ll see strong results from this experiment without a robust pre-trained model. However, the goal is to develop strategies that can be applied in the future as more advanced pre-trained models become available.</p><h1 id="d908" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Fine-tuning strategies</h1><p id="c52e" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">I first attempted fine-tuning with basic cross-entropy loss. Supervised fine-tuning with cross-entropy loss is a quick way to start teaching the model but a basic loss function like this has limitations as we will see below. The intuition behind this stage of training is that it would nudge the model in the right direction and it would pick up any patterns or any customised ABC notation the dataset may have which the model may not have seen before.</p><h2 id="dad3" class="pn nz fq bf oa po pp pq od pr ps pt og nl pu pv pw np px py pz nt qa qb qc qd bk">Cross-entropy loss with teacher forcing</h2><p id="c7c5" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">First, we trained it in a typical supervised fine-tuning manner for language models. I used the <code class="cx qq qr qs qf b">SFTtrainer</code> from the <code class="cx qq qr qs qf b">trl</code> library for this, which uses cross-entropy loss with teacher forcing defined step by step below:</p><ol class=""><li id="cdbc" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qn pb pc bk">The model predicts the next token in the sequence.</li><li id="b056" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qn pb pc bk">The loss is calculated based on the difference between the predicted probabilities (logits) and the actual next token.</li><li id="62d0" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qn pb pc bk">For the next prediction, the model is given the actual correct token (ground truth), rather than its own prediction. This is known as teacher forcing, it helps stabilise training and significantly speed it up, especially in the early stages.</li></ol><p id="9d5e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The results from this training phase were poor. It degraded the performance of the original model. The model, which previously handled tempo and instrument recognition well, now mostly got these wrong. It also began producing garbled text output with endless repetition. This occurred even when setting a low learning rate, applying gradient clipping, and using low LoRA ranks to mitigate large changes to the model. Overall, it seemed the model was very sensitive to the training applied.</p><p id="d7c1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, while this training phase may offer some improvements, it won’t lead to optimal performance due to the limitations of our basic loss function. This function struggles to fully capture the model’s performance nuances. For example, when using teacher forcing, instrument predictions can yield deceptively low loss across certain token sections. If an instrument name begins with “V”, the model might confidently predict “Violin” or “Viola” based on our dataset, regardless of accuracy. Additionally, the loss function may not accurately reflect near-misses, such as predicting a tempo of 195 instead of 200 — a small difference that’s reasonably accurate but potentially penalised heavily dependent on the distribution of probabilities amongst logits. It’s possible that neighbouring numbers also have high probabilities.</p><h2 id="5d23" class="pn nz fq bf oa po pp pq od pr ps pt og nl pu pv pw np px py pz nt qa qb qc qd bk">RLHF with PPO</h2><p id="7e65" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Because of these limitations, we can create our own custom loss function that can more accurately score the response from the model. That is, given a predicted sequence from the model, the loss function could give it a score between 0 and 1 on how good it is.</p><p id="8fa8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, integrating this custom loss function into supervised fine-tuning presents a significant challenge. The issue stems from the non-linearity introduced by the custom loss function, which prevents the direct calculation of gradients. Let’s break this down:</p><p id="23da" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In traditional SFT with cross-entropy loss:</p><ul class=""><li id="f5ed" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">The model outputs logits (raw scores) for each token in its vocabulary</li><li id="d38e" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">These logits directly represent the model’s prediction probabilities</li><li id="9de4" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">The loss function compares these probabilities to the ground truth</li><li id="3e3a" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Gradients can be computed directly through this comparison</li><li id="f6f6" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">The chain rule of calculus allows us to propagate these gradients back through the model</li></ul><p id="61b1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With our custom loss function:</p><ul class=""><li id="54af" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">The model must first generate complete text output</li><li id="dd6b" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">This generation process involves sampling from probability distributions</li><li id="a564" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Our loss function then analyses this text output (checking tempo, notes, etc.)</li><li id="edad" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">This creates a non-differentiable step between the model’s logits and our loss calculation</li><li id="b35a" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">The sampling and text analysis steps break the gradient chain needed for backpropagation</li></ul><p id="25d9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To overcome this, reinforcement learning techniques like Proximal Policy Optimisation (PPO) can be employed. PPO is specifically designed to handle non-differentiable loss functions and can optimise the model by considering the entire policy (the model’s output distribution), rather than relying on gradient information from logits.</p><p id="8c7a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="pm">Note, there’s a</em><a class="af oz" href="https://medium.com/search?q=PPO+explained" rel="noopener"><em class="pm"> lot of great articles</em></a><em class="pm"> on here explaining PPO!</em></p><p id="a689" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The key insight of PPO is that instead of trying to directly backpropagate through the non-differentiable steps, it:</p><ol class=""><li id="3390" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qn pb pc bk">Treats the model’s outputs as actions in a reinforcement learning framework</li><li id="99dd" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qn pb pc bk">Uses the custom loss function as a reward signal</li><li id="9b1d" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qn pb pc bk">Updates the model’s policy (its probability distributions over tokens) to maximise expected reward</li><li id="9461" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qn pb pc bk">Does this while ensuring the updated policy doesn’t deviate too far from the current one</li></ol><p id="09eb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This approach allows us to effectively train the model with the custom loss function, ensuring performance improvements without disrupting the core training dynamics. The PPO algorithm’s conservative update strategy helps maintain stability during training, which is particularly important when working with large language models.</p><p id="9efa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Usually, this scoring function would be implemented as a separate LLM in the form of a “reward model” commonly used when fine-tuning models via RLHF, which was a breakthrough first introduced when ChatGPT came out. Due to the nature of this task, we can manually write code to score the responses, which uses fewer resources and is quicker.</p><p id="c501" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For time signature and tempo recognition this is easy to calculate. We extract all predicted items with regex, for example extracting the metre:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="403a" class="qi nz fq qf b bg qj qk l ql qm">def extract_metre(self, abc_string):<br/>  return re.search(r'M:(\S+)', abc_string).group(1)</span></pre><p id="f56a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The model should learn the syntax and structure we want it to output in the SFT stage. If it outputs something that will cause our regex to not find anything or error, we can just skip that sample, assuming it’s a small minority of the dataset.</p><p id="8aba" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We extract the predicted tempo and write a function that is more forgiving for small errors but penalises larger errors more heavily:</p><ul class=""><li id="772c" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">For small differences (≤10 BPM), it uses linear scaling.</li><li id="1028" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">For larger differences, it switches to exponential scaling.</li><li id="d07a" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">The final loss is capped between 0 and 1.</li></ul><p id="2ca1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s break down the key components of this custom loss:</p><p id="5ffb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Code for the custom loss is <a class="af oz" href="https://colab.research.google.com/drive/1lpPfn9EFE2rBsasIJNv8Cy9qTvtfXzq-#scrollTo=mOs_gWcjrBgv" rel="noopener ugc nofollow" target="_blank">here</a></p><p id="8f77" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">1. Metre Loss</strong></p><p id="2571" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The metre loss focuses on the time signature of the piece. It compares the predicted metre with the ground truth, considering both the numerator and denominator separately, as well as their ratio. This approach allows for a nuanced evaluation that can handle various time signatures accurately.</p><p id="96bf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The metre loss uses a combination of linear and exponential scaling to penalise differences. Small discrepancies result in a linear increase in loss, while larger differences lead to an exponential increase, capped at a maximum value of 1.</p><p id="0cb5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">2. Tempo Loss</strong></p><p id="a0da" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Tempo loss evaluates the accuracy of the predicted beats per minute (BPM). Similar to the metre loss, it uses a combination of linear and exponential scaling.</p><p id="a0f8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For small tempo differences (≤10 BPM), the function applies linear scaling. Larger differences trigger exponential scaling, ensuring that significant tempo mismatches are penalised more heavily.</p><p id="cdba" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">3. Pitch Loss</strong></p><p id="f0ba" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The pitch loss is perhaps the most crucial component, as it assesses the accuracy of the transcribed notes. This function uses the Levenshtein distance to compare the sequence of notes in each voice.</p><p id="e92b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The pitch loss calculation accounts for multiple voices, matching each predicted voice to the closest ground truth voice. This approach allows for flexibility in voice ordering while still maintaining accuracy in the overall pitch content.</p><p id="f18f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">4. Instrument Loss</strong></p><p id="c90a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The instrument loss evaluates the accuracy of instrument selection for each voice.</p><p id="7a60" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This function considers exact matches, instruments from the same family, and uses string similarity for more nuanced comparisons. It provides a comprehensive assessment of how well the model identifies and assigns instruments to each voice.</p><p id="3c23" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">5. Combining the Losses</strong></p><p id="ac30" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The final loss is a weighted combination of these individual components:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="74fc" class="qi nz fq qf b bg qj qk l ql qm">total_loss = (0.5 * pitch_loss +<br/> 0.15 * metre_loss +<br/> 0.15 * tempo_loss +<br/> 0.2 * instrument_loss)</span></pre><p id="9ce3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This weighting scheme prioritises pitch accuracy while still considering other important aspects of music transcription.</p><h1 id="5918" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Training and hyperparameters</h1><p id="076d" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">PPO training generally requires a lot more memory than SFT for a few reasons:</p><ol class=""><li id="65e8" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qn pb pc bk">Multiple policy evaluations — PPO needs to maintain both the current policy (model weights) and an “old” policy to compute the probability ratio between them. This effectively doubles the model parameters in memory.</li><li id="fa22" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qn pb pc bk">Experience buffer — PPO stores a buffer of experiences (states, actions, rewards, etc.) to perform updates in mini-batches. This buffer can be quite large and takes significant memory.</li><li id="21a6" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qn pb pc bk">Advantage estimation — Computing advantages requires keeping track of value estimates and returns across trajectories, adding another layer of memory overhead.</li><li id="e525" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qn pb pc bk">Additional optimisation objectives — PPO tracks multiple loss components (policy loss, value loss, entropy bonus) and their gradients, whereas SFT has a single loss.</li></ol><p id="b1b9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Because of the above, we’re more limited than SFT in the size of the models we can train and how much it costs. Whereas the above training I could do on an A100 40GB in Colab, for the PPO training I needed more memory. I trained on an H100 80GB, which could train a LoRA with a rank of 128 and a batch size of 8.</p><p id="b84f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">My hyperparameter sweep was narrow, I went with what seemed most intuitive using batch sizes ranging from 1 to 16 and learning rates from 2e-5 to 2e-4.</p><p id="4bcd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The model made no improvements to the task. The text file with the results is <a class="af oz" href="http://asdf" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="add0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I tracked various training metrics using Weights &amp; Biases (WandB). Key metrics included the policy loss, value loss, total loss, KL divergence, and the reward model’s score.</p><p id="bca2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For all hyperparameter runs, the logs no improvement in the rewards and loss calculated over time. The KL divergence remained within the pre-defined threshold.</p></div></div><div class="mr"><div class="ab cb"><div class="lm qt ln qu lo qv cf qw cg qx ci bh"><div class="mm mn mo mp mq ab ke"><figure class="lb mr qy qz ra rb rc paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/891e605d5da483ab80fff11d72e18d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*OELUy92YHhPMK1A0nMB5Fw.png"/></div></figure><figure class="lb mr qy qz ra rb rc paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/de9c931b5e76c6c8b11bbaa5f855d234.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*TFNRTXXQJHizvcqmR_13xA.png"/></div></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="bf9a" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Conclusion</h1><p id="9600" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">While this initial experiment didn’t achieve the desired performance in music transcription, we’ve provided some groundwork for future developments in the space. The challenges encountered have provided valuable insights into both the technical requirements and potential approaches for tackling this complex task. Future work could explore several promising directions:</p><ul class=""><li id="66c0" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">Experimenting with larger pre-trained models as they become available</li><li id="f323" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Expanding the training dataset with more diverse musical examples</li><li id="4d69" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Further refinement of the reward functions to capture more nuanced musical relationships</li><li id="9b2d" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">Exploring hybrid approaches that combine traditional music processing techniques with language model capabilities</li></ul><p id="7d43" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af oz" href="https://colab.research.google.com/drive/1lpPfn9EFE2rBsasIJNv8Cy9qTvtfXzq-" rel="noopener ugc nofollow" target="_blank">Here’s my notebook</a> for running these experiments with Qwen2-Audio! and here’s a link to <a class="af oz" href="https://github.com/jonflynng" rel="noopener ugc nofollow" target="_blank">my github</a> with all notebooks.</p></div></div></div></div>    
</body>
</html>