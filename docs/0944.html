<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Learning Generalist Models for Anomaly Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Learning Generalist Models for Anomaly Detection</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-generalist-models-for-anomaly-detection-53d7a6a74474?source=collection_archive---------5-----------------------#2024-04-14">https://towardsdatascience.com/learning-generalist-models-for-anomaly-detection-53d7a6a74474?source=collection_archive---------5-----------------------#2024-04-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@guansong-pang?source=post_page---byline--53d7a6a74474--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Guansong Pang" class="l ep by dd de cx" src="../Images/449255d53cd68926a62460772561ac24.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*8RW-VCkpDSFMP6Xsrua05g.jpeg"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--53d7a6a74474--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://medium.com/@guansong-pang?source=post_page---byline--53d7a6a74474--------------------------------" rel="noopener follow">Guansong Pang</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--53d7a6a74474--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx ko kp ab q ee kq kr" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="ks"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="kt k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ku an ao ap hr kv kw kx" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ky cn"><div class="l ae"><div class="ab cb"><div class="kz la lb lc ld le ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ku an ao ap hr lf lg kr lh li lj lk ll s lm ln lo lp lq lr ls u lt lu lv"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ku an ao ap hr lf lg kr lh li lj lk ll s lm ln lo lp lq lr ls u lt lu lv"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ku an ao ap hr lf lg kr lh li lj lk ll s lm ln lo lp lq lr ls u lt lu lv"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><blockquote class="lw lx ly"><p id="7ce2" class="lz ma mb mc b md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx fj bk">Generalist Anomaly Detection (GAD) aims to train one single detection model that can generalize to detect anomalies in diverse datasets from different application domains without any further training on the target data.<br/>Work to be published at CVPR 2024 [1].</p></blockquote><h1 id="68fc" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv bk">Overview</h1><p id="69b1" class="pw-post-body-paragraph lz ma fq mc b md nw mf mg mh nx mj mk ml ny mn mo mp nz mr ms mt oa mv mw mx fj bk">Some recent studies have showed that large pre-trained Visual-Language Models (VLMs) like CLIP have strong generalization capabilities on detecting industrial defects from various datasets, but their methods rely heavily on handcrafted text prompts about defects, making them difficult to generalize to anomalies in other applications, e.g., medical image anomalies or semantic anomalies in natural images. <br/>In this work, we propose to train a GAD model with few-shot normal images as sample prompts for AD on diverse datasets on the fly. To this end, we introduce a novel approach that learns an <strong class="mc fr">in</strong>-<strong class="mc fr">c</strong>on<strong class="mc fr">t</strong>ext <strong class="mc fr">r</strong>esidual <strong class="mc fr">l</strong>earning model for GAD, termed <strong class="mc fr">InCTRL</strong>.<br/>It is trained on an auxiliary dataset to discriminate anomalies from normal samples based on a holistic evaluation of the residuals between query images and few-shot normal sample prompts. Regardless of the datasets, per definition of anomaly, larger residuals are expected for anomalies than normal samples, thereby enabling InCTRL to generalize across different domains without further training.</p><p id="e42d" class="pw-post-body-paragraph lz ma fq mc b md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx fj bk">Comprehensive experiments on nine AD datasets are performed to establish a GAD benchmark that encapsulate the detection of industrial defect anomalies, medical anomalies, and semantic anomalies in both one-vs-all and multi-class setting, on which InCTRL is the best performer and significantly outperforms state-of-the-art competing methods. Code is available at <a class="af ob" href="https://github.com/mala-lab/InCTRL" rel="noopener ugc nofollow" target="_blank">https://github.com/mala-lab/InCTRL</a>.</p><h1 id="0fe8" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv bk">Introduction</h1><p id="90c7" class="pw-post-body-paragraph lz ma fq mc b md nw mf mg mh nx mj mk ml ny mn mo mp nz mr ms mt oa mv mw mx fj bk">Anomaly Detection (AD) is a crucial computer vision task that aims to detect samples that substantially deviate from the majority of samples in a dataset, due to its broad real-life applications such as industrial inspection, medical imaging analysis, and scientific discovery, etc. [2–3]. Current AD paradigms are focused on individually building one model on the training data, e.g.,, a set of anomaly-free samples, of each target dataset, such as data reconstruction approach, one-class classification, and knowledge distillation approach. Although these approaches have shown remarkable detection performance on various AD benchmarks, they require the availability of large training data and the skilled detection model training per dataset. Thus, they become infeasible in application scenarios where training on the target dataset is not allowed due to either data privacy issues, e.g., arising from using those data in training the models due to machine unlearning [3], or unavailability of large-scale training data in the deployment of new applications. To tackle these challenges, this work explores the problem of learning <em class="mb">Generalist Anomaly Detection (GAD)</em> models, aiming to train one single detection model that can generalize to detect anomalies in diverse datasets from different application domains without any training on the target data.</p><p id="f440" class="pw-post-body-paragraph lz ma fq mc b md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx fj bk">Being pre-trained on web-scale image-text data, large Visual-Language Models (VLMs) like CLIP have exhibited superior generalization capabilities in recent years, achieving accurate visual recognition across different datasets without any fine-tuning or adaptation on the target data. More importantly, some very recent studies (e.g., WinCLIP [5]) show that these VLMs can also be utilized to achieve remarkable generalization on different defect detection datasets. Nevertheless, a significant limitation of these models is their dependency on a large set of manually crafted prompts specific to defects. This reliance restricts their applicability, making it challenging to extend their use to detecting anomalies in other data domains, e.g., medical image anomalies or semantic anomalies in one-vs-all or multi-class settings.</p><p id="fa64" class="pw-post-body-paragraph lz ma fq mc b md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx fj bk">To address this problem, we propose to train a GAD model that aims to utilize few-shot normal images from any target dataset as sample prompts for supporting GAD on the fly, as illustrated in Figure 1(Top). The few-shot setting is motivated by the fact that it is often easy to obtain few-shot normal images in real-world applications. Furthermore, these few-shot samples are not used for model training/tuning; they are just used as sample prompts for enabling the anomaly scoring of test images during inference. This formulation is fundamentally different from current few-shot AD methods that use these target samples and their extensive augmented versions to train the detection model, which can lead to an overfitting of the target dataset and fail to generalize to other datasets, as shown in Figure 1(Bottom).</p><figure class="of og oh oi oj ok oc od paragraph-image"><div role="button" tabindex="0" class="ol om ed on bh oo"><div class="oc od oe"><img src="../Images/3fe0dead46f46cfcbebafc24c90de4c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U98AfFgNfMrUfHz-K2jdCA.png"/></div></div></figure><p id="ca0d" class="pw-post-body-paragraph lz ma fq mc b md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx fj bk">We then introduce an GAD approach, the first of its kind, that learns an <strong class="mc fr">in</strong>-<strong class="mc fr">c</strong>on<strong class="mc fr">t</strong>ext <strong class="mc fr">r</strong>esidual <strong class="mc fr">l</strong>earning model based on CLIP, termed <strong class="mc fr">InCTRL</strong>. It trains an GAD model to discriminate anomalies from normal samples by learning to identify the residuals/discrepancies between query images and a set of few-shot normal images from auxiliary data. The few-shot normal images, namely <strong class="mc fr">in-context sample prompts</strong>, serve as <em class="mb">prototypes of normal patterns</em>. When comparing with the features of these normal patterns, per definition of anomaly, a larger residual is typically expected for anomalies than normal samples in datasets of different domains, so the learned in-context residual model can generalize to detect diverse types of anomalies across the domains. To capture the residuals better, <strong class="mc fr">InCTRL</strong> models the in-context residuals at both the image and patch levels, gaining an in-depth in-context understanding of what constitutes an anomaly. Further, our in-context residual learning can also enable a seamless incorporation of normal/abnormal text prompt-guided prior knowledge into the detection model, providing an additional strength for the detection from the text-image-aligned semantic space.</p><p id="a09f" class="pw-post-body-paragraph lz ma fq mc b md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx fj bk">Extensive experiments on nine AD datasets are performed to establish a GAD benchmark that encapsulates three types of popular AD tasks, including industrial defect anomaly detection, medical image anomaly detection, and semantic anomaly detection under both one-vs-all and multi-class settings. Our results show that <strong class="mc fr">InCTRL </strong>significantly surpasses existing state-of-the-art methods.</p><h1 id="6e51" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv bk">Approach</h1><p id="b96b" class="pw-post-body-paragraph lz ma fq mc b md nw mf mg mh nx mj mk ml ny mn mo mp nz mr ms mt oa mv mw mx fj bk">Our approach <strong class="mc fr">InCTRL </strong>is designed to effectively model the in-context residual between a query image and a set of few-shot normal images as sample prompts, utilizing the generalization capabilities of CLIP to detect unusual residuals for anomalies from different application domains.<br/>CLIP is a VLM consisting of a text encoder and a visual encoder, with the image and text representations from these encoders well aligned by pre-training on web-scale text-image data. <strong class="mc fr">InCTRL</strong> is optimized using auxiliary data via an in-context residual learning in the image encoder, with the learning augmented by text prompt-guided prior knowledge from the text encoder.</p><p id="b8a0" class="pw-post-body-paragraph lz ma fq mc b md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx fj bk">To be more specific, as illustrated in Fig.2, we first simulate an in-context learning example that contains one query image <strong class="mc fr">x</strong> and a set of few-shot normal sample prompts <strong class="mc fr">P’</strong>, both of which are randomly sampled from the auxiliary data. Through the visual encoder, we then perform multi-layer patch-level and image-level residual learning to respectively capture local and global discrepancies between the query and few-shot normal sample prompts. Further, our model allows a seamless incorporation of normal and abnormal text prompts-guided prior knowledge from the text encoder based on the similarity between these textual prompt embeddings and the query images . The training of <strong class="mc fr">InCTRL</strong> is to optimize a few projection/adaptation layers attached to the visual encoder to learn a larger anomaly score for anomaly samples than normal samples in the training data, with the original parameters in both encoders frozen; during inference, a test image, together with the few-shot normal image prompts from the target dataset and the text prompts, is put forward through our adapted CLIP-based GAD network, whose output is the anomaly score for the test image.</p><figure class="of og oh oi oj ok oc od paragraph-image"><div role="button" tabindex="0" class="ol om ed on bh oo"><div class="oc od oq"><img src="../Images/b71f47a82528a9d0f1aa0512687df9c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3XD-qxGGoIe1M8CP5nO-Iw.png"/></div></div></figure><h1 id="9f38" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv bk">Empirical Results</h1><p id="e3cb" class="pw-post-body-paragraph lz ma fq mc b md nw mf mg mh nx mj mk ml ny mn mo mp nz mr ms mt oa mv mw mx fj bk"><strong class="mc fr">Datasets and Evaluation Metrics. </strong>To verify the efficiency of our method, we conduct comprehensive experiments across nine real-world AD datasets, including five industrial defect inspection dataset (MVTec AD, VisA, AITEX, ELPV, SDD), two medical image datasets (BrainMRI, HeadCT), and two semantic anomaly detection datasets: MNIST and CIFAR-10 under both one-vs-all and multi-class protocols. Under the one-vs-all protocol, one class is used as normal, with the other classes treated as abnormal; while under the multi-class protocol, images of even-number classes from MNIST and animal-related classes from CIFAR-10 are treated as normal, with the images of the other classes are considered as anomalies.</p><p id="c0b2" class="pw-post-body-paragraph lz ma fq mc b md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx fj bk">To assess the GAD performance, MVTec AD, the combination of its training and test sets, is used as the auxiliary training data, on which GAD models are trained, and they are subsequently evaluated on the test set of the other eight datasets without any further training. We train the model on VisA when evaluating the performance on MVTec AD. <br/>The few-shot normal prompts for the target data are randomly sampled from the training set of target datasets and remain the same for all models for fair comparison. We evaluate the performance with the number of few-shot normal prompt set to K = 2, 4, 8. The reported results are averaged over three independent runs with different random seeds.</p><p id="3d5c" class="pw-post-body-paragraph lz ma fq mc b md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx fj bk">As for evaluation metrics, we use two popular metrics AUROC (Area Under the Receiver Operating Characteristic) and AUPRC (Area Under the Precision-Recall Curve) to evaluate the AD performance.</p><p id="d1b6" class="pw-post-body-paragraph lz ma fq mc b md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx fj bk"><strong class="mc fr">Results. </strong>The main results are reporeted in Tables 1 and 2. For the 11 industrial defect AD datasets, <strong class="mc fr">InCTRL </strong>significantly outperforms all competing models on almost all cases across the three few-shot settings in both AUROC and AUPRC. With more few-shot image prompts, the performance of all methods generally gets better. <strong class="mc fr">InCTRL </strong>can<strong class="mc fr"> </strong>utilize the increasing few-shot samples well and remain the superiority over the competing methods.</p><figure class="of og oh oi oj ok oc od paragraph-image"><div role="button" tabindex="0" class="ol om ed on bh oo"><div class="oc od oq"><img src="../Images/3315761a9681154f009c7313b51c54e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jumK2fIvjt7KvWyxaGZKzA.png"/></div></div></figure><figure class="of og oh oi oj ok oc od paragraph-image"><div role="button" tabindex="0" class="ol om ed on bh oo"><div class="oc od oq"><img src="../Images/541a7bcbe2af78ff4e3fb584a8151144.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sFw4YZo6iJFizSBSp5hA2A.png"/></div></div></figure><p id="2f0d" class="pw-post-body-paragraph lz ma fq mc b md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx fj bk"><strong class="mc fr">Ablation Study.</strong> We examine the contribution of three key components of our approach on the generalization: text prompt-guided features (T), patch-level residuals (P), and image-level residuals (I), as well as their combinations. The results are reported in Table 3. The experiment results indicate that for industrial defect AD datasets, visual residual features play a more significant role compared to text prompt-based features, particularly on datasets like ELPV, SDD, and AITEX. On the medical image AD datasets, both visual residuals and textual knowledge contribute substantially to performance enhancement, exhibiting a complementary relation. On semantic AD datasets, the results are dominantly influenced by patch-level residuals and/or text prompt-based features. Importantly, our three components are generally mutually complementary, resulting in the superior detection generalization across the datasets.</p><figure class="of og oh oi oj ok oc od paragraph-image"><div role="button" tabindex="0" class="ol om ed on bh oo"><div class="oc od oq"><img src="../Images/4abc7255039bcdccd2675268acada161.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jjx2K_KNg9GfHBOwu_8E2Q.png"/></div></div></figure><p id="3ec4" class="pw-post-body-paragraph lz ma fq mc b md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx fj bk"><strong class="mc fr">Significance of In-context Residual Learning.</strong> To assess the importance of learning the residuals in <strong class="mc fr">InCTRL</strong>, we experiment with two alternative operations in both multi-layer patch-level and image-level residual learning: replacing the residual operation with 1) a <strong class="mc fr">concatenation</strong> operation and 2) an <strong class="mc fr">average</strong> operation, with all the other components of <strong class="mc fr">InCTRL </strong>fixed. As shown in Table 3, the in-context residual learning generalizes much better than the other two alternative ways, significantly enhancing the model’s performance in GAD across three distinct domains.</p><h1 id="9c8c" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv bk">Conclusion</h1><p id="438f" class="pw-post-body-paragraph lz ma fq mc b md nw mf mg mh nx mj mk ml ny mn mo mp nz mr ms mt oa mv mw mx fj bk">In this work we introduce a GAD task to evaluate the generalization capability of AD methods in identifying anomalies across various scenarios without any training on the target datasets. This is the first study dedicated to a generalist approach to anomaly detection, encompassing industrial defects, medical anomalies, and semantic anomalies. Then we propose an approach, called <strong class="mc fr">InCTRL</strong>, to addressing this problem under a few-shot setting. <strong class="mc fr">InCTRL </strong>achieves a superior GAD generalization by holistic in-context residual learning. Extensive experiments are performed on nine AD datasets to establish a GAD evaluation benchmark for the aforementioned three popular AD tasks, on which <strong class="mc fr">InCTRL </strong>significantly and consistently outperforms SotA competing models across multiple few-shot settings.</p><p id="458d" class="pw-post-body-paragraph lz ma fq mc b md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx fj bk"><em class="mb">Please check out the full paper [1] for more details of the approach and the experiments. Code is publicly available at </em><a class="af ob" href="https://github.com/mala-lab/InCTRL" rel="noopener ugc nofollow" target="_blank">https://github.com/mala-lab/InCTRL</a>.</p><h1 id="ec2f" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv bk">References</h1><p id="051b" class="pw-post-body-paragraph lz ma fq mc b md nw mf mg mh nx mj mk ml ny mn mo mp nz mr ms mt oa mv mw mx fj bk">[1] Zhu, Jiawen, and Guansong Pang. “Toward Generalist Anomaly Detection via In-context Residual Learning with Few-shot Sample Prompts.” <em class="mb">arXiv preprint arXiv:2403.06495</em> (2024).<br/>[2] Pang, Guansong, et al. “Deep learning for anomaly detection: A review.” <em class="mb">ACM computing surveys (CSUR)</em> 54.2 (2021): 1–38.<br/>[3] Cao, Yunkang, et al. “A Survey on Visual Anomaly Detection: Challenge, Approach, and Prospect.” <em class="mb">arXiv preprint arXiv:2401.16402</em> (2024).<br/>[4] Xu, Jie, et al. “Machine unlearning: Solutions and challenges.” <em class="mb">IEEE Transactions on Emerging Topics in Computational Intelligence</em> (2024).<br/>[5] Jeong, Jongheon, et al. “Winclip: Zero-/few-shot anomaly classification and segmentation.” <em class="mb">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2023.</p></div></div></div></div>    
</body>
</html>