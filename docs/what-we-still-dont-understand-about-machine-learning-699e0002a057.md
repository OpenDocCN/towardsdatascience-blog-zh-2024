# 我们仍然不理解的机器学习问题

> 原文：[https://towardsdatascience.com/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=collection_archive---------1-----------------------#2024-07-26](https://towardsdatascience.com/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=collection_archive---------1-----------------------#2024-07-26)

## 机器学习中的未知问题，研究人员难以理解——从批量归一化到SGD隐藏的奥秘

[](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)[![Hesam Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------) [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------) ·阅读时间12分钟·2024年7月26日

--

![](../Images/1a58ae922fc1a99ffd5a8d87485bad57.png)

我们仍然不理解的机器学习问题。（作者）

> 如果你还不是会员，[点击这里免费阅读](/what-we-still-dont-understand-about-machine-learning-699e0002a057?sk=e2498900bd4ada1e8abe37e056f6ebc9) 👈

令人惊讶的是，机器学习中的一些基础主题至今仍然是研究人员未解之谜，尽管它们是基本的且常用的，但似乎仍然神秘莫测。机器学习的一个有趣之处在于，我们构建了有效的系统，然后才去弄清楚它们为何有效！

在这里，我旨在探讨一些机器学习概念中的未知领域，以展示尽管这些想法看起来基础，实际上它们是通过一层层抽象构建而成的。这有助于我们练习质疑**我们知识的深度**。

在本文中，我们探讨了深度学习中的几个关键现象，这些现象挑战了我们对神经网络的传统理解。

+   我们从**批量归一化**开始，并讨论其尚未完全理解的基本机制。

+   我们考察了一个反直觉的观察结果，即**过参数化**模型往往**具有更好的泛化能力**，这与经典的机器学习理论相矛盾。

+   我们探索了**梯度下降**的隐式正则化效应，这似乎自然地使神经网络倾向于更简单、更具普适性的解决方案。

+   最后，我们讨论了彩票票假说（Lottery Ticket Hypothesis），该假说提出，大型神经网络包含较小的子网络，当它们被单独训练时，能够实现**相当的性能**。

*本文包含4个部分，它们之间没有直接关联，所以可以随意跳到你更感兴趣的部分。*

· [1\. 批量归一化](#de8b)

∘ [我们不理解的东西](#95c8)

∘ [批量归一化的注意事项](#ec44)

· [2\. 过度参数化与泛化](#82b3)

∘ [理解深度学习需要重新思考泛化](#6185)

· [3\. 神经网络中的隐式正则化](#f626)

∘ [实验 1](#2783)

∘ [实验 2](#ff53)

∘ [梯度下降作为一种自然正则化器](#85f8)

· [4\. 彩票票假设](#b35e)

· [最后的话。](#821e)

∘ [让我们连接！](#03ee)

· [进一步阅读](#397f)

· [参考文献](#f3e3)

# 1\. 批量归一化

由*谢尔盖·约夫*和*克里斯蒂安·谢格迪*于2015年提出的[1]，批量归一化是一种加速神经网络训练并提高其稳定性的方法。之前已知，将输入数据的均值调整为零、方差调整为一，可以实现更快的收敛。作者进一步使用这一思想，提出了**批量归一化**，以使隐藏层的输入具有零均值和单位方差。

![](../Images/3d64e4c77a25558d72ba330536db8ba1.png)

一个概述ResNet中使用的残差块以及其中批量归一化的应用。（作者提供）

自其提出以来，批量归一化在神经网络中变得十分常见。其中一个例子，就是它在著名的ResNet架构中的应用。因此我们可以自信地说，它的有效性是毋庸置疑的。

一项有趣的研究[2]表明，尽管训练完整的ResNet-866网络达到了**93**%的准确率，但冻结所有参数，仅训练批量归一化层的参数，结果只达到了**83**%的准确率——仅有10%的差异。

批量归一化有三个方面的好处：

+   通过对每一层的输入进行归一化，它**加速**了训练过程。

+   它减少了对**初始条件**的敏感性，这意味着我们不需要非常小心地初始化权重。

+   批量归一化还充当了**正则化器**。它提高了模型的泛化能力，在某些情况下，减少了对其他正则化技术的需求。

## 我们不理解的东西

尽管批量归一化的正面效果显而易见，但没人真正理解其背后的有效性原因。最初，批量归一化论文的作者将其提出作为解决**内部协变量偏移**问题的方法。

*关于内部协变量偏移的一个提示是，你会发现它有各种不同的定义，乍一看似乎没有关联。* ***这是我想要定义它的方式。***

神经网络的层是在反向传播过程中从输出层（结束）更新到输入层（开始）的。内部协变量偏移指的是在训练过程中，由于前一层参数的更新，输入层的分布发生变化的现象。

> 当我们改变前面几层的参数时，也会改变后续层的输入分布，而这些后续层已经更新，以更好地适应较旧的分布。

内部协变量偏移（internal covariate shift）会减缓训练过程，并使网络更难以收敛，因为每一层必须不断适应前一层更新引入的输入分布的变化。

原始批量归一化论文的作者认为，其有效性的原因在于缓解了内部协变量偏移问题。然而，后来的一篇论文[3]认为，批量归一化的成功与内部协变量偏移无关，而是由于**平滑优化景观**的作用。

![](../Images/2f88c5bb3a672aa66fcd7373c0c431a1.png)

两种损失景观的比较：一个是高度粗糙和陡峭的损失表面（左），另一个是较为平滑的损失表面（右）。([Source](https://arxiv.org/abs/1712.09913))

上图来自[4]，该文并非关于批量归一化的内容，但很好地展示了平滑损失景观的样子。然而，批量归一化通过平滑损失景观而有效的理论本身也存在挑战和疑问。

## 批量归一化的注意事项

由于我们对批量归一化工作原理的理解有限，以下是在网络中使用它时需要考虑的事项：

+   使用批量归一化时，**训练（train）**和**推理（inference）**模式是不同的。使用错误的模式可能导致难以识别的意外行为。[5]

+   批量归一化（Batch Norm）在很大程度上依赖于**小批量（minibatch）**的大小。因此，虽然它减少了对谨慎初始化权重的需求，但选择合适的小批量大小变得更加关键。[5]

+   关于在激活函数之前还是之后使用批量归一化，目前仍存在争论。[6]

+   虽然批量归一化具有**正则化（regularizer）**的效果，但它与其他正则化方法（如*丢弃法（dropout）*或*权重衰减（weight decay）*）的相互作用尚不完全明确。

关于批量归一化仍有许多问题待解答，但研究仍在进行，以揭示这些层如何影响神经网络。

# 2\. 过度参数化与泛化

![](../Images/c4c6c3f08203f8d9473bd7ce0197c417.png)

面部识别实验表明，网络中权重的最优数量可以远大于数据点的数量。([source](https://clgiles.ist.psu.edu/papers/UMD-CS-TR-3617.what.size.neural.net.to.use.pdf))

大型网络挑战了我们过去对神经网络工作原理的理解。

传统上认为，使用过度参数化的模型会导致过拟合。因此，解决方案通常是限制网络的大小，或添加正则化以防止过拟合训练数据。

出人意料的是，在神经网络的情况下，使用更大的网络可能会改善**泛化误差**（|训练误差 - 测试误差|）。换句话说，更大的网络有更好的泛化能力。[7] 这与传统的复杂度度量标准（如[VC维度](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension)——一个量化从样本中学习难度的指标）所宣称的内容相矛盾。[8]

这一理论还挑战了一个关于深度神经网络（DNN）是否通过*记忆*训练数据来实现其性能的辩论。[9] 如果它们是通过记忆数据来做的，那它们怎么可能泛化到预测未见过的数据呢？如果它们不记忆数据，而只是学习数据中的潜在模式，那它们又是如何预测正确的标签，即使我们给标签引入了一定的噪声？

![](../Images/d8dbde067c84e6356a774b2dcf646f1f.png)

分类器的过拟合。（放大图——[来源](https://commons.wikimedia.org/wiki/File:Overfitting.svg)）

## *理解深度学习需要重新思考泛化能力*

关于这一主题的有趣论文是*理解深度学习需要重新思考泛化能力*[10]。作者认为，传统的方法无法解释为什么更大的网络能够良好泛化，同时这些网络也能拟合随机数据。

本文的一部分重点解释了**显式正则化**（如权重衰减、丢弃法和数据增强）在泛化误差中的作用：

> 显式正则化可能会改善泛化性能，但既不是必要的，也不是单独足以控制泛化误差。L2正则化（权重衰减）有时甚至有助于优化，说明它在深度学习中的理解仍然不完全。[10]

即使使用了丢弃法和权重衰减，InceptionV3仍然能够非常好地拟合**随机**训练集，超出了预期。这一含义并不是贬低正则化，而是更强调通过改变模型**架构**可以获得更大的收益。

![](../Images/e203216819fb3accb40cefdf6557f216.png)

正则化对泛化能力的影响。[10]

那么，是什么让一个能够良好泛化的神经网络与那些泛化不良的网络不同呢？这似乎是一个“兔子洞”。我们仍然需要重新思考一些问题：

+   我们对模型**有效容量**的理解。

+   我们对模型复杂度和大小的衡量。模型参数或FLOP是否仅仅是好的度量标准？显然不是。

+   泛化能力的定义以及如何衡量它。

![](../Images/cd130cd6128053c768b899942736cb1f.png)

随着网络规模（H）的不断增大，训练误差和测试误差持续下降，并且没有发生过拟合。[11]

关于大规模网络和参数数量对泛化能力的影响，已有大量论文和博客文章，其中一些甚至互相矛盾。

我们目前的理解可能表明，尽管较大的网络倾向于过拟合，但它们仍能较好地泛化。这可能是由于它们的深度，使得它们能够学习比浅层网络更复杂的模式。这主要取决于领域——某些数据类型可能更适合使用较小的模型，并遵循奥卡姆剃刀原理（不要错过这篇文章以进一步阅读👇）。

# 3\. 神经网络中的隐式正则化

机器学习的核心是梯度下降——我们为了在损失景观中找到局部最小值而采取的步骤。梯度下降（GD）以及随机梯度下降（SGD）是任何开始学习机器学习的人首先理解的算法。

尽管算法看起来简单，可能有人认为它没有太多深度。然而，在机器学习中，你永远无法找到池塘的底部。

神经网络是否从梯度下降的隐式正则化中受益，这种正则化推动它们找到*更简单*和*更通用*的解决方案？这是否是之前提到的过参数化网络能够泛化的原因？

![](../Images/304be222467e499f2e5c6c5d56844e50.png)

二维中的梯度下降。（来源：[Wikimedia Commons](https://en.wikipedia.org/wiki/File:Gradient_Descent_in_2D.webm)）

有两个实验需要特别注意：

## 实验1

当[11]的作者使用SGD并且没有显式正则化，训练CIFAR-10和MNIST数据集的模型时，他们得出结论：随着网络大小的增加，测试和训练误差不断减少。这与认为较大的网络由于过拟合而有更高测试误差的观点相悖。即使在网络中添加更多的参数，泛化误差也没有增加。随后，他们通过添加随机标签噪声强制网络过拟合。如下面的图所示，即使标签随机噪声达到5%，测试误差仍然进一步降低，而且没有明显的过拟合迹象。

![](../Images/99f78afda3879ffd5d8b592f2199a524.png)

网络随着大小（H）增加并且标签噪声为5%的测试和训练误差。左侧是MNIST，右侧是CIFAR-10。[11]

## 实验2

一篇重要的论文，*寻找真实的归纳偏置* [12]，通过拟合一个使用线性可分数据集的预测器进行实验。作者展示了如何在没有正则化的情况下，使用梯度下降的逻辑回归本能地将解偏向于最大间隔分离器（也称为硬间隔SVM）。这是梯度下降的一个有趣且令人惊讶的行为。因为即使损失和优化**并未直接涉及**任何鼓励最大化间隔的项（比如在支持向量机中找到的那些项），梯度下降本能地将解偏向于最大间隔分类器。

![](../Images/67d2bde1fbdf0706ff73b0f3baeddee9.png)

H3表示硬间隔支持向量机（SVM）如何对数据集进行分类。（来源：[Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Svm_separating_hyperplanes_(SVG).svg)）

## 梯度下降作为一种自然的正则化方法

实验表明存在一种隐式正则化效应，优化过程似乎偏向更简单和更稳定的解决方案。这意味着梯度下降（GD）更倾向于简单的模型，通常会收敛到一种特殊类型的局部最小值，即“平坦”最小值，相比于更尖锐的最小值，平坦最小值通常具有**较低的泛化误差**。这有助于解释为何深度学习模型在实际任务中往往表现良好，即使在超出训练数据的情况下。这表明优化过程本身可以视为一种隐式正则化，导致模型不仅在训练数据上最小化误差，而且在预测未见数据时也表现出鲁棒性。对此的完整理论解释仍然是一个活跃的研究领域。

或许这篇文章也会对你有趣，讲述了深度神经网络是如何以及为何趋向于统一的现实表示：

[](/platonic-representation-hypothesis-c812813d7248?source=post_page-----699e0002a057--------------------------------) [## 柏拉图表示法：AI深度网络模型是否正在趋同？

### 人工智能模型是否正在朝向统一的现实表示发展？柏拉图表示法……

[towardsdatascience.com](/platonic-representation-hypothesis-c812813d7248?source=post_page-----699e0002a057--------------------------------)

# 4. 彩票票据假说

模型剪枝可以通过减少训练好的神经网络的参数达到90%。如果操作得当，可以在不降低准确度的情况下实现这一点。但你只能在模型训练完成后**进行**剪枝。如果我们能够在训练前移除多余的参数，这意味着可以使用更少的时间和资源。

彩票票据假说[13]认为，神经网络包含一些子网络，当它们单独训练时，可以达到与原始网络相当的测试准确度。这些子网络——*中奖的票据*，拥有使其训练成功的初始权重——*彩票*。

作者通过**迭代剪枝**方法找到了这些子网络：

+   **训练网络**：首先，训练原始的未剪枝网络。

+   **剪枝**：训练后，剪枝**p%**的权重。

+   **重置权重**：剩余的权重被重置为初始初始化时的原始值。

+   **再训练**：剪枝后的网络被重新训练，以查看它是否能达到与之前的网络相同或更高的性能。

+   **重复**：直到达到原始网络期望的稀疏度，或者剪枝后的网络无法再与未剪枝网络匹敌时，才会停止此过程。

![](../Images/40897b86443a85999f980a18cdfaea41.png)

迭代剪枝在《彩票票据假说》论文中的应用。（作者）

提出的迭代训练方法在计算上非常昂贵，要求在多个实验中训练一个网络15次或更多次。

为什么在神经网络中会出现这种现象仍是一个研究领域。是否有可能是SGD在训练网络时只关注成功的网络部分，而不是网络的全部？为什么某些随机初始化会包含如此高效的子网络？如果你想深入探讨这个理论，不要错过[13]和[14]。

# 最后的话。

*感谢您阅读本文！* 我已经尽力提供一篇准确的文章，但如果您认为需要修改，请分享您的意见和建议。

## 让我们保持联系！

*免费订阅以接收新文章通知！你还可以在* [*LinkedIn*](https://www.linkedin.com/in/hesamsheikh/) *和* [*Twitter*](https://x.com/itsHesamSheikh)* 上找到我。*

# 进一步阅读

如果你已经看到这里，你可能也会对以下文章感兴趣：

[](https://pub.towardsai.net/learn-anything-with-ai-and-the-feynman-technique-00a33f6a02bc?source=post_page-----699e0002a057--------------------------------) [## 利用AI和费曼技巧学习任何东西

### 通过应用AI和诺贝尔奖得主的方法，分四个简单步骤学习任何概念

[pub.towardsai.net](https://pub.towardsai.net/learn-anything-with-ai-and-the-feynman-technique-00a33f6a02bc?source=post_page-----699e0002a057--------------------------------) [](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----699e0002a057--------------------------------) [## 实践中的协作AI代理全面指南

### 定义，并建立一个代理团队，精炼你的简历和求职信

[towardsdatascience.com](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----699e0002a057--------------------------------) [](https://pub.towardsai.net/chatgpt-as-a-game-engine-to-play-flappy-bird-ee4adff46f48?source=post_page-----699e0002a057--------------------------------) [## 我在ChatGPT中玩了Flappy Bird

### GPT-4非常棒，但它足够好，能够作为游戏引擎吗？我用一个简单的LangChain尝试了这个，做了一个Flappy Bird的游戏……

[pub.towardsai.net](https://pub.towardsai.net/chatgpt-as-a-game-engine-to-play-flappy-bird-ee4adff46f48?source=post_page-----699e0002a057--------------------------------)

# 参考文献

[1] Ioffe, S., & Szegedy, C. (2015)。 《批量归一化：通过减少内部协方差偏移加速深度网络训练》。 [arXiv](https://arxiv.org/abs/1502.03167)。

[2] [超出常规](https://www.deeplearning.ai/the-batch/outside-the-norm/)，DeepLearning.AI

[3] Santurkar, Shibani; Tsipras, Dimitris; Ilyas, Andrew; Madry, Aleksander (2018年5月29日)。 “批量归一化如何帮助优化？” arXiv：[1805.11604](https://arxiv.org/abs/1805.11604)

[4] Li, H., Xu, Z., Taylor, G., Studer, C., & Goldstein, T. (2018). 可视化神经网络的损失景观。[arXiv](https://arxiv.org/abs/1712.09913)

[5] [批归一化的危险](https://www.alexirpan.com/2017/04/26/perils-batch-norm.html)

[6] [https://x.com/svpino/status/1588501331316121601](https://x.com/svpino/status/1588501331316121601)

[7] Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., & Srebro, N. (2018). 朝着理解过度参数化在神经网络泛化中的作用迈进。[arXiv](https://arxiv.org/abs/1805.12076)

[8] [为什么深度学习在糟糕的VC维度下仍然被炒作？](https://cs.stackexchange.com/questions/75327/why-is-deep-learning-hyped-despite-bad-vc-dimension)

[9] [深度网络不是通过记忆化学习的](https://openreview.net/pdf?id=rJv6ZgHYg)

[10] Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2017). 理解深度学习需要重新思考泛化问题。[*arXiv:1611.03530*](https://arxiv.org/abs/1611.03530)

[11] Neyshabur, B., Tomioka, R., & Srebro, N. (2015). 寻找真实的归纳偏差：关于隐式正则化在深度学习中的作用。*arXiv:1412.6614*

[12] Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., & Srebro, N. (2017). 梯度下降在可分数据上的隐式偏差。*arXiv:1710.10345*

[13] Frankle, J., & Carbin, M. (2019). 彩票票假设：寻找稀疏的、可训练的神经网络。*arXiv:1803.03635*

[14] [https://www.lesswrong.com/posts/Z7R6jFjce3J2Ryj44/exploring-the-lottery-ticket-hypothesis](https://www.lesswrong.com/posts/Z7R6jFjce3J2Ryj44/exploring-the-lottery-ticket-hypothesis)
