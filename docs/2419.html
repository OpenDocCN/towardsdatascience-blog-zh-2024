<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Prompt Caching in LLMs: Intuition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Prompt Caching in LLMs: Intuition</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/prompt-caching-in-llms-intuition-5cfc151c4420?source=collection_archive---------4-----------------------#2024-10-04">https://towardsdatascience.com/prompt-caching-in-llms-intuition-5cfc151c4420?source=collection_archive---------4-----------------------#2024-10-04</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1ae8" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A brief tour of how caching works in attention-based models</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@rodrigonader?source=post_page---byline--5cfc151c4420--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Rodrigo Nader" class="l ep by dd de cx" src="../Images/c1715d46ef7939ff85fc7c908e92b2f1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*T9ODfMcw6l74f192b4xDiQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5cfc151c4420--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@rodrigonader?source=post_page---byline--5cfc151c4420--------------------------------" rel="noopener follow">Rodrigo Nader</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5cfc151c4420--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">4 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 4, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/9da5fcfecad7b9e5ffa7ce5a6172ef0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*r6fL6PSsxmIXN76RKVD9Cg.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by Author using ChatGPT</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="21a7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">I’ve been exploring articles about how <em class="of">Prompt Caching</em> works, and while a few blogs touch on its usefulness and how to implement it, I haven’t found much on the actual mechanics or the intuition behind it.</p><p id="0512" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The question really comes down to this: GPT-like model generation relies on the relationships between <strong class="nl fr">every token in a prompt</strong>. <em class="of">How could caching just part of a prompt even make sense?</em></p><p id="c228" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Surprisingly, it does. Let’s dive in!</p></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="6f8b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">Prompt caching</strong> has recently emerged as a significant advancement in reducing computational overhead, latency, and cost, especially for applications that frequently reuse prompt segments.</p><p id="8dc4" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">To clarify, these are cases where you have a long, static pre-prompt (context) and keep adding new user questions to it. Each time the API model is called, it needs to completely re-process the <strong class="nl fr">entire prompt.</strong></p><p id="4aad" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">Google</strong> was the first to introduce <a class="af oo" href="https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview" rel="noopener ugc nofollow" target="_blank"><strong class="nl fr">Context Caching</strong></a> with the Gemini model, while <a class="af oo" href="https://www.anthropic.com/news/prompt-caching" rel="noopener ugc nofollow" target="_blank"><strong class="nl fr">Anthropic</strong></a> and <a class="af oo" href="https://platform.openai.com/docs/guides/prompt-caching" rel="noopener ugc nofollow" target="_blank"><strong class="nl fr">OpenAI</strong></a> have recently integrated their prompt caching capabilities, claiming great cost and latency reduction for long prompts.</p><h1 id="6297" class="op oq fq bf or os ot gq ou ov ow gt ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">What is Prompt Caching?</h1><p id="b8b6" class="pw-post-body-paragraph nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe fj bk">Prompt caching is a technique that stores parts of a prompt (such as system messages, documents, or template text) to be efficiently reused. This avoids reprocessing the same prompt structure repeatedly, improving efficiency.</p><p id="4586" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">There are multiple ways to implement Prompt Caching, so the techniques can vary by provider, but we’ll try to abstract the concept out of two popular approaches:</p><ul class=""><li id="75d0" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pq pr ps bk"><a class="af oo" href="https://aclanthology.org/2023.nlposs-1.24.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="nl fr">GPTCache: Semantic Cache for LLMs</strong></a></li><li id="b9e1" class="nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe pq pr ps bk"><a class="af oo" href="https://arxiv.org/pdf/2311.04934" rel="noopener ugc nofollow" target="_blank"><strong class="nl fr">Prompt Cache: Modular Attention Reuse</strong></a></li></ul><p id="d641" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The overall process goes as follows:</p><ol class=""><li id="e83a" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe py pr ps bk">When a prompt comes in, it goes through tokenization, vectorization, and full model inference (typically an attention model for LLMs).</li><li id="641f" class="nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe py pr ps bk">The system stores the relevant data (tokens and their embeddings) in a <strong class="nl fr">cache layer</strong> outside the model. The numerical vector representation of tokens is stored in memory.</li><li id="d8ee" class="nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe py pr ps bk">On the next call, the system checks if a part of the new prompt is already stored in the cache (e.g., based on embedding similarity).</li><li id="b2b8" class="nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe py pr ps bk">Upon a cache hit, the cached portion is retrieved, skipping both tokenization and full model inference.</li></ol><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pz"><img src="../Images/e7f90c2a611b4e1abbbfd64dac73e832.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DTcCtJBnBM4k00Uuf1OwbQ.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx"><a class="af oo" href="https://aclanthology.org/2023.nlposs-1.24.pdf" rel="noopener ugc nofollow" target="_blank">https://aclanthology.org/2023.nlposs-1.24.pdf</a></figcaption></figure><h1 id="b596" class="op oq fq bf or os ot gq ou ov ow gt ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">So… What Exactly is Cached?</h1><p id="ddea" class="pw-post-body-paragraph nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe fj bk">In its most basic form, different levels of caching can be applied depending on the approach, ranging from simple to more complex. This can include storing tokens, token embeddings, or even internal states to avoid reprocessing:</p><ul class=""><li id="54c9" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pq pr ps bk"><strong class="nl fr">Tokens</strong>: The next level involves caching the <strong class="nl fr">tokenized representation</strong> of the prompt, avoiding the need to re-tokenize repeated inputs.</li><li id="aaf8" class="nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe pq pr ps bk"><strong class="nl fr">Token Encodings</strong>: Caching these allows the model to skip <strong class="nl fr">re-encoding</strong> previously seen inputs and only process the <em class="of">new</em> parts of the prompt.</li><li id="e743" class="nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe pq pr ps bk"><strong class="nl fr">Internal States</strong>: At the most complex level, caching internal states such as key-value pairs (see below) stores <strong class="nl fr">relationships between tokens</strong>, so the model only computes <em class="of">new</em> relationships.</li></ul><h1 id="71a4" class="op oq fq bf or os ot gq ou ov ow gt ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">Caching Key-Value States</h1><p id="aac2" class="pw-post-body-paragraph nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe fj bk">In transformer models, tokens are processed in pairs: <strong class="nl fr">Keys</strong> and <strong class="nl fr">Values</strong>.</p><ul class=""><li id="52f3" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pq pr ps bk"><strong class="nl fr">Keys</strong> help the model decide how much importance or “attention” each token should give to other tokens.</li><li id="6cec" class="nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe pq pr ps bk"><strong class="nl fr">Values</strong> represent the actual content or meaning that the token contributes in context.</li></ul><p id="0bd6" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">For example, in the sentence <em class="of">“Harry Potter is a wizard, and his friend is Ron,”</em> the Key for <em class="of">“Harry”</em> is a vector with relationships with each one of the other words in the sentence:</p><p id="a904" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><code class="cx qa qb qc qd b">["Harry", "Potter"], ["Harry"", "a"], ["Harry", "wizard"], etc...</code></p><h1 id="8436" class="op oq fq bf or os ot gq ou ov ow gt ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">How KV Prompt Caching Works</h1><ol class=""><li id="a655" class="nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe py pr ps bk"><strong class="nl fr">Precompute and Cache KV States</strong>: The model computes and stores KV pairs for frequently used prompts, allowing it to skip re-computation and retrieve these pairs from the cache for efficiency.</li><li id="4c3d" class="nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe py pr ps bk"><strong class="nl fr">Merging Cached and New Context</strong>: In new prompts, the model retrieves cached KV pairs for previously used sentences while computing new KV pairs for any new sentences.</li><li id="9c55" class="nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe py pr ps bk"><strong class="nl fr">Cross-Sentence KV Computation</strong>: The model computes new KV pairs that link cached tokens from one sentence to new tokens in another, enabling a holistic understanding of their relationships.</li></ol></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qe"><img src="../Images/46f1763f48212864ed71f0fe6cb78be1.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*s35XznpNXAqdtOMTsAujOQ.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx"><a class="af oo" href="https://arxiv.org/abs/2311.04934" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2311.04934</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="aa3a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">In summary:</strong></p><blockquote class="qf qg qh"><p id="88ee" class="nj nk of nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">All of the relationships between tokens of the cached prompt are already computed. Only new relationships between NEW-OLD or NEW-NEW tokens must be computed.</p></blockquote><h1 id="b4a0" class="op oq fq bf or os ot gq ou ov ow gt ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">Is This the End of RAG?</h1><p id="2e0f" class="pw-post-body-paragraph nj nk fq nl b go pl nn no gr pm nq nr ns pn nu nv nw po ny nz oa pp oc od oe fj bk">As models’ context sizes increase, prompt caching will make a great difference by avoiding repetitive processing. As a result, some might lean toward just using huge prompts and skipping retrieval processes entirely.</p><p id="b9ec" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">But here’s the catch: as contexts grow, models lose focus. Not because models will do a bad job but because finding answers in a big chunk of data is a subjective task that depends on the use case needs.</p><p id="39a1" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Systems capable of storing and managing vast volumes of vectors will remain essential, and RAG goes beyond caching prompts by offering something critical: control.</p><p id="5ab3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With RAG, you can filter and retrieve only the most relevant chunks from your data rather than relying on the model to process everything. A modular, separated approach ensures less noise, giving you more transparency and precision than full context feeding.</p><p id="7485" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Finally, larger context models emerging will probably ask for better storage for prompt vectors instead of simple caching. Does that take us back to… vector stores?</p></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="ae53" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">At Langflow, we’re building the fastest path from RAG prototyping to production. It’s open-source and features a free cloud service! Check it out at <a class="af oo" href="https://github.com/langflow-ai/langflow" rel="noopener ugc nofollow" target="_blank">https://github.com/langflow-ai/langflow</a> ✨</p></div></div></div></div>    
</body>
</html>