- en: 'Gaussian Naive Bayes, Explained: A Visual Guide with Code Examples for Beginners'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c?source=collection_archive---------2-----------------------#2024-10-12](https://towardsdatascience.com/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c?source=collection_archive---------2-----------------------#2024-10-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: CLASSIFICATION ALGORITHM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Bell-shaped assumptions for better predictions**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--04949cef383c--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--04949cef383c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--04949cef383c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--04949cef383c--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--04949cef383c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--04949cef383c--------------------------------)
    ·9 min read·Oct 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3461eb23bbba050007a17f806c580568.png)'
  prefs: []
  type: TYPE_IMG
- en: '`⛳️ More [CLASSIFICATION ALGORITHM](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c),
    explained: · [Dummy Classifier](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    · [K Nearest Neighbor Classifier](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    · [Bernoulli Naive Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    ▶ [Gaussian Naive Bayes](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    · [Decision Tree Classifier](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    · [Logistic Regression](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    · [Support Vector Classifier](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    · [Multilayer Perceptron](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
  prefs: []
  type: TYPE_NORMAL
- en: Building on our previous article about [Bernoulli Naive Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6),
    which handles binary data, we now explore Gaussian Naive Bayes for continuous
    data. Unlike the binary approach, this algorithm assumes each feature follows
    a normal (Gaussian) distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we’ll see how Gaussian Naive Bayes handles continuous, bell-shaped data
    — ringing in accurate predictions — all **without getting into the intricate math**
    of Bayes’ Theorem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c31e0cb8eff429b558a8a46174d03a76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like other Naive Bayes variants, Gaussian Naive Bayes makes the “naive” assumption
    of feature independence. It assumes that the features are conditionally independent
    given the class label.
  prefs: []
  type: TYPE_NORMAL
- en: However, while Bernoulli Naive Bayes is suited for datasets with binary features,
    Gaussian Naive Bayes assumes that the features follow **a continuous normal (Gaussian)**
    distribution. Although this assumption may not always hold true in reality, it
    simplifies the calculations and often leads to surprisingly accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d0e6eec6460b4247cf38d8dc3c8600e.png)'
  prefs: []
  type: TYPE_IMG
- en: Bernoulli NB assumes binary data, Multinomial NB works with discrete counts,
    and Gaussian NB handles continuous data assuming a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this article, we’ll use this artificial golf dataset (made by author)
    as an example. This dataset predicts whether a person will play golf based on
    weather conditions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc1873cedde634b5e602a6129e33ac2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: ‘RainfallAmount’ (in mm), ‘Temperature’ (in Celcius), ‘Humidity’ (in
    %), ‘WindSpeed’ (in km/h) and ‘Play’ (Yes/No, target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Main Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gaussian Naive Bayes works with continuous data, assuming each feature follows
    a Gaussian (normal) distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the probability of each class in the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each feature and class, estimate the mean and variance of the feature values
    within that class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For a new instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. For each class, calculate the probability density function (PDF) of each
    feature value under the Gaussian distribution of that feature within the class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Multiply the class probability by the product of the PDF values for all features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Predict the class with the highest resulting probability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/a090060bd91a5d5c69880a4926db7175.png)'
  prefs: []
  type: TYPE_IMG
- en: Gaussian Naive Bayes uses the normal distribution to model the likelihood of
    different feature values for each class. It then combines these likelihoods to
    make a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming non-Gaussian distributed data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember that this algorithm naively assume that all the input features are
    having Gaussian/normal distribution?
  prefs: []
  type: TYPE_NORMAL
- en: Since we are not really sure about the distribution of our data, especially
    for features that clearly don’t follow a Gaussian distribution, applying a [power
    transformation](/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb)
    (like Box-Cox) before using Gaussian Naive Bayes can be beneficial. This approach
    can help make the data more Gaussian-like, which aligns better with the assumptions
    of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90d0627b51b81e1b87930635d3497adc.png)'
  prefs: []
  type: TYPE_IMG
- en: All columns are scaled using Power Transformation (Box-Cox Transformation) and
    then standardized.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now we are ready for the training.
  prefs: []
  type: TYPE_NORMAL
- en: Training Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '1. **Class Probability Calculation**: For each class, calculate its probability:
    (Number of instances in this class) / (Total number of instances)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c06266178b47854c1548da3d97e124f7.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '2\. **Feature Probability Calculation** : For each feature and each class,
    calculate the mean (μ) and standard deviation (σ) of the feature values within
    that class using the training data. Then, calculate the probability using Gaussian
    Probability Density Function (PDF) formula.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1e80a1082f80606387c89d90f62303c.png)'
  prefs: []
  type: TYPE_IMG
- en: For each weather condition, determine the mean and standard deviation for both
    “YES” and “NO” instances. Then calculate their PDF using the PDF formula for normal/Gaussian
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a40cfb56adeb49c44ff0899ca2e3dbc1.png)'
  prefs: []
  type: TYPE_IMG
- en: The same process is applied to all of the other features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9b65e3ad3d83feb7d761c151b3593320.png)'
  prefs: []
  type: TYPE_IMG
- en: '3\. **Smoothing**: Gaussian Naive Bayes uses a unique smoothing approach. Unlike
    Laplace smoothing [in other variants](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6),
    it adds a tiny value (0.000000001 times the largest variance) to all variances.
    This prevents numerical instability from division by zero or very small numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: Prediction/Classification Step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given a new instance with continuous features:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. **Probability Collection**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each possible class:'
  prefs: []
  type: TYPE_NORMAL
- en: · Start with the probability of this class occurring (class probability).
  prefs: []
  type: TYPE_NORMAL
- en: · For each feature in the new instance, calculate the probability density function
    of that feature within the class.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf29443203d3ddc521fc69caf88fa8bf.png)'
  prefs: []
  type: TYPE_IMG
- en: For ID 14, we calculate the PDF each of the feature for both “YES” and “NO”
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. **Score Calculation & Prediction**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each class:'
  prefs: []
  type: TYPE_NORMAL
- en: · Multiply all the collected PDF values together.
  prefs: []
  type: TYPE_NORMAL
- en: · The result is the score for this class.
  prefs: []
  type: TYPE_NORMAL
- en: · The class with the highest score is the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c0e57938597cec51337f265f8a137f9.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9c97ff87d73e99807c7113e900035931.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation Step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/5a17bec3594c45626bd3222026fed233.png)'
  prefs: []
  type: TYPE_IMG
- en: For this particular dataset, this accuracy is considered quite good.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Key Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GaussianNB is known for its simplicity and effectiveness. The main thing to
    remember about its parameters is:'
  prefs: []
  type: TYPE_NORMAL
- en: '**priors**: This is the most notable parameter, [similar to Bernoulli Naive
    Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6).
    In most cases, you don’t need to set it manually. By default, it’s calculated
    from your training data, which often works well.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**var_smoothing**: This is a stability parameter that you rarely need to adjust.
    (the default is 0.000000001)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The key takeaway is that this algoritm is designed to work well out-of-the-box.
    In most situations, you can use it without worrying about parameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Pros & Cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Simplicity**: Maintains the easy-to-implement and understand trait.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Efficiency**: Remains swift in training and prediction, making it suitable
    for large-scale applications with continuous features.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Flexibility with Data**: Handles both small and large datasets well, adapting
    to the scale of the problem at hand.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Continuous Feature Handling**: Thrives with continuous and real-valued features,
    making it ideal for tasks like predicting real-valued outputs or working with
    data where features vary on a continuum.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Independence Assumption**: Still assumes that features are conditionally
    independent given the class, which might not hold in all real-world scenarios.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Gaussian Distribution Assumption**: Works best when feature values truly
    follow a normal distribution. Non-normal distributions may lead to suboptimal
    performance (but can be fixed with Power Transformation we’ve discussed)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sensitivity to Outliers**: Can be significantly affected by outliers in the
    training data, as they skew the mean and variance calculations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gaussian Naive Bayes stands as an efficient classifier for a wide range of applications
    involving continuous data. Its ability to handle real-valued features extends
    its use beyond binary classification tasks, making it a go-to choice for numerous
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: While it makes some assumptions about data (feature independence and normal
    distribution), when these conditions are met, it gives robust performance, making
    it a favorite among both beginners and seasoned data scientists for its balance
    of simplicity and power.
  prefs: []
  type: TYPE_NORMAL
- en: 🌟 Gaussian Naive Bayes Simplified
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of the [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)
    and its implementation in scikit-learn, readers can refer to the official documentation,
    which provides comprehensive information on its usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘾𝙡𝙖𝙨𝙨𝙞𝙛𝙞𝙘𝙖𝙩𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----04949cef383c--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----04949cef383c--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This “dummy” doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----04949cef383c--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  prefs: []
  type: TYPE_NORMAL
