- en: 'Gaussian Naive Bayes, Explained: A Visual Guide with Code Examples for Beginners'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c?source=collection_archive---------2-----------------------#2024-10-12](https://towardsdatascience.com/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c?source=collection_archive---------2-----------------------#2024-10-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: CLASSIFICATION ALGORITHM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Bell-shaped assumptions for better predictions**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--04949cef383c--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--04949cef383c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--04949cef383c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--04949cef383c--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--04949cef383c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--04949cef383c--------------------------------)
    Â·9 min readÂ·Oct 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3461eb23bbba050007a17f806c580568.png)'
  prefs: []
  type: TYPE_IMG
- en: '`â›³ï¸ More [CLASSIFICATION ALGORITHM](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c),
    explained: Â· [Dummy Classifier](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    Â· [K Nearest Neighbor Classifier](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    Â· [Bernoulli Naive Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    â–¶ [Gaussian Naive Bayes](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    Â· [Decision Tree Classifier](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    Â· [Logistic Regression](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    Â· [Support Vector Classifier](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    Â· [Multilayer Perceptron](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
  prefs: []
  type: TYPE_NORMAL
- en: Building on our previous article about [Bernoulli Naive Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6),
    which handles binary data, we now explore Gaussian Naive Bayes for continuous
    data. Unlike the binary approach, this algorithm assumes each feature follows
    a normal (Gaussian) distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Here, weâ€™ll see how Gaussian Naive Bayes handles continuous, bell-shaped data
    â€” ringing in accurate predictions â€” all **without getting into the intricate math**
    of Bayesâ€™ Theorem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c31e0cb8eff429b558a8a46174d03a76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like other Naive Bayes variants, Gaussian Naive Bayes makes the â€œnaiveâ€ assumption
    of feature independence. It assumes that the features are conditionally independent
    given the class label.
  prefs: []
  type: TYPE_NORMAL
- en: However, while Bernoulli Naive Bayes is suited for datasets with binary features,
    Gaussian Naive Bayes assumes that the features follow **a continuous normal (Gaussian)**
    distribution. Although this assumption may not always hold true in reality, it
    simplifies the calculations and often leads to surprisingly accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d0e6eec6460b4247cf38d8dc3c8600e.png)'
  prefs: []
  type: TYPE_IMG
- en: Bernoulli NB assumes binary data, Multinomial NB works with discrete counts,
    and Gaussian NB handles continuous data assuming a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this article, weâ€™ll use this artificial golf dataset (made by author)
    as an example. This dataset predicts whether a person will play golf based on
    weather conditions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc1873cedde634b5e602a6129e33ac2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: â€˜RainfallAmountâ€™ (in mm), â€˜Temperatureâ€™ (in Celcius), â€˜Humidityâ€™ (in
    %), â€˜WindSpeedâ€™ (in km/h) and â€˜Playâ€™ (Yes/No, target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Main Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gaussian Naive Bayes works with continuous data, assuming each feature follows
    a Gaussian (normal) distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the probability of each class in the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each feature and class, estimate the mean and variance of the feature values
    within that class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For a new instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. For each class, calculate the probability density function (PDF) of each
    feature value under the Gaussian distribution of that feature within the class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Multiply the class probability by the product of the PDF values for all features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Predict the class with the highest resulting probability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/a090060bd91a5d5c69880a4926db7175.png)'
  prefs: []
  type: TYPE_IMG
- en: Gaussian Naive Bayes uses the normal distribution to model the likelihood of
    different feature values for each class. It then combines these likelihoods to
    make a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming non-Gaussian distributed data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember that this algorithm naively assume that all the input features are
    having Gaussian/normal distribution?
  prefs: []
  type: TYPE_NORMAL
- en: Since we are not really sure about the distribution of our data, especially
    for features that clearly donâ€™t follow a Gaussian distribution, applying a [power
    transformation](/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb)
    (like Box-Cox) before using Gaussian Naive Bayes can be beneficial. This approach
    can help make the data more Gaussian-like, which aligns better with the assumptions
    of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90d0627b51b81e1b87930635d3497adc.png)'
  prefs: []
  type: TYPE_IMG
- en: All columns are scaled using Power Transformation (Box-Cox Transformation) and
    then standardized.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now we are ready for the training.
  prefs: []
  type: TYPE_NORMAL
- en: Training Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '1. **Class Probability Calculation**: For each class, calculate its probability:
    (Number of instances in this class) / (Total number of instances)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c06266178b47854c1548da3d97e124f7.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '2\. **Feature Probability Calculation** : For each feature and each class,
    calculate the mean (Î¼) and standard deviation (Ïƒ) of the feature values within
    that class using the training data. Then, calculate the probability using Gaussian
    Probability Density Function (PDF) formula.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1e80a1082f80606387c89d90f62303c.png)'
  prefs: []
  type: TYPE_IMG
- en: For each weather condition, determine the mean and standard deviation for both
    â€œYESâ€ and â€œNOâ€ instances. Then calculate their PDF using the PDF formula for normal/Gaussian
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a40cfb56adeb49c44ff0899ca2e3dbc1.png)'
  prefs: []
  type: TYPE_IMG
- en: The same process is applied to all of the other features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9b65e3ad3d83feb7d761c151b3593320.png)'
  prefs: []
  type: TYPE_IMG
- en: '3\. **Smoothing**: Gaussian Naive Bayes uses a unique smoothing approach. Unlike
    Laplace smoothing [in other variants](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6),
    it adds a tiny value (0.000000001 times the largest variance) to all variances.
    This prevents numerical instability from division by zero or very small numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: Prediction/Classification Step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given a new instance with continuous features:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. **Probability Collection**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each possible class:'
  prefs: []
  type: TYPE_NORMAL
- en: Â· Start with the probability of this class occurring (class probability).
  prefs: []
  type: TYPE_NORMAL
- en: Â· For each feature in the new instance, calculate the probability density function
    of that feature within the class.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf29443203d3ddc521fc69caf88fa8bf.png)'
  prefs: []
  type: TYPE_IMG
- en: For ID 14, we calculate the PDF each of the feature for both â€œYESâ€ and â€œNOâ€
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. **Score Calculation & Prediction**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each class:'
  prefs: []
  type: TYPE_NORMAL
- en: Â· Multiply all the collected PDF values together.
  prefs: []
  type: TYPE_NORMAL
- en: Â· The result is the score for this class.
  prefs: []
  type: TYPE_NORMAL
- en: Â· The class with the highest score is the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c0e57938597cec51337f265f8a137f9.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9c97ff87d73e99807c7113e900035931.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation Step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/5a17bec3594c45626bd3222026fed233.png)'
  prefs: []
  type: TYPE_IMG
- en: For this particular dataset, this accuracy is considered quite good.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Key Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GaussianNB is known for its simplicity and effectiveness. The main thing to
    remember about its parameters is:'
  prefs: []
  type: TYPE_NORMAL
- en: '**priors**: This is the most notable parameter, [similar to Bernoulli Naive
    Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6).
    In most cases, you donâ€™t need to set it manually. By default, itâ€™s calculated
    from your training data, which often works well.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**var_smoothing**: This is a stability parameter that you rarely need to adjust.
    (the default is 0.000000001)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The key takeaway is that this algoritm is designed to work well out-of-the-box.
    In most situations, you can use it without worrying about parameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Pros & Cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Simplicity**: Maintains the easy-to-implement and understand trait.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Efficiency**: Remains swift in training and prediction, making it suitable
    for large-scale applications with continuous features.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Flexibility with Data**: Handles both small and large datasets well, adapting
    to the scale of the problem at hand.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Continuous Feature Handling**: Thrives with continuous and real-valued features,
    making it ideal for tasks like predicting real-valued outputs or working with
    data where features vary on a continuum.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Independence Assumption**: Still assumes that features are conditionally
    independent given the class, which might not hold in all real-world scenarios.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Gaussian Distribution Assumption**: Works best when feature values truly
    follow a normal distribution. Non-normal distributions may lead to suboptimal
    performance (but can be fixed with Power Transformation weâ€™ve discussed)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sensitivity to Outliers**: Can be significantly affected by outliers in the
    training data, as they skew the mean and variance calculations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gaussian Naive Bayes stands as an efficient classifier for a wide range of applications
    involving continuous data. Its ability to handle real-valued features extends
    its use beyond binary classification tasks, making it a go-to choice for numerous
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: While it makes some assumptions about data (feature independence and normal
    distribution), when these conditions are met, it gives robust performance, making
    it a favorite among both beginners and seasoned data scientists for its balance
    of simplicity and power.
  prefs: []
  type: TYPE_NORMAL
- en: ğŸŒŸ Gaussian Naive Bayes Simplified
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of the [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)
    and its implementation in scikit-learn, readers can refer to the official documentation,
    which provides comprehensive information on its usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¾ğ™¡ğ™–ğ™¨ğ™¨ğ™ğ™›ğ™ğ™˜ğ™–ğ™©ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----04949cef383c--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----04949cef383c--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----04949cef383c--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  prefs: []
  type: TYPE_NORMAL
