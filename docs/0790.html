<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Model Evaluations Versus Task Evaluations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Model Evaluations Versus Task Evaluations</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/model-evaluations-versus-task-evaluations-5bc742054957?source=collection_archive---------7-----------------------#2024-03-26">https://towardsdatascience.com/model-evaluations-versus-task-evaluations-5bc742054957?source=collection_archive---------7-----------------------#2024-03-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/d871ad67918406174ff5c74003dc75fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EUdxUoiGAxBe1Bh49bVWcA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image created by author using Dall-E 3</figcaption></figure><div/><div><h2 id="ee11" class="pw-subtitle-paragraph hh gj gk bf b hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw cq dx">Understanding the difference for LLM applications</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hx hy hz ia ib ab"><div><div class="ab ic"><div><div class="bm" aria-hidden="false"><a href="https://aparnadhinak.medium.com/?source=post_page---byline--5bc742054957--------------------------------" rel="noopener follow"><div class="l id ie by if ig"><div class="l ed"><img alt="Aparna Dhinakaran" class="l ep by dd de cx" src="../Images/e431ee69563ecb27c86f3428ba53574c.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*VbKXdndNnweCZQQa2TohWw.png"/><div class="ih by l dd de em n ii eo"/></div></div></a></div></div><div class="ij ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5bc742054957--------------------------------" rel="noopener follow"><div class="l ik il by if im"><div class="l ed"><img alt="Towards Data Science" class="l ep by br in cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ih by l br in em n ii eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="io ab q"><div class="ab q ip"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b iq ir bk"><a class="af ag ah ai aj ak al am an ao ap aq ar is" data-testid="authorName" href="https://aparnadhinak.medium.com/?source=post_page---byline--5bc742054957--------------------------------" rel="noopener follow">Aparna Dhinakaran</a></p></div></div></div><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b iq ir dx"><button class="iv iw ah ai aj ak al am an ao ap aq ar ix iy iz" disabled="">Follow</button></p></div></div></span></div></div><div class="l ja"><span class="bf b bg z dx"><div class="ab cn jb jc jd"><div class="je jf ab"><div class="bf b bg z dx ab jg"><span class="jh l ja">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar is ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5bc742054957--------------------------------" rel="noopener follow"><p class="bf b bg z ji jj jk jl jm jn jo jp bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="jq jr l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="h k w ea eb q"><div class="kx l"><div class="ab q ky kz"><div class="pw-multi-vote-icon ed jh la lb lc"><div class=""><div class="ld le lf lg lh li lj am lk ll lm lc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ln lo lp lq lr ls lt"><p class="bf b dy z dx"><span class="le">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ld lu lv ab q ee lw lx" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="ly"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q ki kj kk kl km kn ko kp kq kr ks kt ku kv kw"><div class="lz k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ma an ao ap ix mb mc md" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep me cn"><div class="l ae"><div class="ab cb"><div class="mf mg mh mi mj gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ma an ao ap ix mk ml lx mm mn mo mp mq s mr ms mt mu mv mw mx u my mz na"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ma an ao ap ix mk ml lx mm mn mo mp mq s mr ms mt mu mv mw mx u my mz na"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ma an ao ap ix mk ml lx mm mn mo mp mq s mr ms mt mu mv mw mx u my mz na"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="0a22" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For a moment, imagine an airplane. What springs to mind? Now imagine a Boeing 737 and a <a class="af nx" href="https://www.af.mil/About-Us/Fact-Sheets/Display/Article/104531/cv-22-osprey/" rel="noopener ugc nofollow" target="_blank">V-22 Osprey</a>. Both are aircraft designed to move cargo and people, yet they serve different purposes — one more general (commercial flights and freight), the other very specific (infiltration, exfiltration, and resupply missions for special operations forces). They look far different because they are built for different activities.</p><p id="11a1" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">With the rise of LLMs, we have seen our first truly general-purpose ML models. Their generality helps us in so many ways:</p><ul class=""><li id="ed4e" class="nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ny nz oa bk">The same engineering team can now do sentiment analysis and structured data extraction</li><li id="0457" class="nb nc gk nd b hi ob nf ng hl oc ni nj nk od nm nn no oe nq nr ns of nu nv nw ny nz oa bk">Practitioners in many domains can share knowledge, making it possible for the whole industry to benefit from each other’s experience</li><li id="b7d5" class="nb nc gk nd b hi ob nf ng hl oc ni nj nk od nm nn no oe nq nr ns of nu nv nw ny nz oa bk">There is a wide range of industries and jobs where the same experience is useful</li></ul><p id="2c1e" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">But as we see with aircraft, generality requires a very different assessment from excelling at a particular task, and at the end of the day business value often comes from solving particular problems.</p><p id="8e7a" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This is a good analogy for the difference between model and task evaluations. Model evals are focused on overall general assessment, but task evals are focused on assessing performance of a particular task.</p><h1 id="e4ad" class="og oh gk bf oi oj ok hk ol om on hn oo op oq or os ot ou ov ow ox oy oz pa pb bk">There Is More Than One LLM Eval</h1><figure class="pc pd pe pf pg fw"><div class="ph ji l ed"><div class="pi pj l"/></div></figure><p id="99f1" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The term <em class="pk">LLM evals</em> is thrown around quite generally. <a class="af nx" rel="noopener" target="_blank" href="/how-to-best-leverage-openais-evals-framework-c38bcef0ec47">OpenAI released some tooling to do LLM evals</a> very early, for example. Most practitioners are more concerned with LLM task evals, but that distinction is not always clearly made.</p><h2 id="0615" class="pl oh gk bf oi pm pn po ol pp pq pr oo nk ps pt pu no pv pw px ns py pz qa qb bk">What’s the Difference?</h2><p id="afe1" class="pw-post-body-paragraph nb nc gk nd b hi qc nf ng hl qd ni nj nk qe nm nn no qf nq nr ns qg nu nv nw fj bk">Model evals look at the “general fitness” of the model. How well does it do on a variety of tasks?</p><p id="96ec" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Task evals, on the other hand, are specifically designed to look at how well the model is suited for your particular application.</p><p id="3338" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Someone who works out generally and is quite fit would likely fare poorly against a professional sumo wrestler in a real competition, and model evals can’t stack up against task evals in assessing your particular needs.</p><h1 id="933b" class="og oh gk bf oi oj ok hk ol om on hn oo op oq or os ot ou ov ow ox oy oz pa pb bk">Model Evals</h1><p id="3577" class="pw-post-body-paragraph nb nc gk nd b hi qc nf ng hl qd ni nj nk qe nm nn no qf nq nr ns qg nu nv nw fj bk">Model evals are specifically meant for building and fine-tuning generalized models. They are based on a set of questions you ask a model and a set of ground-truth answers that you use to grade responses. Think of taking the SATs.</p><p id="183f" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">While every question in a model eval is different, there is usually a general area of testing. There is a theme or skill each metric is specifically targeted at. For example, HellaSwag performance has become a popular way to measure LLM quality.</p><p id="512a" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The <a class="af nx" href="https://rowanzellers.com/hellaswag/" rel="noopener ugc nofollow" target="_blank">HellaSwag</a> dataset consists of a collection of contexts and multiple-choice questions where each question has multiple potential completions. Only one of the completions is sensible or logically coherent, while the others are plausible but incorrect. These completions are designed to be challenging for AI models, requiring not just linguistic understanding but also common sense reasoning to choose the correct option.</p><p id="acb3" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is an example:<br/><em class="pk">A tray of potatoes is loaded into the oven and removed. A large tray of cake is flipped over and placed on counter. a large tray of meat</em></p><p id="624b" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="pk">A. is placed onto a baked potato</em></p><p id="c66b" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="pk">B. ls, and pickles are placed in the oven</em></p><p id="4d2e" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="pk">C. is prepared then it is removed from the oven by a helper when done.</em></p><p id="ebc9" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Another example is MMLU. <a class="af nx" href="https://paperswithcode.com/dataset/mmlu" rel="noopener ugc nofollow" target="_blank">MMLU</a> features tasks that span multiple subjects, including science, literature, history, social science, mathematics, and professional domains like law and medicine. This diversity in subjects is intended to mimic the breadth of knowledge and understanding required by human learners, making it a good test of a model’s ability to handle multifaceted language understanding challenges.</p><p id="6ca0" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here are some examples — can you solve them?</p><p id="5031" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="pk">For which of the following thermodynamic processes is the increase in the internal energy of an ideal gas equal to the heat added to the gas?</em></p><p id="c502" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="pk">A. Constant Temperature</em></p><p id="2ee7" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="pk">B. Constant Volume</em></p><p id="ba87" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="pk">C. Constant Pressure</em></p><p id="cb9d" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="pk">D. Adiabatic</em></p><figure class="pc pd pe pf pg fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qh"><img src="../Images/2336f2e5e5ddc480643c4db20ca56fda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SbcG-XrnwVU1tYD5WHn1fQ.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by author</figcaption></figure><p id="7005" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The <a class="af nx" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="noopener ugc nofollow" target="_blank">Hugging Face Leaderboard</a> is perhaps the best known place to get such model evals. The leaderboard tracks open source large language models and keeps track of many model evaluation metrics. This is typically a great place to start understanding the difference between open source LLMs in terms of their performance across a variety of tasks.</p><p id="44c3" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Multimodal models require even more evals. The <a class="af nx" href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf" rel="noopener ugc nofollow" target="_blank">Gemini paper</a> demonstrates that multi-modality introduces a host of other benchmarks like VQAv2, which tests the ability to understand and integrate visual information. This information goes beyond simple object recognition to interpreting actions and relationships between them.</p><p id="e502" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Similarly, there are metrics for audio and video information and how to integrate across modalities.</p><p id="bc3f" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The goal of these tests is to differentiate between two models or two different snapshots of the same model. Picking a model for your application is important, but it is something you do once or at most very infrequently.</p><figure class="pc pd pe pf pg fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qh"><img src="../Images/aa0265520e614b0de21aa34794f3abb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pE8_Ezj5JK8tDhXjplIhHg.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by author</figcaption></figure><h1 id="385f" class="og oh gk bf oi oj ok hk ol om on hn oo op oq or os ot ou ov ow ox oy oz pa pb bk">Task Evals</h1><p id="d8a1" class="pw-post-body-paragraph nb nc gk nd b hi qc nf ng hl qd ni nj nk qe nm nn no qf nq nr ns qg nu nv nw fj bk">The much more frequent problem is one solved by task evaluations. The goal of task-based evaluations is to analyze the performance of the model using LLM as a judge.</p><ul class=""><li id="8eb6" class="nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ny nz oa bk">Did your retrieval system fetch the right data?</li><li id="8794" class="nb nc gk nd b hi ob nf ng hl oc ni nj nk od nm nn no oe nq nr ns of nu nv nw ny nz oa bk">Are there hallucinations in your responses?</li><li id="9521" class="nb nc gk nd b hi ob nf ng hl oc ni nj nk od nm nn no oe nq nr ns of nu nv nw ny nz oa bk">Did the system answer important questions with relevant answers?</li></ul><p id="818b" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Some may feel a bit unsure about an LLM evaluating other LLMs, but we have humans evaluating other humans all the time.</p><p id="f5b6" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The real distinction between model and task evaluations is that for a model eval we ask many different questions, but for a task eval the question stays the same and it is the data we change. For example, say you were operating a chatbot. You could use your task eval on hundreds of customer interactions and ask it, “<em class="pk">Is there a hallucination here?”</em> The question stays the same across all the conversations.</p><figure class="pc pd pe pf pg fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qh"><img src="../Images/864534f219b59a6179b2009f4f190190.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rx25ZQapa3rBmZbOf_l5fw.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by author</figcaption></figure><p id="6847" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">There are several libraries aimed at helping practitioners build these evaluations: <a class="af nx" href="https://docs.ragas.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">Ragas</a>, <a class="af nx" href="https://phoenix.arize.com/" rel="noopener ugc nofollow" target="_blank">Phoenix</a> (full disclosure: the author leads the team that developed Phoenix), <a class="af nx" href="https://github.com/openai/evals/tree/main/evals" rel="noopener ugc nofollow" target="_blank">OpenAI</a>, <a class="af nx" href="https://docs.llamaindex.ai/en/latest/optimizing/evaluation/evaluation.html" rel="noopener ugc nofollow" target="_blank">LlamaIndex</a>.</p><h2 id="a0d8" class="pl oh gk bf oi pm pn po ol pp pq pr oo nk ps pt pu no pv pw px ns py pz qa qb bk">How do they work?</h2><p id="75f7" class="pw-post-body-paragraph nb nc gk nd b hi qc nf ng hl qd ni nj nk qe nm nn no qf nq nr ns qg nu nv nw fj bk">The task eval grades performance of every output from the application as a whole. Let’s look at what it takes to put one together.</p><h2 id="367b" class="pl oh gk bf oi pm pn po ol pp pq pr oo nk ps pt pu no pv pw px ns py pz qa qb bk">Establishing a benchmark</h2><p id="0d69" class="pw-post-body-paragraph nb nc gk nd b hi qc nf ng hl qd ni nj nk qe nm nn no qf nq nr ns qg nu nv nw fj bk">The foundation rests on establishing a robust benchmark. This starts with creating a golden dataset that accurately reflects the scenarios the LLM will encounter. This dataset should include ground truth labels — often derived from meticulous human review — to serve as a standard for comparison. Don’t worry, though, you can usually get away with dozens to hundreds of examples here. Selecting the right LLM for evaluation is also critical. While it may differ from the application’s primary LLM, it should align with goals of cost-efficiency and accuracy.</p><h2 id="cc28" class="pl oh gk bf oi pm pn po ol pp pq pr oo nk ps pt pu no pv pw px ns py pz qa qb bk">Crafting the evaluation template</h2><p id="b173" class="pw-post-body-paragraph nb nc gk nd b hi qc nf ng hl qd ni nj nk qe nm nn no qf nq nr ns qg nu nv nw fj bk">The heart of the task evaluation process is the evaluation template. This template should clearly define the input (e.g., user queries and documents), the evaluation question (e.g., the relevance of the document to the query), and the expected output formats (binary or multi-class relevance). Adjustments to the template may be necessary to capture nuances specific to your application, ensuring it can accurately assess the LLM’s performance against the golden dataset.</p><p id="6949" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is an example of a template to evaluate a Q&amp;A task.</p><pre class="pc pd pe pf pg qi qj qk bp ql bb bk"><span id="f380" class="qm oh gk qj b bg qn qo l qp qq">You are given a question, an answer and reference text. You must determine whether the given answer correctly answers the question based on the reference text. Here is the data:<br/>  [BEGIN DATA]<br/>  ************<br/>  [QUESTION]: {input}<br/>  ************<br/>  [REFERENCE]: {reference}<br/>  ************<br/>  [ANSWER]: {output}<br/>  [END DATA]<br/>Your response should be a single word, either "correct" or "incorrect", and should not contain any text or characters aside from that word.<br/>"correct" means that the question is correctly and fully answered by the answer. <br/>"incorrect" means that the question is not correctly or only partially answered by the answer. </span></pre><h2 id="42a1" class="pl oh gk bf oi pm pn po ol pp pq pr oo nk ps pt pu no pv pw px ns py pz qa qb bk">Metrics and iteration</h2><p id="3e2b" class="pw-post-body-paragraph nb nc gk nd b hi qc nf ng hl qd ni nj nk qe nm nn no qf nq nr ns qg nu nv nw fj bk">Running the eval across your golden dataset allows you to generate key metrics such as accuracy, precision, recall, and F1-score. These provide insight into the evaluation template’s effectiveness and highlight areas for improvement. Iteration is crucial; refining the template based on these metrics ensures the evaluation process remains aligned with the application’s goals without overfitting to the golden dataset.</p><p id="fe89" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In task evaluations, relying solely on overall accuracy is insufficient since we always expect significant class imbalance. Precision and recall offer a more robust view of the LLM’s performance, emphasizing the importance of identifying both relevant and irrelevant outcomes accurately. A balanced approach to metrics ensures that evaluations meaningfully contribute to enhancing the LLM application.</p><h2 id="e535" class="pl oh gk bf oi pm pn po ol pp pq pr oo nk ps pt pu no pv pw px ns py pz qa qb bk">Application of LLM evaluations</h2><p id="ab62" class="pw-post-body-paragraph nb nc gk nd b hi qc nf ng hl qd ni nj nk qe nm nn no qf nq nr ns qg nu nv nw fj bk">Once an evaluation framework is in place, the next step is to apply these evaluations directly to your LLM application. This involves integrating the evaluation process into the application’s workflow, allowing for real-time assessment of the LLM’s responses to user inputs. This continuous feedback loop is invaluable for maintaining and improving the application’s relevance and accuracy over time.</p><h2 id="4e08" class="pl oh gk bf oi pm pn po ol pp pq pr oo nk ps pt pu no pv pw px ns py pz qa qb bk">Evaluation across the system lifecycle</h2><p id="ff4b" class="pw-post-body-paragraph nb nc gk nd b hi qc nf ng hl qd ni nj nk qe nm nn no qf nq nr ns qg nu nv nw fj bk">Effective task evaluations are not confined to a single stage but are integral throughout the LLM system’s life cycle. From pre-production benchmarking and testing to ongoing performance assessments in production, <a class="af nx" href="https://arize.com/blog-course/llm-evaluation-the-definitive-guide/" rel="noopener ugc nofollow" target="_blank">LLM evaluation</a> ensures the system remains responsive to user need.</p><h2 id="e993" class="pl oh gk bf oi pm pn po ol pp pq pr oo nk ps pt pu no pv pw px ns py pz qa qb bk">Example: is the model hallucinating?</h2><p id="7361" class="pw-post-body-paragraph nb nc gk nd b hi qc nf ng hl qd ni nj nk qe nm nn no qf nq nr ns qg nu nv nw fj bk">Let’s look at a hallucination example in more detail.</p><figure class="pc pd pe pf pg fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qr"><img src="../Images/474e7210ed5ea5d8d8d011a5c4e42f96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qtlesa5tEMtUEvoqQIDg0Q.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Example by author</figcaption></figure><p id="29ee" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Since hallucinations are a common problem for most practitioners, there are some benchmark datasets available. These are a great first step, but you will often need to have a customized dataset within your company.</p><p id="7edf" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The next important step is to develop the prompt template. Here again a good library can help you get started. We saw an example prompt template earlier, here we see another specifically for hallucinations. You may need to tweak it for your purposes.</p><pre class="pc pd pe pf pg qi qj qk bp ql bb bk"><span id="5209" class="qm oh gk qj b bg qn qo l qp qq">In this task, you will be presented with a query, a reference text and an answer. The answer is<br/>generated to the question based on the reference text. The answer may contain false information, you<br/>must use the reference text to determine if the answer to the question contains false information,<br/>if the answer is a hallucination of facts. Your objective is to determine whether the reference text<br/>contains factual information and is not a hallucination. A 'hallucination' in this context refers to<br/>an answer that is not based on the reference text or assumes information that is not available in<br/>the reference text. Your response should be a single word: either "factual" or "hallucinated", and<br/>it should not include any other text or characters. "hallucinated" indicates that the answer<br/>provides factually inaccurate information to the query based on the reference text. "factual"<br/>indicates that the answer to the question is correct relative to the reference text, and does not<br/>contain made up information. Please read the query and reference text carefully before determining<br/>your response.<br/><br/>    [BEGIN DATA]<br/>    ************<br/>    [Query]: {input}<br/>    ************<br/>    [Reference text]: {reference}<br/>    ************<br/>    [Answer]: {output}<br/>    ************<br/>    [END DATA]<br/><br/>    Is the answer above factual or hallucinated based on the query and reference text?<br/><br/>Your response should be a single word: either "factual" or "hallucinated", and it should not include any other text or characters. <br/>"hallucinated" indicates that the answer provides factually inaccurate information to the query based on the reference text.<br/>"factual" indicates that the answer to the question is correct relative to the reference text, and does not contain made up information.<br/>Please read the query and reference text carefully before determining your response. </span></pre><p id="022d" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now you are ready to give your eval LLM the queries from your golden dataset and have it label hallucinations. When you look at the results, remember that there should be class imbalance. You want to track precision and recall instead of overall accuracy.</p><p id="7dea" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">It is very useful to construct a confusion matrix and plot it visually. When you have such a plot, you can feel reassurance about your LLM’s performance. If the performance is not to your satisfaction, you can always optimize the prompt template.</p><figure class="pc pd pe pf pg fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qs"><img src="../Images/a3804aa615cd2edded178962eaeb8b6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Q8DINVBECVtNOeYMPGnGA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx"><em class="qt">Example of evaluating performance of the task eval so users can build confidence in their evals</em></figcaption></figure><p id="5347" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">After the eval is built, you now have a powerful tool that can label all your data with known precision and recall. You can use it to track hallucinations in your system both during development and production phases.</p><h1 id="87d5" class="og oh gk bf oi oj ok hk ol om on hn oo op oq or os ot ou ov ow ox oy oz pa pb bk">Summary of Differences</h1><p id="a284" class="pw-post-body-paragraph nb nc gk nd b hi qc nf ng hl qd ni nj nk qe nm nn no qf nq nr ns qg nu nv nw fj bk">Let’s sum up the differences between task and model evaluations.</p><figure class="pc pd pe pf pg fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qu"><img src="../Images/7e289f5124ebc71d4065f18dc29f0bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D-WVFO084kfKt8JDn-Y-cQ.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Table by author</figcaption></figure><h1 id="3541" class="og oh gk bf oi oj ok hk ol om on hn oo op oq or os ot ou ov ow ox oy oz pa pb bk">Takeaways</h1><p id="f0d9" class="pw-post-body-paragraph nb nc gk nd b hi qc nf ng hl qd ni nj nk qe nm nn no qf nq nr ns qg nu nv nw fj bk">Ultimately, both model evaluations and task evaluations are important in putting together a functional LLM system. It is important to understand <a class="af nx" href="https://arize.com/blog-course/large-language-model-evaluations-vs-llm-task-evaluations-in-llm-application-development/" rel="noopener ugc nofollow" target="_blank">when and how to apply each</a>. For most practitioners, the majority of their time will be spent on task evals, which provide a measure of system performance on a specific task.</p></div></div></div></div>    
</body>
</html>