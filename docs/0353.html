<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Solving Differential Equations With Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Solving Differential Equations With Neural Networks</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/solving-differential-equations-with-neural-networks-4c6aa7b31c51?source=collection_archive---------2-----------------------#2024-02-06">https://towardsdatascience.com/solving-differential-equations-with-neural-networks-4c6aa7b31c51?source=collection_archive---------2-----------------------#2024-02-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="73c5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How Neural Networks are strong tools for solving differential equations without the use of training data</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@rodrigopesilva?source=post_page---byline--4c6aa7b31c51--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Rodrigo Silva" class="l ep by dd de cx" src="../Images/d260f05ed9887c5072e0590db1481be2.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*heNbe-BQVIx12kMGCHmuHA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4c6aa7b31c51--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@rodrigopesilva?source=post_page---byline--4c6aa7b31c51--------------------------------" rel="noopener follow">Rodrigo Silva</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4c6aa7b31c51--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">13</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/8dd6dcc5577d527b7f03e38984b6ec26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AEra0NgQ0Z56SB2tHznFYA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@linusmimietz?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Linus Mimietz</a> on <a class="af nc" href="https://unsplash.com/photos/water-drops-macro-photography-XSQHuGGRO3g?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="bb89" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Differential equations are one of the protagonists in physical sciences, with vast applications in engineering, biology, economy, and even social sciences. Roughly speaking, they tell us how a quantity varies in time (or some other parameter, but usually we are interested in time variations). We can understand how a population, or a stock price, or even how the opinion of some society towards certain themes changes over time.</p><p id="07b4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Typically, the methods used to solve DEs are not analytical (i.e. there is no "closed formula" for the solution) and we have to resource to numerical methods. However, numerical methods can be expensive from a computational standpoint, and worse than that: the accumulated error can be significantly large.</p><p id="a2f3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This article will showcase how a Neural Network can be a valuable ally to solve a differential equation, and how we can borrow concepts from Physics-Informed Neural Networks to tackle the question: can we use a machine learning approach to solve a DE?</p><h1 id="dda7" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">A pinch of Physics-Informed Neural Networks</h1><p id="aa36" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">In this section, I will talk about Physics-Informed Neural Networks very briefly. I suppose you know the "neural network" part, but what makes them be informed by physics? Well, they are not exactly informed by physics, but rather by a (differential) equation.</p><p id="bb7e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Usually, neural networks are trained to find patterns and figure out what's going on with a set of training data. However, when you train a neural network to obey the behavior of your training data and hopefully fit unseen data, your model is highly dependent on the data itself, and not on the underlying nature of your system. It sounds almost like a philosophical matter, but it is more practical than that: if your data comes from measurements of ocean currents, these currents have to obey the physics equations that describe ocean currents. Notice, however, that your neural network is completely agnostic about these equations and is only trying to fit data points.</p><p id="6b40" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is where physics informed comes into play. If, besides learning how to fit your data, your model also learns how to fit the equations that govern that system, the predictions of your neural network will be much more precise and will generalize much better, just citing some advantages of physics-informed models.</p><p id="bd85" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Notice that the governing equations of your system don't have to involve physics at all, the "physics-informed" thing is just nomenclature (and the technique is most used by physicists anyway). If your system is the traffic in a city and you happen to have a good mathematical model that you want your neural network's predictions to obey, then physics-informed neural networks are a good fit for you.</p><h2 id="f9c1" class="pa oa fq bf ob pb pc pd oe pe pf pg oh nm ph pi pj nq pk pl pm nu pn po pp pq bk">How do we inform these models?</h2><p id="cded" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Hopefully, I've convinced you that it is worth the trouble to make the model aware of the underlying equations that govern our system. However, how can we do this? There are several approaches to this, but the main one is to adapt the loss function to have a term that accounts for the governing equations, aside from the usual data-related part. That is, the loss function <em class="pr">L </em>will be composed of the sum</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ps"><img src="../Images/dcc1113601f71c77b0fa9933bacd9824.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*Pp9tmSKMtkiv1nKu-S6LUw.png"/></div></figure><p id="3168" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here, the data loss is the usual one: a mean squared difference, or some other suited form of loss function; but the equation part is the charming one. Imagine that your system is governed by the following differential equation:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pt"><img src="../Images/47f34c0f8617929070a5eca85e17a597.png" data-original-src="https://miro.medium.com/v2/resize:fit:186/format:webp/1*qLICxXMl0xDutWPM3c1TBw.png"/></div></figure><p id="5633" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">How can we fit this into the loss function? Well, since our task when training a neural network is to minimize the loss function, what we want is to minimize the following expression:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pu"><img src="../Images/6808838b818b5eea8947e358a9851645.png" data-original-src="https://miro.medium.com/v2/resize:fit:226/format:webp/1*OCUtaGzzl1_EkZZrJGIBDQ.png"/></div></figure><p id="1917" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So our equation-related loss function turns out to be</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/7c629affee0d0c244f4cac4841b471ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*-A_28ggFS3d-eIPa85EUkQ.png"/></div></figure><p id="4cea" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">that is, it is the mean difference squared of our DE. If we manage to minimize this (a.k.a. make this term as close to zero as possible) we automatically satisfy the system's governing equation. Pretty clever, right?</p><p id="58a4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, the extra term <em class="pr">L_IC </em>in the loss function needs to be addressed: it accounts for the initial conditions of the system. If a system's initial conditions are not provided, there are infinitely many solutions for a differential equation. For instance, a ball thrown from the ground level has its trajectory governed by the same differential equation as a ball thrown from the 10th floor; however, we know for sure that the paths made by these balls will not be the same. What changes here are the initial conditions of the system. How does our model know which initial conditions we are talking about? It is natural at this point that we enforce it using a loss function term! For our DE, let's impose that when <em class="pr">t = 0</em>, <em class="pr">y = 1</em>. Hence, we want to minimize an initial condition loss function that reads:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pw"><img src="../Images/5a6c9d547fb8fbe2346528fa9fad2675.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*KTrsnVPb9uCkDGiqQTltxA.png"/></div></figure><p id="67c7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If we minimize this term, then we automatically satisfy the initial conditions of our system. Now, what is left to be understood is how to use this to solve a differential equation.</p><h1 id="c31f" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Solving a differential equation</h1><p id="06dd" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">If a neural network can be trained either with the data-related term of the loss function (this is what is usually done in classical architectures), and can also be trained with both the data and the equation-related term (this is physics-informed neural networks I just mentioned), it must be true that it can be trained to minimize <em class="pr">only</em> the equation-related term. This is exactly what we are going to do! The only loss function used here will be the <em class="pr">L_equation</em>. Hopefully, this diagram below illustrates what I've just said: today we are aiming for the right-bottom type of model, our DE solver NN.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk px"><img src="../Images/b5397415a3249d038332a3d4300f1e2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xo7t7r5on0IYzaeDhI_tRg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 1: diagram showing the kinds of neural networks with respect to their loss functions. In this article, we are aiming for the right-bottom one. Image by author.</figcaption></figure><h2 id="64c5" class="pa oa fq bf ob pb pc pd oe pe pf pg oh nm ph pi pj nq pk pl pm nu pn po pp pq bk">Code implementation</h2><p id="68e4" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">To showcase the theoretical learnings we've just got, I will implement the proposed solution in Python code, using the PyTorch library for machine learning.</p><p id="93e9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The first thing to do is to create a neural network architecture:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="c7e0" class="qc oa fq pz b bg qd qe l qf qg">import torch<br/>import torch.nn as nn<br/><br/>class NeuralNet(nn.Module):<br/>    def __init__(self, hidden_size, output_size=1,input_size=1):<br/>        super(NeuralNet, self).__init__()<br/>        self.l1 = nn.Linear(input_size, hidden_size)<br/>        self.relu1 = nn.LeakyReLU()<br/>        self.l2 = nn.Linear(hidden_size, hidden_size)<br/>        self.relu2 = nn.LeakyReLU()<br/>        self.l3 = nn.Linear(hidden_size, hidden_size)<br/>        self.relu3 = nn.LeakyReLU()<br/>        self.l4 = nn.Linear(hidden_size, output_size)<br/><br/>    def forward(self, x):<br/>        out = self.l1(x)<br/>        out = self.relu1(out)<br/>        out = self.l2(out)<br/>        out = self.relu2(out)<br/>        out = self.l3(out)<br/>        out = self.relu3(out)<br/>        out = self.l4(out)<br/>        return out</span></pre><p id="df5e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This one is just a simple MLP with LeakyReLU activation functions. Then, I will define the loss functions to calculate them later during the training loop:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="b5b7" class="qc oa fq pz b bg qd qe l qf qg"># Create the criterion that will be used for the DE part of the loss<br/>criterion = nn.MSELoss()<br/><br/># Define the loss function for the initial condition<br/>def initial_condition_loss(y, target_value):<br/>    return nn.MSELoss()(y, target_value)</span></pre><p id="79e0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, we shall create a time array that will be used as train data, and instantiate the model, and also choose an optimization algorithm:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="26f8" class="qc oa fq pz b bg qd qe l qf qg"># Time vector that will be used as input of our NN<br/>t_numpy = np.arange(0, 5+0.01, 0.01, dtype=np.float32)<br/>t = torch.from_numpy(t_numpy).reshape(len(t_numpy), 1)<br/>t.requires_grad_(True)<br/><br/># Constant for the model<br/>k = 1<br/><br/># Instantiate one model with 50 neurons on the hidden layers<br/>model = NeuralNet(hidden_size=50)<br/><br/># Loss and optimizer<br/>learning_rate = 8e-3<br/>optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)<br/><br/># Number of epochs<br/>num_epochs = int(1e4)</span></pre><p id="1c80" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Finally, let's start our training loop:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="de89" class="qc oa fq pz b bg qd qe l qf qg">for epoch in range(num_epochs):<br/><br/>    # Randomly perturbing the training points to have a wider range of times<br/>    epsilon = torch.normal(0,0.1, size=(len(t),1)).float()<br/>    t_train = t + epsilon<br/><br/>    # Forward pass<br/>    y_pred = model(t_train)<br/><br/>    # Calculate the derivative of the forward pass w.r.t. the input (t)<br/>    dy_dt = torch.autograd.grad(y_pred, <br/>                                t_train, <br/>                                grad_outputs=torch.ones_like(y_pred), <br/>                                create_graph=True)[0]<br/><br/>    # Define the differential equation and calculate the loss<br/>    loss_DE = criterion(dy_dt + k*y_pred, torch.zeros_like(dy_dt))<br/><br/>    # Define the initial condition loss<br/>    loss_IC = initial_condition_loss(model(torch.tensor([[0.0]])), <br/>                                     torch.tensor([[1.0]]))<br/><br/>    loss = loss_DE + loss_IC<br/><br/>    # Backward pass and weight update<br/>    optimizer.zero_grad()<br/>    loss.backward()<br/>    optimizer.step()</span></pre><p id="47df" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Notice the use of <code class="cx qh qi qj pz b">torch.autograd.grad</code> function to automatically differentiate the output <em class="pr">y_pred</em> with respect to the input <em class="pr">t </em>to compute the loss function.</p><h2 id="2781" class="pa oa fq bf ob pb pc pd oe pe pf pg oh nm ph pi pj nq pk pl pm nu pn po pp pq bk">Results</h2><p id="dd55" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">After training, we can see that the loss function rapidly converges. Fig. 2 shows the loss function plotted against the epoch number, with an inset showing the region where the loss function has its fastest drop.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qk"><img src="../Images/84b3cb68272023de4d8a4d0c14124f5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s5eRYvD4OtgX8rfBBEyYTw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 2: Loss function by epochs. On the inset, we can see the region of most rapid convergence. Image by author.</figcaption></figure><p id="01d7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You probably have noticed that this neural network is not a common one. It has no train data (our train data was a hand-crafted vector of timestamps, which is simply the time domain that we wanted to investigate), so all information it gets from the system comes in the form of a loss function. Its only purpose is to solve a differential equation within the time domain it was crafted to solve. Hence, to test it, it's only fair that we use the time domain it was trained on. Fig. 3 shows a comparison between the NN prediction and the theoretical answer (that is, the analytical solution).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ql"><img src="../Images/d6fa652093b89341224469a1ca0dfa2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0qvDaisHB_Da9d_3uY5vTQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 3: Neural network prediction and the analytical solution prediction of the differential equation shown. Image by author.</figcaption></figure><p id="fc98" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can see a pretty good agreement between the two, which is very good for the neural network.</p><p id="f3d1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One caveat of this approach is that it does not generalize well for future times. Fig. 4 shows what happens if we slide our time data points five steps ahead, and the result is simply mayhem.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qm"><img src="../Images/47df1064835a3b04232af56c64966f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bft3Uy67vsgbjFmfrzBilQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 4: Neural network and analytical solution for unseen data points. Image by author.</figcaption></figure><p id="5167" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Hence, the lesson here is that this approach is made to be a numerical solver for differential equations within a time domain, and it should not be used as a regular neural network to make predictions with unseen out-of-train-domain data and expect it to generalize well.</p><h1 id="c327" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion</h1><p id="afa2" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">After all, one remaining question is:</p><blockquote class="qn qo qp"><p id="da38" class="nd ne pr nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="fq">Why bother to train a neural network that does not generalize well to unseen data, and on top of that is obviously worse than the analytical solution, since it has an intrinsic statistical error?</em></p></blockquote><p id="e033" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">First, the example provided here was an example of a differential equation whose analytical solution is known. For unknown solutions, numerical methods must be used nevertheless. With that being said, numerical methods for differential equation solving usually accumulate error. That means if you try to solve the equation for many time steps, the solution will lose its accuracy along the way. The neural network solver, on the other hand, learns how to solve the DE for all data points at each of its training epochs.</p><p id="d0f4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Another reason is that neural networks are good interpolators, so if you want to know the value of the function in unseen data (but this "unseen data" has to lie within the time interval you trained) the neural network will promptly give you a value that classic numeric methods will not be able to promptly give.</p><h1 id="fe7a" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">References</h1><p id="eb24" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">[1] Marios Mattheakis et al., <a class="af nc" href="https://arxiv.org/abs/2001.11107" rel="noopener ugc nofollow" target="_blank">Hamiltonian neural networks for solving equations of motion</a>, <em class="pr">arXiv preprint arXiv:2001.11107v5</em>, 2022.</p><p id="db8a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[2] Mario Dagrada, <a class="af nc" rel="noopener" target="_blank" href="/solving-differential-equations-with-neural-networks-afdcf7b8bcc4">Introduction to Physics-informed Neural Networks</a>, 2022.</p></div></div></div></div>    
</body>
</html>