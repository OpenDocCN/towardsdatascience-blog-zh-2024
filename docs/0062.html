<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Conditional Variational Autoencoders with Learnable Conditional Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Conditional Variational Autoencoders with Learnable Conditional Embeddings</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/conditional-variational-autoencoders-with-learnable-conditional-embeddings-e22ee5359a2a?source=collection_archive---------1-----------------------#2024-01-08">https://towardsdatascience.com/conditional-variational-autoencoders-with-learnable-conditional-embeddings-e22ee5359a2a?source=collection_archive---------1-----------------------#2024-01-08</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f2be" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">An approach to add conditions to CVAE models without retraining</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://tdrose1.medium.com/?source=post_page---byline--e22ee5359a2a--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Tim Rose" class="l ep by dd de cx" src="../Images/12bcd585b5dad388dad140b4ca049392.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*N3JzWpvjY8XM9qeWWAASDw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e22ee5359a2a--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://tdrose1.medium.com/?source=post_page---byline--e22ee5359a2a--------------------------------" rel="noopener follow">Tim Rose</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e22ee5359a2a--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="c2f0" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Requirements</h1><p id="dbad" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This article is about conditional variational autoencoders (CVAE) and requires a minimal understanding of this type of model. If you are not familiar with CVAEs, I can recommend the following articles: <a class="af ob" href="https://avandekleut.github.io/vae/" rel="noopener ugc nofollow" target="_blank">VAEs with PyTorch</a>, <a class="af ob" rel="noopener" target="_blank" href="/understanding-conditional-variational-autoencoders-cd62b4f57bf8">Understanding CVAEs</a>. Please familiarize yourself with CVAEs before reading this article. My code examples are written in Python using <a class="af ob" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">PyTorch</a> and <a class="af ob" href="https://lightning.ai/docs/pytorch/stable/" rel="noopener ugc nofollow" target="_blank">PyTorch Lightning</a>.</p><h1 id="a712" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Introduction</h1><p id="0913" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">I recently came across the paper: <a class="af ob" href="https://doi.org/10.1038/s41592-023-02035-2" rel="noopener ugc nofollow" target="_blank">“Population-level integration of single-cell datasets enables multi-scale analysis across samples”</a>, where the authors developed a CVAE model with learnable conditional embeddings. I found this idea pretty interesting and think it is worth sharing here. In this article, I will not discuss the biological applications of the proposed model, but instead, break down their idea for a simple example case on handwritten digits from the MNIST dataset.</p><p id="8187" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">So, let’s get started. The C in CVAE stands for “conditional”. This means that the encoder and decoder in addition to the input data (e.g. image for the encoder and latent vector for the decoder) are provided with an encoding for a condition. Therefore, the encoder does not need to represent the condition in the latent space since the decoder will also get this information as an extra input. Hence, the encoder can regress out the condition and learn e.g. the handwriting style as a latent representation.</p><p id="2fae" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">In practice, conditions in CVAE models are commonly one-hot encoded. E.g. for the MNIST dataset with 10 different digits, we would use a size 10 vector. PyTorch provides a function to create one-hot encodings from integer labels:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="ead4" class="oq mk fq on b bg or os l ot ou">import torch<br/><br/>digits = torch.tensor([0,1,2,3])<br/>torch.nn.functional.one_hot(digits, 10)</span></pre><pre class="ov om on oo bp op bb bk"><span id="5666" class="oq mk fq on b bg or os l ot ou">tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br/>        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],<br/>        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],<br/>        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])</span></pre><p id="2810" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">One-hot encodings work well, however, a trained model is limited to the conditions provided during training, due defined dimensions of the condition vector. If a CVAE model is trained on the MNSIT dataset for the digits 0–7 (using a size 8 one hot encoding), inference cannot be performed for digits 8 and 9.</p><p id="d56d" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">In the publication that inspired this article, the authors are interested in the latent space generated from the encoder and want to integrate new conditions into the latent space without retraining the model. To achieve this, they use embedding vectors for each condition, whose values are learned during training. If a new condition is added to the model, all model weights can be frozen except the values of a new condition embedding vector. What I find interesting about this approach, is the assumption that the model is essentially learning another latent space of condition (digit) representation hat can be used to interpolate and create embeddings for new digits the model has not seen before.</p><p id="05b8" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">Before implementing such a CVAE model, let’s make a simple CVAE with one-hot encoded conditions which we can later compare to the new model.</p><h1 id="728f" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">CVAE with one-hot encoded conditions</h1><p id="f0e2" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">First, let’s load all required python packages:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="4d02" class="oq mk fq on b bg or os l ot ou">import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as functional<br/>import torch.utils<br/>import torch.distributions<br/>import torchvision<br/>import lightning.pytorch as pl<br/><br/><br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/># If you don't have access to a GPU use device='cpu'<br/>device = 'cuda'</span></pre><p id="5cd6" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">Next, we load the MNIST data:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="9c07" class="oq mk fq on b bg or os l ot ou">data = torch.utils.data.DataLoader(<br/>        torchvision.datasets.MNIST('.', # Choose a path<br/>               transform=torchvision.transforms.ToTensor(),<br/>               download=True),<br/>        batch_size=128,<br/>        shuffle=True)</span></pre><p id="01dc" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">The model consists of an encoder and decoder (I adapted the model and plots from a minimal VAE example from <a class="af ob" href="https://avandekleut.github.io/vae/" rel="noopener ugc nofollow" target="_blank">this post</a>). We first define the encoder, which takes as input the images and one-hot encoding and outputs a latent vector z. Additionally, it computes the <a class="af ob" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank">KL-divergence</a> as an additional loss term for the CVAE:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="146a" class="oq mk fq on b bg or os l ot ou">class CondVariationalEncoder(nn.Module):<br/>   <br/>    # The encoder gets the label as a one-hot encoding<br/>    def __init__(self, latent_dims, n_classes):<br/>        super(CondVariationalEncoder, self).__init__()<br/>        # The dimensions of the one-hot encoding are concatenated to the input<br/>        self.linear1 = nn.Linear(784 + n_classes, 512)<br/>        self.linear2 = nn.Linear(512, latent_dims)<br/>        self.linear3 = nn.Linear(512, latent_dims)<br/><br/>        self.N = torch.distributions.Normal(0, 1)<br/>        # Get sampling working on GPU<br/>        self.N.loc = self.N.loc.cuda()<br/>        self.N.scale = self.N.scale.cuda()<br/>        self.kl = 0<br/>   <br/>    # The labels are provided as variable `y`<br/>    def forward(self, x, y):<br/>        x = torch.flatten(x, start_dim=1)<br/>        x = x.view(-1, 1*28*28)<br/>        # Here the label one-hot encoding is concatenated to the image<br/>        x = functional.relu(self.linear1(torch.cat((x,y),dim=1)))<br/>        # Mean<br/>        mu =  self.linear2(x)<br/>        # Variance<br/>        sigma = torch.exp(self.linear3(x))<br/>       <br/>        # Sample latent vector for images<br/>        z = mu + sigma*self.N.sample(mu.shape)<br/>        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()<br/>        return z</span></pre><p id="b37d" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">The decoder is much simpler since it just uses the images and one-hot encodings to infer images:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="a4e9" class="oq mk fq on b bg or os l ot ou">class CondVariationalDecoder(nn.Module):<br/>   <br/>    # The decoder gets the label as a one-hot encoding<br/>    def __init__(self, latent_dims, n_classes):<br/>        super(CondVariationalDecoder, self).__init__()<br/>        # The dimensions of the one-hot encoding are concatenated to the input<br/>        self.linear1 = nn.Linear(latent_dims + n_classes, 512)<br/>        self.linear2 = nn.Linear(512, 784)<br/>       <br/>    # Labels are provided as variable `y`<br/>    def forward(self, z, y):<br/>        # Here the label one-hot encoding is concatenated to the image<br/>        z = functional.relu(self.linear1(torch.cat((z,y),dim=1)))<br/>        z = torch.sigmoid(self.linear2(z))<br/>        return z.reshape((-1, 1, 28, 28))</span></pre><p id="b7cb" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">In the next step, we combine them into one module:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="e0ea" class="oq mk fq on b bg or os l ot ou">class CondVariationalAutoencoder(nn.Module):<br/>    def __init__(self, latent_dims, n_classes):<br/>        super(CondVariationalAutoencoder, self).__init__()<br/>        self.encoder = CondVariationalEncoder(latent_dims, n_classes)<br/>        self.decoder = CondVariationalDecoder(latent_dims, n_classes)<br/>   <br/>    def forward(self, x, y):<br/>        z = self.encoder(x, y)<br/>        return self.decoder(z, y)</span></pre><p id="9066" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">This module performs both the encoding and decoding in the forward pass.</p><p id="731b" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">Since we will train the model using the lightning framework, we need one more wrapper for the CVAE module:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="ecce" class="oq mk fq on b bg or os l ot ou">class CVAEModel(pl.LightningModule):<br/>    def __init__(self, latent_dims, n_classes):<br/>        super().__init__()<br/>        self.cvae = CondVariationalAutoencoder(latent_dims, n_classes)<br/>        self.n_classes = n_classes<br/><br/>    # Lightning requires a training step function in which the forward <br/>    # step is executed and loss calculated<br/>    def training_step(self, batch, batch_idx):<br/>        x, y = batch<br/>        y_oh = torch.nn.functional.one_hot(y, num_classes=self.n_classes)<br/>       <br/>        x_hat = self.cvae(x, y_oh)<br/>        loss = loss = ((x - x_hat)**2).sum() + self.cvae.encoder.kl<br/>       <br/>        self.log('Training loss', loss, on_step=False, on_epoch=True,<br/>                 logger=False, prog_bar=True)<br/>       <br/>        return loss<br/>   <br/>    # Defining the optimizer<br/>    def configure_optimizers(self):<br/>        return torch.optim.Adam(self.parameters(), lr=0.02)</span></pre><p id="6d71" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">In the lightning module, we also perform the one-hot encoding of the digit labels during each forward pass.</p><p id="10db" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">Now, let’s train the model:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="011e" class="oq mk fq on b bg or os l ot ou">latent_dims=2<br/>model = CVAEModel(latent_dims=latent_dims, n_classes=10)<br/><br/>trainer = pl.Trainer(devices=1, accelerator='gpu', max_epochs=10)<br/>trainer.fit(model, data)</span></pre><p id="9b37" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">We will not perform a quantitative evaluation of the model performance for this article, but we can visually check how well the model is able to generate digits from the latent space. To do so, we create a grid in the latent space and letting the decoder generate images of the digits we are interested in. Since we only used a 2-dimensional latent space, we can use a 2D grid. For this, we define a plotting function:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="8149" class="oq mk fq on b bg or os l ot ou">def plot_reconstructed(autoencoder, r0=(-3, 3), r1=(-3, 3),<br/>                       n=8, number=2, device='cuda'):<br/>    # Define plot array:<br/>    fig, axs = plt.subplots(n, n)<br/><br/>    # Loop over a grid in the latent space<br/>    for i, a in enumerate(np.linspace(*r1, n)):<br/>        for j, b in enumerate(np.linspace(*r0, n)):<br/>           <br/>            z = torch.Tensor([[a, b]]).to(device)<br/>            # One-hot encoding of the integer<br/>            y = functional.one_hot(torch.tensor([number]),<br/>                                   num_classes=10).to(device)<br/>            # Forwarding the data through the decoder<br/>            x_hat = autoencoder.decoder(z, y)<br/>           <br/>            x_hat = x_hat.reshape(28, 28).detach().cpu().numpy()<br/>            axs[i, j].imshow(x_hat)<br/>            axs[i, j].axis('off')<br/>    plt.show()</span></pre><p id="f7cc" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">With this we can have a look at the generated images:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="0b81" class="oq mk fq on b bg or os l ot ou">model = model.to(device)<br/>plot_reconstructed(model.cvae, number=8, device=device)</span></pre><figure class="oh oi oj ok ol oz ow ox paragraph-image"><div class="ow ox oy"><img src="../Images/0082c26ff50997ffdd2d5e627ddf604d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*Jf4frXLMTLPxs0rj8q_Djw.png"/></div><figcaption class="pb pc pd ow ox pe pf bf b bg z dx">Inferred images for the number 8 from a over a grid of the latent space. Image created by the author.</figcaption></figure><p id="1324" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">Especially in the center of the latent space, the digits are very clear (Try to use other numbers as conditions and plot them yourself). Overall, the decoder is able to generate readable images of handwritten digits for all the provided numbers during training.</p><p id="3feb" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">We are also interested in the latent space representation of all the digits in the training data. As mentioned before, we expect the model to remove digit-related differences in the latent space and, therefore, e.g. no clusters of images from the same digit. Below, we can visualize the 2D latent space and color it by the digit label. Further, we expect the latent space to be normally distributed around zero (due to our KL loss term).</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="81b9" class="oq mk fq on b bg or os l ot ou">def plot_latent_cvae(autoencoder, data, num_batches=100, device='cpu'):<br/>    for i, (x, y) in enumerate(data):<br/>        z = autoencoder.encoder(x.to(device),<br/>                                torch.nn.functional.one_hot(torch.tensor(y),<br/>                                                            num_classes=10).to(device))<br/>        z = z.detach().cpu().numpy()<br/>        plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10')<br/>        if i &gt; num_batches:<br/>            plt.colorbar()<br/>            break<br/>model = model.to(device)<br/>plot_latent_cvae(model.cvae, data, device=device)</span></pre><figure class="oh oi oj ok ol oz ow ox paragraph-image"><div class="ow ox pg"><img src="../Images/b31dbe1d53e4a044e4e51587f5943a22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*Js2hTrjEEy0sdGbScIwoeA.png"/></div><figcaption class="pb pc pd ow ox pe pf bf b bg z dx">Latent space of the CVAE model on the training data colored by the condition. Image created by the author.</figcaption></figure><h1 id="758b" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Learnable conditional embeddings</h1><p id="401b" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">After building a first model, we switch to learnable embeddings. Instead of using one-hot encoding, we will now use learnable embeddings. These are also unique vectors for each condition, but with values that will be updated in the training process. The model optimizer will update the embedding vectors together with all other model parameters to improve the loss during training.</p><p id="f065" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">So we do not only have a latent space for images but also an embedding space for the conditions (digits). One of the central aspects of the paper is that the model is encoding information about (in our example case) the condition in the embedding vector. This means that we can add new digits to the model that were not included during training and the model might be able to infer the correct digit just with a new adjusted condition embedding. For this, all model weights are frozen and only the embeddings for the new digits which should be added to the model are optimized. In the publication, the authors intend to add a new condition to the latent space, but in this article, we will check how well the model can generate images of unseen digits.</p><p id="283e" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">First, we train a model on all digits to check the general ability of the CVAE model to train with learnable embeddings. For this, we define a new lightning module, but in this case include an embedding variable, which stores the embeddings and provides them in the forward pass:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="8470" class="oq mk fq on b bg or os l ot ou">class eCVAEModel(pl.LightningModule):<br/>    # Here we need to define the number of classes and embedding dimensions<br/>    def __init__(self, latent_dims, n_classes, embedding_dims):<br/>        super().__init__()<br/>        # We can use the CVAE model from the previous notebook,<br/>        # but instead of using the number of classes for a one-hot encoding,<br/>        # we use the embedding dimensions<br/>        self.cvae = CondVariationalAutoencoder(latent_dims, embedding_dims)<br/>        self.n_classes = n_classes<br/>        self.embedding_dims = embedding_dims<br/>        self.embed_cond = nn.Embedding(num_embeddings=n_classes,<br/>                                       embedding_dim=embedding_dims,<br/>                                       max_norm=True)<br/><br/>    def training_step(self, batch, batch_idx):<br/>        x, y = batch<br/><br/>        # Instead of a one-hot encoding,<br/>        # the embeddings are used as conditional variables<br/>        x_hat = self.cvae(x, self.embed_cond(y))<br/>        loss = loss = ((x - x_hat)**2).sum() + self.cvae.encoder.kl<br/>       <br/>        self.log('Training loss', loss, on_step=False, on_epoch=True,<br/>                 logger=False, prog_bar=True)<br/>       <br/>        return loss<br/>   <br/>    # Defining the optimizer<br/>    def configure_optimizers(self):<br/>        return torch.optim.Adam(self.parameters(), lr=0.02)</span></pre><p id="c82c" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">Next, we can train the model:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="b8e4" class="oq mk fq on b bg or os l ot ou">emodel = eCVAEModel(latent_dims=latent_dims, n_classes=10, embedding_dims=5)<br/><br/>trainer = pl.Trainer(devices=1, accelerator='gpu', max_epochs=10)<br/>trainer.fit(emodel, data)</span></pre><p id="5011" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">After training, we need a slightly updated plotting function to show generated images from the latent space that utilizes the condition embeddings:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="89de" class="oq mk fq on b bg or os l ot ou">def plot_reconstructed_ecvae(model, r0=(-3, 3), r1=(-3, 3),<br/>                             n=8, number=2, device='cuda'):<br/>    # Define plot array:<br/>    fig, axs = plt.subplots(n, n)<br/><br/>    # Loop over a grid in the latent space<br/>    for i, a in enumerate(np.linspace(*r1, n)):<br/>        for j, b in enumerate(np.linspace(*r0, n)):<br/>           <br/>            z = torch.Tensor([[a, b]]).to(device)<br/>            # One-hot encoding of the integer<br/>            y = model.embed_cond(torch.tensor([number]).to(device))<br/>            # Forwarding the data through the decoder<br/>            x_hat = model.cvae.decoder(z, y)<br/>           <br/>            x_hat = x_hat.reshape(28, 28).to('cpu').detach().numpy()<br/>            axs[i, j].imshow(x_hat)<br/>            axs[i, j].axis('off')<br/>    plt.show()</span></pre><p id="d566" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">And we can finally generate new images. We again plot number 8 digits and can see that the model is equally well able to generate the digit.</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="90b5" class="oq mk fq on b bg or os l ot ou">emodel = emodel.to(device)<br/>plot_reconstructed_ecvae(emodel, number=8, device=device)</span></pre><figure class="oh oi oj ok ol oz ow ox paragraph-image"><div class="ow ox oy"><img src="../Images/eecb76b70fc22020189c4b8555bb9534.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*byBeJGD5ZEruheBjzQEmsw.png"/></div><figcaption class="pb pc pd ow ox pe pf bf b bg z dx">Inferred images for the number 8 from a over a grid of the latent space using learnable condition embeddings. Image created by the author.</figcaption></figure><p id="3371" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">To visualize the latent space, we also need a slightly updated plotting function:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="8fd6" class="oq mk fq on b bg or os l ot ou">def plot_latent_ecvae(model, data, num_batches=100, device='cpu'):<br/>    for i, (x, y) in enumerate(data):<br/>        y_embed = model.embed_cond(torch.tensor(y, device=device))<br/>        z = model.cvae.encoder(x.to(device), y_embed)<br/>        z = z.detach().cpu().numpy()<br/>        plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10')<br/>        if i &gt; num_batches:<br/>            plt.colorbar()<br/>            break<br/><br/>model = model.to(device)<br/>plot_latent_ecvae(emodel, data, device=device)</span></pre><figure class="oh oi oj ok ol oz ow ox paragraph-image"><div class="ow ox pg"><img src="../Images/39195f96429cbe2746e91b97cedfcd2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*j5ZHtcK5eIcW34A6C9KRgg.png"/></div><figcaption class="pb pc pd ow ox pe pf bf b bg z dx">Latent space of the CVAE model with learnable condition embeddings on the training data colored by the condition. Image created by the author.</figcaption></figure><p id="b929" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">While we can see a slightly changes distribution, we cannot see strong digit-related clusters. This shows that the model using learnable embeddings is similarly able to condition on the digits as the one hot encoded model (While the model is actually using fewer parameters since we only use an embedding of size 5 instead of 10 for the one hot encoding).</p><h1 id="932b" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Adding new conditions to the model</h1><p id="5bb3" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Finally, we are going to train the model only on the digits 0–7. After training, we then optimize the condition embeddings for the digits 8 and 9 while freezing all other model weights. This allows us to add these conditions to the latent space (and generate new images) without retraining the whole model. To do so, we create two new dataloaders, one for providing images of the digits 0–7 (<strong class="nh fr">datatrain</strong>) and another for providing the images for the digits 8 and 9 (<strong class="nh fr">data89</strong>):</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="d54c" class="oq mk fq on b bg or os l ot ou"># Creating dataloaders excluding 8 &amp; 9 digits<br/># Code adapted from:<br/># https://stackoverflow.com/questions/75034387/remove-digit-from-mnist-pytorch<br/>dstrain = torchvision.datasets.MNIST('/scratch/trose/mnist',<br/>                                transform=torchvision.transforms.ToTensor(),<br/>                                download=True)<br/>idxn9 = dstrain.targets!=9<br/>idxn8 = dstrain.targets!=8<br/>idx = idxn9 &amp; idxn8<br/><br/>dstrain.targets = dstrain.targets[idx]<br/>dstrain.data = dstrain.data[idx]<br/><br/># Data containing only 8 &amp; 9 digits<br/>ds89 = torchvision.datasets.MNIST('/scratch/trose/mnist',<br/>                                transform=torchvision.transforms.ToTensor(),<br/>                                download=True)<br/>idx9 = ds89.targets==9<br/>idx8 = ds89.targets==8<br/>idx89 = idx9 | idx8<br/>ds89.targets = ds89.targets[idx89]<br/>ds89.data = ds89.data[idx89]<br/><br/>datatrain = torch.utils.data.DataLoader(dstrain, batch_size=128, shuffle=True)<br/>data89 = torch.utils.data.DataLoader(ds89, batch_size=128, shuffle=True)</span></pre><p id="6f31" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">We then first train the model on images of 0–7 digits:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="392c" class="oq mk fq on b bg or os l ot ou">emodel89 = eCVAEModel(latent_dims=latent_dims, n_classes=10, embedding_dims=5)<br/><br/>trainer = pl.Trainer(devices=1, accelerator='gpu', max_epochs=10)<br/>trainer.fit(emodel89, datatrain)</span></pre><p id="4781" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">And then freeze all model parameters, except the condition embeddings. After that, we then optimize the embeddings only for images of the digits 8 and 9:</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="bb69" class="oq mk fq on b bg or os l ot ou"># Freeze model parameters<br/>for name, param in emodel89.named_parameters():<br/>    if name == 'embed_cond.weight':<br/>        param.requires_grad = True<br/>    else:<br/>        param.requires_grad = False<br/><br/># Training just on 8 &amp; 9 digits on frozen weights<br/># The model is using the previously created embedding vectors in the model<br/># that were not updated for 8 &amp; 9 in the previous training.<br/>trainer89 = pl.Trainer(devices=1, accelerator='gpu', max_epochs=10)<br/>trainer89.fit(emodel89, data89)</span></pre><p id="b5c4" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">Let’s visualize generated numbers and the latent space. Below we can see that the model is able to generate the images of the digit 8 similarly to our previous models, even though the model was not trained on these images and only the condition embedding vector has been updated.</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="cb23" class="oq mk fq on b bg or os l ot ou">emodel89 = emodel.to(device)<br/>plot_reconstructed_ecvae(emodel89, number=8, device=device)</span></pre><figure class="oh oi oj ok ol oz ow ox paragraph-image"><div class="ow ox oy"><img src="../Images/eecb76b70fc22020189c4b8555bb9534.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*byBeJGD5ZEruheBjzQEmsw.png"/></div><figcaption class="pb pc pd ow ox pe pf bf b bg z dx">Inferred images for the number 8 from a over a grid of the latent space using learnable condition embeddings, which were not part of the training of the full model. Image created by the author.</figcaption></figure><p id="61bf" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">If we visualize the latent space, we neither see clusters of 8 and 9 digits nor any strong outliers in the distribution.</p><pre class="oh oi oj ok ol om on oo bp op bb bk"><span id="c00f" class="oq mk fq on b bg or os l ot ou">emodel89 = emodel89.to(device)<br/>plot_latent_ecvae(emodel89, data, device=device)</span></pre><figure class="oh oi oj ok ol oz ow ox paragraph-image"><div class="ow ox pg"><img src="../Images/36c1a488d22e9868c97c4d16c51529d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*g29SDhRJ5--pH6c_t0hWpw.png"/></div><figcaption class="pb pc pd ow ox pe pf bf b bg z dx">Latent space of the CVAE model with learnable condition embeddings on the training data colored by the condition, where the digits 8&amp;9 have been added without retraining of the model. Image created by the author.</figcaption></figure><p id="dc11" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">While we did not do any kind of systematic evaluation of the model performance in this article, we can see that learned embedding can be very useful for adding new conditions to CVAE models, without retraining the whole model.</p><p id="e994" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">One-hot encodings are commonly used in machine learning, but I hope to have shown you an interesting alternative for it in CVAE models. If you are also interested in the applications of such an approach (e.g. in biology), I recommend the publication <a class="af ob" href="https://doi.org/10.1038/s41592-023-02035-2" rel="noopener ugc nofollow" target="_blank">“Population-level integration of single-cell datasets enables multi-scale analysis across samples”</a>, which was the basis for this article. It also contains a few other interesting ideas for customizing CVAE models for specific applications.</p><p id="47af" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">Thank you for reading and feel free to explore the source code for this article and play with the models. You can find all the code on GitHub:<a class="af ob" href="https://github.com/tdrose/blogpost-subfigures-code" rel="noopener ugc nofollow" target="_blank"> </a><a class="af ob" href="https://github.com/tdrose/medium-articles-code" rel="noopener ugc nofollow" target="_blank">https://github.com/tdrose/medium-articles-code</a><a class="af ob" href="https://github.com/tdrose/lightning-cvae." rel="noopener ugc nofollow" target="_blank">.</a></p><p id="f2c2" class="pw-post-body-paragraph nf ng fq nh b go oc nj nk gr od nm nn no oe nq nr ns of nu nv nw og ny nz oa fj bk">All images were created by the author.</p></div></div></div></div>    
</body>
</html>