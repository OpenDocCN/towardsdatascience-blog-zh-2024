["```py\n!pip install langchain langchain-openai chromadb renumics-spotlight\n%env OPENAI_API_KEY=<your-api-key>\n```", "```py\nimport hashlib\nimport json\nfrom langchain_core.documents import Document\n\ndef stable_hash_meta(doc: Document) -> str:\n    \"\"\"\n    Stable hash document based on its metadata.\n    \"\"\"\n    return hashlib.sha1(json.dumps(doc.metadata, sort_keys=True).encode()).hexdigest()\n\n...\nsplits = text_splitter.split_documents(docs)\nsplits_ids = [\n    {\"doc\": split, \"id\": stable_hash_meta(split.metadata)} for split in splits\n]\n\nexisting_ids = docs_vectorstore.get()[\"ids\"]\nnew_splits_ids = [split for split in splits_ids if split[\"id\"] not in existing_ids]\n\ndocs_vectorstore.add_documents(\n    documents=[split[\"doc\"] for split in new_splits_ids],\n    ids=[split[\"id\"] for split in new_splits_ids],\n)\ndocs_vectorstore.persist()\n```", "```py\nfrom ragas.testset import TestsetGenerator\n\ngenerator = TestsetGenerator.from_default(\n    openai_generator_llm=\"gpt-3.5-turbo-16k\", \n    openai_filter_llm=\"gpt-3.5-turbo-16k\"\n)\n\ntestset_ragas_gpt35 = generator.generate(docs, 100)\n```", "```py\nfor i, row in df_questions_answers.iterrows():\n    if row[\"answer\"] is None or pd.isnull(row[\"answer\"]):\n        response = rag_chain.invoke(row[\"question\"])\n\n        df_questions_answers.loc[df_questions_answers.index[i], \"answer\"] = response[\n            \"answer\"\n        ]\n        df_questions_answers.loc[df_questions_answers.index[i], \"source_documents\"] = [\n            stable_hash_meta(source_document.metadata)\n            for source_document in response[\"source_documents\"]\n        ] \n```", "```py\n# prepare the dataframe for evaluation\ndf_qa_eval = df_questions_answers.copy()\n\n# adapt the ground truth to the ragas naming and format\ndf_qa_eval.rename(columns={\"ground_truth\": \"ground_truths\"}, inplace=True)\ndf_qa_eval[\"ground_truths\"] = [\n    [gt] if not isinstance(gt, list) else gt for gt in df_qa_eval[\"ground_truths\"]\n]\n```", "```py\n# evaluate the answer correctness if not already done\nfields = [\"question\", \"answer\", \"contexts\", \"ground_truths\"]\nfor i, row in df_qa_eval.iterrows():\n    if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n        evaluation_result = evaluate(\n            Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n            [answer_correctness],\n        )\n        df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n            \"answer_correctness\"\n        ] \n```", "```py\ndf_questions_answers[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]\n```", "```py\n# Explode 'source_documents' so each document ID is in its own row alongside the question ID\ndf_questions_exploded = df_qa_eval.explode(\"source_documents\")\n\n# Group by exploded 'source_documents' (document IDs) and aggregate\nagg = (\n    df_questions_exploded.groupby(\"source_documents\")\n    .agg(\n        num_questions=(\"id\", \"count\"),  # Count of questions referencing the document\n        question_ids=(\n            \"id\",\n            lambda x: list(x),\n        ),  # List of question IDs referencing the document\n    )\n    .reset_index()\n    .rename(columns={\"source_documents\": \"id\"})\n)\n\n# Merge the aggregated information back into df_documents\ndf_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n\n# Use apply to replace NaN values with empty lists for 'question_ids'\ndf_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n    lambda x: x if isinstance(x, list) else []\n)\n# Replace NaN values in 'num_questions' with 0\ndf_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)\n```", "```py\ndf = pd.concat([df_qa_eval, df_documents_agg], axis=0)\n```", "```py\n umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit\numap_all = umap.transform(df[\"embedding\"].values.tolist())\ndf[\"umap\"] = umap_all.tolist() \n```", "```py\nquestion_embeddings = np.array(df[df[\"question\"].notna()][\"embedding\"].tolist())\ndf[\"nearest_question_dist\"] = [  # brute force, could be optimized using ChromaDB\n    np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings)])\n    for doc_emb in df[\"embedding\"].values\n]\n```", "```py\nimport pandas as pd\ndf = pd.read_parquet(\"df_f1_rag_docs_and_questions.parquet\")\n```", "```py\nfrom renumics import spotlight\n\nspotlight.show(df)\nspotlight.show(\n    df,\n    layout=\"/home/markus/Downloads/layout_rag_1.json\",\n    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n)\n```"]