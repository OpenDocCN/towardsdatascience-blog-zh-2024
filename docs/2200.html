<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Introducing Semantic Tag Filtering: Enhancing Retrieval with Tag Similarity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Introducing Semantic Tag Filtering: Enhancing Retrieval with Tag Similarity</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introducing-semantic-tag-filtering-enhancing-retrieval-with-tag-similarity-4f1b2d377a10?source=collection_archive---------2-----------------------#2024-09-09">https://towardsdatascience.com/introducing-semantic-tag-filtering-enhancing-retrieval-with-tag-similarity-4f1b2d377a10?source=collection_archive---------2-----------------------#2024-09-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="5e39" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">Semantic Tag Filtering</h2><div/><div><h2 id="325a" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">How to use Semantic Similarity to improve tag filtering</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ardito.bryan?source=post_page---byline--4f1b2d377a10--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Michelangiolo Mazzeschi" class="l ep by dd de cx" src="../Images/9211748ac638d2ed07679ac73ea17296.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*MkUxrUogzkaAyb_Nf76wRQ.jpeg"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4f1b2d377a10--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@ardito.bryan?source=post_page---byline--4f1b2d377a10--------------------------------" rel="noopener follow">Michelangiolo Mazzeschi</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4f1b2d377a10--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">3</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><blockquote class="mo mp mq"><p id="5413" class="mr ms mt mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">***To understand this article, the knowledge of both <strong class="mu ga">Jaccard similarity </strong>and <strong class="mu ga">vector search </strong>is required. The implementation of this algorithm has been released on <a class="af no" href="https://github.com/atlantis-nova/simtag" rel="noopener ugc nofollow" target="_blank">GitHub</a> and is fully open-source.</p></blockquote><p id="2db5" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Over the years, we have discovered how to retrieve information from different modalities, such as <strong class="mu ga">numbers</strong>, <strong class="mu ga">raw text</strong>, <strong class="mu ga">images</strong>, and also <strong class="mu ga">tags</strong>.<br/>With the growing popularity of customized UIs, <strong class="mu ga">tag search systems</strong> have become a convenient way of easily filtering information with a good degree of accuracy. Some cases <strong class="mu ga">where tag search is commonly employed</strong> are the retrieval of social media posts, articles, games, movies, and even resumes.</p><p id="eb85" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">However, traditional tag search lacks flexibility. If we are to filter the samples that contain exactly the given tags, there might be cases when, especially for databases containing only a few thousand samples, <strong class="mu ga">there might not be any (or only a few) matching samples for our query</strong>.</p></div></div><div class="np"><div class="ab cb"><div class="lr nq ls nr lt ns cf nt cg nu ci bh"><figure class="ny nz oa ob oc np od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw nx"><img src="../Images/8deb50b654d346627375b7d90a6963a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*IeTz_HjMjMX_bmI76khBYA.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">difference of the two searches in front of a scarcity of results, Image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><blockquote class="mo mp mq"><p id="e558" class="mr ms mt mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><em class="fq">***Through the following article I am trying to introduce several new algorithms that, to the extent of my knowledge, I have been unable to find. </em><strong class="mu ga"><em class="fq">I am open to criticism</em></strong><em class="fq"> and </em><strong class="mu ga"><em class="fq">welcome any feedback.</em></strong></p></blockquote><h2 id="14c8" class="op oq fq bf or os ot ou ov ow ox oy oz nb pa pb pc nf pd pe pf nj pg ph pi fw bk">How does traditional tag search work?</h2><p id="89e4" class="pw-post-body-paragraph mr ms fq mu b gt pj mw mx gw pk mz na nb pl nd ne nf pm nh ni nj pn nl nm nn fj bk">Traditional systems employ an algorithm called <strong class="mu ga">Jaccard similarity</strong> (commonly executed through the <strong class="mu ga">minhash algo</strong>), which is able to compute the similarity between two sets of elements (in our case, those elements are tags). As previously clarified, the search is not flexible at all (sets either <strong class="mu ga">contain </strong>or <strong class="mu ga">do not contain</strong> the queried tags).</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw po"><img src="../Images/db30a66064d36f7c16764a5cf985b2d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sanmTvAlBwt7qfKcG7sb7w.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">example of a <strong class="bf or">simple AND bitwise operation</strong> (this is not Jaccard similarity, but can give you an approximate idea of the filtering method), Image by author</figcaption></figure><h2 id="f5b7" class="op oq fq bf or os ot ou ov ow ox oy oz nb pa pb pc nf pd pe pf nj pg ph pi fw bk">Can we do better?</h2><p id="0bed" class="pw-post-body-paragraph mr ms fq mu b gt pj mw mx gw pk mz na nb pl nd ne nf pm nh ni nj pn nl nm nn fj bk">What if, instead, rather than just filtering a sample from matching tags, we could take into account all the other labels in the sample that are not identical, but are <strong class="mu ga">similar</strong> to our chosen tags? We could be making the algorithm more flexible, expanding the results to non-perfect matches, but still good matches. We would be applying <strong class="mu ga">semantic similarity </strong>directly to<strong class="mu ga"> </strong>tags, rather than text.</p><h1 id="740f" class="pp oq fq bf or pq pr gv ov ps pt gy oz pu pv pw px py pz qa qb qc qd qe qf qg bk">Introducing Semantic Tag Search</h1><p id="9009" class="pw-post-body-paragraph mr ms fq mu b gt pj mw mx gw pk mz na nb pl nd ne nf pm nh ni nj pn nl nm nn fj bk">As briefly explained, this new approach attempts to combine the <strong class="mu ga">capabilities of semantic search</strong> with tag filtering systems. For this algorithm to be built, we need only one thing:</p><ul class=""><li id="f14d" class="mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn qh qi qj bk">A database of <strong class="mu ga">tagged samples</strong></li></ul><p id="5806" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The reference data I will be using is the open-source collection of the <strong class="mu ga">Steam game library</strong> (<a class="af no" href="https://www.kaggle.com/datasets/fronkongames/steam-games-dataset" rel="noopener ugc nofollow" target="_blank">downloadable from Kaggle</a> — <a class="af no" href="https://www.mit.edu/~amini/LICENSE.md" rel="noopener ugc nofollow" target="_blank">MIT License</a>) — approx. 40,000 samples, which is a good amount of samples to test our algorithm. As we can see from the displayed dataframe, each game has several assigned tags, with over 400 unique tags in our database.</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qk"><img src="../Images/98840614196d9a8643fe33a8f184b312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BVXDImlB3DzFchPA6nUV9A.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">Screenshot of the Steam dataframe available in the example notebook, Image by author</figcaption></figure><p id="5337" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Now that we have our starting data, we can proceed: the algorithm will be articulated in the following steps:</p><ol class=""><li id="08f2" class="mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn ql qi qj bk">Extracting tags relationships</li><li id="eda0" class="mr ms fq mu b gt qm mw mx gw qn mz na nb qo nd ne nf qp nh ni nj qq nl nm nn ql qi qj bk">Encoding queries and samples</li><li id="dd67" class="mr ms fq mu b gt qm mw mx gw qn mz na nb qo nd ne nf qp nh ni nj qq nl nm nn ql qi qj bk">Perform the semantic tag search using vector retrieval</li><li id="d439" class="mr ms fq mu b gt qm mw mx gw qn mz na nb qo nd ne nf qp nh ni nj qq nl nm nn ql qi qj bk">Validation</li></ol><p id="9e77" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">In this article, <strong class="mu ga">I will only explore the math</strong> behind this new approach (for an in-depth <a class="af no" href="https://github.com/atlantis-nova/simtag/blob/main/notebooks/steam_example.ipynb" rel="noopener ugc nofollow" target="_blank">explanation of the code with a working demo</a>, please refer to the following notebook: instructions on how to use simtag are available in the <a class="af no" href="https://github.com/atlantis-nova/simtag" rel="noopener ugc nofollow" target="_blank">README.md file on root</a>).</p><h2 id="a81e" class="op oq fq bf or os ot ou ov ow ox oy oz nb pa pb pc nf pd pe pf nj pg ph pi fw bk">1. Extracting tag relationships</h2><p id="cae1" class="pw-post-body-paragraph mr ms fq mu b gt pj mw mx gw pk mz na nb pl nd ne nf pm nh ni nj pn nl nm nn fj bk">The first question that comes to mind is how can we find the <strong class="mu ga">relationships between our tags</strong>. Note that there are several algorithms used to obtain the same result:</p><ul class=""><li id="6b62" class="mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn qh qi qj bk">Using s<strong class="mu ga">tatistical methods<br/></strong>The simplest employable method we can use to extract tag relationships is called <strong class="mu ga">co-occurrence matrix</strong>, which is the format that (for both its effectiveness and simplicity) I will employ in this article.</li><li id="da22" class="mr ms fq mu b gt qm mw mx gw qn mz na nb qo nd ne nf qp nh ni nj qq nl nm nn qh qi qj bk">Using <strong class="mu ga">Deep Learning<br/></strong>The most advanced ones are all based on Embeddings neural networks (such as <a class="af no" href="https://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">Word2Vec</a> in the past, now it is common to use transformers, such as LLMs) that can extract the semantic relationships between samples. Creating a neural network to extract tag relationships (in the form of an autoencoder) is a possibility, and it is usually advisable when facing certain circumstances.</li><li id="6194" class="mr ms fq mu b gt qm mw mx gw qn mz na nb qo nd ne nf qp nh ni nj qq nl nm nn qh qi qj bk">Using <strong class="mu ga">a pre-trained model<br/></strong>Because tags are defined using human language, it is possible to employ existing pre-trained models to compute already existing similarities. This will likely be much faster and less troubling. However, each dataset has its uniqueness. Using a pre-trained model <strong class="mu ga">will ignore the customer behavior</strong>.<br/>Ex. We will later see how 2D has a strong relationship with Fantasy: such a pair will never be discovered using pre-trained models.</li></ul><p id="5001" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The choice of the algorithm may depend on many factors, especially when we have to work with a huge data pool or we have scalability concerns (ex. <strong class="mu ga"># tags will equal our vector length</strong>: if we have too many tags, we need to use Machine Learning to stem this problem.</p><h2 id="5c69" class="op oq fq bf or os ot ou ov ow ox oy oz nb pa pb pc nf pd pe pf nj pg ph pi fw bk">a. Build co-occurence matrix using Michelangiolo similarity</h2><p id="90b8" class="pw-post-body-paragraph mr ms fq mu b gt pj mw mx gw pk mz na nb pl nd ne nf pm nh ni nj pn nl nm nn fj bk">As mentioned, I will be using the <strong class="mu ga">co-occurrence matrix </strong>as a means to extract these relationships. My goal is to find the relationship between <strong class="mu ga">every pair of tags</strong>, and I will be doing so by applying the following count across the entire collection of samples using IoU (Intersection over Union) over the set of all samples (S):</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qr"><img src="../Images/edef7f7dd297a3d5f926a284a19e904b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mN67AIbYCqyBwl9vXiEPJA.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">formula to compute the <strong class="bf or">similarity between a pair of tags, </strong>Image by author</figcaption></figure><p id="91ae" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">This algorithm is very similar to Jaccard similarity. While it operates on samples, the one I introduce operates on elements, but since (<strong class="mu ga">as of my knowledge</strong>) this specific application has not been codified, yet, we can name it <strong class="mu ga">Michelangiolo similarity. (</strong>To be fair, the use of this algorithm has been previously mentioned in a <a class="af no" href="https://stackoverflow.com/questions/76910725/pyspark-getting-jaccard-similarity-from-co-ocurrence-matrix" rel="noopener ugc nofollow" target="_blank">StackOverflow question</a>, yet, never codified).</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qs"><img src="../Images/52f16770cab51be3ec71fcaf42785d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9sno9Qir2TzorUoN5yLu1w.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">difference between<strong class="bf or"> Jaccard similarity</strong> and <strong class="bf or">Michelangiolo similarity, </strong>Image by author</figcaption></figure><p id="8396" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">For 40,000 samples, it takes about an hour to extract the <strong class="mu ga">similarity matrix</strong>, this will be the result:</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qt"><img src="../Images/c04b998382818044cd5070897e0922e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FAuknha5wIkoi0kQwcU_VA.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx"><strong class="bf or">co-occurrence matrix</strong> of all unique tags in our sample list <strong class="bf or">S, </strong>Image by author</figcaption></figure><p id="decd" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Let us make a manual check of the top 10 samples for some very common tags, to see if the result makes sense:</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qu"><img src="../Images/d5702dfcde2134de320553a4ddd6af25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KyszYGiXAlkTw0c16UAzGA.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">sample relationships extracted from the <strong class="bf or">co-occurrence matrix, </strong>Image by author</figcaption></figure><p id="6418" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The result looks very promising! We started from plain categorical data (only convertible to 0 and 1), but we have extracted the semantic relationship between tags (without even using a neural network).</p><h2 id="c557" class="op oq fq bf or os ot ou ov ow ox oy oz nb pa pb pc nf pd pe pf nj pg ph pi fw bk">b. Use a pre-trained neural network</h2><p id="2769" class="pw-post-body-paragraph mr ms fq mu b gt pj mw mx gw pk mz na nb pl nd ne nf pm nh ni nj pn nl nm nn fj bk">Equally, we can extract existing relationships between our samples using <a class="af no" href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2" rel="noopener ugc nofollow" target="_blank">a pre-trained encoder</a>. This solution, however, ignores the relationships that can only be extracted from our data, only focusing on existing semantic relationships of the human language. This may not be a well-suited solution to work on top of retail-based data.</p><p id="16e2" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">On the other hand, by using a neural network we would not need to build a relationship matrix: hence, this is a proper solution for scalability. For example, if we had to analyze a large batch of Twitter data, we reach 53.300 tags. Computing a co-occurrence matrix from this number of tags will result in a sparse matrix of size <strong class="mu ga">2,500,000,000</strong> (quite a non-practical feat). Instead, by using a standard encoder that outputs a vector length of 384, the resulting matrix will have a total size of <strong class="mu ga">19,200,200</strong>.</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qv"><img src="../Images/4e906835f1508685df5e19c9bcddf4a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-UUYS-vBnw_ilGCOIyWkFg.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">snapshot of an encoded set of tags usin a pre-trained encoder</figcaption></figure><h2 id="6493" class="op oq fq bf or os ot ou ov ow ox oy oz nb pa pb pc nf pd pe pf nj pg ph pi fw bk">2. Encoding queries and samples</h2><p id="0038" class="pw-post-body-paragraph mr ms fq mu b gt pj mw mx gw pk mz na nb pl nd ne nf pm nh ni nj pn nl nm nn fj bk">Our goal is to build a search engine capable of supporting the semantic tag search: with the format we have been building, the only technology capable of supporting such an enterprise is <strong class="mu ga">vector search</strong>. Hence, we need to find a proper encoding algorithm to convert both our samples and queries into vectors.</p><p id="fdaf" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">In most encoding algorithms, <strong class="mu ga">we encode both queries and samples using the same algorithm</strong>. However, each sample contains more than one tag, each represented by a different set of relationships that <strong class="mu ga">we need to capture in a single vector</strong>.</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qw"><img src="../Images/133e3c177bc6e5f2c72451c58c86686f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Lo877bGeorDY2_NWqlu7w.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx"><strong class="bf or">Covariate Encoding, </strong>Image by author</figcaption></figure><p id="4522" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">In addition, we need to address the aforementioned problem of scalability, and we will do so by using a <strong class="mu ga">PCA module</strong> (when we use a co-occurrence matrix, instead, we can skip the PCA because there is no need to compress our vectors).</p><p id="28ab" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">When the number of tags becomes too large, we need to abandon the possibility of computing a co-occurrence matrix, because it scales at a squared rate. Therefore, we can extract the vector of each existing tag using a pre-trained neural network (the first step in the PCA module). For example, <strong class="mu ga">all-MiniLM-L6-v2</strong> converts each tag into a vector of length 384.</p><p id="c51f" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">We can then transpose the obtained matrix, and compress it: we will initially encode our queries/samples using 1 and 0 for the available tag indexes, resulting in an initial vector of the same length as our initial matrix (53,300). At this point, we can use our pre-computed PCA instance <strong class="mu ga">to compress the same sparse vector in 384 dims</strong>.</p><h2 id="c0c9" class="op oq fq bf or os ot ou ov ow ox oy oz nb pa pb pc nf pd pe pf nj pg ph pi fw bk">Encoding samples</h2><p id="f936" class="pw-post-body-paragraph mr ms fq mu b gt pj mw mx gw pk mz na nb pl nd ne nf pm nh ni nj pn nl nm nn fj bk">In the case of our samples, the process ends just right after the PCA compression (when activated).</p><h2 id="a021" class="op oq fq bf or os ot ou ov ow ox oy oz nb pa pb pc nf pd pe pf nj pg ph pi fw bk">Encoding queries: Covariate Encoding</h2><p id="bcc8" class="pw-post-body-paragraph mr ms fq mu b gt pj mw mx gw pk mz na nb pl nd ne nf pm nh ni nj pn nl nm nn fj bk">Our query, however, needs to be encoded differently: we need to take into account <strong class="mu ga">the relationships associated with each existing tag</strong>. This process is executed by first summing our compressed vector to the compressed matrix (the total of all existing relationships). Now that we have obtained a matrix (384x384), <strong class="mu ga">we will need to average it</strong>, obtaining our query vector.</p><p id="d2df" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Because we will make use of Euclidean search, it will first prioritize the search for features with the highest score (ideally, the one we activated using the number 1), but it will also consider the additional minor scores.</p><h2 id="d913" class="op oq fq bf or os ot ou ov ow ox oy oz nb pa pb pc nf pd pe pf nj pg ph pi fw bk">Weighted search</h2><p id="0dc9" class="pw-post-body-paragraph mr ms fq mu b gt pj mw mx gw pk mz na nb pl nd ne nf pm nh ni nj pn nl nm nn fj bk">Because we are averaging vectors together, we can even apply a weight to this calculation, and the vectors will be impacted differently from the query tags.</p><h2 id="b8e5" class="op oq fq bf or os ot ou ov ow ox oy oz nb pa pb pc nf pd pe pf nj pg ph pi fw bk">3. Perform the semantic tag search using vector retrieval</h2><p id="17fb" class="pw-post-body-paragraph mr ms fq mu b gt pj mw mx gw pk mz na nb pl nd ne nf pm nh ni nj pn nl nm nn fj bk">The question you might be asking is: why did we undergo <strong class="mu ga">this complex encoding process</strong>, rather than just inputting the pair of tags into a function and obtaining a score — <strong class="mu ga">f(query, sample)</strong>?</p><p id="182b" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">If you are familiar with vector-based search engines, you will already know the answer. By performing calculations by pairs, in the case of just 40,000 samples the computing power required is huge (can take up to <strong class="mu ga">10 seconds for a single query</strong>): it is not a scalable practice. However, if we choose to perform a vector retrieval of 40,000 samples, the search will finish in <strong class="mu ga">0.1 seconds</strong>: it is a highly scalable practice, which in our case is perfect.</p><h2 id="7a5d" class="op oq fq bf or os ot ou ov ow ox oy oz nb pa pb pc nf pd pe pf nj pg ph pi fw bk">4. Validate</h2><p id="c165" class="pw-post-body-paragraph mr ms fq mu b gt pj mw mx gw pk mz na nb pl nd ne nf pm nh ni nj pn nl nm nn fj bk">For an algorithm to be effective, needs to be validated. For now, we lack a proper mathematical validation (at first sight, averaging similarity scores from M already shows very promising results, but further research is needed for an objective metric backed up by proof).</p><p id="643a" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">However, existing results are quite intuitive when visualized using a comparative example. The following is the <strong class="mu ga">top search result (what you are seeing are the tags assigned to this game)</strong> of both search methods.</p></div></div><div class="np"><div class="ab cb"><div class="lr nq ls nr lt ns cf nt cg nu ci bh"><figure class="ny nz oa ob oc np od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw nx"><img src="../Images/c69b5bc43d2afec62188a510b720321f.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*a3db5HhCsv9R-rPrIgwOBw.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">comparison between<strong class="bf or"> traditional tag search</strong> and <strong class="bf or">semantic tag search, </strong>Image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><ul class=""><li id="b5ef" class="mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn qh qi qj bk"><strong class="mu ga">Traditional tag search<br/></strong>We can see how traditional search might (without additional rules, samples are filtered based on the availability of all tags, and not sorted) return a sample with a higher number of tags, but many of them may not be relevant.</li><li id="aa5e" class="mr ms fq mu b gt qm mw mx gw qn mz na nb qo nd ne nf qp nh ni nj qq nl nm nn qh qi qj bk"><strong class="mu ga">Semantic tag search</strong><br/>Semantic tag search sorts all samples based on the relevance of all tags, in simple terms, it <strong class="mu ga">disqualifies samples containing irrelevant tags</strong>.</li></ul><p id="4e17" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The real advantage of this new system is that when traditional search does not return enough samples, <strong class="mu ga">we can select as many as we want</strong> using semantic tag search.</p></div></div><div class="np"><div class="ab cb"><div class="lr nq ls nr lt ns cf nt cg nu ci bh"><figure class="ny nz oa ob oc np od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw nx"><img src="../Images/8deb50b654d346627375b7d90a6963a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*IeTz_HjMjMX_bmI76khBYA.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">difference of the two searches in front of a scarcity of results, Image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="3a7a" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">In the example above, using traditional tag filtering <strong class="mu ga">does not return any game </strong>from the Steam library. However, by using semantic tag filtering we still get results that are not perfect, but <strong class="mu ga">the best ones matching our query</strong>. The ones you are seeing are the tags of the top 5 games matching our search.</p><h1 id="236a" class="pp oq fq bf or pq pr gv ov ps pt gy oz pu pv pw px py pz qa qb qc qd qe qf qg bk">Conclusion</h1><p id="0a66" class="pw-post-body-paragraph mr ms fq mu b gt pj mw mx gw pk mz na nb pl nd ne nf pm nh ni nj pn nl nm nn fj bk">Before now, it was not possible to filter tags also taking into account their semantic relationships <strong class="mu ga">without resorting to complex methods</strong>, such as clustering, deep learning, or multiple knn searches.</p><p id="3b85" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The degree of flexibility offered by this algorithm <strong class="mu ga">should allow the detachment from traditional manual labeling methods</strong>, which force the user to choose between a pre-defined set of tags, and open the possibility of using LLMs of VLMs <strong class="mu ga">to freely assign tags to a text or an image without being confined to a pre-existing structure</strong>, opening up new options for scalable and improved search methods.</p><p id="da09" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">It is with my best wishes that I open this algorithm to the world, and I hope it will be utilized to its full potential.</p></div></div></div></div>    
</body>
</html>