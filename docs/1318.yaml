- en: An Introduction to Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ç®€ä»‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/an-introduction-to-reinforcement-learning-995737d3f1d?source=collection_archive---------2-----------------------#2024-05-27](https://towardsdatascience.com/an-introduction-to-reinforcement-learning-995737d3f1d?source=collection_archive---------2-----------------------#2024-05-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/an-introduction-to-reinforcement-learning-995737d3f1d?source=collection_archive---------2-----------------------#2024-05-27](https://towardsdatascience.com/an-introduction-to-reinforcement-learning-995737d3f1d?source=collection_archive---------2-----------------------#2024-05-27)
- en: '*A deep dive into the rudiments of reinforcement learning, including model-based
    and model-free methods*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*æ·±å…¥æ¢è®¨å¼ºåŒ–å­¦ä¹ çš„åŸºç¡€çŸ¥è¯†ï¼ŒåŒ…æ‹¬åŸºäºæ¨¡å‹çš„æ–¹æ³•å’Œæ— æ¨¡å‹çš„æ–¹æ³•*'
- en: '[](https://angjelinhila.medium.com/?source=post_page---byline--995737d3f1d--------------------------------)[![Angjelin
    Hila](../Images/44d826eda81d6fb95ec9c65a1da5933d.png)](https://angjelinhila.medium.com/?source=post_page---byline--995737d3f1d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--995737d3f1d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--995737d3f1d--------------------------------)
    [Angjelin Hila](https://angjelinhila.medium.com/?source=post_page---byline--995737d3f1d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://angjelinhila.medium.com/?source=post_page---byline--995737d3f1d--------------------------------)[![Angjelin
    Hila](../Images/44d826eda81d6fb95ec9c65a1da5933d.png)](https://angjelinhila.medium.com/?source=post_page---byline--995737d3f1d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--995737d3f1d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--995737d3f1d--------------------------------)
    [Angjelin Hila](https://angjelinhila.medium.com/?source=post_page---byline--995737d3f1d--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--995737d3f1d--------------------------------)
    Â·34 min readÂ·May 27, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--995737d3f1d--------------------------------)
    Â·é˜…è¯»æ—¶é—´ï¼š34åˆ†é’ŸÂ·2024å¹´5æœˆ27æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6efc546a2ac8232ca42d0a0aa55498c1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6efc546a2ac8232ca42d0a0aa55498c1.png)'
- en: 'Used on a creative commons license from: [https://elifesciences.org/digests/57443/reconstructing-the-brain-of-fruit-flies#copyright](https://elifesciences.org/digests/57443/reconstructing-the-brain-of-fruit-flies#copyright)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åˆ›ä½œå…±äº«è®¸å¯ï¼Œæ¥æºï¼š[https://elifesciences.org/digests/57443/reconstructing-the-brain-of-fruit-flies#copyright](https://elifesciences.org/digests/57443/reconstructing-the-brain-of-fruit-flies#copyright)
- en: What is Reinforcement Learning?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ
- en: One path toward engineering intelligence lies with emulating biological organisms.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ¨¡ä»¿ç”Ÿç‰©ä½“çš„æ–¹å¼æ¥æ„å»ºæ™ºèƒ½ç³»ç»Ÿæ˜¯ä¸€æ¡å¯è¡Œçš„é“è·¯ã€‚
- en: Biological organisms transduce information from the environment, process it
    (what cognitive science studies), and output behaviour conducive to survival.
    Such behaviours, at the most basic level, involve foraging for food, reproducing,
    and avoiding harm. They also involves the wide spectrum of human activity such
    as play, creativity, problem-solving, design and engineering, socializing, romance,
    and intellectual life.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿç‰©ä½“ä»ç¯å¢ƒä¸­è½¬åŒ–ä¿¡æ¯ï¼Œå¤„ç†è¿™äº›ä¿¡æ¯ï¼ˆè¿™æ­£æ˜¯è®¤çŸ¥ç§‘å­¦ç ”ç©¶çš„å†…å®¹ï¼‰ï¼Œå¹¶è¾“å‡ºæœ‰åˆ©äºç”Ÿå­˜çš„è¡Œä¸ºã€‚è¿™äº›è¡Œä¸ºåœ¨æœ€åŸºæœ¬çš„å±‚é¢ä¸ŠåŒ…æ‹¬è§…é£Ÿã€ç¹æ®–å’Œé¿å…ä¼¤å®³ã€‚å®ƒä»¬è¿˜åŒ…æ‹¬å¹¿æ³›çš„äººç±»æ´»åŠ¨ï¼Œå¦‚æ¸¸æˆã€åˆ›é€ åŠ›ã€é—®é¢˜è§£å†³ã€è®¾è®¡ä¸å·¥ç¨‹ã€ç¤¾äº¤ã€æµªæ¼«ä»¥åŠæ™ºåŠ›ç”Ÿæ´»ã€‚
- en: Now, how do we engineer a system that is able to do all of the above?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•è®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿå®Œæˆä»¥ä¸Šæ‰€æœ‰ä»»åŠ¡çš„ç³»ç»Ÿå‘¢ï¼Ÿ
- en: If we were to model a simple organism as a function of some environment, we
    would need a model of the agent, the environment, and some function that moves
    that agent from some present state to some desired state.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬è¦å°†ä¸€ä¸ªç®€å•çš„ç”Ÿç‰©ä½“å»ºæ¨¡ä¸ºæŸç§ç¯å¢ƒçš„å‡½æ•°ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»£ç†æ¨¡å‹ã€ä¸€ä¸ªç¯å¢ƒæ¨¡å‹ï¼Œä»¥åŠæŸä¸ªå‡½æ•°ï¼Œç”¨ä»¥å°†ä»£ç†ä»å½“å‰çŠ¶æ€ç§»åŠ¨åˆ°æœŸæœ›çŠ¶æ€ã€‚
- en: 'In psychology, two major schools of thought purport to explain human behaviour:
    *behaviourism* and *cognitive science*. Behaviourists understand behaviour as
    a function of learning mechanisms, where learning can be attributed to output
    behaviour. Cognitive science, on the other hand, models agent interaction with
    the environment through the information-processing approach. In this approach,
    the agent transduces external stimuli into an internal representation initially
    by the senses and subsequently subjects it to layers of transformation and integration
    all the way up to thinking and reasoning faculties, before returning some behaviourial
    output. In the former approach, learning is understood largely as a function of
    environmental conditioning, whereas in the latter, mental representations are
    considered indispensable in predicting behaviour. Reinforcement learning borrows
    mostly from the behaviorist approach where environmental reward dictates the evolution
    of the agent within search space.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¿ƒç†å­¦ä¸­ï¼Œæœ‰ä¸¤ä¸ªä¸»è¦çš„å­¦æ´¾è¯•å›¾è§£é‡Šäººç±»è¡Œä¸ºï¼š*è¡Œä¸ºä¸»ä¹‰*å’Œ*è®¤çŸ¥ç§‘å­¦*ã€‚è¡Œä¸ºä¸»ä¹‰è€…å°†è¡Œä¸ºç†è§£ä¸ºå­¦ä¹ æœºåˆ¶çš„åŠŸèƒ½ï¼Œå­¦ä¹ å¯ä»¥å½’å› äºè¾“å‡ºè¡Œä¸ºã€‚è€Œè®¤çŸ¥ç§‘å­¦åˆ™é€šè¿‡ä¿¡æ¯å¤„ç†æ–¹æ³•å»ºæ¨¡ä»£ç†ä¸ç¯å¢ƒçš„äº’åŠ¨ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œä»£ç†é€šè¿‡æ„Ÿå®˜é¦–å…ˆå°†å¤–éƒ¨åˆºæ¿€è½¬åŒ–ä¸ºå†…éƒ¨è¡¨å¾ï¼Œéšåå°†å…¶è¿›è¡Œå±‚å±‚è½¬åŒ–å’Œæ•´åˆï¼Œä¸€ç›´åˆ°æ€ç»´å’Œæ¨ç†èƒ½åŠ›ï¼Œå†è¿”å›æŸç§è¡Œä¸ºè¾“å‡ºã€‚åœ¨å‰ä¸€ç§æ–¹æ³•ä¸­ï¼Œå­¦ä¹ ä¸»è¦è¢«ç†è§£ä¸ºç¯å¢ƒæ¡ä»¶ä½œç”¨çš„åŠŸèƒ½ï¼Œè€Œåœ¨åä¸€ç§æ–¹æ³•ä¸­ï¼Œå¿ƒç†è¡¨å¾è¢«è®¤ä¸ºåœ¨é¢„æµ‹è¡Œä¸ºæ—¶ä¸å¯æˆ–ç¼ºã€‚å¼ºåŒ–å­¦ä¹ å¤§å¤šå€Ÿé‰´äº†è¡Œä¸ºä¸»ä¹‰æ–¹æ³•ï¼Œå…¶ä¸­ç¯å¢ƒå¥–åŠ±å†³å®šäº†ä»£ç†åœ¨æœç´¢ç©ºé—´ä¸­çš„æ¼”åŒ–ã€‚
- en: '[Operant conditioning](https://en.wikipedia.org/wiki/Operant_conditioning),
    the school of behaviorist psychology that reigned in the 1950s-60s, defined learning
    as the product of the environmental mechanisms of reward and punishment. Precursors
    to operant conditioning included the [Law of Effect](https://en.wikipedia.org/wiki/Law_of_effect)
    proposed by Edward Thorndike which proposed that behaviors that produce satisfying
    effects are more likely to recur, whereas behaviors that produce dissatisfying
    effects less likely. B.F. Skinner operationalized effects in terms of reinforcement
    and punishment. Reinforcement increases the likelihood of the recurrence of a
    behavior, whether it be approach or removal of the inhibitory factor. Approach
    is termed positive reinforcement, and the reversal of avoidance, negative reinforcement.
    An example of positive reinforcement includes becoming good at a sport and winning
    often. An example of negative reinforcement includes removing the inhibitory stimulus,
    e.g. the school bully who taunts you during games. Operant conditioning predicts
    that youâ€™re likely to repeat behaviours that receive the greatest reward. Punishment,
    on the other hand, consists of controlling the behavioural effect by either adding
    a negative consequence (positive punishment) or removing the reward associated
    with the behaviour (negative punishment). When fouling causes expulsion from the
    game, it illustrates positive punishment. When you perform poorly and lose games
    it illustrates negative punishment, which may cause avoidance of playing in the
    future.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ“ä½œæ€§æ¡ä»¶ä½œç”¨](https://en.wikipedia.org/wiki/Operant_conditioning)ï¼Œæ˜¯20ä¸–çºª50è‡³60å¹´ä»£ç››è¡Œçš„è¡Œä¸ºä¸»ä¹‰å¿ƒç†å­¦æµæ´¾ï¼Œå®šä¹‰å­¦ä¹ ä¸ºç¯å¢ƒä¸­å¥–åŠ±å’Œæƒ©ç½šæœºåˆ¶çš„äº§ç‰©ã€‚æ“ä½œæ€§æ¡ä»¶ä½œç”¨çš„å‰èº«åŒ…æ‹¬çˆ±å¾·åÂ·æ¡‘ä»£å…‹æå‡ºçš„[æ•ˆæœæ³•åˆ™](https://en.wikipedia.org/wiki/Law_of_effect)ï¼Œè¯¥æ³•åˆ™è®¤ä¸ºäº§ç”Ÿæ»¡è¶³æ•ˆæœçš„è¡Œä¸ºæ›´å¯èƒ½é‡å¤ï¼Œè€Œäº§ç”Ÿä¸æ»¡è¶³æ•ˆæœçš„è¡Œä¸ºåˆ™ä¸å¤ªå¯èƒ½é‡å¤ã€‚B.F.
    æ–¯é‡‘çº³é€šè¿‡å¼ºåŒ–å’Œæƒ©ç½šæ¥æ“ä½œåŒ–æ•ˆæœã€‚å¼ºåŒ–é€šè¿‡å¢åŠ è¡Œä¸ºé‡å¤çš„å¯èƒ½æ€§æ¥èµ·ä½œç”¨ï¼Œæ— è®ºæ˜¯æ¥è¿‘è¿˜æ˜¯å»é™¤æŠ‘åˆ¶å› ç´ ã€‚æ¥è¿‘ç§°ä¸ºæ­£å‘å¼ºåŒ–ï¼Œè€Œé¿å…çš„åè½¬ç§°ä¸ºè´Ÿå‘å¼ºåŒ–ã€‚æ­£å‘å¼ºåŒ–çš„ä¾‹å­åŒ…æ‹¬åœ¨ä¸€é¡¹è¿åŠ¨ä¸­å˜å¾—æ“…é•¿å¹¶é¢‘ç¹è·èƒœã€‚è´Ÿå‘å¼ºåŒ–çš„ä¾‹å­åŒ…æ‹¬å»é™¤æŠ‘åˆ¶æ€§åˆºæ¿€ï¼Œä¾‹å¦‚åœ¨æ¯”èµ›ä¸­æŒ‘è¡…ä½ çš„å­¦æ ¡æ¶éœ¸ã€‚æ“ä½œæ€§æ¡ä»¶ä½œç”¨é¢„æµ‹ä½ æ›´å¯èƒ½é‡å¤è·å¾—æœ€å¤§å¥–åŠ±çš„è¡Œä¸ºã€‚å¦ä¸€æ–¹é¢ï¼Œæƒ©ç½šåŒ…æ‹¬é€šè¿‡å¢åŠ è´Ÿé¢åæœï¼ˆæ­£å‘æƒ©ç½šï¼‰æˆ–å»é™¤ä¸è¡Œä¸ºç›¸å…³çš„å¥–åŠ±ï¼ˆè´Ÿå‘æƒ©ç½šï¼‰æ¥æ§åˆ¶è¡Œä¸ºæ•ˆæœã€‚å½“çŠ¯è§„å¯¼è‡´è¢«é€å‡ºæ¯”èµ›æ—¶ï¼Œå®ƒè¯´æ˜äº†æ­£å‘æƒ©ç½šã€‚å½“ä½ è¡¨ç°ä¸ä½³å¹¶å¤±å»æ¯”èµ›æ—¶ï¼Œå®ƒè¯´æ˜äº†è´Ÿå‘æƒ©ç½šï¼Œè¿™å¯èƒ½å¯¼è‡´æœªæ¥é¿å…å‚ä¸æ¯”èµ›ã€‚'
- en: Much of the game of life in human society is replete with *secondary reinforcers*
    or socially constructed rewards and punishments that condition behaviour. These
    include money, grades, university admittance criteria, rules for winning and losing
    games, which build upon natural reinforcers that are closer to biological needs
    like food, reproduction, and social approbation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: äººç±»ç¤¾ä¼šä¸­çš„â€œç”Ÿæ´»æ¸¸æˆâ€å……æ»¡äº†*äºŒçº§å¼ºåŒ–ç‰©*æˆ–ç¤¾ä¼šæ„å»ºçš„å¥–åŠ±ä¸æƒ©ç½šï¼Œè¿™äº›å¥–åŠ±ä¸æƒ©ç½šå¡‘é€ è¡Œä¸ºã€‚è¿™äº›åŒ…æ‹¬é‡‘é’±ã€æˆç»©ã€å¤§å­¦å½•å–æ ‡å‡†ã€èµ¢è¾“æ¸¸æˆçš„è§„åˆ™ï¼Œå®ƒä»¬å»ºç«‹åœ¨æ¥è¿‘ç”Ÿç‰©éœ€æ±‚çš„è‡ªç„¶å¼ºåŒ–ç‰©ä¹‹ä¸Šï¼Œå¦‚é£Ÿç‰©ã€ç¹æ®–å’Œç¤¾ä¼šè®¤åŒã€‚
- en: Memory plays an important role in learning because it enables the retention
    of prior experiences. Evidence shows that memory encodes the rewards and punishments
    more so than the content of the experience (Tyng et al., 2017). Subjects are likely
    to remember rewarding experiences fondly and thereby likely to repeat them, and
    negative experiences unfavourably, and likely to avoid them in the future. The
    mechanisms of memory are complicated and diverse, and evidence suggests that subjects
    play an active role in reshaping their memories by recalling them (Spens & Burgess,
    2024). This fact complicates the picture for behaviorism because the subjectâ€™s
    interpretation of an experience can be retrospectively modified and reframed,
    making prediction on conditioning principles alone difficult. Furthermore, rewards
    and punishments oversimplify the landscape of positive and negative affects, which
    comprises a complex terrain of valleys and troughs, nested dependencies, and is
    better modeled as a continuous spectrum rather than a binary space.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è®°å¿†åœ¨å­¦ä¹ ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿä¿æŒå…ˆå‰çš„ç»éªŒã€‚è¯æ®è¡¨æ˜ï¼Œè®°å¿†æ›´å¤šåœ°ç¼–ç å¥–åŠ±å’Œæƒ©ç½šï¼Œè€Œä¸æ˜¯ç»éªŒçš„å†…å®¹ï¼ˆTyng et al., 2017ï¼‰ã€‚å—è¯•è€…æ›´æœ‰å¯èƒ½å¯¹å¥–åŠ±æ€§çš„ç»éªŒäº§ç”Ÿç§¯æå›å¿†ï¼Œå› æ­¤æ›´å¯èƒ½é‡å¤è¿™äº›ç»éªŒï¼Œè€Œå¯¹è´Ÿé¢çš„ç»éªŒäº§ç”Ÿä¸åˆ©å›å¿†ï¼Œå¹¶å¯èƒ½åœ¨æœªæ¥é¿å…å®ƒä»¬ã€‚è®°å¿†çš„æœºåˆ¶å¤æ‚å¤šæ ·ï¼Œè¯æ®è¡¨æ˜ï¼Œå—è¯•è€…é€šè¿‡å›å¿†è‡ªå·±çš„è®°å¿†åœ¨é‡æ–°å¡‘é€ è¿™äº›è®°å¿†æ–¹é¢æ‰®æ¼”äº†ç§¯æè§’è‰²ï¼ˆSpens
    & Burgess, 2024ï¼‰ã€‚è¿™ä¸€äº‹å®ä½¿å¾—è¡Œä¸ºä¸»ä¹‰çš„å›¾æ™¯å˜å¾—æ›´åŠ å¤æ‚ï¼Œå› ä¸ºå—è¯•è€…å¯¹ç»éªŒçš„è§£è¯»å¯ä»¥è¢«å›æº¯æ€§åœ°ä¿®æ”¹å’Œé‡æ–°æ„å»ºï¼Œä»è€Œä½¿å¾—ä»…å‡­æ¡ä»¶åå°„åŸç†è¿›è¡Œé¢„æµ‹å˜å¾—å›°éš¾ã€‚æ­¤å¤–ï¼Œå¥–åŠ±å’Œæƒ©ç½šè¿‡äºç®€åŒ–äº†ç§¯æå’Œæ¶ˆææƒ…æ„Ÿçš„æ™¯è§‚ï¼Œè€Œè¿™ç§æƒ…æ„Ÿæ™¯è§‚å®é™…ä¸Šæ˜¯ä¸€ä¸ªå¤æ‚çš„é¢†åŸŸï¼Œå……æ»¡äº†ä½è°·å’Œæ³¢åŠ¨ã€åµŒå¥—ä¾èµ–ï¼Œæ›´é€‚åˆè¢«å»ºæ¨¡ä¸ºä¸€ä¸ªè¿ç»­çš„å…‰è°±ï¼Œè€Œéä¸€ä¸ªäºŒå…ƒç©ºé—´ã€‚
- en: These complexities notwithstanding, reinforcement learning comprises an array
    of mathematical techniques that adapt the behavioural ontology of agent, environment,
    and rewards in order to model artificial intelligence. As we will see below, aspects
    of reinforcement learning emerge from control theory, whose precursors extend
    into physics and engineering, and other aspects emerge more directly from psychology
    and biology. Since both the objects of control theory and living systems comprise
    dynamical systems that must stay within an optimal range of far-from thermodynamic
    equilibrium, the underlying principles are amenable to the goals of reinforcement
    learning and artificial intelligence more broadly.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å­˜åœ¨è¿™äº›å¤æ‚æ€§ï¼Œå¼ºåŒ–å­¦ä¹ åŒ…æ‹¬ä¸€ç³»åˆ—æ•°å­¦æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯é€šè¿‡è°ƒæ•´ä»£ç†ã€ç¯å¢ƒå’Œå¥–åŠ±çš„è¡Œä¸ºæœ¬ä½“æ¥å»ºæ¨¡äººå·¥æ™ºèƒ½ã€‚æ­£å¦‚æˆ‘ä»¬ä¸‹é¢å°†çœ‹åˆ°çš„ï¼Œå¼ºåŒ–å­¦ä¹ çš„æŸäº›æ–¹é¢æºäºæ§åˆ¶ç†è®ºï¼Œè€Œæ§åˆ¶ç†è®ºçš„å‰èº«æ‰©å±•åˆ°ç‰©ç†å­¦å’Œå·¥ç¨‹å­¦ï¼Œå¦ä¸€äº›æ–¹é¢åˆ™ç›´æ¥æºè‡ªå¿ƒç†å­¦å’Œç”Ÿç‰©å­¦ã€‚ç”±äºæ§åˆ¶ç†è®ºçš„å¯¹è±¡å’Œç”Ÿç‰©ç³»ç»Ÿéƒ½æ˜¯åŠ¨åŠ›å­¦ç³»ç»Ÿï¼Œå¿…é¡»ä¿æŒåœ¨è¿œç¦»çƒ­åŠ›å­¦å¹³è¡¡çš„æœ€ä½³èŒƒå›´å†…ï¼Œå› æ­¤å…¶åŸºæœ¬åŸç†é€‚ç”¨äºå¼ºåŒ–å­¦ä¹ å’Œäººå·¥æ™ºèƒ½çš„æ›´å¹¿æ³›ç›®æ ‡ã€‚
- en: Dynamic programming emerged chiefly from control theory as a mathematical optimization
    method that enables larger problems to be broken down recursively into sub-problems
    as a means of solving the larger problem. Generally speaking, recursion refers
    to a function that passes itself directly or indirectly as a parameter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨æ€è§„åˆ’ä¸»è¦æºè‡ªæ§åˆ¶ç†è®ºï¼Œæ˜¯ä¸€ç§æ•°å­¦ä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒä½¿å¾—è¾ƒå¤§çš„é—®é¢˜å¯ä»¥é€’å½’åœ°åˆ†è§£ä¸ºå­é—®é¢˜ï¼Œä»è€Œè§£å†³æ›´å¤§çš„é—®é¢˜ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œé€’å½’æŒ‡çš„æ˜¯ä¸€ä¸ªå‡½æ•°ç›´æ¥æˆ–é—´æ¥åœ°å°†è‡ªèº«ä½œä¸ºå‚æ•°ä¼ é€’ã€‚
- en: In this article, we will focus chiefly on the elements of dynamic programming,
    with a focus on discrete and finite games. However, dynamic programming exhibits
    a variety of limitations that are in part addressed by model-free approaches to
    reinforcement learning and others by combining dynamic programming with artificial
    neural networks, once called *neurodynamic programming*. More broadly, the marriage
    of reinforcement learning and artificial neural networks is termed deep reinforcement
    learning. These models incorporate the strengths of deep learning within reinforcement
    learning techniques. The most popular of these algorithms include the **Deep Q-Networks
    (DQN)**, which were introduced by [DeepMind](https://deepmind.google/) in 2013\.
    This family of algorithms leverages deep learning to approximate the Q-function.
    Since function-approximation is one of the shortcomings of reinforcement learning,
    these algorithms represent a major improvement of the reinforcement paradigm.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä¸»è¦é›†ä¸­è®¨è®ºåŠ¨æ€è§„åˆ’çš„å…ƒç´ ï¼Œé‡ç‚¹è®¨è®ºç¦»æ•£å’Œæœ‰é™åšå¼ˆã€‚ç„¶è€Œï¼ŒåŠ¨æ€è§„åˆ’è¡¨ç°å‡ºè®¸å¤šé™åˆ¶ï¼Œè¿™äº›é™åˆ¶éƒ¨åˆ†é€šè¿‡å¼ºåŒ–å­¦ä¹ çš„æ— æ¨¡å‹æ–¹æ³•å¾—åˆ°è§£å†³ï¼Œå¦ä¸€äº›åˆ™é€šè¿‡å°†åŠ¨æ€è§„åˆ’ä¸äººå·¥ç¥ç»ç½‘ç»œç»“åˆæ¥è§£å†³ï¼Œè¿™ç§æ–¹æ³•æ›¾è¢«ç§°ä¸º*ç¥ç»åŠ¨æ€è§„åˆ’*ã€‚æ›´å¹¿æ³›åœ°è¯´ï¼Œå¼ºåŒ–å­¦ä¹ å’Œäººå·¥ç¥ç»ç½‘ç»œçš„ç»“åˆè¢«ç§°ä¸ºæ·±åº¦å¼ºåŒ–å­¦ä¹ ã€‚è¿™äº›æ¨¡å‹ç»“åˆäº†æ·±åº¦å­¦ä¹ åœ¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ä¸­çš„ä¼˜åŠ¿ã€‚è¿™äº›ç®—æ³•ä¸­æœ€æµè¡Œçš„æ˜¯**æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰**ï¼Œè¯¥ç®—æ³•ç”±[DeepMind](https://deepmind.google/)åœ¨2013å¹´æå‡ºã€‚è¿™ä¸ªç®—æ³•å®¶æ—åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¥é€¼è¿‘Qå‡½æ•°ã€‚ç”±äºå‡½æ•°é€¼è¿‘æ˜¯å¼ºåŒ–å­¦ä¹ çš„ä¸€ä¸ªä¸è¶³ä¹‹å¤„ï¼Œè¿™äº›ç®—æ³•ä»£è¡¨äº†å¼ºåŒ–å­¦ä¹ èŒƒå¼çš„ä¸€å¤§è¿›æ­¥ã€‚
- en: Other shortcomings addressed by **DQN** include conferring flexibility in capturing
    nonlinear dynamics, admitting a much wider range of dimensions without becoming
    computationally intractable from the *curse of dimensionality*, and greater generalization
    capacity over the environment.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**DQN**æ‰€è§£å†³çš„å…¶ä»–ä¸è¶³ä¹‹å¤„åŒ…æ‹¬ï¼šåœ¨æ•æ‰éçº¿æ€§åŠ¨æ€æ–¹é¢æä¾›çµæ´»æ€§ï¼›èƒ½å¤Ÿæ¥å—æ›´å¹¿æ³›çš„ç»´åº¦èŒƒå›´ï¼Œè€Œä¸ä¼šå› *ç»´åº¦ç¾éš¾*è€Œå˜å¾—åœ¨è®¡ç®—ä¸Šæ— æ³•å¤„ç†ï¼›ä»¥åŠåœ¨ç¯å¢ƒä¸­çš„æ›´å¼ºæ³›åŒ–èƒ½åŠ›ã€‚'
- en: Neurodynamic programming represents a step in the direction of leveraging the
    cognitive paradigm in psychology to address the shortcomings of the purely behaviourist
    approach. It is worth noting, however, that while scientific progress has been
    made in understanding the hierarchical organization and processing of lower-level
    perceptual information, the scaffolding of that information to thought and consciousness
    remains, more or less, scientifically elusive. For this reason, artificial neural
    networks (ANNs) as yet lack the complex generalization capacity of human intelligence
    which tends to learn with exponentially smaller samples than ANNs. We will discuss
    the implications of adopting the principles of reinforcement learning toward artificial
    general intelligence (AGI) in the last section of the article.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»åŠ¨æ€è§„åˆ’ä»£è¡¨äº†åœ¨å¿ƒç†å­¦ä¸­åˆ©ç”¨è®¤çŸ¥èŒƒå¼çš„æ–¹å‘ï¼Œä»¥è§£å†³çº¯è¡Œä¸ºä¸»ä¹‰æ–¹æ³•çš„ä¸è¶³ä¹‹å¤„ã€‚ç„¶è€Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡åœ¨ç†è§£è¾ƒä½å±‚æ¬¡æ„ŸçŸ¥ä¿¡æ¯çš„å±‚æ¬¡ç»“æ„å’Œå¤„ç†æ–¹é¢å–å¾—äº†ç§‘å­¦è¿›å±•ï¼Œä½†å°†è¿™äº›ä¿¡æ¯ä¸æ€ç»´å’Œæ„è¯†çš„æ„æ¶è¿æ¥èµ·æ¥ï¼Œä»ç„¶åœ¨æŸç§ç¨‹åº¦ä¸Šç§‘å­¦ä¸Šéš¾ä»¥æ‰æ‘¸ã€‚å› æ­¤ï¼Œäººå·¥ç¥ç»ç½‘ç»œï¼ˆANNsï¼‰å°šç¼ºä¹äººç±»æ™ºèƒ½é‚£ç§å¤æ‚çš„æ³›åŒ–èƒ½åŠ›ï¼Œäººç±»æ™ºèƒ½å¾€å¾€èƒ½åœ¨æ¯”ANNså°‘å¾—å¤šçš„æ ·æœ¬ä¸‹è¿›è¡ŒæŒ‡æ•°çº§çš„å­¦ä¹ ã€‚æˆ‘ä»¬å°†åœ¨æ–‡ç« çš„æœ€åä¸€éƒ¨åˆ†è®¨è®ºé‡‡ç”¨å¼ºåŒ–å­¦ä¹ åŸåˆ™å¯¹äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„å½±å“ã€‚
- en: Decision Theory & Control Theory
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å†³ç­–ç†è®ºä¸æ§åˆ¶ç†è®º
- en: Before delving into the mathematical elements of dynamic programming and reinforcement
    learning, it is important to flesh out the relationship between the philosophical
    and mathematical branch of decision theory and reinforcement learning. While decision
    theory consists primarily of mathematical formalizations of rational choice theory,
    they overlap with the goals of reinforcement learning insofar as reinforcement
    learning seeks to scaffold its models into successful artificial agents that can
    interact with complex environments and information landscapes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥æ¢è®¨åŠ¨æ€è§„åˆ’å’Œå¼ºåŒ–å­¦ä¹ çš„æ•°å­¦å…ƒç´ ä¹‹å‰ï¼Œé‡è¦çš„æ˜¯è¦é˜æ˜å†³ç­–ç†è®ºçš„å“²å­¦å’Œæ•°å­¦åˆ†æ”¯ä¸å¼ºåŒ–å­¦ä¹ ä¹‹é—´çš„å…³ç³»ã€‚å†³ç­–ç†è®ºä¸»è¦æ˜¯ç†æ€§é€‰æ‹©ç†è®ºçš„æ•°å­¦å½¢å¼åŒ–ï¼Œå®ƒä»¬ä¸å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡å­˜åœ¨é‡å ï¼Œå› ä¸ºå¼ºåŒ–å­¦ä¹ æ—¨åœ¨å°†å…¶æ¨¡å‹æ„å»ºæˆèƒ½å¤Ÿä¸å¤æ‚ç¯å¢ƒå’Œä¿¡æ¯é¢†åŸŸäº’åŠ¨çš„æˆåŠŸäººå·¥æ™ºèƒ½ä»£ç†ã€‚
- en: '**Decision theory**, also known as **choice theory**, was developed in the
    20th century at the heel of the growing formalization of [instrumental reason](https://plato.stanford.edu/entries/rationality-instrumental/).
    Specifically, it uses probability theory to quantify the probability of agent
    actions given their preferences. A crowning achievement of this formalization
    effort was the [***Von-Neumann-Morgenstern utility***](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem)procedure.
    In a nutshell, the procedure states that agents tend to choose actions that maximize
    utility given the utility expectations of available choices.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**å†³ç­–ç†è®º**ï¼Œä¹Ÿç§°ä¸º**é€‰æ‹©ç†è®º**ï¼Œæ˜¯åœ¨20ä¸–çºªéšç€[å·¥å…·ç†æ€§](https://plato.stanford.edu/entries/rationality-instrumental/)å½¢å¼åŒ–çš„å‘å±•è€Œäº§ç”Ÿçš„ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒä½¿ç”¨æ¦‚ç‡è®ºæ¥é‡åŒ–åœ¨ç»™å®šåå¥½ä¸‹ï¼Œæ™ºèƒ½ä½“è¡Œä¸ºçš„å‘ç”Ÿæ¦‚ç‡ã€‚è¿™ä¸ªå½¢å¼åŒ–åŠªåŠ›çš„ä¸€ä¸ªé‡è¦æˆæœæ˜¯[***å†¯Â·è¯ºä¾æ›¼-æ‘©æ ¹æ–¯å¦æ•ˆç”¨ç†è®º***](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem)ã€‚ç®€è€Œè¨€ä¹‹ï¼Œè¯¥ç¨‹åºè¡¨æ˜ï¼Œæ™ºèƒ½ä½“å€¾å‘äºé€‰æ‹©é‚£äº›åœ¨å¯é€‰è¡Œä¸ºä¸­æœ€å¤§åŒ–æ•ˆç”¨çš„è¡ŒåŠ¨ã€‚'
- en: 'Control theory emerges from the fields of mechanical and electrical engineering
    and concerns optimizing the states and performance of dynamical systems relative
    to desired parameters, such as maintaining some steady-state temperature range.
    The essential mechanism consists of a controller that measures the desired variable
    and compares it to a set point, whose difference is fed as feedback for correction.
    The broad strokes of control theory mirror metabolic processes of living organisms,
    who maintain a set point of internal temperature against variable external conditions.
    The connection of control theory to decision theory is obvious: both rely on feedback
    from the environment to maintain or advance the state of the system toward some
    form of optimality.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ§åˆ¶ç†è®ºèµ·æºäºæœºæ¢°å’Œç”µæ°”å·¥ç¨‹é¢†åŸŸï¼Œæ—¨åœ¨ä¼˜åŒ–åŠ¨æ€ç³»ç»Ÿçš„çŠ¶æ€å’Œæ€§èƒ½ï¼Œä½¿å…¶ç›¸å¯¹äºæœŸæœ›çš„å‚æ•°è¾¾åˆ°æœ€ä½³çŠ¶æ€ï¼Œä¾‹å¦‚ä¿æŒä¸€å®šçš„ç¨³æ€æ¸©åº¦èŒƒå›´ã€‚å…¶åŸºæœ¬æœºåˆ¶åŒ…æ‹¬ä¸€ä¸ªæ§åˆ¶å™¨ï¼Œè¯¥æ§åˆ¶å™¨æµ‹é‡æœŸæœ›çš„å˜é‡ï¼Œå¹¶å°†å…¶ä¸è®¾å®šç‚¹è¿›è¡Œæ¯”è¾ƒï¼ŒäºŒè€…çš„å·®å€¼ä½œä¸ºåé¦ˆç”¨äºä¿®æ­£ã€‚æ§åˆ¶ç†è®ºçš„å¹¿ä¹‰æ¡†æ¶ä¸ç”Ÿç‰©ä½“çš„æ–°é™ˆä»£è°¢è¿‡ç¨‹ç›¸ä¼¼ï¼Œç”Ÿç‰©ä½“é€šè¿‡è°ƒèŠ‚å†…éƒ¨æ¸©åº¦æ¥åº”å¯¹å¤–éƒ¨ç¯å¢ƒçš„å˜åŒ–ã€‚æ§åˆ¶ç†è®ºä¸å†³ç­–ç†è®ºçš„è”ç³»æ˜¾è€Œæ˜“è§ï¼šä¸¤è€…éƒ½ä¾èµ–äºæ¥è‡ªç¯å¢ƒçš„åé¦ˆï¼Œä»¥ç»´æŒæˆ–æ¨åŠ¨ç³»ç»ŸçŠ¶æ€å‘æŸç§æœ€ä¼˜çŠ¶æ€å‘å±•ã€‚
- en: 'Mathematically, subsets of both control and decision problems can be reduced
    to optimization problems solvable through dynamic programming. Dynamical programming
    solves general stochastic optimal control problems (afflicted by the [***curse
    of dimensionality***](https://en.wikipedia.org/wiki/Curse_of_dimensionality) â€”
    meaning that computational requirements grow exponentially with the number of
    state variables) by decomposing them into smaller sub-problems and computing the
    value function. As we demonstrate the rudiments of reinforcement learning, we
    will delve into the heart of dynamic programming: the recursive relationship between
    the state and value functions of the agent.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°å­¦ä¸Šï¼Œæ§åˆ¶å’Œå†³ç­–é—®é¢˜çš„å­é›†å¯ä»¥å½’ç»“ä¸ºé€šè¿‡åŠ¨æ€è§„åˆ’æ±‚è§£çš„ä¼˜åŒ–é—®é¢˜ã€‚åŠ¨æ€è§„åˆ’é€šè¿‡å°†è¿™äº›é—®é¢˜åˆ†è§£ä¸ºæ›´å°çš„å­é—®é¢˜ï¼Œå¹¶è®¡ç®—ä»·å€¼å‡½æ•°ï¼Œä»è€Œè§£å†³ä¸€èˆ¬çš„éšæœºæœ€ä¼˜æ§åˆ¶é—®é¢˜ï¼ˆå—åˆ°[***ç»´åº¦ç¾éš¾***](https://en.wikipedia.org/wiki/Curse_of_dimensionality)çš„å›°æ‰°â€”â€”è¿™æ„å‘³ç€éšç€çŠ¶æ€å˜é‡æ•°é‡çš„å¢åŠ ï¼Œè®¡ç®—éœ€æ±‚å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼‰ã€‚åœ¨æˆ‘ä»¬æ¼”ç¤ºå¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬åŸç†æ—¶ï¼Œå°†æ·±å…¥æ¢è®¨åŠ¨æ€è§„åˆ’çš„æ ¸å¿ƒï¼šæ™ºèƒ½ä½“çš„çŠ¶æ€å‡½æ•°ä¸ä»·å€¼å‡½æ•°ä¹‹é—´çš„é€’å½’å…³ç³»ã€‚
- en: Reinforcement learning and decision theory overlap in defining a procedure for
    maximizing reward or utility. However, whereas utility is explicitly defined in
    decision theory, which aims to model economic behaviour, in reinforcement learning
    utility is substituted by cumulative reward. Different policies relative to different
    task goals can be applied toward maximizing cumulative reward, which, as we will
    see, depends on the inverse relationship between the polar directions of exploration
    and exploitation termed the *exploration-exploitation dilemma*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ä¸å†³ç­–ç†è®ºåœ¨å®šä¹‰æœ€å¤§åŒ–å¥–åŠ±æˆ–æ•ˆç”¨çš„ç¨‹åºæ—¶æœ‰é‡å ã€‚ç„¶è€Œï¼Œå°½ç®¡æ•ˆç”¨åœ¨å†³ç­–ç†è®ºä¸­è¢«æ˜ç¡®å®šä¹‰ï¼Œè¯¥ç†è®ºæ—¨åœ¨æ¨¡æ‹Ÿç»æµè¡Œä¸ºï¼Œè€Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ•ˆç”¨è¢«ç´¯ç§¯å¥–åŠ±æ‰€å–ä»£ã€‚ç›¸å¯¹äºä¸åŒä»»åŠ¡ç›®æ ‡ï¼Œå¯ä»¥åº”ç”¨ä¸åŒçš„ç­–ç•¥æ¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼Œè€Œç´¯ç§¯å¥–åŠ±çš„æœ€å¤§åŒ–å–å†³äºæ¢ç´¢ä¸å¼€å‘ä¹‹é—´çš„åå‘å…³ç³»ï¼Œè¿™ä¸€å…³ç³»è¢«ç§°ä¸º*æ¢ç´¢-å¼€å‘å›°å¢ƒ*ã€‚
- en: Letâ€™s begin by outlining the ontology underlying reinforcement models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆæ¦‚è¿°å¼ºåŒ–å­¦ä¹ æ¨¡å‹èƒŒåçš„æœ¬ä½“è®ºã€‚
- en: States, Actions & Rewards
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çŠ¶æ€ã€è¡ŒåŠ¨ä¸å¥–åŠ±
- en: Reinforcement learning leverages the theoretical apparatus of decision theory
    to construct models comprising agents, environments, and a dynamic evolution rule.
    The evolution rule permits an agent to pursue rewards within its environment,
    also termed *observation*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ åˆ©ç”¨å†³ç­–ç†è®ºçš„ç†è®ºæ¡†æ¶ï¼Œæ„å»ºç”±ä»£ç†ã€ç¯å¢ƒå’ŒåŠ¨æ€æ¼”åŒ–è§„åˆ™ç»„æˆçš„æ¨¡å‹ã€‚æ¼”åŒ–è§„åˆ™å…è®¸ä»£ç†åœ¨å…¶ç¯å¢ƒå†…è¿½æ±‚å¥–åŠ±ï¼Œä¹Ÿç§°ä¸º*è§‚å¯Ÿ*ã€‚
- en: The agent is defined as an output from the environment to a decision. We call
    a particular decision an action. The mapping from the present state of the network
    to an action is called the policy. The policy guides actions as mappings from
    states to outcomes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç†è¢«å®šä¹‰ä¸ºä»ç¯å¢ƒåˆ°å†³ç­–çš„è¾“å‡ºã€‚æˆ‘ä»¬ç§°æŸä¸ªç‰¹å®šçš„å†³ç­–ä¸ºåŠ¨ä½œã€‚å½“å‰ç½‘ç»œçŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ç§°ä¸ºç­–ç•¥ã€‚ç­–ç•¥æŒ‡å¯¼åŠ¨ä½œï¼Œä»çŠ¶æ€åˆ°ç»“æœçš„æ˜ å°„ã€‚
- en: 'Formally, therefore, a policy is a function that maps *a state* to an *action*.
    It can be represented by the conditional probability of an action given the current
    state, where the Greek symbol ğ›‘stands for policy:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä»å½¢å¼ä¸Šè®²ï¼Œç­–ç•¥æ˜¯ä¸€ä¸ªå°†*çŠ¶æ€*æ˜ å°„åˆ°*åŠ¨ä½œ*çš„å‡½æ•°ã€‚å®ƒå¯ä»¥é€šè¿‡ç»™å®šå½“å‰çŠ¶æ€çš„æ¡ä»¶æ¦‚ç‡æ¥è¡¨ç¤ºï¼Œå¸Œè…Šå­—æ¯ğ›‘ä»£è¡¨ç­–ç•¥ï¼š
- en: '![](../Images/a4965d65edbd1fab0ea23d52a505410b.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4965d65edbd1fab0ea23d52a505410b.png)'
- en: 'Transition dynamics define the next state given the input reward as a probability
    distribution over all possible states and reward values:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è½¬ç§»åŠ¨æ€å®šä¹‰äº†ç»™å®šè¾“å…¥å¥–åŠ±æ—¶ä¸‹ä¸€ä¸ªçŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ¶µç›–æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€å’Œå¥–åŠ±å€¼ï¼š
- en: '![](../Images/f1a6a4ad7ea89b2d60b2754eca35ed07.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1a6a4ad7ea89b2d60b2754eca35ed07.png)'
- en: The formula above defines the probability of the next state and reward pair
    as equal to the conditional probability of the next state ***sâ€™*** and reward
    ***r*** given the current state ***s*** and action ***a***.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å…¬å¼å®šä¹‰äº†ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±å¯¹çš„æ¦‚ç‡ï¼Œç­‰äºç»™å®šå½“å‰çŠ¶æ€***s***å’ŒåŠ¨ä½œ***a***æ—¶ï¼Œä¸‹ä¸€ä¸ªçŠ¶æ€***s'***å’Œå¥–åŠ±***r***çš„æ¡ä»¶æ¦‚ç‡ã€‚
- en: An action changes the environment by accruing a reward. The reward, in turn,
    changes the **agent state** or **observation**. The reward input determines the
    future action outputs based on the *policy*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŠ¨ä½œé€šè¿‡è·å¾—å¥–åŠ±æ¥æ”¹å˜ç¯å¢ƒã€‚å¥–åŠ±åè¿‡æ¥ä¼šæ”¹å˜**ä»£ç†çŠ¶æ€**æˆ–**è§‚å¯Ÿ**ã€‚å¥–åŠ±è¾“å…¥åŸºäº*ç­–ç•¥*å†³å®šæœªæ¥çš„åŠ¨ä½œè¾“å‡ºã€‚
- en: 'Generally, there are two types of policies:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œç­–ç•¥æœ‰ä¸¤ç§ç±»å‹ï¼š
- en: '***Deterministic:*** given present state/environment, thereâ€™s one and only
    one action the agent can take.'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***ç¡®å®šæ€§ï¼š*** ç»™å®šå½“å‰çŠ¶æ€/ç¯å¢ƒï¼Œä»£ç†åªèƒ½é‡‡å–ä¸€ä¸ªå”¯ä¸€çš„åŠ¨ä½œã€‚'
- en: ''
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Stochastic:*** given present state/environment, there are multiple actions
    an agent can take.'
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***éšæœºæ€§ï¼š*** ç»™å®šå½“å‰çŠ¶æ€/ç¯å¢ƒï¼Œä»£ç†å¯ä»¥é‡‡å–å¤šç§ä¸åŒçš„åŠ¨ä½œã€‚'
- en: A reward is typically formalized as a scalar value, x.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±é€šå¸¸è¢«å½¢å¼åŒ–ä¸ºä¸€ä¸ªæ ‡é‡å€¼xã€‚
- en: 'Given a particular reward, the agent is faced with an optimization dilemma:
    *should the agent maximize short-term rewards or cumulative rewards over its complete
    life history*?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªç‰¹å®šçš„å¥–åŠ±ï¼Œä»£ç†é¢ä¸´ä¸€ä¸ªä¼˜åŒ–å›°å¢ƒï¼š*ä»£ç†åº”è¯¥æœ€å¤§åŒ–çŸ­æœŸå¥–åŠ±ï¼Œè¿˜æ˜¯åœ¨å…¶æ•´ä¸ªç”Ÿå‘½å†å²ä¸­æœ€å¤§åŒ–ç´¯è®¡å¥–åŠ±*ï¼Ÿ
- en: This is known as the [**exploration-exploitation dilemma**](https://en.wikipedia.org/wiki/Exploration-exploitation_dilemma).
    In other words, the transition function should aim to optimize the trade-off between
    exploring the environment and exploiting the knowledge it has accumulated by reaping
    maximal reward.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¢«ç§°ä¸º[**æ¢ç´¢-åˆ©ç”¨å›°å¢ƒ**](https://en.wikipedia.org/wiki/Exploration-exploitation_dilemma)ã€‚æ¢å¥è¯è¯´ï¼Œè½¬ç§»å‡½æ•°åº”å½“æ—¨åœ¨ä¼˜åŒ–æ¢ç´¢ç¯å¢ƒä¸åˆ©ç”¨å·²ç§¯ç´¯çŸ¥è¯†ä¹‹é—´çš„æƒè¡¡ï¼Œä»è€Œè·å¾—æœ€å¤§çš„å¥–åŠ±ã€‚
- en: 'Optimal solutions to the exploration-exploitation dilemma depend on the type
    of tasks we want the model to learn, which range from finite to undefined (continuously
    or discretely infinite). The game of chess, for example, can be formalized as
    an episodic task because it has a a finite configuration space and a predefined
    end-state of three possible outcomes: win, lose, draw. This means that optimal
    successor states given current states can be computed through deterministic transition
    dynamics, where for every state there is a single optimal action.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è§£å†³æ¢ç´¢-åˆ©ç”¨å›°å¢ƒçš„æœ€ä¼˜æ–¹æ¡ˆå–å†³äºæˆ‘ä»¬å¸Œæœ›æ¨¡å‹å­¦ä¹ çš„ä»»åŠ¡ç±»å‹ï¼Œè¿™äº›ä»»åŠ¡çš„èŒƒå›´ä»æœ‰é™åˆ°æœªå®šä¹‰ï¼ˆè¿ç»­æˆ–ç¦»æ•£çš„æ— é™ï¼‰ã€‚ä¾‹å¦‚ï¼Œå›½é™…è±¡æ£‹å¯ä»¥è¢«å½¢å¼åŒ–ä¸ºä¸€ä¸ªé˜¶æ®µæ€§ä»»åŠ¡ï¼Œå› ä¸ºå®ƒæœ‰ä¸€ä¸ªæœ‰é™çš„é…ç½®ç©ºé—´ï¼Œå¹¶ä¸”å…·æœ‰é¢„å®šä¹‰çš„ä¸‰ç§å¯èƒ½ç»“æœçš„ç»ˆå±€ï¼šèƒœåˆ©ã€å¤±è´¥ã€å¹³å±€ã€‚è¿™æ„å‘³ç€ï¼Œåœ¨å½“å‰çŠ¶æ€ä¸‹ï¼Œæœ€ä¼˜çš„åç»§çŠ¶æ€å¯ä»¥é€šè¿‡ç¡®å®šæ€§è½¬ç§»åŠ¨æ€è®¡ç®—å‡ºæ¥ï¼Œå…¶ä¸­æ¯ä¸ªçŠ¶æ€å¯¹åº”ä¸€ä¸ªå”¯ä¸€çš„æœ€ä¼˜åŠ¨ä½œã€‚
- en: However, most tasks do not have a finite configuration space nor predefined
    end-states. We classify these as **continuous tasks**, and optimize them through
    **model-free** approaches. In model-free approaches, instead of computing the
    transition dynamics, the model samples from the environment in order to compute
    optimal successor states. Put differently, instead of planning its actions through
    foresight, it uses trial-and-error to learn about the environment.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¤§å¤šæ•°ä»»åŠ¡æ²¡æœ‰æœ‰é™çš„é…ç½®ç©ºé—´ï¼Œä¹Ÿæ²¡æœ‰é¢„å®šä¹‰çš„ç»ˆæ­¢çŠ¶æ€ã€‚æˆ‘ä»¬å°†è¿™äº›ä»»åŠ¡å½’ç±»ä¸º**è¿ç»­ä»»åŠ¡**ï¼Œå¹¶é€šè¿‡**æ¨¡å‹æ— å…³**çš„æ–¹æ³•è¿›è¡Œä¼˜åŒ–ã€‚åœ¨æ¨¡å‹æ— å…³çš„æ–¹æ³•ä¸­ï¼Œä»£æ›¿è®¡ç®—è½¬ç§»åŠ¨æ€ï¼Œæ¨¡å‹ä»ç¯å¢ƒä¸­é‡‡æ ·ä»¥è®¡ç®—æœ€ä¼˜çš„åç»§çŠ¶æ€ã€‚æ¢å¥è¯è¯´ï¼Œæ¨¡å‹ä¸æ˜¯é€šè¿‡é¢„æµ‹è§„åˆ’è¡ŒåŠ¨ï¼Œè€Œæ˜¯é€šè¿‡è¯•é”™æ³•æ¥å­¦ä¹ ç¯å¢ƒã€‚
- en: 'There are generally two approaches to model-free reinforcement learning: [**Monte
    Carlo approach**](https://en.wikipedia.org/wiki/Monte_Carlo_method)and [**Temporal-difference
    learning**](https://en.wikipedia.org/wiki/Temporal_difference_learning). Since
    averages over sufficient samples converge to expectations, model-free approaches
    estimate expectations through sample means. Monte Carlo methods compute value
    functions by estimating the expected cumulative returns of a sufficiently large
    sample of state-action pairs. Some Monte-Carlo methods evaluate the value function
    only at the end of the task for episodic tasks. For continuous tasks, the definition
    of an episode varies and can be set by the designer such as based on time intervals.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œæ¨¡å‹æ— å…³çš„å¼ºåŒ–å­¦ä¹ æœ‰ä¸¤ç§æ–¹æ³•ï¼š[**è’™ç‰¹å¡ç½—æ–¹æ³•**](https://en.wikipedia.org/wiki/Monte_Carlo_method)å’Œ[**æ—¶åºå·®åˆ†å­¦ä¹ **](https://en.wikipedia.org/wiki/Temporal_difference_learning)ã€‚ç”±äºè¶³å¤Ÿæ ·æœ¬çš„å¹³å‡å€¼ä¼šæ”¶æ•›åˆ°æœŸæœ›å€¼ï¼Œæ¨¡å‹æ— å…³çš„æ–¹æ³•é€šè¿‡æ ·æœ¬å‡å€¼æ¥ä¼°è®¡æœŸæœ›å€¼ã€‚è’™ç‰¹å¡ç½—æ–¹æ³•é€šè¿‡ä¼°è®¡è¶³å¤Ÿå¤§æ ·æœ¬çš„çŠ¶æ€-åŠ¨ä½œå¯¹çš„é¢„æœŸç´¯ç§¯å›æŠ¥æ¥è®¡ç®—ä»·å€¼å‡½æ•°ã€‚ä¸€äº›è’™ç‰¹å¡ç½—æ–¹æ³•åªåœ¨ä»»åŠ¡ç»“æŸæ—¶è¯„ä¼°ä»·å€¼å‡½æ•°ï¼Œé€‚ç”¨äºé˜¶æ®µæ€§ä»»åŠ¡ã€‚å¯¹äºè¿ç»­ä»»åŠ¡ï¼Œé˜¶æ®µçš„å®šä¹‰æœ‰æ‰€ä¸åŒï¼Œå¯ä»¥ç”±è®¾è®¡è€…è®¾ç½®ï¼Œä¾‹å¦‚åŸºäºæ—¶é—´é—´éš”æ¥å®šä¹‰ã€‚
- en: Unlike Monte-Carlo search, Temporal-difference learning, meanwhile, estimate
    the value function incrementally by leveraging differences between time-steps.
    Because of the incremental approach of temporal-difference methods, they exhibit
    a lower variance from the actual expected value than Monte-Carlo methods who rely
    on sampling means.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è’™ç‰¹å¡ç½—æœç´¢ä¸åŒï¼Œæ—¶åºå·®åˆ†å­¦ä¹ åˆ™é€šè¿‡åˆ©ç”¨æ—¶é—´æ­¥ä¹‹é—´çš„å·®å¼‚æ¥é€æ­¥ä¼°è®¡ä»·å€¼å‡½æ•°ã€‚ç”±äºæ—¶åºå·®åˆ†æ–¹æ³•çš„å¢é‡å¼ç‰¹ç‚¹ï¼Œå®ƒä»¬åœ¨ä¸å®é™…æœŸæœ›å€¼çš„åå·®ä¸Šï¼Œæ¯”ä¾èµ–äºæ ·æœ¬å‡å€¼çš„è’™ç‰¹å¡ç½—æ–¹æ³•å…·æœ‰æ›´ä½çš„æ–¹å·®ã€‚
- en: '***To recapitulate*:** the agent navigates its environment through mappings
    from current state and action-space pairs to state-spaces. Transition dynamics
    compute all possible mappings for finite configuration spaces with predefined
    end-states. In lieu of a predefined end-state and finite state-spaces, model-free
    approaches continuously sample from the environment to find the best policy.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ€»ç»“*ï¼š**æ™ºèƒ½ä½“é€šè¿‡å½“å‰çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å¯¹åˆ°çŠ¶æ€ç©ºé—´çš„æ˜ å°„æ¥å¯¼èˆªå…¶ç¯å¢ƒã€‚è½¬ç§»åŠ¨æ€è®¡ç®—æ‰€æœ‰å¯èƒ½çš„æ˜ å°„ï¼Œé€‚ç”¨äºå…·æœ‰é¢„å®šä¹‰ç»ˆæ­¢çŠ¶æ€çš„æœ‰é™é…ç½®ç©ºé—´ã€‚æ²¡æœ‰é¢„å®šä¹‰ç»ˆæ­¢çŠ¶æ€å’Œæœ‰é™çŠ¶æ€ç©ºé—´æ—¶ï¼Œæ¨¡å‹æ— å…³çš„æ–¹æ³•åˆ™é€šè¿‡æŒç»­é‡‡æ ·ç¯å¢ƒæ¥å¯»æ‰¾æœ€ä½³ç­–ç•¥ã€‚'
- en: Dynamic programming computes state-transition probabilities and expected reward
    from all state-action pairs. To understand how this process works, we need to
    understand Markov processes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨æ€è§„åˆ’é€šè¿‡æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹è®¡ç®—çŠ¶æ€è½¬ç§»æ¦‚ç‡å’Œé¢„æœŸå¥–åŠ±ã€‚è¦ç†è§£è¿™ä¸€è¿‡ç¨‹æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£é©¬å°”å¯å¤«è¿‡ç¨‹ã€‚
- en: Next weâ€™re going to learn the mathematical model that enables the agent to compute
    the optimal successor state. As we discussed earlier, optimality resolves into
    the exploration-exploitation dilemma, which varies with the type of task weâ€™re
    trying to model. Looking into the structure of rewards will help us understand
    this better.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æˆ‘ä»¬å°†å­¦ä¹ ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿè®¡ç®—æœ€ä¼˜åç»§çŠ¶æ€çš„æ•°å­¦æ¨¡å‹ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„ï¼Œæœ€ä¼˜æ€§é—®é¢˜è½¬åŒ–ä¸ºæ¢ç´¢-å¼€å‘å›°å¢ƒï¼Œè¿™ä¸€å›°å¢ƒéšç€æˆ‘ä»¬å°è¯•å»ºæ¨¡çš„ä»»åŠ¡ç±»å‹è€Œå˜åŒ–ã€‚æ·±å…¥äº†è§£å¥–åŠ±ç»“æ„å°†å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£è¿™ä¸€ç‚¹ã€‚
- en: Quantifying Reward
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¥–åŠ±é‡åŒ–
- en: We quantify reward in reinforcement learning through a scalar value accrued
    to the agent from the environment upon taking an action. The value of this reward
    indicates the immediate goodness of the action with respect to its end-goal.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªæ ‡é‡å€¼æ¥é‡åŒ–å¥–åŠ±ï¼Œè¿™ä¸ªå€¼æ˜¯æ™ºèƒ½ä½“åœ¨é‡‡å–æŸä¸ªåŠ¨ä½œåä»ç¯å¢ƒä¸­è·å¾—çš„ã€‚è¿™ä¸ªå¥–åŠ±çš„å€¼è¡¨ç¤ºè¯¥åŠ¨ä½œç›¸å¯¹äºå…¶æœ€ç»ˆç›®æ ‡çš„å³æ—¶å¥½åã€‚
- en: Cumulative reward, or *return*, on the other hand, refers to the sum of all
    hitherto accumulated rewards from the environment. The goal of the agent is not
    merely to optimize immediate reward but to optimize cumulative reward. The former
    represents myopic agents who maximize short-term gains, whereas the latter far-sighted
    agents who seek to maximize long-term gains.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œç´¯è®¡å¥–åŠ±æˆ– *å›æŠ¥*ï¼ŒæŒ‡çš„æ˜¯ä»ç¯å¢ƒä¸­è‡³ä»Šç´¯è®¡çš„æ‰€æœ‰å¥–åŠ±ä¹‹å’Œã€‚ä»£ç†çš„ç›®æ ‡ä¸ä»…ä»…æ˜¯ä¼˜åŒ–å³æ—¶å¥–åŠ±ï¼Œè€Œæ˜¯ä¼˜åŒ–ç´¯è®¡å¥–åŠ±ã€‚å‰è€…ä»£è¡¨çš„æ˜¯ç›®å…‰çŸ­æµ…çš„ä»£ç†ï¼Œè¿½æ±‚çŸ­æœŸåˆ©ç›Šï¼Œè€Œåè€…åˆ™æ˜¯ç›®å…‰è¿œå¤§çš„ä»£ç†ï¼Œå¯»æ±‚æœ€å¤§åŒ–é•¿æœŸæ”¶ç›Šã€‚
- en: Since for the most part we want agents to maximize highest rewards sooner rather
    than later, discounting is introduced to incentivize current maximal reward over
    later maximal reward.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå¤§å¤šæ•°æƒ…å†µä¸‹æˆ‘ä»¬å¸Œæœ›ä»£ç†å°½æ—©æœ€å¤§åŒ–å¥–åŠ±è€Œä¸æ˜¯è¿Ÿäº›æ—¶å€™ï¼Œå› æ­¤å¼•å…¥æŠ˜æ‰£å› å­æ¥æ¿€åŠ±å½“å‰çš„æœ€å¤§å¥–åŠ±è€Œéæœªæ¥çš„æœ€å¤§å¥–åŠ±ã€‚
- en: 'We quantify cumulative reward G with discounting by the expression below:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹å…¬å¼é‡åŒ–å¸¦æœ‰æŠ˜æ‰£çš„ç´¯è®¡å¥–åŠ± Gï¼š
- en: '![](../Images/f2c4acce8007ae4321599db32ae9de51.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2c4acce8007ae4321599db32ae9de51.png)'
- en: 'Here, cumulative reward **G** equals to the sum of products of a reward and
    its discount factor gamma ğœ¸, which is always a value between 0 and 1: {0,1}. Gamma
    is incrementally exponentiated with each time-step, which means that across infinite
    time-steps gamma approaches zero.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œç´¯è®¡å¥–åŠ± **G** ç­‰äºå¥–åŠ±ä¸æŠ˜æ‰£å› å­ gamma ğœ¸ çš„ä¹˜ç§¯ä¹‹å’Œï¼Œgamma çš„å€¼å§‹ç»ˆä»‹äº 0 å’Œ 1 ä¹‹é—´ï¼š{0,1}ã€‚gamma ä¼šéšç€æ¯ä¸ªæ—¶é—´æ­¥çš„å¢åŠ é€æ¸é€’å¢ï¼Œè¿™æ„å‘³ç€åœ¨æ— é™æ—¶é—´æ­¥ä¸­ï¼Œgamma
    ä¼šè¶‹è¿‘äºé›¶ã€‚
- en: As gamma approaches 0, it incentivizes short-term gains, whereas if gamma approaches
    1, it incentivizes long-term gains since across infinite iterations the reward
    sum will itself approach infinity.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ gamma è¶‹è¿‘äº 0 æ—¶ï¼Œå®ƒæ¿€åŠ±çŸ­æœŸæ”¶ç›Šï¼Œè€Œå½“ gamma è¶‹è¿‘äº 1 æ—¶ï¼Œå®ƒæ¿€åŠ±é•¿æœŸæ”¶ç›Šï¼Œå› ä¸ºåœ¨æ— é™æ¬¡è¿­ä»£ä¸­ï¼Œå¥–åŠ±æ€»å’Œå°†è¶‹è¿‘äºæ— ç©·å¤§ã€‚
- en: Because most tasks are bounded in time, gamma discounting imposes upper bounds
    on rewards when it is below the value of 1.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºå¤§å¤šæ•°ä»»åŠ¡æ˜¯æœ‰æ—¶é—´é™åˆ¶çš„ï¼Œå½“ gamma å°äº 1 æ—¶ï¼ŒæŠ˜æ‰£å› å­ä¼šå¯¹å¥–åŠ±æ–½åŠ ä¸Šé™ã€‚
- en: 'The condensed equation for cumulative reward with discounting is given below,
    where G stands for the sum of expected rewards *R,* which is multiplied by the
    discounting factor, gamma. Cumulative reward is therefore computed as the sum
    of the reward and the discounting factor:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹è¿°æ˜¯å¸¦æœ‰æŠ˜æ‰£å› å­çš„ç´¯è®¡å¥–åŠ±çš„ç®€åŒ–å…¬å¼ï¼Œå…¶ä¸­ G ä»£è¡¨æœŸæœ›å¥–åŠ±çš„æ€»å’Œ *R*ï¼Œè¯¥å¥–åŠ±ä¹˜ä»¥æŠ˜æ‰£å› å­ gammaã€‚å› æ­¤ï¼Œç´¯è®¡å¥–åŠ±æ˜¯å¥–åŠ±å’ŒæŠ˜æ‰£å› å­çš„æ€»å’Œï¼š
- en: '![](../Images/44e62f6e99a708c217910ec297a8f62f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44e62f6e99a708c217910ec297a8f62f.png)'
- en: Markov Decision Process (MDP)
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)
- en: So far weâ€™ve discussed the probabilistic definition of the policy as a mapping
    from a state to an action, transition dynamics as the probability of moving from
    one state to another given reward, and the formula for how reward is calculated.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬è®¨è®ºäº†ç­–ç•¥çš„æ¦‚ç‡å®šä¹‰ï¼Œå³ä»ä¸€ä¸ªçŠ¶æ€åˆ°ä¸€ä¸ªåŠ¨ä½œçš„æ˜ å°„ï¼Œè½¬ç§»åŠ¨æ€æ˜¯æŒ‡åœ¨ç»™å®šå¥–åŠ±çš„æƒ…å†µä¸‹ä»ä¸€ä¸ªçŠ¶æ€è½¬ç§»åˆ°å¦ä¸€ä¸ªçŠ¶æ€çš„æ¦‚ç‡ï¼Œä»¥åŠå¥–åŠ±è®¡ç®—çš„å…¬å¼ã€‚
- en: Now, weâ€™re going to step back a little and provide some supplementary theory
    that defines these probabilistic transition chains. We will start with something
    called a **Markov process**. Markov processes are stochastic processes that satisfy
    the **Markov property.** A stochastic process is a process that varies randomly.
    The Markov property states that *for every state, successor states are conditioned
    only by present states*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å°†ç¨å¾®é€€åä¸€æ­¥ï¼Œæä¾›ä¸€äº›è¡¥å……ç†è®ºï¼Œå®šä¹‰è¿™äº›æ¦‚ç‡è½¬ç§»é“¾ã€‚æˆ‘ä»¬å°†ä»ä¸€ç§è¢«ç§°ä¸º **é©¬å°”å¯å¤«è¿‡ç¨‹** çš„ä¸œè¥¿å¼€å§‹ã€‚é©¬å°”å¯å¤«è¿‡ç¨‹æ˜¯æ»¡è¶³ **é©¬å°”å¯å¤«æ€§è´¨**
    çš„éšæœºè¿‡ç¨‹ã€‚éšæœºè¿‡ç¨‹æ˜¯æŒ‡éšæœºå˜åŒ–çš„è¿‡ç¨‹ã€‚é©¬å°”å¯å¤«æ€§è´¨è¡¨æ˜ï¼Œ*å¯¹äºæ¯ä¸ªçŠ¶æ€ï¼Œåç»§çŠ¶æ€ä»…ç”±å½“å‰çŠ¶æ€å†³å®š*ã€‚
- en: Because prior states do not bear on future states, processes that satisfy the
    Markov property are called memoryless. Imagine a set of fixed destinations that
    recur daily as you leave your home to go work before returning home again. In
    other words, we have a cyclical process with a beginning and an end. Now further
    imagine that your decision to move from one destination to the next only depends
    on your current destination, not your previous history of destinations. Initially,
    every connected destination would have an equal distribution of probabilities.
    For example, if upon leaving home you have the option to drive or take the metro,
    weâ€™d ascribe initial probabilities to each of these possible future states as
    0.5\. Over iterations of all possible routes these probabilities might stabilize
    to some frequency distribution with some routes skewing preferentially over others.
    (This type of probability is called empirical probability because it averages
    outcomes over possible events relative to a finite number of tests) That distribution
    equilibrium would be the Markov chain or process.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå…ˆå‰çš„çŠ¶æ€ä¸ä¼šå½±å“æœªæ¥çš„çŠ¶æ€ï¼Œæ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨çš„è¿‡ç¨‹è¢«ç§°ä¸ºæ— è®°å¿†è¿‡ç¨‹ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œæ¯å¤©ä½ ä»å®¶é‡Œå‡ºå‘å»ä¸Šç­ï¼Œç„¶åå†å›åˆ°å®¶é‡Œï¼Œè¿™æ˜¯ä¸€ç»„å›ºå®šçš„ç›®çš„åœ°ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæœ‰å§‹æœ‰ç»ˆçš„å¾ªç¯è¿‡ç¨‹ã€‚ç°åœ¨å†è¿›ä¸€æ­¥æƒ³è±¡ï¼Œä½ ä»ä¸€ä¸ªç›®çš„åœ°åˆ°ä¸‹ä¸€ä¸ªç›®çš„åœ°çš„å†³ç­–ä»…ä¾èµ–äºä½ å½“å‰çš„ç›®çš„åœ°ï¼Œè€Œä¸è€ƒè™‘ä½ ä¹‹å‰çš„ç›®çš„åœ°å†å²ã€‚æœ€åˆï¼Œæ¯ä¸ªè¿æ¥çš„ç›®çš„åœ°éƒ½æœ‰ç›¸ç­‰çš„æ¦‚ç‡åˆ†å¸ƒã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ ç¦»å¼€å®¶æ—¶å¯ä»¥é€‰æ‹©å¼€è½¦æˆ–ä¹˜ååœ°é“ï¼Œæˆ‘ä»¬ä¼šç»™è¿™ä¸¤ç§å¯èƒ½çš„æœªæ¥çŠ¶æ€åˆ†åˆ«èµ‹äºˆåˆå§‹æ¦‚ç‡0.5ã€‚åœ¨æ‰€æœ‰å¯èƒ½çš„è·¯çº¿è¿­ä»£ä¸­ï¼Œè¿™äº›æ¦‚ç‡å¯èƒ½ä¼šç¨³å®šä¸ºæŸç§é¢‘ç‡åˆ†å¸ƒï¼Œå…¶ä¸­æŸäº›è·¯çº¿ç›¸è¾ƒå…¶ä»–è·¯çº¿ä¼šåå‘äºå‘ç”Ÿã€‚ï¼ˆè¿™ç§ç±»å‹çš„æ¦‚ç‡å«åšç»éªŒæ¦‚ç‡ï¼Œå› ä¸ºå®ƒé€šè¿‡ç›¸å¯¹äºæœ‰é™æ¬¡æ•°æµ‹è¯•çš„å¯èƒ½äº‹ä»¶çš„ç»“æœå¹³å‡æ¥è®¡ç®—ï¼‰è¯¥åˆ†å¸ƒçš„å¹³è¡¡çŠ¶æ€å°†æ˜¯é©¬å°”å¯å¤«é“¾æˆ–è¿‡ç¨‹ã€‚
- en: 'Now youâ€™re probably thinking: how do you define events and states? Isnâ€™t the
    world infinitely complex to be talking about fixed possible states and stable
    probability distributions?'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å¯èƒ½åœ¨æƒ³ï¼šå¦‚ä½•å®šä¹‰äº‹ä»¶å’ŒçŠ¶æ€ï¼Ÿéš¾é“è¿™ä¸ªä¸–ç•Œä¸ä¼šå› ä¸ºè¿‡äºå¤æ‚è€Œæ— æ³•è®¨è®ºå›ºå®šçš„å¯èƒ½çŠ¶æ€å’Œç¨³å®šçš„æ¦‚ç‡åˆ†å¸ƒå—ï¼Ÿ
- en: Quite true, but since weâ€™re after a mathematical formalism of agents in environments,
    we need therefore to distinguish between the types of tasks or environments we
    are trying to model. To do this, we need to specify the representation of both
    *time steps* and *state spaces*, that is, the distributions of all possible states.
    The square matrix below provides a definition of Markov chains with respect to
    axes of **state-space** and **time:**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡é”™ï¼Œä½†ç”±äºæˆ‘ä»¬æ­£åœ¨è¿½æ±‚ä¸€ä¸ªå…³äºä»£ç†ä¸ç¯å¢ƒçš„æ•°å­¦å½¢å¼åŒ–ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦åŒºåˆ†æˆ‘ä»¬è¦å»ºæ¨¡çš„ä»»åŠ¡æˆ–ç¯å¢ƒçš„ç±»å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ˜ç¡®*æ—¶é—´æ­¥*å’Œ*çŠ¶æ€ç©ºé—´*çš„è¡¨ç¤ºï¼Œå³æ‰€æœ‰å¯èƒ½çŠ¶æ€çš„åˆ†å¸ƒã€‚ä¸‹æ–¹çš„æ–¹é˜µå®šä¹‰äº†å…³äº**çŠ¶æ€ç©ºé—´**å’Œ**æ—¶é—´**è½´çš„é©¬å°”å¯å¤«é“¾ï¼š
- en: '![](../Images/fee2c33c281ecd124d0ffef43bbd67d6.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fee2c33c281ecd124d0ffef43bbd67d6.png)'
- en: The state-space can be defined as countable/finite or continuous, where finite
    state-spaces describe all the possible configurations of the system through combinatorics,
    while continuous state-spaces describe all the possible configurations through
    a continuous function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: çŠ¶æ€ç©ºé—´å¯ä»¥å®šä¹‰ä¸ºå¯æ•°/æœ‰é™æˆ–è¿ç»­ï¼Œå…¶ä¸­æœ‰é™çŠ¶æ€ç©ºé—´é€šè¿‡ç»„åˆæ•°å­¦æè¿°ç³»ç»Ÿçš„æ‰€æœ‰å¯èƒ½é…ç½®ï¼Œè€Œè¿ç»­çŠ¶æ€ç©ºé—´é€šè¿‡è¿ç»­å‡½æ•°æè¿°æ‰€æœ‰å¯èƒ½çš„é…ç½®ã€‚
- en: Finite and countably infinite spaces take integers or rational numbers as their
    measurable space, whereas continuous spaces take real numbers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰é™å’Œå¯æ•°æ— é™ç©ºé—´å°†æ•´æ•°æˆ–æœ‰ç†æ•°ä½œä¸ºå…¶å¯æµ‹ç©ºé—´ï¼Œè€Œè¿ç»­ç©ºé—´åˆ™å–å®æ•°ã€‚
- en: Likewise, the axis of time can be defined as discrete or continuous.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼Œæ—¶é—´è½´ä¹Ÿå¯ä»¥å®šä¹‰ä¸ºç¦»æ•£æˆ–è¿ç»­çš„ã€‚
- en: Discrete-time processes count phase-transitions as discontinuous but can be
    modeled on either countable or uncountable state-spaces, where uncountable refers
    to infinite decimal expansions of real numbers. This is in fact how your computer
    counts time â€” it does so in discrete steps. The interval between the steps varies
    across architectures, but a cycle is usually measured as the length of the time-step
    required to change a register state.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ç¦»æ•£æ—¶é—´è¿‡ç¨‹å°†ç›¸å˜è§†ä¸ºä¸è¿ç»­çš„ï¼Œä½†å¯ä»¥åœ¨å¯æ•°æˆ–ä¸å¯æ•°çŠ¶æ€ç©ºé—´ä¸Šå»ºæ¨¡ï¼Œå…¶ä¸­ä¸å¯æ•°æ˜¯æŒ‡å®æ•°çš„æ— é™å°æ•°æ‰©å±•ã€‚å®é™…ä¸Šï¼Œä½ çš„è®¡ç®—æœºå°±æ˜¯è¿™æ ·è®¡ç®—æ—¶é—´çš„â€”â€”å®ƒä»¥ç¦»æ•£æ­¥éª¤æ¥è¿›è¡Œæ—¶é—´è®¡ç®—ã€‚æ­¥éª¤ä¹‹é—´çš„é—´éš”å› æ¶æ„è€Œå¼‚ï¼Œä½†ä¸€ä¸ªå‘¨æœŸé€šå¸¸è¢«è¡¡é‡ä¸ºæ”¹å˜å¯„å­˜å™¨çŠ¶æ€æ‰€éœ€çš„æ—¶é—´æ­¥é•¿ã€‚
- en: Continuous-time chains count phase-transitions as continuous and can be modeled
    on countable or uncountable state-spaces.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ç»­æ—¶é—´é“¾å°†ç›¸å˜è§†ä¸ºè¿ç»­çš„ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å¯æ•°æˆ–ä¸å¯æ•°çŠ¶æ€ç©ºé—´ä¸Šå»ºæ¨¡ã€‚
- en: 'The term Markov process is typically reserved for continuous-time processes,
    whereas the term Markov chain describes a subset of those: discrete-time, stochastic
    control processes. In the rest of the article, we will focus on *discrete-time,
    finite state-spaces*.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: é©¬å°”å¯å¤«è¿‡ç¨‹ä¸€è¯é€šå¸¸ç”¨äºæè¿°è¿ç»­æ—¶é—´è¿‡ç¨‹ï¼Œè€Œé©¬å°”å¯å¤«é“¾ä¸€è¯æè¿°çš„æ˜¯å…¶ä¸­çš„ä¸€ä¸ªå­é›†ï¼šç¦»æ•£æ—¶é—´çš„éšæœºæ§åˆ¶è¿‡ç¨‹ã€‚åœ¨æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†é‡ç‚¹è®¨è®º*ç¦»æ•£æ—¶é—´ï¼Œæœ‰é™çŠ¶æ€ç©ºé—´*ã€‚
- en: 'So far our Markov chains are very simplistic as they describe transitions between
    states with fixed probabilities. Weâ€™re missing two ingredients important for modelling
    behaviour in our ontology: ***actions*** and ***rewards***.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„é©¬å°”å¯å¤«é“¾éå¸¸ç®€åŒ–ï¼Œå®ƒä»¬ä»…æè¿°äº†å…·æœ‰å›ºå®šæ¦‚ç‡çš„çŠ¶æ€è½¬ç§»ã€‚æˆ‘ä»¬è¿˜ç¼ºå°‘ä¸¤ä¸ªåœ¨å»ºæ¨¡è¡Œä¸ºæ—¶è‡³å…³é‡è¦çš„è¦ç´ ï¼š***è¡ŒåŠ¨***å’Œ***å¥–åŠ±***ã€‚
- en: 'Adducing rewards to transition probabilities constitute ***Markov Reward Processes***.
    Markov Reward Processes assign a reward to each transition state, (defined as
    a positive or negative integer) thereby nudging the system toward some desired
    state. Recall our cumulative reward formula as the sum of expected rewards multiplied
    with some discounting factor. A Markov Reward Process allows us then to calculate
    the value of the state ***v(s)*** as the probability of cumulative reward G (where
    G is averaged over a large sample of iterations) given initial state S:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å¼•å…¥å¥–åŠ±åˆ°è½¬ç§»æ¦‚ç‡æ„æˆäº†***é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹***ã€‚é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹ä¸ºæ¯ä¸ªè½¬ç§»çŠ¶æ€åˆ†é…ä¸€ä¸ªå¥–åŠ±ï¼ˆå®šä¹‰ä¸ºæ­£æ•´æ•°æˆ–è´Ÿæ•´æ•°ï¼‰ï¼Œä»è€Œæ¨åŠ¨ç³»ç»Ÿå‘æŸä¸ªæœŸæœ›çš„çŠ¶æ€ç§»åŠ¨ã€‚å›æƒ³æˆ‘ä»¬çš„ç´¯ç§¯å¥–åŠ±å…¬å¼ï¼Œå®ƒæ˜¯æœŸæœ›å¥–åŠ±ä¹˜ä»¥æŸä¸ªæŠ˜æ‰£å› å­çš„æ€»å’Œã€‚é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹å…è®¸æˆ‘ä»¬è®¡ç®—çŠ¶æ€***v(s)***çš„å€¼ï¼Œå³ç»™å®šåˆå§‹çŠ¶æ€Sï¼Œç´¯ç§¯å¥–åŠ±Gçš„æ¦‚ç‡ï¼ˆå…¶ä¸­Gæ˜¯é€šè¿‡å¤§é‡è¿­ä»£çš„æ ·æœ¬å¹³å‡å¾—åˆ°çš„ï¼‰ï¼š
- en: '![](../Images/da2bb307dc6da5b4bdfc6121f8abadc8.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da2bb307dc6da5b4bdfc6121f8abadc8.png)'
- en: 'The last variable we need to adduce in order to scaffold to ***Markov Decision
    Processes*** are actions. The agent begins with equally distributed probabilities
    of a set of possible actions and subsequently updates the transition function
    as a mapping from current state and action to the next state and reward. Weâ€™ve
    ended up full-circle to the transition dynamics that we described earlier:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ”¯æ’‘åˆ°***é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹***ï¼Œæˆ‘ä»¬éœ€è¦å¼•å…¥çš„æœ€åä¸€ä¸ªå˜é‡æ˜¯è¡ŒåŠ¨ã€‚æ™ºèƒ½ä½“é¦–å…ˆä»¥ä¸€ç»„å¯èƒ½è¡ŒåŠ¨çš„å‡åŒ€åˆ†å¸ƒæ¦‚ç‡å¼€å§‹ï¼Œç„¶åæ ¹æ®å½“å‰çŠ¶æ€å’Œè¡ŒåŠ¨æ›´æ–°è½¬ç§»å‡½æ•°ï¼Œå°†å…¶æ˜ å°„åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±ã€‚æˆ‘ä»¬å·²ç»å›åˆ°äº†ä¹‹å‰æè¿°çš„è½¬ç§»åŠ¨æ€ï¼š
- en: '![](../Images/f1a6a4ad7ea89b2d60b2754eca35ed07.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1a6a4ad7ea89b2d60b2754eca35ed07.png)'
- en: Dynamic Programming & Bellman Optimality
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ¨æ€è§„åˆ’ä¸è´å°”æ›¼æœ€ä¼˜æ€§
- en: This brings us to the concept of dynamic programming, developed by Bellman (1957).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¼•å‡ºäº†åŠ¨æ€è§„åˆ’çš„æ¦‚å¿µï¼Œè¯¥æ¦‚å¿µç”±è´å°”æ›¼ï¼ˆ1957å¹´ï¼‰æå‡ºã€‚
- en: Understanding dynamic programming will help us understand approximation methods
    like Monte Carlo Search and Temporal Difference, which do not require complete
    knowledge of the environment like dynamic programming does. These model-free methods
    approximate the deterministic policy of dynamic programming in lieu of perfect
    information. As such, they provide powerful mechanisms that approximate real-world
    learning.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£åŠ¨æ€è§„åˆ’å°†å¸®åŠ©æˆ‘ä»¬ç†è§£åƒè’™ç‰¹å¡æ´›æœç´¢å’Œæ—¶é—´å·®åˆ†ç­‰è¿‘ä¼¼æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä¸åƒåŠ¨æ€è§„åˆ’é‚£æ ·éœ€è¦å®Œå…¨çš„ç¯å¢ƒçŸ¥è¯†ã€‚è¿™äº›æ— æ¨¡å‹çš„æ–¹æ³•é€šè¿‡ä»£æ›¿å®Œç¾çš„ä¿¡æ¯æ¥è¿‘ä¼¼åŠ¨æ€è§„åˆ’çš„ç¡®å®šæ€§ç­–ç•¥ã€‚å› æ­¤ï¼Œå®ƒä»¬æä¾›äº†å¼ºå¤§çš„æœºåˆ¶æ¥è¿‘ä¼¼ç°å®ä¸–ç•Œçš„å­¦ä¹ ã€‚
- en: The core idea behind how dynamic programming searches and finds the optimal
    agent state concerns the relationship between the ***state-value function*** and
    the ***action-value function***. These are recursively related.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨æ€è§„åˆ’å¦‚ä½•æœç´¢å¹¶æ‰¾åˆ°æœ€ä¼˜æ™ºèƒ½ä½“çŠ¶æ€çš„æ ¸å¿ƒæ€æƒ³æ¶‰åŠåˆ°***çŠ¶æ€å€¼å‡½æ•°***ä¸***è¡ŒåŠ¨å€¼å‡½æ•°***ä¹‹é—´çš„å…³ç³»ã€‚è¿™äº›å‡½æ•°æ˜¯é€’å½’ç›¸å…³çš„ã€‚
- en: 'Letâ€™s expound on these ideas with a relatable example. Letâ€™s say that you are
    at a suboptimal state in your life and want to change this. Letâ€™s further say
    that you have a tangible goal or place you would like to be in the future within
    some realistic time horizon. In order to arrive at the grand goal (here you can
    substitute anything: better job, start a family etc), you will need to take a
    series of smaller steps or actions that will be conducive to your desired outcome.
    Translated in the language of reinforcement learning, your current state will
    be assigned a value. Given your current state and value, you will take actions.
    These actions will also be evaluated with respect to your overall goal and current
    state. A good action will receive a higher valuation than a bad action. Feedback
    from the environment will determine the value of the action (how these are determined
    varies with the task). The evaluation of the state will affect the valuation of
    the available actions and successor states. And the evaluation of the actions
    will recursively affect the value of the current state. In other words, actions
    and states are dynamically connected through recursion.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®¹æ˜“ç†è§£çš„ä¾‹å­æ¥é˜è¿°è¿™äº›æ¦‚å¿µã€‚å‡è®¾ä½ ç›®å‰å¤„äºç”Ÿæ´»ä¸­çš„ä¸€ä¸ªæ¬¡ä¼˜çŠ¶æ€ï¼Œå¹¶ä¸”å¸Œæœ›æ”¹å˜è¿™ç§æƒ…å†µã€‚è¿›ä¸€æ­¥å‡è®¾ä½ æœ‰ä¸€ä¸ªåˆ‡å®å¯è¡Œçš„ç›®æ ‡æˆ–ä¸€ä¸ªå¸Œæœ›åœ¨æŸä¸ªç°å®æ—¶é—´èŒƒå›´å†…åˆ°è¾¾çš„åœ°æ–¹ã€‚ä¸ºäº†å®ç°è¿™ä¸€å®å¤§ç›®æ ‡ï¼ˆä½ å¯ä»¥åœ¨è¿™é‡Œæ›¿æ¢ä»»ä½•ç›®æ ‡ï¼šæ›´å¥½çš„å·¥ä½œã€å»ºç«‹å®¶åº­ç­‰ï¼‰ï¼Œä½ éœ€è¦é‡‡å–ä¸€ç³»åˆ—è¾ƒå°çš„æ­¥éª¤æˆ–è¡ŒåŠ¨ï¼Œè¿™äº›æ­¥éª¤æˆ–è¡ŒåŠ¨å°†æœ‰åŠ©äºå®ç°ä½ æƒ³è¦çš„ç»“æœã€‚ç”¨å¼ºåŒ–å­¦ä¹ çš„è¯­è¨€æ¥è¯´ï¼Œä½ å½“å‰çš„çŠ¶æ€å°†è¢«èµ‹äºˆä¸€ä¸ªå€¼ã€‚æ ¹æ®ä½ å½“å‰çš„çŠ¶æ€å’Œä»·å€¼ï¼Œä½ å°†é‡‡å–è¡ŒåŠ¨ã€‚è¿™äº›è¡ŒåŠ¨ä¹Ÿå°†æ ¹æ®ä½ æ•´ä½“ç›®æ ‡å’Œå½“å‰çŠ¶æ€è¿›è¡Œè¯„ä¼°ã€‚ä¸€é¡¹å¥½çš„è¡ŒåŠ¨å°†è·å¾—æ¯”ä¸å¥½çš„è¡ŒåŠ¨æ›´é«˜çš„è¯„ä»·ã€‚æ¥è‡ªç¯å¢ƒçš„åé¦ˆå°†å†³å®šè¡ŒåŠ¨çš„ä»·å€¼ï¼ˆè¿™äº›å¦‚ä½•ç¡®å®šå–å†³äºä»»åŠ¡ï¼‰ã€‚çŠ¶æ€çš„è¯„ä¼°å°†å½±å“å¯ç”¨è¡ŒåŠ¨å’Œåç»§çŠ¶æ€çš„ä¼°å€¼ã€‚è¡ŒåŠ¨çš„è¯„ä¼°å°†é€’å½’åœ°å½±å“å½“å‰çŠ¶æ€çš„ä»·å€¼ã€‚æ¢å¥è¯è¯´ï¼Œè¡ŒåŠ¨å’ŒçŠ¶æ€é€šè¿‡é€’å½’åŠ¨æ€åœ°è”ç³»åœ¨ä¸€èµ·ã€‚
- en: Now, in real life, your goal and the action-steps to get to that goal cannot
    be specified as a deterministic system with discrete time-steps and and a discrete
    state-space (though perhaps they could be approximated this way). Instead, dynamic
    programming assumes a specifiable environment much like the game of chess, where
    time-steps and action-spaces are abstracted as discrete and finite. The overlap
    with real life closes on the fact that a larger goal will be approached through
    optimization of smaller sub-goals conducive to that larger goal.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œåœ¨ç°å®ç”Ÿæ´»ä¸­ï¼Œä½ çš„ç›®æ ‡å’Œå®ç°è¯¥ç›®æ ‡çš„è¡ŒåŠ¨æ­¥éª¤ä¸èƒ½è¢«æŒ‡å®šä¸ºä¸€ä¸ªå…·æœ‰ç¦»æ•£æ—¶é—´æ­¥éª¤å’Œç¦»æ•£çŠ¶æ€ç©ºé—´çš„ç¡®å®šæ€§ç³»ç»Ÿï¼ˆå°½ç®¡å®ƒä»¬æˆ–è®¸å¯ä»¥é€šè¿‡è¿™ç§æ–¹å¼è¿›è¡Œè¿‘ä¼¼ï¼‰ã€‚ç›¸åï¼ŒåŠ¨æ€è§„åˆ’å‡è®¾ä¸€ä¸ªå¯æŒ‡å®šçš„ç¯å¢ƒï¼Œå°±åƒå›½é™…è±¡æ£‹æ¸¸æˆä¸€æ ·ï¼Œå…¶ä¸­æ—¶é—´æ­¥éª¤å’Œè¡ŒåŠ¨ç©ºé—´è¢«æŠ½è±¡ä¸ºç¦»æ•£å’Œæœ‰é™çš„ã€‚ä¸ç°å®ç”Ÿæ´»çš„é‡å ä¹‹å¤„åœ¨äºï¼Œä¸€ä¸ªæ›´å¤§çš„ç›®æ ‡å°†é€šè¿‡ä¼˜åŒ–æœ‰åŠ©äºå®ç°è¯¥ç›®æ ‡çš„è¾ƒå°å­ç›®æ ‡æ¥æ¥è¿‘ã€‚
- en: 'Dynamic programming therefore will assume the following values: **(Î©,A,ğ’«)**,
    where **Î©** represents the total of all possible states, **A** an action event
    as a subset of the finite sample space, and **P** as the probability assigned
    to each action event by some policy function ğ….'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒåŠ¨æ€è§„åˆ’å°†å‡è®¾ä»¥ä¸‹å€¼ï¼š**(Î©,A,ğ’«)**ï¼Œå…¶ä¸­**Î©**è¡¨ç¤ºæ‰€æœ‰å¯èƒ½çŠ¶æ€çš„æ€»å’Œï¼Œ**A**æ˜¯ä½œä¸ºæœ‰é™æ ·æœ¬ç©ºé—´å­é›†çš„è¡ŒåŠ¨äº‹ä»¶ï¼Œ**P**æ˜¯ç”±æŸäº›ç­–ç•¥å‡½æ•°ğ…ä¸ºæ¯ä¸ªè¡ŒåŠ¨äº‹ä»¶åˆ†é…çš„æ¦‚ç‡ã€‚
- en: Now, if you think back to our *deterministic transition dynamics*, since the
    sets of states, actions, and rewards are finite, any particular state and reward
    pair will have a probability of those values occurring given some prior state
    and action pair. These probabilities are specified as discrete probability distributions
    of random variables since the state space is discrete. We said that sequences
    consisting of states, actions, and rewards are Markov Decision Processes (MDPs)
    that seek to maximize expected cumulative reward over time, where reward is represented
    as a scalar value.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœä½ å›æƒ³ä¸€ä¸‹æˆ‘ä»¬çš„*ç¡®å®šæ€§è½¬ç§»åŠ¨æ€*ï¼Œç”±äºçŠ¶æ€ã€è¡ŒåŠ¨å’Œå¥–åŠ±çš„é›†åˆæ˜¯æœ‰é™çš„ï¼Œä»»ä½•ç‰¹å®šçš„çŠ¶æ€å’Œå¥–åŠ±å¯¹å°†åœ¨ç»™å®šæŸäº›å…ˆå‰çš„çŠ¶æ€å’Œè¡ŒåŠ¨å¯¹çš„æƒ…å†µä¸‹ï¼Œå…·æœ‰å‘ç”Ÿè¿™äº›å€¼çš„æ¦‚ç‡ã€‚ç”±äºçŠ¶æ€ç©ºé—´æ˜¯ç¦»æ•£çš„ï¼Œè¿™äº›æ¦‚ç‡è¢«æŒ‡å®šä¸ºéšæœºå˜é‡çš„ç¦»æ•£æ¦‚ç‡åˆ†å¸ƒã€‚æˆ‘ä»¬è¯´è¿‡ï¼Œç”±çŠ¶æ€ã€è¡ŒåŠ¨å’Œå¥–åŠ±ç»„æˆçš„åºåˆ—æ˜¯é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå®ƒå¯»æ±‚åœ¨æ—¶é—´ä¸Šæœ€å¤§åŒ–æœŸæœ›çš„ç´¯ç§¯å¥–åŠ±ï¼Œå…¶ä¸­å¥–åŠ±è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªæ ‡é‡å€¼ã€‚
- en: '*Now the question we need to address is how does a Markov Decision Process
    maximize cumulative reward given the assumptions weâ€™ve specified?*The answer is
    provided by the **Bellman Optimality Equations** which relate two functions: ***the
    state-value function*** and the ***action-value function***.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç°åœ¨æˆ‘ä»¬éœ€è¦è§£å†³çš„é—®é¢˜æ˜¯ï¼Œç»™å®šæˆ‘ä»¬å·²æŒ‡å®šçš„å‡è®¾ï¼Œé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹å¦‚ä½•æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼Ÿ*ç­”æ¡ˆç”±**è´å°”æ›¼æœ€ä¼˜æ€§æ–¹ç¨‹**æä¾›ï¼Œå®ƒæ¶‰åŠä¸¤ä¸ªå‡½æ•°ï¼š***çŠ¶æ€å€¼å‡½æ•°***å’Œ***è¡ŒåŠ¨å€¼å‡½æ•°***ã€‚'
- en: State-Value Function
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çŠ¶æ€å€¼å‡½æ•°
- en: The state-value function can be defined as the sum of the probabilities of all
    possible actions an agent can take under a policy ğ…, where, for each action, itâ€™s
    value is determined by the sum of all weighted values of possible successor states.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: çŠ¶æ€å€¼å‡½æ•°å¯ä»¥å®šä¹‰ä¸ºæ™ºèƒ½ä½“åœ¨éµå¾ªç­–ç•¥ğ…çš„æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œçš„æ¦‚ç‡ä¹‹å’Œï¼Œå…¶ä¸­ï¼Œå¯¹äºæ¯ä¸ªåŠ¨ä½œï¼Œå…¶å€¼ç”±æ‰€æœ‰å¯èƒ½åç»§çŠ¶æ€çš„åŠ æƒå€¼ä¹‹å’Œå†³å®šã€‚
- en: Put more simply, the state-value function defines the expected cumulative reward
    an agent can obtain starting from a particular state (s) by following policy ğ….
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€å•æ¥è¯´ï¼ŒçŠ¶æ€å€¼å‡½æ•°å®šä¹‰äº†æ™ºèƒ½ä½“ä»ç‰¹å®šçŠ¶æ€(s)å¼€å§‹ï¼Œéµå¾ªç­–ç•¥ğ…æ—¶å¯ä»¥è·å¾—çš„æœŸæœ›ç´¯ç§¯å¥–åŠ±ã€‚
- en: '![](../Images/ed9537f25ec200181725548dcaa75472.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed9537f25ec200181725548dcaa75472.png)'
- en: State-value function
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: çŠ¶æ€å€¼å‡½æ•°
- en: 'The above equation contains two terms: **a)** *the sum of the probabilities
    of all possible actions an agent can take in state (s) following policy ğ…*, and
    **b)** *an inner sum for each possible action that computes the weighted values
    of all possible successor states*. The term within the square brackets computes
    the contributions of each actionâ€™s possible states as the sum of the immediate
    reward R(s, a, sâ€™) and discounted reward by gamma factor ğ›¾.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°æ–¹ç¨‹åŒ…å«ä¸¤ä¸ªé¡¹ï¼š**a)** *æ™ºèƒ½ä½“åœ¨çŠ¶æ€(s)ä¸‹éµå¾ªç­–ç•¥ğ…æ—¶ï¼Œæ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„æ¦‚ç‡ä¹‹å’Œ*ï¼Œä»¥åŠ**b)** *å¯¹äºæ¯ä¸ªå¯èƒ½åŠ¨ä½œçš„å†…å±‚æ±‚å’Œï¼Œè®¡ç®—æ‰€æœ‰å¯èƒ½åç»§çŠ¶æ€çš„åŠ æƒå€¼*ã€‚æ–¹æ‹¬å·å†…çš„é¡¹è®¡ç®—äº†æ¯ä¸ªåŠ¨ä½œçš„å¯èƒ½çŠ¶æ€çš„è´¡çŒ®ï¼Œå®ƒæ˜¯å³æ—¶å¥–åŠ±R(s,
    a, s')ä¸æŠ˜æ‰£å¥–åŠ±ï¼ˆç”±æŠ˜æ‰£å› å­ğ›¾è®¡ç®—ï¼‰ä¹‹å’Œã€‚
- en: Another way of expressing the state-value function is the following**:**
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨è¾¾çŠ¶æ€å€¼å‡½æ•°çš„å¦ä¸€ç§æ–¹å¼æ˜¯ä»¥ä¸‹**:**
- en: '![](../Images/e4735f6270231dea8d334aa5089bd5bb.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4735f6270231dea8d334aa5089bd5bb.png)'
- en: 'Source: Sutton'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šSutton
- en: The above formula defines the value of the next state as the expected return
    E*ğ…* computed as the conditional probability of getting reward *R* at time *t*
    given state *s* at time *t*. The reward R is calculated as the sum of products
    of expected returns in successor states and gamma discounting.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å…¬å¼å®šä¹‰äº†ä¸‹ä¸€ä¸ªçŠ¶æ€çš„å€¼ï¼Œå³æœŸæœ›å›æŠ¥E*ğ…*ï¼Œå®ƒæ˜¯æ ¹æ®æ—¶é—´*t*æ—¶çš„çŠ¶æ€*s*æ¥è®¡ç®—è·å¾—å¥–åŠ±*R*çš„æ¡ä»¶æ¦‚ç‡ã€‚å¥–åŠ±Rè¢«è®¡ç®—ä¸ºåç»§çŠ¶æ€ä¸­æœŸæœ›å›æŠ¥çš„ä¹˜ç§¯å’Œï¼Œå†ä¹˜ä»¥æŠ˜æ‰£å› å­ğ›¾ã€‚
- en: To help understand this better, imagine an agent in a 3 x 3 grid-world that
    has four possible actions â€” *up, down, right, left*â€” available at each time-step.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™ä¸€ç‚¹ï¼Œå¯ä»¥æƒ³è±¡ä¸€ä¸ªæ™ºèƒ½ä½“å¤„åœ¨ä¸€ä¸ª3x3çš„ç½‘æ ¼ä¸–ç•Œä¸­ï¼Œæ¯ä¸ªæ—¶é—´æ­¥éƒ½æœ‰å››ä¸ªå¯èƒ½çš„åŠ¨ä½œâ€”â€”*ä¸Šã€ä¸‹ã€å³ã€å·¦*â€”â€”å¯ä¾›é€‰æ‹©ã€‚
- en: '![](../Images/98f7736357b69aa644d9598e3f2d1bad.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98f7736357b69aa644d9598e3f2d1bad.png)'
- en: State-space, where values represent rewards.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: çŠ¶æ€ç©ºé—´ï¼Œå…¶ä¸­çš„å€¼ä»£è¡¨å¥–åŠ±ã€‚
- en: We initialize the state-values to 0, and use the Bellman equation for the state-value
    function to optimize the state-values given the distribution of rewards in the
    grid. We use *(row, col)* indexing to identify each position the grid.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†çŠ¶æ€å€¼åˆå§‹åŒ–ä¸º0ï¼Œå¹¶ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹æ¥ä¼˜åŒ–çŠ¶æ€å€¼ï¼Œä¾æ®ç½‘æ ¼ä¸­å¥–åŠ±çš„åˆ†å¸ƒã€‚æˆ‘ä»¬ä½¿ç”¨*(è¡Œ, åˆ—)*ç´¢å¼•æ¥æ ‡è¯†ç½‘æ ¼ä¸­çš„æ¯ä¸ªä½ç½®ã€‚
- en: '![](../Images/f02f7134fbb6f758b0da456370802ae4.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f02f7134fbb6f758b0da456370802ae4.png)'
- en: Initialized state-values prior to optimization.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å‰çš„åˆå§‹åŒ–çŠ¶æ€å€¼ã€‚
- en: 'Assuming that the policy is equally distributed across each action, and with
    a discounting factor of 0.9, the state-value function for the initial state (1,1),
    would be computed in the following way:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ç­–ç•¥åœ¨æ¯ä¸ªåŠ¨ä½œä¸Šå‡åŒ€åˆ†å¸ƒï¼Œå¹¶ä¸”æŠ˜æ‰£å› å­ä¸º0.9ï¼Œåˆ™åˆå§‹çŠ¶æ€(1,1)çš„çŠ¶æ€å€¼å‡½æ•°å°†æŒ‰ç…§ä»¥ä¸‹æ–¹å¼è®¡ç®—ï¼š
- en: '![](../Images/454dc80cbd444d6dc16cd484d4889219.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/454dc80cbd444d6dc16cd484d4889219.png)'
- en: The value of state (1,1)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: çŠ¶æ€(1,1)çš„å€¼
- en: The constants in each inner sum represent the rewards which are distributed
    in the grid according to some outcome we want the agent to achieve. The inner
    sums represent the immediate reward plus the product of the discounting factor
    and the cumulative value of the next state. The ratios in the outer sum represent
    the distribution of total probability given the number of actions. Since there
    are four possible actions, we can weight the inner sums initially by equally distributed
    probabilities summing into total probability. The state-value would then be computed
    for each possible state in the state-space and iterated until the sums converge
    to stable values.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå†…å±‚æ±‚å’Œä¸­çš„å¸¸æ•°ä»£è¡¨æ ¹æ®æˆ‘ä»¬å¸Œæœ›æ™ºèƒ½ä½“å®ç°çš„æŸäº›ç»“æœï¼Œåˆ†é…åˆ°ç½‘æ ¼ä¸­çš„å¥–åŠ±ã€‚å†…å±‚æ±‚å’Œè¡¨ç¤ºå³æ—¶å¥–åŠ±ï¼ŒåŠ ä¸ŠæŠ˜æ‰£å› å­ä¸ä¸‹ä¸€ä¸ªçŠ¶æ€ç´¯ç§¯å€¼çš„ä¹˜ç§¯ã€‚å¤–å±‚æ±‚å’Œä¸­çš„æ¯”ç‡ä»£è¡¨åœ¨ç»™å®šåŠ¨ä½œæ•°é‡çš„æƒ…å†µä¸‹ï¼Œæ€»æ¦‚ç‡çš„åˆ†å¸ƒã€‚ç”±äºæœ‰å››ä¸ªå¯èƒ½çš„åŠ¨ä½œï¼Œæˆ‘ä»¬å¯ä»¥æœ€åˆé€šè¿‡å‡åŒ€åˆ†å¸ƒçš„æ¦‚ç‡åŠ æƒå†…å±‚æ±‚å’Œï¼Œä½¿å…¶æ€»å’Œç­‰äºæ€»æ¦‚ç‡ã€‚ç„¶åï¼Œå°†ä¸ºçŠ¶æ€ç©ºé—´ä¸­çš„æ¯ä¸ªå¯èƒ½çŠ¶æ€è®¡ç®—çŠ¶æ€å€¼ï¼Œå¹¶åå¤è¿­ä»£ï¼Œç›´åˆ°æ±‚å’Œæ”¶æ•›åˆ°ç¨³å®šå€¼ã€‚
- en: Action-Value Function
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ¨ä½œå€¼å‡½æ•°
- en: 'As we saw the action-value function is embedded within the state-value function
    as its second term. This means that the action-value function computes the values
    of all the possible actions in state *(s)* as the sum of the immediate reward
    obtained from the transition from *(s)* to *(sâ€™)* and the expected cumulative
    reward of the next state *(sâ€™)* given the action, given by the formula below:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼ŒåŠ¨ä½œå€¼å‡½æ•°åµŒå¥—åœ¨çŠ¶æ€å€¼å‡½æ•°ä¸­ï¼Œä½œä¸ºå…¶ç¬¬äºŒé¡¹ã€‚è¿™æ„å‘³ç€ï¼ŒåŠ¨ä½œå€¼å‡½æ•°é€šè¿‡ä»¥ä¸‹å…¬å¼è®¡ç®—çŠ¶æ€ *(s)* ä¸‹æ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„å€¼ï¼Œä½œä¸ºä» *(s)*
    åˆ° *(sâ€™)* è½¬ç§»çš„å³æ—¶å¥–åŠ±å’Œåœ¨ç»™å®šåŠ¨ä½œä¸‹ï¼Œä¸‹ä¸€çŠ¶æ€ *(sâ€™)* çš„æœŸæœ›ç´¯è®¡å¥–åŠ±çš„æ€»å’Œï¼š
- en: '![](../Images/6251ff1aecefc258f18326c452328603.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6251ff1aecefc258f18326c452328603.png)'
- en: In other words, the action value function computes the cumulative reward of
    taking action *a* in state *(s)*, where the expected return is the sum of the
    immediate state transition â€” denoted by R(s, a,sâ€™) â€” and the discounted value
    of the cumulative reward of the next state sâ€™â€” denoted by *ğ›¾âˆ‘ğ…*(aâ€™|sâ€™)Q(sâ€™,aâ€™).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼ŒåŠ¨ä½œå€¼å‡½æ•°è®¡ç®—åœ¨çŠ¶æ€ *(s)* ä¸‹é‡‡å–åŠ¨ä½œ *a* çš„ç´¯è®¡å¥–åŠ±ï¼Œå…¶ä¸­æœŸæœ›å›æŠ¥æ˜¯å³æ—¶çŠ¶æ€è½¬ç§»çš„æ€»å’Œâ€”â€”ç”¨ R(s, a, s') è¡¨ç¤ºâ€”â€”ä»¥åŠä¸‹ä¸€ä¸ªçŠ¶æ€
    s' çš„ç´¯è®¡å¥–åŠ±çš„æŠ˜æ‰£å€¼â€”â€”ç”¨ *ğ›¾âˆ‘ğ…*(aâ€™|sâ€™)Q(sâ€™,aâ€™) è¡¨ç¤ºã€‚
- en: Another notation for formulating the action-value function is in terms of the
    expected return E given state and action pair (s, a) when following the optimal
    policy *ğ…:*
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§è¡¨è¾¾åŠ¨ä½œå€¼å‡½æ•°çš„ç¬¦å·æ˜¯åŸºäºç»™å®šçŠ¶æ€å’ŒåŠ¨ä½œå¯¹ (s, a) çš„æœŸæœ›å›æŠ¥ Eï¼Œå½“éµå¾ªæœ€ä¼˜ç­–ç•¥ *ğ…* æ—¶ï¼š
- en: '![](../Images/6733812d0407777b7b25310fab2cbca1.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6733812d0407777b7b25310fab2cbca1.png)'
- en: The state-value functions and the action-value functions are related in the
    sense that the state-value function can be given by the policy and the action-value
    function Q(s,a).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: çŠ¶æ€å€¼å‡½æ•°å’ŒåŠ¨ä½œå€¼å‡½æ•°ä¹‹é—´æœ‰å…³ç³»ï¼Œå…·ä½“è€Œè¨€ï¼ŒçŠ¶æ€å€¼å‡½æ•°å¯ä»¥é€šè¿‡ç­–ç•¥å’ŒåŠ¨ä½œå€¼å‡½æ•° Q(s,a) ç»™å‡ºã€‚
- en: '***Therefore, each function contains itself as a parameter, albeit computing
    the successor transition state, as evinced by the formulas above. The formula
    for V(s) contains V(sâ€™) and the formula for Q(s, a) contains Q(sâ€™,a).***'
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***å› æ­¤ï¼Œæ¯ä¸ªå‡½æ•°éƒ½å°†è‡ªèº«ä½œä¸ºå‚æ•°ï¼Œå°½ç®¡è®¡ç®—çš„æ˜¯åç»§è¿‡æ¸¡çŠ¶æ€ï¼Œå¦‚ä¸Šé¢çš„å…¬å¼æ‰€ç¤ºã€‚V(s) çš„å…¬å¼åŒ…å« V(s'')ï¼ŒQ(s, a) çš„å…¬å¼åŒ…å« Q(s'',
    a)ã€‚***'
- en: ''
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Put differently, they contain each other within their parameters: the value
    of the state V(s) depends on the value of successor states computed through Q(s,a)
    and the value of the action Q(s,a) depends on the value of the successor state
    computed through V(sâ€™).***'
  id: totrans-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***æ¢å¥è¯è¯´ï¼Œå®ƒä»¬åœ¨å„è‡ªçš„å‚æ•°ä¸­åŒ…å«äº†å½¼æ­¤ï¼šçŠ¶æ€å€¼ V(s) ä¾èµ–äºé€šè¿‡ Q(s,a) è®¡ç®—çš„åç»§çŠ¶æ€çš„å€¼ï¼Œè€ŒåŠ¨ä½œå€¼ Q(s,a) ä¾èµ–äºé€šè¿‡ V(s'')
    è®¡ç®—çš„åç»§çŠ¶æ€çš„å€¼ã€‚***'
- en: '![](../Images/8111aa8b4fa8505f9f85a8de520c4e41.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8111aa8b4fa8505f9f85a8de520c4e41.png)'
- en: Backup diagram for the state-value function. S represents the state, *ğ… the
    policy, black dots each available action, and arrows action-reward pairs transitioning
    to next state sâ€™. Source:* [https://goodboychan.github.io/reinforcement_learning/2020/06/06/05-Policy-evaluation.html](https://goodboychan.github.io/reinforcement_learning/2020/06/06/05-Policy-evaluation.html)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: çŠ¶æ€å€¼å‡½æ•°çš„å¤‡ä»½å›¾ã€‚S ä»£è¡¨çŠ¶æ€ï¼Œ*ğ… ä»£è¡¨ç­–ç•¥*ï¼Œé»‘ç‚¹è¡¨ç¤ºæ¯ä¸ªå¯ç”¨åŠ¨ä½œï¼Œç®­å¤´è¡¨ç¤ºåŠ¨ä½œ-å¥–åŠ±å¯¹ï¼ŒæŒ‡å‘ä¸‹ä¸€ä¸ªçŠ¶æ€ s'ã€‚æ¥æºï¼š*[https://goodboychan.github.io/reinforcement_learning/2020/06/06/05-Policy-evaluation.html](https://goodboychan.github.io/reinforcement_learning/2020/06/06/05-Policy-evaluation.html)*
- en: 'As such, the action-value function and the state-value function are recursively
    related: the value of the action-state pairs determine the value of the state,
    which conversely determines the value of the action.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒåŠ¨ä½œå€¼å‡½æ•°å’ŒçŠ¶æ€å€¼å‡½æ•°æ˜¯é€’å½’ç›¸å…³çš„ï¼šåŠ¨ä½œ-çŠ¶æ€å¯¹çš„å€¼å†³å®šäº†çŠ¶æ€çš„å€¼ï¼Œè€ŒçŠ¶æ€çš„å€¼åˆå†³å®šäº†åŠ¨ä½œçš„å€¼ã€‚
- en: The state-value function takes as its prior the state, and yields an expected
    value E. The action value function takes as its prior state and action pairs,
    to compute the reward, the expected cumulative return E.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: çŠ¶æ€å€¼å‡½æ•°ä»¥çŠ¶æ€ä¸ºå…ˆéªŒï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæœŸæœ›å€¼ Eã€‚åŠ¨ä½œå€¼å‡½æ•°ä»¥çŠ¶æ€å’ŒåŠ¨ä½œå¯¹ä¸ºå…ˆéªŒï¼Œç”¨æ¥è®¡ç®—å¥–åŠ±å’ŒæœŸæœ›çš„ç´¯è®¡å›æŠ¥ Eã€‚
- en: 'The **Bellman Optimality Equations** therefore express the recursive iteration
    of the state-value and action-value functions until they converge on optimal values.
    The Bellman Equation for the state-value function is expressed below:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹**å› æ­¤è¡¨è¾¾äº†çŠ¶æ€å€¼å‡½æ•°å’ŒåŠ¨ä½œå€¼å‡½æ•°çš„é€’å½’è¿­ä»£ï¼Œç›´åˆ°å®ƒä»¬æ”¶æ•›åˆ°æœ€ä¼˜å€¼ã€‚çŠ¶æ€å€¼å‡½æ•°çš„è´å°”æ›¼æ–¹ç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š'
- en: '![](../Images/681778dfe0305d39e868e776e33e64b3.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/681778dfe0305d39e868e776e33e64b3.png)'
- en: where the value of the current state is defined as the maximum reward of any
    possible action computed as the reward for taking action **a** at state (**s)**
    and the product of the value of the next action sâ€™ and its discount factor gamma.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‰çŠ¶æ€çš„å€¼è¢«å®šä¹‰ä¸ºä»»ä½•å¯èƒ½è¡ŒåŠ¨çš„æœ€å¤§å¥–åŠ±ï¼Œè¯¥å¥–åŠ±æ˜¯é‡‡å–åŠ¨ä½œ**a**åœ¨çŠ¶æ€(**s**)ä¸‹çš„å¥–åŠ±ï¼Œå¹¶ä¸”æ˜¯ä¸‹ä¸€ä¸ªåŠ¨ä½œsâ€™çš„å€¼ä¸å…¶æŠ˜æ‰£å› å­gammaçš„ä¹˜ç§¯ã€‚
- en: The Bellman equation averages each all possible actions from the current state
    and weights them according to their probability of occurring.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è´å°”æ›¼æ–¹ç¨‹å¹³å‡äº†å½“å‰çŠ¶æ€ä¸‹æ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„æœŸæœ›ï¼Œå¹¶æ ¹æ®å…¶å‘ç”Ÿçš„æ¦‚ç‡å¯¹å®ƒä»¬è¿›è¡ŒåŠ æƒã€‚
- en: '![](../Images/2350ca43f07a422e6d76ec7eb66ccdb2.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2350ca43f07a422e6d76ec7eb66ccdb2.png)'
- en: 'Model Free Methods: Monte Carlo & Temporal Difference'
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ— æ¨¡å‹æ–¹æ³•ï¼šè’™ç‰¹å¡æ´›ä¸æ—¶åºå·®åˆ†
- en: The above example describes a deterministic model where the transition dynamics
    are known and can thus be perfectly computed. This is because we have complete
    knowledge of the environment.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ä¾‹å­æè¿°äº†ä¸€ä¸ªç¡®å®šæ€§æ¨¡å‹ï¼Œå…¶ä¸­è½¬æ¢åŠ¨æ€æ˜¯å·²çŸ¥çš„ï¼Œå› æ­¤å¯ä»¥å®Œç¾è®¡ç®—ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬å¯¹ç¯å¢ƒæœ‰å®Œå…¨çš„äº†è§£ã€‚
- en: However, for most tasks we donâ€™t have complete knowledge of the environment.
    In lieu of this information, we cannot proceed with deterministic transition dynamics
    precisely because we cannot solve the dynamic programming equations. To overcome
    this problem, we can use techniques that borrow from statistics by inferring the
    state of the environment from a sample.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¯¹äºå¤§å¤šæ•°ä»»åŠ¡ï¼Œæˆ‘ä»¬æ— æ³•å®Œå…¨äº†è§£ç¯å¢ƒã€‚åœ¨ç¼ºä¹è¿™äº›ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸èƒ½ç²¾ç¡®åœ°ä½¿ç”¨ç¡®å®šæ€§çš„è½¬æ¢åŠ¨æ€ï¼Œå› ä¸ºæˆ‘ä»¬æ— æ³•æ±‚è§£åŠ¨æ€è§„åˆ’æ–¹ç¨‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å€Ÿé‰´ç»Ÿè®¡å­¦çš„æŠ€æœ¯ï¼Œé€šè¿‡ä»æ ·æœ¬ä¸­æ¨æ–­ç¯å¢ƒçš„çŠ¶æ€ã€‚
- en: In Monte Carlo methods, we approximate expected returns with the average of
    sample returns. As the sample approaches infinity, the average returns converge
    to the true value of expected returns. We do this by letting the agent run through
    an entire episode until termination before computing the value function. We then
    take *N* number of episode samples and use the mean to approximate the expected
    value of the target state. Now, as you might be already wondering, how an episode
    is defined varies with the task and purpose of the model. For example, in a game
    of chess we can define an episode as a run through an entire game or an arbitrary
    series of steps.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è’™ç‰¹å¡æ´›æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ ·æœ¬å›æŠ¥çš„å¹³å‡å€¼æ¥é€¼è¿‘æœŸæœ›å›æŠ¥ã€‚éšç€æ ·æœ¬æ•°è¶‹è¿‘äºæ— é™ï¼Œå¹³å‡å›æŠ¥å°†æ”¶æ•›äºæœŸæœ›å›æŠ¥çš„çœŸå®å€¼ã€‚æˆ‘ä»¬é€šè¿‡è®©æ™ºèƒ½ä½“å®Œæˆä¸€ä¸ªå®Œæ•´çš„å›åˆç›´è‡³ç»“æŸï¼Œç„¶åè®¡ç®—ä»·å€¼å‡½æ•°æ¥å®ç°è¿™ä¸€ç‚¹ã€‚æ¥ç€ï¼Œæˆ‘ä»¬é‡‡æ ·*N*ä¸ªå›åˆï¼Œå¹¶ä½¿ç”¨å‡å€¼æ¥é€¼è¿‘ç›®æ ‡çŠ¶æ€çš„æœŸæœ›å€¼ã€‚ç°åœ¨ï¼Œæ­£å¦‚ä½ å¯èƒ½å·²ç»æƒ³åˆ°äº†ï¼Œå›åˆçš„å®šä¹‰ä¼šæ ¹æ®ä»»åŠ¡å’Œæ¨¡å‹çš„ç›®çš„æœ‰æ‰€ä¸åŒã€‚ä¾‹å¦‚ï¼Œåœ¨å›½é™…è±¡æ£‹æ¸¸æˆä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸€ä¸ªå›åˆå®šä¹‰ä¸ºå®Œæˆä¸€æ•´å±€æ£‹æˆ–è€…æ˜¯ä¸€ä¸ªä»»æ„çš„æ­¥éª¤åºåˆ—ã€‚
- en: 'We can write the MC update rule as the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†è’™ç‰¹å¡æ´›æ›´æ–°è§„åˆ™å†™æˆå¦‚ä¸‹ï¼š
- en: '![](../Images/31148e295cc8a431856d2b6f15cecfba.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31148e295cc8a431856d2b6f15cecfba.png)'
- en: Where V(s) n+1 denotes the value of the next episode, S(s)n denotes the cumulative
    value of the state and G the value of the reward. We add the cumulative reward
    G to the state value and divide by the number of episodes or samples.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­V(s) n+1è¡¨ç¤ºä¸‹ä¸€ä¸ªå›åˆçš„å€¼ï¼ŒS(s)nè¡¨ç¤ºçŠ¶æ€çš„ç´¯è®¡å€¼ï¼ŒGè¡¨ç¤ºå¥–åŠ±çš„å€¼ã€‚æˆ‘ä»¬å°†ç´¯è®¡å¥–åŠ±Gæ·»åŠ åˆ°çŠ¶æ€å€¼ä¸­ï¼Œå¹¶é™¤ä»¥å›åˆæ•°æˆ–æ ·æœ¬æ•°ã€‚
- en: 'We can algebraically rearrange the MC update rule to:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»£æ•°æ–¹å¼é‡æ–°æ’åˆ—è’™ç‰¹å¡æ´›æ›´æ–°è§„åˆ™ä¸ºï¼š
- en: '![](../Images/813ba0814868e265d05b2830a0822db0.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/813ba0814868e265d05b2830a0822db0.png)'
- en: Unlike Monte Carlo methods, where we evaluate the value function only after
    each episode, with **Temporal Difference** (TD) we evaluate the state value function
    after each time-step or increment. Since we start with no information about the
    environment, we have to initialize the values of V(s) to 0 or some other values,
    which will subsequently be updated with every time step.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è’™ç‰¹å¡æ´›æ–¹æ³•ä¸åŒï¼Œåœ¨è’™ç‰¹å¡æ´›æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä»…åœ¨æ¯ä¸€å›åˆç»“æŸåè¯„ä¼°ä»·å€¼å‡½æ•°ï¼Œè€Œåœ¨**æ—¶åºå·®åˆ†**ï¼ˆTDï¼‰æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸€æ­¥æˆ–å¢é‡åè¯„ä¼°çŠ¶æ€ä»·å€¼å‡½æ•°ã€‚ç”±äºæˆ‘ä»¬ä¸€å¼€å§‹å¯¹ç¯å¢ƒæ²¡æœ‰ä»»ä½•ä¿¡æ¯ï¼Œå› æ­¤éœ€è¦å°†V(s)çš„åˆå§‹å€¼è®¾ç½®ä¸º0æˆ–å…¶ä»–å€¼ï¼Œè¿™äº›å€¼å°†åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¹‹åæ›´æ–°ã€‚
- en: 'We compute the value of the state in TD with two steps. First we compute the
    error of the step and next we use an update rule to change the value of the state.
    The error is given by the following difference formula:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡ä¸¤æ­¥è®¡ç®—TDä¸­çš„çŠ¶æ€å€¼ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¡ç®—è¿™ä¸€æ—¶é—´æ­¥çš„è¯¯å·®ï¼Œç„¶åä½¿ç”¨æ›´æ–°è§„åˆ™æ¥æ”¹å˜çŠ¶æ€çš„å€¼ã€‚è¯¯å·®é€šè¿‡ä»¥ä¸‹å·®å€¼å…¬å¼ç»™å‡ºï¼š
- en: '![](../Images/852bb36519b46bf2806382afa7e1cd25.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/852bb36519b46bf2806382afa7e1cd25.png)'
- en: The error formula for TD time-step.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶åºå·®åˆ†æ—¶é—´æ­¥çš„è¯¯å·®å…¬å¼ã€‚
- en: Where, ğœ¹t stands for the error, R(t+1) the reward from the action, V(S t+1)
    the estimated value of the next state, and V(S) the value of the current state.
    The fact that TD uses the estimated value of the next state to evaluate the current
    state is called **bootsrapping**. In effect, we subtract the value of the current
    state from the sum of the reward of the action and the product of the discounting
    factor and the value of the next state. This enables an immediate update of the
    value of the state with every time-step.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œğœ¹tä»£è¡¨è¯¯å·®ï¼ŒR(t+1)æ˜¯åŠ¨ä½œå¸¦æ¥çš„å¥–åŠ±ï¼ŒV(S t+1)æ˜¯ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä¼°è®¡å€¼ï¼ŒV(S)æ˜¯å½“å‰çŠ¶æ€çš„å€¼ã€‚TDä½¿ç”¨ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä¼°è®¡å€¼æ¥è¯„ä¼°å½“å‰çŠ¶æ€ï¼Œè¿™ç§æ–¹æ³•ç§°ä¸º**è‡ªå¼•å¯¼ï¼ˆbootstrappingï¼‰**ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬ä»åŠ¨ä½œçš„å¥–åŠ±å’ŒæŠ˜æ‰£å› å­ä¸ä¸‹ä¸€ä¸ªçŠ¶æ€å€¼çš„ä¹˜ç§¯ä¸­å‡å»å½“å‰çŠ¶æ€çš„å€¼ã€‚è¿™ä½¿å¾—æ¯ä¸ªæ—¶é—´æ­¥éƒ½èƒ½ç«‹å³æ›´æ–°å½“å‰çŠ¶æ€çš„å€¼ã€‚
- en: 'By adding the observed discrepancy between the expected and observed reward
    ğœ¹ times ğ›¼ (the learning rate) we close the discrepancy between observation and
    expectation:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†é¢„æœŸå¥–åŠ±ä¸è§‚å¯Ÿåˆ°çš„å¥–åŠ±ä¹‹é—´çš„å·®å¼‚ğœ¹ä¹˜ä»¥ğ›¼ï¼ˆå­¦ä¹ ç‡ï¼‰ï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†è§‚å¯Ÿä¸é¢„æœŸä¹‹é—´çš„å·®å¼‚ï¼š
- en: '![](../Images/552a7d478c5afd3e49ba451b0df74af5.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/552a7d478c5afd3e49ba451b0df74af5.png)'
- en: The TD update rule for the value function.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ä»·å€¼å‡½æ•°çš„TDæ›´æ–°è§„åˆ™ã€‚
- en: The role of ğ›¼ determines the degree to which the TD algorithm learns, where
    ğ›¼ is a real positive number. Typically, ğ›¼ is set to values like [0.1, 0.01, 0.001].
    A higher value ğ›¼ ensures that the updates are more aggressive, whereas a lower
    value ensures more conservative updates. The value of ğ›¼ affects the exploration-exploitation
    trade-off, where higher ğ›¼ leans on exploration and lower ğ›¼ leans on exploitation.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ğ›¼çš„ä½œç”¨å†³å®šäº†TDç®—æ³•å­¦ä¹ çš„ç¨‹åº¦ï¼Œå…¶ä¸­ğ›¼æ˜¯ä¸€ä¸ªå®æ•°æ­£æ•°ã€‚é€šå¸¸ï¼Œğ›¼ä¼šè®¾ç½®ä¸º[0.1ï¼Œ0.01ï¼Œ0.001]ç­‰å€¼ã€‚è¾ƒé«˜çš„ğ›¼ç¡®ä¿æ›´æ–°æ›´åŠ æ¿€è¿›ï¼Œè€Œè¾ƒä½çš„ğ›¼ç¡®ä¿æ›´æ–°æ›´åŠ ä¿å®ˆã€‚ğ›¼çš„å€¼ä¼šå½±å“æ¢ç´¢ä¸åˆ©ç”¨çš„æƒè¡¡ï¼Œè¾ƒé«˜çš„ğ›¼å€¾å‘äºæ¢ç´¢ï¼Œè€Œè¾ƒä½çš„ğ›¼åˆ™å€¾å‘äºåˆ©ç”¨ã€‚
- en: While both MC and TD methods proceed blindly without any prior knowledge of
    the environment, the merit of Temporal Difference is that it computes online updates
    at every time-step and the merit of Monte Carlo methods is unbiased estimation
    due to relying on sampling alone to estimate the value. A drawback of TD methods
    includes high bias, whereas a drawback of MC methods include overlooking important
    updates, and thereby higher variance. This suggests that the optimum between the
    two learning strategies must exist somewhere in between.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶MCå’ŒTDæ–¹æ³•éƒ½åœ¨æ²¡æœ‰ä»»ä½•å…ˆéªŒç¯å¢ƒçŸ¥è¯†çš„æƒ…å†µä¸‹ç›²ç›®è¿›è¡Œï¼Œä½†æ—¶é—´å·®åˆ†æ³•ï¼ˆTemporal Difference, TDï¼‰çš„æ–¹æ³•ä¼˜ç‚¹åœ¨äºå®ƒåœ¨æ¯ä¸ªæ—¶é—´æ­¥éƒ½è¿›è¡Œåœ¨çº¿æ›´æ–°ï¼Œè€Œè’™ç‰¹å¡ç½—æ–¹æ³•ï¼ˆMonte
    Carlo, MCï¼‰çš„æ–¹æ³•ä¼˜ç‚¹åœ¨äºç”±äºä»…ä¾èµ–é‡‡æ ·æ¥ä¼°è®¡å€¼ï¼Œå®ƒçš„ä¼°è®¡æ˜¯æ— åçš„ã€‚TDæ–¹æ³•çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯é«˜åå·®ï¼Œè€ŒMCæ–¹æ³•çš„ç¼ºç‚¹æ˜¯å¿½è§†äº†é‡è¦çš„æ›´æ–°ï¼Œä»è€Œå¯¼è‡´è¾ƒé«˜çš„æ–¹å·®ã€‚è¿™è¡¨æ˜ï¼Œä¸¤ç§å­¦ä¹ ç­–ç•¥ä¹‹é—´ä¸€å®šå­˜åœ¨ä¸€ä¸ªæœ€ä¼˜è§£ã€‚
- en: The TD approach can be optimized by changing the single-step evaluation strategy
    to n-steps. As we will see, doing this enables compromising between TD and MC.
    When we evaluate the state value every n-steps, we do so by estimating n-steps
    into the future instead of after every step.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†å•æ­¥è¯„ä¼°ç­–ç•¥æ”¹ä¸ºnæ­¥è¯„ä¼°ï¼ŒTDæ–¹æ³•å¯ä»¥å¾—åˆ°ä¼˜åŒ–ã€‚æ­£å¦‚æˆ‘ä»¬å°†è¦çœ‹åˆ°çš„ï¼Œè¿™æ ·åšå¯ä»¥åœ¨TDå’ŒMCä¹‹é—´è¾¾æˆä¸€ä¸ªæŠ˜è¡·ã€‚å½“æˆ‘ä»¬æ¯næ­¥è¯„ä¼°ä¸€æ¬¡çŠ¶æ€å€¼æ—¶ï¼Œæˆ‘ä»¬æ˜¯é€šè¿‡ä¼°è®¡æœªæ¥çš„næ­¥æ¥è¿›è¡Œè¯„ä¼°ï¼Œè€Œä¸æ˜¯åœ¨æ¯ä¸€æ­¥åè¿›è¡Œè¯„ä¼°ã€‚
- en: 'A modified approach to n-step TD is TD(ğ€). TD(ğ€) methods use a parameter called
    ***eligibility traces*** to credit state-action pairs that occurred in the past.
    Instead of estimating n-steps into the future, eligibility traces assign credit
    to state-action pairs over multiple TD steps. Eligibility traces enable past state-action
    pairs to receive credit for contributing to observed-reward transitions. Eligibility
    traces are represented as vectors or matrices associated with each state-action
    pair. The eligibility trace for a time step is computed recursively as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹næ­¥TDçš„ä¿®æ”¹æ–¹æ³•æ˜¯TD(ğ€)ã€‚TD(ğ€)æ–¹æ³•ä½¿ç”¨ä¸€ä¸ªå«åš***èµ„æ ¼è¿¹ï¼ˆeligibility tracesï¼‰***çš„å‚æ•°æ¥å¯¹è¿‡å»å‘ç”Ÿçš„çŠ¶æ€-åŠ¨ä½œå¯¹è¿›è¡ŒåŠ æƒã€‚ä¸å…¶ä¼°è®¡æœªæ¥çš„næ­¥ï¼Œèµ„æ ¼è¿¹é€šè¿‡å¤šä¸ªTDæ­¥éª¤æ¥å¯¹çŠ¶æ€-åŠ¨ä½œå¯¹è¿›è¡ŒåŠ æƒã€‚èµ„æ ¼è¿¹ä½¿å¾—è¿‡å»çš„çŠ¶æ€-åŠ¨ä½œå¯¹èƒ½å¤Ÿå› å…¶å¯¹è§‚å¯Ÿåˆ°çš„å¥–åŠ±è¿‡æ¸¡ä½œå‡ºè´¡çŒ®è€Œè·å¾—å¥–åŠ±ã€‚èµ„æ ¼è¿¹è¡¨ç¤ºä¸ºä¸æ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹ç›¸å…³çš„å‘é‡æˆ–çŸ©é˜µã€‚æ—¶é—´æ­¥çš„èµ„æ ¼è¿¹é€šè¿‡é€’å½’æ–¹å¼è®¡ç®—å¦‚ä¸‹ï¼š
- en: '![](../Images/d31d5d5fee276d703ea2cef7923e17f0.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d31d5d5fee276d703ea2cef7923e17f0.png)'
- en: 'Where the lambda ğ€ parameter controls the degree of bootstrapping. When ğ€ =1,
    bootstrapping is eliminated and the update rule reduces to Monte Carlo. When ğ€
    = 0, it reduces to a TD time-step with bootstrapping termed TD(0). *TD(ğ€) generalizes
    TD and MC as a continuum where TD(0) denotes single step TD and TD(1) denotes
    the limit of extending TD to âˆ steps, which reduces to MC.* As you can see from
    the formula, the eligibility trace parameter is computed recursively, wherein
    the value of the eligibility trace for the next time step takes as input the eligibility
    trace from the previous step. When E(s) = 0, bootstrapping is eliminated. The
    TD(ğ€) update rule is computed the same as the TD and MC update rule except by
    multiplying the eligibility trace to the error as shown below:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œlambda ğ€ å‚æ•°æ§åˆ¶è‡ªä¸¾çš„ç¨‹åº¦ã€‚å½“ ğ€ = 1 æ—¶ï¼Œæ¶ˆé™¤äº†è‡ªä¸¾ï¼Œæ›´æ–°è§„åˆ™é€€åŒ–ä¸ºè’™ç‰¹å¡ç½—æ–¹æ³•ã€‚å½“ ğ€ = 0 æ—¶ï¼Œå®ƒé€€åŒ–ä¸ºå¸¦æœ‰è‡ªä¸¾çš„ TD æ—¶é—´æ­¥ï¼Œç§°ä¸º
    TD(0)ã€‚*TD(ğ€) å°† TD å’Œè’™ç‰¹å¡ç½—æ–¹æ³•è¿›è¡Œäº†æ¨å¹¿ï¼Œå½¢æˆä¸€ä¸ªè¿ç»­ä½“ï¼Œå…¶ä¸­ TD(0) è¡¨ç¤ºå•æ­¥ TDï¼ŒTD(1) è¡¨ç¤ºå°† TD æ‰©å±•åˆ°æ— é™æ­¥æ•°çš„æé™ï¼Œè¿™ä¸€æé™ä¼šé€€åŒ–ä¸ºè’™ç‰¹å¡ç½—æ–¹æ³•ã€‚*
    ä»å…¬å¼ä¸­å¯ä»¥çœ‹å‡ºï¼Œèµ„æ ¼è¿¹å‚æ•°æ˜¯é€’å½’è®¡ç®—çš„ï¼Œå…¶ä¸­ä¸‹ä¸€æ—¶é—´æ­¥çš„èµ„æ ¼è¿¹å€¼ä»¥å‰ä¸€æ—¶é—´æ­¥çš„èµ„æ ¼è¿¹ä¸ºè¾“å…¥ã€‚å½“ E(s) = 0 æ—¶ï¼Œè‡ªä¸¾è¢«æ¶ˆé™¤ã€‚TD(ğ€) æ›´æ–°è§„åˆ™çš„è®¡ç®—æ–¹å¼ä¸
    TD å’Œè’™ç‰¹å¡ç½—æ–¹æ³•ç›¸åŒï¼Œåªæ˜¯å°†èµ„æ ¼è¿¹ä¹˜ä»¥è¯¯å·®ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/f27702af2c30a52497b10c09dfb7c96d.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f27702af2c30a52497b10c09dfb7c96d.png)'
- en: Augmenting Reinforcement Learning with ANNs
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œå¢å¼ºå¼ºåŒ–å­¦ä¹ 
- en: Whether model-based or model-free, RL algorithms encounter scaling problems
    because of the curse of dimensionality, have trouble generalizing across different
    types of environments, and suffer from sample inefficiency.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºæ˜¯åŸºäºæ¨¡å‹çš„è¿˜æ˜¯æ— æ¨¡å‹çš„ï¼Œå¼ºåŒ–å­¦ä¹ ç®—æ³•éƒ½ä¼šé‡åˆ°ç”±äºç»´åº¦ç¾éš¾è€Œäº§ç”Ÿçš„æ‰©å±•é—®é¢˜ï¼Œéš¾ä»¥åœ¨ä¸åŒç±»å‹çš„ç¯å¢ƒä¹‹é—´è¿›è¡Œæ³›åŒ–ï¼Œå¹¶ä¸”é­é‡é‡‡æ ·ä½æ•ˆçš„é—®é¢˜ã€‚
- en: Artificial Neural Networks (ANNs) provide a powerful method of rectifying some
    of the limits inherent within the RL architecture. In particular, ANNs improve
    sampling efficiency, environment generalization, and scaling problems caused by
    the curse of dimensionality. They reduce sample inefficiency through superior
    generalization capacity by virtue of the fact that they learn a general function
    from the data. This also enables them to scale better since the number of hidden
    layers and neurons per hidden layer can be increased. Too many hidden layers and
    neurons, however, can also lead to computational scaling problems (the curse of
    dimensionality is inescapable beyond certain ranges). They are further beset by
    the problem of the *non-stationarity of target states*, since traditionally ANNs
    require the ground truth (in the case of RL this amounts to *expected return*)
    to be set in advance, while RL algorithms find the optimal state through an update
    function, whether on-policy or off-policy.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNsï¼‰ä¸ºè§£å†³å¼ºåŒ–å­¦ä¹ æ¶æ„ä¸­ä¸€äº›å›ºæœ‰çš„å±€é™æ€§æä¾›äº†å¼ºæœ‰åŠ›çš„æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼ŒANNs æ”¹å–„äº†é‡‡æ ·æ•ˆç‡ã€ç¯å¢ƒæ³›åŒ–èƒ½åŠ›ä»¥åŠç”±äºç»´åº¦ç¾éš¾è€Œäº§ç”Ÿçš„æ‰©å±•é—®é¢˜ã€‚ç”±äºå®ƒä»¬é€šè¿‡ä»æ•°æ®ä¸­å­¦ä¹ ä¸€ä¸ªé€šç”¨å‡½æ•°ï¼ŒANNs
    é€šè¿‡å“è¶Šçš„æ³›åŒ–èƒ½åŠ›å‡å°‘äº†é‡‡æ ·ä½æ•ˆæ€§ã€‚è¿™è¿˜ä½¿å¾—å®ƒä»¬èƒ½å¤Ÿæ›´å¥½åœ°æ‰©å±•ï¼Œå› ä¸ºå¯ä»¥å¢åŠ éšè—å±‚çš„æ•°é‡å’Œæ¯ä¸ªéšè—å±‚ä¸­çš„ç¥ç»å…ƒæ•°é‡ã€‚ç„¶è€Œï¼Œéšè—å±‚å’Œç¥ç»å…ƒè¿‡å¤šä¹Ÿå¯èƒ½å¯¼è‡´è®¡ç®—æ‰©å±•é—®é¢˜ï¼ˆç»´åº¦ç¾éš¾åœ¨æŸäº›èŒƒå›´å†…æ˜¯æ— æ³•é¿å…çš„ï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒä»¬è¿˜é¢ä¸´ç€*ç›®æ ‡çŠ¶æ€çš„éå¹³ç¨³æ€§*é—®é¢˜ï¼Œå› ä¸ºä¼ ç»Ÿçš„äººå·¥ç¥ç»ç½‘ç»œéœ€è¦é¢„å…ˆè®¾å®šçœŸå®å€¼ï¼ˆåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œè¿™ç›¸å½“äº*æœŸæœ›å›æŠ¥*ï¼‰ï¼Œè€Œå¼ºåŒ–å­¦ä¹ ç®—æ³•åˆ™é€šè¿‡æ›´æ–°å‡½æ•°æ‰¾åˆ°æœ€ä¼˜çŠ¶æ€ï¼Œæ— è®ºæ˜¯åŸºäºç­–ç•¥è¿˜æ˜¯è„±ç¦»ç­–ç•¥ã€‚
- en: Unlike traditional RL algorithms which rely on probabilistic transition rules,
    the application of ANNs to reinforcement learning uses function approximation
    to compute the state and state-action values. While any number of function approximation
    methods can be applied such as linear approximation and tile-coding, artificial
    neural networks constitute the most powerful technique due to their generalization
    power that leverages nonlinear function approximation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¾èµ–äºæ¦‚ç‡è½¬ç§»è§„åˆ™ä¸åŒï¼Œäººå·¥ç¥ç»ç½‘ç»œï¼ˆANNsï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨ä½¿ç”¨å‡½æ•°é€¼è¿‘æ¥è®¡ç®—çŠ¶æ€å’Œå€¼åŠ¨ä½œå€¼ã€‚è™½ç„¶å¯ä»¥åº”ç”¨å¤šç§å‡½æ•°é€¼è¿‘æ–¹æ³•ï¼Œå¦‚çº¿æ€§é€¼è¿‘å’Œæ ¼ç¼–ç ï¼Œä½†äººå·¥ç¥ç»ç½‘ç»œç”±äºå…¶é€šè¿‡éçº¿æ€§å‡½æ•°é€¼è¿‘çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆä¸ºæœ€å¼ºå¤§çš„æŠ€æœ¯ã€‚
- en: 'Letâ€™s look at two approaches that involve applying artificial neural networks
    to reinforcement learning: **Deep Q Learning (DQN)** and **Deep Temporal Difference
    with eligibility traces (TD(ğ€))**. Since we donâ€™t know the target values in advance,
    MC or TD are used to create an estimate of the target state: the expected return.
    This is then used as the target value to be approximated by the function (really,
    the *gradient* which is the partial derivative of the error of the entire network
    with respect to the network parameter ğœƒ). ANNs approximate the target value by
    computing the error between the target estimate and the output and then computing
    the error through backpropagation and reducing it through an optimization algorithm.
    The most common optimization algorithm is a variation of gradient descent such
    as **stochastic gradient descent**.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ä¸¤ç§å°†äººå·¥ç¥ç»ç½‘ç»œåº”ç”¨äºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼š**æ·±åº¦Qå­¦ä¹ ï¼ˆDQNï¼‰**å’Œ**å¸¦æœ‰èµ„æ ¼è¿¹çš„æ·±åº¦æ—¶é—´å·®åˆ†å­¦ä¹ ï¼ˆTD(ğ€)ï¼‰**ã€‚ç”±äºæˆ‘ä»¬æ— æ³•æå‰çŸ¥é“ç›®æ ‡å€¼ï¼Œå› æ­¤ä½¿ç”¨MCæˆ–TDæ¥åˆ›å»ºç›®æ ‡çŠ¶æ€çš„ä¼°è®¡å€¼ï¼šæœŸæœ›å›æŠ¥ã€‚ç„¶åå°†å…¶ä½œä¸ºç›®æ ‡å€¼ï¼Œé€šè¿‡å‡½æ•°ï¼ˆå®é™…ä¸Šæ˜¯è¯¯å·®çš„åå¯¼æ•°ï¼Œå³æ•´ä¸ªç½‘ç»œè¯¯å·®å¯¹ç½‘ç»œå‚æ•°ğœƒçš„åå¯¼æ•°ï¼‰æ¥é€¼è¿‘ã€‚ANNé€šè¿‡è®¡ç®—ç›®æ ‡ä¼°è®¡å€¼ä¸è¾“å‡ºä¹‹é—´çš„è¯¯å·®ï¼Œæ¥ç€é€šè¿‡åå‘ä¼ æ’­è®¡ç®—è¯¯å·®ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–ç®—æ³•å‡å°‘è¯¥è¯¯å·®æ¥é€¼è¿‘ç›®æ ‡å€¼ã€‚æœ€å¸¸è§çš„ä¼˜åŒ–ç®—æ³•æ˜¯æ¢¯åº¦ä¸‹é™çš„å˜ç§ï¼Œä¾‹å¦‚**éšæœºæ¢¯åº¦ä¸‹é™**ã€‚
- en: '![](../Images/c5e7999c07ba8f2e019d9e979f46bbc1.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5e7999c07ba8f2e019d9e979f46bbc1.png)'
- en: In DQN the artificial neural network takes a state vector as input and outputs
    an action vector, where each value represents the action q-value.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨DQNä¸­ï¼Œäººå·¥ç¥ç»ç½‘ç»œä»¥çŠ¶æ€å‘é‡ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºä¸€ä¸ªåŠ¨ä½œå‘é‡ï¼Œå…¶ä¸­æ¯ä¸ªå€¼ä»£è¡¨åŠ¨ä½œçš„qå€¼ã€‚
- en: '**Off-Policy DQN**'
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç¦»ç­–ç•¥DQN**'
- en: '**Q-Learning** is an off-policy version of **SARSA (States, Actions, Rewards,
    Statesâ€™, Actionsâ€™)**, where the next state-action pair **Q(sâ€™, aâ€™)** is estimated
    by selecting the maximum estimated value of the next state. In other words, Q-Learning
    selects the maximum value of **Q(sâ€™,aâ€™)** across actions available in the next
    state sâ€™. This means that it doesnâ€™t use the policy ğ›‘ to learn Q(sâ€™,aâ€™). SARSA,
    on the other hand, is an on-policy method that selects an action from the previous
    action taken and an estimate of the next state-action pair, Q(sâ€™,aâ€™). This means
    that it uses the policy ğ›‘, namely the probability of an action given the state,
    to learn the Q-function.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**Qå­¦ä¹ **æ˜¯**SARSAï¼ˆçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€çŠ¶æ€''ã€åŠ¨ä½œ''ï¼‰**çš„ç¦»ç­–ç•¥ç‰ˆæœ¬ï¼Œå…¶ä¸­ä¸‹ä¸€çŠ¶æ€-åŠ¨ä½œå¯¹**Q(sâ€™, aâ€™)**é€šè¿‡é€‰æ‹©ä¸‹ä¸€çŠ¶æ€ä¸­æœ€å¤§ä¼°è®¡å€¼æ¥ä¼°ç®—ã€‚æ¢å¥è¯è¯´ï¼ŒQå­¦ä¹ é€‰æ‹©ä¸‹ä¸€çŠ¶æ€sâ€™ä¸­æ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„æœ€å¤§**Q(sâ€™,aâ€™)**å€¼ã€‚è¿™æ„å‘³ç€å®ƒä¸ä½¿ç”¨ç­–ç•¥ğ›‘æ¥å­¦ä¹ Q(sâ€™,aâ€™)ã€‚è€ŒSARSAåˆ™æ˜¯ä¸€ç§åœ¨çº¿ç­–ç•¥æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©å…ˆå‰é‡‡å–çš„åŠ¨ä½œä»¥åŠä¸‹ä¸€çŠ¶æ€-åŠ¨ä½œå¯¹çš„ä¼°ç®—å€¼Q(sâ€™,aâ€™)æ¥ç¡®å®šåŠ¨ä½œã€‚è¿™æ„å‘³ç€å®ƒä½¿ç”¨ç­–ç•¥ğ›‘ï¼Œå³ç»™å®šçŠ¶æ€ä¸‹çš„åŠ¨ä½œæ¦‚ç‡ï¼Œæ¥å­¦ä¹ Qå‡½æ•°ã€‚'
- en: In **Deep Q-Learning**, the action-value function Q(a, s) is represented via
    Q(a,s, ğœƒ ) where ğœƒ represent the neural network parameters. Theta ğœƒ parameters
    are equivalent to weights *w* in neural networks, which are associated with connections
    between neurons. The weights determine the strength of the connections and are
    retroactively adjusted through backpropagation in order to minimize the error.
    DQN takes as input a high-dimensional representation of the environment and outputs
    a vector of action-values for each possible action. The expected return is typically
    approximated through an MC or TD approach. Backpropagation with an optimization
    function are then used to compute policy gradient and reduce the error by adjusting
    the policy network parameters ğœƒ.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨**æ·±åº¦Qå­¦ä¹ **ä¸­ï¼ŒåŠ¨ä½œ-ä»·å€¼å‡½æ•°Q(a, s)é€šè¿‡Q(a,s, ğœƒ )è¡¨ç¤ºï¼Œå…¶ä¸­ğœƒä»£è¡¨ç¥ç»ç½‘ç»œå‚æ•°ã€‚Theta ğœƒå‚æ•°ç­‰åŒäºç¥ç»ç½‘ç»œä¸­çš„æƒé‡*w*ï¼Œè¿™äº›æƒé‡ä¸ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥ç›¸å…³ã€‚æƒé‡å†³å®šäº†è¿æ¥çš„å¼ºåº¦ï¼Œå¹¶é€šè¿‡åå‘ä¼ æ’­æ¥è°ƒæ•´ï¼Œä»¥æœ€å°åŒ–è¯¯å·®ã€‚DQNä»¥ç¯å¢ƒçš„é«˜ç»´è¡¨ç¤ºä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºæ¯ä¸ªå¯èƒ½åŠ¨ä½œçš„åŠ¨ä½œ-ä»·å€¼å‘é‡ã€‚æœŸæœ›å›æŠ¥é€šå¸¸é€šè¿‡MCæˆ–TDæ–¹æ³•è¿›è¡Œé€¼è¿‘ã€‚ç„¶åä½¿ç”¨å¸¦ä¼˜åŒ–å‡½æ•°çš„åå‘ä¼ æ’­è®¡ç®—ç­–ç•¥æ¢¯åº¦ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç­–ç•¥ç½‘ç»œå‚æ•°ğœƒæ¥å‡å°‘è¯¯å·®ã€‚
- en: Because ANNs are highly sensitive to new information, it can cause catastrophic
    forgetting, where new information can overwrite previously written information.
    A method to manage catastrophic forgetting is to employ experience reply, a technique
    that stores past experiences and reuses them to train the network.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºäººå·¥ç¥ç»ç½‘ç»œï¼ˆANNsï¼‰å¯¹æ–°ä¿¡æ¯éå¸¸æ•æ„Ÿï¼Œè¿™å¯èƒ½å¯¼è‡´ç¾éš¾æ€§é—å¿˜ï¼Œå³æ–°ä¿¡æ¯å¯èƒ½ä¼šè¦†ç›–å…ˆå‰å†™å…¥çš„ä¿¡æ¯ã€‚ç®¡ç†ç¾éš¾æ€§é—å¿˜çš„ä¸€ç§æ–¹æ³•æ˜¯é‡‡ç”¨ç»éªŒå›æ”¾ï¼Œè¿™æ˜¯ä¸€ç§å­˜å‚¨è¿‡å»ç»éªŒå¹¶é‡å¤ä½¿ç”¨å®ƒä»¬æ¥è®­ç»ƒç½‘ç»œçš„æŠ€æœ¯ã€‚
- en: '**On-Policy Deep TD(ğ€)**'
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç­–ç•¥æ¢¯åº¦æ·±åº¦TD(ğ€)**'
- en: ANNs can also be applied to **TD(Î»)** methods, where the state observation is
    fed as input into an ANN, which then approximates the action-value function as
    output. Due to the on-policy nature of **TD(Î»)**, Deep **TD(Î»)** approaches are
    best suited for tasks that require long-term dependencies between states.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNsï¼‰ä¹Ÿå¯ä»¥åº”ç”¨äº**TD(Î»)**æ–¹æ³•ï¼Œå…¶ä¸­çŠ¶æ€è§‚å¯Ÿä½œä¸ºè¾“å…¥ä¼ é€’ç»™ANNï¼Œåè€…å°†åŠ¨ä½œå€¼å‡½æ•°è¿‘ä¼¼ä¸ºè¾“å‡ºã€‚ç”±äº**TD(Î»)**æ–¹æ³•çš„æ”¿ç­–æ€§ï¼Œæ·±åº¦**TD(Î»)**æ–¹æ³•æœ€é€‚åˆäºé‚£äº›éœ€è¦é•¿æœŸçŠ¶æ€ä¾èµ–çš„ä»»åŠ¡ã€‚
- en: Training online learning methods like **TD(Î»)** can be challenging because the
    distribution of the environment changes with every or n steps due to bootstrapping.
    This is called *nonstationarity* and impedes the convergence of the ANN parameters
    ğœƒ toward optimality. The interdependence of succeeding states in online learning
    can cause catastrophic forgetting, where the update interferes with past learning.
    Furthermore, the combination of eligibility traces which assign credit to past
    actions and ANNs can create additional complications in the backpropagation step.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒåœ¨çº¿å­¦ä¹ æ–¹æ³•å¦‚**TD(Î»)**å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç¯å¢ƒçš„åˆ†å¸ƒä¼šå› æ¯æ¬¡æˆ–æ¯næ­¥çš„å¼•å¯¼è€Œå‘ç”Ÿå˜åŒ–ã€‚è¿™è¢«ç§°ä¸º*éå¹³ç¨³æ€§*ï¼Œå®ƒä¼šé˜»ç¢ANNå‚æ•°ğœƒå‘æœ€ä¼˜å€¼çš„æ”¶æ•›ã€‚åœ¨çº¿å­¦ä¹ ä¸­åç»­çŠ¶æ€çš„ç›¸äº’ä¾èµ–æ€§å¯èƒ½ä¼šå¯¼è‡´ç¾éš¾æ€§é—å¿˜ï¼Œå³æ›´æ–°ä¼šå¹²æ‰°è¿‡å»çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œå°†å½’å› è¿¹ä¸ANNç»“åˆä½¿ç”¨ï¼Œèµ‹äºˆè¿‡å»åŠ¨ä½œçš„å¥–åŠ±ï¼Œå¯èƒ½ä¼šåœ¨åå‘ä¼ æ’­æ­¥éª¤ä¸­å¼•å‘é¢å¤–çš„å¤æ‚æ€§ã€‚
- en: A way to mitigate these challenges involves utilizing a technique called **experience
    replay**. Experience replay stores agent learned episodes as vectors of [s, a,
    r, sâ€™] in a memory buffer. During training, the network samples from its memory
    buffer of stored learned vectors to update the network parameters. This provides
    the network with greater stability and makes it less prone to catastrophic interference
    from high-variance new experiences that result in a larger error or temporal difference
    between steps.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: åº”å¯¹è¿™äº›æŒ‘æˆ˜çš„ä¸€ç§æ–¹æ³•æ˜¯åˆ©ç”¨ä¸€ç§å«åš**ç»éªŒå›æ”¾**çš„æŠ€æœ¯ã€‚ç»éªŒå›æ”¾å°†æ™ºèƒ½ä½“å­¦ä¹ åˆ°çš„ç»å†å­˜å‚¨ä¸º[s, a, r, s']å‘é‡ï¼Œä¿å­˜åœ¨ä¸€ä¸ªè®°å¿†ç¼“å†²åŒºä¸­ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç½‘ç»œä»å…¶è®°å¿†ç¼“å†²åŒºä¸­é‡‡æ ·å­˜å‚¨çš„å­¦ä¹ å‘é‡ï¼Œä»¥æ›´æ–°ç½‘ç»œå‚æ•°ã€‚è¿™ä¸ºç½‘ç»œæä¾›äº†æ›´å¤§çš„ç¨³å®šæ€§ï¼Œä½¿å…¶ä¸æ˜“å—åˆ°æ¥è‡ªæ–°ç»éªŒçš„é«˜æ–¹å·®å¸¦æ¥çš„ç¾éš¾æ€§å¹²æ‰°ï¼Œè¿™äº›æ–°ç»éªŒä¼šå¯¼è‡´æ­¥éª¤ä¹‹é—´çš„æ›´å¤§è¯¯å·®æˆ–æ—¶é—´å·®å¼‚ã€‚
- en: Deep **TD(Î»)** algorithms have shown to excel in continuous control tasks wherethe
    state-space is continuous and the target unknown or unclear. These include continuous
    control tasks in robotics, autonomous cars, and financial markets.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦**TD(Î»)**ç®—æ³•å·²è¢«è¯æ˜åœ¨çŠ¶æ€ç©ºé—´è¿ç»­ä¸”ç›®æ ‡æœªçŸ¥æˆ–ä¸æ˜ç¡®çš„è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬æœºå™¨äººå­¦ã€è‡ªåŠ¨é©¾é©¶æ±½è½¦å’Œé‡‘èå¸‚åœºä¸­çš„è¿ç»­æ§åˆ¶ä»»åŠ¡ã€‚
- en: Reinforcement Learning and Artificial General Intelligence
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ä¸äººå·¥é€šç”¨æ™ºèƒ½
- en: What are the implications of reinforcement learning for artificial general intelligence?
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ å¯¹äººå·¥é€šç”¨æ™ºèƒ½çš„å¯ç¤ºæ˜¯ä»€ä¹ˆï¼Ÿ
- en: Notwithstanding the fact that â€œintelligenceâ€ is an ill-formed variable since
    it meshes disparate competencies into a single notion, whatâ€™s termed â€œgeneral
    intelligenceâ€ sits on top of evolved competencies of living organisms, which require
    the transduction of worldly information for survival and reproduction. Intelligence,
    even in the human context, cannot be extricated from the contours of organismic
    viability. This isnâ€™t, however, the orthodoxy. The general wisdom argues that
    intelligence is more akin to a program or software that computes inferences on
    the basis of available information.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡â€œæ™ºèƒ½â€æ˜¯ä¸€ä¸ªä¸å¤Ÿæ˜ç¡®çš„å˜é‡ï¼Œå› ä¸ºå®ƒå°†ä¸åŒçš„èƒ½åŠ›èåˆä¸ºä¸€ä¸ªå•ä¸€çš„æ¦‚å¿µï¼Œä½†æ‰€è°“çš„â€œé€šç”¨æ™ºèƒ½â€å»ºç«‹åœ¨ç”Ÿç‰©ä½“è¿›åŒ–å‡ºçš„èƒ½åŠ›ä¹‹ä¸Šï¼Œè¿™äº›èƒ½åŠ›éœ€è¦è½¬åŒ–ä¸–ç•Œä¿¡æ¯ä»¥ç¡®ä¿ç”Ÿå­˜å’Œç¹æ®–ã€‚å³ä¾¿åœ¨äººçš„è¯­å¢ƒä¸­ï¼Œæ™ºèƒ½ä¹Ÿæ— æ³•è„±ç¦»ç”Ÿç‰©ä½“ç”Ÿå­˜èƒ½åŠ›çš„æ¡†æ¶ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ˜¯ä¸»æµè§‚ç‚¹ã€‚æ™®éçš„çœ‹æ³•è®¤ä¸ºï¼Œæ™ºèƒ½æ›´åƒæ˜¯ä¸€ä¸ªç¨‹åºæˆ–è½¯ä»¶ï¼Œæ ¹æ®å¯ç”¨ä¿¡æ¯è¿›è¡Œæ¨ç†è®¡ç®—ã€‚
- en: The latter conception consists of two models, which are mistakenly thought of
    as competing. One model describes intelligence as following procedures, whereas
    the other describes intelligence as generalizing from data for optimal prediction.
    The former is generally much better understood, whereas the latter amounts to
    a cluster of techniques that reliably improve the strength of predictions. Animal
    intelligence is largely based on the latter model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: åè€…çš„æ¦‚å¿µåŒ…å«äº†ä¸¤ä¸ªæ¨¡å‹ï¼Œé€šå¸¸è¢«è¯¯è®¤ä¸ºæ˜¯ç›¸äº’ç«äº‰çš„ã€‚ä¸€ä¸ªæ¨¡å‹å°†æ™ºèƒ½æè¿°ä¸ºéµå¾ªç¨‹åºï¼Œè€Œå¦ä¸€ä¸ªåˆ™å°†æ™ºèƒ½æè¿°ä¸ºä»æ•°æ®ä¸­æ¦‚æ‹¬ä»¥è¿›è¡Œæœ€ä¼˜é¢„æµ‹ã€‚å‰è€…é€šå¸¸è¢«æ›´å¥½åœ°ç†è§£ï¼Œè€Œåè€…åˆ™æ˜¯ä¸€ç»„å¯ä»¥å¯é åœ°æé«˜é¢„æµ‹èƒ½åŠ›çš„æŠ€æœ¯ã€‚åŠ¨ç‰©æ™ºèƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸ŠåŸºäºåè€…çš„æ¨¡å‹ã€‚
- en: The most successful paradigm of the second model is deep learning through artificial
    neural networks. The chief advantage of ANN architectures is that they enable
    generalization from data without prior information or concepts, although this
    is not to be confused with unsupervised learning. ANNs first build a model through
    training and then make predictions on the basis of that model on new data. It
    is thought, therefore, that the brain does something similar (after factoring
    pre-training from evolution). However, there are currently two weaknesses within
    ANNs. The first weakness is that the goal or outcome has to be set by the human
    designer. An ANN cannot of its own accord conceive of goals. It cannot, a fortiriori,
    tell the difference between truth and falsity of its own accord. The human designer
    must supply the true outcome in order for the model to learn to approximate that
    outcome. The second weakness is that an ANN, without reinforcement learning, cannot
    search an environment to optimize its own state. For this reason, the combination
    of the generalization and predictive power of ANNs with the decision optimization
    power of reinforcement learning makes for a formidable amalgamation.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒç§æ¨¡å‹ä¸­æœ€æˆåŠŸçš„èŒƒå¼æ˜¯é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œè¿›è¡Œçš„æ·±åº¦å­¦ä¹ ã€‚äººå·¥ç¥ç»ç½‘ç»œæ¶æ„çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºï¼Œå®ƒä»¬å¯ä»¥åœ¨æ²¡æœ‰å…ˆéªŒä¿¡æ¯æˆ–æ¦‚å¿µçš„æƒ…å†µä¸‹ä»æ•°æ®ä¸­è¿›è¡Œæ³›åŒ–ï¼Œå°½ç®¡è¿™ä¸èƒ½ä¸æ— ç›‘ç£å­¦ä¹ æ··æ·†ã€‚äººå·¥ç¥ç»ç½‘ç»œé¦–å…ˆé€šè¿‡è®­ç»ƒå»ºç«‹æ¨¡å‹ï¼Œç„¶åæ ¹æ®è¯¥æ¨¡å‹å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚å› æ­¤ï¼Œäººä»¬è®¤ä¸ºï¼Œå¤§è„‘åšçš„äº‹æƒ…ä¸æ­¤ç›¸ä¼¼ï¼ˆåœ¨è€ƒè™‘è¿›åŒ–å‰è®­ç»ƒåï¼‰ã€‚ç„¶è€Œï¼Œå½“å‰äººå·¥ç¥ç»ç½‘ç»œå­˜åœ¨ä¸¤ä¸ªå¼±ç‚¹ã€‚ç¬¬ä¸€ä¸ªå¼±ç‚¹æ˜¯ï¼Œç›®æ ‡æˆ–ç»“æœå¿…é¡»ç”±äººç±»è®¾è®¡è€…è®¾å®šã€‚äººå·¥ç¥ç»ç½‘ç»œä¸èƒ½è‡ªè¡Œæ„æƒ³ç›®æ ‡ã€‚æ›´è¿›ä¸€æ­¥ï¼Œå®ƒä¸èƒ½è‡ªè¡ŒåŒºåˆ†çœŸå‡ã€‚äººç±»è®¾è®¡è€…å¿…é¡»æä¾›çœŸå®çš„ç»“æœï¼Œä»¥ä¾¿æ¨¡å‹å­¦ä¹ æ¥è¿‘è¯¥ç»“æœã€‚ç¬¬äºŒä¸ªå¼±ç‚¹æ˜¯ï¼Œå¦‚æœæ²¡æœ‰å¼ºåŒ–å­¦ä¹ ï¼Œäººå·¥ç¥ç»ç½‘ç»œæ— æ³•åœ¨ç¯å¢ƒä¸­è¿›è¡Œæœç´¢ï¼Œä»¥ä¼˜åŒ–è‡ªèº«çŠ¶æ€ã€‚å› æ­¤ï¼Œäººå·¥ç¥ç»ç½‘ç»œçš„æ³›åŒ–å’Œé¢„æµ‹èƒ½åŠ›ä¸å¼ºåŒ–å­¦ä¹ çš„å†³ç­–ä¼˜åŒ–èƒ½åŠ›ç›¸ç»“åˆï¼Œå½¢æˆäº†å¼ºå¤§çš„èåˆã€‚
- en: 'It is on this basis that some have argued that reinforcement learning represents
    the clearest path toward artificial general intelligence (Sutton, 2014). The intuition
    behind this is clear: reinforcement learning comes closest to modelling living
    systems, which when enhanced with other successful architectures like transformers
    may lead to a model of AI that replicates (and exceeds!) all human capabilities.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè¿™ä¸€ç‚¹ï¼Œæœ‰äººè®¤ä¸ºå¼ºåŒ–å­¦ä¹ ä»£è¡¨äº†é€šå‘äººå·¥é€šç”¨æ™ºèƒ½çš„æœ€æ¸…æ™°è·¯å¾„ï¼ˆSuttonï¼Œ2014ï¼‰ã€‚å…¶ç›´è§‚æ€§å¾ˆæ˜æ˜¾ï¼šå¼ºåŒ–å­¦ä¹ æœ€æ¥è¿‘äºæ¨¡æ‹Ÿç”Ÿç‰©ç³»ç»Ÿï¼Œå½“ä¸å…¶ä»–æˆåŠŸçš„æ¶æ„ï¼ˆå¦‚å˜æ¢å™¨ï¼‰ç»“åˆæ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸€ç§èƒ½å¤Ÿå¤åˆ¶ï¼ˆå¹¶è¶…è¶Šï¼ï¼‰æ‰€æœ‰äººç±»èƒ½åŠ›çš„äººå·¥æ™ºèƒ½æ¨¡å‹ã€‚
- en: However, if humans are the basis of general intelligence, then the conception
    of general intelligence cannot be one that divorces intelligence from survival
    constraints and some form of embodiment. On the other hand, if general intelligence
    can be defined without reference to living organisms, then it isnâ€™t clear what
    it would look like â€” purely abstract models escape satisfactory formalization
    despite attempts like Marcus Hutterâ€™s [AIXI](https://en.wikipedia.org/wiki/AIXI).
    In the abstract, it can be conceived of some perfectly rational agent that solves
    problems by virtue of reasoning and computational power alone. The cleavage between
    information and embodiment is a gambit for a much wider discussion that is beyond
    the scope of this article. If interested, this [paper](https://evanthompson.me/wp-content/uploads/2012/11/pcs-life-and-mind.pdf)
    provides a good starting point.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¦‚æœäººç±»æ˜¯é€šç”¨æ™ºèƒ½çš„åŸºç¡€ï¼Œé‚£ä¹ˆé€šç”¨æ™ºèƒ½çš„æ¦‚å¿µå°±ä¸èƒ½è„±ç¦»ç”Ÿå­˜çº¦æŸå’ŒæŸç§å½¢å¼çš„ä½“ç°ã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœé€šç”¨æ™ºèƒ½å¯ä»¥ä¸ä¾èµ–äºç”Ÿç‰©ä½“æ¥å®šä¹‰ï¼Œé‚£ä¹ˆå®ƒå°†æ˜¯ä»€ä¹ˆæ ·å­å°±ä¸æ¸…æ¥šäº†â€”â€”çº¯ç²¹çš„æŠ½è±¡æ¨¡å‹å°½ç®¡æœ‰åƒMarcus
    Hutterçš„[AIXI](https://en.wikipedia.org/wiki/AIXI)è¿™æ ·çš„å°è¯•ï¼Œä»ç„¶é€ƒè„±äº†ä»¤äººæ»¡æ„çš„å½¢å¼åŒ–ã€‚ä»æŠ½è±¡ä¸Šæ¥çœ‹ï¼Œå¯ä»¥æ„æƒ³å‡ºä¸€äº›é€šè¿‡æ¨ç†å’Œè®¡ç®—èƒ½åŠ›å•ç‹¬è§£å†³é—®é¢˜çš„å®Œç¾ç†æ€§ä»£ç†ã€‚ä¿¡æ¯ä¸ä½“ç°ä¹‹é—´çš„åˆ†è£‚æ˜¯ä¸€ä¸ªæ›´å¹¿æ³›è®¨è®ºçš„ç­¹ç ï¼Œè¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ã€‚å¦‚æœæ„Ÿå…´è¶£ï¼Œè¿™ç¯‡[è®ºæ–‡](https://evanthompson.me/wp-content/uploads/2012/11/pcs-life-and-mind.pdf)æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚
- en: However, there are good reasons to doubt that reinforcement learning suffices
    for artificial general intelligence. Some reasons for this include the very definition
    of general intelligence. Most current AI researchers still rely on a behaviourist
    conception of intelligence without factoring explicit internal representations
    as necessary ingredients. And they have good reason to think so. Symbolic AI,
    in which hopes of general AI were pinned before the success of deep learning,
    proved to be a failure. Symbolic AI refers to approaches to artificial intelligence
    based primarily on explicitly coded logical rules and knowledge stores for optimal
    inference generation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæœ‰å……åˆ†çš„ç†ç”±æ€€ç–‘å¼ºåŒ–å­¦ä¹ æ˜¯å¦è¶³ä»¥å®ç°äººå·¥é€šç”¨æ™ºèƒ½ã€‚å…¶åŸå› ä¹‹ä¸€ä¸é€šç”¨æ™ºèƒ½çš„å®šä¹‰æœ¬èº«æœ‰å…³ã€‚å¤§å¤šæ•°ç°æœ‰çš„äººå·¥æ™ºèƒ½ç ”ç©¶è€…ä»ç„¶ä¾èµ–äºè¡Œä¸ºä¸»ä¹‰çš„æ™ºèƒ½è§‚å¿µï¼Œè€Œæ²¡æœ‰å°†æ˜¾å¼çš„å†…éƒ¨è¡¨å¾ä½œä¸ºå¿…è¦çš„ç»„æˆéƒ¨åˆ†æ¥è€ƒè™‘ã€‚å¹¶ä¸”ï¼Œä»–ä»¬æœ‰å……åˆ†çš„ç†ç”±è¿™ä¹ˆè®¤ä¸ºã€‚åœ¨æ·±åº¦å­¦ä¹ æˆåŠŸä¹‹å‰ï¼Œè±¡å¾æ€§äººå·¥æ™ºèƒ½æ›¾æ˜¯äººä»¬å¯„å¸Œæœ›äºå®ç°é€šç”¨äººå·¥æ™ºèƒ½çš„æ–¹å‘ï¼Œä½†ç»“æœè¯æ˜å®ƒæ˜¯å¤±è´¥çš„ã€‚è±¡å¾æ€§äººå·¥æ™ºèƒ½æŒ‡çš„æ˜¯ä¸»è¦ä¾é æ˜¾å¼ç¼–ç çš„é€»è¾‘è§„åˆ™å’ŒçŸ¥è¯†åº“æ¥è¿›è¡Œæœ€ä¼˜æ¨ç†ç”Ÿæˆçš„äººå·¥æ™ºèƒ½æ–¹æ³•ã€‚
- en: 'The tension between symbolic AI and neural networks may, however, be unfounded.
    Many researchers believe that the quest for artificial general intelligence lies
    in combining these approaches in the right way. Reasons for thinking that neural
    nets approximate the native ontology of the brain include the fact that mathematical
    logic is not quite how the brain reasons: that is, it doesnâ€™t compute necessary
    and sufficient conditions, or crisp membership, as much as graded membership,
    which is approximated by the likes of fuzzy logic and at which ANNs excel.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè±¡å¾æ€§äººå·¥æ™ºèƒ½ä¸ç¥ç»ç½‘ç»œä¹‹é—´çš„ç´§å¼ å…³ç³»å¯èƒ½æ˜¯æ²¡æœ‰æ ¹æ®çš„ã€‚è®¸å¤šç ”ç©¶è€…è®¤ä¸ºï¼Œäººå·¥é€šç”¨æ™ºèƒ½çš„è¿½æ±‚åœ¨äºä»¥æ­£ç¡®çš„æ–¹å¼ç»“åˆè¿™äº›æ–¹æ³•ã€‚è®¤ä¸ºç¥ç»ç½‘ç»œæ¥è¿‘å¤§è„‘å›ºæœ‰æœ¬ä½“è®ºçš„åŸå› ä¹‹ä¸€æ˜¯ï¼Œæ•°å­¦é€»è¾‘å¹¶ä¸æ˜¯å¤§è„‘æ¨ç†çš„æ–¹å¼ï¼šä¹Ÿå°±æ˜¯è¯´ï¼Œå¤§è„‘å¹¶ä¸åƒè®¡ç®—å¿…è¦å’Œå……åˆ†æ¡ä»¶æˆ–æ¸…æ™°çš„æˆå‘˜èµ„æ ¼é‚£æ ·æ¨ç†ï¼Œè€Œæ˜¯æ›´å€¾å‘äºæ¨ç†æ¸è¿›çš„æˆå‘˜èµ„æ ¼ï¼Œè¿™ä¸€ç‚¹é€šè¿‡æ¨¡ç³Šé€»è¾‘ç­‰æ–¹å¼è¿›è¡Œäº†é€¼è¿‘ï¼Œè€Œäººå·¥ç¥ç»ç½‘ç»œï¼ˆANNsï¼‰åœ¨è¿™æ–¹é¢è¡¨ç°å¾—å°¤ä¸ºå‡ºè‰²ã€‚
- en: 'Neural networks consist of a black-box hierarchical architecture of hidden
    layers parametrized to achieve the desired output through highly calibrated dynamic
    learning rates, activation functions, connection weights, and optimization algorithms
    calibrated to minimize error. Beyond highly calibrated hyperparameters like the
    above, the human designer does not understand how information is processed in
    the hidden layers. The assumption is that the same is the case with the brain,
    where information is not stored as combinations of discrete representational units
    (whether analog or imagistic) but as a vast, distributed architecture of billions
    of neurons. What we think of as linguistically structured thoughts are not internally
    represented in the brain that way at all: thereâ€™s no specific combination of neurons
    that stand for the word *being* or the sentence â€œ*Existence as determinate being
    is in essence being for anotherâ€* for example.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œç”±ä¸€ä¸ªé»‘ç›’å¼çš„å±‚æ¬¡ç»“æ„ç»„æˆï¼Œéšè—å±‚çš„å‚æ•°ç»è¿‡ç²¾ç¡®è°ƒæ ¡ï¼Œä»¥é€šè¿‡é«˜åº¦æ ¡å‡†çš„åŠ¨æ€å­¦ä¹ ç‡ã€æ¿€æ´»å‡½æ•°ã€è¿æ¥æƒé‡å’Œä¼˜åŒ–ç®—æ³•æ¥å®ç°æœŸæœ›çš„è¾“å‡ºï¼Œå¹¶åŠªåŠ›æœ€å°åŒ–è¯¯å·®ã€‚é™¤äº†ä¸Šè¿°é«˜åº¦è°ƒæ ¡çš„è¶…å‚æ•°å¤–ï¼Œäººç±»è®¾è®¡è€…å¹¶ä¸ç†è§£éšè—å±‚ä¸­ä¿¡æ¯æ˜¯å¦‚ä½•å¤„ç†çš„ã€‚å‡è®¾å¤§è„‘ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œå…¶ä¸­ä¿¡æ¯ä¸æ˜¯ä½œä¸ºç¦»æ•£è¡¨å¾å•å…ƒçš„ç»„åˆï¼ˆæ— è®ºæ˜¯æ¨¡æ‹Ÿçš„è¿˜æ˜¯å½¢è±¡çš„ï¼‰å­˜å‚¨çš„ï¼Œè€Œæ˜¯ä½œä¸ºç”±æ•°åäº¿ä¸ªç¥ç»å…ƒç»„æˆçš„åºå¤§åˆ†å¸ƒå¼æ¶æ„å­˜å‚¨çš„ã€‚æˆ‘ä»¬æ‰€è®¤ä¸ºçš„è¯­è¨€ç»“æ„åŒ–æ€ç»´åœ¨å¤§è„‘ä¸­å¹¶ä¸æ˜¯ä»¥è¿™ç§æ–¹å¼è¡¨ç¤ºçš„ï¼šä¾‹å¦‚ï¼Œå¹¶æ²¡æœ‰ç‰¹å®šçš„ç¥ç»å…ƒç»„åˆæ¥ä»£è¡¨è¯è¯­*being*æˆ–å¥å­â€œ*ä½œä¸ºå†³å®šæ€§å­˜åœ¨çš„å­˜åœ¨ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸ºä»–è€…çš„å­˜åœ¨*â€ã€‚
- en: Linguistic competence is instead embedded in a vast network of semantic connections
    and reproduction rules reinforced through experience and augmented by imagistic
    and analog representations. In other words, language and thought as we represent
    them reflectively, but also behaviourally in writing and speech, do not have brain
    analogues that mirror their explicit structure (in other words, isomorphic mapping
    between grammar and the native ontology of the brain), but are instead embedded
    in distributed networks of neural assemblies characterized by degrees of connectivity
    and connection strengths.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­è¨€èƒ½åŠ›åˆ™åµŒå…¥åœ¨ä¸€ä¸ªåºå¤§çš„è¯­ä¹‰è¿æ¥ç½‘ç»œå’Œé€šè¿‡ç»éªŒå¼ºåŒ–çš„å†ç°è§„åˆ™ä¸­ï¼Œå¹¶é€šè¿‡å½¢è±¡å’Œæ¨¡æ‹Ÿè¡¨å¾åŠ ä»¥å¢å¼ºã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬æ‰€åæ€æ€§åœ°è¡¨ç¤ºçš„è¯­è¨€å’Œæ€ç»´ï¼ˆä¹ŸåŒ…æ‹¬é€šè¿‡ä¹¦å†™å’Œè¨€è¯­è¡¨ç°å‡ºçš„è¡Œä¸ºï¼‰åœ¨å¤§è„‘ä¸­å¹¶æ²¡æœ‰é•œåƒå…¶æ˜¾å¼ç»“æ„çš„è„‘éƒ¨ç±»æ¯”ï¼ˆæ¢å¥è¯è¯´ï¼Œå¤§è„‘çš„æœ¬ä½“è®ºä¸è¯­æ³•ä¹‹é—´æ²¡æœ‰åŒæ„æ˜ å°„ï¼‰ï¼Œè€Œæ˜¯åµŒå…¥åœ¨ç”±ç¥ç»ç»„ç¾¤æ„æˆçš„åˆ†å¸ƒå¼ç½‘ç»œä¸­ï¼Œè¿™äº›ç½‘ç»œçš„ç‰¹å¾æ˜¯è¿æ¥åº¦å’Œè¿æ¥å¼ºåº¦çš„ä¸åŒã€‚
- en: 'On the other hand, it seems that neural nets seem unable to instantiate the
    structured thought-processes that some argue are the seat of reason and human
    intelligence. After all, explicit reasoning constitutes the chief means of human
    intellectual achievement, and this does not appear to be something that current
    neural nets are able to replicate. A salient illustration comes from GÃ¶delâ€™s Incompleteness
    Theorems, where a formal system alone cannot establish the truth of certain statements
    on the basis of proof alone. ([If interested, check out this article that I wrote
    that explains Godelâ€™s proof](https://angjelinhila.medium.com/alethic-limits-set-theory-entscheidungsproblem-turing-computability-ed0ed282b026)).
    Meanwhile, the human subject can verify the truth of such a statement despite
    failure of axiomatic deduction. Foregoing the complicated and contested implications
    of this uncoupling of truth and proof for computation, it is additionally worth
    noting that the human agent actively pursues theories of the world, whereas current
    RL algorithms do so in a very rudimentary sense, though robotics will likely eventually
    advance toward similar capabilities. Meanwhile the linguistic state of the art,
    LLMs, regurgitate linguistically indistinguishable analogues to human speech and
    writing when prompted while exhibiting exponentially faster recall speeds and
    stores of information orders of magnitude larger. Much hangs in the balance of
    understanding this distinction: humans actively pursue theories of the world as
    well as other creative pursuits as part of their cultural programming, which co-opts
    mechanisms tailored toward survival and reproductive success. In other words,
    all human activity occurs within the basin of evolutionary constraints. As such,
    humans and all living organisms, constitute autonomous systems that replicate
    and endogenously reproduce their own identity conditions. Human and animal intelligence
    are therefore inextricable from the boundary conditions of survival, barring any
    measure of cultural independence from strict adaptationism ([a big topic which
    engenders wide disagreement](https://en.wikipedia.org/wiki/Sociobiology)).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œä¼¼ä¹ç¥ç»ç½‘ç»œæ— æ³•å®ç°ä¸€äº›äººè®¤ä¸ºæ˜¯ç†æ€§å’Œäººç±»æ™ºæ…§æ ¹æºçš„ç»“æ„åŒ–æ€ç»´è¿‡ç¨‹ã€‚æ¯•ç«Ÿï¼Œæ˜ç¡®çš„æ¨ç†æ„æˆäº†äººç±»æ™ºåŠ›æˆå°±çš„ä¸»è¦æ–¹å¼ï¼Œè€Œè¿™ä¼¼ä¹ä¸æ˜¯å½“å‰ç¥ç»ç½‘ç»œèƒ½å¤Ÿå¤åˆ¶çš„å†…å®¹ã€‚ä¸€ä¸ªæ˜¾è‘—çš„ä¾‹å­æ¥è‡ªå“¥å¾·å°”çš„ä¸å®Œå¤‡æ€§å®šç†ï¼Œå…¶ä¸­ä¸€ä¸ªå½¢å¼ç³»ç»Ÿå•ç‹¬æ— æ³•ä»…å‡­è¯æ˜ç¡®ç«‹æŸäº›é™ˆè¿°çš„çœŸç†ã€‚ï¼ˆ[å¦‚æœæœ‰å…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹æˆ‘å†™çš„è¿™ç¯‡æ–‡ç« ï¼Œè§£é‡Šäº†å“¥å¾·å°”çš„è¯æ˜](https://angjelinhila.medium.com/alethic-limits-set-theory-entscheidungsproblem-turing-computability-ed0ed282b026)ï¼‰ã€‚ä¸æ­¤åŒæ—¶ï¼Œäººç±»ä¸»ä½“å°½ç®¡å…¬ç†æ¨å¯¼å¤±è´¥ï¼Œä¾ç„¶èƒ½å¤ŸéªŒè¯è¿™æ ·çš„é™ˆè¿°çš„çœŸç†ã€‚æ’‡å¼€è¿™ç§çœŸç†ä¸è¯æ˜è„±é’©å¯¹è®¡ç®—çš„å¤æ‚ä¸”æœ‰äº‰è®®çš„å«ä¹‰ä¸è°ˆï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œäººç±»ä¸»åŠ¨è¿½æ±‚å…³äºä¸–ç•Œçš„ç†è®ºï¼Œè€Œå½“å‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨æŸç§éå¸¸åŸºç¡€çš„å±‚é¢ä¸Šä¹Ÿèƒ½åšåˆ°è¿™ä¸€ç‚¹ï¼Œå°½ç®¡æœºå™¨äººæŠ€æœ¯æœ€ç»ˆå¯èƒ½ä¼šæœç€ç±»ä¼¼çš„èƒ½åŠ›å‘å±•ã€‚ä¸æ­¤åŒæ—¶ï¼Œè¯­è¨€å­¦çš„å‰æ²¿æŠ€æœ¯â€”â€”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œåœ¨è¢«æç¤ºæ—¶ä¼šå¤è¿°å‡ºä¸äººç±»è¨€è¯­å’Œå†™ä½œåœ¨è¯­è¨€ä¸Šæ— æ³•åŒºåˆ†çš„ç±»ä¼¼ç‰©ï¼ŒåŒæ—¶å±•ç¤ºå‡ºæŒ‡æ•°çº§æ›´å¿«çš„å›å¿†é€Ÿåº¦å’Œä¿¡æ¯å­˜å‚¨é‡ï¼Œæ•°é‡çº§è¿œè¿œæ›´å¤§ã€‚ç†è§£è¿™ä¸€åŒºåˆ«è‡³å…³é‡è¦ï¼šäººç±»ä½œä¸ºæ–‡åŒ–ç¼–ç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œç§¯æè¿½æ±‚å…³äºä¸–ç•Œçš„ç†è®ºä»¥åŠå…¶ä»–åˆ›é€ æ€§æ´»åŠ¨ï¼Œè€Œè¿™äº›æ´»åŠ¨ä½¿ç”¨çš„æ˜¯ä¸ºç”Ÿå­˜å’Œç¹è¡æˆåŠŸé‡èº«å®šåˆ¶çš„æœºåˆ¶ã€‚æ¢å¥è¯è¯´ï¼Œæ‰€æœ‰äººç±»æ´»åŠ¨éƒ½å‘ç”Ÿåœ¨è¿›åŒ–çº¦æŸçš„æ¡†æ¶å†…ã€‚å› æ­¤ï¼Œäººç±»å’Œæ‰€æœ‰ç”Ÿç‰©éƒ½æ„æˆäº†è‡ªä¸»ç³»ç»Ÿï¼Œå¤åˆ¶å¹¶å†…ç”Ÿåœ°å†ç”Ÿäº§è‡ªèº«çš„èº«ä»½æ¡ä»¶ã€‚äººç±»å’ŒåŠ¨ç‰©çš„æ™ºæ…§å› æ­¤ä¸ç”Ÿå­˜çš„è¾¹ç•Œæ¡ä»¶å¯†ä¸å¯åˆ†ï¼Œé™¤éå­˜åœ¨æŸç§å½¢å¼çš„æ–‡åŒ–ç‹¬ç«‹æ€§ï¼Œèƒ½å¤Ÿè„±ç¦»ä¸¥æ ¼çš„é€‚åº”ä¸»ä¹‰([è¿™æ˜¯ä¸€ä¸ªå¼•å‘å¹¿æ³›äº‰è®®çš„å¤§è¯é¢˜](https://en.wikipedia.org/wiki/Sociobiology))ã€‚
- en: Current AI does not approximate autonomous systems that endogenously propel
    themselves in the world. Nor do they generate their own environmental milieu and
    reconfigure their own search spaces in the way humans and other animals do. The
    absence of this constraint currently allows the human designer to set AIâ€™s informational
    salience, e.g. text-generation, environmental detection etc. Even if the architecture
    evolves into a bona fide general problem-solving machine, unless it becomes capable
    of reflective awareness it cannot be said to possess general intelligence. Definitions
    of general intelligence canonically omit the variable of global awareness â€” the
    equivalent to what the ancient Greeks termed ***nous*** â€” as the hallmark of human
    intelligence. They do so because reflective and global awareness remain recalcitrant
    to reverse engineering and analysis into parts. For this reason, reflective awareness
    is dismissed as an ingredient of intelligence. However, admitting recalcitrance
    to current scientific explanation does not by the same token imply rejecting physicalism
    or an an endorsement of non-naturalism. Rather, it signals admission of lack of
    understanding. Given this gap in understanding, I hypothesize that reflective
    awareness is an extension of sentience which is a fundamental property of living
    organisms. In asserting this, I do not imply that autonomous systems cannot be
    engineered through means other than natural selection, though I leave open the
    possibility that they may remain opaque to scientific analysis in the foreseeable
    future. If reinforcement learning hopes to amount to general intelligence, the
    agent should posses as a prior a powerful architecture that not only hosts complex
    representations of the world, but maintains a global view from the inside of those
    very representations. This means that while model-world interactivity is indispensable
    to the task, the native architecture will require a complex hierarchical internal
    structure with capacities for multi-modal information processing and integration.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰çš„äººå·¥æ™ºèƒ½å¹¶æœªæ¥è¿‘é‚£äº›èƒ½å¤Ÿè‡ªæˆ‘é©±åŠ¨ã€åœ¨ä¸–ç•Œä¸­è‡ªä¸»æ¨è¿›çš„ç³»ç»Ÿã€‚å®ƒä»¬ä¹Ÿä¸ä¼šåƒäººç±»å’Œå…¶ä»–åŠ¨ç‰©é‚£æ ·ç”Ÿæˆè‡ªèº«çš„ç¯å¢ƒï¼Œå¹¶é‡æ–°é…ç½®è‡ªèº«çš„æœç´¢ç©ºé—´ã€‚ç›®å‰ï¼Œç¼ºä¹è¿™ä¸€çº¦æŸä½¿å¾—äººå·¥è®¾è®¡è€…å¯ä»¥è®¾å®šäººå·¥æ™ºèƒ½çš„ä¿¡æ¯æ˜¾è‘—æ€§ï¼Œä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆã€ç¯å¢ƒæ£€æµ‹ç­‰ã€‚å³ä½¿æ¶æ„å‘å±•æˆä¸ºä¸€ä¸ªçœŸæ­£çš„é€šç”¨é—®é¢˜è§£å†³æœºå™¨ï¼Œé™¤éå®ƒèƒ½å¤Ÿå…·å¤‡åæ€æ„è¯†ï¼Œå¦åˆ™æ— æ³•è¯´å®ƒæ‹¥æœ‰é€šç”¨æ™ºèƒ½ã€‚é€šç”¨æ™ºèƒ½çš„å®šä¹‰é€šå¸¸çœç•¥äº†å…¨çƒæ„è¯†è¿™ä¸€å˜é‡â€”â€”è¿™ç­‰åŒäºå¤å¸Œè…Šäººæ‰€ç§°çš„***nous***â€”â€”ä½œä¸ºäººç±»æ™ºèƒ½çš„æ ‡å¿—ã€‚ä¹‹æ‰€ä»¥å¦‚æ­¤ï¼Œæ˜¯å› ä¸ºåæ€æ€§å’Œå…¨çƒæ„è¯†ä»ç„¶éš¾ä»¥é€†å‘å·¥ç¨‹å¹¶æ‹†è§£ä¸ºéƒ¨åˆ†ã€‚æ­£å› å¦‚æ­¤ï¼Œåæ€æ€§æ„è¯†å¸¸å¸¸è¢«å¿½è§†ä¸ºæ™ºèƒ½çš„ä¸€éƒ¨åˆ†ã€‚ç„¶è€Œï¼Œæ‰¿è®¤å½“å‰ç§‘å­¦è§£é‡Šçš„å›°éš¾ï¼Œå¹¶ä¸æ„å‘³ç€æ‹’ç»ç‰©ç†ä¸»ä¹‰æˆ–æ”¯æŒéè‡ªç„¶ä¸»ä¹‰ã€‚ç›¸åï¼Œè¿™è¡¨æ˜æˆ‘ä»¬å°šæœªç†è§£è¿™ä¸€ç‚¹ã€‚é‰´äºè¿™ä¸€ç†è§£çš„ç©ºç™½ï¼Œæˆ‘å‡è®¾åæ€æ€§æ„è¯†æ˜¯æ„ŸçŸ¥çš„å»¶ä¼¸ï¼Œè€Œæ„ŸçŸ¥æ˜¯ç”Ÿç‰©æœ‰æœºä½“çš„åŸºæœ¬å±æ€§ã€‚åœ¨æå‡ºè¿™ä¸€è§‚ç‚¹æ—¶ï¼Œæˆ‘å¹¶ä¸æ„å‘³ç€è‡ªä¸»ç³»ç»Ÿä¸èƒ½é€šè¿‡è‡ªç„¶é€‰æ‹©ä»¥å¤–çš„æ‰‹æ®µè¿›è¡Œå·¥ç¨‹è®¾è®¡ï¼Œå°½ç®¡æˆ‘ç•™æœ‰å¼€æ”¾çš„å¯èƒ½æ€§ï¼Œè®¤ä¸ºå®ƒä»¬å¯èƒ½åœ¨å¯é¢„è§çš„æœªæ¥ä»ç„¶å¯¹ç§‘å­¦åˆ†æä¿æŒä¸é€æ˜ã€‚å¦‚æœå¼ºåŒ–å­¦ä¹ å¸Œæœ›å‘å±•ä¸ºé€šç”¨æ™ºèƒ½ï¼Œé‚£ä¹ˆæ™ºèƒ½ä½“åº”è¯¥å…ˆå…·å¤‡ä¸€ä¸ªå¼ºå¤§çš„æ¶æ„ï¼Œä¸ä»…èƒ½æ‰¿è½½ä¸–ç•Œçš„å¤æ‚è¡¨å¾ï¼Œè¿˜èƒ½ä»è¿™äº›è¡¨å¾çš„å†…éƒ¨ç»´æŒå…¨çƒè§†é‡ã€‚è¿™æ„å‘³ç€ï¼Œå°½ç®¡æ¨¡å‹ä¸ä¸–ç•Œçš„äº¤äº’æ€§å¯¹ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä½†åŸç”Ÿæ¶æ„å°†éœ€è¦ä¸€ä¸ªå¤æ‚çš„åˆ†å±‚å†…éƒ¨ç»“æ„ï¼Œå…·å¤‡å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†å’Œæ•´åˆçš„èƒ½åŠ›ã€‚
- en: '**Selected References**'
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç²¾é€‰å‚è€ƒæ–‡çŒ®**'
- en: Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M.
    G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S.,
    Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg,
    S., & Hassabis, D. (2015). Human-level Control through Deep Reinforcement Learning.
    *Nature*, *518*(7540), 529â€“533\. [https://doi.org/10.1038/nature14236](https://doi.org/10.1038/nature14236)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M.
    G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S.,
    Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg,
    S., & Hassabis, D. (2015). é€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ å®ç°äººç±»çº§æ§åˆ¶. *Nature*, *518*(7540), 529â€“533. [https://doi.org/10.1038/nature14236](https://doi.org/10.1038/nature14236)
- en: Neftci, E. O., & Averbeck, B. B. (2019, March 4). *Reinforcement learning in
    artificial and Biological Systems*. Nature News. [https://www.nature.com/articles/s42256-019-0025-4](https://www.nature.com/articles/s42256-019-0025-4)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Neftci, E. O., & Averbeck, B. B. (2019å¹´3æœˆ4æ—¥). *äººå·¥å’Œç”Ÿç‰©ç³»ç»Ÿä¸­çš„å¼ºåŒ–å­¦ä¹ *. Nature News.
    [https://www.nature.com/articles/s42256-019-0025-4](https://www.nature.com/articles/s42256-019-0025-4)
- en: 'Sharma, S. (2024, March 7). *Learning to Mix ğ‘›-Step Returns: Generalizing ğœ†-Returns
    for Deep Reinforcement Learning*. Ar5iv. [https://ar5iv.labs.arxiv.org/html/1705.07445](https://ar5iv.labs.arxiv.org/html/1705.07445)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Sharma, S. (2024å¹´3æœˆ7æ—¥). *å­¦ä¹ æ··åˆğ‘›æ­¥å›æŠ¥ï¼šå°†ğœ†-å›æŠ¥æ¨å¹¿åˆ°æ·±åº¦å¼ºåŒ–å­¦ä¹ *. Ar5iv. [https://ar5iv.labs.arxiv.org/html/1705.07445](https://ar5iv.labs.arxiv.org/html/1705.07445)
- en: 'Sanghi, Nimish. *Deep Reinforcement Learning with Python: With PYTORCH, Tensorflow
    and Openai Gym*. Apress, 2021.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Sanghi, Nimish. *ä½¿ç”¨Pythonè¿›è¡Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼šåŒ…æ‹¬PYTORCHã€Tensorflowå’ŒOpenai Gym*ã€‚Apressï¼Œ2021å¹´ã€‚
- en: Silver, D., Singh, S., Precup, D., & Sutton, R. S. (2021). Reward is enough.
    *Artificial Intelligence*, *299*, 103535\. [https://doi.org/10.1016/j.artint.2021.103535](https://doi.org/10.1016/j.artint.2021.103535)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Silver, D., Singh, S., Precup, D., & Sutton, R. S. (2021å¹´)ã€‚å¥–åŠ±å°±è¶³å¤Ÿäº†ã€‚*äººå·¥æ™ºèƒ½*ï¼Œ*299*ï¼Œ103535\.
    [https://doi.org/10.1016/j.artint.2021.103535](https://doi.org/10.1016/j.artint.2021.103535)
- en: Spens, E., & Burgess, N. (2024, January 19). *A generative model of memory construction
    and consolidation*. Nature News. [https://www.nature.com/articles/s41562-023-01799-z](https://www.nature.com/articles/s41562-023-01799-z)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Spens, E., & Burgess, N. (2024å¹´1æœˆ19æ—¥)ã€‚*è®°å¿†æ„å»ºä¸å·©å›ºçš„ç”Ÿæˆæ¨¡å‹*ã€‚ã€Šè‡ªç„¶æ–°é—»ã€‹ã€‚ [https://www.nature.com/articles/s41562-023-01799-z](https://www.nature.com/articles/s41562-023-01799-z)
- en: Sutton, Richard S. *Introduction to Reinforcement Learning*. MIT Press.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Sutton, Richard S. *å¼ºåŒ–å­¦ä¹ å¯¼è®º*ã€‚MITå‡ºç‰ˆç¤¾ã€‚
- en: Tyng, C. M., Amin, H. U., Saad, M. N. M., & Malik, A. S. (2017, August 24).
    *The influences of emotion on learning and memory*. Frontiers in psychology. [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5573739/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5573739/)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Tyng, C. M., Amin, H. U., Saad, M. N. M., & Malik, A. S. (2017å¹´8æœˆ24æ—¥)ã€‚*æƒ…ç»ªå¯¹å­¦ä¹ å’Œè®°å¿†çš„å½±å“*ã€‚ã€Šå¿ƒç†å­¦å‰æ²¿ã€‹ã€‚
    [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5573739/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5573739/)
- en: White, A., Modayil, J., & Sutton, R. (2014). Surprise and Curiosity for Big
    Data Robotics. *Association for the Advancement of Artificial Intelligence*, 19â€“22.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: White, A., Modayil, J., & Sutton, R. (2014å¹´)ã€‚å¤§æ•°æ®æœºå™¨äººä¸­çš„æƒŠè®¶ä¸å¥½å¥‡å¿ƒã€‚*äººå·¥æ™ºèƒ½ä¿ƒè¿›åä¼š*ï¼Œ19â€“22ã€‚
