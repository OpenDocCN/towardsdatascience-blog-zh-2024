- en: A Walkthrough of Nvidia’s Latest Multi-Modal LLM Family
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 英伟达最新的多模态大语言模型家族全览
- en: 原文：[https://towardsdatascience.com/a-walkthrough-of-nvidias-latest-multi-modal-llm-family-fdc067b59596?source=collection_archive---------6-----------------------#2024-10-10](https://towardsdatascience.com/a-walkthrough-of-nvidias-latest-multi-modal-llm-family-fdc067b59596?source=collection_archive---------6-----------------------#2024-10-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-walkthrough-of-nvidias-latest-multi-modal-llm-family-fdc067b59596?source=collection_archive---------6-----------------------#2024-10-10](https://towardsdatascience.com/a-walkthrough-of-nvidias-latest-multi-modal-llm-family-fdc067b59596?source=collection_archive---------6-----------------------#2024-10-10)
- en: From LLaVA, Flamingo, to NVLM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 LLaVA、Flamingo 到 NVLM
- en: '[](https://mengliuz.medium.com/?source=post_page---byline--fdc067b59596--------------------------------)[![Mengliu
    Zhao](../Images/0b950a0785fa065db3319ed5be4a91de.png)](https://mengliuz.medium.com/?source=post_page---byline--fdc067b59596--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fdc067b59596--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fdc067b59596--------------------------------)
    [Mengliu Zhao](https://mengliuz.medium.com/?source=post_page---byline--fdc067b59596--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mengliuz.medium.com/?source=post_page---byline--fdc067b59596--------------------------------)[![孟柳·赵](../Images/0b950a0785fa065db3319ed5be4a91de.png)](https://mengliuz.medium.com/?source=post_page---byline--fdc067b59596--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fdc067b59596--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fdc067b59596--------------------------------)
    [孟柳·赵](https://mengliuz.medium.com/?source=post_page---byline--fdc067b59596--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fdc067b59596--------------------------------)
    ·6 min read·Oct 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fdc067b59596--------------------------------)
    ·阅读时间 6 分钟·2024年10月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Multi-modal LLM development has been advancing fast in recent years.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，多模态大语言模型（LLM）发展迅速。
- en: Although the commercial multi-modal models like [GPT-4v](https://openai.com/index/gpt-4v-system-card/),
    [GPT-4o](https://openai.com/index/gpt-4v-system-card/), [Gemini](https://gemini.google.com/),
    and [Claude 3.5 Sonnet](https://www.anthropic.com/news/claude-3-5-sonnet) are
    the most eye-catching performers these days, the open-source models such as [LLaVA](https://arxiv.org/abs/2304.08485),
    [Llama 3-V](https://huggingface.co/spaces/MBZUAI/LLaMA-3-V), [Qwen-VL](https://arxiv.org/abs/2308.12966)
    have been steadily catching up in terms of performance on public benchmarks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管商业化的多模态模型，如 [GPT-4v](https://openai.com/index/gpt-4v-system-card/)、[GPT-4o](https://openai.com/index/gpt-4v-system-card/)、[Gemini](https://gemini.google.com/)
    和 [Claude 3.5 Sonnet](https://www.anthropic.com/news/claude-3-5-sonnet) 目前是最引人注目的表现者，但开源模型，如
    [LLaVA](https://arxiv.org/abs/2304.08485)、[Llama 3-V](https://huggingface.co/spaces/MBZUAI/LLaMA-3-V)、[Qwen-VL](https://arxiv.org/abs/2308.12966)
    在公开基准测试上的表现正稳步追赶。
- en: 'Just last month, Nvidia released their open-source [multi-modal LLM family](https://arxiv.org/pdf/2409.11402)
    called NVLM. The family comprises three architectures: a) decoder-based, b) cross-attention-based,
    and c) hybrid. The decoder-based model takes both the image and text tokens to
    a pre-trained LLM, such as the LLaVA model. The cross-attention-based model uses
    the image token embeddings as the keys and values while using the text token embeddings
    as the queries; since the attention is calculated using different sources, it’s
    called “cross-attention” as in the original transformer decoder rather than the
    self-attention as in decoder-only models. The hybrid architecture is a unique
    design merging the decoder and cross-attention architecture for the benefit of
    multi-modal reasoning, fewer training parameters, and taking high-resolution input.
    The 72B decoder-based NVLM-D model achieved an impressive performance, beating
    state-of-the-art open-source and commercial models on tasks like natural image
    understanding and OCR.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 就在上个月，英伟达发布了他们的开源 [多模态大语言模型家族](https://arxiv.org/pdf/2409.11402)，称为 NVLM。该家族包括三种架构：a)
    基于解码器的，b) 基于交叉注意力的，和 c) 混合型。基于解码器的模型将图像和文本令牌传递给预训练的大语言模型，如 LLaVA 模型。基于交叉注意力的模型将图像令牌嵌入作为键和值，而将文本令牌嵌入作为查询；由于注意力是使用不同来源的计算，因此它被称为“交叉注意力”，类似于原始的
    Transformer 解码器，而非解码器模型中的自注意力。混合架构是一种独特的设计，结合了解码器和交叉注意力架构，旨在实现多模态推理、减少训练参数并处理高分辨率输入。72B
    解码器基础的 NVLM-D 模型在自然图像理解和 OCR 等任务上取得了令人印象深刻的表现，超越了当前最先进的开源和商业模型。
- en: 'In this article, I’m going to walk through the following things:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将介绍以下内容：
- en: the dynamic high-resolution (DHR) vision encoder, which all the NVLM models
    adopt
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态高分辨率（DHR）视觉编码器，所有NVLM模型都采用了这一技术
- en: the decoder-based model, NVLM-D, compared to LLaVA
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于解码器的模型，NVLM-D，与LLaVA的比较
- en: the gated cross-attention model, NVLM-X, compared to Flamingo
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 门控交叉注意力模型，NVLM-X，与Flamingo的比较
- en: the hybrid model, NVLM-H
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合模型，NVLM-H
- en: In the end, I’ll show the NVLM-D 72B performance. Compared to state-of-the-art
    open-source and commercial models, the NVLM-D model shows stability over text-based
    tasks and superior performance on natural understanding and OCR tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我将展示NVLM-D 72B的性能。与最先进的开源和商业模型相比，NVLM-D模型在基于文本的任务中表现出稳定性，并且在自然理解和OCR任务中具有更优越的表现。
- en: '![](../Images/e4ae48dd6785464df1a1ed3950a52a99.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4ae48dd6785464df1a1ed3950a52a99.png)'
- en: 'Image source: [https://pxhere.com/en/photo/821032](https://pxhere.com/en/photo/821032)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[https://pxhere.com/en/photo/821032](https://pxhere.com/en/photo/821032)
- en: Dynamic High-Resolution-based Vision Encoder (DHR)
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于动态高分辨率的视觉编码器（DHR）
- en: One of the prominent advantages of NVLM models is that they excel in processing
    OCR-related tasks, which require high-resolution image inputs. NVML adopts the
    dynamic high-resolution approach proposed in the [InternVL 1.5](https://arxiv.org/abs/2404.16821)
    [technical report](https://arxiv.org/abs/2404.16821) to retain high resolution.
    The DHR approach first converts a high resolution image into a pre-defined aspect
    ratio size (also called dynamic aspect ratio matching), before splitting it into
    non-overlapping 448*448 tiles with an extra thumb-nail image, which can retain
    better global information.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: NVLM模型的一个显著优势是它们在处理OCR相关任务方面表现出色，这类任务需要高分辨率的图像输入。NVLM采用了[InternVL 1.5](https://arxiv.org/abs/2404.16821)
    [技术报告](https://arxiv.org/abs/2404.16821)中提出的动态高分辨率方法，以保持高分辨率。DHR方法首先将高分辨率图像转换为预定义的纵横比大小（也称为动态纵横比匹配），然后将其拆分为不重叠的448*448图块，并附加一个缩略图，这样可以更好地保留全局信息。
- en: '![](../Images/aa5821f98f063e6cc79a71e582fb62f7.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa5821f98f063e6cc79a71e582fb62f7.png)'
- en: 'DHR pipeline. Image source: [https://arxiv.org/abs/2409.11402](https://arxiv.org/pdf/2409.11402)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: DHR管道。图片来源：[https://arxiv.org/abs/2409.11402](https://arxiv.org/pdf/2409.11402)
- en: The image above shows a detailed explanation of the DHR pipeline. An input image
    is shown on the left, and a list of 6 different pre-defined aspect ratios is searched
    and matched to the original image shape. Then, the reshaped image is cropped into
    six non-overlapping tiles of 448*448, with an extra underresolution thumbnail
    image to capture global information. The sequence of n tiles (n=6+1=7 in this
    case) is passed into the ViT separately and converted into a sequence of length
    n with 1024 tokens (448/14*448/14=1024), each of embedding dimension d. To reduce
    the computational cost, a [pixel reshuffle](https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html)
    operation is employed to resize the 32*32 patch to 16*16, which reduces the final
    output token size to 256 with an increased embedding dimension of 4*d.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了DHR管道的详细说明。左侧显示了输入图像，并列出了6种不同的预定义纵横比，它们会被搜索并匹配到原始图像形状。然后，将重塑后的图像裁剪成六个不重叠的448*448的块，并附加一个低分辨率的缩略图，用以捕捉全局信息。接下来，n个图块（在本例中n=6+1=7）被分别输入到ViT中，并转换为一个长度为n、每个嵌入维度为d的1024个token的序列（448/14*448/14=1024）。为了减少计算成本，采用了[pixel
    reshuffle](https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html)操作，将32*32的图块调整为16*16，从而将最终的输出token大小减少到256，且嵌入维度增大为4*d。
- en: 'Decoder-only Models: NVLM-D vs LLaVA'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器模型：NVLM-D与LLaVA
- en: '[LLaVA](https://arxiv.org/pdf/2304.08485) is a well-known decoder-based Multi-modal
    LLM, which takes in the image X_v and uses a pre-trained CLIP encoder ViT-L/14
    as vision encoder Z_v, with a trainable linear project layer W to convert into
    embedding tokens H_v, which can be digested together with other text tokens. The
    LLaVA architecture is shown below.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLaVA](https://arxiv.org/pdf/2304.08485)是一个著名的基于解码器的多模态大语言模型，它接收图像X_v并使用预训练的CLIP编码器ViT-L/14作为视觉编码器Z_v，经过一个可训练的线性投影层W转换为嵌入token
    H_v，之后这些token可以与其他文本token一起处理。LLaVA的架构如图所示。'
- en: '![](../Images/7bc3a807281d96c9b810bc6efe211c0b.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bc3a807281d96c9b810bc6efe211c0b.png)'
- en: 'LLaVA architecture. Image source: [https://arxiv.org/pdf/2304.08485](https://arxiv.org/pdf/2304.08485)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA架构。图片来源：[https://arxiv.org/pdf/2304.08485](https://arxiv.org/pdf/2304.08485)
- en: In contrast, the NVLM-D architecture takes in encoded tile sequence tokens using
    the DHR vision encoder and inserts tile tags in between before concatenating with
    the text tokens for the transformer layer ingestion. The architecture is shown
    below.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，NVLM-D架构使用DHR视觉编码器输入编码后的瓦片序列令牌，并在其中插入瓦片标签，然后与文本令牌拼接，供变换器层处理。架构如下所示。
- en: '![](../Images/755d770abda63648b8988169b6a01bd5.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/755d770abda63648b8988169b6a01bd5.png)'
- en: 'Decoder-based NVLM-D architecture with DHR vision encoder. Image source: [https://arxiv.org/pdf/2409.11402](https://arxiv.org/pdf/2409.11402)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 带有DHR视觉编码器的基于解码器的NVLM-D架构。图像来源：[https://arxiv.org/pdf/2409.11402](https://arxiv.org/pdf/2409.11402)
- en: 'Cross-attention Models: NVLM-X vs Flamingo'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉注意力模型：NVLM-X与Flamingo
- en: Comparing to LLaVA, the [Flamingo model](https://arxiv.org/pdf/2204.14198) uses
    a more complicated cross-attention technique, which takes the vision embeddings
    as keys (K) and values (V), while the text embeddings as queries (Q). Moreover,
    the vision encoder is a CNN-based model with a Perceiver Resampler, which takes
    in a sequence of image(s) with temporal positional embedding to train learnable
    latent query vectors using cross attention. A more detailed discussion of the
    Perceiver Resampler can be found in [my latest article here](https://medium.com/towards-data-science/from-set-transformer-to-perceiver-sampler-2f18e741d242).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与LLaVA相比，[Flamingo模型](https://arxiv.org/pdf/2204.14198)使用了更复杂的交叉注意力技术，它将视觉嵌入作为键（K）和值（V），而将文本嵌入作为查询（Q）。此外，视觉编码器是基于CNN的模型，配有感知重采样器（Perceiver
    Resampler），它接受一系列带有时间位置嵌入的图像，以使用交叉注意力训练可学习的潜在查询向量。关于感知重采样器的更详细讨论，可以在[我的最新文章中找到](https://medium.com/towards-data-science/from-set-transformer-to-perceiver-sampler-2f18e741d242)。
- en: '![](../Images/e218b29b9ea9c17dc58910a33ba598e7.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e218b29b9ea9c17dc58910a33ba598e7.png)'
- en: 'The Flamingo architecture. Image source: [https://arxiv.org/pdf/2204.14198](https://arxiv.org/pdf/2204.14198)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Flamingo架构。图像来源：[https://arxiv.org/pdf/2204.14198](https://arxiv.org/pdf/2204.14198)
- en: To fuse the vision embedding and text embedding, the Flamingo freezes the pre-trained
    LLM layers and further adds a trainable gated cross-attention layer in between,
    which is shown below. The gated attention uses a tanh gating with a learnable
    alpha parameter after the cross-attention layer and the subsequent linear layer.
    When the tanh gating is initialized as zero, the only information passed is through
    the skip connection, so the whole model will still be the original LLM to increase
    stability.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了融合视觉嵌入和文本嵌入，Flamingo冻结了预训练的LLM层，并在其中进一步添加了一个可训练的门控交叉注意力层，如下所示。门控注意力使用tanh门控和一个可学习的alpha参数，在交叉注意力层和随后的线性层之后。当tanh门控初始化为零时，唯一传递的信息是通过跳跃连接，因此整个模型仍将是原始LLM，以提高稳定性。
- en: '![](../Images/9902afd9e93f34aeb02ccdd62c39b92c.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9902afd9e93f34aeb02ccdd62c39b92c.png)'
- en: 'The gated cross attention design from Flamingo. Image source: [https://arxiv.org/pdf/2204.14198](https://arxiv.org/pdf/2204.14198)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Flamingo的门控交叉注意力设计。图像来源：[https://arxiv.org/pdf/2204.14198](https://arxiv.org/pdf/2204.14198)
- en: In comparison, the NVLM-X removes the Perceiver Resampler design for the benefit
    of OCR tasks to keep the more spatial relationship and only uses the DHR encoder
    output for the gated cross-attention. Unlike the decoder-based model, the NVLM-X
    concatenates the tile tags to the text tokens before sending them into the gated
    cross-attention. The whole architecture is shown below.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，NVLM-X去除了感知重采样器设计，以便更好地支持OCR任务，从而保持更多的空间关系，并且只使用DHR编码器输出用于门控交叉注意力。与基于解码器的模型不同，NVLM-X在将瓦片标签传入门控交叉注意力之前，将它们与文本令牌拼接。整个架构如下所示。
- en: '![](../Images/db129490cd7e9c6b52b27d59900e2ddf.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db129490cd7e9c6b52b27d59900e2ddf.png)'
- en: 'NVLM-X architecture with gated cross-attention design. Image source: [https://arxiv.org/pdf/2409.11402](https://arxiv.org/pdf/2409.11402)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 带有门控交叉注意力设计的NVLM-X架构。图像来源：[https://arxiv.org/pdf/2409.11402](https://arxiv.org/pdf/2409.11402)
- en: 'Hybrid Models: NVLM-H'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合模型：NVLM-H
- en: The hybrid model is a unique design by NVLM. The thumbnail image token is added
    to the text tokens as input to the self-attention layer, which preserves the benefit
    of multi-modal reasoning from the decoder-based model. The other image tiles and
    tile tags are passed into the gated cross-attention layer to capture finer image
    details while minimizing total model parameters. The detailed architecture is
    shown below.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型是NVLM的独特设计。缩略图像令牌被添加到文本令牌中，作为自注意力层的输入，这保留了基于解码器的模型的多模态推理优势。其他图像瓦片和瓦片标签则传入门控交叉注意力层，以捕捉更精细的图像细节，同时最小化整体模型参数。详细架构如下所示。
- en: '![](../Images/542156bcdf3846a012a7fdaf0b97400c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/542156bcdf3846a012a7fdaf0b97400c.png)'
- en: 'NVML-H architecture. Image source: [https://arxiv.org/pdf/2409.11402](https://arxiv.org/pdf/2409.11402)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: NVML-H 架构。图片来源：[https://arxiv.org/pdf/2409.11402](https://arxiv.org/pdf/2409.11402)
- en: Performance
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: So, how’s the performance of NVLM compared to other state-of-the-art models?
    The paper lists benchmark performances comparing NVLM-D 72B to other open-source
    models like Llama-3 V and commercial models like GPT-4o. The NVLM-D achieved above-average
    performance on most benchmarks and excelled in the OCR and natural image understanding
    tasks due to the high-resolution image features and the model’s intrinsic multi-modal
    reasoning ability. Compared to Llama 3-V 70B and InternVL2-Llama3–76B, which have
    the equivalent amount of parameter numbers, the NVLM-D shows the advantage of
    having more consistent behaviours on the text-only tasks, the VQA task and the
    image understanding tasks. The detailed comparison is shown as follows.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，NVLM 与其他最先进模型的性能如何呢？本文列出了 NVLM-D 72B 与其他开源模型（如 Llama-3 V）和商业模型（如 GPT-4o）在基准测试中的比较。NVLM-D
    在大多数基准测试中表现优于平均水平，特别是在 OCR 和自然图像理解任务中表现突出，这得益于其高分辨率的图像特征和模型本身的多模态推理能力。与参数数量相当的
    Llama 3-V 70B 和 InternVL2-Llama3–76B 相比，NVLM-D 在仅文本任务、视觉问答（VQA）任务和图像理解任务中表现出更加一致的优势。详细比较如下所示。
- en: '![](../Images/c5af6c78b569aaf809cc7513259c4b2f.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5af6c78b569aaf809cc7513259c4b2f.png)'
- en: 'NVLM-D performance compared to other open-source and commercial models on public
    benchmarks. Image source: [https://arxiv.org/pdf/2409.11402](https://arxiv.org/pdf/2409.11402)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: NVLM-D 在公共基准测试中与其他开源和商业模型的性能比较。图片来源：[https://arxiv.org/pdf/2409.11402](https://arxiv.org/pdf/2409.11402)
- en: 'It’s also interesting to note that, although the decoder-based model is very
    powerful, the training throughput (numbers of sampled trained per second) is much
    lower than the cross-attention-based model. The paper explains that the decoder-based
    model takes a much longer sequence length than the cross-attention-based model,
    which causes a much higher GPU consumption and lower throughput. The detailed
    training comparison is shown below:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的发现是，尽管基于解码器的模型非常强大，但其训练吞吐量（每秒训练样本数）远低于基于跨注意力的模型。论文解释说，基于解码器的模型需要更长的序列长度，这导致了更高的
    GPU 消耗和较低的吞吐量。详细的训练比较如下所示：
- en: '![](../Images/60f00ce4fbd64cf616749bbbf76b53b4.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60f00ce4fbd64cf616749bbbf76b53b4.png)'
- en: 'Training detail comparison. Image source: [https://arxiv.org/pdf/2409.11402](https://arxiv.org/pdf/2409.11402)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 训练详细比较。图片来源：[https://arxiv.org/pdf/2409.11402](https://arxiv.org/pdf/2409.11402)
- en: References
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Dai et al., NVLM: Open Frontier-Class Multimodal LLMs. arXiv 2024.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 戴等人，《NVLM：开放前沿级多模态大语言模型》。arXiv 2024。
- en: Chen et al., How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal
    Models with Open-Source Suites. arXiv 2024.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人，《我们距离 GPT-4V 有多远？通过开源工具套件缩小与商业多模态模型的差距》。arXiv 2024。
- en: Liu et al., Visual Instruction Tuning. NeurIPS 2023.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人，《视觉指令微调》。NeurIPS 2023。
- en: 'Bai et al., Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization,
    Text Reading, and Beyond. arXiv 2023.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白等人，《Qwen-VL：一个多功能的视觉语言模型，用于理解、定位、文本读取及更多》。arXiv 2023。
- en: 'Alayrac et al., Flamingo: a Visual Language Model for Few-Shot Learning. NeurIPS
    2022.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alayrac 等人，《Flamingo：一个用于少量样本学习的视觉语言模型》。NeurIPS 2022。
