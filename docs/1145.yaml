- en: 'FanFabler: Fine-Tuning Llama 3 to Be a Multilingual Fanfic Writing Assistant'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/fanfabler-fine-tuning-llama-3-to-be-a-multilingual-fanfic-writing-assistant-dfc664ed4a72?source=collection_archive---------3-----------------------#2024-05-07](https://towardsdatascience.com/fanfabler-fine-tuning-llama-3-to-be-a-multilingual-fanfic-writing-assistant-dfc664ed4a72?source=collection_archive---------3-----------------------#2024-05-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How I used a custom training dataset and information retrieval for global storytelling.
    好样的! Bravo! वाह! ¡Guau! 브라보!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://robgon.medium.com/?source=post_page---byline--dfc664ed4a72--------------------------------)[![Robert
    A. Gonsalves](../Images/96b4da0f602a1cd9d1e1d2917868cbee.png)](https://robgon.medium.com/?source=post_page---byline--dfc664ed4a72--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--dfc664ed4a72--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--dfc664ed4a72--------------------------------)
    [Robert A. Gonsalves](https://robgon.medium.com/?source=post_page---byline--dfc664ed4a72--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--dfc664ed4a72--------------------------------)
    ·21 min read·May 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25eeb2a66bdf45b1cd5b94d5e6790f45.png)'
  prefs: []
  type: TYPE_IMG
- en: '**FanFabler: A Multilingual Fanfic Writing Assistant**, Image created using
    an AI image creation program, DALL-E 3, edited by the Author'
  prefs: []
  type: TYPE_NORMAL
- en: The rise of Large Language Models (LLMs) has ushered in a new era of text-based
    AI systems. Although these models are very good and highly capable, their training
    predominantly focuses on English. The largest commercial LLMs generate text well
    using “low resource” languages, while the smaller open-source models don’t fare
    well with non-European languages.
  prefs: []
  type: TYPE_NORMAL
- en: However, Meta trained the new Llama 3 model with a wider variety of languages,
    as they announced in a [post](https://ai.meta.com/blog/meta-llama-3/) when it
    was released[1].
  prefs: []
  type: TYPE_NORMAL
- en: To train the best language model, the curation of a large, high-quality training
    dataset is paramount. In line with our design principles, we invested heavily
    in pretraining data. … To prepare for upcoming multilingual use cases, over 5%
    of the Llama 3 pretraining dataset consists of high-quality non-English data that
    covers over 30 languages. However, we do not expect the same level of performance
    in these languages as in English. — Meta
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Five percent doesn’t sound like much, but it’s more than its previous versions
    of Llama [2] and other small LLMs like Mistral [3]…
  prefs: []
  type: TYPE_NORMAL
