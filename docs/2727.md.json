["```py\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n        num_rows: ...\n    })\n    test: Dataset({\n        features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n        num_rows: ...\n    })\n})\n```", "```py\ndef extract_dialogue(input_text):\n    # Split the input by lines and initialize variables\n    lines = input_text.strip().split(\"\\n\\n\")\n    dialogue_list = []\n\n    # Iterate through each line and extract the dialogue\n    for line in lines:\n        # Check if the line starts with \"Human\" or \"Assistant\" and split accordingly\n        if line.startswith(\"Human:\"):\n            role = \"user\"\n            content = line.replace(\"Human: \", \"\").strip()\n        elif line.startswith(\"Assistant:\"):\n            role = \"assistant\"\n            content = line.replace(\"Assistant: \", \"\").strip()\n        else:\n            # If the line doesn't start with \"Human\" or \"Assistant\", it's part of the previous message's content\n            # Append it to the last message's content\n            dialogue_list[-1][\"content\"] += \"\\n\\n\" + line.strip()\n            continue\n\n        # Append the extracted dialogue piece to the list\n        dialogue_list.append({\"role\": role, \"content\": content})\n\n    return dialogue_list\n\ndef process(row):\n        row[\"chosen\"] = extract_dialogue(row[\"chosen\"])\n        row[\"rejected\"] = extract_dialogue(row[\"rejected\"])\n        row[\"prompt\"] = row[\"chosen\"][0][\"content\"]\n        return row\n\nds_processed = ds.map(\n        process,\n        load_from_cache_file=False,\n    )\n```", "```py\n# Adjusting to llama prompt template format: https://github.com/meta-llama/llama-recipes\nsystem_prompt = \"Please answer the user's question to the best of your knowledge. If you don't know the answer respond that you don't know.\"\n\ndef encode_dialogue_turn(message):\n    return f'<|start_header_id|>{message.get(\"role\")}<|end_header_id|>{message.get(\"content\")}<|eot_id|>'\n\ndef encode_dialogue(dialogue):\n    if system_prompt:\n        return f'<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|>{functools.reduce(lambda a, b: a + encode_dialogue_turn(b), dialogue, \"\")}'\n    else:\n        return f'<|begin_of_text|>{functools.reduce(lambda a, b: a + encode_dialogue_turn(b), dialogue, \"\")}'\n\ndef encode_row(item):\n    return {\"chosen\": encode_dialogue(item[\"chosen\"]), \"rejected\": encode_dialogue(item[\"rejected\"]), \"prompt\": item[\"prompt\"]}\n\ndef encode_dataset(dataset):\n    return list(map(encode_row, dataset))\n\nencoded_dataset = ds_processed.map(encode_row)\n```", "```py\n# Tokenize and stack into target format\ndef preprocess_function(examples):\n    new_examples = {\n        \"input_ids_chosen\": [],\n        \"attention_mask_chosen\": [],\n        \"input_ids_rejected\": [],\n        \"attention_mask_rejected\": [],\n    }\n    for chosen, rejected in zip(examples[\"chosen\"], examples[\"rejected\"]):\n        tokenized_chosen = tokenizer(chosen)\n        tokenized_rejected = tokenizer(rejected)\n\n        new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n        new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n        new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n        new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n\n    return new_examples\n\ntokenized_dataset_hhrlhf = encoded_dataset.map(\n        preprocess_function,\n        batched=True,\n    ).remove_columns([\"chosen\", \"rejected\", \"prompt\"])\n```", "```py\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'query'],\n        num_rows: ...\n    })\n    test: Dataset({\n        features: ['input_ids', 'query'],\n        num_rows: ...\n    })\n})\n```", "```py\ndef extract_questions(dataset):\n    ret_questions = []\n    for topic in dataset:\n        paragraphs = topic['paragraphs']\n        for paragraph in paragraphs:\n            qas = paragraph['qas']\n            for qa in qas:\n                ret_questions.append([{\n            \"role\": \"system\", \"content\": f'Instruction: Please answer the user\\'s question to the best of your knowledge. If you don\\'t know the answer respond that you don\\'t know.',\n        }, {\n            \"role\": \"user\", \"content\": qa['question'],\n        }])\n    return ret_questions\n\n# Adjusting to llama prompt template format: https://github.com/meta-llama/llama-recipes\ndef encode_dialogue_turn(message):\n    message = message\n    return f'<|start_header_id|>{message.get(\"role\")}<|end_header_id|>{message.get(\"content\")}<|eot_id|>'\n\ndef encode_dialogue(dialogue):\n    return {'input': f'<|begin_of_text|>{functools.reduce(lambda a, b: a + encode_dialogue_turn(b), dialogue, \"\")}'}\n\ndef encode_dataset(dataset):\n    #print(dataset)\n    return list(map(encode_dialogue, dataset))\n\nencoded_train = encode_dataset(extract_questions(d_train['data']))\nencoded_test = encode_dataset(extract_questions(d_test['data']))\n```", "```py\n# Restrict training context size (due to memory limitations, can be adjusted)\ninput_min_text_length = 1\ninput_max_text_length = 2048\n\ndef create_and_prepare_dataset(tokenizer, dataset):\n\n    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n\n    def tokenize(example):\n        text_size = input_size()\n        example[\"input_ids\"] = tokenizer.encode(example[\"input\"])[:text_size]\n        example[\"query\"] = tokenizer.decode(example[\"input_ids\"])\n        return example\n\n    dataset = dataset.map(tokenize, batched=False)\n\n    dataset.set_format(\"torch\")\n    return dataset\n\ntokenized_dataset_squad = create_and_prepare_dataset(tokenizer, dataset_dict).remove_columns([\"input\"])\n```", "```py\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\ndef find_all_linear_names(hf_model):\n    lora_module_names = set()\n    for name, module in hf_model.named_modules():\n        if isinstance(module, bnb.nn.Linear4bit):\n            names = name.split(\".\")\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n        lora_module_names.remove(\"lm_head\")\n    return list(lora_module_names)\n```", "```py\n# Start training with remote decorator (https://docs.aws.amazon.com/sagemaker/latest/dg/train-remote-decorator.html). Additional job config is being pulled in from config.yaml. \n@remote(keep_alive_period_in_seconds=0, volume_size=100, job_name_prefix=f\"train-{model_id.split('/')[-1].replace('.', '-')}-reward\", use_torchrun=True, nproc_per_node=4)\ndef train_fn(\n        model_name,\n        train_ds,\n        test_ds=None,\n        lora_r=8,\n        lora_alpha=32,\n        lora_dropout=0.1,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        gradient_accumulation_steps=1,\n        learning_rate=2e-4,\n        num_train_epochs=1,\n        fsdp=\"\",\n        fsdp_config=None,\n        chunk_size=10000,\n        gradient_checkpointing=False,\n        merge_weights=False,\n        seed=42,\n        token=None,\n        model_hub_repo_id=None,\n        range_train=None,\n        range_eval=None\n):\n\n    set_seed(seed)\n\n    # Initialize Accelerator object handling distributed training\n    accelerator = Accelerator()\n\n    # Login to HuggingFace\n    if token is not None:\n        login(token=token)\n\n    # Load tokenizer. Padding side is \"left\" because focus needs to be on completion\n    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side = \"left\")\n\n    # Set tokenizer's pad Token\n    tokenizer.pad_token = tokenizer.eos_token \n    tokenizer.pad_token_id = tokenizer.eos_token_id \n```", "```py\n # Load data from S3\n    s3 = s3fs.S3FileSystem()\n    dataset = load_from_disk(train_ds)  \n\n    # Allow for partial dataset training\n    if range_train:\n        train_dataset = dataset[\"train\"].select(range(range_train))\n    else: \n        train_dataset = dataset[\"train\"]\n\n    if range_eval:\n        eval_dataset = dataset[\"test\"].select(range(range_eval))\n    else:\n        eval_dataset = dataset[\"test\"]\n```", "```py\n # Specify quantization config\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        quant_storage_dtype=torch.bfloat16\n    )\n\n    # Load model with classification head for reward\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        #num_labels=1,\n        trust_remote_code=True,\n        quantization_config=bnb_config,\n        attn_implementation=\"flash_attention_2\",\n        use_cache=False if gradient_checkpointing else True,\n        cache_dir=\"/tmp/.cache\"\n    )\n\n    # Pre-LoRA trainable paremeters\n    print_trainable_parameters(model)     \n\n    # Set model pad token id\n    model.config.pad_token_id = tokenizer.pad_token_id\n\n    # Prepare model for quantized training\n    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=gradient_checkpointing)\n```", "```py\n # Get lora target modules\n    modules = find_all_linear_names(model)\n    print(f\"Found {len(modules)} modules to quantize: {modules}\")\n\n    # Specify LoRA config\n    config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n        target_modules=modules,\n        lora_dropout=lora_dropout,\n        bias=\"none\",\n        task_type=\"SEQ_CLS\"\n    )\n\n    # Make sure to not train for CLM\n    if config.task_type != \"SEQ_CLS\":\n        warnings.warn(\n            \"You are using a `task_type` that is different than `SEQ_CLS` for PEFT. This will lead to silent bugs\"\n            \" Make sure to pass --lora_task_type SEQ_CLS when using this script.\"\n        )\n\n    # Create PeftModel\n    model = get_peft_model(model, config)\n\n    # Post-LoRA trainable paremeters\n    print_trainable_parameters(model) \n```", "```py\n # Specify training config\n    reward_config = RewardConfig(\n                        per_device_train_batch_size=per_device_train_batch_size,\n                        per_device_eval_batch_size=per_device_eval_batch_size,\n                        gradient_accumulation_steps=gradient_accumulation_steps,\n                        gradient_checkpointing=gradient_checkpointing,\n                        logging_strategy=\"steps\",\n                        logging_steps=100,\n                        log_on_each_node=False,\n                        num_train_epochs=num_train_epochs,\n                        learning_rate=learning_rate,\n                        bf16=True,\n                        ddp_find_unused_parameters=False,\n                        fsdp=fsdp,\n                        fsdp_config=fsdp_config,\n                        save_strategy=\"no\",\n                        output_dir=\"outputs\",\n                        max_length=512, \n                        remove_unused_columns=False,\n                        gradient_checkpointing_kwargs = {\"use_reentrant\": False}\n                        )\n\n    # Initialize RewardTrainer object handling training\n    trainer = RewardTrainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=reward_config,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n    )\n\n    trainer.train()\n\n    trainer.model.save_pretrained(\"/opt/ml/model\", safe_serialization=True)\n\n    if model_hub_repo_id is not None:\n        trainer.model.push_to_hub(repo_id=model_hub_repo_id)\n\n    with accelerator.main_process_first():\n        tokenizer.save_pretrained(\"/opt/ml/model\")\n```", "```py\n# Start training job\ntrain_fn(\n    model_id,\n    train_ds=dataset_path_hhrlhf,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,\n    gradient_checkpointing=True,\n    num_train_epochs=1,\n    token=hf_token,\n    model_hub_repo_id=model_hub_repo_id,\n    range_train=100,\n    range_eval=10\n)\n```", "```py\n# Start training with remote decorator (https://docs.aws.amazon.com/sagemaker/latest/dg/train-remote-decorator.html). Additional job config is being pulled in from config.yaml. \n@remote(keep_alive_period_in_seconds=0, volume_size=100, job_name_prefix=f\"train-{model_id.split('/')[-1].replace('.', '-')}-multi-adapter-ppo\", use_torchrun=True, nproc_per_node=4)\ndef train_fn(\n        model_name,\n        train_ds,\n        rm_adapter,\n        log_with=None,\n        use_safetensors=None,\n        use_score_scaling=False,\n        use_score_norm=False,\n        score_clip=None,\n        seed=42,\n        token=None,\n        model_hub_repo_id=None,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        gradient_accumulation_steps=2,\n        gradient_checkpointing=True,\n        num_train_epochs=1,\n        merge_weights=True,\n        range_train=None,\n        ):\n\n    set_seed(seed)\n\n    # Initialize Accelerator object handling distributed training\n    accelerator = Accelerator()\n\n    # Login to HuggingFace \n    if token is not None:\n        login(token=token)\n\n    # Load tokenizer. Padding side is \"left\" because focus needs to be on completion\n    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side = \"left\")\n\n    # Set tokenizer's pad Token\n    tokenizer.pad_token = tokenizer.eos_token \n    tokenizer.pad_token_id = tokenizer.eos_token_id  \n\n    # Load data from S3\n    s3 = s3fs.S3FileSystem()\n    dataset = load_from_disk(train_ds)  \n\n    # Allow for partial dataset training\n    if range_train:\n        train_dataset = dataset[\"train\"].select(range(range_train))\n    else: \n        train_dataset = dataset[\"train\"]\n```", "```py\n # Specify LoRA config\n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n```", "```py\n # Specify quantization config\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16\n    )\n```", "```py\n # Load model\n    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n        model_name,\n        #device_map='auto',\n        peft_config=lora_config,\n        quantization_config=bnb_config,\n        reward_adapter=rm_adapter,\n        use_safetensors=use_safetensors,\n        #attn_implementation=\"flash_attention_2\",\n    )\n\n    # Set model pad token id\n    model.config.pad_token_id = tokenizer.pad_token_id\n\n    if gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n\n    # Trainable paremeters\n    print_trainable_parameters(model) \n```", "```py\n # Specify PPO training config\n    config = PPOConfig(\n        model_name,\n        log_with=None,\n        learning_rate=1e-5,\n        batch_size=per_device_train_batch_size,\n        mini_batch_size=1,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        optimize_cuda_cache=True,\n        seed=42,\n        use_score_scaling=False,\n        use_score_norm=False,\n        score_clip=None,\n    )\n\n    # Initialize PPOTrainer object handling training\n    ppo_trainer = PPOTrainer(\n        config,\n        model,\n        ref_model=None,\n        tokenizer=tokenizer,\n        dataset=train_dataset,\n        data_collator=collator,\n    )\n\n    # Specifying inference params\n    generation_kwargs = {\n        \"top_k\": 0.0,\n        \"top_p\": 0.9,\n        \"do_sample\": True,\n        \"pad_token_id\": tokenizer.pad_token_id,\n        \"max_new_tokens\": 32,\n    }\n```", "```py\nstep = 0\n\n    for _epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n\n        question_tensors = batch[\"input_ids\"]\n\n        # Inference through model being fine-tuned\n        response_tensors = ppo_trainer.generate(\n            question_tensors,\n            return_prompt=False,\n            **generation_kwargs,\n        )\n\n        # Decode response\n        batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n\n        # Concat query and response\n        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n\n        # Tokenize query - response pair\n        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(ppo_trainer.accelerator.device)\n\n        # Compute reward score\n        raw_rewards = ppo_trainer.accelerator.unwrap_model(ppo_trainer.model).compute_reward_score(**inputs)\n        rewards = [raw_rewards[i, -1, 1] for i in range(len(raw_rewards))]  # take last token\n\n        # Run PPO step\n        stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n        ppo_trainer.log_stats(stats, batch, rewards)\n\n        step = step + 1      \n\n    if accelerator.is_main_process:\n\n        ppo_trainer.save_pretrained(\"/opt/ml/model\", safe_serialization=True)\n\n        if model_hub_repo_id is not None:\n            ppo_trainer.push_to_hub(repo_id=model_hub_repo_id)\n            tokenizer.push_to_hub(repo_id=model_hub_repo_id)\n\n    with accelerator.main_process_first():\n        tokenizer.save_pretrained(\"/opt/ml/model\")\n```", "```py\ntrain_fn(\n    model_id,\n    train_ds=dataset_path_squad,\n    rm_adapter=rm_adapter,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    num_train_epochs=1,\n    token=hf_token,\n    model_hub_repo_id=model_hub_repo_id,\n    range_train=100\n)\n```", "```py\n# sagemaker config\ninstance_type = \"ml.g5.4xlarge\"\nnumber_of_gpu = 1\nhealth_check_timeout = 300\n\n# TGI config\nconfig = {\n'HF_MODEL_ID': \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n'LORA_ADAPTERS': \"**HF_REPO_ID**\",\n'SM_NUM_GPUS': json.dumps(1), # Number of GPU used per replica\n'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text),\n'QUANTIZE': \"bitsandbytes\", # comment in to quantize\n'HUGGING_FACE_HUB_TOKEN': hf_token\n}\n\nimage_uri = get_huggingface_llm_image_uri(\n    \"huggingface\",\n    version=\"2.0\"\n)\n\n# create HuggingFaceModel\nllm_model = HuggingFaceModel(\n    role=role,\n    image_uri=image_uri,\n    env=config\n)\n```", "```py\n# Deploy model to an endpoint\n# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\nllm = llm_model.deploy(\n    endpoint_name=f'llama-31-8b-instruct-rlhf-{datetime.now().strftime(\"%Y%m%d%H%M%S\")}', # alternatively \"llama-2-13b-hf-nyc-finetuned\"\n    initial_instance_count=1,\n    instance_type=instance_type,\n    container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n)\n\nparameters = {\n        \"top_p\": 0.8,\n        \"temperature\": 0.1,\n        \"return_full_text\": True,\n        \"stop\": [],\n    }\n\nencoded_message = encode_dialogue([{'content': 'Who won the FIFA World cup 2014 in Brazil?', 'role': 'user'}])\n\nresponse = llm.predict({\"inputs\": encoded_message['input'], **parameters})\n```", "```py\n# Delete model and endpoint\nllm.delete_model()\nllm.delete_endpoint()\n```"]