["```py\nfrom langchain_community.document_loaders import PyPDFLoader\n\n# Load PDFs\npdf_paths = [\"/content/gitlab_handbook.pdf\"]\ndocuments = []\n\nfor path in pdf_paths:\n    loader = PyPDFLoader(path)\n    documents.extend(loader.load())\n\nfrom langchain_openai import ChatOpenAI\nllm = ChatOpenAI(model=\"gpt-4o\")\n```", "```py\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Initialize the text splitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n\n# Split documents into chunks\nsplits = text_splitter.split_documents(documents)\n```", "```py\ntotal_tokens = 0\n\nfor chunk in splits:\n    text = chunk.page_content  # Assuming `page_content` is where the text is stored\n    num_tokens = llm.get_num_tokens(text)  # Get the token count for each chunk\n    total_tokens += num_tokens\n\nprint(f\"Total number of tokens in the book: {total_tokens}\")\n\n# Total number of tokens in the book: 254006\n```", "```py\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\n\n# Embed the chunks\nchunk_texts = [chunk.page_content for chunk in splits]  # Extract the text from each chunk\nchunk_embeddings = embeddings.embed_documents(chunk_texts)\n```", "```py\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\nnum_clusters = 15\n# Convert the list of embeddings to a NumPy array\nchunk_embeddings_array = np.array(chunk_embeddings)\n\n# Perform K-means clustering\nkmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(chunk_embeddings)\n```"]