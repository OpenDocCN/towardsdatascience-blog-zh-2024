- en: 'Einstein Notation: A New Lens on Transformers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/einstein-notation-a-new-lens-on-transformers-761390a7960b?source=collection_archive---------1-----------------------#2024-11-20](https://towardsdatascience.com/einstein-notation-a-new-lens-on-transformers-761390a7960b?source=collection_archive---------1-----------------------#2024-11-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Transforming the Math of the Transformer Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ch.mittendorf?source=post_page---byline--761390a7960b--------------------------------)[![Dr.
    Christoph Mittendorf](../Images/466a7a53b8261f4df61461090dcfc743.png)](https://medium.com/@ch.mittendorf?source=post_page---byline--761390a7960b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--761390a7960b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--761390a7960b--------------------------------)
    [Dr. Christoph Mittendorf](https://medium.com/@ch.mittendorf?source=post_page---byline--761390a7960b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--761390a7960b--------------------------------)
    ·8 min read·Nov 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27d78cf2967592bbfc4e2308237ab91d.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformer (Created by author using FLUX1-schnell)
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll embark on a playful journey through the world of transformers,
    unraveling the complexities of their architecture using the Einstein notation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformer models have revolutionized the field of natural language processing
    (and beyond), achieving state-of-the-art results on a variety of tasks. They have
    impressive performance but the underlying mathematical operations can be complex
    and difficult to grasp — especially without breaking down the individual layers.
    In this article, I propose using the Einstein notation to express the mathematical
    operations within a transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the Einstein notation is normally used in Physics and Mathematics
    such as in General Relativity, Electromagnetism, Quantum and Fluid Mechanics but
    also in Linear Algebra to represent matrix operations in a more compact form.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to write the mathematical operations of every layer in a concise
    and elegant way. By leveraging implicit summation over repeated indices, Einstein
    notation can simplify the representation of tensor operations, making it (potentially)
    easier to understand and therefore implement the individual layers of the transformer
    models…
  prefs: []
  type: TYPE_NORMAL
