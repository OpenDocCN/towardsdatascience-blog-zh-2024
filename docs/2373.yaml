- en: 'To Mask or Not to Mask: The Effect of Prompt Tokens on Instruction Tuning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 屏蔽还是不屏蔽：提示令牌对指令微调的影响
- en: 原文：[https://towardsdatascience.com/to-mask-or-not-to-mask-the-effect-of-prompt-tokens-on-instruction-tuning-016f85fd67f4?source=collection_archive---------4-----------------------#2024-09-30](https://towardsdatascience.com/to-mask-or-not-to-mask-the-effect-of-prompt-tokens-on-instruction-tuning-016f85fd67f4?source=collection_archive---------4-----------------------#2024-09-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/to-mask-or-not-to-mask-the-effect-of-prompt-tokens-on-instruction-tuning-016f85fd67f4?source=collection_archive---------4-----------------------#2024-09-30](https://towardsdatascience.com/to-mask-or-not-to-mask-the-effect-of-prompt-tokens-on-instruction-tuning-016f85fd67f4?source=collection_archive---------4-----------------------#2024-09-30)
- en: Implementing prompt-loss-weight, and why we should replace prompt-masking with
    prompt-weighting
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现提示丢失权重，并且为什么我们应该用提示加权替代提示屏蔽
- en: '[](https://medium.com/@davidsvaughn?source=post_page---byline--016f85fd67f4--------------------------------)[![David
    Vaughn](../Images/74a3d9c03f6a67f01d8f781795041715.png)](https://medium.com/@davidsvaughn?source=post_page---byline--016f85fd67f4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--016f85fd67f4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--016f85fd67f4--------------------------------)
    [David Vaughn](https://medium.com/@davidsvaughn?source=post_page---byline--016f85fd67f4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@davidsvaughn?source=post_page---byline--016f85fd67f4--------------------------------)[![David
    Vaughn](../Images/74a3d9c03f6a67f01d8f781795041715.png)](https://medium.com/@davidsvaughn?source=post_page---byline--016f85fd67f4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--016f85fd67f4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--016f85fd67f4--------------------------------)
    [David Vaughn](https://medium.com/@davidsvaughn?source=post_page---byline--016f85fd67f4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--016f85fd67f4--------------------------------)
    ·30 min read·Sep 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--016f85fd67f4--------------------------------)
    ·30分钟阅读·2024年9月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ac4d2d01af7244482a4c7cdc453a8730.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac4d2d01af7244482a4c7cdc453a8730.png)'
- en: Image made with Midjourney
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 Midjourney 制作
- en: '[`*link to full code on* [*GitHub*](https://github.com/davidsvaughn/prompt-loss-weight)`][`*reach
    me on* [*LinkedIn*](https://www.linkedin.com/in/davidsvaughn/)`]'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[`*完整代码链接在* [*GitHub*](https://github.com/davidsvaughn/prompt-loss-weight)`][`*通过*
    [*LinkedIn*](https://www.linkedin.com/in/davidsvaughn/) 与我联系`]'
- en: 'In the last several months I’ve noticed quite a few [discussions](https://yonigottesman.github.io/2024/05/13/mask-user-tokens.html),
    [here](https://magazine.sebastianraschka.com/p/llm-research-insights-instruction?open=false#%C2%A7instruction-masking-during-instruction-finetuning)
    and [there](https://github.com/huggingface/trl/issues/632), even over [here](https://x.com/corbtt/status/1806336011804484017),
    on the question of whether or not to zero-mask (ignore) prompt tokens when fine-tuning
    on prompt-completion style data (i.e. instruction-tuning). I’ve seen various terms
    used, such as:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几个月中，我注意到关于是否在针对提示-完成风格数据（即指令微调）进行微调时对提示令牌进行零屏蔽（忽略）的问题，出现了不少讨论，包括在[这里](https://yonigottesman.github.io/2024/05/13/mask-user-tokens.html)、[这里](https://magazine.sebastianraschka.com/p/llm-research-insights-instruction?open=false#%C2%A7instruction-masking-during-instruction-finetuning)、[那里](https://github.com/huggingface/trl/issues/632)，甚至[这里](https://x.com/corbtt/status/1806336011804484017)。我看到使用了不同的术语，例如：
- en: '*instruction-masking*'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*指令屏蔽*'
- en: '*prompt-masking*'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提示屏蔽*'
- en: '*user-masking*'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用户屏蔽*'
- en: '*completion-only-training*'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*仅完成训练*'
- en: Whatever you call it, there seems to be no clear consensus about what the standard
    practice should be. Depending on which open source library you use for fine-tuning,
    the defaults can vary widely.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你怎么称呼它，似乎没有明确的共识关于应该采用什么标准做法。根据你使用的开源库，微调的默认设置可能差异很大。
- en: 'For example, the [Axolotl](https://hamel.dev/notes/llm/finetuning/09_template_free.html)
    library masks prompt tokens by default (through it’s `train_on_inputs=False` default
    setting). However, the very popular [HuggingFace Trainer](https://huggingface.co/docs/transformers/en/main_classes/trainer)
    does *not* mask prompt tokens by default. One can choose to mask out the prompt
    by using `[DataCollatorForCompletionOnlyLM](https://huggingface.co/docs/trl/main/en/sft_trainer#train-on-completions-only)`,
    but this comes with some [significant limitations](https://github.com/huggingface/trl/issues/1385)
    — notably, the lack of support for [*sample packing*](https://www.hopsworks.ai/dictionary/sample-packing)
    — which can be a deal-breaker when dealing with large datasets, as it was for
    me. (*Note: a nice solution was proposed* [*here*](https://github.com/huggingface/trl/issues/632#issuecomment-1972630547)).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[水陆两栖兽](https://hamel.dev/notes/llm/finetuning/09_template_free.html)库默认屏蔽提示令牌（通过其`train_on_inputs=False`默认设置）。然而，广受欢迎的[HuggingFace
    Trainer](https://huggingface.co/docs/transformers/en/main_classes/trainer)默认*不*屏蔽提示令牌。可以选择使用`[DataCollatorForCompletionOnlyLM](https://huggingface.co/docs/trl/main/en/sft_trainer#train-on-completions-only)`来屏蔽提示，但这带来了一些[显著的限制](https://github.com/huggingface/trl/issues/1385)——尤其是缺乏对[*样本打包*](https://www.hopsworks.ai/dictionary/sample-packing)的支持——这在处理大型数据集时可能成为致命问题，就像我遇到的情况一样。（*注意：这里提出了一个不错的解决方案*[*在此*](https://github.com/huggingface/trl/issues/632#issuecomment-1972630547)）。
- en: 'Many guides, demos, notebooks, tutorials, etc. for LLM fine-tuning that I have
    come across do *not* mention prompt-masking, for example:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 许多我遇到的关于LLM微调的指南、演示、笔记本、教程等，*没有*提到提示屏蔽，例如：
- en: '[How to Fine-Tune LLMs in 2024 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在2024年使用Hugging Face微调LLM](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl)'
- en: '[How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2--Vmlldzo1NjY0MjE1)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何微调LLM-第二部分-指令微调-Llama-2](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2--Vmlldzo1NjY0MjE1)'
- en: '[HuggingFace Alignment Handbook](https://github.com/huggingface/alignment-handbook/blob/main/scripts/run_sft.py)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingFace对齐手册](https://github.com/huggingface/alignment-handbook/blob/main/scripts/run_sft.py)'
- en: Niels Rogge’s [SFT Tutorial](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niels Rogge的[SFT教程](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb)
- en: this [Fine-tune Llama 2 Notebook](https://github.com/mlabonne/llm-course/blob/main/Fine_tune_Llama_2_in_Google_Colab.ipynb)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个[微调Llama 2笔记本](https://github.com/mlabonne/llm-course/blob/main/Fine_tune_Llama_2_in_Google_Colab.ipynb)
- en: 'But it’s also possible to find examples with *default* prompt-masking:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但也可以找到带有*默认*提示屏蔽的示例：
- en: this [FastChat example](https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py#L150)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个[FastChat示例](https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py#L150)
- en: PyTorch/[torchtune](https://github.com/pytorch/torchtune/blob/4efd7fdb48677a4ac6d78f394be7b4ecfbabf7de/torchtune/datasets/_instruct.py#L44C66-L44C80)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch/[torchtune](https://github.com/pytorch/torchtune/blob/4efd7fdb48677a4ac6d78f394be7b4ecfbabf7de/torchtune/datasets/_instruct.py#L44C66-L44C80)
- en: '[Axolotl](https://hamel.dev/notes/llm/finetuning/09_template_free.html) (mentioned
    above)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[水陆两栖兽](https://hamel.dev/notes/llm/finetuning/09_template_free.html)（上文提到）'
- en: '*Spoiler alert*: this article does *not* attempt to settle this issue once
    and for all. It began as a humble investigation inspired by a simple idea — *I
    wanted to compare fine-tuning* ***with*** *and* ***without******prompt masking****,
    while in both cases* ***separately******tracking the validation set prompt loss
    and completion loss****.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*剧透警告*：本文并*不*尝试一劳永逸地解决这个问题。它始于一个简单的想法，灵感来自一次谦虚的调查——*我想比较带有和不带有*提示屏蔽*的微调*，并在这两种情况下*分别*跟踪验证集的提示损失和完成损失*。'
- en: My hypothesis was this might yield useful insights into the prompt-masking question.
    Then I came across the concept of ***prompt-loss-weight***, an elegant generalization
    of *binary token-masking* into *real-valued token-weighting* (the weighting happens
    inside the loss function, as we’ll see).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我的假设是，这可能为提示屏蔽问题提供有价值的见解。然后，我遇到了***提示损失权重***这一概念，它是将*二进制令牌屏蔽*优雅地推广到*实值令牌加权*（加权发生在损失函数内部，正如我们所看到的）。
- en: 'Integrating a *prompt-loss-weight* (PLW) parameter into the fine-tuning pipeline
    enables a smoother, more fine-grained control over the influence of prompt tokens
    on the fine-tuning process. Simply put: *PLW=0* equates to prompt-masking, while
    *PLW=1* equates to no masking. In addition, using 0<*PLW<1* allows one to smoothly
    modulate the influence of prompt tokens between these two extremes.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 将*提示损失权重*（PLW）参数集成到微调流程中，可以更平滑、更精细地控制提示标记对微调过程的影响。简单来说：*PLW=0*等同于提示掩蔽，而*PLW=1*等同于不进行掩蔽。此外，使用0<*PLW<1*可以在这两种极端之间平滑地调节提示标记的影响。
- en: With this re-framing, the question of *whether or not to mask* prompt tokens
    is subsumed by the deeper question of *how much to weight* prompt tokens. The
    optimal weighting may vary depending on the specific use case and dataset. By
    adding *prompt-loss-weight* to your toolkit, you’ll gain the flexibility to experiment
    with different weighting strategies, leading to more effective fine-tuning outcomes
    tailored to your particular needs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种重新框定，*是否掩蔽*提示标记的问题被更深层次的问题所涵盖，即*如何加权*提示标记。最优的加权方式可能根据具体的使用案例和数据集有所不同。通过将*提示损失权重*添加到你的工具包中，你将获得实验不同加权策略的灵活性，从而实现更有效的微调结果，更好地满足你的特定需求。
- en: Since I couldn’t find any implementations of *prompt-loss-weight*, I decided
    to try implementing it myself. I’ll guide you through the customizations I had
    to make to several parts of the standard HuggingFace LLM toolset to make this
    work. Afterwards, we’ll use our updated toolset to explore the original questions
    about prompt tokens by running some fine-tuning experiments on the [RACE dataset](https://huggingface.co/datasets/ehovy/race)
    (a multiple choice QA dataset hosted on HuggingFace).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我找不到任何*提示损失权重*的实现，因此我决定尝试自己实现它。我将指导你完成对标准HuggingFace LLM工具集的几部分自定义修改，以便使其生效。之后，我们将使用更新后的工具集，通过在[RACE数据集](https://huggingface.co/datasets/ehovy/race)（一个托管在HuggingFace上的多项选择问答数据集）上运行一些微调实验，来探索关于提示标记的原始问题。
- en: Some LLM Background
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些LLM背景知识
- en: 'LLMs operate on *tokens* rather than *words*. For the purposes of this article
    we will use these two terms interchangeably, but it’s good to note the difference.
    Tokens are defined as *frequently occurring sequences of characters*, and often
    coincide roughly with words (and may even include the preceding space as well).
    A fun exercise is to play around with the [GPT-4 tokenizer](https://platform.openai.com/tokenizer),
    which I used to generate the following example (color-coding reveals the underlying
    tokens):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的操作基于*标记*而非*单词*。为了本篇文章的目的，我们将这两个术语交替使用，但值得注意的是，它们有所不同。标记被定义为*频繁出现的字符序列*，通常大致与单词相同（有时甚至包括前面的空格）。一个有趣的练习是玩一下[GPT-4分词器](https://platform.openai.com/tokenizer)，我用它生成了以下示例（颜色编码揭示了底层的标记）：
- en: '![](../Images/1386e5fce4f3a6e55312fb3c2f3b1cec.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1386e5fce4f3a6e55312fb3c2f3b1cec.png)'
- en: screenshot from [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)的截图
- en: The type of generative LLMs that most of us work with everyday are *next-token-prediction*
    machines. They have been trained (sometimes referred to as *pre-training*) on
    massive amounts of human generated text (books, newspapers, the internet, etc.)
    so that when fed a random snippet of sensible text, they are very good at predicting
    what the next word should be. This is sometimes referred to as *Causal Language
    Modeling*. When applied repeatedly, this *autoregressive text generation* process
    can generate very human-like sentences, paragraphs, articles, and so on.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们日常使用的大多数生成性LLM是*下一个标记预测*机器。它们已经在大量人类生成的文本（书籍、报纸、互联网等）上进行了训练（有时称为*预训练*），因此当输入一段合理的文本片段时，它们非常擅长预测下一个单词应该是什么。这有时被称为*因果语言建模*。当这一*自回归文本生成*过程反复应用时，可以生成非常类似人类的句子、段落、文章等。
- en: Often we will want to take one of these *foundation model* LLMs, that have been
    pre-trained on massive amounts of text (like the [Llama family of models from
    Meta](https://huggingface.co/meta-llama)), and continue the training a bit further,
    i.e. *fine-tune* them on a much smaller text dataset. This practice has roots
    in the broader field of *transfer learning*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常希望使用这些已经在大量文本上进行预训练的*基础模型*LLM（例如[Meta的Llama模型家族](https://huggingface.co/meta-llama)），然后继续进行进一步训练，也就是在一个更小的文本数据集上对其进行*微调*。这一做法源自于更广泛的*迁移学习*领域。
- en: The goal here is to gently tweak, or customize, the LLM’s *next-token-prediction*
    behavior without majorly disrupting or corrupting the basic underlying “intelligence”
    that is manifested in the model weights — this leads to LLMs that retain most
    of the emergent abilities of the foundation model (like reading comprehension,
    the ability to converse, to reason…), but are now specialized for a specific task.
    For example, *instruction-tuning* means fine-tuning an LLM so that it can follow
    instructions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是轻微调整或定制LLM的*下一个标记预测*行为，而不破坏或损坏模型权重中体现的基本“智能”——这使得LLM能够保留基础模型的大部分突现能力（如阅读理解、对话能力、推理能力等），但现在专门化于特定任务。例如，*指令调优*意味着对LLM进行微调，使其能够遵循指令。
- en: 'There are many instruction-tuning datasets available on [HuggingFace datasets
    hub](https://huggingface.co/datasets), organized by task. Some datasets are for
    [question answering](https://huggingface.co/datasets?task_categories=task_categories%3Aquestion-answering),
    or [text summarization](https://huggingface.co/datasets?task_categories=task_categories%3Asummarization).
    In the vast majority of cases, all these datasets share the same basic underlying
    schema, each data sample containing:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在[HuggingFace数据集中心](https://huggingface.co/datasets)上有许多可用的指令调优数据集，按任务组织。一些数据集用于[问答](https://huggingface.co/datasets?task_categories=task_categories%3Aquestion-answering)，或者[文本摘要](https://huggingface.co/datasets?task_categories=task_categories%3Asummarization)。在绝大多数情况下，这些数据集都共享相同的基本底层架构，每个数据样本包含：
- en: '*a* ***prompt***, a.k.a. the *instruction*'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*一个* ***提示***，即*指令*'
- en: '*a* ***completion***, a.k.a. the *response*'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*一个* ***完成***，即*回应*'
- en: In this setting, the goal of fine-tuning is to increase (ultimately *maximize*)
    the probability that the LLM will generate the *completion* when given the *prompt*
    as input. In other words, the response “*completes*” the prompt. We rarely, if
    ever, have any interest in altering the probability that the LLM will generate
    the prompt itself… which is just the input to the LLM.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，微调的目标是增加（最终是*最大化*）LLM在输入*提示*时生成*完成内容*的概率。换句话说，回应“*完成*”了提示。我们很少，甚至几乎没有兴趣改变LLM生成提示本身的概率……因为提示只是LLM的输入。
- en: '![](../Images/d7e84fbf0c1f4bafe9f5883f11183b9a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7e84fbf0c1f4bafe9f5883f11183b9a.png)'
- en: Text Summarization Example (image by the author)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要示例（图像来自作者）
- en: Consider [text summarization](https://huggingface.co/datasets?task_categories=task_categories%3Asummarization),
    for instance. A typical *prompt* might consist of an instruction to summarize
    a long news article together with the article itself, and the *completion* would
    be the requested summary (see the [EdinburghNLP/xsum](https://huggingface.co/datasets/EdinburghNLP/xsum)
    dataset on HuggingFace). The goal of fine-tuning a foundation LLM on this dataset
    would be to increase the likelihood that the LLM will generate the summary when
    given the instruction+article, *not* that the LLM will generate the article itself,
    or generate the second half of the article if shown the first half.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以[文本摘要](https://huggingface.co/datasets?task_categories=task_categories%3Asummarization)为例。一个典型的*提示*可能包括一个总结长篇新闻文章的指令，以及文章本身，而*完成内容*则是请求的摘要（参见HuggingFace上的[EdinburghNLP/xsum](https://huggingface.co/datasets/EdinburghNLP/xsum)数据集）。在这个数据集上对基础LLM进行微调的目标是增加LLM在给定指令+文章时生成摘要的可能性，而*不是*生成文章本身，或者如果只显示文章的前半部分，则生成文章的后半部分。
- en: However, a popular approach that has emerged for fine-tuning LLMs on *prompt-completion*
    style datasets is to largely ignore the *prompt-completion* distinction, and fine-tune
    the model on the entire text sequence — basically just continuing the same process
    that was used to pre-train the foundation model, even though instruction tuning
    has a quite different goal from pre-training. This leads to *teaching the LLM
    to generate the prompt as well as the completion*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，针对*提示-完成*风格数据集的LLM微调中，出现了一种流行的方法，就是基本忽略*提示-完成*的区别，直接对整个文本序列进行微调——基本上延续了预训练基础模型时使用的相同过程，尽管指令调优的目标与预训练的目标完全不同。这导致了*教会LLM生成提示以及完成内容*。
- en: 'I’m not entirely sure why this is the case, but most likely this habit was
    simply inherited from older, foundation model training protocols, where there
    was originally no such distinction. From what I can gather, the basic attitude
    seems to be: *well, what’s the harm? Just fine-tune on the entire sequence, and
    the model will still learn to do what you want (to generate the completion given
    the prompt)… it will just learn some extra stuff too.*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我不完全确定为什么会这样，但最可能的原因是，这种习惯仅仅是从旧的基础模型训练协议中继承下来的，最初并没有这种区分。从我所了解的情况来看，基本的态度似乎是：*好吧，这有什么害处呢？只需在整个序列上进行微调，模型仍然会学会你想要的（根据提示生成结果）……它只是也会学到一些额外的东西。*
- en: Prompt-Masking -vs- Prompt-Dampening
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示遮罩 - 与 - 提示削弱
- en: The most obvious solution would be to eliminate (or *zero-mask*) the prompt
    tokens out of the learning process. PyTorch allows for manually masking input
    tokens from training, through the `ignore_index=-100` parameter of the [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)
    function. Setting all the label ids corresponding to the prompt tokens to `-100`forces
    CrossEntropyLoss to ignore these tokens in the loss computation, which results
    in training only on the completion tokens (in my opinion, this is a very poorly
    documented feature — I only stumbled upon it by accident — there’s a reference
    buried in [here](https://huggingface.co/docs/transformers/model_doc/llama2#transformers.LlamaForCausalLM.forward.cache_position)
    in the Llama documentation).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的解决方案是将提示标记从学习过程中剔除（或*零遮罩*）。PyTorch 允许通过 [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)
    函数的 `ignore_index=-100` 参数手动遮罩输入标记。将所有与提示标记对应的标签 ID 设置为 `-100`，迫使 CrossEntropyLoss
    忽略这些标记的损失计算，从而仅对完成标记进行训练（在我看来，这是一个非常文档化不足的功能——我只是偶然发现的——在 [这里](https://huggingface.co/docs/transformers/model_doc/llama2#transformers.LlamaForCausalLM.forward.cache_position)
    的 Llama 文档中埋藏着一处引用）。
- en: By itself, this is not really a solution to prompt-masking. It’s only a means
    for masking arbitrary tokens once those tokens have been located by some other
    means. Some of the prompt-masking references listed earlier employ this technique,
    while others explicitly create a binary-mask to accomplish the same thing. While
    useful, this solution is still a binary switch rather than the continuous dial
    that *prompt-loss-weight* allows.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 单独来看，这并不真正是对提示遮罩的解决方案。它只是通过其他方式找到某些标记后，遮罩这些标记的一种手段。一些之前提到的提示遮罩方法采用了这种技术，而另一些则明确创建了一个二进制遮罩来实现相同的效果。虽然有用，但这个解决方案仍然是一个二进制开关，而不是*prompt-loss-weight*所允许的连续调节器。
- en: 'However, this begs the question: if prompt-masking *does* improve instruction-tuning,
    what’s the point of having a non-zero *prompt-loss-weight* at all? Why would we
    want to merely *dampen* the influence of prompt tokens rather than eliminate it
    completely?'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这引出了一个问题：如果提示遮罩*确实*能够改善指令微调，那么设置一个非零的*prompt-loss-weight*究竟有什么意义呢？为什么我们要仅仅*削弱*提示标记的影响，而不是完全消除它？
- en: 'Recently a paper was posted on *arxiv* titled [Instruction Fine-Tuning: Does
    Prompt Loss Matter?](https://arxiv.org/abs/2401.13586) The authors suggest that
    a small amount of prompt learning may act as a *regularizer* during fine-tuning,
    preventing the model from over-fitting the completion text. They hypothesize:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '最近有一篇论文发布在 *arxiv* 上，标题是 [Instruction Fine-Tuning: Does Prompt Loss Matter?](https://arxiv.org/abs/2401.13586)
    作者建议，少量的提示学习可能作为微调过程中的一种*正则化器*，防止模型过拟合完成文本。他们假设：'
- en: …that [a non-zero] PLW provides a unique regularizing effect that cannot be
    easily replaced with other regularizers…
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …[非零] PLW 提供了一种独特的正则化效果，这种效果无法轻易地通过其他正则化方法替代…
- en: 'Even the folks at OpenAI seem to acknowledge the benefits of using a small
    but non-zero prompt-loss-weight. Apparently they once exposed this very PLW parameter
    through their fine-tuning API, and [there’s still some documentation about it
    online](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#heading=h.8gg2gpi3wek2),
    in which it’s noted that:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是 OpenAI 的人员似乎也承认使用一个小的但非零的提示损失权重（PLW）是有益的。显然，他们曾通过微调 API 曝露了这个 PLW 参数，并且
    [在线上仍然有一些文档](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#heading=h.8gg2gpi3wek2)，其中指出：
- en: a small amount of prompt learning helps preserve or enhance the model’s ability
    to understand inputs (from [Best practices for fine-tuning GPT-3 to classify text](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit))
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 少量的提示学习有助于保持或增强模型理解输入的能力（来自[最佳实践：微调GPT-3进行文本分类](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit)）
- en: although they have since removed this parameter. According to the old [docs](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#heading=h.8gg2gpi3wek2),
    though, they used a default value of `PLW=0.1` (10%), meaning prompt tokens get
    weighted 1/10ᵗʰ as much as completion tokens.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管他们后来移除了这个参数。然而，根据旧版[文档](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#heading=h.8gg2gpi3wek2)，他们使用了`PLW=0.1`（10%）的默认值，这意味着提示词的权重是完成词的1/10。
- en: Generation Ratio
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成比率
- en: 'In the previously mentioned paper ([Instruction Fine-Tuning: Does Prompt Loss
    Matter?](https://arxiv.org/abs/2401.13586)) the authors introduce a useful quantity.
    Given an instruction dataset, they define the G***eneration Ratio***, or ***Rg***:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '在之前提到的论文中（[Instruction Fine-Tuning: Does Prompt Loss Matter?](https://arxiv.org/abs/2401.13586)），作者介绍了一个有用的量。给定一个指令数据集，他们定义了**生成比率**，或称***Rg***：'
- en: the generation ratio **Rg** is the ratio of completion length to prompt length.
    We then divide instruction data into two broad categories. Data with **Rg<1**
    are short-completion data, and data with **Rg >1** are long-completion data. When
    applied to an entire dataset, we take **R̅g** to be the mean completion-prompt
    ratio.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 生成比率**Rg**是完成长度与提示长度的比率。然后我们将指令数据分为两大类。**Rg<1**的数据为短完成数据，**Rg>1**的数据为长完成数据。当应用于整个数据集时，我们取**R̅g**为平均的完成与提示比率。
- en: 'For datasets with small **R̅g** values (i.e. the completion is *shorter* than
    the prompt) they found that PLW actually *does* matter (i.e. using the wrong PLW
    value can degrade performance). And if you think about it, *many* common instruction-tuning
    datasets have this property of having a shorter completion length than prompt
    length, almost by design (think: *text summarization, information extraction*)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有较小**R̅g**值的数据集（即完成部分比提示部分*短*），他们发现PLW确实*很重要*（即使用错误的PLW值会降低性能）。如果你仔细想想，*许多*常见的指令微调数据集实际上具有完成部分比提示部分短的特性，这几乎是有意为之（比如：*文本摘要、信息提取*）
- en: 'As a fun exercise, I computed the **R̅g** values for several popular instruction
    datasets on HuggingFace ([code here](http://gen_ratios.py)):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个有趣的练习，我计算了在HuggingFace上几个流行指令数据集的**R̅g**值（[代码在这里](http://gen_ratios.py)）：
- en: '**7.6** | [Alpaca](https://huggingface.co/datasets/yahma/alpaca-cleaned) (general
    instruction)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**7.6** | [Alpaca](https://huggingface.co/datasets/yahma/alpaca-cleaned)（通用指令）'
- en: '**6.0** | [OpenHermes](https://huggingface.co/datasets/teknium/openhermes)
    (general instruction)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**6.0** | [OpenHermes](https://huggingface.co/datasets/teknium/openhermes)（通用指令）'
- en: '**3.6** | [Python-18k](https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca)
    (code instruction)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3.6** | [Python-18k](https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca)（代码指令）'
- en: '**2.0** | [Databricks-Dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
    (general instruction)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2.0** | [Databricks-Dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)（通用指令）'
- en: '**1.1** | [OpenOrca](https://huggingface.co/datasets/polinaeterna/OpenOrca)
    (general instruction)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1.1** | [OpenOrca](https://huggingface.co/datasets/polinaeterna/OpenOrca)（通用指令）'
- en: '**0.2** | [SAMSum](https://huggingface.co/datasets/knkarthick/samsum) (text
    summarization)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**0.2** | [SAMSum](https://huggingface.co/datasets/knkarthick/samsum)（文本摘要）'
- en: '**0.1** | [XSum](https://huggingface.co/datasets/EdinburghNLP/xsum) (text summarization)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**0.1** | [XSum](https://huggingface.co/datasets/EdinburghNLP/xsum)（文本摘要）'
- en: '**0.01** | [RACE](https://huggingface.co/datasets/ehovy/race) (QA/multiple
    choice)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**0.01** | [RACE](https://huggingface.co/datasets/ehovy/race)（问答/多项选择）'
- en: '![](../Images/e303e9214ea3b772300d4dfb7e898950.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e303e9214ea3b772300d4dfb7e898950.png)'
- en: Mean Generation Ratio (**R**̅g) for some instruction datasets (image by the
    author)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一些指令数据集的平均生成比率（**R**̅g）（图片来自作者）
- en: 'When summarizing any set of values by its average, its good practice to look
    at the full distribution of values as a sanity check. The arithmetic mean can
    be misleading on data that is highly skewed or otherwise deviates from being roughly
    normally distributed. I plotted histograms showing the full **Rg** distribution
    for each dataset (top row). The bottom row shows the same histograms but with
    the x-axis log-scaled:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过平均值总结任何一组数据时，良好的做法是查看数据的完整分布以进行合理性检查。算术均值在数据高度偏斜或偏离大致正态分布时可能具有误导性。我绘制了显示每个数据集**Rg**分布的直方图（顶部行）。底部行显示了相同的直方图，但x轴进行了对数缩放：
- en: '![](../Images/e3c3bb49517c6bbfaeaf2c2a52ad25e8.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3c3bb49517c6bbfaeaf2c2a52ad25e8.png)'
- en: Linear and Log-scaled **Rg** Histograms (image by the author)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 线性和对数缩放的**Rg**直方图（图片来自作者）
- en: 'These plots suggest that when a dataset’s **Rg** distribution covers multiple
    orders of magnitude or has non-negligible representation in both the **Rg>1**
    and **Rg<1** regions (such as in the case with [OpenOrca](https://huggingface.co/datasets/polinaeterna/OpenOrca)
    and other datasets with **R̅g>1)** the distribution can become highly skewed.
    As a result, the arithmetic mean may be disproportionately influenced by larger
    values, potentially misrepresenting the distribution’s central tendency. In such
    cases, computing the mean in log-space (then optionally transforming it back to
    the original scale) might provide a more meaningful summary statistic. In other
    words, it could make sense to use the *geometric mean*:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表表明，当数据集的**Rg**分布跨越多个数量级或在**Rg>1**和**Rg<1**区域都有显著表示（例如在[OpenOrca](https://huggingface.co/datasets/polinaeterna/OpenOrca)和其他**R̅g>1**的数据集中），分布可能会高度偏斜。因此，算术均值可能会受到较大值的不成比例影响，从而可能会误导分布的集中趋势。在这种情况下，在对数空间中计算均值（然后可以选择将其转换回原始尺度）可能会提供更有意义的汇总统计量。换句话说，使用*几何均值*可能更为合理：
- en: The RACE Reading Comprehension Dataset
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RACE阅读理解数据集
- en: Based on the above **R̅g** table, I decided the [RACE **R**e**A**ding **C**omprehension
    Dataset from **E**xaminations](https://huggingface.co/datasets/ehovy/race) (**R̅g=0.01**)
    would be a good candidate for investigation. Multiple choice QA seemed like an
    ideal test-bed for exploring the effects of prompt-masking, since the prompt is
    naturally very long relative to the completion. Regardless of prompt length, the
    completion is *always* 1 character long, namely ***A***, ***B***, ***C*** or ***D***
    (if you ignore special tokens, delimiters, etc). My hunch was that *if* there
    are any effects from modulating prompt token weights, they would certainly be
    noticeable here.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上面的**R̅g**表格，我决定将[RACE **R**e**A**ding **C**omprehension Dataset from **E**xaminations](https://huggingface.co/datasets/ehovy/race)（**R̅g=0.01**）作为一个很好的研究对象。多项选择的QA似乎是探索提示屏蔽效果的理想测试平台，因为与完成任务相比，提示通常非常长。无论提示长度如何，完成任务*始终*只有1个字符，即***A***、***B***、***C***或***D***（如果忽略特殊标记、分隔符等）。我的直觉是，*如果*有任何来自调节提示标记权重的影响，它们肯定会在这里显现出来。
- en: 'As stated in the [*dataset card*](https://huggingface.co/datasets/ehovy/race#dataset-card-for-race):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[*数据集卡片*](https://huggingface.co/datasets/ehovy/race#dataset-card-for-race)中所述：
- en: RACE is a large-scale reading comprehension dataset with more than 28,000 passages
    and nearly 100,000 questions. The dataset is collected from English examinations
    in China, which are designed for middle school and high school students. The dataset
    can be served as the training and test sets for machine comprehension.
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: RACE是一个大规模的阅读理解数据集，包含超过28,000篇文章和近100,000个问题。该数据集来源于中国的英语考试，面向中学和高中学生。该数据集可以用作机器理解的训练集和测试集。
- en: 'The QA schema is simple: the prompt presents a *question*, possibly some context
    (the *article* field), and then lists four *options*. The completion (*answer*)
    is always one of: A, B, C, D. This [dataset viewer](https://huggingface.co/datasets/ehovy/race/viewer/all/train)
    hosted on HuggingFace allows browsing the full set, but here’s a small example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: QA模式很简单：提示提供一个*问题*，可能会有一些上下文（*文章*字段），然后列出四个*选项*。完成（*答案*）总是其中之一：A、B、C、D。这个[数据集查看器](https://huggingface.co/datasets/ehovy/race/viewer/all/train)托管在HuggingFace上，允许浏览完整的数据集，但这里是一个小例子：
- en: '![](../Images/11d3acd64e1dcc054e819da9cdc05dec.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11d3acd64e1dcc054e819da9cdc05dec.png)'
- en: RACE example (screenshot from [https://huggingface.co/datasets/ehovy/race/viewer/all/train](https://huggingface.co/datasets/ehovy/race/viewer/all/train))
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: RACE示例（截图来自[https://huggingface.co/datasets/ehovy/race/viewer/all/train](https://huggingface.co/datasets/ehovy/race/viewer/all/train))
- en: Cross Entropy Loss
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉熵损失
- en: Before we jump into the full implementation of *prompt-loss-weight*, and try
    it out on the RACE data, we need a basic understanding of loss and where it comes
    from. Simply put, loss is a measure of how well our model (LLM) “fits” (explains,
    predicts) our data. During fine-tuning (and also pre-training), we “move” the
    model closer to the data by tweaking the network weights in such a way that decreases
    the loss. The [chain rule (of calculus)](https://en.wikipedia.org/wiki/Chain_rule)
    gives us a precise algorithm for computing these tweaks, given the loss function
    and the network architecture.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始全面实施*提示-损失权重*并尝试在RACE数据集上进行实验之前，我们需要先了解损失的基本概念及其来源。简单来说，损失是衡量我们的模型（LLM）如何“拟合”（解释、预测）数据的一个指标。在微调（以及预训练）过程中，我们通过调整网络权重使模型更接近数据，从而减少损失。[链式法则（微积分）](https://en.wikipedia.org/wiki/Chain_rule)为我们提供了一个精确的算法，用于在给定损失函数和网络架构的情况下计算这些调整。
- en: The most common loss function in LLM fine-tuning is called *Cross Entropy Loss*
    (CEL). For this reason, most discussions of CEL are framed around the definition
    of [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy), which comes from
    information theory. While it’s true that “cross-entropy” is right there in the
    name, a more intuitive understanding can be achieved when approaching CEL through
    the lens of *maximum likelihood estimation* (MLE). I’ll try to explain it from
    both angles.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM微调中，最常见的损失函数叫做*交叉熵损失*（CEL）。因此，大多数关于CEL的讨论都是围绕[交叉熵](https://en.wikipedia.org/wiki/Cross-entropy)的定义展开的，而交叉熵来源于信息论。虽然“交叉熵”在名称中就提到了，但通过*最大似然估计*（MLE）的角度来看，更直观的理解可以帮助我们理解CEL。我会从两个角度来尝试解释。
- en: We have already established that LLMs are wired for *next token prediction.*
    What this means is that the LLM is basically just a mathematical function that
    takes as input a sequence of tokens, and outputs a *conditional probability distribution
    for the next token* over the entire token vocabulary **V**. In other words, it
    outputs a vector of probability values of dimension **|V|** that sums to 1\. (in
    set notation **|S|** denotes the number of elements, or *cardinality*, of set
    **S**)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经明确LLM是为了*下一个令牌预测*而设计的。这意味着LLM本质上是一个数学函数，它将令牌序列作为输入，并输出整个令牌词汇**V**中下一个令牌的*条件概率分布*。换句话说，它输出一个维度为**|V|**的概率值向量，并且这些概率的总和为1。（在集合符号中，**|S|**表示集合**S**的元素数量，或称为*基数*）
- en: 'Let’s take a small toy example to illustrate how this works. Imagine that our
    training data contains the 4-token sequence: `The bird flew away`. Given the first
    3 tokens (`The bird flew`), an LLM might output the following vector of probabilities
    for every possible 4ᵗʰ token — for the sake of simplicity, we’ll imagine that
    the 5 candidate tokens listed (in magenta) are the only possibilities (i.e. **|V|**=5).
    The function ***p(***⋅***)*** represents the conditional probabilities output
    by the LLM (notice they sum to 1):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个小的玩具例子来说明这个过程。假设我们的训练数据包含以下4个令牌序列：`The bird flew away`。给定前3个令牌（`The bird
    flew`），LLM可能会输出每一个可能的第4个令牌的概率向量——为了简单起见，我们假设列出的5个候选令牌（用洋红色标出）是唯一的可能性（即**|V|**=5）。函数***p(***⋅***)***表示LLM输出的条件概率（注意它们的和为1）：
- en: '![](../Images/90250fa83a062b9d6e7288751b823954.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90250fa83a062b9d6e7288751b823954.png)'
- en: (image by the author)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来自作者）
- en: When training (or fine-tuning) an LLM on a token sequence, we step through the
    sequence token-by-token and compare the *next-token-distribution* generated by
    the LLM to the *actual next token* in the sequence, and from there we calculate
    the CEL for that token.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练（或微调）LLM时，我们会逐个遍历令牌序列，比较LLM生成的*下一个令牌分布*与序列中的*实际下一个令牌*，然后基于此计算该令牌的CEL。
- en: Notice here that the actual 4ᵗʰ token in the sequence (`away`) does *not* have
    the highest probability in the table. During training, we would like to tweak
    the weights slightly so as to increase the probability of `away`, while decreasing
    the others. The *key* is having the right loss function… it allows us to compute
    exactly how much to tweak each weight, for each token.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的是，序列中的实际第4个令牌（`away`）在表中的概率并*不是*最高的。在训练过程中，我们希望稍微调整权重，以增加`away`的概率，同时减少其他令牌的概率。*关键*是选择正确的损失函数……它可以帮助我们精确计算每个令牌的每个权重需要调整多少。
- en: Once the loss is computed for each token, the final loss is computed as the
    *average per-token-loss over all tokens*. But first we must establish the formula
    for this per-token-loss.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出每个 token 的损失，最终的损失将计算为所有 token 的*每 token 损失的平均值*。但首先，我们必须为这个每 token 的损失建立公式。
- en: Information Theory Interpretation
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息论解释
- en: Continuing the toy problem, to compute CEL for the 4ᵗʰ token position, we compare
    the *actual* 4ᵗʰ token to the generated distribution ***p(***⋅***)*** over all
    5 *possible* 4ᵗʰ tokens. In fact, we treat the actual 4ᵗʰ token as a distribution
    ***q(***⋅***)*** in its own right (albeit a degenerate one) that has a value of
    1 for the token appearing in the data -`away`- and a value of 0 for all other
    possible 4ᵗʰ tokens (this is sometimes called *one-hot encoding*).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 继续这个玩具问题，要计算第四个 token 位置的 CEL，我们将*实际的*第四个 token 与所有五个*可能的*第四个 token 的生成分布 ***p(***⋅***)***
    进行比较。事实上，我们将实际的第四个 token 视为一个分布 ***q(***⋅***)***（尽管它是一个退化的分布），其在数据 -`away`- 中出现的
    token 值为 1，其他所有可能的第四个 token 的值为 0（这有时称为*独热编码*）。
- en: '![](../Images/81d0d62dc8f46ee4732ffc2a1d94ae0c.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81d0d62dc8f46ee4732ffc2a1d94ae0c.png)'
- en: (image by the author)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由作者提供）
- en: 'The reason we contort the training data into this strange *one-hot* encoded
    probability representation ***q(***⋅***)*** is so we can apply the formula for
    *c****ross-entropy***, which is a measure of the *divergence* between two discrete
    probability distributions (BTW, not symmetric w.r.t. q,p):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练数据变形成这种奇怪的*独热*编码的概率表示 ***q(***⋅***)*** 的原因，是为了能够应用 *交叉熵* 公式，这是衡量两个离散概率分布之间*散度*的一种方法（顺便提一下，它对
    q 和 p 不是对称的）：
- en: '![](../Images/22ded3dd0a7b6c664adecd0f0a103d4a.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22ded3dd0a7b6c664adecd0f0a103d4a.png)'
- en: 'where *x* indexes over all possible states (i.e. 5 tokens). This works out
    to:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x* 索引所有可能的状态（即 5 个 token）。这可以表示为：
- en: '![](../Images/f9fbd5b86aad874b097af917647bc44c.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9fbd5b86aad874b097af917647bc44c.png)'
- en: So basically CEL is just using the ***q*** vector to select from the ***p***
    vector the single value corresponding to the token that *actually* appears in
    the data -`away`- (i.e. multiplying it by 1), and throwing away all other values
    (i.e. multiplying by 0). So we are indexing over all possible states (tokens)
    only to select one and ignore the rest.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，基本上 CEL 就是使用 ***q*** 向量从 ***p*** 向量中选择与数据中*实际*出现的 token -`away`- 对应的单个值（即将其乘以
    1），并丢弃所有其他值（即乘以 0）。因此，我们只是遍历所有可能的状态（token），只选择一个并忽略其他所有。
- en: MLE Interpretation
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大似然估计（MLE）解释
- en: When fine-tuning an LLM, we seek the LLM weights θ that maximize the probability
    of the training data given those weights, often called the *likelihood* of the
    weights ℒ(θ) = ℙ(D|θ). And so we require an expression for this quantity. Luckily,
    there’s an easy way to compute this from next token probabilities, which the LLM
    already gives us.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调 LLM 时，我们寻求最大化在给定这些权重下训练数据的概率的 LLM 权重 θ，这通常称为这些权重的*似然* ℒ(θ) = ℙ(D|θ)。因此，我们需要一个表达式来计算这一数量。幸运的是，LLM
    已经为我们提供了从下一个 token 概率中计算这一点的简便方法。
- en: 'Starting with the *other* [chain rule (of probability)](https://en.wikipedia.org/wiki/Chain_rule_(probability)),
    we decompose the joint probability of a token sequence **S** into a *product of
    conditional probabilities*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从*另一个* [链式法则（概率）](https://en.wikipedia.org/wiki/Chain_rule_(probability))开始，我们将
    token 序列 **S** 的联合概率分解为*条件概率的乘积*：
- en: '![](../Images/662b2b28ff0a03a1ab0eb95b7d2d0af2.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/662b2b28ff0a03a1ab0eb95b7d2d0af2.png)'
- en: Chain Rule (probability)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则（概率）
- en: This decomposition establishes the connection between next-token-prediction
    and the joint probability of the full token sequence — the joint probability is
    just the product of all the conditionals.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分解建立了下一个 token 预测与完整 token 序列联合概率之间的联系——联合概率只是所有条件概率的乘积。
- en: 'Using *i* to index over the tokens of a token sequence ***S*** *= (t₁,t₂,t₃,…,
    tᵢ ,…)*, we’ll use the following shorthand to denote the conditional probability
    output by an LLM for the *iᵗʰ* token in a sequence, given the LLM weights θ and
    the previous *i-1* tokens:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *i* 索引 token 序列 ***S*** *= (t₁,t₂,t₃,…, tᵢ ,…)* 中的 token，我们将使用以下简写来表示 LLM
    为序列中第 *i* 个 token 输出的条件概率，给定 LLM 权重 θ 和前 *i-1* 个 token：
- en: '![](../Images/a9c88478067a2b32bf304997b5fef52e.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9c88478067a2b32bf304997b5fef52e.png)'
- en: It should be emphasized that *pᵢ* is **not** a vector here (i.e. a distribution
    over all possible next tokens) but represents only the probability computed for
    the actual *iᵗʰ* token, i.e. the yellow highlighted row in the above example.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 应该强调的是，*pᵢ*在这里**不是**一个向量（即不是对所有可能的下一个标记的分布），而只是表示为实际的*iᵗʰ*标记计算的概率，即上面示例中的黄色高亮行。
- en: 'If we take the logarithm of the joint probability of a sequence, a product
    becomes a sum (since log is monotonic, this doesn’t affect optimization):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们取序列的联合概率的对数，乘积会变成和（因为对数是单调的，这不会影响优化）：
- en: '![](../Images/0d0d77cf33b63e3499587c73e8d0518b.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d0d77cf33b63e3499587c73e8d0518b.png)'
- en: 'Now we can connect the final sum-of-logs expression (right here☝)️ to the formula
    for *Average Cross Entropy Loss* ***L*** over a token sequence:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将最终的对数和表达式（就在这里☝）️与标记序列上的*平均交叉熵损失* ***L*** 的公式连接起来：
- en: '![](../Images/1273a070cbec685a1eaedc31f0bed68a.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1273a070cbec685a1eaedc31f0bed68a.png)'
- en: which is the causal language model objective function. Often the “*Average”*
    is dropped from the name, and it’s just called “*Cross Entropy Loss*,” but it’s
    good to remember that CEL is technically computed at the token level, and then
    averaged across tokens. From this final expression it should hopefully be clear
    that *minimizing the CEL* is equivalent to *maximizing the probability of the
    token sequence*, which is what MLE seeks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是因果语言模型的目标函数。通常，名称中的“*平均*”会被省略，直接称为“*交叉熵损失*”，但记住，从技术上讲，CEL是按标记级别计算的，然后对所有标记进行平均。希望从这个最终表达式中可以清楚地看出，*最小化CEL*等同于*最大化标记序列的概率*，这正是MLE所追求的目标。
- en: 'One convenience resulting from the form of this expression is that it is very
    easy to modify if we want to compute the loss over *any subset* of the tokens.
    Recall that we may sometimes be interested in finding the LLM weights θ that maximize
    the probability of the completion given the prompt:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表达式的一大便利之处在于，如果我们想要计算*任何子集*标记的损失，修改起来非常容易。回想一下，我们有时可能有兴趣找到能够最大化给定提示下完成的概率的LLM权重θ：
- en: '![](../Images/f48bcca2c0d444a7816af6bea26f8ba1.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f48bcca2c0d444a7816af6bea26f8ba1.png)'
- en: 'We could easily adjust the loss for this scenario by simply averaging only
    over the completion tokens. If we use “𝕀c”todenote theset of all completion token
    indices, then we can express *completion loss* as:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过仅对完成标记进行平均，轻松调整此情形下的损失。如果我们使用“𝕀c”表示所有完成标记的索引集，那么我们可以将*完成损失*表示为：
- en: '![](../Images/8c7af5c95c0b21617495b92bf0560582.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c7af5c95c0b21617495b92bf0560582.png)'
- en: Since the loss for each token is already conditioned on all previous tokens
    in the sequence, this means that the prompt is automatically accounted for in
    the conditional, even if we average over completion tokens only.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个标记的损失已经基于序列中所有先前的标记进行了条件化，这意味着即使我们只对完成标记进行平均，提示也会自动包含在条件中。
- en: Prompt Loss Weight
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示损失权重
- en: 'Now that we have established CEL as an *average* of per-token losses over a
    token sequence*,* we can define the *weighted average* version of CEL:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定CEL是标记序列上每个标记损失的*平均*值，我们可以定义CEL的*加权平均*版本：
- en: '![](../Images/695a0bfe03541c9f8f5e9b7fc9968c4e.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/695a0bfe03541c9f8f5e9b7fc9968c4e.png)'
- en: Depending how we set the weights *wᵢ*, we can use this formula to define multiple
    losses. For example, if we set all weights *wᵢ =1* then we recover the standard,
    full sequence CEL from before. However, if we set *wᵢ =1* only for completion
    tokens, and *wᵢ = 0* for prompt tokens, then we get *completion loss*. And likewise,
    *prompt loss* is defined by setting *wᵢ =1* only over prompt tokens, and *wᵢ =
    0* otherwise.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们如何设置权重*wᵢ*，我们可以使用这个公式定义多种损失。例如，如果我们将所有权重*wᵢ =1*，那么我们就恢复了之前的标准全序列CEL。然而，如果我们仅对完成标记设置*wᵢ
    =1*，对提示标记设置*wᵢ = 0*，那么我们得到*完成损失*。同样，*提示损失*通过仅在提示标记上设置*wᵢ =1*，其他情况下设置*wᵢ = 0*来定义。
- en: Since we rarely (if ever) want to down-weight the completion tokens, we fix
    the completion token weights at *wᵢ =1*, but for the prompt tokens we can define
    a continuous value on the [0:1] interval called `prompt_loss_weight`. This way
    we can tune how much to weight the prompt tokens during training, from *wᵢ = 0*
    (completion loss) all the way to *wᵢ =1* (standard full sequence loss). Or, we
    could even use *wᵢ =0.1* to give the prompt tokens a small but non-zero weight.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们很少（如果有的话）希望对完成令牌进行下调权重，我们将完成令牌的权重固定为*wᵢ =1*，但对于提示令牌，我们可以在[0:1]区间上定义一个连续值，称为`prompt_loss_weight`。这样，我们可以在训练过程中调节提示令牌的权重，从*wᵢ
    = 0*（完成损失）一直到*wᵢ = 1*（标准完整序列损失）。或者，我们甚至可以使用*wᵢ = 0.1*，为提示令牌赋予一个小但非零的权重。
- en: Loss Implementation
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失实现
- en: Let’s take a look under the hood at how loss is normally computed in the [HuggingFacetransformers](https://huggingface.co/docs/transformers/en/index#-transformers)
    package. Since we’ll be fine-tuning the [Llama-2–7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)
    model in our experiments, we’ll look at [LlamaForCausalLM](https://github.com/huggingface/transformers/blob/52920b5dd5ad3b5e94209ef392ab5ceccbb1c869/src/transformers/models/llama/modeling_llama.py#L1104),
    specifically at the [forward pass](https://github.com/huggingface/transformers/blob/52920b5dd5ad3b5e94209ef392ab5ceccbb1c869/src/transformers/models/llama/modeling_llama.py#L1216),
    where loss is computed during training.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解损失是如何在[HuggingFace Transformers](https://huggingface.co/docs/transformers/en/index#-transformers)包中通常计算的。由于我们将在实验中微调[Llama-2–7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)模型，我们将查看[LlamaForCausalLM](https://github.com/huggingface/transformers/blob/52920b5dd5ad3b5e94209ef392ab5ceccbb1c869/src/transformers/models/llama/modeling_llama.py#L1104)，特别是[前向传播](https://github.com/huggingface/transformers/blob/52920b5dd5ad3b5e94209ef392ab5ceccbb1c869/src/transformers/models/llama/modeling_llama.py#L1216)，其中在训练过程中计算损失。
- en: 'Recall that loss is a way of comparing each *actual* token to the LLM’s *prediction*
    for that token (given the preceding actual tokens) — and so the loss function
    needs access to these two data structures. In this case, loss is fed two tensors:
    `logits`and `labels`. The `labels` tensor holds the actual tokens (*token ids*
    to be exact). The`logits` tensor holds the predicted next-token-probabilities,
    prior to *softmax* normalization (which forces them to sum to 1 — it turns out
    that it’s more efficient to leave these values in their raw, pre-normalized form).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，损失是将每个*实际*令牌与LLM对该令牌的*预测*进行比较的一种方式（给定前面的实际令牌）——因此损失函数需要访问这两个数据结构。在这种情况下，损失输入两个张量：`logits`和`labels`。`labels`张量保存实际令牌（准确来说是*令牌ID*）。`logits`张量保存预测的下一个令牌概率，在*softmax*归一化之前（归一化会将这些概率的和强制为1——事实证明，保持这些值在原始、未经归一化的形式中更为高效）。
- en: 'The `logits` tensor is 3D, with shape `[B,N,|V|]`, where `B` is batch size,
    `N` is sequence length (in tokens), and `|V|` is token vocabulary size. The 2D
    `labels` tensor just contains the token sequence itself, so it has shape `[B,N]`.
    Here is the key section of code where CEL is normally computed:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`logits`张量是3维的，形状为`[B,N,|V|]`，其中`B`是批量大小，`N`是序列长度（单位为令牌），`|V|`是令牌词汇大小。2维的`labels`张量仅包含令牌序列本身，因此它的形状是`[B,N]`。以下是通常计算CEL（交叉熵损失）的关键代码部分：'
- en: '[PRE0]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For each position *i* along the 2nd dimension of `logits`, this tensor contains
    probabilities for predicting the *next* token (token *i+1*) given all the preceding
    tokens up *through* the *i*ᵗʰ token. These probabilities need to be compared to
    the actual *i+1*ˢᵗ token in `labels`. This is why the *shift-by-1* happens in
    the first several lines — to bring these two values into alignment for each token.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`logits`的第2维度上每个位置的*i*，这个张量包含了在给定所有前面的令牌（直到第*i*个令牌）的情况下，预测*下一个*令牌（令牌*i+1*）的概率。这些概率需要与`labels`中实际的*i+1*ˢᵗ令牌进行比较。这就是为什么在前几行中会发生*偏移-1*的原因——为了使这两个值在每个令牌上对齐。
- en: '![](../Images/b7d018f5390df31764fa30e84168a1a4.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7d018f5390df31764fa30e84168a1a4.png)'
- en: '(image by the author, inspired by: [https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2--Vmlldzo1NjY0MjE1](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2--Vmlldzo1NjY0MjE1))'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: （图像来源：作者，灵感来自：[https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2--Vmlldzo1NjY0MjE1](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2--Vmlldzo1NjY0MjE1)）
- en: What happens next is just that the first 2 dimensions are combined into 1 (flattened),
    and the tensors are passed to `[CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`,
    a PyTorch function, which outputs the final loss value.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来发生的事情是，前两个维度被合并成一个（展平），然后张量被传递给 `[CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`，这是一个
    PyTorch 函数，它输出最终的损失值。
- en: Custom Loss Function
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义损失函数
- en: 'By default, `[CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`
    averages over all tokens to output a single scalar value. This final averaging
    (over all tokens) is called a *reduction* operation. But if we instantiate the
    loss with *no* reduction operation:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`[CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`
    会对所有标记进行平均，以输出一个单一的标量值。这一最终的平均（对所有标记的平均）称为*归约*操作。但如果我们用*无*归约操作来实例化损失：
- en: '[PRE1]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: then *no* averaging will be done, and the final loss would instead be a 1-D
    tensor (of length `BxN`) containing the losses for each token (the loss tensor
    would be 2D, shape `[B,N]`, without the prior flattening step). That is how we
    get access to the per-token losses to compute our own *weighted* average.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 那么*不*进行平均，最终的损失将是一个一维张量（长度为 `BxN`），包含每个标记的损失（损失张量将是二维的，形状为 `[B,N]`，没有先前的展平步骤）。这就是我们如何获得每个标记的损失，以便计算我们自己的*加权*平均值。
- en: 'During tokenization ([see full code for details](https://github.com/davidsvaughn/prompt-loss-weight))
    we create two additional binary masks for each sequence, the *prompt mask* and
    the *completion mask.* A binary mask is just a vector of ones and zeros. The prompt
    mask marks all the prompt tokens with 1s (0s otherwise) and the completion mask
    does the opposite. Then we can use a simple linear combination of these two masks
    to get the weights *wᵢ* for the weighted average version of CEL, multiplying the
    prompt mask by PLW and adding to the completion mask:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词过程中（[查看完整代码详情](https://github.com/davidsvaughn/prompt-loss-weight)），我们为每个序列创建了两个额外的二进制掩码，即*提示掩码*和*完成掩码*。二进制掩码仅仅是由
    1 和 0 组成的向量。提示掩码将所有提示标记标记为 1（否则为 0），而完成掩码则相反。然后，我们可以通过这两个掩码的简单线性组合，得到加权平均版本的 CEL
    的权重 *wᵢ*，即将提示掩码乘以 PLW 并加到完成掩码上：
- en: '![](../Images/92685972975d64282b6f7fbd95bec926.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92685972975d64282b6f7fbd95bec926.png)'
- en: loss weights = prompt_loss_weight * prompt_mask + completion_mask (image by
    the author)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 损失权重 = prompt_loss_weight * prompt_mask + completion_mask（图像由作者提供）
- en: 'We subclass from HuggingFace [Trainer](https://huggingface.co/docs/transformers/en/main_classes/trainer)
    to define a new trainer class called `PLWTrainer`. We’ll start by overriding just
    two functions:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 HuggingFace 的 [Trainer](https://huggingface.co/docs/transformers/en/main_classes/trainer)
    子类化，定义了一个名为 `PLWTrainer` 的新训练器类。我们将首先重写两个函数：
- en: '`__init__()`: constructor receives extra `prompt_loss_weight`parameter'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__init__()`: 构造函数接收额外的 `prompt_loss_weight` 参数'
- en: '`compute_loss()`: computes weighted loss using `prompt_loss_weight`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute_loss()`: 使用 `prompt_loss_weight` 计算加权损失'
- en: '[PRE2]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If no explicit value is passed to the constructor for `prompt_loss_weight`,
    the default value (`prompt_loss_weight=1`) means we revert to the inherited behavior
    of the original [Trainer](https://huggingface.co/docs/transformers/en/main_classes/trainer)
    (i.e. minimizing full sequence loss). However, if we pass in other values for
    `prompt_loss_weight`, we get back a whole spectrum of different loss functions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果构造函数没有显式传入 `prompt_loss_weight` 的值，默认值（`prompt_loss_weight=1`）意味着我们会恢复到原始
    [Trainer](https://huggingface.co/docs/transformers/en/main_classes/trainer) 的继承行为（即最小化完整序列的损失）。然而，如果我们传入其他的
    `prompt_loss_weight` 值，我们就可以得到一系列不同的损失函数。
- en: We’re almost ready to try our new loss function! But first we need to make sure
    we’re equipped to observe and understand what effect it’s having on the fine-tuning
    process, if any…
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎准备好尝试新的损失函数了！但在此之前，我们需要确保能够观察和理解它对微调过程产生的影响（如果有的话）…
- en: Validation Metrics
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证指标
- en: Tracking Prompt & Completion Losses Separately
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分别跟踪提示和完成的损失
- en: 'During fine-tuning, it is common practice to track model performance on a *hold-out*
    set in order to decide when to end training. The *hold-out* set, also called the
    *validation set*, is just a random subset of data that is literally “held-out”
    from the training data to ensure it isn’t learned/memorized by the model. The
    model’s performance on this set is seen as a proxy/estimate for how the model
    would perform in the real-world on new, unseen data. This is where the classic
    “training vs. validation curve” taught in most intro ML courses comes from:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，通常的做法是跟踪模型在*验证集*上的表现，以决定何时结束训练。*验证集*，也称为*保留集*，是从训练数据中“保留”的一个随机子集，以确保模型不会学习或记住它。模型在此集上的表现被视为模型在实际应用中面对新数据时的表现估计。这就是大多数初级机器学习课程中所讲的经典“训练与验证曲线”产生的原因：
- en: '![](../Images/7833346133535cecf9c7e0bde26846c3.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7833346133535cecf9c7e0bde26846c3.png)'
- en: (image by the author)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来自作者）
- en: 'The lesson here is that the minimum point of the green (validation) curve represents
    the *optimal* number of training steps, past which the model starts to *overfit*,
    or memorize, the training data, rather than continuing to learn generalizable
    patterns from the data. It’s impossible to know the *true* optimal stopping point,
    but tracking validation set metrics allows us to estimate it fairly well. Still,
    there is a trade-off: a larger validation set leads to a better estimate, but
    also leads to a smaller training set, so we don’t want to hold-out too many samples.
    5%–15% is a good rule-of-thumb.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的教训是，绿色（验证）曲线的最小点代表了*最佳*的训练步数，超过这个点，模型开始*过拟合*或记住训练数据，而不是继续从数据中学习可泛化的模式。虽然无法知道*真正*的最佳停止点，但通过跟踪验证集上的度量，我们可以相对准确地估计它。然而，这也有一个权衡：更大的验证集会带来更好的估计，但也意味着训练集会变小，所以我们不希望保留过多的样本。5%–15%是一个很好的经验法则。
- en: 'Typically, when fine-tuning LLMs, the objective loss function being minimized
    on the training set also becomes the default metric used to track the validation
    set performance, and thus determine the optimal stopping point. The discussion
    usually centers around **two options**:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在微调大型语言模型（LLM）时，最小化训练集上的目标损失函数也成为用于跟踪验证集表现的默认度量，从而确定最佳停止时机。讨论通常集中在**两种选择**上：
- en: Minimize *full sequence loss* on train set — and track it on validation set
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化训练集上的*完整序列损失*——并在验证集上跟踪
- en: Minimize *completion loss* on train set — and track it on validation set
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化训练集上的*完成损失*——并在验证集上跟踪
- en: 'But — we’re free to track *any* metric (or metrics) we want on the validation
    set, not just the loss being used as the training objective . This leads to the
    original idea that inspired this article — I wanted to try a **third option**:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 但是——我们可以自由地在验证集上跟踪*任何*度量（或多个度量），不仅仅是用作训练目标的损失。这引出了启发本文的最初想法——我想尝试**第三种选择**：
- en: '![](../Images/423fd1c1435ec4c639a2c13363cc66fa.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/423fd1c1435ec4c639a2c13363cc66fa.png)'
- en: 'However, after re-framing my approach around PLW, this evolved into:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在围绕PLW调整我的方法后，这一过程演变成了：
- en: '![](../Images/4110bdc6e30da009b53840226b43f8bc.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4110bdc6e30da009b53840226b43f8bc.png)'
- en: To do this, we first need to write a custom metric to decompose validation full
    sequence loss into prompt loss and completion loss, which we do in the next section.
    We’ll use the same tricks we used in our custom loss function.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们首先需要编写一个自定义度量，将验证的完整序列损失分解为提示损失和完成损失，我们将在下一节中进行说明。我们将使用在自定义损失函数中使用过的相同技巧。
- en: '***Digression***: you may notice in the LLM community that practitioners sometimes
    sidestep the *stopping criteria* issue altogether by following a simple rule like
    *always fine-tune for one epoch only*, or something similar. Sometimes this makes
    sense, like when fine-tuning a model to produce text that’s more *subjective*,
    like emails, or poetry, or jokes. But when the fine-tuning dataset is aimed more
    at *correctness*, like writing code, solving math problems, or multiple choice
    QA (an example we will see below), then it definitely *does* make sense to monitor
    the validation loss, and/or other validation metrics. So it’s important to make
    sure we do it carefully.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '***插曲***：你可能会注意到，在大型语言模型（LLM）社区中，实践者有时完全绕过了*停止准则*问题，遵循像*仅微调一个周期*之类的简单规则。有时这很有道理，比如当微调模型生成更多*主观*内容时，如电子邮件、诗歌或笑话。但是当微调数据集更侧重于*正确性*时，比如编写代码、解决数学问题或多选问答（我们将在下文中看到的一个例子），那么确实有必要监控验证损失和/或其他验证度量。所以我们需要确保谨慎操作。'
- en: However, this is not to say that the *correctness* of a token sequence is a
    simple linear function of individual token correctness. The semantic meaning of
    a token sequence can be a complex, highly *non-linear* function of the meaning
    of the individual tokens. That’s why it’s easy to construct many examples where
    one tiny change at the token level can dramatically alter the meaning of the whole
    — just insert “not” at the right place to completely invert the meaning of a sentence!.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不意味着标记序列的*正确性*是单个标记正确性的简单线性函数。标记序列的语义意义可以是个复杂的、高度*非线性*的函数，依赖于各个标记的意义。这就是为什么很容易构造出许多例子，其中标记级别的一个微小变化可以显著改变整个句子的意义——只需在合适的位置插入“not”就能完全颠倒句子的意思！
- en: Even so, in many cases the average per-token loss can still serve as a good
    indicator for the overall quality of LLM predictions during training/fine-tuning.
    This is because the standard practice of [teacher forcing](https://en.wikipedia.org/wiki/Teacher_forcing)
    ensures that each token prediction is conditioned on the “correct” (i.e. ground
    truth) previous tokens from the train/validation data, as opposed to conditioning
    each token prediction on the model’s own previous token predictions (which is
    what happens during inference/text-generation).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，在许多情况下，平均每个标记的损失仍然可以作为训练/微调过程中LLM预测整体质量的一个良好指标。这是因为[教师强制](https://en.wikipedia.org/wiki/Teacher_forcing)的标准做法确保每个标记的预测是基于训练/验证数据中“正确”（即地面真实值）的前一个标记，而不是基于模型自身的前一个标记预测（这是推理/文本生成过程中发生的情况）。
- en: But no single metric is perfect, which is why it’s always important to use multiple
    evaluation methods, including task-specific metrics, along with human evaluation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 但没有一个度量是完美的，这就是为什么始终使用多种评估方法，包括特定任务的度量和人工评估，变得非常重要。
- en: Defining Custom `Metrics`
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义自定义`度量`
- en: A common method for defining custom validation metrics, when using [HuggingFace
    Trainer](https://huggingface.co/docs/transformers/en/main_classes/trainer), is
    to override Trainer’s default `compute_metrics()` function that is periodically
    run on the validation set during training. However, this function does not, by
    default, receive enough information for computing prompt loss or completion loss.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义验证度量的常见方法之一，在使用[HuggingFace Trainer](https://huggingface.co/docs/transformers/en/main_classes/trainer)时，是重写Trainer的默认`compute_metrics()`函数，该函数在训练期间会定期在验证集上运行。然而，默认情况下，这个函数并不会接收到足够的信息来计算提示损失或完成损失。
- en: Specifically, for each validation set sequence `compute_metrics()` receives
    the *predicted* tokens and the *actual* tokens. This is only suitable for computing
    certain metrics like token accuracy, but not for computing loss. Luckily, we can
    tinker with the data that’s passed into `compute_metrics()` by overriding another
    function, `preprocess_logits_for_metrics()`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，对于每个验证集序列，`compute_metrics()`接收到的是*预测*标记和*实际*标记。这仅适用于计算某些度量，例如标记准确性，但不适用于计算损失。幸运的是，我们可以通过重写另一个函数`preprocess_logits_for_metrics()`来修改传递给`compute_metrics()`的数据。
- en: To compute *loss*, we need access to the actual probability distributions contained
    in the `logits`. Recall that an LLM for next token prediction will, at each point
    along a token sequence, produce a probability distribution over all possible tokens
    in the vocabulary (`|V|=32000`) for the next token. This distribution is stored
    in `logits`, which has shape `[B,N,|V|]`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算*损失*，我们需要访问`logits`中包含的实际概率分布。回想一下，一个用于下一个标记预测的LLM，在标记序列的每个点上，会为下一个标记生成一个覆盖词汇表中所有可能标记的概率分布（`|V|=32000`）。这个分布存储在`logits`中，其形状为`[B,N,|V|]`。
- en: By default, `preprocess_logits_for_metrics()`will take the *argmax* (along the
    last dimension, the `|V|` dimension) of this `logits` tensor, and pass these token
    indices along to `compute_metrics()`
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`preprocess_logits_for_metrics()`将对这个`logits`张量沿最后一维（即`|V|`维度）进行*argmax*操作，并将这些标记索引传递给`compute_metrics()`。
- en: '[PRE3]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These predictions represent the tokens the LLM *would have* predicted for every
    token position in every validation sequence, given the preceding tokens (final
    token prediction is chopped off because there’s no ground truth to compare it
    to). But as we have seen, to compute per-token losses we actually don’tneed to
    know the highest probability tokens (predictions returned by *argmax*) — we need
    to know the probability the LLM assigned to the *actual* tokens in each validation
    sequence, given the preceding tokens.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预测表示 LLM *本该* 为每个验证序列中的每个令牌位置预测的令牌，给定前面的令牌（最后一个令牌的预测被截断，因为没有真实值与之比较）。但正如我们所见，要计算每个令牌的损失，我们实际上不需要知道最高概率的令牌（通过
    *argmax* 返回的预测）——我们需要知道 LLM 为每个验证序列中的 *实际* 令牌分配的概率，给定前面的令牌。
- en: 'One solution would just be to pass the entire `logits` tensor along to `compute_metrics()`*,*
    and then compute losses in there, along with any other metrics, like accuracy*.*
    There is a serious problem with that approach, though: the way [Trainer](https://huggingface.co/docs/transformers/en/main_classes/trainer)
    is set up, the `preprocess_logits_for_metrics()` function is run (in batches)
    on the GPU(s), but`compute_metrics()`is run on the CPU (on the entire validation
    set as a whole — i.e. all batches recombined). And, the *reason* `preprocess_logits_for_metrics()`
    is run on GPU is that the `logits` tensor can get **extremely** large.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是直接将整个 `logits` 张量传递给 `compute_metrics()`*，* 然后在其中计算损失以及其他任何指标，比如准确度*。*
    然而，这种方法有一个严重的问题：由于 [Trainer](https://huggingface.co/docs/transformers/en/main_classes/trainer)
    的设置，`preprocess_logits_for_metrics()` 函数是在 GPU（按批次）上运行的，而 `compute_metrics()`
    是在 CPU 上运行的（针对整个验证集，即所有批次重新组合后）。并且，`preprocess_logits_for_metrics()` 在 GPU 上运行的
    *原因* 是 `logits` 张量可能变得 **极其** 大。
- en: Just to give you an idea how large, in my experiments, I have been using a batch
    size (B) of 8, and sequence length (N) of 2048, which leads to a tensor containing
    B x N x |V| = 8 x 2048 x 32000 ≈ **4.2 billion** values (per-GPU)!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅给你一个概念我的实验中，批次大小（B）为 8，序列长度（N）为 2048，这导致一个张量包含 B x N x |V| = 8 x 2048 x 32000
    ≈ **42 亿** 个值（每个 GPU）！
- en: The GPU can handle this giant tensor, but the CPU would explode if we tried
    to pass it along. We must perform some sort of reduction first, inside `preprocess_logits_for_metrics()`,
    to eliminate this giant 3rd dimension.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 可以处理这个巨大的张量，但如果我们尝试传递它，CPU 会爆炸。我们必须首先在 `preprocess_logits_for_metrics()`
    内执行某种缩减操作，以去除这个巨大的第三维度。
- en: There’s no single right way to do this. One option would be to select from `logits`
    the probability generated for every actual (true) token, and pass these along
    to `compute_metrics()`, then compute the losses there on the CPU*.* That would
    certainly work*.* However, a better idea would be to use the full processing power
    of the GPU(s) to do a bit more computation inside `preprocess_logits_for_metrics()`beforehanding
    things off to the CPU side.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这没有单一正确的方法。一种选择是从 `logits` 中选择为每个实际（真实）令牌生成的概率，并将其传递给 `compute_metrics()`，然后在
    CPU 上计算损失*。* 这当然可以工作*。* 然而，更好的主意是使用 GPU 的全部处理能力，在 `preprocess_logits_for_metrics()`
    内进行更多的计算，然后再将任务交给 CPU 端。
- en: Recall that cross entropy loss over a token sequence is just the *average* *per-token*
    *loss* over the whole token sequence. So we can use `preprocess_logits_for_metrics()`tocompute
    a tensor containing all the *per-token* losses, and pass this tensor to `compute_metrics()`to
    do the averaging later on*.*
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，交叉熵损失是针对令牌序列的 *平均* *每个令牌* *损失*。因此，我们可以使用 `preprocess_logits_for_metrics()`
    来计算包含所有 *每个令牌* 损失的张量，并将这个张量传递给 `compute_metrics()` 以便稍后进行平均*。*
- en: 'One minor complication is that `preprocess_logits_for_metrics()`is set up to
    pass a *single* value on to `compute_metrics()`*.* However, we need to pass along
    *two* separate tensors. Since we’re interested in tracking multiple metrics on
    the validation set (prompt loss and completion loss, as well as completion token
    accuracy) — we require two tensors: *predictions* for completion accuracy, and
    *per-token-losses* for both losses. Luckily, the single value passed from `preprocess_logits_for_metrics()`
    to `compute_metrics()` can be a either a single tensor or tuple of tensors.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小的复杂问题是，`preprocess_logits_for_metrics()` 被设置为将一个 *单一* 值传递给 `compute_metrics()`*。*
    然而，我们需要传递 *两个* 单独的张量。因为我们有兴趣在验证集上跟踪多个指标（如提示损失和完成损失，以及完成令牌准确度）——我们需要两个张量：*predictions*
    用于完成准确度，*per-token-losses* 用于两个损失。幸运的是，从 `preprocess_logits_for_metrics()` 传递到
    `compute_metrics()` 的单一值可以是单个张量或张量元组。
- en: 'Specifically, `compute_metrics()` receives a single argument `data` which is
    an instance of the utility class `[transformers.EvalPrediction](https://github.com/huggingface/transformers/blob/174890280b340b89c5bfa092f6b4fb0e2dc2d7fc/src/transformers/trainer_utils.py#L149)`.
    The value returned by`preprocess_logits_for_metrics()`is assigned to the `.predictions`
    field of `EvalPrediction` (after batches are gathered into a single tensor, and
    converted to numpy arrays). The [spec for](https://github.com/huggingface/transformers/blob/174890280b340b89c5bfa092f6b4fb0e2dc2d7fc/src/transformers/trainer_utils.py#L161)
    `[.predictions](https://github.com/huggingface/transformers/blob/174890280b340b89c5bfa092f6b4fb0e2dc2d7fc/src/transformers/trainer_utils.py#L161)`
    indicates that it can hold either a single array or a tuple of arrays (`predictions:
    Union[np.ndarray, Tuple[np.ndarray]]`) so we are good to go.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，`compute_metrics()`接收一个单一的参数`data`，它是一个工具类`[transformers.EvalPrediction](https://github.com/huggingface/transformers/blob/174890280b340b89c5bfa092f6b4fb0e2dc2d7fc/src/transformers/trainer_utils.py#L149)`的实例。`preprocess_logits_for_metrics()`返回的值被分配给`EvalPrediction`的`.predictions`字段（在批次被聚合成一个张量并转换为numpy数组之后）。[spec
    for](https://github.com/huggingface/transformers/blob/174890280b340b89c5bfa092f6b4fb0e2dc2d7fc/src/transformers/trainer_utils.py#L161)
    `[.predictions](https://github.com/huggingface/transformers/blob/174890280b340b89c5bfa092f6b4fb0e2dc2d7fc/src/transformers/trainer_utils.py#L161)`表示它可以容纳一个单一的数组或一个数组元组（`predictions:
    Union[np.ndarray, Tuple[np.ndarray]]`），因此我们可以继续进行。'
- en: '[PRE4]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now we can define `compute_metrics()`…
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义`compute_metrics()`…
- en: '[PRE5]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This should all look familiar because we are using the same ideas we used to
    define our custom loss function. Again, we rely on `prompt_mask` and `completion_mask`
    to select the proper token subsets for computing each loss. If you are wondering
    where `prompt_mask` and `completion_mask` are defined, it happens outside the
    function scope but they are made available using a [function closure](https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python),
    a method often employed in “function factories” ([see full script for details](https://github.com/davidsvaughn/prompt-loss-weight/blob/main/run_plw.py)).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切看起来应该很熟悉，因为我们正在使用与定义自定义损失函数时相同的思想。同样，我们依赖于`prompt_mask`和`completion_mask`来选择适当的令牌子集以计算每个损失。如果你在想`prompt_mask`和`completion_mask`是在哪里定义的，它们发生在函数范围之外，但通过[函数闭包](https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python)使其可用，这是一种在“函数工厂”中经常使用的方法（[查看完整脚本了解详情](https://github.com/davidsvaughn/prompt-loss-weight/blob/main/run_plw.py)）。
- en: The completion token *accuracy* is computed only on the actual multiple choice
    answer token (i.e. *A,B,C,D*), whereas completion *loss* includes other special
    tokens used in the chat template (i.e. spaces, *bos_token*, *eos_token*, etc).
    The referenced `ABCD_token_ids` allows us to isolate the answer tokens and ignore
    other tokens.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 完成令牌的*准确性*仅在实际的多项选择答案令牌（即*A,B,C,D*）上计算，而完成*损失*则包括在聊天模板中使用的其他特殊令牌（即空格、*bos_token*、*eos_token*等）。引用的`ABCD_token_ids`使我们能够隔离答案令牌并忽略其他令牌。
- en: Experiments
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: Finally, let’s do some fine-tuning runs while varying PLW…
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们在改变PLW的同时进行一些微调运行……
- en: 'Full Sequence Training: PLW=1'
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整序列训练：PLW=1
- en: 'Implementation details: I use [Llama-2–7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)
    as the base model, and fine-tune it on a subset of the [RACE reading comprehension
    dataset](https://huggingface.co/datasets/ehovy/race) using the [LoRA](https://arxiv.org/pdf/2106.09685.pdf)
    (Low-Rank Adaptation) method via the HuggingFace [PEFT](https://huggingface.co/docs/peft/index)
    (Parameter Efficient Fine-Tuning) library. I was able to speed up fine-tuning
    considerably with multi-GPU training using Microsoft’s [DeepSpeed](https://github.com/microsoft/DeepSpeed)
    library. Again, [see full code](https://github.com/davidsvaughn/prompt-loss-weight/blob/main/run_plw.py)
    for all the details.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 实现细节：我使用了[Llama-2–7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)作为基础模型，并通过HuggingFace的[PEFT](https://huggingface.co/docs/peft/index)（参数高效微调）库，使用[LoRA](https://arxiv.org/pdf/2106.09685.pdf)（低秩适应）方法对[RACE阅读理解数据集](https://huggingface.co/datasets/ehovy/race)的一个子集进行了微调。我通过微软的[DeepSpeed](https://github.com/microsoft/DeepSpeed)库，使用多GPU训练显著加速了微调过程。再次提醒，[查看完整代码](https://github.com/davidsvaughn/prompt-loss-weight/blob/main/run_plw.py)了解所有细节。
- en: This first plot below tracks the evolution of all validation set metrics when
    *minimizing the standard, full sequence loss* on the training set. Each curve
    has it’s own y-axis labels (color-coded) since they are all on different scales
    (except prompt and full sequence loss, which use the same scale, on left). You
    can see that response accuracy tracks very closely with completion loss, but opposite
    in direction, as should be expected. I’ve drawn dashed blue and green lines through
    the minima of completion loss and full sequence loss, to show where each intersects
    with accuracy.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下第一张图展示了在*最小化标准的完整序列损失*时，所有验证集指标的演变。每条曲线都有自己的 y 轴标签（按颜色编码），因为它们都在不同的尺度上（除了提示和完整序列损失，它们使用相同的尺度，位于左侧）。可以看到，响应准确率与完成损失的变化非常接近，但方向相反，正如预期的那样。我在完成损失和完整序列损失的最小值处绘制了虚线蓝色和绿色线条，显示它们与准确率交点的位置。
- en: '![](../Images/b21a59b05a8cbdc459de2d5a853ca7f6.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b21a59b05a8cbdc459de2d5a853ca7f6.png)'
- en: RACE Validation Set Metrics (image made with [*Matplotlib*](https://matplotlib.org/)by
    the author)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: RACE 验证集指标（图像由作者使用[*Matplotlib*](https://matplotlib.org/)制作）
- en: The main thing to observe is how **the minima of prompt loss and completion
    loss are extremely out of sync** — since prompt loss dominates full sequence loss
    (remember **R̅g = 0.01**) the full sequence loss is basically just prompt loss
    shifted down slightly, and they share the same arg-min.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 主要需要观察的是**提示损失和完成损失的最小值完全不同步**——由于提示损失主导了完整序列损失（记住**R̅g = 0.01**），因此完整序列损失基本上只是提示损失略微下移的结果，并且它们共享相同的最小值。
- en: This means that if you blindly follow popular practice and use the minimum of
    validation full sequence loss as the stopping criterion — just shy of epoch 2—
    where completion loss is still very high — **the fine-tuned model would only have
    53% accuracy!**
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着如果你盲目遵循流行做法，使用验证集完整序列损失的最小值作为停止标准——刚好在第 2 个 epoch 之前——此时完成损失仍然很高——**微调后的模型准确率仅为
    53%！**
- en: But, **by merely *tracking* thecompletion loss separately** (as opposed to direct
    minimization by using PLW=0 in our custom loss function, which we’ll do next)
    you would continue fine-tuning to 4.5 epochs, where completion loss reaches its
    minimum, and **increase accuracy to 75% !**
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，**仅仅通过*追踪*完成损失**（与使用 PLW=0 在我们自定义损失函数中进行直接最小化相对，我们接下来会这样做），你将继续微调到 4.5 个
    epoch，在这个点上完成损失达到最小值，并且**准确率提高到 75%！**
- en: 'Completion Only Training: PLW=0'
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仅完成训练：PLW=0
- en: 'Now, we’ll swing to the opposite end of the spectrum and completely mask out
    the prompt tokens. All we have to do is initialize the `PLWTrainer` with `prompt_loss_weight=0`.
    Here are those results plotted:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将转向范围的另一端，完全屏蔽掉提示符令牌。我们所需要做的就是将 `PLWTrainer` 初始化为 `prompt_loss_weight=0`。以下是这些结果的绘图：
- en: '![](../Images/b8283e524f2a14f9a107312392e9473f.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8283e524f2a14f9a107312392e9473f.png)'
- en: RACE Validation Set Metrics (image made with [*Matplotlib*](https://matplotlib.org/)by
    the author)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: RACE 验证集指标（图像由作者使用[*Matplotlib*](https://matplotlib.org/)制作）
- en: 'Two important things have changed:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 有两件重要的事情发生了变化：
- en: '**fine-tuning converges *much* faster** **to the minimum completion loss**
    -**and optimal accuracy - taking < 2 epochs (instead of 4.5 epochs)**'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调收敛*速度快得多*** **到达最小完成损失** - **并且最优准确率 - 只需不到 2 个 epoch（而不是 4.5 个 epoch）**'
- en: '**the optimal accuracy is higher as well — jumping from 75% to 80%**'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最优准确率也提高了——从 75% 跃升到 80%**'
- en: Another interesting thing to notice is that the prompt loss doesn’t go down
    at all, like in the previous plot, but just kind of floats around, even drifting
    slightly higher (pay close attention to the prompt loss y-axis scale — on the
    left). In other words, *there is absolutely no learning over the prompt tokens,*
    and eliminating them from fine-tuning has improved both the convergence speed
    and the maximum accuracy achieved. Seems like win/win!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的现象是，提示损失并没有像之前的图一样下降，而是保持在一个水平上，甚至稍微上升（请特别注意提示损失的 y 轴刻度——在左侧）。换句话说，*提示符令牌完全没有学习*，而从微调中剔除它们反而提高了收敛速度和最大准确率。看起来是双赢！
- en: Exploring The Full PLW Spectrum
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索完整的 PLW 范围
- en: 'Recall that if we use any fractional value `0 < PLW < 1` then the influence
    of prompt tokens on the total loss is dampened but not eliminated. Below I have
    plotted the validation set completion loss and the QA accuracy at six different
    `PLW` values: `[1, 0.5, 0.2, 0.1, 0.01, 0]`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，如果我们使用任何分数值 `0 < PLW < 1`，则提示符令牌对总损失的影响会被减弱，但不会消除。下面我绘制了在六个不同 `PLW` 值下的验证集完成损失和
    QA 准确率：[1, 0.5, 0.2, 0.1, 0.01, 0]
- en: '![](../Images/bd4e5ddd4711e58372372e2a9d760156.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd4e5ddd4711e58372372e2a9d760156.png)'
- en: RACE Validation Set Metrics (image made with [*Matplotlib*](https://matplotlib.org/)by
    the author)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: RACE 验证集指标（图像由作者使用 [*Matplotlib*](https://matplotlib.org/) 制作）
- en: What is most striking is how much faster the completion loss converges to the
    minimum for the three lowest `PLW` values `[0,0.01,0.1]`. The fastest convergence
    seems to happen at `PLW=0`, but only by a small amount compared to the next two
    smallest values. Looking at the accuracies, it appears that any of the three lowest
    `PLW` values will achieve the optimal accuracy (~80%) by around epoch 2.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最引人注目的是，三个最小 `PLW` 值 `[0,0.01,0.1]` 的完成损失收敛到最小值的速度要快得多。最快的收敛似乎发生在 `PLW=0` 时，但与下两个最小值相比，只有微小的差异。查看准确率后，似乎这三个最小的
    `PLW` 值中的任何一个都将在大约第 2 个周期时达到最优准确率（~80%）。
- en: It’s also interesting to compare the convergence behavior of each completion
    loss curve to its corresponding accuracy curve. After reaching their minima, all
    six completion loss curves begin to slowly increase, while all accuracycurves
    level off without decreasing… How can we explain this?
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 同样有趣的是，将每个完成损失曲线的收敛行为与其相应的准确率曲线进行比较。在达到最小值后，所有六条完成损失曲线都开始缓慢上升，而所有准确率曲线则趋于平稳而不下降……我们如何解释这一现象呢？
- en: 'Digression: Loss or Token Accuracy — Which to track?'
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离题：损失还是 token 准确率 — 哪个更值得追踪？
- en: Recall that next token prediction is done by selecting the token with the highest
    probability given the previous tokens. The formula for token accuracy only considers
    if the token is correct or not, whereas the formula for Cross Entropy Loss actually
    takes into account the *values* of these probabilities. So what could be happening
    to explain the difference between these two graphs?
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，下一 token 的预测是通过选择给定前一个 tokens 的条件下概率最高的 token 来完成的。token 准确率的公式只考虑 token
    是否正确，而交叉熵损失的公式实际上考虑了这些概率的 *值*。那么，解释这两张图之间差异的原因可能是什么呢？
- en: Well, since the token accuracies are holding steady, this implies that the tokens
    having the highest probabilities (the *argmax* tokens) are remaining fairly constant,
    but those *max* *values* must be steadily declining — in other words, the *model
    is becoming less confident about its (mostly correct) token predictions*. This
    could be viewed as just mild case of overfitting, where the max values are affected,
    but not enough to affect the argmax values.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，由于 token 准确率保持稳定，这意味着具有最高概率的 token（即 *argmax* token）保持相对恒定，但这些 *最大* *值* 必须在稳步下降——换句话说，*模型对其（大部分正确的）token
    预测的信心在减弱*。这可以被视为一种轻度的过拟合情况，其中最大值受到影响，但不足以影响 argmax 值。
- en: This example illustrates why some say that [tracking token accuracy is better
    than tracking validation loss](https://twitter.com/Teknium1/status/1703227799979663408).
    Personally, I think its silly to argue about which one is better than the other,
    because you don’t have to choose… track both of them! Both are valuable indicators.
    Token accuracy may be ultimately what you care about maximizing (in many cases,
    anyway…). But I would also like to know if and when a model is becoming less confident
    in its (mostly) correct predictions (like we see above) so I would track completion
    loss as well.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子说明了为什么有些人说 [追踪 token 准确率比追踪验证损失更好](https://twitter.com/Teknium1/status/1703227799979663408)。就我个人而言，我认为争论哪一个更好是愚蠢的，因为你不必选择……可以同时追踪两者！两者都是有价值的指标。token
    准确率可能最终是你关心的最大化目标（在很多情况下，至少是这样）。但我也想知道模型是否以及何时对其（大部分）正确的预测变得不那么自信（就像我们上面看到的那样），所以我也会追踪完成损失。
- en: Better yet, the optimal strategy (in my opinion) would be to also track the
    model’s performance on a benchmark, or a *suite of benchmarks*, to get a fuller
    picture of how it’s evolving throughout the fine-tuning process. It could be the
    case that your LLM *is* getting better and better in terms of pure token accuracy
    on the validation set, but at the same time its *responses are becoming more repetitive
    and robotic sounding*, because the validation set is not diverse enough (I have
    actually seen this happen, in my day job). It’s always important to keep in mind
    what the true, ultimate goal is… and in almost all cases, token accuracy on the
    validation set is a mediocre proxy at best for your true goal.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是，最优策略（在我看来）应该是同时跟踪模型在基准测试或*多个基准测试*上的表现，以便更全面地了解模型在微调过程中的演变。可能出现的情况是，尽管你的LLM在验证集上的纯粹标记准确性不断提高，但它的*回应变得更加重复和机械化*，因为验证集不够多样（实际上我在我的日常工作中见过这种情况）。始终记住真正的最终目标是什么……在几乎所有情况下，验证集上的标记准确性最多只是你真正目标的一个平庸代理。
- en: Conclusion
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Our exploration into the effects of varying prompt-loss-weight on LLM instruction-tuning
    has highlighted several important concepts:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对变化的提示-损失权重（PLW）对大型语言模型（LLM）指令微调效果的探索突出了几个重要概念：
- en: '**Decoupling training objective from validation metrics**: Even without changing
    how prompt tokens are weighted inside the training objective function, we saw
    that we could improve our results just by *tracking* the right validation metric
    (i.e. completion loss, or accuracy).'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将训练目标与验证指标解耦**：即使不改变训练目标函数中提示词的权重，我们也发现，通过*跟踪*正确的验证指标（例如完成损失或准确度），我们可以改善结果。'
- en: '**PLW *can* effect model performance**: By decreasing PLW, we saw our fine-tuned
    model performance improve. Surprisingly, full prompt-masking was not required
    to achieve maximal improvement, since decreasing PLW below 0.1 seemed to have
    no additional effect. Whether or not this behavior translates to other datasets
    must be evaluated on a case by case basis.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PLW *确实可以*影响模型性能**：通过减少PLW，我们发现微调后的模型性能有所提高。令人惊讶的是，实际上并不需要完全的提示屏蔽来实现最大改进，因为将PLW减少到0.1以下似乎不会产生额外的效果。是否这种行为可以推广到其他数据集，必须逐个案例评估。'
- en: '**PLW *can* effect convergence speed**: Again, by decreasing PLW, we saw our
    fine-tuned model converge much faster to its optimum. This effect may be largely
    independent of the effect on model performance — i.e. depending on the dataset,
    either effect may appear without the other.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PLW *确实可以*影响收敛速度**：同样地，通过减少PLW，我们发现微调后的模型收敛速度更快，能够更快达到最佳状态。这个效应可能在很大程度上独立于模型性能的变化——即根据数据集的不同，这两个效应可能会独立出现。'
- en: '**Dataset-Specific Optimization**: Depending on the specific dataset and task,
    it’s very likely that the optimal PLW will vary widely. It’s even possible that
    in many cases it could have no effect at all. The dramatic improvements seen with
    the RACE dataset may not generalize to all fine-tuning scenarios, highlighting
    the need for experimentation.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集特定优化**：根据具体的数据集和任务，最优的提示-损失权重（PLW）很可能会有很大差异。在许多情况下，PLW甚至可能没有任何效果。在RACE数据集上看到的显著改进可能无法推广到所有的微调场景，这凸显了实验的重要性。'
- en: 'Future research directions could include:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的研究方向可能包括：
- en: Exploring the effects of PLW on a wider range of datasets beyond instruction
    datasets, such as those with larger generation ratios, or with longer chat dialogues
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索PLW对更多数据集的影响，超越指令数据集，像那些具有更大生成比率或更长对话的聊天数据集
- en: Developing adaptive PLW strategies that adjust dynamically during the fine-tuning
    process
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发在微调过程中动态调整的自适应PLW策略
- en: Examining the impact of PLW on other aspects of model performance, such as generalization
    and robustness
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究PLW对模型性能其他方面的影响，例如泛化能力和鲁棒性
- en: '*I hope you’ve found this slightly useful. I’m always open to feedback and
    corrections! You can reach me on* [*LinkedIn*](https://www.linkedin.com/in/davidsvaughn/)
    *The images in this post are mine, unless otherwise noted.*'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*希望你觉得这稍微有点用。我随时欢迎反馈和修改！你可以通过* [*LinkedIn*](https://www.linkedin.com/in/davidsvaughn/)
    *联系我。本文中的图片是我的，除非另有说明。*'
- en: Resources
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: All codes related to this tutorial can be accessed [here](https://github.com/davidsvaughn/prompt-loss-weight).
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与本教程相关的所有代码可以通过[此链接](https://github.com/davidsvaughn/prompt-loss-weight)访问。
