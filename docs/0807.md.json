["```py\nclass PositionalEncoder(nn.Module):\n    def __init__(self, d_model, max_length):\n        super(PositionalEncoder, self).__init__()\n        self.d_model = d_model\n        self.max_length = max_length\n\n        # Initialize the positional encoding matrix\n        pe = torch.zeros(max_length, d_model)\n\n        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n\n        # Calculate and assign position encodings to the matrix\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.pe = pe.unsqueeze(0)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)] # update embeddings\n        return x\n```", "```py\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.head_dim = d_model // num_heads\n\n        self.query_linear = nn.Linear(d_model, d_model)\n        self.key_linear = nn.Linear(d_model, d_model)\n        self.value_linear = nn.Linear(d_model, d_model)      \n        self.output_linear = nn.Linear(d_model, d_model)\n```", "```py\ndef split_heads(self, x, batch_size):\n        # Split the sequence embeddings in x across the attention heads\n        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)\n```", "```py\ndef compute_attention(self, query, key, mask=None):\n      # Compute dot-product attention scores\n      # dimensions of query and key are (batch_size * num_heads, seq_length, head_dim)\n      scores = query @ key.transpose(-2, -1) / math.sqrt(self.head_dim)\n      # Now, dimensions of scores is (batch_size * num_heads, seq_length, seq_length)\n      if mask is not None:\n          scores = scores.view(-1, scores.shape[0] // self.num_heads, mask.shape[1], mask.shape[2]) # for compatibility\n          scores = scores.masked_fill(mask == 0, float('-1e20')) # mask to avoid attention on padding tokens\n          scores = scores.view(-1, mask.shape[1], mask.shape[2]) # reshape back to original shape\n      # Normalize attention scores into attention weights\n      attention_weights = F.softmax(scores, dim=-1)\n\n      return attention_weights\n```", "```py\ndef forward(self, query, key, value, mask=None):\n      batch_size = query.size(0)\n\n      query = self.split_heads(self.query_linear(query), batch_size)\n      key = self.split_heads(self.key_linear(key), batch_size)\n      value = self.split_heads(self.value_linear(value), batch_size)\n\n      attention_weights = self.compute_attention(query, key, mask)\n\n      # Multiply attention weights by values, concatenate and linearly project outputs\n      output = torch.matmul(attention_weights, value)\n      output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n      return self.output_linear(output)\n```", "```py\nclass FeedForwardSubLayer(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(FeedForwardSubLayer, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n```", "```py\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output)) # skip connection and normalization\n        ff_output = self.feed_forward(x)\n        return self.norm2(x + self.dropout(ff_output)) # skip connection and normalization\n```", "```py\nclass TransformerEncoder(nn.Module):\n    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n        super(TransformerEncoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n\n    def forward(self, x, mask):\n        x = self.embedding(x)\n        x = self.positional_encoding(x)\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n```", "```py\nclass ClassifierHead(nn.Module):\n    def __init__(self, d_model, num_classes):\n        super(ClassifierHead, self).__init__()\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        logits = self.fc(x[:, 0, :]) # first token corresponds to the classification token\n        return F.softmax(logits, dim=-1)\n```", "```py\nnum_classes = 2 # binary classification\nd_model = 256 # dimension of the embedding vectors\nnum_heads = 4 # number of heads for self-attention\nnum_layers = 4 # number of encoder layers\nd_ff = 512\\. # dimension of the dense layers in the encoder layers\nsequence_length = 256 # maximum sequence length \ndropout = 0.4 # dropout to avoid overfitting\nnum_epochs = 20\nbatch_size = 32\n\nloss_function = torch.nn.CrossEntropyLoss()\n\ndataset = load_dataset(\"imdb\")\ndataset = balance_and_create_dataset(dataset, 1200, 200) # check GitHub repo\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', model_max_length=sequence_length)\n```", "```py\nprint(tokenized_datasets['train']['input_ids'][0])\n```", "```py\ntokenized_datasets = dataset.map(encode_examples, batched=True)\ntokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n\ntrain_dataloader = DataLoader(tokenized_datasets['train'], batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(tokenized_datasets['test'], batch_size=batch_size, shuffle=True)\n\nvocab_size = tokenizer.vocab_size\n\nencoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\nclassifier = ClassifierHead(d_model, num_classes)\n\noptimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=1e-4)\n```", "```py\ndef train(dataloader, encoder, classifier, optimizer, loss_function, num_epochs):\n    for epoch in range(num_epochs):        \n        # Collect and store embeddings before each epoch starts for visualization purposes (check repo)\n        all_embeddings, all_labels = collect_embeddings(encoder, dataloader)\n        reduced_embeddings = visualize_embeddings(all_embeddings, all_labels, epoch, show=False)\n        dic_embeddings[epoch] = [reduced_embeddings, all_labels]\n\n        encoder.train()\n        classifier.train()\n        correct_predictions = 0\n        total_predictions = 0\n        for batch in tqdm(dataloader, desc=\"Training\"):\n            input_ids = batch['input_ids']\n            attention_mask = batch['attention_mask'] # indicate where padded tokens are\n            # These 2 lines make the attention_mask a matrix instead of a vector\n            attention_mask = attention_mask.unsqueeze(-1)\n            attention_mask = attention_mask & attention_mask.transpose(1, 2) \n            labels = batch['label']\n            optimizer.zero_grad()\n            output = encoder(input_ids, attention_mask)\n            classification = classifier(output)\n            loss = loss_function(classification, labels)\n            loss.backward()\n            optimizer.step()\n            preds = torch.argmax(classification, dim=1)\n            correct_predictions += torch.sum(preds == labels).item()\n            total_predictions += labels.size(0)\n\n        epoch_accuracy = correct_predictions / total_predictions\n        print(f'Epoch {epoch} Training Accuracy: {epoch_accuracy:.4f}')\n```"]