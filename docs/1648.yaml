- en: 'The Machine Learning Guide for Predictive Accuracy: Interpolation and Extrapolation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-machine-learning-guide-for-predictive-accuracy-interpolation-and-extrapolation-45dd270ee871?source=collection_archive---------4-----------------------#2024-07-04](https://towardsdatascience.com/the-machine-learning-guide-for-predictive-accuracy-interpolation-and-extrapolation-45dd270ee871?source=collection_archive---------4-----------------------#2024-07-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Evaluating machine learning models beyond training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rkiuchir.medium.com/?source=post_page---byline--45dd270ee871--------------------------------)[![Ryota
    Kiuchi, Ph.D.](../Images/5459c434848898345d932320c4a01312.png)](https://rkiuchir.medium.com/?source=post_page---byline--45dd270ee871--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--45dd270ee871--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--45dd270ee871--------------------------------)
    [Ryota Kiuchi, Ph.D.](https://rkiuchir.medium.com/?source=post_page---byline--45dd270ee871--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--45dd270ee871--------------------------------)
    ·13 min read·Jul 4, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, data-driven approaches such as machine learning (ML) and deep
    learning (DL) have been applied to a wide range of tasks including machine translation
    and personal customized recommendations. These technologies reveal some patterns
    within the given training dataset by analyzing numerous data. However, if the
    given dataset has some biases and does not include the data that you want to know
    or predict, it might be difficult to get the correct answer from the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/941658a16eabebab03560ebfa237dbf8.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Stephen Dawson](https://unsplash.com/@dawson2406?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s think about the case of ChatGPT. The latest version of ChatGPT at this
    time is ChatGPT 4o, and this model is trained on data until June 2023 (at the
    period of this article). Therefore, if you ask about something that happened in
    2024 not included in the training data, you will not get an accurate answer. This
    is well-known as “hallucination,” and OpenAI added the preprocessing procedure
    to return a fixed answer as “unanswerable” for such kinds of questions. On the
    other hand, ChatGPT’s training data is also basically based on documents written
    in English, so it is not good at local domain knowledge outside of English-native
    countries such as Japan and France. Therefore, many companies and research groups
    put a lot of effort into customizing their LLM by including the region or domain-specific
    knowledge using RAG (Retrieval-Augmented Generation) or fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, identifying what training data is used is important for understanding
    the applicability and limitations of AI models. On the other hand, one of the
    biggest challenges in data-driven approaches is that these technologies often
    need to perform beyond the range of the training dataset. These demands are typically
    seen in new product development in material science, predicting the effects of
    new pharmaceutical compounds, and predicting consumer behavior when launching
    products in the markets. These scenarios require the correct predictions in the
    sparse area and outside of the training data, which refer to interpolation and
    extrapolation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4fc75309a8d53d6ae33f3b7315e6bf84.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Elevate](https://unsplash.com/@elevatebeer?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Interpolation involves making predictions within the known data range. If the
    training data is densely and uniformly distributed, accurate predictions can be
    obtained within that range. However, in practice, preparing such data is uncommon.
    On the other hand, extrapolation refers to making predictions outside the known
    data points’ range. Although predictions in such areas are highly desired, data-driven
    approaches typically struggle the most. Consequently, it is significantly important
    to understand the performance of both interpolation and extrapolation for each
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60d5eb36eebb5fa24ed0c4f4c92269d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Created by author
  prefs: []
  type: TYPE_NORMAL
- en: 'This article examines various machine learning algorithms for their interpolation
    and extrapolation capabilities. We prepare an artificial training dataset and
    evaluate these capabilities by visualizing each model’s prediction results. The
    target of machine learning algorithms are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Symbolic Regressor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVR (Support Vector Regression)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian Process Regressor (GPR)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Tree Regressor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forest Regressor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LightGBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, we also evaluate ensemble models such as Voting Regressor and Stacking
    Regressor.
  prefs: []
  type: TYPE_NORMAL
- en: Codes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Full of codes are available from below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/rkiuchir/blog_TDS/tree/main/02_compare_regression?source=post_page-----45dd270ee871--------------------------------)
    [## blog_TDS/02_compare_regression at main · rkiuchir/blog_TDS'
  prefs: []
  type: TYPE_NORMAL
- en: Blog Contents for Towards Data Science. Contribute to rkiuchir/blog_TDS development
    by creating an account on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/rkiuchir/blog_TDS/tree/main/02_compare_regression?source=post_page-----45dd270ee871--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Data Generation and Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Firstly, we generate the artificial data using a simple nonlinear function
    that is slightly modified from the [symbolic regressor’s tutorial in gplearn](https://gplearn.readthedocs.io/en/stable/examples.html)
    by adding the exponential term. This function consists of linear, quadratic, and
    exponential terms, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0172ad132b748c46722140f59278115d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *x₀* and *x₁* take a range of -1 to 1\. The plane of ground truth is
    as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: Since we examine the performance of each ML model in terms of interpolation
    and extrapolation, different datasets will be needed for each case.
  prefs: []
  type: TYPE_NORMAL
- en: For interpolation, we evaluate the model performance within the same range as
    with the training dataset. Thus, each model will be trained with discretized data
    points within the range of -1 to 1 and evaluated the predicted surface within
    the same range.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, for extrapolation, the capability of the model within the
    range outside of the training dataset will be required. We will train the model
    using the discretized data points within the range of -0.5 to 1 for both *x₀*
    and *x₁* and assess the predicted surface within the range of -1 to 1\. Consequently,
    the difference between the ground truth and predicted surface in the range of
    -1 to -0.5 for both *x₀* and *x₁* reveals the model capability in terms of extrapolation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, the impact of the number of points for the training dataset
    will also be evaluated by examining two cases: 20 and 100 points.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, 100 data points are generated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Introduction to Machine Learning Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we evaluate the performance of interpolation and extrapolation
    for the major 7 machine learning algorithms. In addition, the 6 ensemble models
    using 7 algorithms are also considered. Each algorithm has different structures
    and aspects, that introduce pros and cons for predicting performance. Here we
    summarize the characteristics of each algorithm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Symbolic Regression**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A trained model is expressed as the mathematical expressions fitted based on
    the genetic algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model is defined as the function, contributing high interpretability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appropriate for the task that target variable can be expressed as a function
    of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good at interpolation but may have some potential in extrapolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](/find-hidden-laws-within-your-data-with-symbolic-regression-ebe55c1a4922?source=post_page-----45dd270ee871--------------------------------)
    [## Find Hidden Laws Within Your Data with Symbolic Regression'
  prefs: []
  type: TYPE_NORMAL
- en: Automatically discover fundamental formulas like Kepler and Newton
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/find-hidden-laws-within-your-data-with-symbolic-regression-ebe55c1a4922?source=post_page-----45dd270ee871--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Support Vector Regression (SVR)**'
  prefs: []
  type: TYPE_NORMAL
- en: Based on a Support Vector Machine (SVM) that can efficiently handle the nonlinear
    relationship in the high dimensional spaces using the kernel method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using different types of kernels such as linear, RBF, polynomial, and sigmoid
    kernels, a model can express complex data patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good at interpolation but less stable in extrapolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](/the-complete-guide-to-support-vector-machine-svm-f1a820d8af0b?source=post_page-----45dd270ee871--------------------------------)
    [## The Complete Guide to Support Vector Machine (SVM)'
  prefs: []
  type: TYPE_NORMAL
- en: Understand its inner workings and implement SVMs in four different scenarios
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-complete-guide-to-support-vector-machine-svm-f1a820d8af0b?source=post_page-----45dd270ee871--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Gaussian Process Regression (GPR)**'
  prefs: []
  type: TYPE_NORMAL
- en: Based on the Bayesian method, the prediction is expressed as the probability
    which includes the predicted value and its uncertainty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks to the uncertainty estimation, GPR used for Bayesian Optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using different types of kernels such as linear, RBF, polynomial, and sigmoid
    kernels, a model can express complex data patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good at interpolation, and some potential for extrapolation selecting appropriate
    kernel selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](/quick-start-to-gaussian-process-regression-36d838810319?source=post_page-----45dd270ee871--------------------------------)
    [## Quick Start to Gaussian Process Regression'
  prefs: []
  type: TYPE_NORMAL
- en: A quick guide to understanding Gaussian process regression (GPR) and using scikit-learn’s
    GPR package
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/quick-start-to-gaussian-process-regression-36d838810319?source=post_page-----45dd270ee871--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Decision Tree**'
  prefs: []
  type: TYPE_NORMAL
- en: Simple tree-shape algorithm which successively splits the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to understand and interpret but tends to overfit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step-like estimation for interpolation and not good at extrapolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](/decision-tree-in-machine-learning-e380942a4c96?source=post_page-----45dd270ee871--------------------------------)
    [## Decision Tree in Machine Learning'
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree is a flowchart-like structure in which each internal node represents
    a test on a feature (e.g. whether…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/decision-tree-in-machine-learning-e380942a4c96?source=post_page-----45dd270ee871--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. Random Forest**'
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble-based algorithm which is called “Bagging” consisting of multiple
    decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By combining multiple diverse trees, this algorithm can reduce overfitting risk
    and have a high interpolation performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More stable predictions than single decision trees but not good at extrapolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](/understanding-random-forest-58381e0602d2?source=post_page-----45dd270ee871--------------------------------)
    [## Understanding Random Forest'
  prefs: []
  type: TYPE_NORMAL
- en: How the Algorithm Works and Why it Is So Effective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/understanding-random-forest-58381e0602d2?source=post_page-----45dd270ee871--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**6\. XGBoost**'
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble-based algorithm which is called “Boosting” combines multiple decision
    trees by sequentially reducing errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commonly used for competition such as Kaggle because of the good prediction
    performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More stable predictions than single decision trees but not good at extrapolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](https://medium.com/sfu-cspmp/xgboost-a-deep-dive-into-boosting-f06c9c41349?source=post_page-----45dd270ee871--------------------------------)
    [## XGBoost: A Deep Dive into Boosting'
  prefs: []
  type: TYPE_NORMAL
- en: Every day we hear about the breakthroughs in Artificial Intelligence. However,
    have you wondered what challenges it…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/sfu-cspmp/xgboost-a-deep-dive-into-boosting-f06c9c41349?source=post_page-----45dd270ee871--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**7\. LightGBM**'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to XGBoost, but with faster training speed and memory efficiency, which
    is more suitable for the larger datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More stable predictions than single decision trees but not good at extrapolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc?source=post_page-----45dd270ee871--------------------------------)
    [## What is LightGBM, How to implement it? How to fine tune the parameters?'
  prefs: []
  type: TYPE_NORMAL
- en: Hello,
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc?source=post_page-----45dd270ee871--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**8\. Voting Regressor**'
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble learning method combining predictions from multiple models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixing different model characteristics, which contribute to more robust predictions
    than a single model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evaluated in three combinations in this article:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: – Support Vector Regressor + Random Forest
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – Gaussian Process Regressor + Random Forest
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – Random Forest + XGBoost
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html?source=post_page-----45dd270ee871--------------------------------)
    [## VotingRegressor'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gallery examples: Plot individual and voting regression predictions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html?source=post_page-----45dd270ee871--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**9\. Stacking Regressor**'
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble learning method that uses predictions from multiple models as input
    for a final prediction model, “meta-model”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meta model covers individual model weaknesses and combines each model's strengths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evaluated in three combinations in this article:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '– Base model: Support Vector Regressor + Random Forest; Meta-model: Random
    Forest'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '– Base model: Gaussian Process Regressor + Random Forest; Meta-model: Random
    Forest'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '– Base model: Random Forest + XGBoost; Meta-model: Random Forest'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html?source=post_page-----45dd270ee871--------------------------------)
    [## StackingRegressor'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gallery examples: Combine predictors using stacking'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html?source=post_page-----45dd270ee871--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Using these algorithms, we will evaluate both interpolation and extrapolation
    performance with the dataset we generated earlier. In the following sections,
    the training methods and evaluation approaches for each model will be explained.
  prefs: []
  type: TYPE_NORMAL
- en: Model Training and Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Basically, except for tree-based approaches such as Random Forest, XGBoost,
    and LightGBM, most machine learning algorithms require feature scaling. However,
    since we only use two features such as *x₀* and *x₁* which take the same range,
    -1 to 1 (interpolation) or -0.5 to 1 (extrapolation) in this practice, we will
    skip the feature scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Model Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For simplicity, the default hyper parameters are used for all algorithms except
    LightGBM of which default parameters are suitable for the larger dataset.
  prefs: []
  type: TYPE_NORMAL
- en: As introduced in the earlier section, we will use different datasets for the
    evaluation of interpolation and extrapolation during model training.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation and Visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After model training, we will predict using very finely discretized data. Based
    on these predicted values, the prediction surface will be drawn using the [Plotly
    surface function](https://plotly.com/python/3d-surface-plots/).
  prefs: []
  type: TYPE_NORMAL
- en: 'These procedures are done by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation of Interpolation Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The prediction surfaces for each algorithm are shown for training data cases
    of 100 and 20 points respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '100 Training Points:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The original interactive figures can be viewed from [here](https://chart-studio.plotly.com/~rkiuchi/87)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6738a6729ec30307118a6d37ce47b23.png)'
  prefs: []
  type: TYPE_IMG
- en: '20 Training Points:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The original interactive figures can be viewed from [here](https://chart-studio.plotly.com/~rkiuchi/89)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5f138d525a873a3a2daeb3f6986afb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are the summarized features for each algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Symbolic Regressor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This algorithm performs almost perfectly in interpolation with 100 data points,
    but moderately good with 20 data points. This is because the Symbolic Regressor
    approximates the mathematical expressions and the simple functional form is used
    in this practice. Thanks to this feature, the predicted surface is notably smooth
    which is different from the tree-based algorithms explained later.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Regressor (SVR), Gaussian Process Regressor (GPR)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For kernel-based algorithms SVR and GPR, although the predicted surfaces slightly
    differ from the ground truth, interpolation performance is generally good with
    100 data points. In addition, the prediction surface obtained from these models
    is smooth similar to one estimated by Symbolic Regressor. However, in the case
    of 20 points, there is a significant difference between the predicted surface
    and the ground truth especially for SVR.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Tree, Random Forest, XGBoost, LightGBM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Firstly, the prediction surfaces estimated by these five tree-based models are
    not smooth but more step-like shapes. This characteristic arises from the structure
    and learning method of decision trees. Decision trees split the data recursively
    based on a threshold for one of the features. Each data point is assigned to some
    leaf nodes whose values are represented as the average value of the data points
    in that node. Therefore, the prediction values are constant within each leaf node,
    resulting in a step-like prediction surface.
  prefs: []
  type: TYPE_NORMAL
- en: The estimates of a single decision tree clearly show this characteristic. On
    the other hand, ensemble methods like Random Forests, XGBoost, and LightGBM, which
    consist of many decision trees within a single model, generate relatively smoother
    prediction surfaces due to the more different thresholds based on the many different
    shapes of decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Voting Regressor, Stacking Regressor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Voting Regressor combines the results of two algorithms by averaging them.
    For combinations like Random Forest + SVR, and Random Forest + GPR, the prediction
    surfaces reflect characteristics that mix the kernel-based and tree-based models.
    On the other hand, the combination of tree-based models like Random Forest and
    XGBoost relatively reduces the step-like shape prediction surface than one estimated
    from the single model.
  prefs: []
  type: TYPE_NORMAL
- en: The Stacking Regressor, which uses a meta-model to compute final predictions
    based on the outputs of multiple models, also shows step-like surfaces, because
    of the Random Forest used as the meta-model. This characteristic will be changed
    if kernel-based algorithms like SVR or GPR are used as the meta-model.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation of Extrapolation Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained earlier, each model is trained with data ranging from -0.5 to 1
    for both *x₀* and *x₁* and those performances will be evaluated within the range
    of -1 to 1\. Therefore, we get to know the extrapolation ability to inspect the
    prediction surface with the range of -1 to -0.5 for both *x₀* and *x₁.*
  prefs: []
  type: TYPE_NORMAL
- en: The prediction surfaces for each algorithm are shown for training data cases
    of 100 and 20 points respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '100 Training Points:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The original interactive figures can be viewed from [here](https://chart-studio.plotly.com/~rkiuchi/91)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2352079e0966edba848a71e21f702ba.png)'
  prefs: []
  type: TYPE_IMG
- en: '20 Training Points:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The original interactive figures can be viewed from [here](https://chart-studio.plotly.com/~rkiuchi/93)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b2bef5db2a4e1cf26276c879f3848cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Symbolic Regressor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The predicted surface within the area of extrapolation obtained by the Symbolic
    Regressor which is trained with 100 data points is almost accurately estimated
    similar to the interpolation evaluation. However, with only 20 training data points
    used, the predicted surface differs from the ground truth especially in the edge
    of the surface, indicating that the obtained functional form is not well estimated.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Regressor (SVR), Gaussian Process Regressor (GPR)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although both SVR and GPR are kernel-based algorithms, the obtained results
    are totally different. For both of 20 and 100 data points, while the predicted
    surface from SVR is well not estimated, GPR predicts almost perfectly even within
    the range of extrapolation.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Tree, Random Forest, XGBoost, LightGBM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although there are some differences among the results from these tree-based
    models, the predicted surfaces are constant in the range of extrapolation. This
    is because that decision trees rely on splits and no splits are generated in extrapolation
    regions, which cause constant values.
  prefs: []
  type: TYPE_NORMAL
- en: Voting Regressor, Stacking Regressor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As seen above, the kernel-based algorithms have better performance compared
    to the tree-based ones. The Voting Regressor with the combination of Random Forest
    and XGBoost, and all three Stacking Regressors whose meta-model is Random Forest
    predict constant in the range of extrapolation. On the other hand, the prediction
    surfaces derived from the Voting Regressor with the combination of Random Forest
    + SVR, and Random Forest + GPR have the blended characteristics of kernel-based
    and tree-based models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we evaluated the interpolation and extrapolation performance
    of the various machine learning algorithms. Since the ground truth data we used
    is expressed as a simple functional foam, symbolic regressor and kernel-based
    algorithms provide a better performance, especially for extrapolation than tree-based
    algorithms. However, more complex tasks that cannot be expressed in mathematical
    formulas might bring different results.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you so much for reading this article! I hope this article helps you understand
    the interpolation and extrapolation performance of machine learning models, making
    it easier to select and apply the right models for your projects.
  prefs: []
  type: TYPE_NORMAL
- en: '***Your clap to this article and subscription to*** [***my newsletter***](https://rkiuchir.medium.com/subscribe)
    ***would motivate me a lot!***'
  prefs: []
  type: TYPE_NORMAL
- en: Links
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Other articles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](/how-openais-sora-is-changing-the-game-an-insight-into-its-core-technologies-bd1ad17170df?source=post_page-----45dd270ee871--------------------------------)
    [## How OpenAI’s Sora is Changing the Game: An Insight into Its Core Technologies'
  prefs: []
  type: TYPE_NORMAL
- en: A masterpiece of state of the art technologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-openais-sora-is-changing-the-game-an-insight-into-its-core-technologies-bd1ad17170df?source=post_page-----45dd270ee871--------------------------------)
    [](/create-interactive-globe-earthquake-plot-in-python-b0b52b646f27?source=post_page-----45dd270ee871--------------------------------)
    [## Create “Interactive Globe + Earthquake Plot in Python
  prefs: []
  type: TYPE_NORMAL
- en: 'How to create a cool interactive figure in Python: the Globe plotted by Plotly.'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/create-interactive-globe-earthquake-plot-in-python-b0b52b646f27?source=post_page-----45dd270ee871--------------------------------)
    [](/pandas-cheat-sheet-for-data-preprocessing-cd1bcd607426?source=post_page-----45dd270ee871--------------------------------)
    [## Pandas Cheat Sheet for Data Preprocessing
  prefs: []
  type: TYPE_NORMAL
- en: Practical guide about how to preprocess data with Pandas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/pandas-cheat-sheet-for-data-preprocessing-cd1bcd607426?source=post_page-----45dd270ee871--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Personal website
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rkiuchir.github.io/?source=post_page-----45dd270ee871--------------------------------)
    [## R. Kiuchi — Seismology'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: rkiuchir.github.io](https://rkiuchir.github.io/?source=post_page-----45dd270ee871--------------------------------)
  prefs: []
  type: TYPE_NORMAL
