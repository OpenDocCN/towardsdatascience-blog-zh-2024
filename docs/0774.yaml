- en: How to build an OpenAI-compatible API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-build-an-openai-compatible-api-87c8edea2f06?source=collection_archive---------0-----------------------#2024-03-24](https://towardsdatascience.com/how-to-build-an-openai-compatible-api-87c8edea2f06?source=collection_archive---------0-----------------------#2024-03-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Create a server to replicate OpenAI’s Chat Completions API, enabling any LLM
    to integrate with tools written for the OpenAI API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@saarb?source=post_page---byline--87c8edea2f06--------------------------------)[![Saar
    Berkovich](../Images/8a834597e8c6cce1b948f6aa17bfe8be.png)](https://medium.com/@saarb?source=post_page---byline--87c8edea2f06--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--87c8edea2f06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--87c8edea2f06--------------------------------)
    [Saar Berkovich](https://medium.com/@saarb?source=post_page---byline--87c8edea2f06--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--87c8edea2f06--------------------------------)
    ·6 min read·Mar 24, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3678fe82d7cec7940e7288da517234fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author using OpenAI DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: It is early 2024, and the Gen AI market is being dominated by [OpenAI](https://iot-analytics.com/leading-generative-ai-companies/).
    For good reasons, too — they have the first mover’s advantage, being the first
    to provide an easy-to-use API for an LLM, and they also offer arguably the most
    capable LLM to date, GPT 4\. Given that this is the case, developers of all sorts
    of tools ([agents](https://docs.agpt.co/autogpt/), [personal assistants](https://github.com/QuivrHQ/quivr),
    [coding extensions](https://github.com/jupyterlab/jupyter-ai)), have turned to
    OpenAI for their LLM needs.
  prefs: []
  type: TYPE_NORMAL
- en: While there are many reasons to fuel your Gen AI creations with OpenAI’s GPT,
    there are plenty of reasons to opt for an alternative. Sometimes, it might be
    less cost-efficient, and at other times your data privacy policy may prohibit
    you from using OpenAI, or maybe you’re hosting an open-source LLM (or your own).
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s market dominance means that many of the tools you might want to use
    only support the OpenAI API. Gen AI & LLM providers like OpenAI, Anthropic, and
    Google all seem to creating different API schemas (perhaps intentionally), which
    adds a lot of extra work for devs who want to support all of them.
  prefs: []
  type: TYPE_NORMAL
- en: So, as a quick weekend project, I decided to implement a Python [FastAPI](https://fastapi.tiangolo.com/)
    server that is compatible with the OpenAI API specs, so that you can wrap virtually
    any LLM you like (either managed like Anthropic’s Claude, or self-hosted) to mimic
    the OpenAI API. Thankfully, the OpenAI API specs, have a `base_url` parameter
    you can set to effectively point the client to your server, instead of OpenAI’s
    servers, and most of the developers of aforementioned tools allow you to set this
    parameter to your liking.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, I’ve followed OpenAI’s Chat API reference openly available [here](https://platform.openai.com/docs/api-reference/chat),
    with some help from the code of [vLLM](https://github.com/vllm-project/vllm),
    an Apache-2.0 licensed inference server for LLMs that also offers OpenAI API compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: Game Plan
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be building a mock API that mimics the way OpenAI’s Chat Completion
    API (`/v1/chat/completions`) works. While this implementation is in Python and
    uses FastAPI, I kept it quite simple so that it can be easily transferable to
    another modern coding language like TypeScript or Go. We will be using the Python
    official [OpenAI client library](https://github.com/openai/openai-python) to test
    it — the idea is that if we can get the library to think our server is OpenAI,
    we can get any program that uses it to think the same.
  prefs: []
  type: TYPE_NORMAL
- en: First step — chat completions API, no streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ll start with implementing the non-streaming bit. Let’s start with modeling
    our request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The [PyDantic](https://docs.pydantic.dev/latest/) model represents the request
    from the client, aiming to replicate the API reference. For the sake of brevity,
    this model does not implement the entire specs, but rather the bare bones needed
    for it to work. If you’re missing a parameter that is a part of the [API specs](https://platform.openai.com/docs/api-reference/chat)
    (like `top_p`), you can simply add it to the model.
  prefs: []
  type: TYPE_NORMAL
- en: The `ChatCompletionRequest` models the parameters OpenAI uses in their requests.
    The chat API specs require specifying a list of `ChatMessage` (like a chat history,
    the client is usually in charge of keeping it and feeding back in at every request).
    Each chat message has a `role` attribute (usually `system`, `assistant` , or `user`
    ) and a `content` attribute containing the actual message text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll write our FastAPI chat completions endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: That simple.
  prefs: []
  type: TYPE_NORMAL
- en: Testing our implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assuming both code blocks are in a file called `main.py`, we’ll install two
    Python libraries in our environment of choice (always best to create a new one):
    `pip install fastapi openai` and launch the server from a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Using another terminal (or by launching the server in the background), we will
    open a Python console and copy-paste the following code, taken straight from [OpenAI’s
    Python Client Reference](https://github.com/openai/openai-python#Usage):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you’ve done everything correctly, the response from the server should be
    correctly printed. It’s also worth inspecting the `chat_completion` object to
    see that all relevant attributes are as sent from our server. You should see something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b41ac9b9749709c7180ac0bf9d59a281.png)'
  prefs: []
  type: TYPE_IMG
- en: Code by the author, formatted using [Carbon](https://carbon.now.sh/)
  prefs: []
  type: TYPE_NORMAL
- en: Leveling up — supporting streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As LLM generation tends to be slow (computationally expensive), it’s worth streaming
    your generated content back to the client, so that the user can see the response
    as it’s being generated, without having to wait for it to finish. If you recall,
    we gave `ChatCompletionRequest` a boolean `stream` property — this lets the client
    request that the data be streamed back to it, rather than sent at once.
  prefs: []
  type: TYPE_NORMAL
- en: This makes things just a bit more complex. We will create a [generator function](https://wiki.python.org/moin/Generators)
    to wrap our mock response (in a real-world scenario, we will want a generator
    that is hooked up to our LLM generation)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: And now, we would modify our original endpoint to return a StreamingResponse
    when `stream==True`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Testing the streaming implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After restarting the uvicorn server, we’ll open up a Python console and put
    in this code (again, taken from OpenAI’s library docs)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see each word in the server’s response being slowly printed, mimicking
    token generation. We can inspect the last `chunk` object to see something like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad32225e29fca2ff7ea30226264ff67f.png)'
  prefs: []
  type: TYPE_IMG
- en: Code by the author, formatted using [Carbon](https://carbon.now.sh/)
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, in the gist below, you can see the entire code for the server.
  prefs: []
  type: TYPE_NORMAL
- en: Final Notes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many other interesting things we can do here, like supporting other
    request parameters, and other OpenAI abstractions like Function Calls and the
    Assistant API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lack of standardization in LLM APIs makes it difficult to switch providers,
    for both companies and developers of LLM-wrapping packages. In the absence of
    any standard, the approach I’ve taken here is to abstract the LLM behind the specs
    of the biggest and most mature API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
