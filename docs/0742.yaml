- en: Deep Dive into Vector Databases by Hand ✍︎
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手工深入了解向量数据库 ✍︎
- en: 原文：[https://towardsdatascience.com/deep-dive-into-vector-databases-by-hand-e9ab71f54f80?source=collection_archive---------0-----------------------#2024-03-20](https://towardsdatascience.com/deep-dive-into-vector-databases-by-hand-e9ab71f54f80?source=collection_archive---------0-----------------------#2024-03-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deep-dive-into-vector-databases-by-hand-e9ab71f54f80?source=collection_archive---------0-----------------------#2024-03-20](https://towardsdatascience.com/deep-dive-into-vector-databases-by-hand-e9ab71f54f80?source=collection_archive---------0-----------------------#2024-03-20)
- en: Explore what exactly happens behind-the-scenes in Vector Databases
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索向量数据库背后究竟发生了什么
- en: '[](https://medium.com/@srijanie.dey?source=post_page---byline--e9ab71f54f80--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--e9ab71f54f80--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e9ab71f54f80--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e9ab71f54f80--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--e9ab71f54f80--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@srijanie.dey?source=post_page---byline--e9ab71f54f80--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--e9ab71f54f80--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e9ab71f54f80--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e9ab71f54f80--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--e9ab71f54f80--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e9ab71f54f80--------------------------------)
    ·8 min read·Mar 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e9ab71f54f80--------------------------------)
    ·阅读时间8分钟·2024年3月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The other day I asked my favorite Large Language Model (LLM) to help me explain
    vectors to my almost 4-year old. In seconds, it spit out a story filled with mythical
    creatures and magic and vectors. And Voila! I had a sketch for a new children’s
    book, and it was impressive because the unicorn was called ‘LuminaVec’.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 前几天，我请我最喜欢的大型语言模型（LLM）帮助我向将近4岁的孩子解释向量。几秒钟内，它就编出了一个充满神话生物、魔法和向量的故事。结果！我得到了一个新的儿童书籍草图，而且这很令人印象深刻，因为独角兽被叫做‘LuminaVec’。
- en: '![](../Images/4f137ee159351953eddd05976c88351a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f137ee159351953eddd05976c88351a.png)'
- en: Image by the author (‘LuminaVec’ as interpreted by my almost 4-year old)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供（‘LuminaVec’，由我将近4岁的孩子解释）
- en: So, how did the model help weave this creative magic? Well, the answer is by
    using vectors (in real life) and most probably vector databases. How so? Let me
    explain.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，模型是如何帮助编织这种创造性魔法的呢？答案是通过使用向量（在现实生活中），而且很可能是向量数据库。怎么做到的呢？让我来解释一下。
- en: '**Vectors and Embedding**'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**向量与嵌入**'
- en: First, the model doesn’t understand the exact words I typed in. What helps it
    understand the words are their numerical representations which are in the form
    of **vectors**. These vectors help the model find similarity among the different
    words while focusing on meaningful information about each. It does this by using
    **embeddings** which are low-dimensional vectors that try to capture the semantics
    and context of the information.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，模型并不理解我输入的具体单词。帮助它理解这些单词的是它们的数字表示形式，这些表示是以**向量**的形式存在的。这些向量帮助模型在不同单词之间找到相似性，同时聚焦于每个单词的有意义信息。它通过使用**嵌入**来做到这一点，嵌入是低维度的向量，旨在捕捉信息的语义和上下文。
- en: In other words, vectors in an embedding are lists of numbers that specify the
    position of an object with respect to a reference space. These objects can be
    features that define a variable in a dataset. With the help of these numerical
    vector values, we can determine how close or how far one feature is from the other
    — are they similar (close) or not similar (far)?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，嵌入中的向量是由一系列数字组成的，这些数字指定了一个对象相对于参考空间的位置。这些对象可以是定义数据集变量的特征。在这些数字向量值的帮助下，我们可以判断一个特征与另一个特征之间的距离——它们是相似（接近）还是不相似（远离）？
- en: Now these vectors are quite powerful but when we are talking about LLMs, we
    need to be extra cautious about them because of the word ‘large’. As it happens
    to be with these ‘large’ models, these vectors may quickly become long and more
    complex, spanning over hundreds or even thousands of dimensions. If not dealt
    with carefully, the processing speed and mounting expense could become cumbersome
    very fast!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量确实很强大，但当我们谈论大语言模型（LLM）时，我们需要格外小心，因为有一个“large”字眼。正如这些“大”模型的情况一样，这些向量可能会迅速变得非常长且复杂，跨越数百甚至数千维。如果不小心处理，处理速度和不断增加的成本可能会很快变得沉重！
- en: '**Vector Databases**'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**向量数据库**'
- en: 'To address this issue, we have our mighty warrior : Vector databases.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们有我们的强大战士：向量数据库。
- en: '**Vector databases** are special databases that contain these vector embeddings.
    Similar objects have vectors that are closer to each other in the vector database,
    while dissimilar objects have vectors that are farther apart. So, rather than
    parsing the data every time a query comes in and generating these vector embeddings,
    which induces huge resources, it is much faster to run the data through the model
    once, store it in the vector database and retrieve it as needed. This makes vector
    databases one of the most powerful solutions addressing the problem of scale and
    speed for these LLMs.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量数据库**是包含这些向量嵌入的特殊数据库。相似的对象在向量数据库中的向量彼此靠近，而不相似的对象的向量则相距较远。因此，与其每次查询时都解析数据并生成这些向量嵌入，这会消耗大量资源，不如一次性将数据通过模型运行，存储在向量数据库中，按需检索。这样，向量数据库就成为解决大规模和高速度问题的最强大方案之一，尤其是对于这些大语言模型（LLM）。'
- en: So, going back to the story about the rainbow unicorn, sparkling magic and powerful
    vectors — when I had asked the model that question, it may have followed a process
    as this -
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，回到关于彩虹独角兽、闪亮魔法和强大向量的故事——当我问模型这个问题时，它可能遵循了这样的过程——
- en: The embedding model first transformed the question to a vector embedding.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入模型首先将问题转化为一个向量嵌入。
- en: This vector embedding was then compared to the embeddings in the vector database(s)
    related to fun stories for 5-year olds and vectors.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，这个向量嵌入与与5岁小朋友的有趣故事和向量相关的向量数据库中的嵌入进行了比较。
- en: Based on this search and comparison, the vectors that were the most similar
    were returned. The result should have consisted of a list of vectors ranked in
    their order of similarity to the query vector.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于这种搜索和比较，返回了最相似的向量。结果应包括按与查询向量的相似度排序的向量列表。
- en: '**How does it really work?**'
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**它到底是如何工作的？**'
- en: To distill things even further, how about we go on a ride to resolve these steps
    on the micro-level? Time to go back to the basics! Thanks to Prof. Tom Yeh, we
    have [this beautiful handiwork](https://www.linkedin.com/posts/tom-yeh_vectordatabase-rag-deeplearning-activity-7158816842730336257-BaSm/)
    that explains the behind-the-scenes workings of the vectors and vector databases.
    (All the images below, unless otherwise noted, are by Prof. Tom Yeh from the above-mentioned
    LinkedIn post, which I have edited with his permission. )
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更进一步简化，怎么让我们以微观层面来解决这些步骤呢？是时候回到基础了！感谢Tom Yeh教授，我们有了[这份精美的手工作品](https://www.linkedin.com/posts/tom-yeh_vectordatabase-rag-deeplearning-activity-7158816842730336257-BaSm/)，它解释了向量和向量数据库背后的工作原理。（除非另有说明，以下所有图片均来自Tom
    Yeh教授在上述LinkedIn帖子中的内容，我已在他的许可下进行了编辑。）
- en: 'So, here we go:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们开始吧：
- en: For our example, we have a dataset of three sentences with 3 words (or tokens)
    for each.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们有一个包含三句话的数据集，每句话包含3个单词（或标记）。
- en: How are you
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你好吗
- en: Who are you
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是谁
- en: Who am I
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我是谁
- en: And our query is the sentence ‘am I you’.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 而我们的查询是句子“我是不是你”。
- en: '![](../Images/4ca936346c784c3aa80fc715e10d7c76.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ca936346c784c3aa80fc715e10d7c76.png)'
- en: 'In real life, a database may contain billions of sentences (think Wikipedia,
    news archives, journal papers, or any collection of documents) with tens of thousands
    of max number of tokens. Now that the stage is set, let the process begin :'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，数据库可能包含数十亿句子（想象一下维基百科、新闻档案、期刊论文或任何文档集合），每个句子有成千上万的标记。现在，舞台已经搭建好，开始过程吧：
- en: '[1] **Embedding** : The first step is generating vector embeddings for all
    the text that we want to be using. To do so, we search for our corresponding words
    in a table of 22 vectors, where 22 is the vocabulary size for our example.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] **嵌入**：第一步是为我们想要使用的所有文本生成向量嵌入。为此，我们需要在一个包含22个向量的表格中查找对应的单词，其中22是我们示例中的词汇量。'
- en: '![](../Images/023a9008e550114092798502507fc77f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/023a9008e550114092798502507fc77f.png)'
- en: In real life, the vocabulary size can be tens of thousands. The word embedding
    dimensions are in the thousands (e.g., 1024, 4096).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际生活中，词汇表的大小可以达到数万个。词嵌入的维度通常为几千（例如，1024、4096）。
- en: 'By searching for the words **how are you** in the vocabulary, the word embedding
    for it looks as this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在词汇表中查找单词**how are you**，它的词嵌入如下所示：
- en: '![](../Images/dc7015288b3985f7b4515faea57913a0.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc7015288b3985f7b4515faea57913a0.png)'
- en: '[2] **Encoding** : The next step is encoding the word embedding to obtain a
    sequence of feature vectors, one per word. For our example, the encoder is a simple
    perceptron consisting of a Linear layer with a ReLU activation function.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] **编码**：下一步是对词嵌入进行编码，以获得每个词的特征向量序列。以我们的示例为例，编码器是一个简单的感知机，由一个带有ReLU激活函数的线性层组成。'
- en: 'A quick recap:'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 快速回顾：
- en: ''
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Linear transformation** : The input embedding vector is multiplied by the
    weight matrix W and then added with the bias vector **b**,'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**线性变换**：输入嵌入向量与权重矩阵W相乘，然后与偏置向量**b**相加。'
- en: ''
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: z = **W**x+**b**, where **W** is the weight matrix, x is our word embedding
    and **b** is the bias vector.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: z = **W**x+**b**，其中**W**是权重矩阵，x是我们的词嵌入，**b**是偏置向量。
- en: ''
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ReLU activation function** : Next, we apply the ReLU to this intermediate
    z.'
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ReLU激活函数**：接下来，我们对这个中间值z应用ReLU。'
- en: ''
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ReLU returns the element-wise maximum of the input and zero. Mathematically,
    **h** = max{0,z}.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ReLU返回输入和零之间的逐元素最大值。数学表达式为**h** = max{0,z}。
- en: 'Thus, for this example the text embedding looks like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于这个例子，文本嵌入看起来像这样：
- en: '![](../Images/8e5be65c8020dce0f414b15799b47c96.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e5be65c8020dce0f414b15799b47c96.png)'
- en: To show how it works, let’s calculate the values for the last column as an example.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示它是如何工作的，让我们以最后一列为例计算值。
- en: '**Linear transformation** :'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性变换**：'
- en: '[1.0 + 1.1 + 0.0 +0.0] + 0 = 1'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.0 + 1.1 + 0.0 + 0.0] + 0 = 1'
- en: '[0.0 + 1.1 + 0.0 + 1.0] + 0 = 1'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[0.0 + 1.1 + 0.0 + 1.0] + 0 = 1'
- en: '[1.0 + (0).1+ 1.0 + 0.0] + (-1) = -1'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.0 + (0).1 + 1.0 + 0.0] + (-1) = -1'
- en: '[1.0 + (-1).1+ 0.0 + 0.0] + 0 = -1'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.0 + (-1).1 + 0.0 + 0.0] + 0 = -1'
- en: '**ReLU**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReLU**'
- en: max {0,1} =1
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: max {0,1} = 1
- en: max {0,1} = 1
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: max {0,1} = 1
- en: max {0,-1} = 0
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: max {0,-1} = 0
- en: max {0,-1} = 0
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: max {0,-1} = 0
- en: And thus we get the last column of our feature vector. We can repeat the same
    steps for the other columns.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到了特征向量的最后一列。我们可以对其他列重复相同的步骤。
- en: '[3] **Mean Pooling** : In this step, we club the feature vectors by averaging
    over the columns to obtain a single vector. This is often called text embedding
    or sentence embedding.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] **均值池化**：在此步骤中，我们通过对列进行平均化来将特征向量合并为一个单一的向量。这通常被称为文本嵌入或句子嵌入。'
- en: '![](../Images/642821d670d86f1fdf7a4a517b699aa9.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/642821d670d86f1fdf7a4a517b699aa9.png)'
- en: Other techniques for pooling such as CLS, SEP can be used but Mean Pooling is
    the one used most widely.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用其他池化技术，如CLS、SEP，但均值池化是使用最广泛的方法。
- en: '[4] **Indexing** : The next step involves reducing the dimensions of the text
    embedding vector, which is done with the help of a projection matrix. This projection
    matrix could be random. The idea here is to obtain a short representation which
    would allow faster comparison and retrieval.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] **索引**：下一步涉及减少文本嵌入向量的维度，这是通过投影矩阵来完成的。这个投影矩阵可以是随机的。这里的想法是获得一个简短的表示形式，从而允许更快的比较和检索。'
- en: '![](../Images/46267cff30f8b3c4e7c90644cb6c1110.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46267cff30f8b3c4e7c90644cb6c1110.png)'
- en: This result is kept away in the vector storage.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果被保存在向量存储中。
- en: '[5] **Repeat** : The above steps [1]-[4] are repeated for the other sentences
    in the dataset “who are you” and “who am I”.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] **重复**：上述步骤[1]-[4]会对数据集中的其他句子“who are you”和“who am I”重复进行。'
- en: Now that we have indexed our dataset in the vector database, we move on to the
    actual query and see how these indices play out to give us the solution.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在向量数据库中对数据集进行了索引，接下来我们开始进行实际查询，看看这些索引如何发挥作用，最终为我们提供解决方案。
- en: '**Query** : “am I you”'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**查询**：“am I you”'
- en: '[6] To get started, we repeat the same steps as above — embedding, encoding
    and indexing to obtain a 2d-vector representation of our query.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 要开始，我们重复上面的相同步骤——嵌入、编码和索引，以获得查询的2D向量表示。'
- en: '![](../Images/e663b2d6b7a1fbead2dd8aecddf0b0f4.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e663b2d6b7a1fbead2dd8aecddf0b0f4.png)'
- en: '[7] **Dot Product (Finding Similarity)**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] **点积（寻找相似性）**'
- en: Once the previous steps are done, we perform dot products. This is important
    as these dot products power the idea of comparison between the query vector and
    our database vectors. To perform this step, we transpose our query vector and
    multiply it with the database vectors.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成了之前的步骤，我们就会执行点积计算。这一步非常重要，因为这些点积是查询向量与数据库向量之间进行比较的核心。为了执行这一步，我们将查询向量转置，并与数据库向量相乘。
- en: '![](../Images/5f8e8e0f37546e7eae19877f760935ad.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f8e8e0f37546e7eae19877f760935ad.png)'
- en: '[8] **Nearest Neighbor**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] **最近邻**'
- en: The final step is performing a linear scan to find the largest dot product,
    which for our example is 60/9\. This is the vector representation for “who am
    I”. In real life, a linear scan could be incredibly slow as it may involve billions
    of values, the alternative is to use an Approximate Nearest Neighbor (ANN) algorithm
    like the Hierarchical Navigable Small Worlds (HNSW).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是执行线性扫描以找到最大的点积，在我们的示例中是60/9\。这就是“我是谁”的向量表示。在实际生活中，线性扫描可能极其缓慢，因为它可能涉及数十亿个值，替代方法是使用近似最近邻（ANN）算法，如分层可导航小世界（HNSW）。
- en: '![](../Images/428df143322e2653d7d0108e44d3b3d3.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/428df143322e2653d7d0108e44d3b3d3.png)'
- en: And that brings us to the end of this elegant method.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这也为我们带来了这优雅方法的结尾。
- en: Thus, by using the vector embeddings of the datasets in the vector database,
    and performing the steps above, we were able to find the sentence closest to our
    query. Embedding, encoding, mean pooling, indexing and then dot products form
    the core of this process.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过使用向量数据库中数据集的向量嵌入并执行上述步骤，我们能够找到最接近查询的句子。嵌入、编码、均值池化、索引以及点积构成了这个过程的核心。
- en: '**The ‘large’ picture**'
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**‘大’图景**'
- en: However, to bring in the ‘large’ perspective one more time -
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了再一次引入‘大’的视角——
- en: A dataset may contain millions or billions of sentences.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个数据集可能包含数百万或数十亿个句子。
- en: The number of tokens for each of them can be tens of thousands.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们每个的标记数量可能达到数万。
- en: The word embedding dimensions can be in the thousands.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词嵌入的维度可以达到数千。
- en: As we put all of these data and steps together, we are talking about performing
    operations on dimensions that are mammoth-like in size. And so, to power through
    this magnificent scale, vector databases come to the rescue. Since we started
    this article talking about LLMs, it would be a good place to say that because
    of the scale-handling capability of vector databases, they have come to play a
    significant role in Retrieval Augmented Generation (RAG). The scalability and
    speed offered by vector databases enable efficient retrieval for the RAG models,
    thus paving the way for an efficient generative model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将所有这些数据和步骤整合在一起时，我们实际上是在处理尺寸庞大的维度。因此，为了应对这一壮丽的规模，向量数据库发挥了重要作用。既然我们在文章开头提到了LLMs，那么值得一提的是，由于向量数据库的规模处理能力，它们在检索增强生成（RAG）中发挥了重要作用。向量数据库提供的可扩展性和速度，使得RAG模型的高效检索成为可能，从而为高效生成模型铺平了道路。
- en: All in all it is quite right to say that vector databases are powerful. No wonder
    they have been here for a while — starting their journey of helping recommendation
    systems to now powering the LLMs, their rule continues. And with the pace vector
    embeddings are growing for different AI modalities, it seems like vector databases
    are going to continue their rule for a good amount of time in the future!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，毫无疑问，向量数据库是强大的。难怪它们已经存在了一段时间——从开始帮助推荐系统，到如今为大规模语言模型（LLM）提供动力，它们的统治地位持续不断。随着向量嵌入在不同人工智能模式中的快速发展，似乎向量数据库在未来一段时间内将继续保持其主导地位！
- en: '![](../Images/133be6d4c727fa95744bcee970b82180.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/133be6d4c727fa95744bcee970b82180.png)'
- en: Image by the author
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: P.S. If you would like to work through this exercise on your own, here is a
    link to a blank template for your use.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 附言：如果你想独立完成这个练习，这里有一个空白模板供你使用。
- en: '[Blank Template for hand-exercise](http://bit.ly/48ViLuY)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[空白模板用于手动练习](http://bit.ly/48ViLuY)'
- en: Now go have fun and create some **‘luminous vectoresque’** magic!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在去尽情享受并创造一些**‘辉煌的向量魔法’**吧！
