<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Explaining LLMs for RAG and Summarization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Explaining LLMs for RAG and Summarization</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explaining-llms-for-rag-and-summarization-067e486020b4?source=collection_archive---------9-----------------------#2024-11-21">https://towardsdatascience.com/explaining-llms-for-rag-and-summarization-067e486020b4?source=collection_archive---------9-----------------------#2024-11-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="d4c7" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A fast and low-resource method using similarity-based attribution</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@daniel-klitzke?source=post_page---byline--067e486020b4--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Daniel Klitzke" class="l ep by dd de cx" src="../Images/4471634e1a0f0546402d582dcc36c7c4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Lx-ErwMknkXr0fxAEaHJ2A.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--067e486020b4--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@daniel-klitzke?source=post_page---byline--067e486020b4--------------------------------" rel="noopener follow">Daniel Klitzke</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--067e486020b4--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/56119ebe547d500ce80bfe7a215b9ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ncfjCpCN8XqYBj7wyZziow.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Information flow between an input document and its summary as computed by the proposed explainability method. (image created by author)</figcaption></figure><h1 id="2350" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">TL;DR</h1><ul class=""><li id="f5d8" class="ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow bk">Explaining LLMs is very <strong class="oa fr">slow </strong>and<strong class="oa fr"> resource-intensive.</strong></li><li id="928f" class="ny nz fq oa b go ox oc od gr oy of og oh oz oj ok ol pa on oo op pb or os ot ou ov ow bk">This article proposes a task-specific explanation technique or <strong class="oa fr">RAG Q&amp;A</strong> and <strong class="oa fr">Summarization.</strong></li><li id="39e9" class="ny nz fq oa b go ox oc od gr oy of og oh oz oj ok ol pa on oo op pb or os ot ou ov ow bk">The approach is <strong class="oa fr">model agnostic</strong> and is <strong class="oa fr">similarity-based.</strong></li><li id="c1b8" class="ny nz fq oa b go ox oc od gr oy of og oh oz oj ok ol pa on oo op pb or os ot ou ov ow bk">The approach is <strong class="oa fr">low-resource</strong> and <strong class="oa fr">low-latency</strong>, so can run almost everywhere.</li><li id="dbcc" class="ny nz fq oa b go ox oc od gr oy of og oh oz oj ok ol pa on oo op pb or os ot ou ov ow bk">I provided the <strong class="oa fr">code </strong>on <a class="af pc" href="https://github.com/Renumics/rag-explanation-blogpost" rel="noopener ugc nofollow" target="_blank">Github</a>, using the <a class="af pc" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">Huggingface Transformers</a> ecosystem.</li></ul><h1 id="9038" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Motivation</h1><p id="935a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">There are a lot of good reasons to get explanations for your model outputs. For example, they could help you <strong class="oa fr">find problems</strong> with your model, or they just could be a way to provide more <strong class="oa fr">transparency</strong> to the user, thereby facilitating user trust. This is why, for models like XGBoost, I have regularly applied methods like <a class="af pc" href="https://github.com/shap/shap" rel="noopener ugc nofollow" target="_blank">SHAP</a> to get more insights into my model’s behavior.</p><p id="824a" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Now, with myself more and more dealing with LLM-based ML systems, I wanted to <strong class="oa fr">explore ways of explaining LLM models</strong> the same way I did with more traditional ML approaches. However, I quickly found myself being stuck because:</p><ol class=""><li id="033d" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pi ov ow bk"><strong class="oa fr">SHAP</strong> does offer <a class="af pc" href="https://shap.readthedocs.io/en/latest/text_examples.html" rel="noopener ugc nofollow" target="_blank">examples for text-based models</a>, but for me, they failed with newer models, as SHAP did not support the embedding layers.</li><li id="0e2c" class="ny nz fq oa b go ox oc od gr oy of og oh oz oj ok ol pa on oo op pb or os ot pi ov ow bk"><strong class="oa fr">Captum</strong> also offers a tutorial for <a class="af pc" href="https://captum.ai/tutorials/Llama2_LLM_Attribution" rel="noopener ugc nofollow" target="_blank">LLM attribution</a>; however, both presented methods also had their very specific drawbacks. Concretely, the perturbation-based method was simply too slow, while the gradient-based method was letting my GPU memory explode and ultimately failed.</li></ol><p id="6c3e" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">After playing with quantization and even spinning up GPU cloud instances with still limited success I had enough I took a step back.</p><h1 id="6f63" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">A Similarity-based Approach</h1><p id="5c75" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">For understanding the approach, let’s first briefly define what we want to achieve. Concretely, we want to<strong class="oa fr"> identify and highlight sections in our input text</strong> (e.g. long text document or RAG context) that are highly relevant to our model output (e.g., a summary or RAG answer).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pj"><img src="../Images/4a22728cf380398f046abd250ab10b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ohIWME6pZ2CmFkhYdxYN-A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Typical flow of tasks our explainability method is applicable to. (image created by author)</figcaption></figure><p id="f939" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">In case of <strong class="oa fr">summarization</strong>, our method would have to highlight parts of the original input text that are highly reflected in the summary. In case of a <strong class="oa fr">RAG system</strong>, our approach would have to highlight document chunks from the RAG context that are showing up in the answer.</p><p id="cff3" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Since directly explaining the LLM itself has proven intractable for me, I instead propose to <strong class="oa fr">model the relation between model inputs and outputs</strong> via a separate text similarity model. Concretely, I implemented the following simple but effective approach:</p><ol class=""><li id="7bb6" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pi ov ow bk">I <strong class="oa fr">split the model inputs and outputs</strong> into sentences.</li><li id="99ca" class="ny nz fq oa b go ox oc od gr oy of og oh oz oj ok ol pa on oo op pb or os ot pi ov ow bk">I <strong class="oa fr">calculate pairwise similarities</strong> between all sentences.</li><li id="b9b2" class="ny nz fq oa b go ox oc od gr oy of og oh oz oj ok ol pa on oo op pb or os ot pi ov ow bk">I then <strong class="oa fr">normalize the similarity scores</strong> using Softmax</li><li id="0de2" class="ny nz fq oa b go ox oc od gr oy of og oh oz oj ok ol pa on oo op pb or os ot pi ov ow bk">After that, I <strong class="oa fr">visualize the similarities</strong> between input and output sentences in a nice plot</li></ol><p id="910e" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">In code, this is implemented as shown below. For running the code you need the <a class="af pc" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">Huggingface Transformers</a>, <a class="af pc" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">Sentence Transformers</a>, and <a class="af pc" href="https://github.com/nltk/nltk" rel="noopener ugc nofollow" target="_blank">NLTK</a> libraries.</p><blockquote class="pk pl pm"><p id="1032" class="ny nz pn oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Please, also check out this <a class="af pc" href="https://github.com/Renumics/rag-explanation-blogpost" rel="noopener ugc nofollow" target="_blank">Github Repository</a> for the full code accompanying this blog post.</p></blockquote><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="748b" class="ps nd fq pp b bg pt pu l pv pw">from sentence_transformers import SentenceTransformer<br/>from nltk.tokenize import sent_tokenize<br/>import numpy as np<br/><br/># Original text truncated for brevity ...<br/>text = """This section briefly summarizes the state of the art in the area of semantic segmentation and semantic instance segmentation. As the majority of state-of-the-art techniques in this area are deep learning approaches we will focus on this area. Early deep learning-based approaches that aim at assigning semantic classes to the pixels of an image are based on patch classification. Here the image is decomposed into superpixels in a preprocessing step e.g. by applying the SLIC algorithm [1].<br/><br/>Other approaches are based on so-called Fully Convolutional Neural Networks (FCNs). Here not an image patch but the whole image are taken as input and the output is a two-dimensional feature map that assigns class probabilities to each pixel. Conceptually FCNs are similar to CNNs used for classification but the fully connected layers are usually replaced by transposed convolutions which have learnable parameters and can learn to upsample the extracted features to the final pixel-wise classification result. ..."""<br/><br/># Define a concise summary that captures the key points<br/>summary = "Semantic segmentation has evolved from early patch-based classification approaches using superpixels to more advanced Fully Convolutional Networks (FCNs) that process entire images and output pixel-wise classifications."<br/><br/># Load the embedding model<br/>model = SentenceTransformer('BAAI/bge-small-en')<br/><br/># Split texts into sentences<br/>input_sentences = sent_tokenize(text)<br/>summary_sentences = sent_tokenize(summary)<br/><br/># Calculate embeddings for all sentences<br/>input_embeddings = model.encode(input_sentences)<br/>summary_embeddings = model.encode(summary_sentences)<br/><br/># Calculate similarity matrix using cosine similarity<br/>similarity_matrix = np.zeros((len(summary_sentences), len(input_sentences)))<br/>for i, sum_emb in enumerate(summary_embeddings):<br/>    for j, inp_emb in enumerate(input_embeddings):<br/>        similarity = np.dot(sum_emb, inp_emb) / (np.linalg.norm(sum_emb) * np.linalg.norm(inp_emb))<br/>        similarity_matrix[i, j] = similarity<br/><br/># Calculate final attribution scores (mean aggregation)<br/>final_scores = np.mean(similarity_matrix, axis=0)<br/><br/># Create and print attribution dictionary<br/>attributions = {<br/>    sentence: float(score)<br/>    for sentence, score in zip(input_sentences, final_scores)<br/>}<br/><br/>print("\nInput sentences and their attribution scores:")<br/>for sentence, score in attributions.items():<br/>    print(f"\nScore {score:.3f}: {sentence}")</span></pre><p id="0b6d" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">So, as you can see, so far, that is pretty simple. Obviously, we don’t explain the model itself. However, we might be able to get a good sense of <strong class="oa fr">relations between input and output sentences</strong> for this specific type of tasks (summarization / RAG Q&amp;A). But how does this actually perform and how to visualize the attribution results to make sense of the output?</p><h1 id="8711" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Evaluation for RAG and Summarization</h1><p id="eb8e" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">To visualize the outputs of this approach, I created <strong class="oa fr">two visualizations</strong> that are suitable for <strong class="oa fr">showing the feature attributions or connections between input and output</strong> of the LLM, respectively.</p><p id="3fbb" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">These visualizations were generated for a <strong class="oa fr">summary</strong> of the LLM input that goes as follows:</p><blockquote class="pk pl pm"><p id="4e93" class="ny nz pn oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">This section discusses the state of the art in semantic segmentation and instance segmentation, focusing on deep learning approaches. Early patch classification methods use superpixels, while more recent fully convolutional networks (FCNs) predict class probabilities for each pixel. FCNs are similar to CNNs but use transposed convolutions for upsampling. Standard architectures include U-Net and VGG-based FCNs, which are optimized for computational efficiency and feature size. For instance segmentation, proposal-based and instance embedding-based techniques are reviewed, including the use of proposals for instance segmentation and the concept of instance embeddings.</p></blockquote><h2 id="c6a8" class="px nd fq bf ne py pz qa nh qb qc qd nk oh qe qf qg ol qh qi qj op qk ql qm qn bk">Visualizing the Feature Attributions</h2><p id="1551" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">For visualizing the <strong class="oa fr">feature attributions</strong>, my choice was to simply stick to the original representation of the input data as close as possible.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qo"><img src="../Images/1fcb363021a34a497d7e7c691eb1de3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qFWxgSiTven8jCTfUpJpLw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Visualization of sentence-wise feature attribution scores based on color mapping. (image created by author)</figcaption></figure><p id="b30a" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Concretely, I simply plot the sentences, including their calculated attribution scores. Therefore, I <strong class="oa fr">map the attribution scores to the colors</strong> of the respective sentences.</p><p id="7316" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">In this case, this shows us some dominant patterns in the summarization and the source sentences that the information might be stemming from. Concretely, the <strong class="oa fr">dominance of mentions of FCNs</strong> as an architecture variant mentioned in the text, as well as the <strong class="oa fr">mention of proposal- and instance embedding-based instance segmentation methods,</strong> are clearly highlighted.</p><p id="ef51" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">In general, this method turned out to work pretty well for easily capturing attributions on the input of a summarization task, as it is very <strong class="oa fr">close to the original representation</strong> and adds very <strong class="oa fr">low clutter</strong> to the data. I could imagine also providing such a visualization to the user of a RAG system on demand. Potentially, the outputs could also be further processed to threshold to certain especially relevant chunks; then, this could also be displayed to the user by default to <strong class="oa fr">highlight relevant sources</strong>.</p><blockquote class="pk pl pm"><p id="2859" class="ny nz pn oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Again, check out the <a class="af pc" href="https://github.com/Renumics/rag-explanation-blogpost" rel="noopener ugc nofollow" target="_blank">Github Repository</a> to get the visualization code</p></blockquote><h2 id="e34c" class="px nd fq bf ne py pz qa nh qb qc qd nk oh qe qf qg ol qh qi qj op qk ql qm qn bk">Visualizing the Information Flow</h2><p id="131a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Another visualization technique focuses not on the feature attributions, but mostly on the <strong class="oa fr">flow of information</strong> between input text and summary.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/f032fd38c1af76db7f413c88f23bb556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vpIsE_h60N74IE0oFdgh_Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Visualization of the information flow between sentences in Input text and summary as Sankey diagram. (image created by author)</figcaption></figure><p id="c2b7" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Concretely, what I do here, is to first determine the <strong class="oa fr">major connections between input and output</strong> sentences based on the attribution scores. I then visualize those connections using a Sankey diagram. Here, the width of the flow connections is the <strong class="oa fr">strength of the connection</strong>, and the coloring is done based on the sentences in the summary for better <strong class="oa fr">traceability</strong>.</p><p id="043b" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Here, it shows that the summary mostly follows the order of the text. However, there are few parts where the LLM might have <strong class="oa fr">combined information</strong> from the beginning and the end of the text, e.g., the summary mentions a focus on deep learning approaches in the first sentence. This is taken from the last sentence of the input text and is clearly shown in the flow chart.</p><p id="93f1" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">In general, I found this to be useful, especially to get a sense on how much the LLM is aggregating information together <strong class="oa fr">from different parts</strong> of the input, rather than just copying or rephrasing certain parts. In my opinion, this can also be useful to estimate how much <strong class="oa fr">potential for error</strong> there is if an output is relying too much on the LLM for making connections between different bits of information.</p><h1 id="542a" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Possible Extensions and Adaptations</h1><p id="4e52" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In the <a class="af pc" href="https://github.com/Renumics/rag-explanation-blogpost" rel="noopener ugc nofollow" target="_blank">code provided on Github</a> I implemented certain extensions of the basic approach shown in the previous sections. Concretely I explored the following:</p><ol class=""><li id="3375" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pi ov ow bk">Use of <strong class="oa fr">different aggregations,</strong> such as max, for the similarity score.<br/>This can make sense as not necessarily the mean similarity to output sentences is relevant. Already one good hit could be relevant for out explanation.</li><li id="81a3" class="ny nz fq oa b go ox oc od gr oy of og oh oz oj ok ol pa on oo op pb or os ot pi ov ow bk">Use of <strong class="oa fr">different window sizes</strong>, e.g., taking chunks of three sentences to compute similarities.<br/>This again makes sense if suspecting that one sentence alone is not enough content to truly capture relatedness of two sentences so a larger context is created.</li><li id="f208" class="ny nz fq oa b go ox oc od gr oy of og oh oz oj ok ol pa on oo op pb or os ot pi ov ow bk">Use of <strong class="oa fr">cross-encoding-based models,</strong> such as rerankers.<br/>This could be useful as rerankers are more rexplicitely modeling the relatedness of two input documents in one model, being way more sensitive to nuanced language in the two documents. See also my recent post on <a class="af pc" href="https://medium.com/towards-data-science/reranking-using-huggingface-transformers-for-optimizing-retrieval-in-rag-pipelines-fbfc6288c91f" rel="noopener">Towards Data Science</a>.</li></ol><p id="7693" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">As said, all this is demoed in the provided Code so make sure to check that out as well.</p><h1 id="d5dd" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Conclusion</h1><p id="49a8" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In general, I found it pretty challenging to find tutorials that truly demonstrate explainability techniques for non-toy scenarios in RAG and summarization. Especially techniques that are useful in “real-time” scenarios, and are thus providing <strong class="oa fr">low-latency</strong> seemed to be scarce. However, as shown in this post, simple solutions can already give quite nice results when it comes to <strong class="oa fr">showing relations</strong> between documents and answers in a RAG use case. I will definitely explore this further and see how I can probably use that in RAG production scenarios, as <strong class="oa fr">providing traceable outputs</strong> to the users has proven invaluable to me. If you are interested in the topic and want to get more content in this style, follow me here on <a class="af pc" href="https://medium.com/@daniel-klitzke" rel="noopener">Medium</a> and on <a class="af pc" href="https://www.linkedin.com/in/daniel-klitzke/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>.</p></div></div></div></div>    
</body>
</html>