["```py\npip install ragas\n```", "```py\n# Installing Python packages & hiding\n!pip install --quiet \\\n  chromadb \\\n  datasets \\\n  langchain \\\n  langchain_chroma \\\n  optuna \\\n  plotly \\\n  polars \\\n  ragas \\\n  1> /dev/null\n```", "```py\n# Importing the packages\nfrom functools import reduce\nimport json\nimport os\nimport requests\nimport warnings\n\nimport chromadb\nfrom chromadb.api.models.Collection import Collection as ChromaCollection\nfrom datasets import load_dataset, Dataset\nfrom getpass import getpass\nfrom langchain_chroma import Chroma\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\nfrom langchain_core.runnables.base import RunnableSequence\nfrom langchain_community.document_loaders import WebBaseLoader, PolarsDataFrameLoader\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom operator import itemgetter\nimport optuna\nimport pandas as pd\nimport plotly.express as px\nimport polars as pl\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    answer_relevancy,\n    faithfulness,\n    context_recall,\n    context_precision,\n    answer_correctness\n)\nfrom ragas.testset.generator import TestsetGenerator\nfrom ragas.testset.evolutions import simple, reasoning, multi_context, conditional\n```", "```py\n# Providing api key for OPENAI\nOPENAI_API_KEY = getpass(\"OPENAI_API_KEY\")\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n```", "```py\n# Examining question evolution types evailable in ragas library\nurls = [\"https://en.wikipedia.org/wiki/Large_language_model\"]\nwikis_loader = WebBaseLoader(urls)\nwikis = wikis_loader.load()\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\ngenerator_llm = llm\ncritic_llm = llm\nembeddings = OpenAIEmbeddings()py\n\ngenerator = TestsetGenerator.from_langchain(\n    generator_llm,\n    critic_llm,\n    embeddings\n)\n\n# Change resulting question type distribution\nlist_of_distributions = [{simple: 1},\n                         {reasoning: 1},\n                         {multi_context: 1},\n                         {conditional: 1}]\n\n# This step COSTS $$$ ...\nquestion_evolution_types = list(\n    map(lambda x: generator.generate_with_langchain_docs(wikis, 1, x), \n        list_of_distributions)\n)\n\n# Displaying examples\nexamples = reduce(lambda x, y: pd.concat([x, y], axis=0),\n                                     [x.to_pandas() for x in question_evolution_types])\nexamples = examples.loc[:, [\"evolution_type\", \"question\", \"ground_truth\"]]\nexamples\n```", "```py\n# Defining a function to get document collection from vector db with given hyperparemeters\n# The function embeds the documents only if collection is missing\n# This development version as for production one would rather implement document level check\ndef get_vectordb_collection(chroma_client,\n                            documents,\n                            embedding_model=\"text-embedding-ada-002\",\n                            chunk_size=None, overlap_size=0) -> ChromaCollection:\n\n    if chunk_size is None:\n      collection_name = \"full_text\"\n      docs_pp = documents\n    else:\n      collection_name = f\"{embedding_model}_chunk{chunk_size}_overlap{overlap_size}\"\n\n      text_splitter = CharacterTextSplitter(\n        separator=\".\",\n        chunk_size=chunk_size,\n        chunk_overlap=overlap_size,\n        length_function=len,\n        is_separator_regex=False,\n      )\n\n      docs_pp = text_splitter.transform_documents(documents)\n\n    embedding = OpenAIEmbeddings(model=embedding_model)\n\n    langchain_chroma = Chroma(client=chroma_client,\n                              collection_name=collection_name,\n                              embedding_function=embedding,\n                              )\n\n    existing_collections = [collection.name for collection in chroma_client.list_collections()]\n\n    if chroma_client.get_collection(collection_name).count() == 0:\n      langchain_chroma.from_documents(collection_name=collection_name,\n                                        documents=docs_pp,\n                                        embedding=embedding)\n    return langchain_chroma\n```", "```py\n# Defininig a function to get a simple RAG as Langchain chain with given hyperparemeters\n# RAG returns also the context documents retrieved for evaluation purposes in RAGAs\n\ndef get_chain(chroma_client,\n              documents,\n              embedding_model=\"text-embedding-ada-002\",\n              llm_model=\"gpt-3.5-turbo\",\n              chunk_size=None,\n              overlap_size=0,\n              top_k=4,\n              lambda_mult=0.25) -> RunnableSequence:\n\n    vectordb_collection = get_vectordb_collection(chroma_client=chroma_client,\n                                                  documents=documents,\n                                                  embedding_model=embedding_model,\n                                                  chunk_size=chunk_size,\n                                                  overlap_size=overlap_size)\n\n    retriever = vectordb_collection.as_retriever(top_k=top_k, lambda_mult=lambda_mult)\n\n    template = \"\"\"Answer the question based only on the following context.\n    If the context doesn't contain entities present in the question say you don't know.\n\n    {context}\n\n    Question: {question}\n    \"\"\"\n    prompt = ChatPromptTemplate.from_template(template)\n    llm = ChatOpenAI(model=llm_model)\n\n    def format_docs(docs):\n        return \"\\n\\n\".join([doc.page_content for doc in docs])\n\n    chain_from_docs = (\n      RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n      | prompt\n      | llm\n      | StrOutputParser()\n    )\n\n    chain_with_context_and_ground_truth = RunnableParallel(\n      context=itemgetter(\"question\") | retriever,\n      question=itemgetter(\"question\"),\n      ground_truth=itemgetter(\"ground_truth\"),\n    ).assign(answer=chain_from_docs)\n\n    return chain_with_context_and_ground_truth\n```", "```py\n# Setting up a ChromaDB client\nchroma_client = chromadb.EphemeralClient()\n\n# Testing full text rag\n\nwith warnings.catch_warnings():\n  rag_prototype = get_chain(chroma_client=chroma_client, \n                            documents=news, \n                            chunk_size=1000, \n                            overlap_size=200)\n\nrag_prototype.invoke({\"question\": 'What happened in Minneapolis to the bridge?',\n                      \"ground_truth\": \"x\"})[\"answer\"]\n```", "```py\n# Getting the tiny extract of CCN Daily Mail dataset\nsynthetic_evaluation_set_url = \"https://gist.github.com/gox6/0858a1ae2d6e3642aa132674650f9c76/raw/synthetic-evaluation-set-cnn-daily-mail.csv\"\nsynthetic_evaluation_set_pl = pl.read_csv(synthetic_evaluation_set_url, separator=\",\").drop(\"index\")\n```", "```py\n# Train/test split\n# We need at least 2 sets: train and test for RAG optimization.\n\nshuffled = synthetic_evaluation_set_pl.sample(fraction=1, \n                                              shuffle=True, \n                                              seed=6)\ntest_fraction = 0.5\n\ntest_n = round(len(synthetic_evaluation_set_pl) * test_fraction)\ntrain, test = (shuffled.head(-test_n), \n               shuffled.head( test_n))\n```", "```py\n# We create the helper function to generate the RAG ansers together with Ground Truth based on synthetic evaluation set\n# The dataset for RAGAS evaluation should contain the columns: question, answer, ground_truth, contexts\n# RAGAs expects the data in Huggingface Dataset format\n\ndef generate_rag_answers_for_synthetic_questions(chain,\n                                                 synthetic_evaluation_set) -> pl.DataFrame:\n\n  df = pl.DataFrame()\n\n  for row in synthetic_evaluation_set.iter_rows(named=True):\n    rag_output = chain.invoke({\"question\": row[\"question\"], \n                               \"ground_truth\": row[\"ground_truth\"]})\n    rag_output[\"contexts\"] = [doc.page_content for doc \n                              in rag_output[\"context\"]]\n    del rag_output[\"context\"]\n    rag_output_pp = {k: [v] for k, v in rag_output.items()}\n    df = pl.concat([df, pl.DataFrame(rag_output_pp)], how=\"vertical\")\n\n  return df\n```", "```py\ndef objective(trial):\n\n  embedding_model = trial.suggest_categorical(name=\"embedding_model\",\n                                              choices=[\"text-embedding-ada-002\", 'text-embedding-3-small'])\n\n  chunk_size = trial.suggest_int(name=\"chunk_size\",\n                                 low=500,\n                                 high=1000,\n                                 step=100)\n\n  overlap_size = trial.suggest_int(name=\"overlap_size\",\n                                   low=100,\n                                   high=400,\n                                   step=50)\n\n  top_k = trial.suggest_int(name=\"top_k\",\n                            low=1,\n                            high=10,\n                            step=1)\n\n  challenger_chain = get_chain(chroma_client,\n                            news,\n                            embedding_model=embedding_model,\n                            llm_model=\"gpt-3.5-turbo\",\n                            chunk_size=chunk_size,\n                            overlap_size= overlap_size ,\n                            top_k=top_k,\n                            lambda_mult=0.25)\n\n  challenger_answers_pl = generate_rag_answers_for_synthetic_questions(challenger_chain , train)\n  challenger_answers_hf = Dataset.from_pandas(challenger_answers_pl.to_pandas())\n\n  challenger_result = evaluate(challenger_answers_hf,\n                               metrics=[answer_correctness],\n                              )\n\n  return challenger_result['answer_correctness']\n```", "```py\nsampler = optuna.samplers.TPESampler(seed=6)\nstudy = optuna.create_study(study_name=\"RAG Optimisation\",\n                            direction=\"maximize\",\n                            sampler=sampler)\nstudy.set_metric_names(['answer_correctness'])\n\neducated_guess = {\"embedding_model\": \"text-embedding-3-small\", \n                  \"chunk_size\": 1000,\n                  \"overlap_size\": 200,\n                  \"top_k\": 3}\n\nstudy.enqueue_trial(educated_guess)\n\nprint(f\"Sampler is {study.sampler.__class__.__name__}\")\nstudy.optimize(objective, timeout=180)\n```", "```py\nBest trial with answer_correctness: 0.700130617593832\nHyper-parameters for the best trial: {'embedding_model': 'text-embedding-ada-002', 'chunk_size': 700, 'overlap_size': 400, 'top_k': 9}\n```"]