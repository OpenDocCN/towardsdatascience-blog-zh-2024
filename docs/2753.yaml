- en: Of LLMs, Gradients, and Quantum Mechanics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/of-llms-gradients-and-quantum-mechanics-bdcaaf940fbb?source=collection_archive---------6-----------------------#2024-11-12](https://towardsdatascience.com/of-llms-gradients-and-quantum-mechanics-bdcaaf940fbb?source=collection_archive---------6-----------------------#2024-11-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Can Quantum Computing help improving our ability to train Large Neural Networks
    encoding language models (LLMs)?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://riccardo-disipio.medium.com/?source=post_page---byline--bdcaaf940fbb--------------------------------)[![Riccardo
    Di Sipio](../Images/07d5e8829a0bba4f32a91e261378d969.png)](https://riccardo-disipio.medium.com/?source=post_page---byline--bdcaaf940fbb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bdcaaf940fbb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bdcaaf940fbb--------------------------------)
    [Riccardo Di Sipio](https://riccardo-disipio.medium.com/?source=post_page---byline--bdcaaf940fbb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bdcaaf940fbb--------------------------------)
    ·13 min read·Nov 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c29f7df67db71e4b0399ae2ff25a9683.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by Alessio Soggetti (@asoggetti) from Unsplash.com
  prefs: []
  type: TYPE_NORMAL
- en: What is “training”?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the lingo of Artificial Intelligence (AI) studies, “training” means optimizing
    a statistical *model*, often implemented as a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)),
    to make predictions based on some input data and a measure of how good these predictions
    are (“cost” or “loss” function). There are three [main paradigms](https://www.geeksforgeeks.org/supervised-unsupervised-learning/)
    in which such procedure can happen: *supervised*, *unsupervised* (often [autoregressive](https://aws.amazon.com/what-is/autoregressive-models/)),
    and *reinforcement* learning. In **supervised** learning, each data point is labelled
    so the model predictions can be directly compared to the true values (*e.g.* this
    is the image of a cat or a dog). In **unsupervised** training, there are no explicit
    labels, but the comparison is carried out with features extracted from the data
    itself (*e.g.* predicting the next word in a sentence). Finally, **reinforcement**
    learning is based on optimizing the long-term returns of a sequence of decisions
    (predictions) based on the interaction between the statistical model and the environment
    (should the car slow down or speed up at a yellow traffic light?).'
  prefs: []
  type: TYPE_NORMAL
- en: In all these cases, the optimization of the **parameters of the model** is a
    lengthy process which requires a…
  prefs: []
  type: TYPE_NORMAL
