["```py\npip install pyod\n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom pyod.models.ecod import ECOD\nfrom pyod.models.iforest import IForest\nfrom pyod.models.lof import LOF\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.gmm import GMM\nfrom pyod.models.abod import ABOD\nimport time\n\nnp.random.seed(0)\n\nnum_rows = 100_000\nnum_cols = 10\ndata_corr = pd.DataFrame({0: np.random.random(num_rows)}) \n\nfor i in range(1, num_cols):\n  data_corr[i] = data_corr[i-1] + (np.random.random(num_rows) / 10.0)\n\ncopy_row = data_corr[0].argmax()\ndata_corr.loc[num_rows-1, 2] = data_corr.loc[copy_row, 2]\ndata_corr.loc[num_rows-1, 4] = data_corr.loc[copy_row, 4]\ndata_corr.loc[num_rows-1, 6] = data_corr.loc[copy_row, 6]\ndata_corr.loc[num_rows-1, 8] = data_corr.loc[copy_row, 8]\n\nstart_time = time.process_time() \npca = PCA(n_components=num_cols)\npca.fit(data_corr)\ndata_corr_pca = pd.DataFrame(pca.transform(data_corr), \n columns=[x for x in range(num_cols)])\nprint(\"Time for PCA tranformation:\", (time.process_time() - start_time))\n```", "```py\nnp.random.seed(0)\n\ndata_extreme = pd.DataFrame()\nfor i in range(num_cols):\n    data_extreme[i] = np.random.random(num_rows)\n\ncopy_row = data_extreme[0].argmax()\ndata_extreme.loc[num_rows-1, 2] = data_extreme[2].max() * 1.5\ndata_extreme.loc[num_rows-1, 4] = data_extreme[4].max() * 1.5\ndata_extreme.loc[num_rows-1, 6] = data_extreme[6].max() * 1.5\ndata_extreme.loc[num_rows-1, 8] = data_extreme[8].max() * 1.5\n\nstart_time = time.process_time() \npca = PCA(n_components=num_cols)\npca.fit(data_corr)\ndata_extreme_pca = pd.DataFrame(pca.transform(data_corr), \n    columns=[x for x in range(num_cols)])\n\nprint(\"Time for PCA tranformation:\", (time.process_time() - start_time))\n```", "```py\ndef evaluate_detector(df, clf, model_type):\n    \"\"\"\n    params:\n    df: data to be assessed, in a pandas dataframe\n    clf: outlier detector\n    model_type: string indicating the type of the outlier detector\n    \"\"\"\n\n    global scores_df\n\n    if \"ECOD\" in model_type:\n       clf = ECOD()\n    start_time = time.process_time()\n    clf.fit(df)\n    time_for_fit = (time.process_time() - start_time)\n\n    start_time = time.process_time()\n    pred = clf.decision_function(df)\n    time_for_predict = (time.process_time() - start_time)\n\n    scores_df[f'{model_type} Scores'] = pred\n    scores_df[f'{model_type} Rank'] =\\\n    scores_df[f'{model_type} Scores'].rank(ascending=False)\n\n    print(f\"{model_type:<20} Fit Time: {time_for_fit:.2f}\")\n    print(f\"{model_type:<20} Predict Time: {time_for_predict:.2f}\")\n```", "```py\ndef evaluate_dataset_variations(df, df_pca, clf, model_name): \n    evaluate_detector(df, clf, model_name)\n    evaluate_detector(df_pca, clf, f'{model_name} (PCA)')\n    evaluate_detector(df_pca[[0, 1, 2]], clf, f'{model_name} (PCA - 1st 3)')\n    evaluate_detector(df_pca[[7, 8, 9]], clf, f'{model_name} (PCA - last 3)')\n```", "```py\ndef evaluate_dataset(df, df_pca): \n  clf = IForest()\n  evaluate_dataset_variations(df, df_pca, clf, 'IF')\n\n  clf = LOF(novelty=True)\n  evaluate_dataset_variations(df, df_pca, clf, 'LOF')\n\n  clf = ECOD()\n  evaluate_dataset_variations(df, df_pca, clf, 'ECOD')\n\n  clf = HBOS()\n  evaluate_dataset_variations(df, df_pca, clf, 'HBOS')\n\n  clf = GMM()\n  evaluate_dataset_variations(df, df_pca, clf, 'GMM')\n\n  clf = ABOD()\n  evaluate_dataset_variations(df, df_pca, clf, 'ABOD')\n```", "```py\n# Test the first dataset\n# scores_df stores the outlier scores given to each record by each detector\nscores_df = data_corr.copy()\nevaluate_dataset(data_corr, data_corr_pca)\nrank_columns = [x for x in scores_df.columns if type(x) == str and 'Rank' in x]\nprint(scores_df[rank_columns].tail())\n\n# Test the second dataset\nscores_df = data_extreme.copy()\nevaluate_dataset(data_extreme, data_extreme_pca)\nrank_columns = [x for x in scores_df.columns if type(x) == str and 'Rank' in x]\nprint(scores_df[rank_columns].tail())\n```"]