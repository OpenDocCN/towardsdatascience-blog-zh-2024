<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Feature Engineering for Time-Series Using PySpark on Databricks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Feature Engineering for Time-Series Using PySpark on Databricks</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287?source=collection_archive---------3-----------------------#2024-05-15">https://towardsdatascience.com/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287?source=collection_archive---------3-----------------------#2024-05-15</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="9793" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Discover the potentials of PySpark for time-series data: Ingest, extract, and visualize data, accompanied by practical implementation code</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@johnleungTJ?source=post_page---byline--02b97d62a287--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="John Leung" class="l ep by dd de cx" src="../Images/ef45063e759e3450fa7f3c32b2f292c3.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*-zL9bLkxy32p8chXW888zQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--02b97d62a287--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@johnleungTJ?source=post_page---byline--02b97d62a287--------------------------------" rel="noopener follow">John Leung</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--02b97d62a287--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 15, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="8d90" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">With the increasing demand for high-speed querying and analysis on large datasets, <a class="af ne" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">Apache Spark</a> has stood out as one of the most popular analytical engines in recent years. It is powerful in distributed data processing due to its <a class="af ne" rel="noopener" target="_blank" href="/a-beginners-guide-to-apache-spark-ff301cb4cd92">master-worker architecture</a>. This includes a driver program that coordinates with the cluster manager (master) and controls the execution of distributing smaller tasks to worker nodes. Besides, designed as an in-memory data processing engine, Spark primarily uses RAM to store and manipulate data, rather than relying on disk storage. These coordinately facilitate faster execution of overall tasks.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/f5ba1a26d9aa68b032e07ceb2446dd09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Snni4h4UW_ZcQYmh"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Photo by <a class="af ne" href="https://unsplash.com/@davealmine?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Dawid Zawiła</a> on <a class="af ne" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="bfc6" class="ny nz fq bf oa ob oc od oe of og oh oi mr oj ok ol mv om on oo mz op oq or os bk">Apache Spark: From low-level to high-level</h2><p id="0d3e" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">At the lower level, its architecture is designed based on two main abstractions:</p><ul class=""><li id="468d" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd oy oz pa bk">Resilient Distributed Dataset (<a class="af ne" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" rel="noopener ugc nofollow" target="_blank">RDD</a>) — A low-level data abstraction in which each dataset can be divided into logical portions and executed on cluster worker nodes, thus aiding in parallel programming.</li><li id="792b" class="mi mj fq mk b go pb mm mn gr pc mp mq mr pd mt mu mv pe mx my mz pf nb nc nd oy oz pa bk">Directed Acyclic Graph (<a class="af ne" href="https://sparkbyexamples.com/spark/what-is-dag-in-spark/" rel="noopener ugc nofollow" target="_blank">DAG</a>) — The representation that facilitates optimizing and scheduling the dependencies and sequences of tasks.</li></ul><p id="2280" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">At the higher level, we can leverage a rich set of high-level tools using languages Scala, Python, or R. Examples of tools include <a class="af ne" href="https://spark.apache.org/sql/" rel="noopener ugc nofollow" target="_blank">Spark SQL</a> for SQL and DataFrames, <a class="af ne" href="https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html" rel="noopener ugc nofollow" target="_blank">Pandas API</a> on Spark for Pandas workloads, and <a class="af ne" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank">Structured Streaming</a> for stream processing.</p><p id="4ea8" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">However, before enjoying these functionalities, we may need much effort to self-manage a Spark cluster with the setup of infrastructure and a bunch of complex tools, which could cause a headache.</p><h2 id="b419" class="ny nz fq bf oa ob oc od oe of og oh oi mr oj ok ol mv om on oo mz op oq or os bk">PySpark on Databricks</h2><p id="9658" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">To address these challenges, <a class="af ne" href="https://spark.apache.org/docs/latest/api/python/index.html" rel="noopener ugc nofollow" target="_blank">PySpark</a> on <a class="af ne" href="https://docs.databricks.com/en/introduction/index.html" rel="noopener ugc nofollow" target="_blank">Databricks</a> is recently one of the high-level solutions in the industry. PySpark is the Python API for Spark, while Databricks is a full software platform built on top of Spark. It includes notebooks, infrastructure orchestration (auto-provisioning and scaling), process orchestration (job submission and scheduling), managed clusters, and even source control.</p></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8867" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Using PySpark APIs in Databricks, we will demonstrate and perform a feature engineering project on time series data. In this hands-on journey, we will simulate how Pandas library generally behaves for data processing, with the extra benefits of scalability and parallelism.</p><p id="6044" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="po">Note: If you want to know further how to dynamically orchestrate this Databricks notebook written in PySpark APIs in Azure, you can click </em><a class="af ne" href="https://medium.com/towards-data-science/orchestrating-a-dynamic-time-series-pipeline-with-azure-data-factory-and-databricks-810819608231" rel="noopener"><em class="po">here</em></a><em class="po">.</em></p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng pp"><img src="../Images/65a0654ef823dc1def1a7caf5abd5b30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jFGVo0U1jxlYynzr"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Photo by <a class="af ne" href="https://unsplash.com/@boiq?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Alexandru Boicu</a> on <a class="af ne" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4e50" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Consider a scenario in which you have the electric power consumption data for your household on hand, sampled at a one-minute rate from December 2006 to November 2010. Our objective is to ingest and manipulate data, extract features, and generate visualizations.</p><p id="23d0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The <a class="af ne" href="https://www.kaggle.com/datasets/uciml/electric-power-consumption-data-set/data" rel="noopener ugc nofollow" target="_blank">dataset</a> [with license as <a class="af ne" href="https://opendatacommons.org/licenses/dbcl/1-0/" rel="noopener ugc nofollow" target="_blank">Database: Open Database, Contents: Database Contents</a>], obtained from Kaggle, includes various fields such as date, time, global power (active and reactive), voltage, global intensity, and submetering (1, 2 and 3). We can begin our analysis now.</p><h2 id="a293" class="ny nz fq bf oa ob oc od oe of og oh oi mr oj ok ol mv om on oo mz op oq or os bk">The initial setup</h2><p id="a3f5" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">To begin, we need to create a user account for <a class="af ne" href="https://www.databricks.com/product/faq/community-edition" rel="noopener ugc nofollow" target="_blank">Databricks Community Edition</a>, which gives a suitable Databricks environment for our proof-of-concept purpose. Afterward, we can upload the input data file to the FileStore, a dedicated Databricks path. By clicking “Create Table in Notebook”, you are provided with the code template to initiate data ingestion.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng pq"><img src="../Images/c5e6d630d059bb13a169a9fdc1e5e9e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aD-5_gyqJtuLxJ-FHkhryQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Initial setup (1/2) — Create a user account (Image by author)</figcaption></figure><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng pr"><img src="../Images/f3f45f84fb426e8deba717b1e847a53c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WmkI7UfRBSWsrHX9gj_eqg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Initial setup (2/2) — Create a new table (Image by author)</figcaption></figure><h2 id="8c31" class="ny nz fq bf oa ob oc od oe of og oh oi mr oj ok ol mv om on oo mz op oq or os bk">Create a feature engineering project</h2><p id="7e8e" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk"><strong class="mk fr">#1 Ingest data</strong></p><ul class=""><li id="4a30" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd oy oz pa bk">Static data</li></ul><p id="d282" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We use the method <code class="cx ps pt pu pv b">spark.read()</code> to read our data source and return a DataFrame, a relational table. It supports various data sources such as CSV, JSON, Parquet, and more. In this case, we read the power consumption data in CSV format with a defined schema, where the first row serves as the header, and “;” is as the delimiter.</p><pre class="ni nj nk nl nm pw pv px bp py bb bk"><span id="7fb7" class="pz nz fq pv b bg qa qb l qc qd"># File location and type<br/>file_location = "/FileStore/tables/household_power_consumption.csv"<br/>file_type = "csv"<br/><br/># CSV options<br/>schema = "Date STRING, Time STRING, Global_active_power DOUBLE, Global_reactive_power DOUBLE, Voltage DOUBLE, Global_intensity DOUBLE, Sub_metering_1 DOUBLE, Sub_metering_2 DOUBLE, Sub_metering_3 DOUBLE"<br/>first_row_as_header = "true"<br/>delimiter = ";"<br/><br/># Read CSV files<br/>org_df = spark.read.format(file_type) \<br/>.schema(schema) \<br/>.option("header", first_row_as_header) \<br/>.option("delimiter", delimiter) \<br/>.load(file_location)<br/><br/>display(org_df)</span></pre><p id="f5a5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The output of the DataFrame with the first several rows:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qe"><img src="../Images/0625e5a7006818e72ebba9b31738edb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0auUJ0iUEbnOijWmJyW60w.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Output of the DataFrame (Image by author)</figcaption></figure><ul class=""><li id="f0a7" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd oy oz pa bk">Streaming data</li></ul><p id="b069" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In scenarios where data is continuously generated, we use stream processing techniques to read it incrementally. To demonstrate Spark’s behavior, I partitioned the original dataset into 10 subsets and stored them at the path “/FileStore/tables/stream/” beforehand. We then use another method <code class="cx ps pt pu pv b">spark.readStream()</code>for streaming data.</p><pre class="ni nj nk nl nm pw pv px bp py bb bk"><span id="748d" class="pz nz fq pv b bg qa qb l qc qd">sourceStream=spark.readStream.format("csv") \<br/>.option("header",True) \<br/>.schema(schema) \<br/>.option("mode","dropMalformed") \<br/>.option("maxFilesPerTrigger",1) \<br/>.option("ignoreLeadingWhiteSpace",True) \<br/>.load("dbfs:/FileStore/tables/stream") \</span></pre><p id="4bdc" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">It is worth mentioning that the <code class="cx ps pt pu pv b">mode</code> setting as “dropMalformed” means that we discard the corrupted records, no matter whether the corruption is due to structural inconsistencies or other factors that make them unusable. Also, we choose to process only one file per trigger event.</p><p id="0641" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">By starting to receive data and checking the record count every ten seconds, we can observe the continuous arrival of streaming data.</p><pre class="ni nj nk nl nm pw pv px bp py bb bk"><span id="fbcd" class="pz nz fq pv b bg qa qb l qc qd">import time<br/><br/># Stream the content of the DataFrame<br/>query = sourceStream.writeStream \<br/>.queryName("count") \<br/>.format("memory") \<br/>.outputMode("append") \<br/>.start()<br/><br/># Display the count of rows<br/>for _ in range(10):<br/>  spark.sql("SELECT COUNT(*) AS no_of_rows FROM count").show()<br/>  time.sleep(10)</span></pre><p id="0500" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">#2 Manipulate and explore data</strong></p><ul class=""><li id="9601" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd oy oz pa bk">Data transformation</li></ul><p id="2b87" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Since the number of rows with missing values is relatively negligible, we choose to drop them. Besides, we extract time-related features so that patterns can potentially be observed in higher dimensions later.</p><pre class="ni nj nk nl nm pw pv px bp py bb bk"><span id="a49b" class="pz nz fq pv b bg qa qb l qc qd">from pyspark.sql.functions import col, concat_ws, to_date<br/><br/># Drop rows with missing values<br/>df = org_df.na.drop()<br/><br/># Convert columns "Date" and "Time" into new column "DateTime"<br/>df = df.withColumn("Date", to_date(col("Date"),"d/M/y"))<br/>df = df.withColumn("Date", df["Date"].cast("date"))<br/>df = df.select(concat_ws(" ", to_date(col("Date"),"d/M/y"), col("Time")).alias("DateTime"), "*")<br/>df = df.withColumn("DateTime", df["DateTime"].cast("timestamp"))<br/><br/># Add time-related features<br/>df = df.withColumn("year", year("DateTime"))<br/>df = df.withColumn("month", month("DateTime"))<br/>df = df.withColumn("week_num", weekofyear("DateTime"))<br/>df = df.withColumn("hour", hour("DateTime"))</span></pre><ul class=""><li id="5ff4" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd oy oz pa bk">Data exploration</li></ul><p id="6f37" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We can explore data with various basic <a class="af ne" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html" rel="noopener ugc nofollow" target="_blank">PySpark methods</a>.</p><p id="c3a6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">(1) Select</p><p id="f6c2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The “‘select” method allows us to create a subset of the data frame column-wise. In this example, we select columns in descending order of global active power.</p><pre class="ni nj nk nl nm pw pv px bp py bb bk"><span id="9075" class="pz nz fq pv b bg qa qb l qc qd">df.select(“DateTime”, “Global_active_power”, “Global_intensity”).sort(“Global_active_power”, ascending=False).show(5)</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qf"><img src="../Images/63e2ed1f5139466002ac988a985dccd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nvT_fGjAWEH7i9SYjbBgAg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Output of “select” method (Image by author)</figcaption></figure><p id="272f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">(2) Filter</p><p id="f229" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This filters data points based on column values. In this example, we filter by two columns: “year” and “Global_intensity”.</p><pre class="ni nj nk nl nm pw pv px bp py bb bk"><span id="d7fb" class="pz nz fq pv b bg qa qb l qc qd">df.filter(<br/>    (col("year") == 2009) &amp;<br/>    (col("Global_intensity") &gt; 40)<br/>).count()<br/><br/># Output: 10</span></pre><p id="401c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">(3) groupby</p><p id="a9e4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We can also perform some aggregations. In our dataset, we calculate the average of global active power and sub-meterings for different months.</p><pre class="ni nj nk nl nm pw pv px bp py bb bk"><span id="4c4b" class="pz nz fq pv b bg qa qb l qc qd">df.groupby("month").agg(<br/>     round(mean("Global_active_power"), 2).alias("Avg_global_active_power"),<br/>     round(mean("Sub_metering_1"), 2).alias("Avg_sub_metering_1"),<br/>     round(mean("Sub_metering_2"), 2).alias("Avg_sub_metering_2"),<br/>     round(mean("Sub_metering_3"), 2).alias("Avg_sub_metering_3"),<br/>).sort(["month"]).show(5)</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qg"><img src="../Images/0c23b506d6091e313838198cd696ef74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GV-zkSb77-esqtjNq0hKsw.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Output of “groupby” method (Image by author)</figcaption></figure><p id="0661" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">#3 Extract features using Window functions</strong></p><p id="d0e2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In addition to the above basic PySpark methods and functions, we can leverage <a class="af ne" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Window.html" rel="noopener ugc nofollow" target="_blank">Window functions</a> to generate additional features to capture temporal dependencies and relationships in time series data. Assuming we have a transformed dataset (“df2”) where the total global active power is aggregated by day from one-minute rate samples. Let’s explore how we can obtain these features.</p><p id="a6a8" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">(1) Lag features</p><p id="9f51" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">These represent the metrics’ values from previous days, which aids our models in learning from historical data and identifying trends.</p><pre class="ni nj nk nl nm pw pv px bp py bb bk"><span id="f750" class="pz nz fq pv b bg qa qb l qc qd">from pyspark.sql.window import Window<br/>from pyspark.sql.functions import lag, round<br/><br/># Create a Window specification based on the 'Date' column<br/>windowSpec = Window.orderBy("Date")<br/><br/># Calculate the lagged value of 'Total_global_active_power'<br/>df2 = df2.withColumn("power_lag1", round(lag(col("Total_global_active_power"), 1).over(windowSpec), 2))<br/><br/>display(df2)</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qh"><img src="../Images/434e8ccfb2bb2a4fb421b8df83620e29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Avt7uCS97MXC4j29liYRg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Output — Lag features (Image by author)</figcaption></figure><p id="ffaf" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">(2) Delta features</p><p id="ffc3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This is to take a subsequent step to capture short-term changes or variations over time by calculating the difference between original data fields and the lag features.</p><pre class="ni nj nk nl nm pw pv px bp py bb bk"><span id="4134" class="pz nz fq pv b bg qa qb l qc qd"># Calculate the difference between columns<br/>df2 = df2.withColumn("power_lag1_delta", round(col("power_lag1") - col("Total_global_active_power"), 2))<br/><br/>display(df2)</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qi"><img src="../Images/7a55306d33c5684e2d67047a8f5ece93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e9jhhfVhsFpJlnHTbtiffQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Output — Delta features (Image by author)</figcaption></figure><p id="159c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">(3) Window average features</p><p id="2a54" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">These features calculate the average value of our target data fields within a sliding window, enabling us to capture the smoothed patterns and relatively long-term trends. Here I pick the window sizes as 14 (2 weeks) and 30 (roughly 1 month).</p><pre class="ni nj nk nl nm pw pv px bp py bb bk"><span id="8f33" class="pz nz fq pv b bg qa qb l qc qd"># Add window average fields to the DataFrame for the specified window sizes<br/>def add_window_avg_features(df, window_sizes):<br/>    for window_size in window_sizes:<br/>        window_col_name = f"avg_power_l{window_size}"<br/>        windowSpec = Window.orderBy("Date").rowsBetween(-window_size, 0)<br/>        df = df.withColumn(window_col_name, round(avg(col("Total_global_active_power")).over(windowSpec), 2))<br/>    return df<br/><br/>window_sizes = [14, 30]<br/>df2 = add_window_avg_features(df2, window_sizes)<br/><br/>df2.select("Date", "Total_global_active_power", "avg_power_l14", "avg_power_l30").sort("Date", ascending=False).show(5)</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qj"><img src="../Images/6da0266dc8acdc4ecba82eea68778a0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uarOPKDRUEbsW9npvi2xFQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Output — Window average features (Image by author)</figcaption></figure><p id="0dc4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">(4) Exponentially weighted moving average (EWMA) features</p><p id="e6ee" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">EWMA features are the rectified version of window average features by assigning more weight and emphasis to recent data, and less to past data. A higher value of weight (alpha) means that the EWMA features track more closely to the original time series. Here I pick two separate weights: 0.2 and 0.8.</p><pre class="ni nj nk nl nm pw pv px bp py bb bk"><span id="333a" class="pz nz fq pv b bg qa qb l qc qd">import pyspark.pandas as ps<br/><br/>#  Add EWMA features to the DataFrame for the specified alpha values<br/>def add_ewma_features(df, alphas):<br/>    for alpha in alphas:<br/>        ewma_col_name = f"ewma_power_w{str(alpha).replace('.', '')}"<br/>        windowSpec = Window.orderBy("Date")<br/>        df[ewma_col_name] = df.Total_global_active_power.ewm(alpha=alpha).mean().round(2)<br/>    return df<br/><br/>alphas = [0.2, 0.8]<br/># Convert into a pandas-on-Spark DataFrame, to use EWM function<br/>df2_pd = df2.pandas_api()<br/>df2_pd = add_ewma_features(df2_pd, alphas)<br/># Convert back to a Spark DataFrame<br/>df2 = df2_pd.to_spark()<br/><br/>df2.select("Date", "Total_global_active_power", "ewma_power_w02", "ewma_power_w08").sort("Date", ascending=False).show(5)</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qk"><img src="../Images/6f8d723516896fea4200c35e0e10f34a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DO7N1C4V8vnPqcAC21B7og.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Output — EWMA features (Image by author)</figcaption></figure><p id="7177" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">#4 Generate visualizations on Notebook</strong></p><p id="e8a5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">After extracting time-related data and features using various PySpark functions and methods, we can use the built-in support from Databricks to create visualizations efficiently. This works by dragging and dropping data fields and configuring visual settings in the visualization editor. Some examples are shown below.</p><ul class=""><li id="dc91" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd oy oz pa bk">Scatter plot: Relationship between global active power and global intensity</li></ul><p id="6993" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="po">Interpretation: There is a highly positive correlation between the two fields.</em></p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng ql"><img src="../Images/91d07a8535fb209f7e67989232457ed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fim8xcbWtUWgeO-t3s7zkQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Scatter plot, using a visualization editor (Image by author)</figcaption></figure><ul class=""><li id="c4bd" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd oy oz pa bk">Box plot: The distribution of global active power across hours</li></ul><p id="2bc0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="po">Interpretation: There are relatively large variations of global active power from 7:00 to 21:00.</em></p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qm"><img src="../Images/d737bb1829863d28a0311a0a48f2dafd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZOpTtWdcetFIADhFUiUnA.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Box plot (Image by author)</figcaption></figure><ul class=""><li id="e82a" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd oy oz pa bk">Line chart: The changes in total global active power, EWMA with alpha 0.2, and EWMA with alpha 0.8 from Jan 2008 to Mar 2008</li></ul><p id="c6d5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="po">Interpretation: EWMA with alpha 0.8 sticks more closely to the original time series than EWMA with alpha 0.2.</em></p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qn"><img src="../Images/6505478ffa3793a4eb8a874d7cb56713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xw49_XZROcjYv7JYRIV9uQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Line chart (Image by author)</figcaption></figure><p id="f81e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Besides, we can generate the default data profiles to display summary statistics such as count, % of missing values, and data distributions. This ensures data quality throughout the entire feature engineering process. These above visualizations can alternatively be created by the query output using Databricks SQL.</p><h2 id="190d" class="ny nz fq bf oa ob oc od oe of og oh oi mr oj ok ol mv om on oo mz op oq or os bk">Wrapping it up</h2><p id="2468" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">In our hands-on exploration, we used PySpark for feature engineering of time-series data using the Databricks platform:</p><ul class=""><li id="7107" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd oy oz pa bk">Ingesting static and streaming data, by using the <code class="cx ps pt pu pv b">spark.read()</code> and <code class="cx ps pt pu pv b">spark.readStream()</code> methods respectively.</li><li id="afe6" class="mi mj fq mk b go pb mm mn gr pc mp mq mr pd mt mu mv pe mx my mz pf nb nc nd oy oz pa bk">Manipulating and exploring data, by using a range of basic PySpark functions available in <code class="cx ps pt pu pv b">pyspark.sql.functions</code>, and DataFrame methods.</li><li id="dc83" class="mi mj fq mk b go pb mm mn gr pc mp mq mr pd mt mu mv pe mx my mz pf nb nc nd oy oz pa bk">Extract trend-related features, by calculating the relationship within groups of data using <code class="cx ps pt pu pv b">pyspark.sql.Window</code>.</li><li id="0279" class="mi mj fq mk b go pb mm mn gr pc mp mq mr pd mt mu mv pe mx my mz pf nb nc nd oy oz pa bk">Visualization, by using the built-in features of Databricks Notebook.</li></ul><p id="a177" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">When dealing with a massive dataset, PySpark is often preferred over Pandas due to its scalability and performance capabilities. PySpark’s support for lazy evaluation also means that computations are only performed when necessary, which reduces the overhead. However, Scala can sometimes be a better option, because we can closely catch up with the latest features as Spark itself is written in Scala. And more likely that systems which are less error-prone can be designed with the use of immutable objects. Different languages or libraries have their edges. Ultimately, the choice depends on the enterprise requirements, developers’ learning curves, and integration with other systems.</p></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="9811" class="ny nz fq bf oa ob oc od oe of og oh oi mr oj ok ol mv om on oo mz op oq or os bk">Before you go</h2><p id="e116" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">If you enjoy this reading, I invite you to<strong class="mk fr"> </strong>follow my <a class="af ne" href="https://medium.com/@johnleungTJ" rel="noopener">Medium page</a> and <a class="af ne" href="https://www.linkedin.com/in/john-leung-639800115/" rel="noopener ugc nofollow" target="_blank">LinkedIn page</a>. By doing so, you can stay updated with exciting content related to data science side projects, and Machine Learning Operations (MLOps) demonstrations methodologies.</p><div class="qo qp qq qr qs qt"><a rel="noopener follow" target="_blank" href="/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----02b97d62a287--------------------------------"><div class="qu ab ig"><div class="qv ab co cb qw qx"><h2 class="bf fr hw z io qy iq ir qz it iv fp bk">Performing Customer Analytics with LangChain and LLMs</h2><div class="ra l"><h3 class="bf b hw z io qy iq ir qz it iv dx">Discover the potentials and constraints of LangChain for customer analytics, accompanied by practical implementation…</h3></div><div class="rb l"><p class="bf b dy z io qy iq ir qz it iv dx">towardsdatascience.com</p></div></div><div class="rc l"><div class="rd l re rf rg rc rh lq qt"/></div></div></a></div><div class="qo qp qq qr qs qt"><a rel="noopener follow" target="_blank" href="/managing-the-technical-debts-of-machine-learning-systems-5b85d420ab9d?source=post_page-----02b97d62a287--------------------------------"><div class="qu ab ig"><div class="qv ab co cb qw qx"><h2 class="bf fr hw z io qy iq ir qz it iv fp bk">Managing the Technical Debts of Machine Learning Systems</h2><div class="ra l"><h3 class="bf b hw z io qy iq ir qz it iv dx">Explore the practices with implementation codes for sustainably mitigating the cost of speedy delivery</h3></div><div class="rb l"><p class="bf b dy z io qy iq ir qz it iv dx">towardsdatascience.com</p></div></div><div class="rc l"><div class="ri l re rf rg rc rh lq qt"/></div></div></a></div></div></div></div></div>    
</body>
</html>