<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Position Embeddings for Vision Transformers, Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Position Embeddings for Vision Transformers, Explained</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/position-embeddings-for-vision-transformers-explained-a6f9add341d5?source=collection_archive---------2-----------------------#2024-02-27">https://towardsdatascience.com/position-embeddings-for-vision-transformers-explained-a6f9add341d5?source=collection_archive---------2-----------------------#2024-02-27</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="3308" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">Vision Transformers Explained Series</h2><div/><div><h2 id="e8d5" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">The Math and the Code Behind Position Embeddings in Vision Transformers</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sjcallis?source=post_page---byline--a6f9add341d5--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Skylar Jean Callis" class="l ep by dd de cx" src="../Images/db4d07b27d7feb86bfbb73b1065aa3a0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*9uFAYZilSG5RVGniO2uBnA.jpeg"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a6f9add341d5--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@sjcallis?source=post_page---byline--a6f9add341d5--------------------------------" rel="noopener follow">Skylar Jean Callis</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a6f9add341d5--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 27, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">4</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="d6bb" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><em class="nk">Since their introduction in 2017 with </em>Attention is All You Need<em class="nk">¹, transformers have established themselves as the state of the art for natural language processing (NLP). In 2021, </em>An Image is Worth 16x16 Words<em class="nk">² successfully adapted transformers for computer vision tasks. Since then, numerous transformer-based architectures have been proposed for computer vision.</em></p><p id="d1a5" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga">This article examines why position embeddings are a necessary component of vision transformers, and how different papers implement position embeddings. It includes open-source code for positional embeddings, as well as conceptual explanations. All of the code uses the PyTorch Python package.</strong></p></div></div><div class="nl"><div class="ab cb"><div class="lr nm ls nn lt no cf np cg nq ci bh"><figure class="nu nv nw nx ny nl nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns nt"><img src="../Images/aede1fd421e2408d6dfb862fc962db3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/0*4SXrUaGxOcNmKG6e"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Photo by <a class="af ol" href="https://unsplash.com/@boliviainteligente?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">BoliviaInteligente</a> on <a class="af ol" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="89c7" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This article is part of a collection examining the internal workings of Vision Transformers in depth. Each of these articles is also available as a Jupyter Notebook with executable code. The other articles in the series are:</p><ul class=""><li id="c8f3" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj om on oo bk"><a class="af ol" rel="noopener" target="_blank" href="/vision-transformers-explained-a9d07147e4c8">Vision Transformers, Explained</a><strong class="mq ga"><br/> </strong>→<a class="af ol" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/VisionTransformersExplained.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter Notebook</a></li><li id="58d1" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" rel="noopener" target="_blank" href="/attention-for-vision-transformers-explained-70f83984c673">Attention for Vision Transformers, Explained</a><br/> → <a class="af ol" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/AttentionExplained.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter Notebook</a></li><li id="e346" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" rel="noopener" target="_blank" href="/position-embeddings-for-vision-transformers-explained-a6f9add341d5"><strong class="mq ga">Position Embeddings for Vision Transformers, Explained</strong></a><br/> → <a class="af ol" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/PositionEmbeddingExplained.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter Notebook</a></li><li id="9495" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" rel="noopener" target="_blank" href="/tokens-to-token-vision-transformers-explained-2fa4e2002daa">Tokens-to-Token Vision Transformers, Explained</a><br/> → <a class="af ol" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/TokensToTokenViTExplained.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter Notebook</a></li><li id="0971" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" href="https://github.com/lanl/vision_transformers_explained" rel="noopener ugc nofollow" target="_blank">GitHub Repository for Vision Transformers, Explained Series</a></li></ul><h2 id="8bd0" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">Table of Contents</h2><ul class=""><li id="4332" class="mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj om on oo bk"><a class="af ol" href="#962e" rel="noopener ugc nofollow">Why Use Position Embeddings?</a></li><li id="3945" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" href="#45ca" rel="noopener ugc nofollow">Attention Invariance Up to Permutation</a></li><li id="9a85" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" href="#91b4" rel="noopener ugc nofollow">Position Embeddings in Literature</a></li><li id="e4a3" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" href="#3186" rel="noopener ugc nofollow">An Example Position Embedding</a><br/> — <a class="af ol" href="#f215" rel="noopener ugc nofollow">Defining the Position Embedding</a><br/> —<a class="af ol" href="#b399" rel="noopener ugc nofollow"> Applying Position Embedding to Tokens</a></li><li id="145f" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" href="#5cbc" rel="noopener ugc nofollow">Conclusion</a><br/> — <a class="af ol" href="#5c67" rel="noopener ugc nofollow">Further Reading</a><br/> — <a class="af ol" href="#718a" rel="noopener ugc nofollow">Citations</a></li></ul><h1 id="962e" class="pt ov fq bf ow pu pv gv pa pw px gy pe py pz qa qb qc qd qe qf qg qh qi qj qk bk">Why Use Position Embeddings?</h1><p id="e2e1" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk"><em class="nk">Attention is All You Need</em>¹ states that transformers, due to their lack of recurrence or convolution, are not capable of learning information about the order of a set of tokens. Without a position embedding, transformers are invariant to the order of the tokens. For images, that means that patches of an image can be scrambled without impacting the predicted output.</p><p id="b49e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Let’s look at an example of patch order on this pixel art <em class="nk">Mountain at Dusk</em> by Luis Zuno (<a class="af ol" href="http://twitter.com/ansimuz" rel="noopener ugc nofollow" target="_blank">@ansimuz</a>)³. The original artwork has been cropped and converted to a single channel image. This means that each pixel has a value between zero and one. Single channel images are typically displayed in grayscale; however, we’ll be displaying it in a purple color scheme because its easier to see.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="295b" class="qp ov fq qm b bg qq qr l qs qt">mountains = np.load(os.path.join(figure_path, 'mountains.npy'))<br/><br/>H = mountains.shape[0]<br/>W = mountains.shape[1]<br/>print('Mountain at Dusk is H =', H, 'and W =', W, 'pixels.')<br/>print('\n')<br/><br/>fig = plt.figure(figsize=(10,6))<br/>plt.imshow(mountains, cmap='Purples_r')<br/>plt.xticks(np.arange(-0.5, W+1, 10), labels=np.arange(0, W+1, 10))<br/>plt.yticks(np.arange(-0.5, H+1, 10), labels=np.arange(0, H+1, 10))<br/>plt.clim([0,1])<br/>cbar_ax = fig.add_axes([0.95, .11, 0.05, 0.77])<br/>plt.clim([0, 1])<br/>plt.colorbar(cax=cbar_ax);<br/>#plt.savefig(os.path.join(figure_path, 'mountains.png'), bbox_inches='tight')</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="a986" class="qp ov fq qm b bg qq qr l qs qt">Mountain at Dusk is H = 60 and W = 100 pixels.</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns qv"><img src="../Images/bac19092e0adc6376b2a9f26c9bcc604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ItBJDmA5Plb3_2pkQypZ5g.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="3052" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">We can split this image up into patches of size 20. (For a more in depth explanation of splitting images into patches, see the <a class="af ol" rel="noopener" target="_blank" href="/vision-transformers-explained-a9d07147e4c8">Vision Transformers article</a>.)</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="172d" class="qp ov fq qm b bg qq qr l qs qt">P = 20<br/>N = int((H*W)/(P**2))<br/>print('There will be', N, 'patches, each', P, 'by', str(P)+'.')<br/>print('\n')<br/><br/>fig = plt.figure(figsize=(10,6))<br/>plt.imshow(mountains, cmap='Purples_r')<br/>plt.clim([0,1])<br/>plt.hlines(np.arange(P, H, P)-0.5, -0.5, W-0.5, color='w')<br/>plt.vlines(np.arange(P, W, P)-0.5, -0.5, H-0.5, color='w')<br/>plt.xticks(np.arange(-0.5, W+1, 10), labels=np.arange(0, W+1, 10))<br/>plt.yticks(np.arange(-0.5, H+1, 10), labels=np.arange(0, H+1, 10))<br/>x_text = np.tile(np.arange(9.5, W, P), 3)<br/>y_text = np.repeat(np.arange(9.5, H, P), 5)<br/>for i in range(1, N+1):<br/>    plt.text(x_text[i-1], y_text[i-1], str(i), color='w', fontsize='xx-large', ha='center')<br/>plt.text(x_text[2], y_text[2], str(3), color='k', fontsize='xx-large', ha='center');<br/>#plt.savefig(os.path.join(figure_path, 'mountain_patches.png'), bbox_inches='tight')</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="8eaf" class="qp ov fq qm b bg qq qr l qs qt">There will be 15 patches, each 20 by 20.</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns qw"><img src="../Images/88fd07364a89357fe4edb4af137ee20d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x25G43UdDV34yR58B79aoA.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="84cb" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The claim is that vision transformers would be unable to distinguish the original image with a version where the patches had been scrambled.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="c192" class="qp ov fq qm b bg qq qr l qs qt">np.random.seed(21)<br/>scramble_order = np.random.permutation(N)<br/>left_x = np.tile(np.arange(0, W-P+1, 20), 3)<br/>right_x = np.tile(np.arange(P, W+1, 20), 3)<br/>top_y = np.repeat(np.arange(0, H-P+1, 20), 5)<br/>bottom_y = np.repeat(np.arange(P, H+1, 20), 5)<br/><br/>scramble = np.zeros_like(mountains)<br/>for i in range(N):<br/>    t = scramble_order[i]<br/>    scramble[top_y[i]:bottom_y[i], left_x[i]:right_x[i]] = mountains[top_y[t]:bottom_y[t], left_x[t]:right_x[t]]<br/>    <br/>fig = plt.figure(figsize=(10,6))<br/>plt.imshow(scramble, cmap='Purples_r')<br/>plt.clim([0,1])<br/>plt.hlines(np.arange(P, H, P)-0.5, -0.5, W-0.5, color='w')<br/>plt.vlines(np.arange(P, W, P)-0.5, -0.5, H-0.5, color='w')<br/>plt.xticks(np.arange(-0.5, W+1, 10), labels=np.arange(0, W+1, 10))<br/>plt.yticks(np.arange(-0.5, H+1, 10), labels=np.arange(0, H+1, 10))<br/>x_text = np.tile(np.arange(9.5, W, P), 3)<br/>y_text = np.repeat(np.arange(9.5, H, P), 5)<br/>for i in range(N):<br/>    plt.text(x_text[i], y_text[i], str(scramble_order[i]+1), color='w', fontsize='xx-large', ha='center')<br/>    <br/>i3 = np.where(scramble_order==2)[0][0]<br/>plt.text(x_text[i3], y_text[i3], str(scramble_order[i3]+1), color='k', fontsize='xx-large', ha='center');<br/>#plt.savefig(os.path.join(figure_path, 'mountain_scrambled_patches.png'), bbox_inches='tight')</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns qw"><img src="../Images/998ab5f46c170298324407b3c25dac1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zRYcfo022gJ5L3T1qMc5qA.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="33dd" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Obviously, this is a very different image from the original, and you wouldn’t want a vision transformer to treat these two images as the same.</p><h1 id="45ca" class="pt ov fq bf ow pu pv gv pa pw px gy pe py pz qa qb qc qd qe qf qg qh qi qj qk bk">Attention Invariance Up to Permutation</h1><p id="bc56" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">Let’s investigate the claim that vision transformers are invariant to the order of the tokens. The component of the transformer that would be invariant to token order is the attention module. While an in depth explanation of the attention module is not the focus of this article, a basis understanding is required. For a more detailed walk through of attention in vision transformers, see the <a class="af ol" rel="noopener" target="_blank" href="/attention-for-vision-transformers-explained-70f83984c673">Attention article</a>.</p><p id="be3c" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Attention is computed from three matrices — <strong class="mq ga">Q</strong>ueries, <strong class="mq ga">K</strong>eys, and <strong class="mq ga">V</strong>alues — each generated from passing the tokens through a linear layer. Once the Q, K, and V matrices are generated, attention is computed using the following formula.</p><figure class="nu nv nw nx ny nl"><div class="qx it l ed"><div class="qy qz l"/></div></figure><p id="4670" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">where <em class="nk">Q, K, V</em>, are the queries, keys, and values, respectively; and dₖ is a scaling value. To demonstrate the invariance of attention to token order, we’ll start with three randomly generated matrices to represent Q, K, and V. The shape of Q, K, and V is as follows:</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div class="nr ns ra"><img src="../Images/f5f80d0fcd4ddbba6b5ab65adeb238e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*xkZWgz3kJWUP2RB5GDlXjA.png"/></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Dimensions of Q, K, and V (image by author)</figcaption></figure><p id="000a" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">We’ll use 4 tokens of projected length 9 in this example. The matrices will contain integers to avoid floating point multiplication errors. Once generated, we’ll switch the position of token 0 and token 2 in all three matrices. Matrices with swapped tokens will be denoted with a subscript <em class="nk">s</em>.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="7086" class="qp ov fq qm b bg qq qr l qs qt">n_tokens = 4<br/>l_tokens = 9<br/>shape = n_tokens, l_tokens<br/>mx = 20 #max integer for generated matricies<br/><br/># Generate Normal Matricies<br/>np.random.seed(21)<br/>Q = np.random.randint(1, mx, shape)<br/>K = np.random.randint(1, mx, shape)<br/>V = np.random.randint(1, mx, shape)<br/><br/># Generate Row-Swapped Matricies<br/>swapQ = copy.deepcopy(Q)<br/>swapQ[[0, 2]] = swapQ[[2, 0]]<br/>swapK = copy.deepcopy(K)<br/>swapK[[0, 2]] = swapK[[2, 0]]<br/>swapV = copy.deepcopy(V)<br/>swapV[[0, 2]] = swapV[[2, 0]]<br/><br/># Plot Matricies<br/>fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(8,8))<br/>fig.tight_layout(pad=2.0)<br/>plt.subplot(3, 2, 1)<br/>mat_plot(Q, 'Q')<br/>plt.subplot(3, 2, 2)<br/>mat_plot(swapQ, r'$Q_S$')<br/>plt.subplot(3, 2, 3)<br/>mat_plot(K, 'K')<br/>plt.subplot(3, 2, 4)<br/>mat_plot(swapK, r'$K_S$')<br/>plt.subplot(3, 2, 5)<br/>mat_plot(V, 'V')<br/>plt.subplot(3, 2, 6)<br/>mat_plot(swapV, r'$V_S$')</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns rb"><img src="../Images/bdab71e349e66c6d7287f798c1d22a04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CvM9nXBjeUHTIwZ1yDydCQ.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="f54f" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The first matrix multiplication in the attention formula is <em class="nk">Q·Kᵀ=A</em>, where the resulting matrix <em class="nk">A</em> is a square with size equal to the number of tokens. When we compute <em class="nk">A</em>ₛ with <em class="nk">Qₛ</em> and <em class="nk">Kₛ,</em> the resulting <em class="nk">A</em>ₛ has both rows [0, 2] and columns [0,2] swapped from <em class="nk">A</em>.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="4afa" class="qp ov fq qm b bg qq qr l qs qt">A = Q @ K.transpose()<br/>swapA = swapQ @ swapK.transpose()<br/>modA = copy.deepcopy(A)<br/>modA[[0,2]] = modA[[2,0]] #swap rows<br/>modA[:, [2, 0]] = modA[:, [0, 2]] #swap cols<br/><br/>fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(8,3))<br/>fig.tight_layout(pad=1.0)<br/>plt.subplot(1, 3, 1)<br/>mat_plot(A, r'$A = Q*K^T$')<br/>plt.subplot(1, 3, 2)<br/>mat_plot(swapA, r'$A_S = Q_S * K_S^T$')<br/>plt.subplot(1, 3, 3)<br/>mat_plot(modA, 'A\nwith rows [0,2] swaped\n and cols [0,2] swaped')</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns rc"><img src="../Images/a1145c9d3fa382b5df7f69b9b79d0860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vo4-6C4sHtWxgb3umP9yww.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="4926" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The next matrix multiplication is <em class="nk">A·V=A, </em>where the resulting matrix <em class="nk">A</em> has the same shape as the initial <em class="nk">Q</em>, <em class="nk">K</em>, and <em class="nk">V</em> matrices. When we compute <em class="nk">A</em>ₛ with <em class="nk">A</em>ₛ and <em class="nk">V</em>ₛ, the resulting <em class="nk">A</em>ₛ has rows [0,2] swapped from <em class="nk">A</em>.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="71da" class="qp ov fq qm b bg qq qr l qs qt">A = A @ V<br/>swapA = swapA @ swapV<br/>modA = copy.deepcopy(A)<br/>modA[[0,2]] = modA[[2,0]] #swap rows<br/><br/>fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 7))<br/>fig.tight_layout(pad=1.0)<br/>plt.subplot(2, 2, 1)<br/>mat_plot(A, r'$A = A*V$')<br/>plt.subplot(2, 2, 2)<br/>mat_plot(swapA, r'$A_S = A_S * V_S$')<br/>plt.subplot(2, 2, 4)<br/>mat_plot(modA, 'A\nwith rows [0,2] swaped')<br/>axs[1,0].axis('off')</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns rd"><img src="../Images/cfc007fb59a66b13dae529fa3c769700.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vU6apRmL3D-s5AriIDPmbg.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="fdb4" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This demonstrates that changing the order of the tokens in the input to an attention layer results in an output attention matrix with the same token rows changed. This remains intuitive, as attention is a computation of the relationship between the tokens. Without position information, changing the token order does not change how the tokens are related. It isn’t obvious to me why this permutation of the output isn’t enough information to convey position to the transformers. However, everything I’ve read says that it isn’t enough, so we accept that and move forward.</p><h1 id="91b4" class="pt ov fq bf ow pu pv gv pa pw px gy pe py pz qa qb qc qd qe qf qg qh qi qj qk bk">Position Embeddings in Literature</h1><p id="8a65" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">In addition to the theoretically justification for positional embeddings, models that utilize position embeddings perform with higher accuracy than models without. However, there isn’t clear evidence supporting one type of position embedding over another.</p><p id="5266" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">In <em class="nk">Attention is All You Need</em>¹, they use a fixed sinusoidal positional embedding. They note that they experimented with a learned positional embedding, but observed “nearly identical results.” Note that this model was designed for NLP applications, specifically translation. The authors proceeded with the fixed embedding because it allowed for varying phrase lengths. This would likely not be a concern in computer vision applications.</p><p id="bef8" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">In <em class="nk">An Image is Worth 16x16 Words², </em>they apply positional embeddings to images. They run ablation studies on four different position embeddings in both fixed and learnable settings. This study encompasses no position embedding, a 1D position embedding, a 2D position embedding, and a relative position embedding. They find that models with a position embedding significantly outperform models without a position embedding. However, there is little difference between their different types of positional embeddings or between the fixed and learnable embeddings. This is congruent with the results in [1] that a position embedding is beneficial, though the exact embedding chosen is of little consequence.</p><p id="40f6" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">In <em class="nk">Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</em>⁴, they use a sinusoidal position embedding that they describe as being the same as in [2]. Their released code mirrors the equations for the sinusoidal position embedding in [1]. Furthermore, their released code fixes the position embedding rather than letting it be a learned parameter with a sinusoidal initialization.</p><h1 id="3186" class="pt ov fq bf ow pu pv gv pa pw px gy pe py pz qa qb qc qd qe qf qg qh qi qj qk bk">An Example Position Embedding</h1><h2 id="f215" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">Defining the Position Embedding</h2><p id="1170" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">Now, we can look at the specifics of a sinusoidal position embedding. The code is based on the publicly available GitHub code for <em class="nk">Tokens-to-Token ViT⁴. </em>Functionally, the position embedding is a matrix with the same shape as the tokens. This looks like:</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div class="nr ns ra"><img src="../Images/f5f80d0fcd4ddbba6b5ab65adeb238e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*xkZWgz3kJWUP2RB5GDlXjA.png"/></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Shape of Positional Embedding Matrix (image by author)</figcaption></figure><p id="c876" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The formulae for the sinusoidal position embedding from [1] look like</p><figure class="nu nv nw nx ny nl"><div class="qx it l ed"><div class="re qz l"/></div></figure><p id="61da" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">where <em class="nk">PE</em> is the position embedding matrix, <em class="nk">i</em> is along the number of tokens, <em class="nk">j</em> is along the length of the tokens, and <em class="nk">d</em> is the token length.</p><p id="6534" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">In code, that looks like</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="dc36" class="qp ov fq qm b bg qq qr l qs qt">def get_sinusoid_encoding(num_tokens, token_len):<br/>    """ Make Sinusoid Encoding Table<br/><br/>        Args:<br/>            num_tokens (int): number of tokens<br/>            token_len (int): length of a token<br/>            <br/>        Returns:<br/>            (torch.FloatTensor) sinusoidal position encoding table<br/>    """<br/><br/>    def get_position_angle_vec(i):<br/>        return [i / np.power(10000, 2 * (j // 2) / token_len) for j in range(token_len)]<br/><br/>    sinusoid_table = np.array([get_position_angle_vec(i) for i in range(num_tokens)])<br/>    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])<br/>    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) <br/><br/>    return torch.FloatTensor(sinusoid_table).unsqueeze(0)</span></pre><p id="abb9" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Let’s generate an example position embedding matrix. We’ll use 176 tokens. Each token has length 768, which is the default in the T2T-ViT⁴ code. Once the matrix is generated, we can plot it.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="2404" class="qp ov fq qm b bg qq qr l qs qt">PE = get_sinusoid_encoding(num_tokens=176, token_len=768)</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="1ca5" class="qp ov fq qm b bg qq qr l qs qt">fig = plt.figure(figsize=(10, 8))<br/>plt.imshow(PE[0, :, :], cmap='PuOr_r')<br/>plt.xlabel('Along Length of Token')<br/>plt.ylabel('Individual Tokens');<br/>cbar_ax = fig.add_axes([0.95, .36, 0.05, 0.25])<br/>plt.clim([-1, 1])<br/>plt.colorbar(label='Value of Position Encoding', cax=cbar_ax);<br/>#plt.savefig(os.path.join(figure_path, 'fullPE.png'), bbox_inches='tight')</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns rf"><img src="../Images/16b01a1c812ab95b2afb2a8a08525acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ADTsUhGY18RJvM7p6Cmoag.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="b5fb" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Let’s zoom in to the beginning of the tokens.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="ffa2" class="qp ov fq qm b bg qq qr l qs qt">fig = plt.figure()<br/>plt.imshow(PE[0, :, 0:301], cmap='PuOr_r')<br/>plt.xlabel('Along Length of Token')<br/>plt.ylabel('Individual Tokens');<br/>cbar_ax = fig.add_axes([0.95, .2, 0.05, 0.6])<br/>plt.clim([-1, 1])<br/>plt.colorbar(label='Value of Position Encoding', cax=cbar_ax);<br/>#plt.savefig(os.path.join(figure_path, 'zoomedinPE.png'), bbox_inches='tight')</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns rg"><img src="../Images/63dfc1170d206e3115781d4cbef6912c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*39ChEhCQihp6gC4rAo2J-g.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="5e05" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">It certainly has a sinusoidal structure!</p><h2 id="b399" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">Applying Position Embedding to Tokens</h2><p id="9d3e" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">Now, we can add our position embedding to our tokens! We’re going to use Mountain at Dusk³ with the same <em class="nk">patch tokenization</em> as <a class="af ol" href="#962e" rel="noopener ugc nofollow">above</a>. That will give us 15 tokens of length 20²=400. For more detail about patch tokenization, see the <a class="af ol" rel="noopener" target="_blank" href="/vision-transformers-explained-a9d07147e4c8">Vision Transformers</a> article. Recall that the patches look like:</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="6cda" class="qp ov fq qm b bg qq qr l qs qt">fig = plt.figure(figsize=(10,6))<br/>plt.imshow(mountains, cmap='Purples_r')<br/>plt.hlines(np.arange(P, H, P)-0.5, -0.5, W-0.5, color='w')<br/>plt.vlines(np.arange(P, W, P)-0.5, -0.5, H-0.5, color='w')<br/>plt.xticks(np.arange(-0.5, W+1, 10), labels=np.arange(0, W+1, 10))<br/>plt.yticks(np.arange(-0.5, H+1, 10), labels=np.arange(0, H+1, 10))<br/>x_text = np.tile(np.arange(9.5, W, P), 3)<br/>y_text = np.repeat(np.arange(9.5, H, P), 5)<br/>for i in range(1, N+1):<br/>    plt.text(x_text[i-1], y_text[i-1], str(i), color='w', fontsize='xx-large', ha='center')<br/>plt.text(x_text[2], y_text[2], str(3), color='k', fontsize='xx-large', ha='center')<br/>cbar_ax = fig.add_axes([0.95, .11, 0.05, 0.77])<br/>plt.clim([0, 1])<br/>plt.colorbar(cax=cbar_ax);<br/>#plt.savefig(os.path.join(figure_path, 'mountain_patches_w_colorbar.png'), bbox_inches='tight')</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns qv"><img src="../Images/11bd562506206b1f5a05c054ff83a67f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DKLHCa5eGhhvZIqHdxNT-Q.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="40b2" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">When we convert those patches into tokens, it looks like</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="baa7" class="qp ov fq qm b bg qq qr l qs qt">tokens = np.zeros((15, 20**2))<br/>for i in range(15):<br/>    patch = gray_mountains[top_y[i]:bottom_y[i], left_x[i]:right_x[i]]<br/>    tokens[i, :] = patch.reshape(1, 20**2)<br/>tokens = tokens.astype(int)<br/>tokens = tokens/255<br/><br/>fig = plt.figure(figsize=(10,6))<br/>plt.imshow(tokens, aspect=5, cmap='Purples_r')<br/>plt.xlabel('Length of Tokens')<br/>plt.ylabel('Number of Tokens')<br/>cbar_ax = fig.add_axes([0.95, .36, 0.05, 0.25])<br/>plt.clim([0, 1])<br/>plt.colorbar(cax=cbar_ax)</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns rh"><img src="../Images/d7d447490aacbe2532d564161b45c48c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_c8_SgBrF4w6_ot3Xr-0-g.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="e898" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Now, we can make a position embedding in the correct shape:</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="f3ee" class="qp ov fq qm b bg qq qr l qs qt">PE = get_sinusoid_encoding(num_tokens=15, token_len=400).numpy()[0,:,:]<br/>fig = plt.figure(figsize=(10,6))<br/>plt.imshow(PE, aspect=5, cmap='PuOr_r')<br/>plt.xlabel('Length of Tokens')<br/>plt.ylabel('Number of Tokens')<br/>cbar_ax = fig.add_axes([0.95, .36, 0.05, 0.25])<br/>plt.clim([0, 1])<br/>plt.colorbar(cax=cbar_ax)</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns ri"><img src="../Images/5276f44b88e53efb58f30c6dd19f32dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bc9Ib0mhFKd3lSWQygVLUA.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="99ac" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">We’re ready now to add the position embedding to the tokens. Purple areas in the position embedding will make the tokens darker, while orange areas will make them lighter.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="5660" class="qp ov fq qm b bg qq qr l qs qt">mountainsPE = tokens + PE<br/>resclaed_mtPE = (position_mountains - np.min(position_mountains)) / np.max(position_mountains - np.min(position_mountains))<br/><br/>fig = plt.figure(figsize=(10,6))<br/>plt.imshow(resclaed_mtPE, aspect=5, cmap='Purples_r')<br/>plt.xlabel('Length of Tokens')<br/>plt.ylabel('Number of Tokens')<br/>cbar_ax = fig.add_axes([0.95, .36, 0.05, 0.25])<br/>plt.clim([0, 1])<br/>plt.colorbar(cax=cbar_ax)</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns rh"><img src="../Images/ca7cf7be43b83f9cc99c411fe5125f5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1f3CFnrFCDdZxmaTKUTzXQ.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="10fe" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">You can see the structure from the original tokens, as well as the structure in the position embedding! Both pieces of information are present to be passed forward into the transformer.</p><h1 id="5cbc" class="pt ov fq bf ow pu pv gv pa pw px gy pe py pz qa qb qc qd qe qf qg qh qi qj qk bk">Conclusion</h1><p id="2eb1" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">Now, you should have some intuition of how position embeddings help vision transformers learn. The code in this article an be found in the <a class="af ol" href="https://github.com/lanl/vision_transformers_explained" rel="noopener ugc nofollow" target="_blank">GitHub repository</a> for this series. The code from the T2T-ViT paper⁴ can be found <a class="af ol" href="https://github.com/yitu-opensource/T2T-ViT" rel="noopener ugc nofollow" target="_blank">here</a>. Happy transforming!</p><p id="5d81" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This article was approved for release by Los Alamos National Laboratory as LA-UR-23–33876. The associated code was approved for a BSD-3 open source license under O#4693.</p><h2 id="5c67" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">Further Reading</h2><p id="70b9" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">To learn more about position embeddings in NLP contexts, see</p><ul class=""><li id="7934" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj om on oo bk">A Gentle Introduction to Positional Encoding in Transformer Models: <a class="af ol" href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/" rel="noopener ugc nofollow" target="_blank">https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/</a></li></ul><p id="a667" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For a video lecture broadly about vision transformers (with relevant chapters noted), see</p><ul class=""><li id="536f" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj om on oo bk">Vision Transformer and its Applications: <a class="af ol" href="https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP" rel="noopener ugc nofollow" target="_blank">https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP</a> <br/> — Vision Transformer is Invariant to Position of Patches 10:44–12:52 (<a class="af ol" href="https://youtu.be/hPb6A92LROc?t=644&amp;si=Keu-5i9BQ5c69mxz" rel="noopener ugc nofollow" target="_blank">https://youtu.be/hPb6A92LROc?t=644&amp;si=Keu-5i9BQ5c69mxz</a>)<br/> — Position Embedding 12:52–14:15 (<a class="af ol" href="https://youtu.be/hPb6A92LROc?t=772&amp;si=spdlYZl-TRgbGgzn" rel="noopener ugc nofollow" target="_blank">https://youtu.be/hPb6A92LROc?t=772&amp;si=spdlYZl-TRgbGgzn</a>)</li></ul><h2 id="718a" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">Citations</h2><p id="17e6" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">[1] Vaswani et al (2017).<em class="nk"> Attention Is All You Need. </em><a class="af ol" href="https://doi.org/10.48550/arXiv.1706.03762" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.1706.03762</a></p><p id="b4ca" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">[2] Dosovitskiy et al (2020). <em class="nk">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. </em><a class="af ol" href="https://doi.org/10.48550/arXiv.2010.11929" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.2010.11929</a></p><p id="508a" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">[3] Luis Zuno (<a class="af ol" href="http://twitter.com/ansimuz" rel="noopener ugc nofollow" target="_blank">@ansimuz</a>). <em class="nk">Mountain at Dusk Background. </em>License CC0: <a class="af ol" href="https://opengameart.org/content/mountain-at-dusk-background" rel="noopener ugc nofollow" target="_blank">https://opengameart.org/content/mountain-at-dusk-background</a></p><p id="4b33" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">[4] Yuan et al (2021). <em class="nk">Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</em>. <a class="af ol" href="https://doi.org/10.48550/arXiv.2101.11986" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.2101.11986</a><br/> → GitHub code: <a class="af ol" href="https://github.com/yitu-opensource/T2T-ViT" rel="noopener ugc nofollow" target="_blank">https://github.com/yitu-opensource/T2T-ViT</a></p></div></div></div></div>    
</body>
</html>