<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Mastering Marketing Mix Modelling In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Mastering Marketing Mix Modelling In Python</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mastering-marketing-mix-modelling-in-python-7bbfe31360f9?source=collection_archive---------1-----------------------#2024-09-26">https://towardsdatascience.com/mastering-marketing-mix-modelling-in-python-7bbfe31360f9?source=collection_archive---------1-----------------------#2024-09-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="9b35" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Part 1 of a hands-on guide to help you master MMM in <strong class="al">pymc</strong></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@raz1470?source=post_page---byline--7bbfe31360f9--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ryan O'Sullivan" class="l ep by dd de cx" src="../Images/7cd161d38d67d2c0b7da2d8f3e7d33fe.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*tAw1S072P0f0sUswKPN6VQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--7bbfe31360f9--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@raz1470?source=post_page---byline--7bbfe31360f9--------------------------------" rel="noopener follow">Ryan O'Sullivan</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--7bbfe31360f9--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">21 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/963728066476cce1befa283fb3499efc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oU4kTQPyifUMDVGxWmjhMQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><h1 id="981f" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">What is this series about?</h1><p id="3391" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Welcome to part 1 of my series on marketing mix modeling (MMM), a hands-on guide to help you master MMM. Throughout this series, we’ll cover key topics such as model training, validation, calibration and budget optimisation, all using the powerful <strong class="oa fr">pymc-marketing</strong> python package. Whether you’re new to MMM or looking to sharpen your skills, this series will equip you with practical tools and insights to improve your marketing strategies.</p><h1 id="5975" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Introduction</h1><p id="1fd8" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In this article, we’ll start by providing some background on Bayesian MMM, including the following topics:</p><ul class=""><li id="7d0f" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">What open source packages can we use?</li><li id="76ff" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">What is Bayesian MMM?</li><li id="c72b" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">What are Bayesian priors?</li><li id="ccdc" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">Are the default priors in <strong class="oa fr">pymc-marketing</strong> sensible?</li></ul><p id="2b84" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We will then move on to a walkthrough in Python using the <strong class="oa fr">pymc-marketing</strong> package, diving deep into the following areas:</p><ul class=""><li id="a3e8" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">Simulating data</li><li id="adcb" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">Training the model</li><li id="2d17" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">Validating the model</li><li id="6759" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">Parameter recovery</li></ul><p id="72f0" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The full notebook can be found here:</p><div class="ph pi pj pk pl pm"><a href="https://github.com/raz1470/pymc_marketing/blob/main/notebooks/1.%20training%20marketing%20mix%20models%20%28MMM%29%20in%20python.ipynb?source=post_page-----7bbfe31360f9--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab ig"><div class="po ab co cb pp pq"><h2 class="bf fr hw z io pr iq ir ps it iv fp bk">pymc_marketing/notebooks/1. training marketing mix models (MMM) in python.ipynb at main ·…</h2><div class="pt l"><h3 class="bf b hw z io pr iq ir ps it iv dx">A demo of the MMM package pymc_marketing. Contribute to raz1470/pymc_marketing development by creating an account on…</h3></div><div class="pu l"><p class="bf b dy z io pr iq ir ps it iv dx">github.com</p></div></div><div class="pv l"><div class="pw l px py pz pv qa lr pm"/></div></div></a></div><h1 id="3476" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">1.0 MMM Background</h1><p id="f610" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Let’s begin with a brief overview of Marketing Mix Modeling (MMM). We’ll explore various open-source packages available for implementation and delve into the principles of Bayesian MMM, including the concept of priors. Finally, we’ll evaluate the default priors used in <strong class="oa fr">pymc-marketing</strong> to gauge their suitability and effectiveness..</p><h2 id="4f47" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">1.1 What open source packages can we use?</h2><p id="5a51" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">When it comes to MMM, there are several open-source packages we can use:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qs"><img src="../Images/52f6723ecd7065016292c73060be7eba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*fMXc-Wk8alWmJvJbffWxsw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="29b2" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">There are several compelling reasons to focus on <strong class="oa fr">pymc-marketing</strong> in this series:</p><ol class=""><li id="ef08" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot qt pa pb bk"><strong class="oa fr">Python Compatibility</strong>: Unlike Robyn, which is available only in R, pymc-marketing caters to a broader audience who prefer working in Python.</li><li id="f717" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot qt pa pb bk"><strong class="oa fr">Current Availability</strong>: Meridian has not yet been released (as of September 23, 2024), making pymc-marketing a more accessible option right now.</li><li id="2463" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot qt pa pb bk"><strong class="oa fr">Future Considerations</strong>: LightweightMMM is set to be decommissioned once Meridian is launched, further solidifying the need for a reliable alternative.</li><li id="a7e8" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot qt pa pb bk"><strong class="oa fr">Active Development</strong>: pymc-marketing is continuously enhanced by its extensive community of contributors, ensuring it remains up-to-date with new features and improvements.</li></ol><p id="24d4" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">You can check out the pymc-marketing package here, they offer some great notebook to illustrate some of the packages functionality:</p><div class="ph pi pj pk pl pm"><a href="https://www.pymc-marketing.io/en/stable/notebooks/index.html?source=post_page-----7bbfe31360f9--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab ig"><div class="po ab co cb pp pq"><h2 class="bf fr hw z io pr iq ir ps it iv fp bk">How-to - pymc-marketing 0.9.0 documentation</h2><div class="pt l"><h3 class="bf b hw z io pr iq ir ps it iv dx">Here you will find a collection of examples and how-to guides for using PyMC-Marketing MMM and CLV models.</h3></div><div class="pu l"><p class="bf b dy z io pr iq ir ps it iv dx">www.pymc-marketing.io</p></div></div></div></a></div><h2 id="1892" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">1.2 What is Bayesian MMM?</h2><p id="d2e5" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">You’ll notice that 3 out of 4 of the packages highlighted above take a Bayesian approach. So let’s take a bit of time to understand what Bayesian MMM looks like! For beginners, Bayesian analysis is a bit of a rabbit hole but we can break it down into 5 key points:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qu"><img src="../Images/fe739acf49ab2eca4bf2ebf2e8a8805b.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*lVCjcq-9CoQSJiO7qD0pkg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><ol class=""><li id="b82d" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot qt pa pb bk"><strong class="oa fr">Bayes’ Theorem </strong>— In Bayesian MMM, Bayes’ theorem is used to update our beliefs about how marketing channels impact sales as we gather new data. For example, if we have some initial belief about how much TV advertising affects sales, Bayes’ theorem allows us to refine that belief after seeing actual data on sales and TV ad spend.</li><li id="35e7" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot qt pa pb bk"><strong class="oa fr">P(θ)</strong> — <strong class="oa fr">Priors</strong> represent our initial beliefs about the parameters in the model, such as the effect of TV spend on sales. For example, if we ran a geo-lift test which estimated that TV ads increase sales by 5%, we might set a prior based on that estimate.</li><li id="4ce1" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot qt pa pb bk"><strong class="oa fr">P(Data | θ) </strong>—<strong class="oa fr"> The likelihood function,</strong> which captures the probability of observing the sales data given specific levels of marketing inputs. For example, if you spent £100,000 on social media and saw a corresponding sales increase, the likelihood tells us how probable that sales jump is based on the assumed effect of social media.</li><li id="b2fa" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot qt pa pb bk"><strong class="oa fr">P(θ | Data) </strong>— <strong class="oa fr">The posterior</strong> is what we ultimately care about in Bayesian MMM — it’s our updated belief about how different marketing channels impact sales after combining the data and our prior assumptions. For instance, after observing new sales data, our initial belief that TV ads drive a 5% increase in sales might be adjusted to a more refined estimate, like 4.7%.</li><li id="64d8" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot qt pa pb bk"><strong class="oa fr">Sampling</strong> — Since Bayesian MMM involves complex models with multiple parameters (e.g. adstock, saturation, marketing channel effects, control effects etc.) computing the posterior directly can be difficult. MCMC (Markov Chain Monte Carlo) allows us to approximate the posterior by generating samples from the distribution of each parameter. This approach is particularly helpful when dealing with models that would be hard to solve analytically.</li></ol><h2 id="f2b6" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">1.3 What are Bayesian priors?</h2><p id="2fcb" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Bayesian priors are supplied as probability distributions. Instead of assigning a fixed value to a parameter, we provide a range of potential values, along with the likelihood of each value being the true one. Common distributions used for priors include:</p><ul class=""><li id="ee20" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk"><strong class="oa fr">Normal: </strong>For parameters where we expect values to cluster around a mean.</li><li id="696f" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">Half-Normal</strong>: For parameters where we want to enforce positivity.</li><li id="4bbc" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">Beta</strong>: For parameters which are constrained between 0 and 1.</li><li id="432b" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">Gamma</strong>: For parameters that are positive and skewed.</li></ul><p id="b648" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">You may hear the term informative priors. Ideally, we supply these based on expert knowledge or randomized experiments. Informative priors reflect strong beliefs about the parameter values. However, when this is not feasible, we can use non-informative priors, which spread probability across a wide range of values. Non-informative priors allow the data to dominate the posterior, preventing prior assumptions from overly influencing the outcome.</p><h2 id="9139" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">1.4 Are the default priors in pymc-marketing sensible?</h2><p id="226b" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">When using the<strong class="oa fr"> pymc-marketing</strong> package, the default priors are designed to be weakly informative, meaning they provide broad guidance without being overly specific. Instead of being overly specific, they guide the model without restricting it too much. This balance ensures the priors guide the model without overshadowing the data..</p><p id="9148" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">To build reliable models, it’s crucial to understand the default priors rather than use them blindly. In the following sections, we will examine the various priors used in <strong class="oa fr">pymc-marketin</strong>g and explain why the default choices are sensible for marketing mix models.</p><p id="ba2b" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We can start by using the code below to inspect the default priors:</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="222c" class="qz nd fq qw b bg ra rb l rc rd">dummy_model = MMM(<br/>    date_column="",<br/>    channel_columns=[""],<br/>    adstock=GeometricAdstock(l_max=4),<br/>    saturation=LogisticSaturation(),<br/>)<br/>dummy_model.default_model_config</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk re"><img src="../Images/9400cb34fdda8bf521e61419a8d59c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*lfGS13fuvscjc5_neiVAbA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="0b79" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Above I have printed out a dictionary containing the 7 default priors — Let’s start by briefly understanding what each one is:</p><ul class=""><li id="4318" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk"><strong class="oa fr">intercept</strong> — The baseline level of sales or target variable in the absence of any marketing spend or other variables. It sets the starting point for the model.</li><li id="65e9" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">likelihood</strong> — When you increase the focus on the likelihood, the model relies more heavily on the observed data and less on the priors. This means the model will be more data-driven, allowing the observed outcomes to have a stronger influence on the parameter estimates</li><li id="73e0" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">gamma_control</strong> — Control variables that account for external factors, such as macroeconomic conditions, holidays, or other non-marketing variables that might influence sales.</li><li id="185b" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">gamma_fourier</strong> — Fourier terms used to model seasonality in the data, capturing recurring patterns or cycles in sales.</li><li id="1a5f" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">adstock_alpha</strong> — Controls the adstock effect, determining how much the impact of marketing spend decays over time.</li><li id="4017" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">saturation_lamda</strong> — Defines the steepness of the saturation curve, determining how quickly diminishing returns set in as marketing spend increases.</li><li id="70e1" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">saturation_beta</strong> — The marketing spend coefficient, which measures the direct effect of marketing spend on the target variable (e.g. sales).</li></ul><p id="ee47" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Next we are going to focus on gaining a more in depth understanding of parameters for marketing and control variables.</p><h2 id="ea58" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">1.4.1 Adstock alpha</h2><p id="7cd0" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Adstock reflects the idea that the influence of a marketing activity is delayed and builds up over time. The<strong class="oa fr"> </strong>adstock alpha (the decay rate)<strong class="oa fr"> </strong>controls how quickly the effect diminishes over time, determining how long the impact of the marketing activity continues to influence sales.</p><p id="9b7c" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">A beta distribution is used as a prior for adstock alpha. Let’s first visualise the beta distribution:</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="1d6a" class="qz nd fq qw b bg ra rb l rc rd">alpha = 1<br/>beta_param = 3<br/><br/>x1 = np.linspace(0, 1, 100)<br/>y1 = beta.pdf(x1, alpha, beta_param)<br/><br/>plt.figure(figsize=(8, 5))<br/>plt.plot(x1, y1, color='blue')<br/>plt.fill_between(x1, y1, color='blue', alpha=0.3)<br/>plt.title('Geometric Adstock: Beta distribution (alpha=1, beta=3)')<br/>plt.xlabel('Adstock alpha')<br/>plt.ylabel('Probability density')<br/>plt.grid(True)<br/>plt.show()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/e3671fd16838d7163f2631f2d0ded101.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DAVA3vgKB3wIvy2-OeMuzw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="b922" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We typically constrain adstock alpha values between 0 and 1, making the beta distribution a sensible choice. Specifically, using a beta(1, 3) prior for adstock alpha reflects the belief that, in most cases, the decay rate should be relatively high, meaning the effect of marketing activities wears off quickly.</p><p id="492c" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">To build further intuition, we can visualise the effect of different adstock alpha values:</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="1889" class="qz nd fq qw b bg ra rb l rc rd">raw_spend = np.array([1000, 900, 800, 700, 600, 500, 400, 300, 200, 100, 0, 0, 0, 0, 0, 0])<br/><br/>adstock_spend_1 = geometric_adstock(x=raw_spend, alpha=0.20, l_max=8, normalize=True).eval().flatten()<br/>adstock_spend_2 = geometric_adstock(x=raw_spend, alpha=0.50, l_max=8, normalize=True).eval().flatten()<br/>adstock_spend_3 = geometric_adstock(x=raw_spend, alpha=0.80, l_max=8, normalize=True).eval().flatten()<br/><br/>plt.figure(figsize=(10, 6))<br/><br/>plt.plot(raw_spend, marker='o', label='Raw Spend', color='blue')<br/>plt.fill_between(range(len(raw_spend)), 0, raw_spend, color='blue', alpha=0.2)<br/><br/>plt.plot(adstock_spend_1, marker='o', label='Adstock (alpha=0.20)', color='orange')<br/>plt.fill_between(range(len(adstock_spend_1)), 0, adstock_spend_1, color='orange', alpha=0.2)<br/><br/>plt.plot(adstock_spend_2, marker='o', label='Adstock (alpha=0.50)', color='red')<br/>plt.fill_between(range(len(adstock_spend_2)), 0, adstock_spend_2, color='red', alpha=0.2)<br/><br/>plt.plot(adstock_spend_3, marker='o', label='Adstock (alpha=0.80)', color='purple')<br/>plt.fill_between(range(len(adstock_spend_3)), 0, adstock_spend_3, color='purple', alpha=0.2)<br/><br/>plt.xlabel('Weeks')<br/>plt.ylabel('Spend')<br/>plt.title('Geometric Adstock')<br/>plt.legend()<br/>plt.grid(True)<br/>plt.show()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rg"><img src="../Images/bdaf927c95b02e76d85e9809154dbf5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WbqM9S49LDrqQB-enIvRaw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><ul class=""><li id="9338" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">Low values of alpha have little impact and are suitable for channels which have a direct response e.g. paid social performance ads with a direct call to action aimed at prospects who have already visited your website.</li><li id="d73a" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">Higher values of alpha have a stronger impact and are suitable for channels which have a longer term effect e.g. brand building videos with no direct call to action aimed at a broad range of prospects.</li></ul><h2 id="ea82" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">1.4.2 Saturation lamda</h2><p id="111a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">As we increase marketing spend, it’s incremental impact of sales slowly starts to reduce — This is known as saturation. Saturation lamda controls the steepness of the saturation curve, determining how quickly diminishing returns set in.</p><p id="9dd7" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">A gamma distribution is used as a prior for saturation lambda. Let’s start by understanding what a gamma distribution looks like:</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="8c47" class="qz nd fq qw b bg ra rb l rc rd">alpha = 3<br/>beta = 1<br/><br/>x2 = np.linspace(0, 10, 1000)<br/>y2 = gamma.pdf(x2, alpha, scale=1/beta)<br/><br/>plt.figure(figsize=(8, 6))<br/>plt.plot(x2, y2, 'b-')<br/>plt.fill_between(x2, y2, alpha=0.2, color='blue')<br/>plt.title('Logistic Saturation: Gamma Distribution (alpha=3, beta=1)')<br/>plt.xlabel('Saturation lamda')<br/>plt.ylabel('Probability density')<br/>plt.grid(True)<br/>plt.show()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/479cf4d94568e019efdd618fce3ed7df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P0sMQ0YQ8bDDumB93YIh2w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="9d36" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">At first glance it can be difficult to understand why the gamma distribution is a sensible choice of prior but plotting the impact of different lamda values helps clarify its appropriateness::</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="e8b2" class="qz nd fq qw b bg ra rb l rc rd">scaled_spend = np.linspace(start=0.0, stop=1.0, num=100)<br/><br/>saturated_spend_1 = logistic_saturation(x=scaled_spend, lam=1).eval()<br/>saturated_spend_2 = logistic_saturation(x=scaled_spend, lam=2).eval()<br/>saturated_spend_4 = logistic_saturation(x=scaled_spend, lam=4).eval()<br/>saturated_spend_8 = logistic_saturation(x=scaled_spend, lam=8).eval()<br/><br/>plt.figure(figsize=(8, 6))<br/>sns.lineplot(x=scaled_spend, y=saturated_spend_1, label="1")<br/>sns.lineplot(x=scaled_spend, y=saturated_spend_2, label="2")<br/>sns.lineplot(x=scaled_spend, y=saturated_spend_4, label="4")<br/>sns.lineplot(x=scaled_spend, y=saturated_spend_8, label="8")<br/><br/>plt.title('Logistic Saturation')<br/>plt.xlabel('Scaled Marketing Spend')<br/>plt.ylabel('Saturated Marketing Spend')<br/>plt.legend(title='Lambda')<br/>plt.grid(True)<br/>plt.show()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/5609476231ba26040181014d24f8e204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BaSRklS__0jTvinbJg9_EQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><ul class=""><li id="394d" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">A lamda value of 1 keeps the relationship linear.</li><li id="6b6e" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">As we increase lamda, the steepness of the saturation curve increases.</li><li id="bc28" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">From the chart we hopefully agree that it seems unlikely to have a saturation much steeper than what we see when the lamda value is 8.</li></ul><h2 id="b501" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">1.4.3 Saturation beta</h2><p id="5c9d" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Saturation beta corresponds to the marketing channel coefficient, measuring the impact of marketing spend.</p><p id="50a2" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The half-normal prior is used as it enforces positivity which is a very reasonable assumption e.g. marketing shouldn’t have a negative effect. When sigma is set as 2, it tends towards low values. This helps regularize the coefficients, pulling them towards lower values unless there is strong evidence in the data that a particular channel has a significant impact.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="d80f" class="qz nd fq qw b bg ra rb l rc rd">sigma = 2<br/><br/>x3 = np.linspace(0, 10, 1000)<br/>y3 = halfnorm.pdf(x3, scale=sigma)<br/><br/>plt.figure(figsize=(8, 6))<br/>plt.plot(x3, y3, 'b-')<br/>plt.fill_between(x3, y3, alpha=0.2, color='blue')<br/>plt.title('Saturation beta prior: HalfNormal Distribution (sigma=2)')<br/>plt.xlabel('Saturation beta')<br/>plt.ylabel('Probability Density')<br/>plt.grid(True)<br/>plt.show()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/eccd7a85e2d0c231a137f92cad24fc2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NGU9c9RXdYqUYCmaC4b6qQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="cbd8" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Remember that both the marketing and target variables are scaled (ranging from 0 to 1), so the beta prior must be in this scaled space.</p><h2 id="3293" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">1.4.4 Gamma control</h2><p id="0cf5" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The gamma control parameter is the coefficient for the control variables that account for external factors, such as macroeconomic conditions, holidays, or other non-marketing variables.</p><p id="b1ae" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">A normal distribution is used which allows for both positive and negative effects:</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="fa92" class="qz nd fq qw b bg ra rb l rc rd">mu = 0<br/>sigma = 2<br/><br/>x = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)<br/>y = norm.pdf(x, mu, sigma)<br/><br/>plt.figure(figsize=(8, 5))<br/>plt.plot(x, y, color='blue')<br/>plt.fill_between(x, y, color='blue', alpha=0.3)<br/>plt.title('Control: Normal distribution (mu=0, sigma=2)')<br/>plt.xlabel('Control value')<br/>plt.ylabel('Probability density')<br/>plt.grid(True)<br/>plt.show()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/707d7b29c3f5727586082ecd0a4dfb96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nreTu2JdJxppYEiTzyeAFA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="e9dd" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Control variables are standardized, with values ranging from -1 to 1, so the gamma control prior must be in this scaled space.</p><h1 id="9322" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">2.0 Python Walkthrough</h1><p id="2caf" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Now we have covered some of the theory, let’s but some of it into practice! In this walkthrough we will cover:</p><ul class=""><li id="d623" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">Simulating data</li><li id="0222" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">Training the model</li><li id="3d7a" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">Validating the model</li><li id="1561" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">Parameter recovery</li></ul><p id="a456" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The aim is to create some realistic training data where we set the parameters ourselves (adstock, saturation, beta etc.), train and validate a model utilising the pymc-marketing package, and then assess how well our model did at recovering the parameters.</p><p id="71ad" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">When it comes to real world MMM data, you won’t know the parameters, but this exercise of parameter recovery is a great way to learn and get confident in MMM.</p><h2 id="2e7a" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">2.1 Simulating data</h2><p id="bab9" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Let’s start by simulating some data to train a model. The benefit of this approach is that we can set the parameters ourselves, which allows us to conduct a parameter recovery exercise to test how well our model performs!</p><p id="bae6" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The function below can be used to simulate some data with the following characteristics:</p><ol class=""><li id="cb9b" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot qt pa pb bk">A trend component with some growth.</li><li id="ab36" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot qt pa pb bk">A seasonal component with oscillation around 0.</li><li id="41a0" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot qt pa pb bk">The trend and seasonal component are used to create demand.</li><li id="3a92" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot qt pa pb bk">A proxy for demand is created which will be available for the model (demand will be an unobserved confounder).</li><li id="a465" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot qt pa pb bk">Demand is a key driver of sales.</li><li id="3622" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot qt pa pb bk">Each marketing channel also contributes to sales, the marketing spend is correlated with demand and the appropriate transformation are applied (scaling, adstock, saturation).</li></ol><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="cc35" class="qz nd fq qw b bg ra rb l rc rd">import pandas as pd<br/>import numpy as np<br/>from pymc_marketing.mmm.transformers import geometric_adstock, logistic_saturation<br/>from sklearn.preprocessing import MaxAbsScaler<br/><br/>def data_generator(start_date, periods, channels, spend_scalar, adstock_alphas, saturation_lamdas, betas, freq="W"):<br/>    '''<br/>    Generates a synthetic dataset for a MMM with trend, seasonality, and channel-specific contributions.<br/><br/>    Args:<br/>        start_date (str or pd.Timestamp): The start date for the generated time series data.<br/>        periods (int): The number of time periods (e.g., days, weeks) to generate data for.<br/>        channels (list of str): A list of channel names for which the model will generate spend and sales data.<br/>        spend_scalar (list of float): Scalars that adjust the raw spend for each channel to a desired scale.<br/>        adstock_alphas (list of float): The adstock decay factors for each channel, determining how much past spend influences the current period.<br/>        saturation_lamdas (list of float): Lambda values for the logistic saturation function, controlling the saturation effect on each channel.<br/>        betas (list of float): The coefficients for each channel, representing the contribution of each channel's impact on sales.<br/><br/>    Returns:<br/>        pd.DataFrame: A DataFrame containing the generated time series data, including demand, sales, and channel-specific metrics.<br/>    '''<br/>    <br/>    # 0. Create time dimension<br/>    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)<br/>    df = pd.DataFrame({'date': date_range})<br/>    <br/>    # 1. Add trend component with some growth<br/>    df["trend"]= (np.linspace(start=0.0, stop=20, num=periods) + 5) ** (1 / 8) - 1<br/>    <br/>    # 2. Add seasonal component with oscillation around 0<br/>    df["seasonality"] = df["seasonality"] = 0.1 * np.sin(2 * np.pi * df.index / 52)<br/>    <br/>    # 3. Multiply trend and seasonality to create overall demand with noise<br/>    df["demand"] = df["trend"] * (1 + df["seasonality"]) + np.random.normal(loc=0, scale=0.10, size=periods)<br/>    df["demand"] = df["demand"] * 1000<br/>    <br/>    # 4. Create proxy for demand, which is able to follow demand but has some noise added<br/>    df["demand_proxy"] = np.abs(df["demand"]* np.random.normal(loc=1, scale=0.10, size=periods))<br/>    <br/>    # 5. Initialize sales based on demand<br/>    df["sales"] = df["demand"]<br/>    <br/>    # 6. Loop through each channel and add channel-specific contribution<br/>    for i, channel in enumerate(channels):<br/>        <br/>        # Create raw channel spend, following demand with some random noise added<br/>        df[f"{channel}_spend_raw"] = df["demand"] * spend_scalar[i]<br/>        df[f"{channel}_spend_raw"] = np.abs(df[f"{channel}_spend_raw"] * np.random.normal(loc=1, scale=0.30, size=periods))<br/>               <br/>        # Scale channel spend<br/>        channel_transformer = MaxAbsScaler().fit(df[f"{channel}_spend_raw"].values.reshape(-1, 1))<br/>        df[f"{channel}_spend"] = channel_transformer .transform(df[f"{channel}_spend_raw"].values.reshape(-1, 1))<br/>        <br/>        # Apply adstock transformation<br/>        df[f"{channel}_adstock"] = geometric_adstock(<br/>            x=df[f"{channel}_spend"].to_numpy(),<br/>            alpha=adstock_alphas[i],<br/>            l_max=8, normalize=True<br/>        ).eval().flatten()<br/>        <br/>        # Apply saturation transformation<br/>        df[f"{channel}_saturated"] = logistic_saturation(<br/>            x=df[f"{channel}_adstock"].to_numpy(),<br/>            lam=saturation_lamdas[i]<br/>        ).eval()<br/>        <br/>        # Calculate contribution to sales<br/>        df[f"{channel}_sales"] = df[f"{channel}_saturated"] * betas[i]<br/>        <br/>        # Add the channel-specific contribution to sales<br/>        df["sales"] += df[f"{channel}_sales"]<br/>    <br/>    return df</span></pre><p id="6f0d" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We can now call the data generator function with some parameters which are realistic:</p><ul class=""><li id="bcd3" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">3 years of weekly data.</li><li id="e808" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">3 channels from different parts of the marketing funnel.</li><li id="91f8" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">Each channel has a different adstock, saturation and beta parameter.</li></ul><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="5efb" class="qz nd fq qw b bg ra rb l rc rd">np.random.seed(10)<br/><br/># Set parameters for data generator<br/>start_date = "2021-01-01"<br/>periods = 52 * 3<br/>channels = ["tv", "social", "search"]<br/>adstock_alphas = [0.50, 0.25, 0.05]<br/>saturation_lamdas = [1.5, 2.5, 3.5]<br/>betas = [350, 150, 50]<br/>spend_scalars = [10, 15, 20]<br/><br/>df = dg.data_generator(start_date, periods, channels, spend_scalars, adstock_alphas, saturation_lamdas, betas)<br/><br/># Scale betas using maximum sales value - this is so it is comparable to the fitted beta from pymc (pymc does feature and target scaling using MaxAbsScaler from sklearn)<br/>betas_scaled = [<br/>    ((df["tv_sales"] / df["sales"].max()) / df["tv_saturated"]).mean(),<br/>    ((df["social_sales"] / df["sales"].max()) / df["social_saturated"]).mean(),<br/>    ((df["search_sales"] / df["sales"].max()) / df["search_saturated"]).mean()<br/>]<br/><br/># Calculate contributions - these will be used later on to see how accurate the contributions from our model are<br/>contributions = np.asarray([<br/>    round((df["tv_sales"].sum() / df["sales"].sum()), 2),<br/>    round((df["social_sales"].sum() / df["sales"].sum()), 2),<br/>    round((df["search_sales"].sum() / df["sales"].sum()), 2),<br/>    round((df["demand"].sum() / df["sales"].sum()), 2)<br/>])<br/><br/>df[["date", "demand", "demand_proxy", "tv_spend_raw", "social_spend_raw", "search_spend_raw", "sales"]]</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rh"><img src="../Images/7ca303d078ee53b448bb1ee9b57a01a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NXKz7AdypBg-MPzWp766Ew.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="e6d7" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Before we move onto training a model, let’s spend some time understanding the data we have generated.</p><ul class=""><li id="da90" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">The intuition behind how we have derived demand is that there has been an increasing trend for organic growth, with the addition of a high and low demand period each year.</li></ul><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="434a" class="qz nd fq qw b bg ra rb l rc rd">plt.figure(figsize=(8, 5))<br/><br/>sns.lineplot(x=df['date'], y=df['trend']*1000, label="Trend", color="green")<br/>sns.lineplot(x=df['date'], y=df['seasonality']*1000, label="Seasonality", color="orange")<br/>sns.lineplot(x=df['date'], y=df['demand'], label="Demand", color="blue")<br/><br/>plt.title('Components', fontsize=16)<br/>plt.xlabel('Date', fontsize=12)<br/>plt.ylabel('Value', fontsize=12)<br/>plt.xticks(rotation=45, ha='right')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/18d3746085c06c6c3d5c76a0b81d6cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u1R8TJxt0jnohh6PHlcY1Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><ul class=""><li id="09c8" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">We created a proxy for demand to be used as a control variable in the model. The idea here is that, in reality demand is an unobserved confounder, but we can sometimes find proxies for demand. One example of this is google search trends data for the product you are selling.</li></ul><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="77ac" class="qz nd fq qw b bg ra rb l rc rd">plt.figure(figsize=(8, 5))<br/><br/>sns.scatterplot(x=df['demand_proxy'], y=df['demand'], color="blue")<br/><br/>plt.title('Demand proxy vs demand', fontsize=16)<br/>plt.xlabel('Demand proxy', fontsize=12)<br/>plt.ylabel('Demand', fontsize=12)<br/>plt.xticks(rotation=45, ha='right')<br/>plt.show()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/bb3c5867096a652e29c78489641372fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s8R1hLxHWKhVzlAkq2sxfQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><ul class=""><li id="07d2" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">Spend follows the same trend for each marketing channel but the random noise we added should allow the model to unpick how each one is contributing to sales.</li></ul><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="ffea" class="qz nd fq qw b bg ra rb l rc rd">plt.figure(figsize=(8, 5))<br/><br/>sns.lineplot(x=df['date'], y=df['tv_spend_raw'], label=channels[0], color="orange")<br/>sns.lineplot(x=df['date'], y=df['social_spend_raw'], label=channels[1], color="blue")<br/>sns.lineplot(x=df['date'], y=df['search_spend_raw'], label=channels[2], color="green")<br/>plt.title('Marketing Channel Spend', fontsize=16)<br/>plt.xlabel('Date', fontsize=12)<br/>plt.ylabel('Value', fontsize=12)<br/>plt.xticks(rotation=45, ha='right')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/2d95c270ebfb611c102827cb886fa0c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pQPE2KUNP62jPXGpE0Yf1A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><ul class=""><li id="f8db" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">We applied two transformation to our marketing data, adstock and saturation. Below we can look at the effect of the different parameters we used for each channels saturation, with search having the steepest saturation and TV almost being linear.</li></ul><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="7087" class="qz nd fq qw b bg ra rb l rc rd">plt.figure(figsize=(8, 5))<br/><br/>sns.lineplot(x=df['tv_adstock'], y=df['tv_saturated'], label=channels[0], color="orange")<br/>sns.lineplot(x=df['social_adstock'], y=df['social_saturated'], label=channels[1], color="blue")<br/>sns.lineplot(x=df['search_adstock'], y=df['search_saturated'], label=channels[2], color="green")<br/><br/>plt.title('Marketing Spend Saturation', fontsize=16)<br/>plt.xlabel('Adstocked spend', fontsize=12)<br/>plt.ylabel('Saturated spend', fontsize=12)<br/>plt.legend()<br/>plt.show()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/f29c738a4a8ac910bf98e1128814e3d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-iQUoFPNJQ3EVFfEyEJOCw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><ul class=""><li id="b27d" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">Below we can see that our variables are highly correlated, something that is very common in MMM data due to marketing teams spending more in peak periods.</li></ul><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="fede" class="qz nd fq qw b bg ra rb l rc rd">plt.figure(figsize=(8, 8))<br/>sns.heatmap(df[["demand", "demand_proxy", "tv_spend_raw", "social_spend_raw", "search_spend_raw", "sales"]].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)<br/>plt.title('Correlation Heatmap')<br/>plt.show()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ri"><img src="../Images/51b7a970f682018e63613dfd3777be03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dr3WBtGU4YWy8Wzl1xE7Cg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><ul class=""><li id="f543" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">And finally let’s take a look at sales — Remember our aim is to understand how marketing has contributed to sales.</li></ul><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="250f" class="qz nd fq qw b bg ra rb l rc rd">plt.figure(figsize=(8, 5))<br/><br/>sns.lineplot(x=df['date'], y=df['sales'], label="sales", color="green")<br/><br/>plt.title('Sales', fontsize=16)<br/>plt.xlabel('Date', fontsize=12)<br/>plt.ylabel('Value', fontsize=12)<br/>plt.xticks(rotation=45, ha='right')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/5279c48b359969074e3dd862faba5bbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KMLLc0rRC37dBsd11Vvy4w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="3660" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Now that we have a good understanding of the data generating process, let’s dive into training the model!</p><h2 id="9ce0" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">2.2 Training the model</h2><p id="58ab" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">It’s now time to move on to training the model. To begin with we need to prepare the training data by:</p><ul class=""><li id="e2b2" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">Splitting data into features and target.</li><li id="6223" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">Creating indices for train and out-of-time slices — The out-of-time slice will help us validate our model.</li></ul><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="3c7b" class="qz nd fq qw b bg ra rb l rc rd"># set date column<br/>date_col = "date"<br/><br/># set outcome column<br/>y_col = "sales"<br/><br/># set marketing variables<br/>channel_cols = ["tv_spend_raw",<br/>                "social_spend_raw",<br/>                "search_spend_raw"]<br/><br/># set control variables<br/>control_cols = ["demand_proxy"]<br/><br/># split data into features and target<br/>X = df[[date_col] + channel_cols + control_cols]<br/>y = df[y_col]<br/><br/># set test (out-of-sample) length<br/>test_len = 8<br/><br/># create train and test indices<br/>train_idx = slice(0, len(df) - test_len)<br/>out_of_time_idx = slice(len(df) - test_len, len(df))</span></pre><p id="7529" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Next we initiate the MMM class. A major challenge in MMM is the fact that the ratio of parameters to training observations is high. One way we can mitigate this is by being pragmatic about the choice of transformations:</p><ul class=""><li id="811d" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">We select geometric adstock which has 1 parameter (per channel) vs 2 compared to using weibull adstock.</li><li id="ec67" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">We select logistic saturation which has 1 parameter (per channel) vs 2 compared to using hill saturation.</li><li id="7381" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">We use a proxy for demand and decide not to include a seasonal component or time varying trend which further reduces our parameters.</li><li id="9245" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk">We decide not to include time varying media parameters on the basis that, although it is true that performance varies over time, we ultimately want a response curve to help us optimise budgets and taking the average performance over the training dataset is a good way to do this.</li></ul><p id="df42" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">To summarise my view here, the more complexity we add to the model, the more we manipulate our marketing spend variables to fit the data which in turn gives us a “better model” (e.g. high R-squared and low mean squared error). But this is at the risk of moving away from the true data generating process which in-turn will give us biased causal effect estimates.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="6faa" class="qz nd fq qw b bg ra rb l rc rd">mmm_default = MMM(<br/>    adstock=GeometricAdstock(l_max=8),<br/>    saturation=LogisticSaturation(),<br/>    date_column=date_col,<br/>    channel_columns=channel_cols,<br/>    control_columns=control_cols,<br/>)<br/><br/>mmm_default.default_model_config</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk re"><img src="../Images/9400cb34fdda8bf521e61419a8d59c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*lfGS13fuvscjc5_neiVAbA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="9be5" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Now let’s fit the model using the train indices. I’ve passed some optional key-word-arguments, let’s spend a bit of time understanding what they do:</p><ul class=""><li id="984f" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk"><strong class="oa fr">Tune</strong> — This sets the number of tuning steps for the sampler. During tuning, the sampler adjusts its parameters to efficiently explore the posterior distribution. These initial 1,000 samples are discarded and not used for inference.</li><li id="aac8" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">Chains</strong> — This specifies the number of independent Markov chains to run. Running multiple chains helps assess convergence and provides a more robust sampling of the posterior.</li><li id="dc8c" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">Draws </strong>— This sets the number of samples to draw from the posterior distribution per chain, after tuning.</li><li id="c32c" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">Target accept </strong>— This is the target acceptance rate for the sampler. It helps balance exploration of the parameter space with efficiency.</li></ul><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="a042" class="qz nd fq qw b bg ra rb l rc rd">fit_kwargs = {<br/>    "tune": 1_000,<br/>    "chains": 4,<br/>    "draws": 1_000,<br/>    "target_accept": 0.9,<br/>}<br/><br/>mmm_default.fit(X[train_idx], y[train_idx], **fit_kwargs)</span></pre><p id="5d96" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">I’d advise sticking with the defaults here, and only changing them if you get some issues in the next steps in terms of divergences.</p><h2 id="ce60" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">2.3 Validating the model</h2><p id="1865" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">After fitting the model, the first step is to check for divergences. Divergences indicate potential problems with either the model or the sampling process. Although delving deeply into divergences is beyond the scope of this article due to its complexity, it’s essential to note their importance in model validation.</p><p id="2350" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Below, we check the number of divergences in our model. A result of 0 divergences indicates a good start.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="c148" class="qz nd fq qw b bg ra rb l rc rd">mmm_default.idata["sample_stats"]["diverging"].sum().item()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rj"><img src="../Images/777b7fb5560a8528aeecf40c563bc8a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:62/format:webp/1*3t3SbT4jreqr7MB2Ilor9A.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="d75b" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Next we can get a comprehensive summary of the MCMC sampling results. Let’s focus on the key ones:</p><ul class=""><li id="6129" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk"><strong class="oa fr">mean</strong> — The average value of the parameter across all samples.</li><li id="2a13" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">hdi_3% and hdi_97%</strong> — The lower and upper bounds of the 94% Highest Density Interval (HDI). A 94% HDI means that there’s a 94% probability that the true value of the parameter lies within this interval, based on the observed data and prior information.</li><li id="762a" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">rhat </strong>— Gelman-Rubin statistic, measuring convergence across chains. Values close to 1 (typically &lt; 1.05) indicate good convergence.</li></ul><p id="8421" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Our R-hats are all very close to 1, which is expected given the absence of divergences. We will revisit the mean parameter values in the next section when we conduct a parameter recovery exercise.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="bb95" class="qz nd fq qw b bg ra rb l rc rd">az.summary(<br/>    data=mmm_default.fit_result,<br/>    var_names=[<br/>        "intercept",<br/>        "y_sigma",<br/>        "saturation_beta",<br/>        "saturation_lam",<br/>        "adstock_alpha",<br/>        "gamma_control",<br/>    ],<br/>)</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rk"><img src="../Images/a240798d58cfb3beced35bc3f44392ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xwje96C_JCKssZux9qPSTQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="91ff" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Next, we generate diagnostic plots that are crucial for assessing the quality of our MCMC sampling:</p><ul class=""><li id="2b26" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk"><strong class="oa fr">Posterior Distribution (left):</strong> Displays the value of each parameter throughout the MCMC sampling. Ideally, these should be smooth and unimodal, with all chains showing similar distributions.</li><li id="9778" class="ny nz fq oa b go pc oc od gr pd of og oh pe oj ok ol pf on oo op pg or os ot oz pa pb bk"><strong class="oa fr">Trace Plots (right):</strong> Illustrate the value of each parameter over the MCMC sampling process. Any trends or slow drifts may indicate poor mixing or non-convergence, while chains that don’t overlap could suggest they are stuck in different modes of the posterior.</li></ul><p id="8e5f" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Looking at our diagnostic plots there are no red flags.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="2a97" class="qz nd fq qw b bg ra rb l rc rd">_ = az.plot_trace(<br/>    data=mmm_default.fit_result,<br/>    var_names=[<br/>        "intercept",<br/>        "y_sigma",<br/>        "saturation_beta",<br/>        "saturation_lam",<br/>        "adstock_alpha",<br/>        "gamma_control",<br/>    ],<br/>    compact=True,<br/>    backend_kwargs={"figsize": (12, 10), "layout": "constrained"},<br/>)<br/>plt.gcf().suptitle("Model Trace", fontsize=16);</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/562fd3b92302d6301106a3300701dbd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AeArPNawhE7gCqHUVItyMg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="8a69" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">For each parameter we have a distribution of possible values which reflects the uncertainty in the parameter estimates. For the next set of diagnostics, we first need to sample from the posterior. This allows us to make predictions, which we will do for the training data.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="7598" class="qz nd fq qw b bg ra rb l rc rd">mmm_default.sample_posterior_predictive(X[train_idx], extend_idata=True, combined=True)</span></pre><p id="5290" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We can begin the posterior predictive checking diagnostics by visually assessing whether the observed data falls within the predicted ranges. It appears that our model has performed well.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="b41b" class="qz nd fq qw b bg ra rb l rc rd">mmm_default.plot_posterior_predictive(original_scale=True);</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/943e1bad64df9f23b6c48ad2687d8052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ZJlrwIxfb-PWD541QQTpA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="9d27" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Next we can calculate residuals between the posterior predictive mean and the actual data. We can plot these residuals over time to check for patterns or autocorrelation. It looks like the residuals oscillate around 0 as expected.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="e314" class="qz nd fq qw b bg ra rb l rc rd">mmm_default.plot_errors(original_scale=True);</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/69861606a8e25d688f660ac44ea6b4b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BkhVcBujC_gHmQXhAg6yfg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="83b7" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We can also check whether the residuals are normally distributed around 0. Again, we pass this diagnostic test.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="ad42" class="qz nd fq qw b bg ra rb l rc rd">errors = mmm_default.get_errors(original_scale=True)<br/><br/>fig, ax = plt.subplots(figsize=(8, 6))<br/>az.plot_dist(<br/>    errors, quantiles=[0.25, 0.5, 0.75], color="C3", fill_kwargs={"alpha": 0.7}, ax=ax<br/>)<br/>ax.axvline(x=0, color="black", linestyle="--", linewidth=1, label="zero")<br/>ax.legend()<br/>ax.set(title="Errors Posterior Distribution");</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/68f34d1d28ffb345180286225eee8eba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XVLc6yOJ7Z0NRivD_QDnxA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="6f60" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Finally it is good practice to assess how good our model is at predicting outside of the training sample. First of all we need to sample from the posterior again but this time using the out-of-time data. We can then plot the observed sales against the predicted.</p><p id="605d" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Below we can see that the observed sales generally lie within the intervals and the model seems to do a good job.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="526e" class="qz nd fq qw b bg ra rb l rc rd">y_out_of_sample = mmm_default.sample_posterior_predictive(<br/>    X_pred=X[out_of_time_idx], extend_idata=False, include_last_observations=True<br/>)<br/><br/>def plot_in_sample(X, y, ax, n_points: int = 15):<br/>    (<br/>        y.to_frame()<br/>        .set_index(X[date_col])<br/>        .iloc[-n_points:]<br/>        .plot(ax=ax, marker="o", color="black", label="actuals")<br/>    )<br/>    return ax<br/><br/><br/>def plot_out_of_sample(X_out_of_sample, y_out_of_sample, ax, color, label):<br/>    y_out_of_sample_groupby = y_out_of_sample["y"].to_series().groupby("date")<br/><br/>    lower, upper = quantiles = [0.025, 0.975]<br/>    conf = y_out_of_sample_groupby.quantile(quantiles).unstack()<br/>    ax.fill_between(<br/>        X_out_of_sample[date_col].dt.to_pydatetime(),<br/>        conf[lower],<br/>        conf[upper],<br/>        alpha=0.25,<br/>        color=color,<br/>        label=f"{label} interval",<br/>    )<br/><br/>    mean = y_out_of_sample_groupby.mean()<br/>    mean.plot(ax=ax, marker="o", label=label, color=color, linestyle="--")<br/>    ax.set(ylabel="Original Target Scale", title="Out of sample predictions for MMM")<br/>    return ax<br/><br/><br/>_, ax = plt.subplots()<br/>plot_in_sample(X, y, ax=ax, n_points=len(X[out_of_time_idx])*3)<br/>plot_out_of_sample(<br/>    X[out_of_time_idx], y_out_of_sample, ax=ax, label="out of sample", color="C0"<br/>)<br/>ax.legend(loc="upper left");</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/d38d396327df862fae164b3befa5a31a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-_mjUKcukauZmvKHrrE13g.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="72c3" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Based on our findings in this section, we believe we have a robust model. In the next section let’s carry out a parameter recovery exercise to see how close to the ground truth parameters we were.</p><h2 id="43fa" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">2.4 Parameter recovery</h2><p id="46ac" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In the last section, we validated the model as we would in a real-life scenario. In this section, we will conduct a parameter recovery exercise: remember, we are only able to do this because we simulated the data ourselves and stored the true parameters.</p><h2 id="3893" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">2.4.1 Parameter recovery — Adstock</h2><p id="8825" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Let’s begin by comparing the posterior distribution of the adstock parameters to the ground truth values that we previously stored in the adstock_alphas list. The model performed reasonably well, achieving the correct rank order, and the true values consistently lie within the posterior distribution.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="04dd" class="qz nd fq qw b bg ra rb l rc rd">fig = mmm_default.plot_channel_parameter(param_name="adstock_alpha", figsize=(9, 5))<br/>ax = fig.axes[0]<br/>ax.axvline(x=adstock_alphas[0], color="C0", linestyle="--", label=r"$\alpha_1$")<br/>ax.axvline(x=adstock_alphas[1], color="C1", linestyle="--", label=r"$\alpha_2$")<br/>ax.axvline(x=adstock_alphas[2], color="C2", linestyle="--", label=r"$\alpha_3$")<br/>ax.legend(loc="upper right");</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rl"><img src="../Images/892a862a32784f26e6da5ec8f9b39a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wDuXc8LrasvqLi4nJwBUUw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><h2 id="7f67" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">2.4.2 Parameter recovery — Saturation</h2><p id="78fd" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">When we examine saturation, the model performs excellently in recovering lambda for TV, but it does not do as well in recovering the true values for social and search, although the results are not disastrous.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="1227" class="qz nd fq qw b bg ra rb l rc rd">fig = mmm_default.plot_channel_parameter(param_name="saturation_lam", figsize=(9, 5))<br/>ax = fig.axes[0]<br/>ax.axvline(x=saturation_lamdas[0], color="C0", linestyle="--", label=r"$\lambda_1$")<br/>ax.axvline(x=saturation_lamdas[1], color="C1", linestyle="--", label=r"$\lambda_2$")<br/>ax.axvline(x=saturation_lamdas[2], color="C2", linestyle="--", label=r"$\lambda_3$")<br/>ax.legend(loc="upper right");</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rl"><img src="../Images/418eee6dc457947cc8bcc7aae7732297.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IBThTJZJB9BROmEepQ4vBQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><h2 id="67c3" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">2.4.3 Parameter recovery — Channel betas</h2><p id="6b03" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Regarding the channel beta parameters, the model achieves the correct rank ordering, but it overestimates values for all channels. In the next section, we will quantify this in terms of contribution.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="89f3" class="qz nd fq qw b bg ra rb l rc rd">fig = mmm_default.plot_channel_parameter(param_name="saturation_beta", figsize=(9, 5))<br/>ax = fig.axes[0]<br/>ax.axvline(x=betas_scaled[0], color="C0", linestyle="--", label=r"$\beta_1$")<br/>ax.axvline(x=betas_scaled[1], color="C1", linestyle="--", label=r"$\beta_2$")<br/>ax.axvline(x=betas_scaled[2], color="C2", linestyle="--", label=r"$\beta_3$")<br/>ax.legend(loc="upper right");</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rl"><img src="../Images/5e5d4ae64fa29861fd4c5733a829deaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ihLQjmZUWCJkBbIMrlDRJg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><h2 id="2632" class="qb nd fq bf ne qc qd qe nh qf qg qh nk oh qi qj qk ol ql qm qn op qo qp qq qr bk">2.4.4 Parameter recovery — Channel contribution</h2><p id="68d6" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">First, we calculate the ground truth contribution for each channel. We will also calculate the contribution of demand: remember we included a proxy for demand not demand itself.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="212e" class="qz nd fq qw b bg ra rb l rc rd">channels = np.array(["tv", "social", "search", "demand"])<br/><br/>true_contributions = pd.DataFrame({'Channels': channels, 'Contributions': contributions})<br/>true_contributions= true_contributions.sort_values(by='Contributions', ascending=False).reset_index(drop=True)<br/>true_contributions = true_contributions.style.bar(subset=['Contributions'], color='lightblue')<br/><br/>true_contributions</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rm"><img src="../Images/036cd2b56ddb8b1aa652170a03e81b41.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*iT78UyHnmlXdgb7XoPFlFg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="3d73" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Now, let’s plot the contributions from the model. The rank ordering for demand, TV, social, and search is correct. However, TV, social, and search are all overestimated. This appears to be driven by the demand proxy not contributing as much as true demand.</p><pre class="mm mn mo mp mq qv qw qx bp qy bb bk"><span id="cab4" class="qz nd fq qw b bg ra rb l rc rd">mmm_default.plot_waterfall_components_decomposition(figsize=(10,6));</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rg"><img src="../Images/9222b02732fff116d22c5b9cb5e18b7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rH6W_96BnotdiJgue72gVg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><h1 id="258b" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Closing thoughts</h1><p id="b0d1" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In section 2.3, we validated the model and concluded that it was robust. However, the parameter recovery exercise demonstrated that our model considerably overestimates the effect of marketing. This overestimation is driven by the confounding factor of demand. In a real-life scenario, it is not possible to run a parameter recovery exercise. So, how would you identify that your model’s marketing contributions are biased? And once identified, how would you address this issue? This leads us to our next article on calibrating models!</p></div></div></div><div class="ab cb rn ro rp rq" role="separator"><span class="rr by bm rs rt ru"/><span class="rr by bm rs rt ru"/><span class="rr by bm rs rt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="ad99" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">I hope you enjoyed this first instalment! Follow me if you want to continue this path towards mastering MMM — In the next article we will shift our focus to calibrating our models using informative priors from experiments.</p></div></div></div></div>    
</body>
</html>