# ä½¿ç”¨ Hugging Face Transformers å¾®è°ƒéŸ³é¢‘å…‰è°±å›¾å˜æ¢å™¨

> åŸæ–‡ï¼š[`towardsdatascience.com/fine-tune-the-audio-spectrogram-transformer-with-transformers-73333c9ef717?source=collection_archive---------4-----------------------#2024-08-21`](https://towardsdatascience.com/fine-tune-the-audio-spectrogram-transformer-with-transformers-73333c9ef717?source=collection_archive---------4-----------------------#2024-08-21)

## å­¦ä¹ å¦‚ä½•å¾®è°ƒéŸ³é¢‘å…‰è°±å›¾å˜æ¢å™¨æ¨¡å‹ï¼Œä»¥ä¾¿è¿›è¡Œæ‚¨è‡ªå·±çš„æ•°æ®éŸ³é¢‘åˆ†ç±»

[](https://medium.com/@marius_s?source=post_page---byline--73333c9ef717--------------------------------)![Marius Steger](https://medium.com/@marius_s?source=post_page---byline--73333c9ef717--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--73333c9ef717--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--73333c9ef717--------------------------------) [Marius Steger](https://medium.com/@marius_s?source=post_page---byline--73333c9ef717--------------------------------)

Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--73333c9ef717--------------------------------) Â·13 åˆ†é’Ÿé˜…è¯»Â·2024 å¹´ 8 æœˆ 21 æ—¥

--

![](img/7f8ad4be45f2ff60c16110e638acb74c.png)

å¾®è°ƒéŸ³é¢‘åˆ†ç±»æ¨¡å‹ï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒï¼Œèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°ä½¿ç”¨æ•°æ®ï¼Œä»è€Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è·å¾—æ›´å¥½çš„ç»“æœ | *å›¾åƒæ¥è‡ªä½œè€…*

éŸ³é¢‘åˆ†ç±»æ˜¯æœºå™¨å­¦ä¹ ä¸­éŸ³é¢‘ç†è§£çš„å…³é”®ä»»åŠ¡ä¹‹ä¸€ï¼Œå¹¶ä¸ºè®¸å¤š AI ç³»ç»Ÿæä¾›äº†æ„å»ºåŸºç¡€ã€‚å®ƒé©±åŠ¨ç€å·¥ä¸šé¢†åŸŸçš„åº”ç”¨ï¼Œä¾‹å¦‚[æµ‹è¯•æ•°æ®è¯„ä¼°](https://renumics.com/use-cases/test-data)ã€é”™è¯¯å’Œå¼‚å¸¸æ£€æµ‹ï¼Œæˆ–é¢„æµ‹æ€§ç»´æŠ¤ã€‚é¢„è®­ç»ƒçš„å˜æ¢å™¨æ¨¡å‹ï¼Œå¦‚éŸ³é¢‘å…‰è°±å›¾å˜æ¢å™¨ï¼ˆASTï¼‰[1]ï¼Œä¸ºè¿™äº›åº”ç”¨æä¾›äº†å¼ºå¤§çš„åŸºç¡€ï¼Œå…·æœ‰é²æ£’æ€§å’Œçµæ´»æ€§ã€‚

å°½ç®¡ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ª AST æ¨¡å‹éœ€è¦å¤§é‡æ•°æ®ï¼Œä½†ä½¿ç”¨å·²ç»å­¦ä¹ äº†éŸ³é¢‘ç‰¹å¾çš„é¢„è®­ç»ƒæ¨¡å‹ä¼šæ›´é«˜æ•ˆã€‚é€šè¿‡ä½¿ç”¨ç‰¹å®šäºæˆ‘ä»¬ç”¨ä¾‹çš„æ•°æ®å¾®è°ƒè¿™äº›æ¨¡å‹ï¼Œå¯¹äºä½¿å…¶é€‚ç”¨äºæˆ‘ä»¬çš„ç‰¹å®šåº”ç”¨ç¨‹åºè‡³å…³é‡è¦ã€‚è¿™ä¸ªè¿‡ç¨‹å°†æ¨¡å‹çš„èƒ½åŠ›è°ƒæ•´ä¸ºæˆ‘ä»¬æ•°æ®é›†çš„ç‹¬ç‰¹ç‰¹å¾ï¼Œå¦‚ç±»åˆ«å’Œæ•°æ®åˆ†å¸ƒï¼Œç¡®ä¿ç»“æœçš„ç›¸å…³æ€§ã€‚

![](img/895d4939ab3d1eb6e2da381a10bcfbe0.png)

éŸ³é¢‘å…‰è°±å›¾å˜æ¢å™¨æ ¹æ®éŸ³é¢‘æ ·æœ¬çš„å…‰è°±å›¾é¢„æµ‹ç±»åˆ« | *å›¾åƒæ¥è‡ªä½œè€…*

AST æ¨¡å‹ä¸ Hugging Face ğŸ¤— [Transformers](https://huggingface.co/docs/transformers/index)åº“é›†æˆï¼Œç”±äºåœ¨éŸ³é¢‘åˆ†ç±»ä»»åŠ¡ä¸­æ˜“äºä½¿ç”¨ä¸”æ€§èƒ½å¼ºå¤§ï¼Œå·²æˆä¸ºçƒ­é—¨é€‰æ‹©ã€‚æœ¬æŒ‡å—å°†å¸¦é¢†æˆ‘ä»¬å®Œæˆå¯¹é¢„è®­ç»ƒ AST æ¨¡å‹ï¼ˆâ€œ[*MIT/ast-finetuned-audioset-10â€“10â€“0.4593*](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)*â€*ï¼‰è¿›è¡Œå¾®è°ƒçš„æ•´ä¸ªè¿‡ç¨‹ï¼Œä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„æ•°æ®ï¼Œåœ¨[ESC50 æ•°æ®é›†](https://github.com/karolpiczak/ESC-50)[2]ä¸Šè¿›è¡Œæ¼”ç¤ºã€‚åˆ©ç”¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿå’Œ PyTorch ä½œä¸ºåç«¯çš„å·¥å…·ï¼Œæˆ‘ä»¬å°†æ¶µç›–ä»æ•°æ®å‡†å¤‡å’Œé¢„å¤„ç†åˆ°æ¨¡å‹é…ç½®å’Œè®­ç»ƒçš„æ‰€æœ‰å†…å®¹ã€‚

> æˆ‘æ ¹æ®è¿‡å»å‡ å¹´ä¸ AST æ¨¡å‹å’Œ Hugging Face ç”Ÿæ€ç³»ç»Ÿçš„ä¸“ä¸šç»éªŒæ’°å†™äº†è¿™ä»½æŒ‡å—ã€‚
> 
> æœ¬æ•™ç¨‹å°†æŒ‡å¯¼æˆ‘ä»¬ä½¿ç”¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿçš„å·¥å…·å¯¹è‡ªå·±çš„éŸ³é¢‘åˆ†ç±»æ•°æ®é›†ä¸Šçš„ AST è¿›è¡Œå¾®è°ƒã€‚
> 
> æˆ‘ä»¬å°†åŠ è½½æ•°æ®ï¼ˆ1ï¼‰ï¼Œé¢„å¤„ç†éŸ³é¢‘ï¼ˆ2ï¼‰ï¼Œè®¾ç½®éŸ³é¢‘å¢å¼ºï¼ˆ3ï¼‰ï¼Œé…ç½®å’Œåˆå§‹åŒ– AST æ¨¡å‹ï¼ˆ4ï¼‰ï¼Œæœ€åï¼Œé…ç½®å’Œå¼€å§‹è®­ç»ƒï¼ˆ5ï¼‰ã€‚

# å¾®è°ƒ AST çš„é€æ­¥æŒ‡å—

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ä½¿ç”¨ pip å®‰è£…æ‰€æœ‰å¿…éœ€çš„è½¯ä»¶åŒ…ï¼š

```py
pip install transformers[torch] datasets[audio] audiomentations
```

## 1\. ä»¥æ­£ç¡®æ ¼å¼åŠ è½½æˆ‘ä»¬çš„æ•°æ®

é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/index)åº“æ¥ç®¡ç†æˆ‘ä»¬çš„æ•°æ®ã€‚è¯¥åº“å°†ååŠ©æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œé¢„å¤„ç†ã€å­˜å‚¨å’Œè®¿é—®æ•°æ®ï¼Œä»¥åŠåœ¨éœ€è¦æ—¶æ‰§è¡Œæ³¢å½¢è½¬æ¢å¹¶å®æ—¶ç¼–ç ä¸ºé¢‘è°±å›¾ã€‚

æˆ‘ä»¬çš„æ•°æ®åº”åŠ è½½åˆ°å…·æœ‰ä»¥ä¸‹ç»“æ„çš„`Dataset`å¯¹è±¡ä¸­ï¼š

```py
Dataset({
    features: ['audio', 'labels'],
    num_rows: 1234
})
```

> åœ¨æ¥ä¸‹æ¥çš„ä¸¤ä¸ªéƒ¨åˆ†ä¸­ï¼Œæˆ‘å°†æ¼”ç¤ºå¦‚ä½•ä»ğŸ¤— Hub åŠ è½½å‡†å¤‡å¥½çš„æ•°æ®é›†ï¼Œä»¥åŠå¦‚ä½•ä»æœ¬åœ°éŸ³é¢‘æ•°æ®å’Œæ ‡ç­¾åˆ›å»ºä¸€ä¸ª`*Dataset*`ã€‚

**ä» Hugging Face Hub åŠ è½½æ•°æ®é›†ï¼š** å¦‚æœæˆ‘ä»¬æ²¡æœ‰æœ¬åœ°éŸ³é¢‘æ•°æ®é›†ï¼Œå¯ä»¥æ–¹ä¾¿åœ°ä½¿ç”¨`load_dataset`å‡½æ•°ä» Hugging Face Hub åŠ è½½æ•°æ®é›†ã€‚

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†åŠ è½½ ESC50 éŸ³é¢‘åˆ†ç±»æ•°æ®é›†ä»¥è¿›è¡Œæ¼”ç¤ºï¼š

```py
from datasets import load_dataset

esc50 = load_dataset("ashraq/esc50", split="train")
```

![](img/d311793e7f3a774f4c6cbf257a504f74.png)

ESC50 æ•°æ®é›†ä¸­ä¸åŒç±»åˆ«çš„é¢‘è°±å›¾ï¼ˆé¡¶éƒ¨ï¼‰å’Œæ³¢å½¢å›¾ï¼ˆåº•éƒ¨ï¼‰| *ä½œè€…åˆ›å»ºçš„å›¾åƒï¼ˆä½¿ç”¨* [*Spotlight*](https://github.com/Renumics/spotlight)*)*

**åŠ è½½æœ¬åœ°éŸ³é¢‘æ–‡ä»¶å’Œæ ‡ç­¾ï¼š** æˆ‘ä»¬å¯ä»¥ä½¿ç”¨åŒ…å«æ–‡ä»¶è·¯å¾„å’Œæ ‡ç­¾çš„å­—å…¸æˆ– pandas DataFrame å°†éŸ³é¢‘æ–‡ä»¶å’Œç›¸å…³æ ‡ç­¾åŠ è½½åˆ°`Dataset`å¯¹è±¡ä¸­ã€‚å¦‚æœæˆ‘ä»¬æœ‰ç±»åï¼ˆå­—ç¬¦ä¸²ï¼‰åˆ°æ ‡ç­¾ç´¢å¼•ï¼ˆæ•´æ•°ï¼‰çš„æ˜ å°„ï¼Œè¿™äº›ä¿¡æ¯å¯ä»¥åœ¨æ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­åŒ…å«ã€‚

è¿™æ˜¯ä¸€ä¸ªå®é™…ç¤ºä¾‹ï¼š

```py
from datasets import Dataset, Audio, ClassLabel, Features

# Define class labels
class_labels = ClassLabel(names=["bang", "dog_bark"])
# Define features with audio and label columns
features = Features({
    "audio": Audio(),  # Define the audio feature
    "labels": class_labels  # Assign the class labels
})
# Construct the dataset from a dictionary
dataset = Dataset.from_dict({
    "audio": ["/audio/fold1/7061-6-0-0.wav", "/audio/fold1/7383-3-0-0.wav"],
    "labels": [0, 1],  # Corresponding labels for the audio files
}, features=features)
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼š

+   `Audio`ç‰¹å¾ç±»ä¼šè‡ªåŠ¨å¤„ç†éŸ³é¢‘æ–‡ä»¶çš„åŠ è½½å’Œå¤„ç†ã€‚

+   `ClassLabel`æœ‰åŠ©äºç®¡ç†åˆ†ç±»æ ‡ç­¾ï¼Œä½¿åœ¨è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­æ›´å®¹æ˜“å¤„ç†ç±»åˆ«ã€‚

> **æ³¨æ„ï¼š** æœ‰å…³å¦‚ä½•ä½¿ç”¨ Hugging Face åŠ è½½éŸ³é¢‘çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ Datasets åº“çš„[æ–‡æ¡£](https://huggingface.co/docs/datasets/audio_load)ã€‚

**æ£€æŸ¥æ•°æ®é›†ï¼š** ä¸€æ—¦æ•°æ®é›†æˆåŠŸåŠ è½½ï¼Œæ¯ä¸ªéŸ³é¢‘æ ·æœ¬éƒ½å¯ä»¥é€šè¿‡`Audio`ç‰¹å¾ç±»è¿›è¡Œè®¿é—®ï¼Œ`Audio`ç‰¹å¾ç±»é€šè¿‡ä»…åœ¨éœ€è¦æ—¶å°†å…¶åŠ è½½åˆ°å†…å­˜ä¸­æ¥ä¼˜åŒ–æ•°æ®å¤„ç†ã€‚è¿™ç§é«˜æ•ˆçš„ç®¡ç†èŠ‚çœäº†è®¡ç®—èµ„æºï¼Œå¹¶åŠ é€Ÿäº†è®­ç»ƒè¿‡ç¨‹ã€‚

ä¸ºäº†æ›´å¥½åœ°ç†è§£æ•°æ®ç»“æ„å¹¶ç¡®ä¿ä¸€åˆ‡æ­£ç¡®åŠ è½½ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥æ•°æ®é›†ä¸­å•ä¸ªæ ·æœ¬ï¼š

```py
print(dataset[0])
```

è¾“å‡ºç¤ºä¾‹ï¼š

```py
{'audio': {'path': '/audio/fold1/7061-6-0-0.wav',
  'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,
         1.52587891e-05, 3.05175781e-05, 0.00000000e+00]),
  'sampling_rate': 44100},
 'labels': 0}
```

è¯¥è¾“å‡ºæ˜¾ç¤ºäº†éŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„ã€æ³¢å½¢æ•°æ®æ•°ç»„ä»¥åŠé‡‡æ ·ç‡ï¼Œå¹¶é™„ä¸Šç›¸åº”çš„æ ‡ç­¾ã€‚

> å¯¹äºæ¥ä¸‹æ¥çš„æ­¥éª¤ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åƒæˆ‘ä»¬è¿™æ ·å‡†å¤‡å¥½çš„æ•°æ®é›†ä½œä¸ºç¤ºä¾‹ï¼Œä¹Ÿå¯ä»¥ç»§ç»­ä½¿ç”¨æ‚¨è‡ªå·±çš„æ•°æ®é›†ã€‚

## 2. é¢„å¤„ç†éŸ³é¢‘æ•°æ®

å¦‚æœæˆ‘ä»¬çš„æ•°æ®é›†æ¥è‡ª Hugging Face Hubï¼Œæˆ‘ä»¬å°†`*audio*`å’Œ`*labels*`åˆ—è½¬æ¢ä¸ºæ­£ç¡®çš„ç‰¹å¾ç±»å‹ï¼š

```py
import numpy as np
from datasets import Audio, ClassLabel

# get target value - class name mappings
df = esc50.select_columns(["target", "category"]).to_pandas()
class_names = df.iloc[np.unique(df["target"], return_index=True)[1]]["category"].to_list()
# cast target and audio column
esc50 = esc50.cast_column("target", ClassLabel(names=class_names))
esc50 = esc50.cast_column("audio", Audio(sampling_rate=16000))
# rename the target feature
esc50 = esc50.rename_column("target", "labels")
num_labels = len(np.unique(esc50["labels"]))
```

åœ¨è¿™æ®µä»£ç ä¸­ï¼š

+   **éŸ³é¢‘è½¬æ¢ï¼š** `Audio`ç‰¹å¾ç±»å¤„ç†éŸ³é¢‘æ–‡ä»¶çš„åŠ è½½å’Œå¤„ç†ï¼Œå¹¶å°†å…¶é‡æ–°é‡‡æ ·åˆ°æ‰€éœ€çš„é‡‡æ ·ç‡ï¼ˆæ­¤å¤„ä¸º 16kHzï¼Œå³`ASTFeatureExtractor`çš„é‡‡æ ·ç‡ï¼‰ã€‚

+   **ç±»åˆ«æ ‡ç­¾è½¬æ¢ï¼š** `ClassLabel`ç‰¹å¾å°†æ•´æ•°æ˜ å°„åˆ°æ ‡ç­¾ï¼Œåä¹‹äº¦ç„¶ã€‚

![](img/5406a4f6a58ec46f62f5875bafeab4cb.png)![](img/113a08208bca4a7f8e5b2e3d018054c5.png)

ä¸€ä¸ªéŸ³é¢‘æ•°ç»„ä½œä¸ºæ³¢å½¢ï¼ˆå·¦ï¼‰å’Œé¢‘è°±å›¾ï¼ˆå³ï¼‰ | *å›¾ç‰‡ç”±ä½œè€…æä¾›*

**ä¸º AST æ¨¡å‹è¾“å…¥åšå‡†å¤‡ï¼š** AST æ¨¡å‹éœ€è¦é¢‘è°±å›¾è¾“å…¥ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°†æ³¢å½¢ç¼–ç ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ ¼å¼ã€‚è¿™æ˜¯é€šè¿‡ä½¿ç”¨`ASTFeatureExtractor`æ¥å®ç°çš„ï¼Œè¯¥æå–å™¨æ˜¯ä»æˆ‘ä»¬æ‰“ç®—åœ¨æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒçš„é¢„è®­ç»ƒæ¨¡å‹çš„é…ç½®ä¸­å®ä¾‹åŒ–çš„ã€‚

```py
from transformers import ASTFeatureExtractor

# we define which pretrained model we want to use and instantiate a feature extractor
pretrained_model = "MIT/ast-finetuned-audioset-10-10-0.4593"
feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)
# we save model input name and sampling rate for later use
model_input_name = feature_extractor.model_input_names[0]  # key -> 'input_values'
SAMPLING_RATE = feature_extractor.sampling_rate
```

> **æ³¨æ„ï¼š** åœ¨ç‰¹å¾æå–å™¨ä¸­è®¾ç½®**å‡å€¼ï¼ˆmeanï¼‰**å’Œ**æ ‡å‡†å·®ï¼ˆstdï¼‰**å€¼ä»¥è¿›è¡Œ**å½’ä¸€åŒ–ï¼ˆnormalizationï¼‰**æ˜¯éå¸¸é‡è¦çš„ï¼Œè¿™äº›å€¼åº”å½“è®¾ä¸º**æˆ‘ä»¬æ•°æ®é›†çš„å€¼**ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç å—æ¥è®¡ç®—è¿™äº›å€¼ï¼š

```py
# calculate values for normalization
feature_extractor.do_normalize = False  # we set normalization to False in order to calculate the mean + std of the dataset
mean = []
std = []

# we use the transformation w/o augmentation on the training dataset to calculate the mean + std
dataset["train"].set_transform(preprocess_audio, output_all_columns=False)
for i, (audio_input, labels) in enumerate(dataset["train"]):
    cur_mean = torch.mean(dataset["train"][i][audio_input])
    cur_std = torch.std(dataset["train"][i][audio_input])
    mean.append(cur_mean)
    std.append(cur_std)
feature_extractor.mean = np.mean(mean)
feature_extractor.std = np.mean(std)
feature_extractor.do_normalize = True
```

**åº”ç”¨é¢„å¤„ç†è½¬æ¢ï¼š** æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥é¢„å¤„ç†éŸ³é¢‘æ•°æ®ï¼Œå°†éŸ³é¢‘æ•°ç»„ç¼–ç ä¸ºæ¨¡å‹æœŸæœ›çš„`input_values`æ ¼å¼ã€‚è¿™ä¸ªå‡½æ•°è®¾ç½®ä¸ºåŠ¨æ€åº”ç”¨ï¼Œå³åœ¨æ¯ä¸ªæ ·æœ¬ä»æ•°æ®é›†ä¸­åŠ è½½æ—¶ï¼Œå®ƒä¼šå®æ—¶å¤„ç†æ•°æ®ã€‚

```py
def preprocess_audio(batch):
    wavs = [audio["array"] for audio in batch["input_values"]]
    # inputs are spectrograms as torch.tensors now
    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors="pt")

    output_batch = {model_input_name: inputs.get(model_input_name), "labels": list(batch["labels"])}
    return output_batch

# Apply the transformation to the dataset
dataset = dataset.rename_column("audio", "input_values")  # rename audio column
dataset.set_transform(preprocess_audio, output_all_columns=False)
```

**æ£€æŸ¥è½¬æ¢åçš„æ•°æ®ï¼š** å¦‚æœæˆ‘ä»¬ç°åœ¨åŠ è½½ä¸€ä¸ªæ ·æœ¬ï¼Œå®ƒå°†å®æ—¶è½¬æ¢ï¼Œç¼–ç åçš„éŸ³é¢‘å°†ä½œä¸º`*input_values*`è¾“å‡ºï¼š

```py
{'input_values': tensor([[-1.2776, -1.2776, -1.2776,  ..., -1.2776, -1.2776, -1.2776],
         [-1.2776, -1.2776, -1.2776,  ..., -1.2776, -1.2776, -1.2776],
         [-1.2776, -1.2776, -1.2776,  ..., -1.2776, -1.2776, -1.2776],
         ...,
         [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670],
         [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670],
         [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670]]),
 'label': 0}
```

> **æ³¨æ„ï¼š** éªŒè¯è½¬æ¢è¿‡ç¨‹æ˜¯å¦ä¿æŒæ•°æ®å®Œæ•´æ€§ï¼Œå¹¶ç¡®ä¿é¢‘è°±å›¾æ­£ç¡®ç”Ÿæˆï¼Œä»¥é¿å…æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°ä»»ä½•é—®é¢˜ï¼Œè¿™æ˜¯è‡³å…³é‡è¦çš„ã€‚

**æ‹†åˆ†æ•°æ®é›†ï¼š** ä½œä¸ºæœ€åä¸€æ­¥æ•°æ®é¢„å¤„ç†ï¼Œæˆ‘ä»¬å°†æ•°æ®é›†æ‹†åˆ†ä¸º`train`å’Œ`test`é›†ï¼ŒåŒæ—¶åˆ©ç”¨æ ‡ç­¾è¿›è¡Œåˆ†å±‚æŠ½æ ·ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿ä¸¤ä¸ªæ•°æ®é›†ä¸­çš„ç±»åˆ«åˆ†å¸ƒä¿æŒä¸€è‡´ã€‚

```py
# split training data
if "test" not in dataset:
    dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=0, stratify_by_column="labels")
```

## 3. æ·»åŠ éŸ³é¢‘å¢å¼º

å¢å¼ºåœ¨é€šè¿‡å¼•å…¥è®­ç»ƒæ•°æ®çš„å˜åŒ–æ€§æ¥æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„é²æ£’æ€§æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è¿™æ¨¡æ‹Ÿäº†ä¸åŒçš„å½•éŸ³æ¡ä»¶ï¼Œå¹¶å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å¯¹æœªè§è¿‡çš„æ•°æ®è¿›è¡Œæ³›åŒ–ã€‚

åœ¨å¼€å§‹è®¾ç½®ä¹‹å‰ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªè§†è§‰å¯¹æ¯”ï¼Œå±•ç¤ºäº†éŸ³é¢‘æ–‡ä»¶çš„**åŸå§‹**é¢‘è°±å›¾å’Œé€šè¿‡ **AddBackgroundNoise** è½¬æ¢å¾—åˆ°çš„å¢å¼ºç‰ˆé¢‘è°±å›¾ã€‚

![](img/d3f6385a1580e5879d6a036094c54a5f.png)![](img/b3fa731cbc376356c42569f444f90b76.png)

éŸ³é¢‘æ–‡ä»¶çš„åŸå§‹é¢‘è°±å›¾ï¼ˆå·¦ï¼‰å’Œé€šè¿‡ [Audiomentations](https://github.com/iver56/audiomentations) åº“çš„ AddBackgroundNoise è½¬æ¢å¢å¼ºåçš„éŸ³é¢‘ï¼ˆå³ï¼‰| *å›¾ç‰‡æ¥æºï¼šä½œè€…*

> **æ³¨æ„ï¼š** å¢å¼ºæ˜¯æé«˜è®­ç»ƒé²æ£’æ€§å’Œå‡å°‘æœºå™¨å­¦ä¹ æ¨¡å‹è¿‡æ‹Ÿåˆçš„æœ‰æ•ˆå·¥å…·ã€‚
> 
> ç„¶è€Œï¼Œå¿…é¡»**ä»”ç»†è€ƒè™‘æ¯ä¸ªè½¬æ¢çš„æ½œåœ¨å½±å“**ã€‚ä¾‹å¦‚ï¼Œæ·»åŠ å™ªéŸ³å¯¹äºè¯­éŸ³æ•°æ®é›†å¯èƒ½æ˜¯åˆé€‚çš„ï¼Œå› ä¸ºå®ƒå¯ä»¥æ¨¡æ‹Ÿç°å®ä¸–ç•Œä¸­çš„èƒŒæ™¯å™ªéŸ³æƒ…å†µã€‚ç„¶è€Œï¼Œå¯¹äºå£°éŸ³åˆ†ç±»ç­‰ä»»åŠ¡ï¼Œè¿™äº›å¢å¼ºå¯èƒ½ä¼šå¯¼è‡´ç±»åˆ«æ··æ·†ï¼Œä»è€Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚

**è®¾ç½®éŸ³é¢‘å¢å¼ºï¼š** ä¸ºäº†åˆ›å»ºä¸€ç»„éŸ³é¢‘å¢å¼ºï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ª [Audiomentations](https://iver56.github.io/audiomentations/) åº“çš„ `Compose` ç±»ï¼Œå®ƒå…è®¸æˆ‘ä»¬å°†å¤šä¸ªå¢å¼ºç»„åˆåœ¨ä¸€èµ·ã€‚

ä¸‹é¢æ˜¯å¦‚ä½•è®¾ç½®å®ƒï¼š

```py
from audiomentations import Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift

audio_augmentations = Compose([
    AddGaussianSNR(min_snr_db=10, max_snr_db=20),
    Gain(min_gain_db=-6, max_gain_db=6),
    GainTransition(min_gain_db=-6, max_gain_db=6, min_duration=0.01, max_duration=0.3, duration_unit="fraction"),
    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.5),
    TimeStretch(min_rate=0.8, max_rate=1.2),
    PitchShift(min_semitones=-4, max_semitones=4),
], p=0.8, shuffle=True)
```

åœ¨è¿™ä¸ªè®¾ç½®ä¸­ï¼š

+   `p=0.8` å‚æ•°æŒ‡å®š `Compose` åºåˆ—ä¸­çš„æ¯ä¸ªå¢å¼ºåœ¨ç»™å®šéŸ³é¢‘æ ·æœ¬ä¸Šæœ‰ 80% çš„æ¦‚ç‡è¢«åº”ç”¨ã€‚è¿™ä¸ªæ¦‚ç‡æ–¹æ³•ç¡®ä¿äº†è®­ç»ƒæ•°æ®çš„å˜åŒ–æ€§ï¼Œé˜²æ­¢æ¨¡å‹è¿‡åº¦ä¾èµ–äºä»»ä½•ç‰¹å®šçš„å¢å¼ºæ¨¡å¼ï¼Œå¹¶æé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚

+   `shuffle=True` å‚æ•°ä¼šéšæœºåŒ–åº”ç”¨å¢å¼ºçš„é¡ºåºï¼Œå¢åŠ äº†å¦ä¸€å±‚å˜åŒ–æ€§ã€‚

> è‹¥è¦æ›´å¥½åœ°ç†è§£è¿™äº›å¢å¼ºåŠå…¶è¯¦ç»†é…ç½®é€‰é¡¹ï¼Œå¯ä»¥æŸ¥çœ‹ [**Audiomentations çš„æ–‡æ¡£**](https://iver56.github.io/audiomentations/)ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªå¾ˆæ£’çš„ [ğŸ¤— **ç©ºé—´**](https://phrasenmaeher-audio-transformat-visualize-transformation-5s1n4t.streamlit.app/)ï¼Œå¯ä»¥åœ¨å…¶ä¸­å®éªŒè¿™äº›éŸ³é¢‘è½¬æ¢ï¼Œå¬åˆ°å¹¶çœ‹åˆ°å®ƒä»¬å¯¹é¢‘è°±å›¾çš„å½±å“ã€‚

**å°†å¢å¼ºé›†æˆåˆ°è®­ç»ƒç®¡é“ä¸­ï¼š** æˆ‘ä»¬åœ¨ `preprocess_audio` è½¬æ¢ä¸­åº”ç”¨è¿™äº›å¢å¼ºï¼ŒåŒæ—¶å°†éŸ³é¢‘æ•°æ®ç¼–ç ä¸ºé¢‘è°±å›¾ã€‚

æ–°çš„é¢„å¤„ç†ä¸å¢å¼ºå¦‚ä¸‹ï¼š

```py
def preprocess_audio_with_transforms(batch):
    # we apply augmentations on each waveform
    wavs = [audio_augmentations(audio["array"], sample_rate=SAMPLING_RATE) for audio in batch["input_values"]]
    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors="pt")

    output_batch = {model_input_name: inputs.get(model_input_name), "labels": list(batch["labels"])}
    return output_batch

# Cast the audio column to the appropriate feature type and rename it
dataset = dataset.cast_column("input_values", Audio(sampling_rate=feature_extractor.sampling_rate))
```

æ­¤å‡½æ•°å°†å®šä¹‰çš„å¢å¼ºåº”ç”¨åˆ°æ¯ä¸ªæ³¢å½¢ï¼Œå¹¶ä½¿ç”¨ `ASTFeatureExtractor` å°†å¢å¼ºåçš„æ³¢å½¢ç¼–ç ä¸ºæ¨¡å‹è¾“å…¥ã€‚

**è®¾ç½®è®­ç»ƒå’ŒéªŒè¯æ‹†åˆ†çš„è½¬æ¢ï¼š** æœ€åï¼Œæˆ‘ä»¬è®¾ç½®è¿™äº›è½¬æ¢å°†åœ¨è®­ç»ƒå’Œè¯„ä¼°é˜¶æ®µåº”ç”¨ï¼š

```py
# with augmentations on the training set
dataset["train"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)
# w/o augmentations on the test set
dataset["test"].set_transform(preprocess_audio, output_all_columns=False)
```

## 4\. é…ç½®å¹¶åˆå§‹åŒ– AST è¿›è¡Œå¾®è°ƒ

ä¸ºäº†å°† AST æ¨¡å‹é€‚åº”æˆ‘ä»¬çš„ç‰¹å®šéŸ³é¢‘åˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´æ¨¡å‹çš„é…ç½®ã€‚å› ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†ä¸é¢„è®­ç»ƒæ¨¡å‹çš„ç±»åˆ«æ•°ä¸åŒï¼Œè€Œä¸”è¿™äº›ç±»åˆ«å¯¹åº”ä¸åŒçš„åˆ†ç±»ã€‚æˆ‘ä»¬éœ€è¦ç”¨ä¸€ä¸ªæ–°çš„åˆ†ç±»å¤´æ›¿æ¢é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„åˆ†ç±»å¤´ï¼Œä»¥è§£å†³æˆ‘ä»¬çš„å¤šç±»é—®é¢˜ã€‚

æ–°çš„åˆ†ç±»å¤´çš„æƒé‡å°†è¢«éšæœºåˆå§‹åŒ–ï¼Œè€Œæ¨¡å‹å…¶ä½™éƒ¨åˆ†çš„æƒé‡å°†ä»é¢„è®­ç»ƒç‰ˆæœ¬åŠ è½½ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥ä»é¢„è®­ç»ƒçš„å­¦ä¹ ç‰¹å¾ä¸­å—ç›Šï¼Œå¹¶åœ¨æˆ‘ä»¬çš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚

è¿™æ˜¯å¦‚ä½•è®¾ç½®å’Œåˆå§‹åŒ–å¸¦æœ‰æ–°åˆ†ç±»å¤´çš„ AST æ¨¡å‹ï¼š

```py
from transformers import ASTConfig, ASTForAudioClassification

# Load configuration from the pretrained model
config = ASTConfig.from_pretrained(pretrained_model)
# Update configuration with the number of labels in our dataset
config.num_labels = num_labels
config.label2id = label2id
config.id2label = {v: k for k, v in label2id.items()}
# Initialize the model with the updated configuration
model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)
model.init_weights()
```

**é¢„æœŸè¾“å‡ºï¼š** æˆ‘ä»¬å°†çœ‹åˆ°ä¸€äº›è­¦å‘Šï¼Œè¡¨æ˜æŸäº›æƒé‡ï¼Œç‰¹åˆ«æ˜¯åˆ†ç±»å±‚ä¸­çš„æƒé‡ï¼Œæ­£åœ¨è¢«é‡æ–°åˆå§‹åŒ–ï¼š

```py
Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:
- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated
- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

## 5\. è®¾ç½®è¯„ä¼°æŒ‡æ ‡å¹¶å¼€å§‹è®­ç»ƒ

åœ¨æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ ğŸ¤— [Transformers](https://github.com/huggingface/transformers) åº“æ¥é…ç½®è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨ ğŸ¤— [Evaluate](https://github.com/huggingface/evaluate) åº“æ¥å®šä¹‰è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

**1\. é…ç½®è®­ç»ƒå‚æ•°ï¼š** `TrainingArguments` ç±»æœ‰åŠ©äºè®¾ç½®è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§å‚æ•°ï¼Œå¦‚å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°å’Œè®­ç»ƒè½®æ•°ã€‚

```py
from transformers import TrainingArguments

# Configure training run with TrainingArguments class
training_args = TrainingArguments(
    output_dir="./runs/ast_classifier",
    logging_dir="./logs/ast_classifier",
    report_to="tensorboard",
    learning_rate=5e-5,  # Learning rate
    push_to_hub=False,
    num_train_epochs=10,  # Number of epochs
    per_device_train_batch_size=8,  # Batch size per device
    eval_strategy="epoch",  # Evaluation strategy
    save_strategy="epoch",
    eval_steps=1,
    save_steps=1,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    logging_strategy="steps",
    logging_steps=20,
)
```

**2\. å®šä¹‰è¯„ä¼°æŒ‡æ ‡ï¼š** å®šä¹‰å¦‚å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ç­‰æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚`compute_metrics` å‡½æ•°å°†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¤„ç†è¿™äº›è®¡ç®—ã€‚

```py
import evaluate
import numpy as np

accuracy = evaluate.load("accuracy")
recall = evaluate.load("recall")
precision = evaluate.load("precision")
f1 = evaluate.load("f1")
AVERAGE = "macro" if config.num_labels > 2 else "binary"

def compute_metrics(eval_pred):
    logits = eval_pred.predictions
    predictions = np.argmax(logits, axis=1)
    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)
    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))
    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))
    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))
    return metrics
```

**3\. è®¾ç½® Trainerï¼š** ä½¿ç”¨ Hugging Face çš„ `Trainer` ç±»æ¥å¤„ç†è®­ç»ƒè¿‡ç¨‹ã€‚è¯¥ç±»é›†æˆäº†æ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ã€‚

```py
from transformers import Trainer

# Setup the trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    compute_metrics=compute_metrics,  # Use the metrics function from above
)
```

é…ç½®å®Œæˆåï¼Œæˆ‘ä»¬å¯åŠ¨è®­ç»ƒè¿‡ç¨‹ï¼š

```py
trainer.train()
```

![](img/e14d2383267ae80949cd038f1e0f21b5.png)

åº”ç”¨éŸ³é¢‘å¢å¼ºçš„è®­ç»ƒæ—¥å¿—ç¤ºä¾‹ | *å›¾åƒç”±ä½œè€…æä¾›*

# ï¼ˆéé‚£ä¹ˆå¯é€‰çš„ï¼‰: è¯„ä¼°ç»“æœ

ä¸ºäº†ç†è§£æ¨¡å‹çš„è¡¨ç°å¹¶æ‰¾å‡ºæ½œåœ¨çš„æ”¹è¿›ç©ºé—´ï¼Œè¯„ä¼°å…¶åœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¸Šçš„é¢„æµ‹è‡³å…³é‡è¦ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ç­‰æŒ‡æ ‡ä¼šè®°å½•åˆ° [TensorBoard](https://www.tensorflow.org/tensorboard)ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ£€æŸ¥æ¨¡å‹éšæ—¶é—´çš„è¿›å±•å’Œæ€§èƒ½ã€‚

**å¯åŠ¨ TensorBoard**ï¼šä¸ºäº†å¯è§†åŒ–è¿™äº›æŒ‡æ ‡ï¼Œåœ¨ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨ TensorBoardï¼š

```py
tensorboard --logdir="./logs"
```

è¿™æä¾›äº†ä¸€ä¸ªå›¾å½¢åŒ–çš„è¡¨ç¤ºï¼Œå±•ç¤ºäº†æ¨¡å‹çš„å­¦ä¹ æ›²çº¿å’ŒæŒ‡æ ‡éšæ—¶é—´çš„æ”¹è¿›ï¼Œå¸®åŠ©æˆ‘ä»¬åŠæ—©å‘ç°æ½œåœ¨çš„è¿‡æ‹Ÿåˆæˆ–æ€§èƒ½ä¸è¶³ã€‚

å¯¹äº**æ›´è¯¦ç»†çš„è§è§£**ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [Renumics](https://renumics.com/) çš„å¼€æºå·¥å…· [**Spotlight**](https://renumics.com/open-source/spotlight) æ£€æŸ¥æ¨¡å‹çš„é¢„æµ‹ã€‚Spotlight å¯ä»¥è®©æˆ‘ä»¬æ¢ç´¢å’Œå¯è§†åŒ–é¢„æµ‹ä»¥åŠæ•°æ®ï¼Œå¸®åŠ©æˆ‘ä»¬è¯†åˆ«å•ä¸ªæ•°æ®ç‚¹çš„æ¨¡å¼ã€æ½œåœ¨åè§å’Œé”™è¯¯åˆ†ç±»ã€‚

![](img/07e84100feee6c38d3dbde8ed89c2a7f.png)

åœ¨ Spotlight ä¸­åŠ è½½äº†å¸¦æœ‰éŸ³é¢‘åµŒå…¥å’Œæ¨¡å‹é¢„æµ‹çš„ ESC50 æ•°æ®é›†ã€‚åœ¨è¿™ä¸ª Hugging Face [Space](https://huggingface.co/spaces/renumics/spotlight-esc50-clap) ä¸­å°è¯•ä¸€ä¸‹å§ã€‚| *ä½œè€…æä¾›çš„å›¾ç‰‡*

**å®‰è£…å’Œä½¿ç”¨ Spotlight**ï¼š

è¦å¼€å§‹ä½¿ç”¨ Spotlightï¼Œè¯·ä½¿ç”¨ pip å®‰è£…å®ƒå¹¶åŠ è½½æ‚¨çš„æ•°æ®é›†è¿›è¡Œæ¢ç´¢ï¼š

```py
pip install renumics-spotlight
```

å¹¶ä½¿ç”¨ä¸€è¡Œä»£ç åŠ è½½ ESC50 æ•°æ®é›†è¿›è¡Œ**äº¤äº’å¼æ¢ç´¢**ï¼š

```py
from renumics import spotlight

spotlight.show(esc50, dtype={"audio": spotlight.Audio})
```

> æœ¬æ•™ç¨‹ä¾§é‡äºå»ºç«‹å¾®è°ƒæµç¨‹ã€‚æœ‰å…³å…¨é¢çš„**è¯„ä¼°**ï¼ŒåŒ…æ‹¬**ä½¿ç”¨ Spotlight**ï¼Œè¯·å‚è€ƒä¸‹é¢æä¾›çš„å…¶ä»–æ•™ç¨‹å’Œèµ„æºä»¥åŠæœ¬æŒ‡å—æœ«å°¾çš„é“¾æ¥ï¼ˆæœ‰ç”¨é“¾æ¥ï¼‰ã€‚

è¿™é‡Œæœ‰ä¸€äº›å¦‚ä½•ä½¿ç”¨ Spotlight è¿›è¡Œæ¨¡å‹è¯„ä¼°çš„ç¤ºä¾‹ï¼š

1.  ä¸€ç¯‡å…³äº**ä½¿ç”¨ Transformers è¿›è¡Œå®æ—¶è¯­éŸ³åˆ†æ**çš„åšæ–‡å’Œæ¼”ç¤ºï¼š[åšæ–‡](https://renumics.com/blog/voice-analytics-with-transformers) & ğŸ¤— [Space](https://huggingface.co/spaces/renumics/emodb-model-comparison)

1.  ä¸€ç¯‡å…³äº**Fine-tuning image classification models from image search**çš„åšæ–‡å’Œç®€çŸ­ç¤ºä¾‹ï¼š[åšæ–‡](https://itnext.io/image-classification-in-2023-8ab7dc552115) & [ä½¿ç”¨æ¡ˆä¾‹](https://renumics.com/next/docs/use-cases/image-fine-tuning)

1.  ä¸€ç¯‡å…³äº**å¦‚ä½•è‡ªåŠ¨æŸ¥æ‰¾å’Œåˆ é™¤å›¾åƒã€éŸ³é¢‘å’Œæ–‡æœ¬åˆ†ç±»æ•°æ®é›†ä¸­é—®é¢˜çš„æ–‡ç« **å’Œç®€çŸ­ç¤ºä¾‹ï¼š[åšæ–‡](https://medium.com/@daniel-klitzke/finding-problematic-data-slices-in-unstructured-data-aeec0a3b9a2a) & [ä½¿ç”¨æ¡ˆä¾‹](https://renumics.com/next/docs/use-cases/audio-classification)

# ç»“è®º

é€šè¿‡æŒ‰ç…§æœ¬æŒ‡å—ä¸­æ¦‚è¿°çš„æ­¥éª¤ï¼Œæˆ‘ä»¬å°†èƒ½å¤Ÿåœ¨ä»»ä½•éŸ³é¢‘åˆ†ç±»æ•°æ®é›†ä¸Šå¾®è°ƒéŸ³é¢‘é¢‘è°±å˜æ¢å™¨ï¼ˆASTï¼‰ã€‚è¿™åŒ…æ‹¬è®¾ç½®æ•°æ®é¢„å¤„ç†ã€åº”ç”¨æœ‰æ•ˆçš„éŸ³é¢‘å¢å¼ºä»¥åŠä¸ºç‰¹å®šä»»åŠ¡é…ç½®æ¨¡å‹ã€‚è®­ç»ƒåï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®šä¹‰çš„æŒ‡æ ‡è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œç¡®ä¿å®ƒç¬¦åˆæˆ‘ä»¬çš„è¦æ±‚ã€‚ä¸€æ—¦æ¨¡å‹ç»è¿‡å¾®è°ƒå’ŒéªŒè¯ï¼Œå°±å¯ä»¥ç”¨äºæ¨æ–­ã€‚

## å…³äºè¿™ä¸ªä¸»é¢˜çš„æ›´å¤šå†…å®¹

è¿™æ˜¯å…³äºç”¨äºå·¥ä¸šéŸ³é¢‘åˆ†ç±»ç”¨ä¾‹çš„éŸ³é¢‘é¢‘è°±å˜æ¢å™¨çš„**ç³»åˆ—æ•™ç¨‹å’Œåšæ–‡**ä¸­çš„ç¬¬äºŒç¯‡ã€‚

+   çœ‹ä¸€çœ‹**ç¬¬ä¸€éƒ¨åˆ†**ï¼š[*å¦‚ä½•åœ¨ HuggingFace ç”Ÿæ€ç³»ç»Ÿä¸­ä½¿ç”¨ SSAST æ¨¡å‹æƒé‡ï¼Ÿ*](https://medium.com/itnext/how-to-use-ssast-model-weigths-in-the-huggingface-ecosystem-0f3fdc8d38da)ï¼Œ

+   å¹¶æŸ¥çœ‹[è¿™ä¸ªåˆ—è¡¨](https://medium.com/@marius_s/list/audio-classification-for-industry-use-cases-cb6d169a7d80)ä»¥è·å–å³å°†å‘å¸ƒçš„æ–‡ç« ã€‚

è¯·ç»§ç»­å…³æ³¨æœ¬ç³»åˆ—çš„åç»­æ–‡ç« ï¼Œæˆ‘ä»¬å°†æ¢è®¨å®é™…ä½¿ç”¨æ¡ˆä¾‹ä¸­çš„ç‰¹å®šæŒ‘æˆ˜ä»¥åŠå¦‚ä½•è°ƒæ•´ AST ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚

## æœ‰ç”¨çš„é“¾æ¥

1.  **ä¸‹è½½** **æœ¬æŒ‡å—**ä½œä¸ºç¬”è®°æœ¬ï¼Œä» Renumics çš„[**èµ„æºé¡µé¢**](https://renumics.com/open-source/resources)**ã€‚**

1.  å…³äºå¦‚ä½•ä½¿ç”¨**Spotlight è¿›è¡ŒéŸ³é¢‘æ¨¡å‹è¯„ä¼°**çš„æ•™ç¨‹ï¼š[**åšå®¢**](https://renumics.com/blog/voice-analytics-with-transformers) & **ğŸ¤—** [**ç©ºé—´**](https://huggingface.co/spaces/renumics/emodb-model-comparison)ï¼ˆæ¼”ç¤ºï¼‰

1.  å…³äºå¦‚ä½•ä½¿ç”¨ Spotlight **è®­ç»ƒå£°å­¦äº‹ä»¶æ£€æµ‹** **ç³»ç»Ÿ**çš„æ•™ç¨‹ï¼š[**åšå®¢**](https://renumics.com/blog/acoustic-event-detection-annotation)

1.  **å®˜æ–¹ ğŸ¤— éŸ³é¢‘è¯¾ç¨‹**: [**ä»‹ç»**](https://huggingface.co/learn/audio-course/chapter0/introduction) & [**å¾®è°ƒ**](https://huggingface.co/learn/audio-course/chapter4/fine-tuning)

æ„Ÿè°¢é˜…è¯»ï¼æˆ‘å« [Marius Steger](https://www.linkedin.com/in/marius-steger/)ï¼Œæ˜¯[Renumics](https://renumics.com/)çš„æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆâ€”â€”æˆ‘ä»¬å¼€å‘äº†[Spotlight](https://github.com/Renumics/spotlight)ï¼Œä¸€æ¬¾å¼€æºå·¥å…·ï¼Œèƒ½å¤Ÿå°†æ‚¨çš„æ•°æ®é©±åŠ¨ AI å·¥ä½œæµæå‡åˆ°ä¸€ä¸ªæ–°çš„æ°´å¹³ã€‚

![](img/cc655344170c51a9f0d18fa1a30b1164.png)

## å‚è€ƒæ–‡çŒ®

[1] Yuan Gong, Yu-An Chung, James Glass: [ASTï¼šéŸ³é¢‘è°±å›¾è½¬æ¢å™¨](https://arxiv.org/abs/2104.01778) (2021), arxiv

[2] Piczak, Karol J.: [ESCï¼šç¯å¢ƒå£°éŸ³åˆ†ç±»æ•°æ®é›†](https://dl.acm.org/doi/10.1145/2733373.2806390) (2015), ACM å‡ºç‰ˆç¤¾
