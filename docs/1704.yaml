- en: Multi-Headed Self Attention — By Hand
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multi-headed-self-attention-by-hand-d2ce1ae031db?source=collection_archive---------4-----------------------#2024-07-12](https://towardsdatascience.com/multi-headed-self-attention-by-hand-d2ce1ae031db?source=collection_archive---------4-----------------------#2024-07-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hand computing the cornerstone of modern AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@danielwarfield1?source=post_page---byline--d2ce1ae031db--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page---byline--d2ce1ae031db--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d2ce1ae031db--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d2ce1ae031db--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page---byline--d2ce1ae031db--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d2ce1ae031db--------------------------------)
    ·6 min read·Jul 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/659a613f94c1461109fe6e7b1e884c8e.png)'
  prefs: []
  type: TYPE_IMG
- en: “Focus” By Daniel Warfield using MidJourney. All images by the author unless
    otherwise specified. Article originally made available on [Intuitively and Exhaustively
    Explained](https://iaee.substack.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Headed Attention is likely the most important architectural paradigm in
    machine learning. This summary goes over all critical mathematical operations
    within multi-headed self attention, allowing you to understand its inner workings
    at a fundamental level. If you’d like to learn more about the intuition behind
    this topic, check out the IAEE article.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----d2ce1ae031db--------------------------------)
    [## Transformers — Intuitively and Exhaustively Explained'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploring the modern wave of machine learning: taking apart the transformer
    step by step'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----d2ce1ae031db--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Defining the Input'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-headed self attention (MHSA) is used in a variety of contexts, each of
    which might format the input differently. In a natural language processing context
    one would likely use a word to vector embedding, paired with positional encoding,
    to calculate a vector that represents each word. Generally, regardless of the
    type of data, multi-headed self attention expects of sequence of vectors, where
    each vector represents something.
  prefs: []
  type: TYPE_NORMAL
