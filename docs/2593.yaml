- en: How AlphaFold 3 Is Like DALLE 2 and Other Learnings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-alphafold-3-is-like-dalle-2-and-other-learnings-1f809010afc7?source=collection_archive---------6-----------------------#2024-10-24](https://towardsdatascience.com/how-alphafold-3-is-like-dalle-2-and-other-learnings-1f809010afc7?source=collection_archive---------6-----------------------#2024-10-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/481d90a32bacb1ec7467f45b59cb4b3a.png)'
  prefs: []
  type: TYPE_IMG
- en: Diffusion (literally) from [Unsplash](https://unsplash.com/photos/three-drinking-glasses-Y1ge0B9_oGE)
  prefs: []
  type: TYPE_NORMAL
- en: Understanding AI applications in bio for machine learning engineers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@meghanheintz?source=post_page---byline--1f809010afc7--------------------------------)[![Meghan
    Heintz](../Images/9eaae6d3d8168086d83ff7100329c51f.png)](https://medium.com/@meghanheintz?source=post_page---byline--1f809010afc7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1f809010afc7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1f809010afc7--------------------------------)
    [Meghan Heintz](https://medium.com/@meghanheintz?source=post_page---byline--1f809010afc7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1f809010afc7--------------------------------)
    ·6 min read·Oct 24, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In our last article, we explored how [AlphaFold 2 and BERT](https://medium.com/towards-data-science/alphafold-2-through-the-context-of-bert-78c9494e99af)
    were connected through transformer architecture. In this piece, we’ll learn how
    the most recent update, [AlphaFold 3](https://www.nature.com/articles/s41586-024-07487-w)
    (hereafter AlphaFold) is more similar to DALLE 2 (hereafter DALLE) and then dive
    into other changes to its architecture and training.
  prefs: []
  type: TYPE_NORMAL
- en: What’s the connection?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AlphaFold and DALLE are another example of how vastly different use cases can
    benefit from architectural learning across domains. DALLE is a text-to-image model
    that generates images from text prompts. AlphaFold 3 is a model for predicting
    biomolecular interactions. The applications of these two models sound like they
    couldn’t be any more different but both rely on diffusion model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Because reasoning about images and text is more intuitive than biomolecular
    interactions, we’ll first explore DALLE’s application. Then we’ll learn about
    how the same concepts are applied by AlphaFold.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A metaphor for understanding the diffusion model: consider tracing the origin
    of a drop of dye in a glass of water. As the dye disperses, it moves randomly
    through the liquid until it is evenly spread. To backtrack to the initial drop’s
    location, you must reconstruct its path step by step since each movement depends
    on the one before. If you repeat this experiment over and over, you’ll be able
    to build a model to predict the dye movement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More concretely, diffusion models are trained to predict and remove noise from
    a dataset. Then upon inference, the model generates a new sample using random
    noise. The architecture comprises three core components: the forward process,
    the reverse process, and the sampling procedure. The forward process takes the
    training data and adds noise at each time step. As you might expect, the reverse
    process removes noise at each step. The sampling procedure (or inference) executes
    the reverse process using the trained model and a noise schedule, transforming
    an initial random noise input back into a structured data sample.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d85e8c619f3ce77d041fed1db3db2f74.png)'
  prefs: []
  type: TYPE_IMG
- en: Simplified illustration of the forward and reverse processes where a pixelated
    heart has noise added and removed back to its original shape. (Created by author)
  prefs: []
  type: TYPE_NORMAL
- en: DALLE and Diffusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DALLE incorporates diffusion model architecture in two major components, the
    prior and the decoder, and removes its predecessor’s autoregressive module. The
    prior model takes text embeddings generated by [CLIP](https://arxiv.org/abs/2103.00020)
    (a model trained on a dataset of images and captions known as Contrastive Language-Image
    Pre-training) and creates an image embedding. During training, the prior is given
    a text embedding and a noised version of the image embedding. The prior learns
    to denoise the image embedding step by step. This process allows the model to
    learn a distribution over image embeddings representing the variability in possible
    images for a given text prompt.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder generates an image from the resulting image embedding starting with
    a random noise image. The reverse diffusion process iteratively removes noise
    from the image using the image embedding (from the prior) at each timestep according
    to the noise schedule. Time step embeddings tell the model about the current stage
    of the denoising process, helping it adjust the noise level removed based on how
    close it is to the final step.
  prefs: []
  type: TYPE_NORMAL
- en: While DALLE 2 utilizes a diffusion model for generating images, its predecessor,
    DALLE 1, relied on an autoregressive approach, sequentially predicting image tokens
    based on the text prompt. This approach was much less computationally efficient,
    required a more complex training and inference process, struggled to produce high-resolution
    images, and often resulted in artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: Its predecessor also did not make use of CLIP. Instead, DALLE 1 learned the
    text-image representations directly. The introduction of CLIP embeddings unified
    these representations making more robust text-to-image representations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b373b945018f0a708e706e5f399225a3.png)'
  prefs: []
  type: TYPE_IMG
- en: High-level overview of DALLE 2 architecture from Hierarchical Text-Conditional
    Image Generation with CLIP Latents by the OpenAI team from [Hierarchical Text-Conditional
    Image Generation with CLIP Latents](https://arxiv.org/pdf/2204.06125)
  prefs: []
  type: TYPE_NORMAL
- en: How AlphaFold uses Diffusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While DALLE’s use of diffusion helps generate detailed visual content, AlphaFold
    leverages similar principles in biomolecular structure prediction (no longer just
    protein folding!).
  prefs: []
  type: TYPE_NORMAL
- en: AlphaFold 2 was not a generative model, as it predicted structures directly
    from given input sequences. Due to the introduction of the diffusion module, AlphaFold
    3 IS a generative model. Just like with DALLE, noise is sampled and then recurrently
    denoised to produce a final structure.
  prefs: []
  type: TYPE_NORMAL
- en: The diffusion module is incorporated by replacing the structure module. This
    architecture change greatly simplifies the model because the structure module
    predicts amino-acid-specific frames and side-chain torsion angles whereas the
    diffusion module predicts the raw atom coordinates. This eliminates several intermediate
    steps in the inference process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b946e3b418bc804980575b2c419aad76.png)'
  prefs: []
  type: TYPE_IMG
- en: AF3 architecture for inference showing where the diffusion module residues.
    From [Accurate structure prediction of biomolecular interactions with AlphaFold
    3](https://www.nature.com/articles/s41586-024-07487-w)
  prefs: []
  type: TYPE_NORMAL
- en: The impetus behind removing these intermediate steps was that the scope of training
    data for this iteration of the model grew substantially. AlphaFold 2 was only
    trained on protein structures, whereas AlphaFold 3 is a “multi-modal” model capable
    of predicting the joint structure of complexes including proteins, nucleic acids,
    small molecules, ions and modified residues. If the model still used the structure
    module, it would have required an excessive number of complex rules about chemical
    bonds and stereochemistry to create valid structures.
  prefs: []
  type: TYPE_NORMAL
- en: The reason why diffusion did not require these rules is because it can be applied
    at coarse and fine-grained levels. For high noise levels, the model is focused
    on capturing the global structure, while at low noise levels, it fine-tunes the
    details. When the noise is minimal, the model refines the local details of the
    structure, such as the precise positions of atoms and their orientations, which
    are crucial for accurate molecular modeling. This means the model can easily work
    with different types of chemical components, not just standard amino acids or
    protein structures.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of working with different types of chemical components appears to
    be that the model can learn more about protein structures from other types of
    structures such as protein-ligand interfaces. It appears that integrating diverse
    data types helps models generalize better across different tasks. This improvement
    is similar to how [Gemini’s](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)
    text comprehension abilities became better the model became multi-modal with the
    incorporation of image and video data.
  prefs: []
  type: TYPE_NORMAL
- en: Other Important Changes to AlphaFold
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**The role of MSA (Multiple Sequence Alignment) was significantly downgraded.**
    The AF2 evoformer is replaced with the simpler pairformer module (a reduction
    of 48 blocks to 4 blocks). As you may recall from my previous article, the MSA
    was thought to help the model learn what parts of the amino acid sequence were
    important evolutionarily. Experimental changes showed that reducing the importance
    of the MSA had a limited impact on model accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hallucination had to be countered.** Generative models are very exciting
    but they come with the baggage of hallucination. Researchers found the model would
    invent plausible-looking structures in unstructured regions. To overcome this,
    a cross-distillation method was used to augment the training data with predicted
    structures [AlphaFold-Multimer (v.2.3)](https://deepmind.google/discover/blog/a-glimpse-of-the-next-generation-of-alphafold/).
    The cross-distillation approach teaches the model to differentiate between structured
    and unstructured regions better. This helps the model to understand when to avoid
    adding artificial details.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Some interactions were easier to predict than others.** Sampling probabilities
    were adjusted for each class of interaction i.e. fewer samples from simple types
    that can be learned in relatively few training steps and visa versa for complex
    ones. This helps avoid under and overfitting across types.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/337acecc2d6ca2b5ef8c357a8543239f.png)'
  prefs: []
  type: TYPE_IMG
- en: Training curves for initial training and fine-tuning stages illustrate how different
    classes reached their best performance at varying training steps. For this reason,
    the training data was subsampled to prevent under and overfitting by class. From
    [Accurate structure prediction of biomolecular interactions with AlphaFold 3](https://www.nature.com/articles/s41586-024-07487-w)
  prefs: []
  type: TYPE_NORMAL
- en: High-Level Learnings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DALLE 2 and AlphaFold 3 made improvements to their predecessors by using diffusion
    modules which simultaneously simplified their architectures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training on a wider range of data types makes generative models more robust.
    Diversifying the types of structures in the AlphaFold training dataset allowed
    the model to improve protein folding predictions and generalize to other biomolecular
    interactions. Similarly, the diversity of text-image pairs used to train CLIP
    improved DALLE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The noise schedule is an important knob when training diffusion models. Turning
    it up or down affects the model’s ability to learn both coarse and fine details.
    Doing so allowed for a significant simplification of AlphaFold because it eliminated
    the need to make intermediate predictions about side-chain torsion angles etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thank you again for reading, and stay tuned for the next installment. Until
    then, keep learning and keep exploring.
  prefs: []
  type: TYPE_NORMAL
