<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Quantizing the Weights of AI Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Quantizing the Weights of AI Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quantizing-the-weights-of-ai-models-39f489455194?source=collection_archive---------6-----------------------#2024-09-07">https://towardsdatascience.com/quantizing-the-weights-of-ai-models-39f489455194?source=collection_archive---------6-----------------------#2024-09-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="3fa0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Reducing high-precision floating-point weights to low-precision integer weights</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@arunnanda?source=post_page---byline--39f489455194--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Arun Nanda" class="l ep by dd de cx" src="../Images/48836e7e13dbe0821bed6902209f2d25.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*kp3EETBZ43AvPhT2YETFQw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--39f489455194--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@arunnanda?source=post_page---byline--39f489455194--------------------------------" rel="noopener follow">Arun Nanda</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--39f489455194--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/50ca7965b4f8806bfb9c64b046896779.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c1ypTszS1x0d_yiMziRvRg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image created by author</figcaption></figure><p id="f3f0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To make AI models more affordable and accessible, many developers and researchers are working towards making the models smaller but equally powerful. Earlier in this series, the article <a class="af ny" href="https://medium.com/@arunnanda/reducing-the-size-of-ai-models-4ab4cfe5887a" rel="noopener"><em class="nz">Reducing the Size of AI Models</em></a> gives a basic introduction to quantization as a successful technique to reduce the size of AI models. Before learning more about the quantization of AI models, it is necessary to understand how the quantization operation works.</p><p id="1fcc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This article, the second in the series, presents a hands-on introduction to the arithmetics of quantization. It starts with a simple example of scaling number ranges and progresses to examples with clipping, rounding, and different types of scaling factors.</p><p id="2977" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are different ways to represent real numbers in computer systems, such as 32-bit floating point numbers, 8-bit integers, and so on. Regardless of the representation, computers can only express numbers in a finite range and of a limited precision. 32-bit floating point numbers (using the <a class="af ny" href="https://en.wikipedia.org/wiki/IEEE_754" rel="noopener ugc nofollow" target="_blank">IEEE 754 32-bit base-2</a> system) have a range from -3.4 * 10³⁸ to +3.4 * 10³⁸. The smallest positive number that can be encoded in this format is of the order of 1 * 10^-38. In contrast, signed 8-bit integers range from -128 to +127.</p><p id="f8a9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Traditionally, model weights are represented as 32-bit floats (or as 16-bit floats, in the case of many large models). When quantized to 8-bit integers (for example), the quantizer function maps the entire range of 32-bit floating point numbers to integers between -128 and +127.</p><h1 id="0664" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Scaling Number Ranges</h1><p id="4fc2" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Consider a rudimentary example: you need to map numbers in the integer range A from -1000 to 1000 to the integer range B from -10 to +10. Intuitively, the number 500 in range A maps to the number 5 in range B. The steps below illustrate how to do this formulaically:</p><ul class=""><li id="2a12" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">To transform a number from one range to another, you need to multiply it by the right scaling factor. The number 500 from range A can be expressed in the range B as follows:</li></ul><blockquote class="pe pf pg"><p id="d9ba" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">500 * scaling_factor = Representation of 500 in Range B = 5</p></blockquote><ul class=""><li id="69b5" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">To calculate the scaling factor, take the ratio of the difference between the maximum and minimum values of the target range to the original range:</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ph"><img src="../Images/014d4744fd61bb9f6832e61bfcaf579c.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*CjLGEM8T1D_twRurxVF6Wg.png"/></div></figure><ul class=""><li id="5ec3" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">To map the number 500, multiply it by the scaling factor:</li></ul><blockquote class="pe pf pg"><p id="7704" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">500 * (1/100) = 5</p></blockquote><ul class=""><li id="6849" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">Based on the above formulation, try to map the number 510:</li></ul><blockquote class="pe pf pg"><p id="1cd4" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">510 * (1/100) = 5.1</p></blockquote><ul class=""><li id="2be6" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">Since the range B consists only of integers, extend the above formula with a rounding function:</li></ul><blockquote class="pe pf pg"><p id="c484" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Round ( 510 * (1/100) ) = 5</p></blockquote><ul class=""><li id="583c" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">Similarly, all the numbers from 500 to 550 in Range A map to the number 5 in Range B. Based on this, notice that the mapping function resembles a step function with uniform steps.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pi"><img src="../Images/d4e43419f9c1b053ac389cfee3cf2ee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yw1yjS1duiRxnxErV40Krg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image created by author</figcaption></figure><p id="51ba" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The X-axis in this figure represents the source Range, A (unquantized weights) and the Y-axis represents the target Range, B (quantized weights).</p><h1 id="317a" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Simple Integer Quantization</h1><p id="c788" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">As a more practical example, consider a floating point range -W to +W, which you want to quantize to signed N-bit integers. The range of signed N-bit integers is -2^(N-1) to +2^(N-1)-1. But, to simplify things for the sake of illustration, assume a range from -2^(N-1) to +2^(N-1). For example, (signed) 8-bit integers range from -16 to +15 but here we assume a range from -16 to +16. This range is symmetric around 0 and the technique is called symmetric range mapping.</p><ul class=""><li id="0fb7" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">The scaling factor, s, is:</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pj"><img src="../Images/068239d0b2064c9224cedafa5b55c062.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*DJKFZQEB5wcEsizwtcgooQ.png"/></div></figure><ul class=""><li id="5b47" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">The quantized number is the product of the unquantized number and the scaling factor. To quantize to integers, we need to round this product to the nearest integer:</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pk"><img src="../Images/ee09c59ddd8ae1647bf91d675272749d.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*cZHwD9mYMSxtV1cyuFDyig.png"/></div></figure><p id="1e0d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To remove the assumption that the target range is symmetric around 0, you also account for the zero-point offset, as explained in the next section.</p><h1 id="4c18" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Zero Point Quantization</h1><p id="0c32" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">The number range -2^(N-1) to +2^(N-1), used in the previous example, is symmetric around 0. The range -2^(N-1) to +2^(N-1)-1, represented by N-bit integers, is not symmetric.</p><p id="7bbd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When the quantization number range is not symmetric, you add a correction, called a zero point offset, to the product of the weight and the scaling factor. This offset shifts the range such that it is effectively symmetric around zero. Conversely, the offset represents the quantized value of the number 0 in the unquantized range. The steps below show how to calculate the zero point offset, z.</p><ul class=""><li id="90af" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">The quantization relation with the offset is expressed as:</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ph"><img src="../Images/66f06ee0cdd5b6771f43382d4ca124de.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*5Hdq84aYR0IDtU5L5OPiIw.png"/></div></figure><ul class=""><li id="b90e" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">Map the extreme points of the original and the quantized intervals. In this context, W_min and W_max refer to the minimum and maximum weights in the original unquantized range.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ph"><img src="../Images/7644979bd7e76323e3ec4c823e5253f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*BKhJVETW4baJGpcOUGqThQ.png"/></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ph"><img src="../Images/02cee12747c02b98131657743af4bff6.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*gM0HBBatJi3lV28I-E21xQ.png"/></div></figure><ul class=""><li id="a0c5" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">Solving these linear equations for the scaling factor, s, we get:</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ph"><img src="../Images/4ed443bd993e9020a69f8210fc10274c.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*de2MbL58wJftNXVPx0A4eQ.png"/></div></figure><ul class=""><li id="a8b7" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">Similarly, we can express the offset, z, in terms of scaling factor s, as:</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ph"><img src="../Images/c5e44ff836ce56a73ecb5b60b7dab817.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*zKMFwNiNyDFSY3Z3T0aEgA.png"/></div></figure><ul class=""><li id="69bf" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">Substituting for s in the above relation:</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pl"><img src="../Images/85564153797cedc0f357a2af83b7d60b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*U3Vp4a8K8l6KdDfFG_861A.png"/></div></figure><ul class=""><li id="02ec" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">Since we are converting from floats to integers, the offset also needs to be an integer. Rounding the above expression:</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pl"><img src="../Images/ec0b5d248c7c1b9563b0e617c5093136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Zqjda6xxbLEsHoqPfnt8hQ.png"/></div></figure><h2 id="7bca" class="pm ob fq bf oc pn po pp of pq pr ps oi nl pt pu pv np pw px py nt pz qa qb qc bk">Meaning of Zero-Point</h2><p id="cb7d" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">In the above discussion, the offset value is called the zero-point offset. It is called the zero-point because it is the quantized value of the floating point weight of 0.</p><p id="3514" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When W = 0 in</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pk"><img src="../Images/e4534f2ff4111df5a09584514fafa485.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*daszCmF4WaSQjJdvPHQ5jw.png"/></div></figure><p id="d908" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You get:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pk"><img src="../Images/ae813a1bc998444a60858c8cd6dfa137.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*berSUfy6xwy5hUkbzRwv7g.png"/></div></figure><p id="8963" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The article, <a class="af ny" href="https://medium.com/@luis.vasquez.work.log/zero-point-quantization-how-do-we-get-those-formulas-4155b51a60d6" rel="noopener"><em class="nz">Zero-point quantization: How do we get those formulas</em></a>, by Luis Vasquez, discusses zero-point quantization with many examples and illustrative pictures.</p><h1 id="aedf" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">De-quantization</h1><p id="79b3" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">The function to obtain an approximation of the original floating point value from the quantized value is called the de-quantization function. It is simply the inverse of the original quantization relation:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pj"><img src="../Images/b92297666cca4e9c43592ade5710a943.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*GUu2CH_mES4geLr1cS3N8Q.png"/></div></figure><p id="6dbe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Ideally, the de-quantized weight should be equal to the original weight. But, because of the rounding operations in the quantization functions, this is not the case. Thus, there is a loss of information involved in the de-quantization process.</p><h1 id="fa62" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Improving the Precision of Quantization</h1><p id="7556" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">The biggest drawback of the above methods is the loss of precision. Bhandare et al, in a 2019 paper titled <a class="af ny" href="https://arxiv.org/pdf/1906.00532" rel="noopener ugc nofollow" target="_blank"><em class="nz">Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model</em></a>, were the first to quantize Transformer models. They demonstrated that naive quantization, as discussed in earlier sections, results in a loss of precision. In gradient descent, or indeed any optimization algorithm, the weights undergo just a slight modification in each pass. It is therefore important for the quantization method to be able to capture fractional changes in the weights.</p><h2 id="6f4f" class="pm ob fq bf oc pn po pp of pq pr ps oi nl pt pu pv np pw px py nt pz qa qb qc bk">Clipping the Range</h2><p id="b2cd" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Quantized intervals have a fixed and limited range of integers. On the other hand, unquantized floating points have a very large range. To increase the precision, it is helpful to reduce (clip) the range of the floating point interval.</p><p id="6aff" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is observed that the weights in a neural network follow a statistical distribution, such as a normal Gaussian distribution. This means, most of the weights fall within a narrow interval, say between W_max and W_min. Beyond W_max and W_min, there are only a few outliers.</p><p id="4d81" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the following description, the weights are clipped, and W_max and W_min refer to the maximum and minimum values of the weights in the clipped range.</p><p id="d27b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Clipping (restricting) the range of the floating point weights to this interval means:</p><ul class=""><li id="0699" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">Weights which fall in the tails of the distribution are clipped — Weights higher than W_max are clipped to W_max. Weights smaller than W_min are clipped to W_min. The range between W_min and W_max is the clipping range.</li><li id="1a88" class="nc nd fq ne b go qd ng nh gr qe nj nk nl qf nn no np qg nr ns nt qh nv nw nx pb pc pd bk">Because the range of the floating point weights is reduced, a smaller unquantized range maps to the same quantized range. Thus, the quantized range can now account for smaller changes in the values of the unquantized weights.</li></ul><p id="00c5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The quantization formula shown in the previous section is modified to include the clipping:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qi"><img src="../Images/47b2f90a3267dd360f1f9a72c85aafca.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*R4F2Oydd0ctB65W1TQZR6g.png"/></div></figure><p id="ee55" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The clipping range is customizable. You can choose how narrow you want this interval to be. If the clipping is overly aggressive, weights that contribute to the model’s accuracy can be lost in the clipping process. Thus, there is a tradeoff — clipping to a very narrow interval increases the precision of the quantization of weights within the interval, but it also reduces the model’s accuracy due to loss of information from those weights which were considered as outliers and got clipped.</p><h2 id="a875" class="pm ob fq bf oc pn po pp of pq pr ps oi nl pt pu pv np pw px py nt pz qa qb qc bk">Determining the Clipping Parameters</h2><p id="36af" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">It has been noted by many researchers that the statistical distribution of model weights has a significant effect on the model’s performance. Thus, it is essential to quantize weights in such a way that these statistical properties are preserved through the quantization. Using statistical methods, such as Kullback Leibler Divergence, it is possible to measure the similarity of the distribution of weights in the quantized and unquantized distributions.</p><p id="729e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The optimal clipped values of W_max and W_min are chosen by iteratively trying different values and measuring the difference between the histograms of the quantized and unquantized weights. This is called calibrating the quantization. Other approaches include minimizing the mean square error between the quantized weights and the full-precision weights.</p><h2 id="7463" class="pm ob fq bf oc pn po pp of pq pr ps oi nl pt pu pv np pw px py nt pz qa qb qc bk">Different Scaling Factors</h2><p id="4b4a" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">There is more than one way to scale floating point numbers to lower precision integers. There are no hard rules on what is the right scaling factor. Researchers have experimented with various approaches. A general guideline is to choose a scaling factor so that the unquantized and quantized distributions have a similar statistical properties.</p><p id="a14d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">MinMax Quantization</strong></p><p id="7cd6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The examples in the previous sections scale each weight by the difference of W_max and W_min (the maximum and minimum weights in the set). This is known as minmax quantization.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pj"><img src="../Images/62fe76ce030770d3a772c5ffab9da9c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*5oq6CnSWRZyfaRk2AyZd4Q.png"/></div></figure><p id="795b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is one of the most common approaches to quantization.</p><p id="a206" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">AbsMax Quantization</strong></p><p id="a022" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is also possible to scale the weights by the absolute value of the maximum weight:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pj"><img src="../Images/fd29fb0f4a10fbb088428c8c3c7238b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*As8AGmseltGDtP7y7QKErw.png"/></div></figure><p id="c08f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Wang et al, in their 2023 paper titled <a class="af ny" href="https://arxiv.org/pdf/2310.11453" rel="noopener ugc nofollow" target="_blank">BitNet: Scaling 1-bit Transformers for Large Language Models</a>, use absmax quantization to build the 1-bit BitNet Transformer architecture. The BitNet architecture is explained later in this series, in <a class="af ny" href="https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3" rel="noopener"><em class="nz">Understanding 1-bit Large Language Models</em></a>.</p><p id="1d62" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">AbsMean Quantization</strong></p><p id="64d5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Another approach is to make the scaling factor equal to the average of the absolute values of all the unquantized weights:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pj"><img src="../Images/408a6180e68e94a389c6b7d8d4d1874c.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*FVzkMslKbg7Njco8LTezTQ.png"/></div></figure><p id="ddcd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Ma et al, in the 2024 paper titled <a class="af ny" href="https://arxiv.org/pdf/2402.17764" rel="noopener ugc nofollow" target="_blank">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a>, use absmean quantization to build a 1.58-bit variant of BitNet. To learn more about 1.58-bit language models, refer to <a class="af ny" href="https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a" rel="noopener"><em class="nz">Understanding 1.58-bit Large Language Models</em></a>.</p><h2 id="3e8e" class="pm ob fq bf oc pn po pp of pq pr ps oi nl pt pu pv np pw px py nt pz qa qb qc bk">Granularity of Quantization</h2><p id="4c43" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">It is possible to quantize all the weights in a model using the same quantization scale. However, for better accuracy, it is also common to calibrate and estimate the range and quantization formula separately for each tensor, channel, and layer. The article <a class="af ny" href="https://medium.com/@arunnanda/different-approaches-to-quantization-e3fac905bd5a" rel="noopener"><em class="nz">Different Approaches to Quantization</em></a> discusses the granularity levels at which quantization is applied.</p><h1 id="85f5" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Extreme Quantization</h1><p id="5597" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Traditional quantization approaches reduce the precision of model weights to 16-bit or 8-bit integers. Extreme quantization refers to quantizing weights to 1-bit and 2-bit integers. Quantization to 1-bit integers ({0, 1}) is called binarization. The simple approach to binarize floating point weights is to map positive weights to +1 and negative weights to -1:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ph"><img src="../Images/7a86ce61c5ee49b12d8679bf2f3c9973.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*AcePriNdWfph38jxx6yFuA.png"/></div></figure><p id="9a93" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Similarly, it is also possible to quantize weights to ternary ({-1, 0, +1}):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pj"><img src="../Images/da4189c9a88e3e11e5870626fa48a3c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*hkDX_ZjJlB9io9haY3HA0w.png"/></div></figure><p id="7ca0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the above system, Delta is a threshold value. In a simplistic approach, one might quantize to ternary as follows:</p><ul class=""><li id="73c9" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">Normalize the unquantized weights to lie between -1 and +1</li><li id="24c0" class="nc nd fq ne b go qd ng nh gr qe nj nk nl qf nn no np qg nr ns nt qh nv nw nx pb pc pd bk">Quantize weights below -0.5 to -1</li><li id="8a09" class="nc nd fq ne b go qd ng nh gr qe nj nk nl qf nn no np qg nr ns nt qh nv nw nx pb pc pd bk">Quantize weights between -0.5 and +0.5 to 0</li><li id="dd96" class="nc nd fq ne b go qd ng nh gr qe nj nk nl qf nn no np qg nr ns nt qh nv nw nx pb pc pd bk">Quantize weights above 0.5 to +1.</li></ul><p id="0dcb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Directly applying binary and ternary quantization leads to poor results. As discussed earlier, the quantization process must preserve the statistical properties of the distribution of the model weights. In practice, it is common to adjust the range of the raw weights before applying the quantization and to experiment with different scaling factors.</p><p id="79a9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Later in this series, the articles <a class="af ny" href="https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3" rel="noopener"><em class="nz">Understanding 1-bit Large Language Models</em></a> and <a class="af ny" href="https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a" rel="noopener"><em class="nz">Understanding 1.58-bit Language Models</em></a> discuss practical examples of binarization and ternarization of weights. The 2017 paper titled<a class="af ny" href="https://arxiv.org/pdf/1612.01064" rel="noopener ugc nofollow" target="_blank"> <em class="nz">Trained Ternary Quantization</em> by Zhu et al</a> and the <a class="af ny" href="https://arxiv.org/pdf/2303.01505" rel="noopener ugc nofollow" target="_blank">2023 survey paper on ternary quantization </a>by Liu et al dive deeper into the details of ternary quantization.</p><p id="04d1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The premise of binarization is that even though this process (binarization) seems to result in a loss of information, using a large number of weights compensates for this loss. The statistical distribution of the binarized weights is similar to that of the unquantized weights. Thus, deep neural networks are still able to demonstrate good performance even with binary weights.</p><h1 id="826a" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Non-uniform Quantization</h1><p id="6c33" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">The quantization methods discussed so far uniformly map the range of unquantized weights to quantized weights. They are called “uniform” because the mapping intervals are equidistant. To clarify, when you mapped the range -1000 to +1000 to the range -10 to +10:</p><ul class=""><li id="6049" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pb pc pd bk">All the numbers from -1000 to -951 are mapped to -10</li><li id="4632" class="nc nd fq ne b go qd ng nh gr qe nj nk nl qf nn no np qg nr ns nt qh nv nw nx pb pc pd bk">The interval from -950 to -851 is mapped to -9</li><li id="0d98" class="nc nd fq ne b go qd ng nh gr qe nj nk nl qf nn no np qg nr ns nt qh nv nw nx pb pc pd bk">The interval from -850 to -751 maps to -8</li><li id="358c" class="nc nd fq ne b go qd ng nh gr qe nj nk nl qf nn no np qg nr ns nt qh nv nw nx pb pc pd bk">and so on…</li></ul><p id="7734" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These intervals are also called bins.</p><p id="4a39" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The disadvantage of uniform quantization is that it does not take into consideration the statistical distribution of the weights themselves. It works best when the weights are equally distributed between W_max and W_min. The range of floating point weights can be considered as divided into uniform bins. Each bin maps to one quantized weight.</p><p id="271c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In reality, floating point weights are not distributed uniformly. Some bins contain a large number of unquantized weights while other bins have very few. Non-uniform quantization aims to create these bins in such a way that bins with a higher density of weights map to a larger interval of quantized weights.</p><p id="1823" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are different ways of representing the non-uniform distribution of weights, such as K-means clustering. However, these methods are not currently used in practice, due to the computational complexity of their implementation. Most practical quantization systems are based on uniform quantization.</p><p id="437e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the hypothetical graph below, in the chart on the right, unquantized weights have a low density of distribution towards the edges and a high density around the middle of the range. Thus, the quantized intervals are larger towards the edges and compact in the middle.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pi"><img src="../Images/6b85f289a7fd0b08c8ae28afa8bbd9d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kt6FvZgQRoeO60eXXPmHsA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image created by author</figcaption></figure><h1 id="b568" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Quantizing Activations and Biases</h1><p id="bf43" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">The activation is quantized similarly as the weights are, but using a different scale. In some cases, the activation is quantized to a higher precision than the weights. In models like <a class="af ny" href="https://medium.com/@arunnanda/extreme-quantization-1-bit-ai-models-07169ee29d96" rel="noopener">BinaryBERT</a>, and the <a class="af ny" href="https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3" rel="noopener">1-bit Transformer — BitNet</a>, the weights are quantized to binary but the activations are in 8-bit.</p><p id="59e1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The biases are not always quantized. Since the bias term only undergoes a simple addition operation (as opposed to matrix multiplication), the computational advantage of quantizing the bias is not significant. Also, the number of bias terms is much less than the number of weights.</p><h1 id="fc34" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Conclusion</h1><p id="d0a8" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">This article explained (with numerical examples) different commonly used ways of quantizing floating point model weights. The mathematical relationships discussed here form the foundation of <a class="af ny" href="https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3" rel="noopener">quantization to 1-bit weights</a> and to <a class="af ny" href="https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a" rel="noopener">1.58-bit weights</a> — these topics are discussed later in the series.</p><p id="8920" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To learn more about the mathematical principles of quantization, refer to <a class="af ny" href="https://arxiv.org/pdf/2112.06126" rel="noopener ugc nofollow" target="_blank">this 2023 survey paper by Weng</a>. <a class="af ny" href="https://leimao.github.io/article/Neural-Networks-Quantization/" rel="noopener ugc nofollow" target="_blank"><em class="nz">Quantization for Neural Networks</em></a> by Lei Mao explains in greater detail the mathematical relations involved in quantized neural networks, including non-linear activation functions like the ReLU. It also has code samples implementing quantization. The next article in this series, <a class="af ny" href="https://medium.com/@arunnanda/quantizing-neural-network-models-8ce49332f1d3" rel="noopener"><em class="nz">Quantizing Neural Network Models</em></a>, presents the high-level processes by which neural network models are quantized.</p></div></div></div></div>    
</body>
</html>