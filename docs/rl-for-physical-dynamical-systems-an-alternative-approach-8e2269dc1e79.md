# 物理动力学系统的强化学习：一种替代方法

> 原文：[https://towardsdatascience.com/rl-for-physical-dynamical-systems-an-alternative-approach-8e2269dc1e79?source=collection_archive---------1-----------------------#2024-07-28](https://towardsdatascience.com/rl-for-physical-dynamical-systems-an-alternative-approach-8e2269dc1e79?source=collection_archive---------1-----------------------#2024-07-28)

## 重新引入遗传算法并与神经网络进行比较

[](https://medium.com/@retter_42511?source=post_page---byline--8e2269dc1e79--------------------------------)[![Robert Etter](../Images/8193c0477b2a7df1177779ca5937854a.png)](https://medium.com/@retter_42511?source=post_page---byline--8e2269dc1e79--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8e2269dc1e79--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8e2269dc1e79--------------------------------) [Robert Etter](https://medium.com/@retter_42511?source=post_page---byline--8e2269dc1e79--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8e2269dc1e79--------------------------------) ·14分钟阅读·2024年7月28日

--

![](../Images/03893c77eceb2ecc28ad297ed423a40e.png)

图片来自 [Tra Nguyen](https://unsplash.com/@thutra0803?utm_source=medium&utm_medium=referral) 于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

# 物理与非线性动力学

控制理论通过经典、鲁棒和最优方法支撑了现代文明。精炼、电信、现代制造等各个领域都依赖于它们。控制理论建立在物理方程提供的见解之上，例如牛顿定律和麦克斯韦方程。这些方程描述了物理系统中的动态和不同力的相互作用。通过它们，我们理解方程如何在不同状态之间转换，其中“状态是足够描述系统的所有信息的集合”[1]，通常是通过流体动力学中的压力或流体粒子的速度，或者在电动力学中的电荷和电流状态来表达。通过推导系统方程，我们可以预测状态如何随时间和空间变化，并通过微分方程表达这种演化。通过这种理解，我们可以采用控制手段，即通过特别施加的力，将这些系统维持在期望的状态或输出上。通常，这种力是根据系统的输出计算的。以车辆巡航控制为例，输入是期望的速度，输出是实际的速度。系统是发动机。状态估计器观察速度，并确定输出与输入速度之间的差异，然后应用控制手段（如调整燃油流量）来减少误差。

然而，尽管控制理论取得了诸多成就，它仍面临着显著的局限性。大多数控制理论是基于线性系统，或是输入的比例变化导致输出的比例变化的系统。虽然这些系统可能非常复杂，但我们对这些系统有着广泛的理解，使我们能够实际控制从深海潜水器、矿山设备到航天器的各种设备。

然而，正如斯坦尼斯瓦夫·乌拉姆所言：“使用‘非线性科学’这样的术语，就像是把大部分动物学称为研究非大象动物。”到目前为止，我们在控制复杂物理系统方面的进展大多通过找到限制它们线性行为的方法。这可能会在多个方面带来效率损失：

· 将复杂系统拆解为可单独控制的组件，优化子系统而非整个系统

· 在较简单但效率较低的操作模式下运行系统，或未能充分利用复杂的物理学原理，例如主动流动控制以减少飞机的阻力

· 严格的操作条件限制，一旦超过可能导致不可预测或灾难性的故障

高级制造业、改进的空气动力学和复杂的电信系统都将受益于更好的非线性系统控制方法。

非线性动力学系统的基本特征是它们对输入的复杂响应。即使在环境或状态发生小幅变化的情况下，非线性系统也会剧烈变化。以控制流体流动的纳维-斯托克斯方程为例：同一组方程既描述了一个平静、缓慢流动的小溪，也描述了一个汹涌的洪流，所有洪流中的漩涡和特征都包含在方程的动态变化之中。

非线性系统带来了困难：与线性系统不同，我们通常无法轻松预测系统在从一个状态过渡到下一个状态时的行为。我们所能做到的最好方法是通过一般分析或广泛的仿真。因此，在非线性系统中，我们面临两个问题：系统识别——即理解系统在给定状态下的行为，以及系统控制——即系统在响应给定输入时的短期和长期变化，以及如何选择输入以获得期望的结果。

# 强化学习与物理学

尽管非线性分析和控制不断取得进展，但我们在利用这些系统方面仍然受到传统基于方程方法的限制。然而，随着计算能力和传感器技术变得更加普及，基于数据的方法提供了一种不同的途径。

数据可用性的剧增催生了机器学习（ML）方法，而强化学习（RL）提供了一种新的方法，以更有效地应对控制非线性动态系统的挑战。RL已经在从自动驾驶汽车到战略和计算机游戏的环境中取得了成功，它是一个机器学习框架，通过“试错”来训练算法或智能体，使其“学会如何在不确定性下做出决策，以最大化长期收益”[1]。换句话说，RL算法解决了系统识别和控制优化的问题，而不是通过操控和分析控制方程来实现，而是通过采样环境来预测哪些输入动作会导致期望的结果。RL算法或智能体根据系统状态应用一套行动策略，并随着对系统更多信息的分析而不断完善这一策略。

许多强化学习（RL）算法基于使用神经网络来开发将状态映射到最佳行为的函数。RL问题可以被构建为状态-行动-奖励三元组。对于给定的状态，某个特定的行动会导致一个特定的奖励。神经网络作为通用函数逼近器，可以进行调整，以准确地逼近整个系统中的状态-行动-奖励三元组函数。为了做到这一点，神经网络必须通过探索系统或环境来获取新知识，然后通过利用额外获得的数据来精炼其策略。RL算法通过它们如何应用数学来探索、利用和在两者之间平衡，从而实现差异化。

然而，神经网络也带来了一些挑战：

· 资源需求。使用神经网络来估算一个能够为每个状态确定奖励和最佳行动的函数可能需要相当长的时间和大量的数据。

· 可解释性。通常很难理解神经网络是如何得出其解决方案的，这限制了它们在提供真正洞见方面的实用性，并且可能使得预测或界定神经网络的行为变得困难。可解释性对于物理系统尤其重要，因为它能够使得几百年来数学上开发出的强大分析工具得以应用，从而为系统提供额外的洞见。

尽管存在一些方法，如[迁移学习](/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)和[拓扑分析](/explainable-deep-neural-networks-2f40b89d4d6f)，来应对这些挑战，但它们仍然是强化学习广泛应用的障碍。然而，另一种替代方法可能在我们专注于物理系统时会有所帮助。回想一下，我们讨论的物理系统是通过数学方程来定义的，或者可以通过数学方程很好地描述。与其开发一个完全任意的函数，我们可以集中精力寻找由常见数学运算符组成的表达式：算术运算、代数运算以及超越函数（如正弦函数、e^x等）。为此目的的一种方法是使用遗传算法。正如在[2]中所描述的，遗传算法可以通过随机生成函数并通过变异和交叉繁殖有前途的候选者来探索函数空间，并通过这些方式利用和优化解决方案。

因此，尽管神经网络在大多数强化学习问题中表现出色，但在物理动态系统中，出现了一位新的挑战者。接下来，我们将更深入地研究遗传算法方法，并查看它如何与领先的强化学习算法——软演员评论家（SAC）进行对比。为此，我们将在基于物理的训练场中使用AWW Sagemaker实验来评估两者的表现。最后，我们将通过评估结果、讨论结论并建议下一步的工作来结束本文。

回想一下，强化学习（RL）面临着两个挑战：探索环境和利用已发现的信息。探索对于找到最佳策略是必要的，考虑到处于任何状态的可能性。如果不进行探索，就可能错过全局最优解，而仅停留在局部最优，且算法可能无法足够泛化，从而在所有状态下都能成功。利用已知信息则是为了将当前解决方案优化到最优。然而，当算法精细化一个特定的解决方案时，它就牺牲了进一步探索系统的能力。

Soft Actor Critic（SAC）是对强大Actor-Critic强化学习方法的改进。Actor-Critic算法家族通过将状态值及其相关奖励的估计与优化特定输入策略分开，来处理探索与利用之间的权衡。随着算法收集新信息，它会更新每个估计器。Actor-Critic在实现上有许多细节；有兴趣的读者可以参考[书籍](https://www.amazon.com/Mastering-Reinforcement-Learning-Python-next-generation/dp/1838644148)或在线教程。SAC通过优先探索那些奖励与估计的评价值差异显著的状态，来优化评价器。[OpenAI](https://spinningup.openai.com/en/latest/algorithms/sac.html)提供了SAC的详细描述。

对于本实验，我们使用了[SAC的Coax实现](https://coax.readthedocs.io/en/latest/examples/pendulum/sac.html)。我查看了几个强化学习库，包括Coach和Spinning Up，但Coax是我找到的少数几个在当前Python版本中“开箱即用”的库之一。Coax库包括广泛的强化学习算法，包括PPO、TD3和DDPG，并且与gymnasium兼容良好。

像 SAC 这样的演员-评论员方法通常通过神经网络作为函数近似器来实现。正如我们上次讨论的那样，还有一种潜在的方法来探索系统并利用潜在的控制策略。遗传算法通过随机生成可能的解决方案来进行探索，并通过变异或组合不同解决方案的元素（繁殖）来利用有前景的策略。在这种情况下，我们将评估遗传算法的遗传编程变体，作为函数近似的另一种手段；具体来说，我们将使用遗传方法随机生成并评估包含常量、状态变量和数学函数的函数树，作为潜在的控制器。

实现的遗传编程（GP）算法改编自[2]，不同之处在于，该文中使用的锦标赛被本实现替换为选择每代的前 64%（Nn，以下为 33%）作为突变的候选，并为剩余部分重新播种，以便更好地探索解空间。为了在每一代中创建个体树，生长函数随机调用算术函数（+，-，*，/）和超越函数（例如 e^x, cos (x)）来构建分支，并将常量或状态变量作为叶子，构成树的终端分支。递归调用用于根据[波兰表示法](https://en.wikipedia.org/wiki/Polish_notation#Computer_programming)（[2]通过LISP实现，我已将其改编为Python）构建表达式，设有规则以避免如除以0等问题，并确保数学一致性，使每个分支最终正确地以常量或传感器值作为叶子。概念上，一个方程树的形式如下：

![](../Images/091d4561cf55ad9e6718afa9897fb8fa.png)

图 1\. 示例函数树，作者基于[2]提供

这导致了一个控制器 b = sin (s1) + e^(s1*s2/3.23) - 0.12，脚本中写作： — + sin s1 e^ / * s1 s2 3.23 0.12，其中 s 表示状态变量。刚开始可能会让人感到困惑，但写出一些例子可以澄清这一方法。

在构建完整代数的树后，每棵树都会通过环境进行性能评估。然后，基于获得的奖励，按控制性能对树进行排名。如果未达到期望的性能，则保留表现最好的树，前 66% 的树通过交叉（交换两棵树的元素）、剪切与生长（替换树的一个元素）、收缩（用常量替换树元素）或重新参数化（替换树中的所有常量）进行变异，参考文献[2]。这使得能够利用最有前景的解决方案。为了继续探索解空间，表现较差的解决方案会被随机新树所替代。每一代之后，新的代际将是随机新个体与顶级表现解决方案的复制或变异的结合。

树会在环境中随机起始位置进行测试。为了防止“幸运”的起始状态影响结果（类似于模型过拟合），树会在一批不同的随机起始状态下进行测试。

遗传编程的超参数包括：

![](../Images/8f7382b6e0934b6d74ed6e925c3f70f3.png)

表 1\. 遗传编程算法的超参数

注释过的代码可以在[github](https://github.com/retter-berkeley/MLC_Genetic)上找到。请注意，我是一个业余编程者，代码可能有些笨拙。希望即使存在不符合 Python 风格或普遍不良的编程实践，代码至少足够可读，能够理解我的方法。

# 评估方法

两种算法在两个不同的 gymnasium 环境中进行了评估。第一个是 gymnasium 基金会提供的简单[摆锤环境](https://gymnasium.farama.org/environments/classic_control/pendulum/)。倒立摆是一个简单的非线性动力学问题。动作空间是可以施加于摆锤的连续扭矩。观察空间与状态相同，是 x、y 坐标和角速度。目标是保持摆锤直立。第二个是相同的 gymnasium 环境，但在观察中加入了随机噪声。噪声服从均值为 0，方差为 0.1 的正态分布，以模拟现实传感器测量值。

强化学习开发中最重要的部分之一是设计一个合适的奖励函数。尽管有许多算法可以解决给定的强化学习问题，但为这些算法定义一个合适的奖励以便优化，是使某个算法在特定问题上成功的关键步骤。我们的奖励需要让我们能够比较两种不同强化学习方法的结果，同时确保每种方法都能够朝着其目标前进。在这里，对于每个轨迹，我们跟踪累积奖励和平均奖励。为了简化这一过程，我们让每个环境运行一个固定数量的时间步，每个时间步根据代理距离目标状态的远近给予负奖励。Pendulum gym 就是这样工作的——在 200 个时间步后截断，并根据钟摆的竖直程度给予负奖励，最大奖励为 0，并在每个时间步都进行强制执行。我们将使用平均奖励来比较这两种方法。

我们的目标是评估每个强化学习（RL）框架的收敛速度。我们将使用 AWS Sagemaker 实验来实现这一目标，它可以自动追踪度量指标（如当前奖励）和参数（如活跃的超参数），并按迭代或 CPU 时间跨运行进行记录。尽管这些监控可以通过 Python 工具实现，但 Experiments 提供了简化的运行参数和性能跟踪、索引功能，以及计算资源的复现。为了设置实验，我参考了[AWS](https://sagemaker-examples.readthedocs.io/en/latest/index.html)提供的示例。SAC 和 GP 算法首先在本地 Jupyter 笔记本中进行评估，然后上传到 git 仓库。每个算法都有自己的仓库和 Sagemaker 笔记本。运行参数被存储以帮助分类运行并跟踪不同实验设置的性能。我们的运行度量，奖励和状态向量，是我们希望用来比较两种算法的因变量。Experiments 自动记录 CPU 时间和迭代次数，作为自变量。

通过这些实验，我们可以将冠军算法——像 SAC 这样的成熟且经过充分开发的强化学习算法——与挑战者算法进行比较，后者是一种不为人知的方法，由一位没有正式强化学习或 Python 培训的业余编码员编写。这个实验将为开发复杂非线性系统控制器的不同方法提供见解。在下一部分，我们将回顾并讨论结果及潜在的后续工作。

第一个实验是默认的 Pendulum gymnasium，在这个实验中，算法尝试确定正确的扭矩，以保持钟摆倒立。它在固定时间后结束，并根据钟摆距离竖直的程度给予负奖励。在 Sagemaker 实验运行之前，SAC 和 GP 算法已经在我的本地机器上运行，以验证收敛性。在 Experiments 中运行可以更好地追踪可比较的计算时间。以下是计算时间与每次迭代平均奖励的结果：

![](../Images/2df0567b0b40db3641e835c194b25f03.png)

作者提供

![](../Images/d1d598103d8a6bbf64d158fd729d2f97.png)

作者提供

我们看到，尽管GP算法相对不够成熟，但它在计算需求上远低于SAC算法。通过本地运行至完成，SAC似乎需要大约40万次迭代才能收敛，耗时数小时。本地实例化被编程为在整个训练过程中记录SAC的进展；有趣的是，SAC似乎先学会了如何让摆锤移动到顶端，然后学会了如何保持摆锤静止，最终将这两者结合起来，这可以解释奖励的下降期，即SAC学习保持摆锤稳定的阶段。通过GP，我们看到奖励以单调递增的方式逐步提升。这是因为最优的函数树始终被保留，所以最好的奖励保持稳定，直到计算出更好的控制器。

第二个实验是将高斯噪声（0, 0.1）添加到状态测量中。我们看到的结果与无噪声的情况相似，只是收敛时间较长。结果如下所示；同样，GP优于SAC。

![](../Images/37c65ede7ac73ece0b7c6e9512a4e9b1.png)

作者提供

![](../Images/aba7ad80a4209ba235a59b35166022d7.png)

作者提供

在这两种情况下，我们看到GP比SAC表现得更快（如前面的例子所示，SAC确实在本地收敛，只是我不想为计算时间支付AWS费用！）。然而，正如大家无疑已经注意到的，这只是一个非常基础的比较，既涉及机器学习也涉及物理系统。例如，超参数调优可能会导致不同的结果。尽管如此，这对于该候选算法来说是一个有前景的开始，表明它值得进一步研究。

从长远来看，我认为GP可能比基于神经网络的方法（如SAC）提供几个优势：

· 可解释性。尽管GP找到的方程可能很复杂，但它是透明的。熟练者可以简化方程，帮助提供对已确定解的物理理解，这对于确定适用区域和增加控制信任度非常有帮助。尽管可解释性是一个活跃的研究领域，但神经网络依然面临这一挑战。

· 信息化机器学习（Informed ML）。遗传规划（GP）使得将系统分析的见解更容易应用。例如，如果已知系统具有正弦波行为，GP算法可以调整以尝试更多的正弦解。或者，如果已知一个类似或简化系统的解，那么可以将该解预先植入算法中。

· 稳定性。通过简单的保障措施（如数学有效性和限制绝对值的增加），GP方法将保持稳定。只要每一代保留最优解，解决方案就会收敛，尽管收敛的时间界限无法保证。常见的强化学习中的神经网络方法并没有这样的保证。

· 发展机会。遗传编程相对不成熟。这里使用的SAC实现是多种应用实现中的一种，而神经网络已经受益于大量努力以提高性能。遗传编程没有享受这种优化；我的实现是围绕函数构建的，而非效率。尽管如此，它在与SAC对比时表现良好，来自更专业开发者的进一步改进可能会带来显著的效率提升。

· 并行性和模块化。与神经网络相比，单独的遗传编程方程较为简单，计算成本来自于在环境中多次运行，而不是环境运行和神经网络的反向传播。将不同的遗传编程方程树分配到不同的处理器上，可以大大提高计算速度。

然而，神经网络方法因其众多优势被广泛使用：

· 范围。神经网络是通用函数逼近器，遗传编程仅限于在函数树中定义的项。因此，基于神经网络的方法能够涵盖更广泛和复杂的情况。我不愿意尝试用遗传编程来玩《星际争霸》或驾驶汽车。

· 跟踪。遗传编程是随机搜索的精细版本，正如实验中所见，导致了改进的停滞。

· 成熟度。由于大量不同神经网络算法的研究工作，现有的经过优化的神经网络模型更容易应用于问题，并具有较高的计算效率。

从机器学习的角度来看，我们仅仅触及了这些算法的表面。需要考虑的一些后续工作包括：

· 超参数调优。

· 控制器简化，例如惩罚遗传编程中控制输入项数的奖励。

· 控制器效率，例如从奖励中扣除控制输入的大小。

· 如上所述，遗传编程的监控与算法改进。

从物理学角度来看，本实验作为进入更真实场景的起点。更复杂的场景可能会显示出神经网络方法赶上或超过遗传编程（GP）。可能的后续工作包括：

· 更复杂的动力学，例如范德波尔方程或更高维度。

· 有限可观察性，而非完全状态可观察性。

· 偏微分方程系统以及优化控制器位置和输入。

[1] E. Bilgin, 《掌握强化学习与Python：使用强化学习技术和最佳实践构建下一代自学习模型》（2020），Packit出版社

[2] T Duriez, S. Brunton, B. Noack, 《机器学习控制——驯服非线性动力学与湍流》（2017），Spring国际出版公司
