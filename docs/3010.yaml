- en: 'Ranking Basics: Pointwise, Pairwise, Listwise'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ranking-basics-pointwise-pairwise-listwise-cd5318f86e1b?source=collection_archive---------3-----------------------#2024-12-14](https://towardsdatascience.com/ranking-basics-pointwise-pairwise-listwise-cd5318f86e1b?source=collection_archive---------3-----------------------#2024-12-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Because thy neighbour matters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@kunals726?source=post_page---byline--cd5318f86e1b--------------------------------)[![Kunal
    Santosh Sawant](../Images/f8689b4e61020ca714c28806d51f9b72.png)](https://medium.com/@kunals726?source=post_page---byline--cd5318f86e1b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cd5318f86e1b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cd5318f86e1b--------------------------------)
    [Kunal Santosh Sawant](https://medium.com/@kunals726?source=post_page---byline--cd5318f86e1b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cd5318f86e1b--------------------------------)
    ·6 min read·Dec 14, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54cb29c9cdd0f38ae586510bb08af75b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image taken from unsplash.com
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s talk about where ranking comes into play. Ranking is a big deal
    in e-commerce and search applications — essentially, any scenario where you need
    to organize documents based on a query. It’s a little different from classic classification
    or regression problems. For instance, in the Titanic dataset, you predict whether
    a passenger survives or not, and in house price prediction, you estimate the price
    of a house. But with ranking, the game changes. Instead of predicting a single
    value or category, you’re trying to order documents based on relevance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take an example: You search for “saree” on an e-commerce website like Amazon.
    You don’t just want a random list of sarees; you want the most relevant ones to
    appear at the top, right? That’s where Learning to Rank (LTR) steps in — it ranks
    documents (or products) based on how well they match your query.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know where ranking fits in, let’s dive into the nitty-gritty of
    different approaches and methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main methods for Learning to Rank (LTR):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pointwise**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pairwise**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Listwise**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To make things easier to follow, let’s establish some notation that we’ll use
    to explain these methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll work with a set of queries ***q1,q2,…,qn*** and each query has a corresponding
    set of documents ***d1,d2,d3,…,dm***​. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Query ***q1*** is associated with documents ***d1***,***d2***,***d3***
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query ***q2*** associated with documents ***d4***,***d5***.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this setup in mind, let’s break down each method and how they approach
    the ranking problem.
  prefs: []
  type: TYPE_NORMAL
- en: Pointwise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the **pointwise approach**, we treat the ranking problem as a simple classification
    task. For each query-document pair, we assign a target label that indicates the
    relevance of the document to the query. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Label `1` if the document is relevant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label `0` if the document is not relevant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using our earlier example, the data would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '***q1,d1***→label: 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***q1,d2***→label: 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***q1,d3***→label: 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***q2,d4***→label: 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***q2,d5***→label: 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We train the model using this labeled data, leveraging features from both the
    queries and the documents to predict the label. After training, the model predicts
    the relevance of each document to a given query as a probability (ranging from
    0 to 1). This probability can be interpreted as the relevance score.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, after training, the model might produce the following scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '***q1​,d1***​→score: 0.6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***q1,d2***→score: 0.1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***q1,d3***→score: 0.4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using these scores, we re-rank the documents in descending order of relevance:
    ***d1,d3,d2***. This new ranking order is then presented to the user, ensuring
    the most relevant documents appear at the top.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pairwise**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main drawback of the **pointwise approach** is that it misses the **context**
    in which the user interacts with a document. When a user clicks on or finds a
    document relevant, there are often multiple factors at play — one of the most
    important being the **neighboring items**.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if a user clicks on a document, it might not necessarily mean
    that the document is highly relevant. It could simply be that the other documents
    presented were of poor quality. Similarly, if you had shown a different set of
    documents for the same query, the user’s interaction might have been entirely
    different.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine presenting ***d4***​ for query ***q1***​. If ***d4​*** is more relevant
    than ***d1***​, the user might have clicked on ***d4​*** instead. This context
    — how documents compare to each other is completely overlooked in the pointwise
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: To capture this **relative relevance**, we turn to the **pairwise approach**.
  prefs: []
  type: TYPE_NORMAL
- en: In the pairwise method, instead of looking at query-document pairs in isolation,
    we focus on **pairs of documents** for the same query and try to predict which
    one is more relevant. This helps incorporate the context of comparison between
    documents.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll generate the data similarly for now, but the way we use it will be slightly
    more complex. Let’s break that down next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine the training data for the **pairwise approach** structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '***q1,(d1,d2)***→label: 1(indicating ***d1***​ is more relevant than ***d2***​)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***q1,(d2,d3)***→label: 0 (indicating ***d2***​ is less relevant than ***d3***​)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***q1,(d1,d3)***→label: 1 (indicating ***d1*** is more relevant than ***d3​***)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***q2,(d4,d5)***→label: 0(indicating ***d4*** is less relevant than ***d5***​)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, we assign the labels based on user interactions. For instance, ***d1***​
    and ***d3***​ both being clicked indicates they are relevant, so we maintain their
    order for simplicity in this explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Training Process:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the training data is in pairs, the model doesn’t directly process these
    pairs. Instead, we treat it similarly to a classification problem, where each
    **query-document pair** is passed to the model separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '***s1 = f(q1,d1)***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***s2 = f(q1,d2)***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***s3 = f(q1,d3)***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model generates scores ***s1,s2,s3***​ for the documents. These scores are
    used to compare the relevance of document pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Penalizing the Model:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the model predicts scores that violate the true order of relevance, it is
    penalized. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: If ***s1<s2***, but the training data indicates ***d1>d2***​, the model is penalized
    because it failed to rank ***d1***​ higher than ***d2***​.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If ***s2<s3***​, and the training data indicates ***d2<d3***​, the model did
    the right thing, so no penalty is applied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**This pairwise comparison helps the model learn the relative order of documents
    for a query, rather than just predicting a standalone relevance score like in
    the pointwise approach.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Challenges:**'
  prefs: []
  type: TYPE_NORMAL
- en: One of the main challenges of implementing pairwise models is the **computational
    complexity** — since we need to compare all possible pairs of documents, the process
    scales as O(n²). Additionally, pairwise methods don’t consider the **global ranking**
    of documents; they focus only on individual pairs during comparisons, which can
    lead to inconsistencies in the overall ranking.
  prefs: []
  type: TYPE_NORMAL
- en: Listwise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In listwise ranking, the goal is to optimize the entire list of documents based
    on their relevance to a query. Instead of treating individual documents separately,
    the focus is on the order in which they appear in the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a breakdown of how this works in ListNet and LambdaRank:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NDCG (Normalized Discounted Cumulative Gain)**: I’ll dive deeper into NDCG
    in another blog, but for now, think of it as a way to measure how well the ordering
    of items matches their relevance. It rewards relevant items appearing at the top
    of the list and normalizes the score for easier comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In listwise ranking, if you have a list of documents (d1, d2, d3), the model
    considers all possible permutations of these documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '***(d1, d2, d3)***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***(d1, d3, d2)***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***(d2, d1, d3)***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***(d2, d3, d1)***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***(d3, d1, d2)***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***(d3, d2, d1)***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training Process:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Score Prediction**: The model predicts a score for each document in the list,
    and the documents are ranked according to these scores.For example: ***s1 = f(q1,d1),
    s2 = f(q1,d2)***'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Ideal Ranking**: The ideal ranking is calculated by sorting the documents
    based on their **true relevance**. For example, ***d1*** might be the most relevant,
    followed by ***d2***, and then ***d3.***'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**NDCG Calculation**: NDCG is calculated for each permutation of the document
    list. It checks how close the predicted ranking is to the ideal ranking, considering
    both relevance and the positions of the documents.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Penalizing Incorrect Rankings**: If the predicted ranking differs from the
    ideal, the NDCG score will drop. For example, if the ideal ranking is ***(d1,
    d3, d2)*** but the model ranks ***(d2, d1, d3)***, the NDCG score will be lower
    because the most relevant document (***d1***) isn’t ranked at the top.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Gradient Calculation**: The model calculates gradients based on how much
    the NDCG score would change if the order of documents was adjusted. These gradients
    guide the model on how to improve its predictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process helps the model learn to optimize the entire ranking list, improving
    the relevance of documents presented to users.
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to Learning to Rank, there’s no one-size-fits-all approach. Pointwise
    models are super easy to set up and update, but they don’t always take into account
    how documents relate to each other. That said, if you need something simple and
    fast, they’re a great option.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, ***pairwise*** and ***listwise*** methods are more powerful
    because they look at how documents compare to one another. But with that **power
    comes more complexity** 😛, and listwise can be a real challenge because of its
    high complexity in training.
  prefs: []
  type: TYPE_NORMAL
- en: Personally, I find the ***pairwise*** approach to be the sweet spot. It strikes
    a good balance between complexity and performance, making it ideal for many situations.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the day, the method you choose really depends on your situation.
    How big and complicated is your dataset? Knowing the pros and cons of each method
    will help you pick the one that works best for what you’re trying to do.
  prefs: []
  type: TYPE_NORMAL
- en: That’s a wrap for today! Stay tuned for the next part, Until then happy ranking!
    😊
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[From RankNet to LambdaRank to LambdaMART: An Overview](https://www.microsoft.com/en-us/research/uploads/prod/2016/02/MSR-TR-2010-82.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Learning to Rank: From Pairwise Approach to Listwise Approach](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to Learning to Rank](https://everdark.github.io/k9/notebooks/ml/learning_to_rank/learning_to_rank.html)'
  prefs: []
  type: TYPE_NORMAL
