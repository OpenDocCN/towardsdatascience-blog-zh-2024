- en: 'Through the Uncanny Mirror: Do LLMs Remember Like the Human Mind?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/through-the-uncanny-mirror-do-llms-remember-like-the-human-mind-cc9c63677610?source=collection_archive---------7-----------------------#2024-09-19](https://towardsdatascience.com/through-the-uncanny-mirror-do-llms-remember-like-the-human-mind-cc9c63677610?source=collection_archive---------7-----------------------#2024-09-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '|LLM|AI|HUMAN MIND|MEMORY|COGNITION|'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exploring the Eerie Parallels and Profound Differences Between AI and Human
    Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page---byline--cc9c63677610--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page---byline--cc9c63677610--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cc9c63677610--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cc9c63677610--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page---byline--cc9c63677610--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cc9c63677610--------------------------------)
    ·10 min read·Sep 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60a5523343be60b49897ceca8b28f392.png)'
  prefs: []
  type: TYPE_IMG
- en: image by the author using DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: '**The limits of my language** are **the limits of my** mind. — Ludwig Wittgenstein'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The true art of memory is the art of attention. — Samuel Johnson
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Language](https://en.wikipedia.org/wiki/Language) is one of the most important
    capabilities of human beings; it enables us to communicate and transfer knowledge,
    and it is considered a pillar of civilization. That is why the incredible capabilities
    displayed by [Large Language Models (LLMs)](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=What%20is%20a%20Large%20Language%20Model%20(LLM)%3F)
    have astounded the world, and made it ask the question: are they intelligent?'
  prefs: []
  type: TYPE_NORMAL
- en: 'All this has been achieved by huge amounts of text and a simple learning function:
    predicting the next word in a sequence. The model behind this success is the [Transformer](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)),
    and today modern derived LLMs are currently used by a large segment of the population
    for tasks such as translation, summarization, question answering, or generating
    articles.'
  prefs: []
  type: TYPE_NORMAL
