- en: Navigating Soft Actor-Critic Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索软演员评论家强化学习
- en: 原文：[https://towardsdatascience.com/navigating-soft-actor-critic-reinforcement-learning-8e1a7406ce48?source=collection_archive---------11-----------------------#2024-12-12](https://towardsdatascience.com/navigating-soft-actor-critic-reinforcement-learning-8e1a7406ce48?source=collection_archive---------11-----------------------#2024-12-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/navigating-soft-actor-critic-reinforcement-learning-8e1a7406ce48?source=collection_archive---------11-----------------------#2024-12-12](https://towardsdatascience.com/navigating-soft-actor-critic-reinforcement-learning-8e1a7406ce48?source=collection_archive---------11-----------------------#2024-12-12)
- en: Understanding the theory and implementation of SAC RL in the context of Bioengineering
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在生物工程背景下理解SAC强化学习的理论与实现
- en: '[](https://medium.com/@mo.abusadeh?source=post_page---byline--8e1a7406ce48--------------------------------)[![Mohammed
    AbuSadeh](../Images/2a7d5ce6964cdc76d9640b1a17ac707b.png)](https://medium.com/@mo.abusadeh?source=post_page---byline--8e1a7406ce48--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8e1a7406ce48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8e1a7406ce48--------------------------------)
    [Mohammed AbuSadeh](https://medium.com/@mo.abusadeh?source=post_page---byline--8e1a7406ce48--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mo.abusadeh?source=post_page---byline--8e1a7406ce48--------------------------------)[![Mohammed
    AbuSadeh](../Images/2a7d5ce6964cdc76d9640b1a17ac707b.png)](https://medium.com/@mo.abusadeh?source=post_page---byline--8e1a7406ce48--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8e1a7406ce48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8e1a7406ce48--------------------------------)
    [Mohammed AbuSadeh](https://medium.com/@mo.abusadeh?source=post_page---byline--8e1a7406ce48--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8e1a7406ce48--------------------------------)
    ·10 min read·Dec 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8e1a7406ce48--------------------------------)
    ·阅读时长10分钟·2024年12月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/117e5ac1c7be96924a7201b5185fd59e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/117e5ac1c7be96924a7201b5185fd59e.png)'
- en: Image generated by the author using ChatGPT-4o
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用ChatGPT-4o生成的图像
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The research domain of Reinforcement Learning (RL) has evolved greatly over
    the past years. The use of deep reinforcement learning methods such as Proximal
    Policy Optimisation (PPO) (Schulman, 2017) and Deep Deterministic Policy Gradient
    (DDPG) (Lillicrap, 2015) have enabled agents to solve tasks in high-dimensional
    environments. However, many of these model-free RL algorithms have struggled with
    stability during the training process. These challenges arise due to the brittle
    convergence properties, high variance in gradient estimation, very high sample
    complexity, and the sensitivity to hyperparameters in continuous action spaces.
    Given these problems, it is imperative to consider a newly devised RL algorithm
    that avoids such issues and expands applicability to complex, real-world problems.
    This new algorithm is the Soft Actor-Critic (SAC) deep RL network. (Haarnoja,
    2018)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）研究领域在过去几年里取得了巨大进展。深度强化学习方法的使用，如近端策略优化（PPO）（Schulman，2017）和深度确定性策略梯度（DDPG）（Lillicrap，2015），使得智能体能够在高维环境中解决任务。然而，许多这些无模型的强化学习算法在训练过程中存在稳定性问题。这些挑战源于脆弱的收敛性、高方差的梯度估计、非常高的样本复杂度以及在连续动作空间中对超参数的敏感性。鉴于这些问题，有必要考虑一种新开发的强化学习算法，避免这些问题并扩展其在复杂现实世界问题中的适用性。这个新算法就是软演员评论家（SAC）深度强化学习网络。（Haarnoja，2018）
- en: '![](../Images/719d9f7573dce860add1e140432f1fdb.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/719d9f7573dce860add1e140432f1fdb.png)'
- en: Model Architecture of Soft Actor-Critic Networks. Image taken from [(Du, 2023)](https://arxiv.org/abs/2301.03220)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 软演员评论家网络的模型架构。图像来源于[(Du, 2023)](https://arxiv.org/abs/2301.03220)
- en: SAC is an off-policy Actor-Critic deep RL algorithm which is designed to address
    the stability and efficiency constraints of its predecessors. The SAC algorithm
    is based on the maximum entropy RL framework which aims for the actor part of
    the network to maximise the expected reward, while maximising entropy. It combines
    off-policy updates with a more stable formulation of the stochastic Actor-Critic
    method. An off-policy algorithm enables faster learning and better sample efficiency
    using experience replay, unlike on-policy methods such as PPO, which require new
    samples for each gradient step. For on-policy methods such as PPO, for each gradient
    step in the learning process, new samples must be collected. The aim of using
    stochastic policies and maximising entropy comes to promote the robustness and
    exploration of the algorithm by encouraging more randomness in the actions. Additionally,
    unlike PPO and DDPG, SAC uses twin Q-networks with a separate Actor network and
    entropy tuning to improve the stability and convergence when combining off-policy
    learning with high dimensional, nonlinear function approximation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: SAC是一种离线策略（off-policy）Actor-Critic深度强化学习算法，旨在解决其前辈算法的稳定性和效率问题。SAC算法基于最大熵强化学习框架，目标是让网络的演员部分最大化预期奖励，同时最大化熵。它结合了离线策略更新和更稳定的随机Actor-Critic方法的表述。与PPO等在线策略方法不同，离线策略算法通过经验回放实现更快的学习和更好的样本效率，后者需要为每一步梯度更新收集新的样本。对于像PPO这样的在线策略方法，在学习过程中的每一步梯度更新都需要收集新的样本。使用随机策略和最大化熵的目的是通过鼓励行为中的更多随机性，促进算法的鲁棒性和探索性。此外，与PPO和DDPG不同，SAC使用了双Q网络、独立的演员网络和熵调整，以提高将离线学习与高维非线性函数逼近相结合时的稳定性和收敛性。
- en: Off-policy RL methods have had a wide impact on bioengineering systems that
    improve patient lives. More specifically, RL has been applied to domains such
    as robotic arm control, drug delivery methods and most notably de novo drug design.
    (Svensson, 2024) Svensson et al. has used a number of on- and off-policy frameworks
    and different variants of replay buffers to learn a RNN-based molecule generation
    policy, to be active against DRD2 (a dopamine receptor). The paper realises that
    using experience replay across the board for high, intermediate and low scoring
    molecules has shown effects in improving the structural diversity and the number
    of active molecules generated. Replay buffers improve sample efficiency in training
    agents. They also reported that the use of off-policy methods and more specifically
    SAC, helps in promoting structural diversity by preventing mode collapse.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 离线策略强化学习方法对改善患者生活的生物工程系统产生了广泛的影响。更具体地说，强化学习已被应用于诸如机器人手臂控制、药物递送方法以及最著名的去 novo药物设计等领域。（Svensson,
    2024）Svensson等人使用了多种在线和离线框架及不同类型的回放缓冲区，以学习基于RNN的分子生成策略，该策略能有效作用于DRD2（一种多巴胺受体）。该论文指出，使用经验回放处理高、中、低评分分子在提高结构多样性和生成活性分子的数量方面产生了效果。回放缓冲区提高了训练代理的样本效率。他们还报告指出，使用离线策略方法，特别是SAC，有助于通过防止模式坍塌来促进结构多样性。
- en: Theoretical Explanation
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理论解释
- en: 'SAC uses ‘soft’ value functions by introducing the objective function with
    an entropy term, **Η(π(a|s))**. Accordingly, the network seeks to maximise both
    the expected return of lifetime rewards and the entropy of the policy. The entropy
    of the policy is defined as the unpredictability of a random variable, which increases
    with the range of possible values. Thus, the new entropy regularised objective
    becomes:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: SAC通过引入带有熵项的目标函数，使用“软”价值函数，**Η(π(a|s))**。因此，网络试图最大化终身奖励的预期回报和策略的熵。策略的熵被定义为一个随机变量的不可预测性，随着可能值范围的增大而增加。因此，新的熵正则化目标变为：
- en: '![](../Images/57384bbf7ccc74498e77313443dd16e2.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57384bbf7ccc74498e77313443dd16e2.png)'
- en: Entropy Regularised Objective
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 熵正则化目标
- en: '**α** is the temperature parameter that balances between exploration and exploitation.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**α** 是一个温度参数，用于平衡探索与利用之间的关系。'
- en: In the implementation of soft value functions, we aim to maximise the entropy
    as the algorithm would assign equal probabilities to actions that have a similar
    Q-value. Maximising entropy also helps with preventing the agent from choosing
    actions that exploit inconsistencies in approximated Q-values. We can finally
    understand how SAC improves brittleness by allowing the network to explore more
    and not assign very high probabilities to one range of actions. This part is inspired
    by [Vaishak V.Kumar](https://medium.com/u/61d2676ad14?source=post_page---user_mention--8e1a7406ce48--------------------------------)’s
    explanation of the entropy maximisation in “Soft Actor-Critic Demystified”.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在软值函数的实现中，我们的目标是最大化熵，因为算法会将相似 Q 值的动作分配相等的概率。最大化熵还有助于防止智能体选择利用近似 Q 值中的不一致性的动作。最终我们可以理解
    SAC 如何通过允许网络进行更多探索，而不是对一类动作赋予过高的概率，从而改善脆弱性。这部分内容灵感来自 [Vaishak V.Kumar](https://medium.com/u/61d2676ad14?source=post_page---user_mention--8e1a7406ce48--------------------------------)
    在《软演员-评论家解密》一文中对熵最大化的解释。
- en: The SAC paper authors discuss that since the state value function approximates
    the soft value, there is really no essential need to train separate function approximators
    for the policy, since they relate to the state value according to the following
    equation. However, training three separate approximators provided better convergence.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: SAC 论文的作者讨论了，由于状态值函数逼近软值，因此实际上并不需要为策略训练单独的函数逼近器，因为它们根据以下公式与状态值相关联。然而，训练三个独立的逼近器能提供更好的收敛性。
- en: '![](../Images/8bb99c228fae7e880d122184030b8231.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bb99c228fae7e880d122184030b8231.png)'
- en: Soft State Value Function
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 软状态值函数
- en: 'The three function approximator networks are characterised as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 三个函数逼近器网络的特征如下：
- en: '**Policy Network (Actor):** the stochastic policy outputs a set of actions
    sampled from a Gaussian distribution. The policy parameters are learned by minimising
    the Kullback-Leibler Divergence as provided in this equation:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略网络（Actor）：** 随机策略输出一组从高斯分布中采样的动作。策略参数通过最小化 Kullback-Leibler 散度来学习，公式如下：'
- en: '![](../Images/79a5cd826f332f6b5fec21de2400c762.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79a5cd826f332f6b5fec21de2400c762.png)'
- en: Minimising KL-Divergence
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化 KL 散度
- en: The KL-divergence compares the relative entropy or the difference between two
    probability distributions. So, in the equation, we are trying to minimise the
    difference between the distributions of the policy function and the exponentiated
    Q-function normalised by a function Z. Since the target density function is the
    Q-function, which is differentiable, we apply a reparametrisation trick on the
    policy to reduce the estimation of the variance.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: KL 散度比较的是两个概率分布之间的相对熵或差异。因此，在该公式中，我们试图最小化策略函数与经过 Z 函数规范化的指数化 Q 函数之间分布的差异。由于目标密度函数是可微分的
    Q 函数，因此我们对策略应用了重新参数化技巧，以减少方差的估计。
- en: '![](../Images/9b4eb81490bd4799501b1cea4cd7cba4.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b4eb81490bd4799501b1cea4cd7cba4.png)'
- en: Reparametrised Policy
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 重新参数化策略
- en: ϵₜ is a vector sampled from a Gaussian distribution which describes the noise.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ϵₜ 是一个从高斯分布中采样的向量，描述了噪声。
- en: 'The policy objective is then updated to the following expression:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，策略目标被更新为以下表达式：
- en: '![](../Images/61de771ed85419f7cf722c7582dcd7c9.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61de771ed85419f7cf722c7582dcd7c9.png)'
- en: Policy Objective
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 策略目标
- en: 'The policy objective is optimised using the following gradient estimation:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 策略目标通过以下梯度估计来优化：
- en: '![](../Images/3247348d35c518e313d104bcdaef9c3d.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3247348d35c518e313d104bcdaef9c3d.png)'
- en: Policy Gradient Estimator
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度估计器
- en: '**Q-Network (Critic):** includes two Q-value networks to estimate the expected
    reward for the state-action pairs. We minimise the soft Q-function parameters
    by using the soft Bellman residual provided here:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q 网络（Critic）：** 包含两个 Q 值网络，用于估计状态-动作对的期望回报。我们通过使用以下软贝尔曼残差来最小化软 Q 函数的参数：'
- en: '![](../Images/326b8933dff984f1c5106331d3507e28.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/326b8933dff984f1c5106331d3507e28.png)'
- en: Soft Q-function Objective
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 软 Q 函数目标
- en: 'where:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![](../Images/4333f2dc897ce235348f2991d1cb661a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4333f2dc897ce235348f2991d1cb661a.png)'
- en: Immediate Q-value
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 即时 Q 值
- en: 'The soft Q-function objective minimises the square differences between the
    networks Q-value estimation and the immediate Q-value. The immediate Q-value (Q
    hat) is obtained from the reward of the current state-action pair added to the
    discounted expectation of the target value function in the following time stamp.
    Finally, the objective is optimised using a stochastic gradient estimation given
    by the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Soft Q-函数目标最小化网络Q值估计与即时Q值之间的平方差。即时Q值（Q hat）是通过当前状态-动作对的奖励与下一时间戳中目标值函数的折扣期望之和得到的。最终，目标通过以下给定的随机梯度估计进行优化：
- en: '![](../Images/8739f195eadfd70126df462e16384612.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8739f195eadfd70126df462e16384612.png)'
- en: Stochastic Gradient Estimator
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度估计器
- en: '**Target Value Network (Critic):** a separate soft value function which helps
    in stabilising the training process. The soft value function approximator minimises
    the squared residual error as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标值网络（Critic）：** 一个独立的Soft值函数，有助于稳定训练过程。Soft值函数逼近器通过以下方式最小化平方残差误差：'
- en: '![](../Images/084c54806c06bf551bded6f9f54b245a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/084c54806c06bf551bded6f9f54b245a.png)'
- en: Soft Value Function Objective
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Soft值函数目标
- en: 'This soft value function objective minimises the square differences between
    the value function and the expectation of the Q-value plus the entropy of the
    policy function **π**. The negative log part of this objective describes the entropy
    of the policy function. We also know that the information entropy is calculated
    using a negative sign to output a positive entropy value, since the log of a probability
    value (between 0 and 1) will be negative. Similarly, the objective is optimised
    using an unbiased gradient estimator, given in the following expression:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Soft值函数目标最小化值函数与Q值的期望加上策略函数**π**的熵之间的平方差。此目标的负对数部分描述了策略函数的熵。我们还知道，信息熵是通过负号来计算，从而输出正的熵值，因为概率值（介于0和1之间）的对数总是负数。类似地，目标通过以下表达式进行优化，使用无偏的梯度估计器：
- en: '![](../Images/94ecdcd4eacc2213b4ba1a9c2dcc2ef8.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94ecdcd4eacc2213b4ba1a9c2dcc2ef8.png)'
- en: Unbiased Gradient Estimator
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 无偏梯度估计器
- en: Code Implementation
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码实现
- en: 'The code implemented in this article is taken from the following Github repository
    (quantumiracle, 2023):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中实现的代码来自以下GitHub仓库（quantumiracle，2023年）：
- en: '[](https://github.com/quantumiracle/Popular-RL-Algorithms?source=post_page-----8e1a7406ce48--------------------------------)
    [## GitHub - quantumiracle/Popular-RL-Algorithms: PyTorch implementation of Soft
    Actor-Critic (SAC)…'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/quantumiracle/Popular-RL-Algorithms?source=post_page-----8e1a7406ce48--------------------------------)
    [## GitHub - quantumiracle/Popular-RL-Algorithms: PyTorch实现Soft Actor-Critic (SAC)…'
- en: PyTorch implementation of Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3),
    Actor-Critic (AC/A2C), Proximal Policy…
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Soft Actor-Critic (SAC)、Twin Delayed DDPG (TD3)、Actor-Critic (AC/A2C)、Proximal
    Policy的PyTorch实现…
- en: github.com](https://github.com/quantumiracle/Popular-RL-Algorithms?source=post_page-----8e1a7406ce48--------------------------------)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/quantumiracle/Popular-RL-Algorithms?source=post_page-----8e1a7406ce48--------------------------------)
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: SAC relies on environments that use continuous action spaces, so the simulation
    provided uses the robotic arm ‘Reacher’ environment for the most part and the
    Pendulum-v1 environment in the gymnasium package.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: SAC依赖于使用连续动作空间的环境，因此所提供的仿真大多使用了机器人臂‘Reacher’环境，以及Gymnasium包中的Pendulum-v1环境。
- en: 'The Pendulum environment was run on a different repository that implements
    the same algorithm but with less deprecated libraries given by (MrSyee, 2020):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Pendulum环境在另一个仓库中运行，该仓库实现了相同的算法，但使用了较少过时的库（MrSyee，2020年）：
- en: '[](https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file&source=post_page-----8e1a7406ce48--------------------------------)
    [## GitHub - MrSyee/pg-is-all-you-need: Policy Gradient is all you need! A step-by-step
    tutorial for…'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file&source=post_page-----8e1a7406ce48--------------------------------)
    [## GitHub - MrSyee/pg-is-all-you-need: Policy Gradient is all you need! 一个详细的PG方法教程…'
- en: Policy Gradient is all you need! A step-by-step tutorial for well-known PG methods.
    - MrSyee/pg-is-all-you-need
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Policy Gradient就是你所需要的！一个详细的PG方法教程。- MrSyee/pg-is-all-you-need
- en: github.com](https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file&source=post_page-----8e1a7406ce48--------------------------------)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file&source=post_page-----8e1a7406ce48--------------------------------)
- en: 'In terms of the network architectures, as mentioned in the *Theory Explanation,*
    there are three main components:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络架构方面，如*理论解释*中所提到，有三个主要组件：
- en: '**Policy Network:** implements a Gaussian Actor network computing the mean
    and log standard deviation for the action distribution.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略网络：** 实现一个高斯演员网络，计算动作分布的均值和对数标准差。'
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Soft Q-Network:** estimates the expected future reward given from a state-action
    pair for a defined optimal policy.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**软Q网络：** 估计给定状态-动作对下的期望未来回报，对于定义的最优策略而言。'
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Value Network:** estimates the state value.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**价值网络：** 估计状态值。'
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The following snippet offers the key steps in updating the different variables
    corresponding to the SAC algorithm. As it starts by sampling a batch from the
    replay buffer for experience replay. Then, before computing the gradients, they
    are initialised to zero to ensure that gradients from previous batches are not
    accumulated. Then performs backpropagation and updates the weights of the network
    during training. The target and loss values are then updated for the Q-networks.
    These steps take place for all three methods.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了更新与SAC算法相关的不同变量的关键步骤。首先，它从重放缓冲区采样一个批次进行经验回放。然后，在计算梯度之前，先将梯度初始化为零，以确保不会累积来自前一批次的梯度。接着执行反向传播，并在训练过程中更新网络的权重。最后，更新Q网络的目标值和损失值。这些步骤适用于所有三种方法。
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, to run the code in the sac.py file, just run the following commands:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要运行sac.py文件中的代码，只需运行以下命令：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Results and Visualisation
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果与可视化
- en: '![](../Images/77207902ecc4b682d7a475496bd4b932.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77207902ecc4b682d7a475496bd4b932.png)'
- en: Training a ‘Reacher’ Robotic Arm, (generated by the author)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 训练“Reacher”机器人手臂，（由作者生成）
- en: In training the SAC agent in both environments, I noticed that the action space
    of the problem affects the efficiency and the performance of the training. Indeed,
    when I trained the agent on the simple pendulum environment, the learning converged
    much faster and with lower oscillations. However, as the Reacher environment includes
    a more complicated continuous space of actions, the algorithm trained relatively
    well, but the big jump in the rewards was not seen as clearly. The Reacher was
    also trained on 4 times the number of episodes as that of the pendulum.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个环境中训练SAC智能体时，我注意到问题的行动空间会影响训练的效率和表现。事实上，当我在简单的摆动环境中训练智能体时，学习收敛得更快，且波动较小。然而，由于Reacher环境包含了更复杂的连续动作空间，算法虽然训练得相对不错，但奖励的巨大跳跃并没有像预期那样显著。Reacher环境的训练回合数是摆动环境的4倍。
- en: '![](../Images/6196e06d1570d595e76a5cff6916d1bf.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6196e06d1570d595e76a5cff6916d1bf.png)'
- en: Learning Performance by Maximising Reward (generated by the author)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最大化奖励的学习表现（由作者生成）
- en: The action distribution below shows that the policy has a diverse range of actions
    that it explores through the training process until it converges on one optimal
    policy. The hallmark of entropy-regularised algorithms such as SAC comes from
    the increase in exploration. We can also notice that the peaks correspond to action
    values with high expected rewards which drives the policy to converge toward a
    more deterministic behaviour.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的动作分布图显示了策略在训练过程中探索的多样化动作，直到它收敛到一个最优策略。像SAC这样的熵正则化算法的特点来自于探索性的增加。我们还可以注意到，峰值对应于具有高预期回报的动作值，这推动策略收敛到更具确定性的行为。
- en: '![](../Images/1231b79d7ed7b5a2561e80ac55eea9aa.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1231b79d7ed7b5a2561e80ac55eea9aa.png)'
- en: Action Space Usage Distribution (generated by the author)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 行动空间使用分布（由作者生成）
- en: Speaking of a more deterministic behaviour, we observe that the entropy has
    decreased on average over the number of training episodes. However, this behaviour
    is expected, since the sole reason we want to maximise the entropy is to encourage
    more exploration. A higher exploration is mainly done early in the training process
    to exhaust most possible state-actions pairs that have higher returns.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 说到更具确定性的行为，我们观察到，熵在训练回合数上平均有所减少。然而，这种行为是可以预期的，因为我们希望最大化熵的唯一原因是鼓励更多的探索。更高的探索主要发生在训练过程的早期，目的是耗尽大多数可能的状态-动作对，从而获得更高的回报。
- en: '![](../Images/206dded1ba5479ad51d65ff6055f4164.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/206dded1ba5479ad51d65ff6055f4164.png)'
- en: Entropy Valuation Over Training Episodes (generated by the author)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 训练回合的熵评估（由作者生成）
- en: Conclusion
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The SAC algorithm is an off-policy RL framework that adopts a balance of exploitation
    and exploration through a new entropy term. The main objective function of the
    SAC algorithm includes maximising both the expected returns and the entropy during
    the training process, which address many of the issues the legacy frameworks suffer
    from. The use of twin Q-networks and automatic temperature tuning address high
    sample complexity, brittle convergence properties and complex hyperparameter tuning.
    SAC has proven to be highly effective in continuous control task domains. The
    results on action distribution and entropy reveal that the algorithm favours exploration
    in early training phases and diverse action sampling. As the agent trains, it
    converges to a more specific policy which reduces the entropy and reaches optimal
    actions. Consequently, it has been effectively used as an alternative for a wide
    range of domains in bioengineering for robotic control, drug discovery and drug
    delivery. Future implementations should focus on scaling the framework to more
    complex tasks and reducing its computational complexity.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: SAC算法是一种离策略强化学习框架，通过引入新的熵项，平衡了开发和探索。SAC算法的主要目标函数包括最大化期望回报和训练过程中的熵，这解决了传统框架中存在的许多问题。使用双Q网络和自动温度调节来应对高样本复杂性、脆弱的收敛性和复杂的超参数调整问题。SAC在连续控制任务领域中已经证明非常有效。关于动作分布和熵的结果表明，算法在早期训练阶段偏向于探索和多样化的动作采样。随着智能体的训练，它趋向于一个更具体的策略，减少了熵并达到了最优动作。因此，它已被有效地应用于生物工程中的各种领域，如机器人控制、药物发现和药物传递。未来的实现应着重于将该框架扩展到更复杂的任务并降低其计算复杂性。
- en: References
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver,
    D. and Wierstra, D. (2015). Continuous control with deep reinforcement learning.
    [online] arXiv.org. Available at: [https://arxiv.org/abs/1509.02971.](https://arxiv.org/abs/1509.02971.)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver,
    D. 和 Wierstra, D. (2015)。使用深度强化学习进行连续控制。[在线] arXiv.org。可用链接：[https://arxiv.org/abs/1509.02971.](https://arxiv.org/abs/1509.02971.)
- en: 'Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O. (2017).
    Proximal Policy Optimization Algorithms. [online] arXiv.org. Available at: [https://arxiv.org/abs/1707.06347.](https://arxiv.org/abs/1707.06347.)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Schulman, J., Wolski, F., Dhariwal, P., Radford, A. 和 Klimov, O. (2017)。邻近策略优化算法。[在线]
    arXiv.org。可用链接：[https://arxiv.org/abs/1707.06347.](https://arxiv.org/abs/1707.06347.)
- en: 'Haarnoja, T., Zhou, A., Abbeel, P. and Levine, S. (2018). Soft Actor-Critic:
    Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.
    arXiv:1801.01290 [cs, stat]. [online] Available at: [https://arxiv.org/abs/1801.01290.](https://arxiv.org/abs/1801.01290.)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Haarnoja, T., Zhou, A., Abbeel, P. 和 Levine, S. (2018)。软演员-评论家：带有随机演员的离策略最大熵深度强化学习。arXiv:1801.01290
    [cs, stat]。[在线] 可用链接：[https://arxiv.org/abs/1801.01290.](https://arxiv.org/abs/1801.01290.)
- en: Du, H., Li, Z., Niyato, D., Yu, R., Xiong, Z., Xuemin, Shen and Dong In Kim
    (2023). Enabling AI-Generated Content (AIGC) Services in Wireless Edge Networks.
    doi:https://doi.org/10.48550/arxiv.2301.03220.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'Du, H., Li, Z., Niyato, D., Yu, R., Xiong, Z., Xuemin, Shen 和 Dong In Kim (2023)。在无线边缘网络中启用AI生成内容（AIGC）服务。doi:
    https://doi.org/10.48550/arxiv.2301.03220.'
- en: Svensson, H.G., Tyrchan, C., Engkvist, O. and Morteza Haghir Chehreghani (2024).
    Utilizing reinforcement learning for de novo drug design. Machine Learning, 113(7),
    pp.4811–4843\. doi:https://doi.org/10.1007/s10994-024-06519-w.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'Svensson, H.G., Tyrchan, C., Engkvist, O. 和 Morteza Haghir Chehreghani (2024)。利用强化学习进行新药设计。机器学习,
    113(7), 第4811–4843页。doi: https://doi.org/10.1007/s10994-024-06519-w.'
- en: 'quantumiracle (2019). *GitHub — quantumiracle/Popular-RL-Algorithms: PyTorch
    implementation of Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), Actor-Critic
    (AC/A2C), Proximal Policy Optimization (PPO), QT-Opt, PointNet..* [online] GitHub.
    Available at: [https://github.com/quantumiracle/Popular-RL-Algorithms](https://github.com/quantumiracle/Popular-RL-Algorithms)
    [Accessed 12 Dec. 2024].'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 'quantumiracle (2019)。*GitHub — quantumiracle/Popular-RL-Algorithms: PyTorch实现的软演员-评论家（SAC），双延迟DDPG（TD3），演员-评论家（AC/A2C），邻近策略优化（PPO），QT-Opt，PointNet..*
    [在线] GitHub。可用链接：[https://github.com/quantumiracle/Popular-RL-Algorithms](https://github.com/quantumiracle/Popular-RL-Algorithms)
    [访问时间：2024年12月12日]。'
- en: 'MrSyee (2019). *GitHub — MrSyee/pg-is-all-you-need: Policy Gradient is all
    you need! A step-by-step tutorial for well-known PG methods.* [online] GitHub.
    Available at: [https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file.](https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file.)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 'MrSyee (2019). *GitHub — MrSyee/pg-is-all-you-need: 策略梯度就是你所需要的一切！一步一步教你掌握著名的PG方法。*
    [在线] GitHub. 可用链接：[https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file.](https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file.)'
- en: '![](../Images/b852f22c4c64850f1155b42ffaf02eb7.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b852f22c4c64850f1155b42ffaf02eb7.png)'
