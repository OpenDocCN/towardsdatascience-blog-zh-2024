- en: Navigating Soft Actor-Critic Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/navigating-soft-actor-critic-reinforcement-learning-8e1a7406ce48?source=collection_archive---------11-----------------------#2024-12-12](https://towardsdatascience.com/navigating-soft-actor-critic-reinforcement-learning-8e1a7406ce48?source=collection_archive---------11-----------------------#2024-12-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding the theory and implementation of SAC RL in the context of Bioengineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mo.abusadeh?source=post_page---byline--8e1a7406ce48--------------------------------)[![Mohammed
    AbuSadeh](../Images/2a7d5ce6964cdc76d9640b1a17ac707b.png)](https://medium.com/@mo.abusadeh?source=post_page---byline--8e1a7406ce48--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8e1a7406ce48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8e1a7406ce48--------------------------------)
    [Mohammed AbuSadeh](https://medium.com/@mo.abusadeh?source=post_page---byline--8e1a7406ce48--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8e1a7406ce48--------------------------------)
    ·10 min read·Dec 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/117e5ac1c7be96924a7201b5185fd59e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author using ChatGPT-4o
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The research domain of Reinforcement Learning (RL) has evolved greatly over
    the past years. The use of deep reinforcement learning methods such as Proximal
    Policy Optimisation (PPO) (Schulman, 2017) and Deep Deterministic Policy Gradient
    (DDPG) (Lillicrap, 2015) have enabled agents to solve tasks in high-dimensional
    environments. However, many of these model-free RL algorithms have struggled with
    stability during the training process. These challenges arise due to the brittle
    convergence properties, high variance in gradient estimation, very high sample
    complexity, and the sensitivity to hyperparameters in continuous action spaces.
    Given these problems, it is imperative to consider a newly devised RL algorithm
    that avoids such issues and expands applicability to complex, real-world problems.
    This new algorithm is the Soft Actor-Critic (SAC) deep RL network. (Haarnoja,
    2018)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/719d9f7573dce860add1e140432f1fdb.png)'
  prefs: []
  type: TYPE_IMG
- en: Model Architecture of Soft Actor-Critic Networks. Image taken from [(Du, 2023)](https://arxiv.org/abs/2301.03220)
  prefs: []
  type: TYPE_NORMAL
- en: SAC is an off-policy Actor-Critic deep RL algorithm which is designed to address
    the stability and efficiency constraints of its predecessors. The SAC algorithm
    is based on the maximum entropy RL framework which aims for the actor part of
    the network to maximise the expected reward, while maximising entropy. It combines
    off-policy updates with a more stable formulation of the stochastic Actor-Critic
    method. An off-policy algorithm enables faster learning and better sample efficiency
    using experience replay, unlike on-policy methods such as PPO, which require new
    samples for each gradient step. For on-policy methods such as PPO, for each gradient
    step in the learning process, new samples must be collected. The aim of using
    stochastic policies and maximising entropy comes to promote the robustness and
    exploration of the algorithm by encouraging more randomness in the actions. Additionally,
    unlike PPO and DDPG, SAC uses twin Q-networks with a separate Actor network and
    entropy tuning to improve the stability and convergence when combining off-policy
    learning with high dimensional, nonlinear function approximation.
  prefs: []
  type: TYPE_NORMAL
- en: Off-policy RL methods have had a wide impact on bioengineering systems that
    improve patient lives. More specifically, RL has been applied to domains such
    as robotic arm control, drug delivery methods and most notably de novo drug design.
    (Svensson, 2024) Svensson et al. has used a number of on- and off-policy frameworks
    and different variants of replay buffers to learn a RNN-based molecule generation
    policy, to be active against DRD2 (a dopamine receptor). The paper realises that
    using experience replay across the board for high, intermediate and low scoring
    molecules has shown effects in improving the structural diversity and the number
    of active molecules generated. Replay buffers improve sample efficiency in training
    agents. They also reported that the use of off-policy methods and more specifically
    SAC, helps in promoting structural diversity by preventing mode collapse.
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical Explanation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SAC uses ‘soft’ value functions by introducing the objective function with
    an entropy term, **Η(π(a|s))**. Accordingly, the network seeks to maximise both
    the expected return of lifetime rewards and the entropy of the policy. The entropy
    of the policy is defined as the unpredictability of a random variable, which increases
    with the range of possible values. Thus, the new entropy regularised objective
    becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57384bbf7ccc74498e77313443dd16e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Entropy Regularised Objective
  prefs: []
  type: TYPE_NORMAL
- en: '**α** is the temperature parameter that balances between exploration and exploitation.'
  prefs: []
  type: TYPE_NORMAL
- en: In the implementation of soft value functions, we aim to maximise the entropy
    as the algorithm would assign equal probabilities to actions that have a similar
    Q-value. Maximising entropy also helps with preventing the agent from choosing
    actions that exploit inconsistencies in approximated Q-values. We can finally
    understand how SAC improves brittleness by allowing the network to explore more
    and not assign very high probabilities to one range of actions. This part is inspired
    by [Vaishak V.Kumar](https://medium.com/u/61d2676ad14?source=post_page---user_mention--8e1a7406ce48--------------------------------)’s
    explanation of the entropy maximisation in “Soft Actor-Critic Demystified”.
  prefs: []
  type: TYPE_NORMAL
- en: The SAC paper authors discuss that since the state value function approximates
    the soft value, there is really no essential need to train separate function approximators
    for the policy, since they relate to the state value according to the following
    equation. However, training three separate approximators provided better convergence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8bb99c228fae7e880d122184030b8231.png)'
  prefs: []
  type: TYPE_IMG
- en: Soft State Value Function
  prefs: []
  type: TYPE_NORMAL
- en: 'The three function approximator networks are characterised as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Policy Network (Actor):** the stochastic policy outputs a set of actions
    sampled from a Gaussian distribution. The policy parameters are learned by minimising
    the Kullback-Leibler Divergence as provided in this equation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/79a5cd826f332f6b5fec21de2400c762.png)'
  prefs: []
  type: TYPE_IMG
- en: Minimising KL-Divergence
  prefs: []
  type: TYPE_NORMAL
- en: The KL-divergence compares the relative entropy or the difference between two
    probability distributions. So, in the equation, we are trying to minimise the
    difference between the distributions of the policy function and the exponentiated
    Q-function normalised by a function Z. Since the target density function is the
    Q-function, which is differentiable, we apply a reparametrisation trick on the
    policy to reduce the estimation of the variance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b4eb81490bd4799501b1cea4cd7cba4.png)'
  prefs: []
  type: TYPE_IMG
- en: Reparametrised Policy
  prefs: []
  type: TYPE_NORMAL
- en: ϵₜ is a vector sampled from a Gaussian distribution which describes the noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The policy objective is then updated to the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61de771ed85419f7cf722c7582dcd7c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Policy Objective
  prefs: []
  type: TYPE_NORMAL
- en: 'The policy objective is optimised using the following gradient estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3247348d35c518e313d104bcdaef9c3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Policy Gradient Estimator
  prefs: []
  type: TYPE_NORMAL
- en: '**Q-Network (Critic):** includes two Q-value networks to estimate the expected
    reward for the state-action pairs. We minimise the soft Q-function parameters
    by using the soft Bellman residual provided here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/326b8933dff984f1c5106331d3507e28.png)'
  prefs: []
  type: TYPE_IMG
- en: Soft Q-function Objective
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4333f2dc897ce235348f2991d1cb661a.png)'
  prefs: []
  type: TYPE_IMG
- en: Immediate Q-value
  prefs: []
  type: TYPE_NORMAL
- en: 'The soft Q-function objective minimises the square differences between the
    networks Q-value estimation and the immediate Q-value. The immediate Q-value (Q
    hat) is obtained from the reward of the current state-action pair added to the
    discounted expectation of the target value function in the following time stamp.
    Finally, the objective is optimised using a stochastic gradient estimation given
    by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8739f195eadfd70126df462e16384612.png)'
  prefs: []
  type: TYPE_IMG
- en: Stochastic Gradient Estimator
  prefs: []
  type: TYPE_NORMAL
- en: '**Target Value Network (Critic):** a separate soft value function which helps
    in stabilising the training process. The soft value function approximator minimises
    the squared residual error as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/084c54806c06bf551bded6f9f54b245a.png)'
  prefs: []
  type: TYPE_IMG
- en: Soft Value Function Objective
  prefs: []
  type: TYPE_NORMAL
- en: 'This soft value function objective minimises the square differences between
    the value function and the expectation of the Q-value plus the entropy of the
    policy function **π**. The negative log part of this objective describes the entropy
    of the policy function. We also know that the information entropy is calculated
    using a negative sign to output a positive entropy value, since the log of a probability
    value (between 0 and 1) will be negative. Similarly, the objective is optimised
    using an unbiased gradient estimator, given in the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94ecdcd4eacc2213b4ba1a9c2dcc2ef8.png)'
  prefs: []
  type: TYPE_IMG
- en: Unbiased Gradient Estimator
  prefs: []
  type: TYPE_NORMAL
- en: Code Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code implemented in this article is taken from the following Github repository
    (quantumiracle, 2023):'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/quantumiracle/Popular-RL-Algorithms?source=post_page-----8e1a7406ce48--------------------------------)
    [## GitHub - quantumiracle/Popular-RL-Algorithms: PyTorch implementation of Soft
    Actor-Critic (SAC)…'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch implementation of Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3),
    Actor-Critic (AC/A2C), Proximal Policy…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/quantumiracle/Popular-RL-Algorithms?source=post_page-----8e1a7406ce48--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: SAC relies on environments that use continuous action spaces, so the simulation
    provided uses the robotic arm ‘Reacher’ environment for the most part and the
    Pendulum-v1 environment in the gymnasium package.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Pendulum environment was run on a different repository that implements
    the same algorithm but with less deprecated libraries given by (MrSyee, 2020):'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file&source=post_page-----8e1a7406ce48--------------------------------)
    [## GitHub - MrSyee/pg-is-all-you-need: Policy Gradient is all you need! A step-by-step
    tutorial for…'
  prefs: []
  type: TYPE_NORMAL
- en: Policy Gradient is all you need! A step-by-step tutorial for well-known PG methods.
    - MrSyee/pg-is-all-you-need
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file&source=post_page-----8e1a7406ce48--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of the network architectures, as mentioned in the *Theory Explanation,*
    there are three main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Policy Network:** implements a Gaussian Actor network computing the mean
    and log standard deviation for the action distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Soft Q-Network:** estimates the expected future reward given from a state-action
    pair for a defined optimal policy.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Value Network:** estimates the state value.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The following snippet offers the key steps in updating the different variables
    corresponding to the SAC algorithm. As it starts by sampling a batch from the
    replay buffer for experience replay. Then, before computing the gradients, they
    are initialised to zero to ensure that gradients from previous batches are not
    accumulated. Then performs backpropagation and updates the weights of the network
    during training. The target and loss values are then updated for the Q-networks.
    These steps take place for all three methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to run the code in the sac.py file, just run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Results and Visualisation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/77207902ecc4b682d7a475496bd4b932.png)'
  prefs: []
  type: TYPE_IMG
- en: Training a ‘Reacher’ Robotic Arm, (generated by the author)
  prefs: []
  type: TYPE_NORMAL
- en: In training the SAC agent in both environments, I noticed that the action space
    of the problem affects the efficiency and the performance of the training. Indeed,
    when I trained the agent on the simple pendulum environment, the learning converged
    much faster and with lower oscillations. However, as the Reacher environment includes
    a more complicated continuous space of actions, the algorithm trained relatively
    well, but the big jump in the rewards was not seen as clearly. The Reacher was
    also trained on 4 times the number of episodes as that of the pendulum.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6196e06d1570d595e76a5cff6916d1bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Learning Performance by Maximising Reward (generated by the author)
  prefs: []
  type: TYPE_NORMAL
- en: The action distribution below shows that the policy has a diverse range of actions
    that it explores through the training process until it converges on one optimal
    policy. The hallmark of entropy-regularised algorithms such as SAC comes from
    the increase in exploration. We can also notice that the peaks correspond to action
    values with high expected rewards which drives the policy to converge toward a
    more deterministic behaviour.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1231b79d7ed7b5a2561e80ac55eea9aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Action Space Usage Distribution (generated by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of a more deterministic behaviour, we observe that the entropy has
    decreased on average over the number of training episodes. However, this behaviour
    is expected, since the sole reason we want to maximise the entropy is to encourage
    more exploration. A higher exploration is mainly done early in the training process
    to exhaust most possible state-actions pairs that have higher returns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/206dded1ba5479ad51d65ff6055f4164.png)'
  prefs: []
  type: TYPE_IMG
- en: Entropy Valuation Over Training Episodes (generated by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The SAC algorithm is an off-policy RL framework that adopts a balance of exploitation
    and exploration through a new entropy term. The main objective function of the
    SAC algorithm includes maximising both the expected returns and the entropy during
    the training process, which address many of the issues the legacy frameworks suffer
    from. The use of twin Q-networks and automatic temperature tuning address high
    sample complexity, brittle convergence properties and complex hyperparameter tuning.
    SAC has proven to be highly effective in continuous control task domains. The
    results on action distribution and entropy reveal that the algorithm favours exploration
    in early training phases and diverse action sampling. As the agent trains, it
    converges to a more specific policy which reduces the entropy and reaches optimal
    actions. Consequently, it has been effectively used as an alternative for a wide
    range of domains in bioengineering for robotic control, drug discovery and drug
    delivery. Future implementations should focus on scaling the framework to more
    complex tasks and reducing its computational complexity.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver,
    D. and Wierstra, D. (2015). Continuous control with deep reinforcement learning.
    [online] arXiv.org. Available at: [https://arxiv.org/abs/1509.02971.](https://arxiv.org/abs/1509.02971.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O. (2017).
    Proximal Policy Optimization Algorithms. [online] arXiv.org. Available at: [https://arxiv.org/abs/1707.06347.](https://arxiv.org/abs/1707.06347.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Haarnoja, T., Zhou, A., Abbeel, P. and Levine, S. (2018). Soft Actor-Critic:
    Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.
    arXiv:1801.01290 [cs, stat]. [online] Available at: [https://arxiv.org/abs/1801.01290.](https://arxiv.org/abs/1801.01290.)'
  prefs: []
  type: TYPE_NORMAL
- en: Du, H., Li, Z., Niyato, D., Yu, R., Xiong, Z., Xuemin, Shen and Dong In Kim
    (2023). Enabling AI-Generated Content (AIGC) Services in Wireless Edge Networks.
    doi:https://doi.org/10.48550/arxiv.2301.03220.
  prefs: []
  type: TYPE_NORMAL
- en: Svensson, H.G., Tyrchan, C., Engkvist, O. and Morteza Haghir Chehreghani (2024).
    Utilizing reinforcement learning for de novo drug design. Machine Learning, 113(7),
    pp.4811–4843\. doi:https://doi.org/10.1007/s10994-024-06519-w.
  prefs: []
  type: TYPE_NORMAL
- en: 'quantumiracle (2019). *GitHub — quantumiracle/Popular-RL-Algorithms: PyTorch
    implementation of Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), Actor-Critic
    (AC/A2C), Proximal Policy Optimization (PPO), QT-Opt, PointNet..* [online] GitHub.
    Available at: [https://github.com/quantumiracle/Popular-RL-Algorithms](https://github.com/quantumiracle/Popular-RL-Algorithms)
    [Accessed 12 Dec. 2024].'
  prefs: []
  type: TYPE_NORMAL
- en: 'MrSyee (2019). *GitHub — MrSyee/pg-is-all-you-need: Policy Gradient is all
    you need! A step-by-step tutorial for well-known PG methods.* [online] GitHub.
    Available at: [https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file.](https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file.)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b852f22c4c64850f1155b42ffaf02eb7.png)'
  prefs: []
  type: TYPE_IMG
