- en: 'Prompt Caching in LLMs: Intuition'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/prompt-caching-in-llms-intuition-5cfc151c4420?source=collection_archive---------4-----------------------#2024-10-04](https://towardsdatascience.com/prompt-caching-in-llms-intuition-5cfc151c4420?source=collection_archive---------4-----------------------#2024-10-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A brief tour of how caching works in attention-based models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rodrigonader?source=post_page---byline--5cfc151c4420--------------------------------)[![Rodrigo
    Nader](../Images/c1715d46ef7939ff85fc7c908e92b2f1.png)](https://medium.com/@rodrigonader?source=post_page---byline--5cfc151c4420--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5cfc151c4420--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5cfc151c4420--------------------------------)
    [Rodrigo Nader](https://medium.com/@rodrigonader?source=post_page---byline--5cfc151c4420--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5cfc151c4420--------------------------------)
    ·4 min read·Oct 4, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9da5fcfecad7b9e5ffa7ce5a6172ef0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author using ChatGPT
  prefs: []
  type: TYPE_NORMAL
- en: I’ve been exploring articles about how *Prompt Caching* works, and while a few
    blogs touch on its usefulness and how to implement it, I haven’t found much on
    the actual mechanics or the intuition behind it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The question really comes down to this: GPT-like model generation relies on
    the relationships between **every token in a prompt**. *How could caching just
    part of a prompt even make sense?*'
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, it does. Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt caching** has recently emerged as a significant advancement in reducing
    computational overhead, latency, and cost, especially for applications that frequently
    reuse prompt segments.'
  prefs: []
  type: TYPE_NORMAL
- en: To clarify, these are cases where you have a long, static pre-prompt (context)
    and keep adding new user questions to it. Each time the API model is called, it
    needs to completely re-process the **entire prompt.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Google** was the first to introduce [**Context Caching**](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview)
    with the Gemini model, while [**Anthropic**](https://www.anthropic.com/news/prompt-caching)
    and [**OpenAI**](https://platform.openai.com/docs/guides/prompt-caching) have
    recently integrated their prompt caching capabilities, claiming great cost and
    latency reduction for long prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: What is Prompt Caching?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompt caching is a technique that stores parts of a prompt (such as system
    messages, documents, or template text) to be efficiently reused. This avoids reprocessing
    the same prompt structure repeatedly, improving efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple ways to implement Prompt Caching, so the techniques can
    vary by provider, but we’ll try to abstract the concept out of two popular approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**GPTCache: Semantic Cache for LLMs**](https://aclanthology.org/2023.nlposs-1.24.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Prompt Cache: Modular Attention Reuse**](https://arxiv.org/pdf/2311.04934)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The overall process goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: When a prompt comes in, it goes through tokenization, vectorization, and full
    model inference (typically an attention model for LLMs).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The system stores the relevant data (tokens and their embeddings) in a **cache
    layer** outside the model. The numerical vector representation of tokens is stored
    in memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next call, the system checks if a part of the new prompt is already stored
    in the cache (e.g., based on embedding similarity).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upon a cache hit, the cached portion is retrieved, skipping both tokenization
    and full model inference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/e7f90c2a611b4e1abbbfd64dac73e832.png)'
  prefs: []
  type: TYPE_IMG
- en: '[https://aclanthology.org/2023.nlposs-1.24.pdf](https://aclanthology.org/2023.nlposs-1.24.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: So… What Exactly is Cached?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In its most basic form, different levels of caching can be applied depending
    on the approach, ranging from simple to more complex. This can include storing
    tokens, token embeddings, or even internal states to avoid reprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tokens**: The next level involves caching the **tokenized representation**
    of the prompt, avoiding the need to re-tokenize repeated inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Token Encodings**: Caching these allows the model to skip **re-encoding**
    previously seen inputs and only process the *new* parts of the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Internal States**: At the most complex level, caching internal states such
    as key-value pairs (see below) stores **relationships between tokens**, so the
    model only computes *new* relationships.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching Key-Value States
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In transformer models, tokens are processed in pairs: **Keys** and **Values**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Keys** help the model decide how much importance or “attention” each token
    should give to other tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Values** represent the actual content or meaning that the token contributes
    in context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, in the sentence *“Harry Potter is a wizard, and his friend is
    Ron,”* the Key for *“Harry”* is a vector with relationships with each one of the
    other words in the sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '`["Harry", "Potter"], ["Harry"", "a"], ["Harry", "wizard"], etc...`'
  prefs: []
  type: TYPE_NORMAL
- en: How KV Prompt Caching Works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Precompute and Cache KV States**: The model computes and stores KV pairs
    for frequently used prompts, allowing it to skip re-computation and retrieve these
    pairs from the cache for efficiency.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Merging Cached and New Context**: In new prompts, the model retrieves cached
    KV pairs for previously used sentences while computing new KV pairs for any new
    sentences.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Cross-Sentence KV Computation**: The model computes new KV pairs that link
    cached tokens from one sentence to new tokens in another, enabling a holistic
    understanding of their relationships.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/46f1763f48212864ed71f0fe6cb78be1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[https://arxiv.org/abs/2311.04934](https://arxiv.org/abs/2311.04934)'
  prefs: []
  type: TYPE_NORMAL
- en: '**In summary:**'
  prefs: []
  type: TYPE_NORMAL
- en: All of the relationships between tokens of the cached prompt are already computed.
    Only new relationships between NEW-OLD or NEW-NEW tokens must be computed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is This the End of RAG?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As models’ context sizes increase, prompt caching will make a great difference
    by avoiding repetitive processing. As a result, some might lean toward just using
    huge prompts and skipping retrieval processes entirely.
  prefs: []
  type: TYPE_NORMAL
- en: 'But here’s the catch: as contexts grow, models lose focus. Not because models
    will do a bad job but because finding answers in a big chunk of data is a subjective
    task that depends on the use case needs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Systems capable of storing and managing vast volumes of vectors will remain
    essential, and RAG goes beyond caching prompts by offering something critical:
    control.'
  prefs: []
  type: TYPE_NORMAL
- en: With RAG, you can filter and retrieve only the most relevant chunks from your
    data rather than relying on the model to process everything. A modular, separated
    approach ensures less noise, giving you more transparency and precision than full
    context feeding.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, larger context models emerging will probably ask for better storage
    for prompt vectors instead of simple caching. Does that take us back to… vector
    stores?
  prefs: []
  type: TYPE_NORMAL
- en: At Langflow, we’re building the fastest path from RAG prototyping to production.
    It’s open-source and features a free cloud service! Check it out at [https://github.com/langflow-ai/langflow](https://github.com/langflow-ai/langflow)
    ✨
  prefs: []
  type: TYPE_NORMAL
