- en: How to Achieve Near Human-Level Performance in Chunking for RAGs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/agentic-chunking-for-rags-091beccd94b1?source=collection_archive---------0-----------------------#2024-08-26](https://towardsdatascience.com/agentic-chunking-for-rags-091beccd94b1?source=collection_archive---------0-----------------------#2024-08-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The costly yet powerful splitting technique for superior RAG retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thuwarakesh.medium.com/?source=post_page---byline--091beccd94b1--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page---byline--091beccd94b1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--091beccd94b1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--091beccd94b1--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page---byline--091beccd94b1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--091beccd94b1--------------------------------)
    ·8 min read·Aug 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82aba3e7f7d96c5ab42e2324773c1286.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nataliya Vaitkevich](https://www.pexels.com/photo/a-person-cutting-melon-4772957/)
  prefs: []
  type: TYPE_NORMAL
- en: Good chunks make good RAGs.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking, embedding, and indexing are critical aspects of RAGs. A RAG app that
    uses the appropriate chunking technique performs well in terms of output quality
    and speed.
  prefs: []
  type: TYPE_NORMAL
- en: When engineering an LLM pipeline, we use different strategies to split the text.
    Recursive character splitting is the most popular technique. It uses a sliding
    window approach with a fixed token length. However, this approach does not guarantee
    that it can sufficiently hold a theme within its window size. Also, there’s a
    risk that part of the context falls into different chunks.
  prefs: []
  type: TYPE_NORMAL
- en: The other technique I love is semantic splitting. Semantic splitting breaks
    the text whenever there’s a significant change between two consecutive sentences.
    It has no length constraints. So, it can have many sentences or very few. But
    it’s more likely to capture the different themes more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: Even the semantic splitting approach has a problem.
  prefs: []
  type: TYPE_NORMAL
- en: What if sentences far from each other are closer in their meaning?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
