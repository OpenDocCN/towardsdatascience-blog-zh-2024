# Mambaï¼šSSMã€ç†è®ºåŠåœ¨ Keras å’Œ TensorFlow ä¸­çš„å®ç°

> åŸæ–‡ï¼š[`towardsdatascience.com/mamba-ssm-theory-and-implementation-in-keras-and-tensorflow-32d6d4b32546?source=collection_archive---------0-----------------------#2024-03-17`](https://towardsdatascience.com/mamba-ssm-theory-and-implementation-in-keras-and-tensorflow-32d6d4b32546?source=collection_archive---------0-----------------------#2024-03-17)

## äº†è§£ SSM å’Œ Mamba çš„å·¥ä½œåŸç†ï¼Œå¹¶å­¦ä¹ å¦‚ä½•å¼€å§‹åœ¨ Keras å’Œ TensorFlow ä¸­å®ç°å®ƒã€‚

[](https://medium.com/@vedantjumle?source=post_page---byline--32d6d4b32546--------------------------------)![Vedant Jumle](https://medium.com/@vedantjumle?source=post_page---byline--32d6d4b32546--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--32d6d4b32546--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--32d6d4b32546--------------------------------) [Vedant Jumle](https://medium.com/@vedantjumle?source=post_page---byline--32d6d4b32546--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--32d6d4b32546--------------------------------) Â·é˜…è¯»æ—¶é—´ 13 åˆ†é’ŸÂ·2024 å¹´ 3 æœˆ 17 æ—¥

--

![](img/2eef43d26c512f0bea0c67ca675348ef.png)

æ¥æºï¼šAI ç”Ÿæˆï¼ˆSDXLï¼‰

è¿™ç¯‡é¢˜ä¸º[â€œMamba: çº¿æ€§æ—¶é—´åºåˆ—å»ºæ¨¡ä¸é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´â€](https://arxiv.org/abs/2312.00752)çš„è®ºæ–‡äº 2023 å¹´ 12 æœˆ 1 æ—¥æäº¤è‡³ arXivï¼Œæå‡ºäº†ä¸€ç§æœ‰è¶£çš„åºåˆ—å»ºæ¨¡æ–¹æ³•ã€‚ä½œè€…ä»¬â€”â€”[Albert Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+A)ã€[Tri Dao](https://arxiv.org/search/cs?searchtype=author&query=Dao%2C+T)â€”â€”ä»‹ç»äº†â€œMambaâ€æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨â€œé€‰æ‹©æ€§â€[çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰](https://en.wikipedia.org/wiki/State-space_representation)ï¼Œå–å¾—äº†ä¸å½“å‰æ— å¤„ä¸åœ¨çš„ Transformer æ¨¡å‹æ€§èƒ½ç›¸åª²ç¾çš„æˆæœã€‚

# Mamba æœ‰ä»€ä¹ˆç‹¬ç‰¹ä¹‹å¤„ï¼Ÿ

éšç€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ LLaMa-2ã€GPT-4ã€Claudeã€Gemini ç­‰çš„å´›èµ·ï¼ŒTransformer è¿‘å¹´æ¥å˜å¾—éå¸¸æµè¡Œï¼Œä½†å®ƒä¹Ÿé¢ä¸´ä¸Šä¸‹æ–‡çª—å£é—®é¢˜ã€‚Transformer çš„é—®é¢˜æ ¹æºåœ¨äºå…¶æ ¸å¿ƒçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€‚

*å¤šå¤´æ³¨æ„åŠ›çš„ä¸»è¦é—®é¢˜æºäºè¿™æ ·ä¸€ä¸ªäº‹å®ï¼šå¯¹äºè¾“å…¥åºåˆ—é•¿åº¦ nï¼Œæ—¶é—´å¤æ‚åº¦å’Œç©ºé—´å¤æ‚åº¦æŒ‰ O(nÂ²)æ¯”ä¾‹å¢é•¿ã€‚è¿™é™åˆ¶äº† LLM çš„ä¸Šä¸‹æ–‡çª—å£é•¿åº¦ã€‚å› ä¸ºï¼Œè¦å°†å…¶å¢åŠ  10 å€ï¼Œæˆ‘ä»¬éœ€è¦å°†ç¡¬ä»¶è¦æ±‚ï¼ˆå°¤å…¶æ˜¯ GPU VRAMï¼‰å¢åŠ  100 å€ã€‚*

å¦ä¸€æ–¹é¢ï¼ŒMamba çš„æ‰©å±•æŒ‰***O(n)!, å³çº¿æ€§***æ¯”ä¾‹å¢é•¿ã€‚

![](img/b097aae23543115f103cea34510def5e.png)

è¯¥å›¾æ‘˜è‡ª Mamba è®ºæ–‡ï¼Œæ¯”è¾ƒäº† FlashAttention å’Œ Mamba æ–¹æ³•ï¼ˆåœ¨å›¾ä¾‹ä¸­ç”± scan(ours)æ ‡å‡ºï¼‰[1]

è¿™ç§çº¿æ€§ç¼©æ”¾å·²å¼•å‘ç ”ç©¶äººå‘˜çš„çŒœæµ‹ï¼Œå³ Mamba å¯èƒ½æ˜¯åºåˆ—å»ºæ¨¡çš„æœªæ¥ã€‚

# Mamba çš„æ ¸å¿ƒï¼šçŠ¶æ€ç©ºé—´æ¨¡å‹

Mamba æ¨¡å‹çš„æ ¸å¿ƒæ¥è‡ªäºçŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ¦‚å¿µã€‚***çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå¦‚ Transformers å’Œ RNNï¼Œå¤„ç†ä¿¡æ¯åºåˆ—ï¼Œå¦‚æ–‡æœ¬ã€éŸ³é¢‘ä¿¡å·ã€è§†é¢‘å¸§ã€DNA åºåˆ—ç­‰ã€‚***

çŠ¶æ€ç©ºé—´æ¨¡å‹æ¥æºäºå°†ç‰©ç†ç³»ç»Ÿæè¿°ä¸ºä¸€ç»„è¾“å…¥ã€è¾“å‡ºå’Œå˜é‡çš„æ€æƒ³ã€‚è¿™äº›å˜é‡åŒ…æ‹¬ï¼š*Aã€Bã€Cã€D*ã€‚SSM çš„è¿‡ç¨‹æ¶‰åŠè®¡ç®—ç»™å®šè¾“å…¥ x(t)çš„*å†…éƒ¨çŠ¶æ€å‘é‡ h(t)*ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹*h(t)*å’Œ*x(t)*è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå…¶ä¸­æƒé‡ä¸º*Aã€Bã€C å’Œ D*ã€‚åœ¨æœ€ç®€å•çš„å½¢å¼ï¼ˆè¿ç»­æ—¶é—´ä¸å˜ï¼‰ä¸‹ï¼Œè¿‡ç¨‹å…¬å¼å¦‚ä¸‹ï¼š

![](img/ebbbac8cb1eb8abaa8dbb62853c93240.png)![](img/047a1d371405fee92bb7ea7c58305c58.png)

æ¥æºï¼šwikipedia[6]

*h(t)*é€šå¸¸è¢«ç§°ä¸ºâ€œéšè—â€çŠ¶æ€æˆ–â€œæ½œåœ¨â€çŠ¶æ€ï¼Œä¸ºäº†æ›´æ¸…æ™°èµ·è§ï¼Œæˆ‘å°†åšæŒç§°å…¶ä¸ºâ€œéšè—â€çŠ¶æ€ã€‚**é‡è¦çš„æ˜¯è¦æ³¨æ„ï¼ŒAã€Bã€C å’Œ D æ˜¯ SSM ä¸­çš„å­¦ä¹ å‚æ•°ã€‚**

## å˜é‡æ˜¯ä»€ä¹ˆï¼Ÿ

**å˜é‡ Aã€Bã€C å’Œ D æ˜¯å­¦ä¹ å¾—åˆ°çš„å‚æ•°ï¼Œ** å®ƒä»¬å¯ä»¥æè¿°ä¸ºï¼š

+   Aï¼šåœ¨è®¡ç®—æ–°çš„éšè—çŠ¶æ€æ—¶ï¼Œåº”è¯¥è€ƒè™‘å¤šå°‘å‰ä¸€ä¸ªéšè—çŠ¶æ€ï¼ˆhï¼‰ã€‚

+   Bï¼šåœ¨è®¡ç®—æ–°çš„éšè—çŠ¶æ€æ—¶ï¼Œè¾“å…¥ï¼ˆxï¼‰åº”è€ƒè™‘å¤šå°‘ã€‚

+   Cï¼šåœ¨è®¡ç®—è¾“å‡ºï¼ˆyï¼‰æ—¶ï¼Œæ–°çš„éšè—çŠ¶æ€åº”è€ƒè™‘å¤šå°‘ã€‚

+   Dï¼šåœ¨è®¡ç®—è¾“å‡ºï¼ˆyï¼‰æ—¶ï¼Œè¾“å…¥ï¼ˆxï¼‰åº”è€ƒè™‘å¤šå°‘ã€‚

D å‡ºç°åœ¨è®¡ç®—çš„æœ€åï¼Œå¹¶ä¸”ä¸å½±å“å¦‚ä½•è®¡ç®—éšè—çŠ¶æ€ã€‚å› æ­¤ï¼Œé€šå¸¸è®¤ä¸ºå®ƒåœ¨ SSM ä¹‹å¤–ï¼Œå¹¶ä¸”å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªè·³è·ƒè¿æ¥ã€‚

## ä»è¿ç»­ç©ºé—´åˆ°ç¦»æ•£ç©ºé—´çš„è½¬å˜

ä¸Šè¿°å…¬å¼é€‚ç”¨äºè¾“å…¥å’Œè¾“å‡ºå±äºè¿ç»­ç©ºé—´çš„ç³»ç»Ÿã€‚ä½†åœ¨è¯¸å¦‚è¯­è¨€å»ºæ¨¡ç­‰æƒ…å†µä¸­ï¼Œè¾“å…¥å’Œè¾“å‡ºå±äºç¦»æ•£ç©ºé—´ï¼ˆè¯æ±‡è¡¨ä¸­çš„æ ‡è®°å€¼ï¼‰ã€‚æ­¤å¤–ï¼Œæ±‚è§£*h(t)*åœ¨è§£æä¸Šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿™å¯ä»¥é€šè¿‡æ‰§è¡Œ***é›¶é˜¶ä¿æŒ***æ¥å®ç°ã€‚

åœ¨é›¶é˜¶ä¿æŒä¸­ï¼Œæ¯æ¬¡æ¥æ”¶åˆ°è¾“å…¥æ—¶ï¼Œæ¨¡å‹ä¼šä¿æŒå…¶å€¼ç›´åˆ°æ¥æ”¶åˆ°ä¸‹ä¸€ä¸ªè¾“å…¥ã€‚è¿™å¯¼è‡´äº†ä¸€ä¸ªè¿ç»­çš„è¾“å…¥ç©ºé—´ã€‚

![](img/236f0dcd1ee1294b2e8064560a582f29.png)

é›¶é˜¶ä¿æŒå¦‚ä½•å·¥ä½œ

è¿™ä¸ªâ€œä¿æŒâ€é•¿åº¦ç”±ä¸€ä¸ªæ–°çš„å‚æ•°å†³å®šï¼Œç§°ä¸º*æ­¥é•¿* ***âˆ†ã€‚å®ƒå¯ä»¥è¢«è§†ä¸ºè¾“å…¥çš„åˆ†è¾¨ç‡ã€‚*** ç†æƒ³æƒ…å†µä¸‹ï¼Œâˆ†åº”ä¸ºæ— ç©·å°ã€‚

ä»æ•°å­¦ä¸Šè®²ï¼Œé›¶é˜¶ä¿æŒå¯ä»¥æè¿°ä¸ºï¼š

![](img/3f6f0258965a22d4970d1e76a3cf457a.png)

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªç¦»æ•£çš„ SSMï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

![](img/dc4b655375a46584a07aed40155d829d.png)

ç”±äº D ä¸ SSM å¤–çš„è·³è·ƒè¿æ¥ä¸€èµ·ä½¿ç”¨ï¼Œè¾“å‡ºå¯ä»¥ç®€åŒ–ä¸ºï¼š

![](img/de141e5ed6870e82fe9f71fe77beb074.png)![](img/71c4d16a62a57ecd5732def8f4b79143.png)

DX(t) çš„æ¶‰åŠè¢«è§†ä¸ºè·³è·ƒè¿æ¥ï¼Œå› æ­¤å®ƒæ¥è‡ª SSM ä¹‹å¤–ã€‚

# SSM å’Œé€’å½’

åœ¨ SSM ä¸­ï¼Œéšè—çŠ¶æ€ä¼šè¢«ä¼ é€’åˆ°æ¥æ”¶åˆ°ä¸‹ä¸€ä¸ªè¾“å…¥æ—¶ã€‚è¿™ç±»ä¼¼äºé€’å½’ç¥ç»ç½‘ç»œçš„å·¥ä½œæ–¹å¼ã€‚

![](img/96f22333a7a3f5bd2ba29b117eef421f.png)

RNN å’Œ SSM çš„æ¯”è¾ƒ

è¿™ç§ SSM çš„é€’å½’æ ¼å¼å¯ä»¥åƒ RNN ä¸€æ ·å±•å¼€ã€‚ä½†ä¸è¿­ä»£ä¸”ç¼“æ…¢çš„ RNN ä¸åŒï¼ŒSSM å¯ä»¥å¹¶è¡Œå¤„ç†è¾“å…¥åºåˆ—ï¼ˆå°±åƒ Transformersï¼‰ï¼Œè¿™ä½¿å¾—è®­ç»ƒè¿‡ç¨‹æ›´å¿«ã€‚

![](img/22282dee4daeabed27c4669ba7ea486f.png)

SSM çš„å±•å¼€å½¢å¼

> æ³¨æ„ï¼Œâ€˜Dâ€™ åœ¨è·³è·ƒè¿æ¥ä¸­ä½¿ç”¨ï¼Œå®ƒä½äº SSM ä¹‹å¤–ã€‚

SSM åŠ é€Ÿè®­ç»ƒçš„å…³é”®è§è§£æ˜¯ä½¿ç”¨é¢„å…ˆè®¡ç®—çš„å·ç§¯æ ¸ä¸­çš„å˜é‡ *A, B, C*ã€‚[Maarten Grootendorst](https://maartengrootendorst.substack.com/i/141228095/the-convolution-representation) å†™äº†ä¸€ç¯‡éå¸¸å¥½çš„è§£é‡Šï¼Œè®²è¿°äº†å¦‚ä½•æ„é€ è¿™ä¸ªæ ‡å‡†çš„â€˜å·ç§¯â€™æ ¸ã€‚ä½†è¿™é‡Œæœ‰ä¸€ä¸ªç®€å•çš„æ•°å­¦è§£é‡Šã€‚

è€ƒè™‘è¾“å‡º *y.* å¯¹äºåºåˆ—é•¿åº¦ *k*ï¼Œè¾“å‡º *y(k)* å°†è¡¨ç¤ºä¸º***(å‡è®¾ h0 = é›¶)***ï¼š

![](img/16c7dd559c5978d453467fe139274186.png)

ç±»ä¼¼åœ°ï¼Œy3 å¯ä»¥è¡¨ç¤ºä¸ºï¼š

![](img/b90cee1adec4657a3ffa7f67760b3abb.png)

æ¨æ–­å‡ºæ¨¡å¼ï¼Œyk å¯ä»¥è¡¨ç¤ºä¸ºï¼š

![](img/9f7775b6a3bce55150dd393d1d9e44da.png)

è¯¥å…¬å¼å¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–ä¸ºï¼š

![](img/9d85bda2ef8d6da968ed73b8e48ace03.png)

é‚£ä¸ªçœ‹èµ·æ¥å¾ˆå¥‡æ€ªçš„ä¹˜æ³•ç¬¦å·è¡¨ç¤ºå·ç§¯æ“ä½œï¼Œå…¶ä¸­å·ç§¯æ ¸æ˜¯ Kã€‚æ³¨æ„åˆ° K ä¸ *x* æ— å…³ï¼Œå› æ­¤ K å¯ä»¥é¢„å…ˆè®¡ç®—ä¸ºå·ç§¯æ ¸ï¼Œè¿™ä½¿å¾—å¤„ç†è¿‡ç¨‹æ›´å¿«ã€‚

# Mamba å’Œâ€œé€‰æ‹©æ€§â€SSM

å°½ç®¡ SSM çš„è®¡ç®—èƒ½åŠ›å¬èµ·æ¥å¾ˆå¼ºå¤§ï¼Œä½†ä¸ Transformers ç›¸æ¯”ï¼Œå®ƒåœ¨ç²¾åº¦ç­‰åº¦é‡æŒ‡æ ‡ä¸Šå´ç›¸å½“ *ä¸€èˆ¬*ã€‚

![](img/0e7755efdbec8214342ce450c6900185.png)

æ ¸å¿ƒé—®é¢˜å‡ºåœ¨å˜é‡ âˆ†ã€Aã€B å’Œ C ä¸Šã€‚äº‹å®è¯æ˜ï¼Œç”±äºæˆ‘ä»¬å°†ç›¸åŒçš„çŸ©é˜µåº”ç”¨äºæ¯ä¸ªè¾“å…¥ï¼Œå®ƒä»¬å®é™…ä¸Šæ— æ³•å¤„ç†åºåˆ—çš„ä¸Šä¸‹æ–‡ã€‚

> SSM åœ¨å¤„ç†æ•°æ®æ—¶ç¼ºä¹çµæ´»æ€§[4]

é‚£ä¹ˆ Mamba æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ï¼Ÿåœ¨ Mamba ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç§å«åšâ€œé€‰æ‹©æ€§â€SSM çš„è¿‡ç¨‹ï¼Œå…¶ä¸­å˜é‡ âˆ†ã€B å’Œ C æ˜¯æ ¹æ®è¾“å…¥è®¡ç®—å‡ºæ¥çš„ã€‚ğŸ¤”ã€‚æˆ‘ä»¬é€šè¿‡å°†å½“å‰è¾“å…¥ä¼ é€’é€šè¿‡çº¿æ€§å±‚ï¼Œå¹¶å°†è¾“å‡ºä½œä¸º âˆ†ã€B å’Œ Cã€‚

![](img/d789181938ab7269cbd528b6982e3263.png)

ä½†è¿™ä½¿å¾— âˆ†ã€B å’Œ C å˜å¾—ä¾èµ–äºè¾“å…¥ï¼Œå› æ­¤å®ƒä»¬ä¸èƒ½é¢„å…ˆè®¡ç®— ğŸ˜¢ï¼Œå¿«é€Ÿå·ç§¯åœ¨è¿™é‡Œè¡Œä¸é€šã€‚ä½†æ˜¯ï¼Œä½œè€…è®¨è®ºäº†ä¸€ç§åŸºäº *å¹¶è¡Œå…³è”æ‰«æ* çš„æ–¹æ³•ã€‚

## å¹¶è¡Œå…³è”æ‰«æ

å¹¶è¡Œå…³è”æ‰«ææ˜¯ä¸€ç§åœ¨å¹¶è¡Œè®¡ç®—ä¸­ä½¿ç”¨çš„å¼ºå¤§æŠ€æœ¯ï¼Œç”¨äºæ‰§è¡Œå‰ç¼€å’Œæ“ä½œï¼Œè¿™æ˜¯ä¸€ç§å¯¹æ•°å­—åºåˆ—è¿›è¡Œç´¯ç§¯çš„æ“ä½œã€‚è¿™ä¸ªæ“ä½œæ˜¯â€œå…³è”çš„â€ï¼Œæ„å‘³ç€æ•°å­—åœ¨æ“ä½œä¸­çš„åˆ†ç»„æ–¹å¼ä¸ä¼šæ”¹å˜ç»“æœã€‚

![](img/dbbd700bf44fb468c40690bafbc4c22c.png)

å¹¶è¡Œå‰ç¼€å’Œæ˜¯å…³è”æ‰«æçš„ä¸€ä¸ªä¾‹å­ã€‚ï¼ˆæ¥æºï¼šNvidiaï¼‰[7]

åœ¨ Mamba æ¨¡å‹çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œé€šè¿‡å®šä¹‰ä¸€ä¸ªå…³è”æ“ä½œç¬¦ï¼Œå¯ä»¥è·å¾—å¹¶è¡Œå…³è”æ‰«ææ“ä½œçš„å…ƒç´ å’Œå…³è”æ“ä½œç¬¦ã€‚è¿™ä½¿å¾—å¯ä»¥å¹¶è¡Œè§£å†³æ•´ä¸ªæ—¶é—´åŒºé—´çš„é—®é¢˜ï¼Œä»è€Œä½¿å­åŒºé—´çš„æ•°é‡çš„æ—¶é—´å¤æ‚åº¦è¾¾åˆ°å¯¹æ•°çº§åˆ«ã€‚

## ç¡¬ä»¶æ„ŸçŸ¥ç®—æ³•

é™¤äº†å…³è”æ‰«æå¤–ï¼Œä½œè€…è¿˜æå‡ºäº†ä¸€ç§ç¡¬ä»¶æ„ŸçŸ¥ç®—æ³•ï¼Œåœ¨è¯¥ç®—æ³•ä¸­ï¼Œä»–ä»¬åˆ©ç”¨äº†ä¸ Nvidia GPU ä¸­ HBM å’Œ SRAM é€Ÿåº¦ç›¸å…³çš„ç‰¹æ€§ã€‚ä»–ä»¬è®¤ä¸ºï¼Œé€šè¿‡ä»¥ä¸‹æ–¹å¼å¯ä»¥åŠ é€Ÿ SSM çŠ¶æ€çš„è®¡ç®—ï¼š

+   ä¿æŒéšè—çŠ¶æ€å’Œ A åœ¨é€Ÿåº¦è¾ƒå¿«ä½†å®¹é‡è¾ƒå°çš„ ***SRAM*** ä¸­ï¼Œ

+   åœ¨é€Ÿåº¦è¾ƒæ…¢ä½†å®¹é‡è¾ƒå¤§çš„ ***HBM*** ä¸­è®¡ç®— âˆ†ã€B å’Œ Cã€‚

+   ç„¶åï¼Œä»–ä»¬å°† âˆ†ã€B å’Œ C è½¬ç§»åˆ° ***SRAM*** ä¸­ï¼Œåœ¨ ***SRAM*** å†…éƒ¨è®¡ç®—æ–°çš„éšè—çŠ¶æ€ã€‚

+   ç„¶åå°† âˆ†ã€B å’Œ C å†™å›åˆ° ***HBM***ã€‚

![](img/daa8208bf094ec66ab66110f20dd51a1.png)

å›¾ç¤ºæ¥è‡ª Mamba è®ºæ–‡ï¼Œå±•ç¤ºäº†ç¡¬ä»¶æ„ŸçŸ¥ç®—æ³•å¦‚ä½•å·¥ä½œ[1]

> åœ¨å®ç°éƒ¨åˆ†ï¼Œæˆ‘ä¸ä¼šè®¨è®ºå¦‚ä½•ä½¿ç”¨ç¡¬ä»¶æ„ŸçŸ¥ç®—æ³•ï¼Œè€Œæ˜¯åªä¼šä½¿ç”¨å¹¶è¡Œå…³è”æ‰«æã€‚

# æœ€ç»ˆçš„ Mamba æ¶æ„

è€ƒè™‘åˆ°è¿™äº›ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Keras å’Œ TensorFlow æ¢ç´¢å¹¶å®ç° Mamba æ¶æ„ã€‚

åœ¨é˜…è¯»è®ºæ–‡å¹¶åˆ†æä»£ç åï¼ŒMamba æ¶æ„å¯ä»¥åˆ†è§£ä¸ºå‡ ä¸ªå…³é”®ç»„ä»¶ï¼Œè¿™äº›ç»„ä»¶è¿æ¥å¦‚ä¸‹ï¼š

![](img/e663ed4dbf34650f50802a442192d16d.png)

Mamba å—çš„æ‹†è§£

Mamba æ¶æ„ç”±å¤šä¸ªå †å çš„â€œMamba å—â€ç»„æˆã€‚ä»ä¸Šé¢çš„å›¾ç¤ºæ¥çœ‹ï¼Œå®ƒç”±ç›¸å½“å¤šçš„ç»„ä»¶ç»„æˆã€‚å¦ä¸€ä¸ªéœ€è¦æ³¨æ„çš„é‡è¦äº‹é¡¹æ˜¯ï¼Œä½œè€…å°†æ¥è‡ªé€‰æ‹©æ€§ SSM çš„è¾“å‡ºæ·»åŠ åˆ°åŸå§‹è¾“å…¥ä¸­ï¼Œç„¶åå¯¹å…¶åº”ç”¨ *å½’ä¸€åŒ–* å±‚ã€‚æ­¤å½’ä¸€åŒ–å¯ä»¥æ˜¯å±‚å½’ä¸€åŒ–ï¼Œæˆ–è€…æ˜¯ [RMS å½’ä¸€åŒ–](https://arxiv.org/abs/1910.07467)ã€‚

# TensorFlow å’Œ Keras å®ç°

è®©æˆ‘ä»¬ä» Mamba çš„ç¼–ç éƒ¨åˆ†å¼€å§‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹ä¾èµ–é¡¹ï¼š

```py
tensorflow[and-cuda]==2.15.0.post1 # if you want to use GPU or
tensorflow==2.15.0.post1 # if you want to only use CPU
transformers==4.36.2 # for using the bert tokenizer
einops==0.7.0 # useful to make matrix manipulation faster
datasets==2.16.1 # to load datasets
# all other modules (like numpy) will be auto installed
```

å¯¼å…¥ï¼š

```py
import tensorflow_datasets as tfds
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers, Model

from dataclasses import dataclass
from einops import rearrange, repeat
from typing import Union

from transformers import AutoTokenizer

import datasets
import math
import numpy as np
```

ä¸ºäº†ç®€åŒ–æ¨¡å‹å‚æ•°çš„å¤„ç†ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªç®€å•çš„ *ModelArgs* æ•°æ®ç±»ä½œä¸ºé…ç½®ç±»ã€‚è¿™æ ·ï¼Œåœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬åªéœ€å°†æ•°æ®ç±»å˜é‡ä½œä¸ºå‚æ•°ä¼ é€’å³å¯ã€‚

```py
@dataclass
class ModelArgs:
    model_input_dims: int = 64
    model_states: int = 64
    projection_expand_factor: int = 2
    conv_kernel_size: int = 4
    delta_t_min: float = 0.001
    delta_t_max: float = 0.1
    delta_t_scale: float = 0.1
    delta_t_init_floor: float = 1e-4
    conv_use_bias: bool = True
    dense_use_bias: bool = False
    layer_id: int = -1
    seq_length: int = 128
    num_layers: int = 5
    dropout_rate: float = 0.2
    use_lm_head: float = False
    num_classes: int = None
    vocab_size: int = None
    final_activation = None
    loss:Union[str, keras.losses.Loss] = None
    optimizer: Union[str, keras.optimizers.Optimizer] = keras.optimizers.AdamW()
    metrics = ['accuracy']

    def __post_init__(self):
        self.model_internal_dim: int = int(self.projection_expand_factor * self.model_input_dims)

        self.delta_t_rank = math.ceil(self.model_input_dims/16)
        if self.layer_id == -1:
            self.layer_id = np.round(np.random.randint(0, 1000), 4)

        if self.vocab_size == None:
            raise ValueError("vocab size cannot be none")

        if self.use_lm_head:
            self.num_classes=self.vocab_size
        else:
            if self.num_classes == None:
                raise ValueError(f'num classes cannot be {self.num_classes}')

            if self.num_classes == 1:
                self.final_activation = 'sigmoid'
            else:
                self.final_activation = 'softmax'

        if self.loss == None:
            raise ValueError(f"loss cannot be {self.loss}")
```

åŠ è½½ *bert-base-uncased* åˆ†è¯å™¨ï¼š

```py
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
vocab_size = tokenizer.vocab_size
```

åœ¨æˆ‘ä»¬å®ç° Mamba å’Œ SSM ç±»ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å®ç°å¹¶è¡Œå…³è”æ‰«æï¼Œä»£ç å¦‚ä¸‹ï¼š

```py
def selective_scan(u, delta, A, B, C, D):
    # first step of A_bar = exp(Î”A), i.e., Î”A
    dA = tf.einsum('bld,dn->bldn', delta, A) 
    dB_u = tf.einsum('bld,bld,bln->bldn', delta, u, B)

    dA_cumsum = tf.pad(
        dA[:, 1:], [[0, 0], [1, 1], [0, 0], [0, 0]])[:, 1:, :, :]

    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip along axis 1

    # Cumulative sum along all the input tokens, parallel prefix sum, 
    # calculates dA for all the input tokens parallely
    dA_cumsum = tf.math.cumsum(dA_cumsum, axis=1)  

    # second step of A_bar = exp(Î”A), i.e., exp(Î”A)
    dA_cumsum = tf.exp(dA_cumsum)  
    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip back along axis 1

    x = dB_u * dA_cumsum
    # 1e-12 to avoid division by 0
    x = tf.math.cumsum(x, axis=1)/(dA_cumsum + 1e-12) 

    y = tf.einsum('bldn,bln->bld', x, C)

    return y + u * D 
```

é€šè¿‡è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å®ç° MambaBlockï¼š

```py
class MambaBlock(layers.Layer):
    def __init__(self, modelargs: ModelArgs, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.args = modelargs
        args = modelargs
        self.layer_id = modelargs.layer_id

        self.in_projection = layers.Dense(
            args.model_internal_dim * 2, 
            input_shape=(args.model_input_dims,), use_bias=False)

        self.conv1d = layers.Conv1D(
            filters=args.model_internal_dim,
            use_bias=args.conv_use_bias,
            kernel_size=args.conv_kernel_size,
            groups=args.model_internal_dim,
            data_format='channels_first',
            padding='causal'
        )

        # this layer takes in current token 'x' 
        # and outputs the input-specific Î”, B, C (according to S6)
        self.x_projection = layers.Dense(args.delta_t_rank + args.model_states * 2, use_bias=False)

        # this layer projects Î” from delta_t_rank to the mamba internal 
        # dimension
        self.delta_t_projection = layers.Dense(args.model_internal_dim, 
                                               input_shape=(args.delta_t_rank,), use_bias=True)

        self.A = repeat(
                tf.range(1, args.model_states+1, dtype=tf.float32), 
                'n -> d n', d=args.model_internal_dim)

        self.A_log = tf.Variable(
                tf.math.log(self.A), 
                trainable=True, dtype=tf.float32, 
                name=f"SSM_A_log_{args.layer_id}")

        self.D = tf.Variable(
                np.ones(args.model_internal_dim), 
                trainable=True, dtype=tf.float32, 
                name=f"SSM_D_{args.layer_id}")

        self.out_projection = layers.Dense(
                args.model_input_dims, 
                input_shape=(args.model_internal_dim,), 
                use_bias=args.dense_use_bias)

    def call(self, x):
        """Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba pape.
        Official Implementation:
            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119
            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311
        """

        (batch_size, seq_len, dimension) = x.shape

        x_and_res = self.in_projection(x) # shape = (batch, seq_len, 2 * model_internal_dimension)
        (x, res) = tf.split(x_and_res, 
                            [self.args.model_internal_dim, 
                             self.args.model_internal_dim], axis=-1)

        x = rearrange(x, 'b l d_in -> b d_in l')
        x = self.conv1d(x)[:, :, :seq_len]
        x = rearrange(x, 'b d_in l -> b l d_in')

        x = tf.nn.swish(x)
        y = self.ssm(x)
        y = y * tf.nn.swish(res)
        return self.out_projection(y)

    def ssm(self, x):
        """Runs the SSM. See:
            - Algorithm 2 in Section 3.2 in the Mamba paper
            - run_SSM(A, B, C, u) in The Annotated S4
            Official Implementation:
            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311
        """
        (d_in, n) = self.A_log.shape

        # Compute âˆ† A B C D, the state space parameters.
        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 "Interpretation of A" for why A isn't selective)
        #     âˆ†, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,
        #                                  and is why Mamba is called **selective** state spaces)

        A = -tf.exp(tf.cast(self.A_log, tf.float32)) # shape -> (d_in, n)
        D = tf.cast(self.D, tf.float32)

        x_dbl = self.x_projection(x) # shape -> (batch, seq_len, delta_t_rank + 2*n)

        (delta, B, C) = tf.split(
                x_dbl, 
                num_or_size_splits=[self.args.delta_t_rank, n, n], 
                axis=-1) # delta.shape -> (batch, seq_len) & B, C shape -> (batch, seq_len, n)

        delta = tf.nn.softplus(self.delta_t_projection(delta)) # shape -> (batch, seq_len, model_input_dim)

        return selective_scan(x, delta, A, B, C, D)
```

æœ€åï¼Œå®ç°å¤–éƒ¨è·³è·ƒè¿æ¥çš„æ®‹å·®å—ã€‚

```py
class ResidualBlock(layers.Layer):
    def __init__(self, modelargs: ModelArgs, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.args = modelargs
        self.mixer = MambaBlock(modelargs)
        self.norm = layers.LayerNormalization(epsilon=1e-5)

    def call(self, x):
        """
        Official Implementation:
            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297

            Note: the official repo chains residual blocks that look like
                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...
            where the first Add is a no-op. This is purely for performance reasons as this
            allows them to fuse the Add->Norm.

            We instead implement our blocks as the more familiar, simpler, and numerically equivalent
                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....

        """
        return self.mixer(self.norm(x)) + x
```

æœ‰äº†è¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥åˆå§‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Mamba æ¨¡å—åˆ›å»ºä¸€ä¸ªç®€å•çš„åˆ†ç±»æ¨¡å‹ï¼Œä½†å®ƒå¯ä»¥å¾ˆå®¹æ˜“åœ°ä¿®æ”¹ä¸ºè¯­è¨€æ¨¡å‹ã€‚è®©æˆ‘ä»¬åŠ è½½*IMDB è¯„è®ºæ•°æ®é›†*æ¥è¿›è¡Œä¸€ä¸ªç®€å•çš„æƒ…æ„Ÿåˆ†ç±»å™¨ã€‚

```py
from datasets import load_dataset
from tqdm import tqdm

dataset = load_dataset("ajaykarthick/imdb-movie-reviews")
```

é¦–å…ˆæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°å°†æ¥æ”¶æ¨¡å‹å‚æ•°å¹¶è¿”å›ä¸€ä¸ªæ¨¡å‹ã€‚

```py
def init_model(args: ModelArgs):
    input_layer = layers.Input(shape=(args.seq_length,), name='input_ids')
    x = layers.Embedding(
                args.vocab_size, 
                args.model_input_dims, 
                input_length=args.seq_length)(input_layer)

    for i in range(args.num_layers):
        x = ResidualBlock(args, name=f"Residual_{i}")(x)
        x = layers.Dropout(args.dropout_rate)(x) # for regularization

    x = layers.LayerNormalization(epsilon=1e-5)(x) # normalization layer

    # use flatten only if we are not using the model as an LM
    if not args.use_lm_head: 
        x = layers.Flatten()(x)
    x = layers.Dense(1024, activation=tf.nn.gelu)(x)
    output_layer = layers.Dense(
                args.num_classes, 
                activation=args.final_activation)(x)

    model = Model(
                inputs=input_layer, 
                outputs=output_layer, name='Mamba_ka_Mamba')
    model.compile(
        loss=args.loss,
        optimizer=args.optimizer,
        metrics=args.metrics
    )

    return model
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥åˆå§‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæ€»ç»“ï¼š

```py
args = ModelArgs(
    model_input_dims=128,
    model_states=32,
    num_layers=12,
    dropout_rate=0.2,
    vocab_size=vocab_size,
    num_classes=1,
    loss='binary_crossentropy',
)
model = init_model(args)
model.summary()
```

```py
Model: "Mamba_ka_Mamba"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_ids (InputLayer)      [(None, 128)]             0         

 embedding_2 (Embedding)     (None, 128, 128)          3906816   

 Residual_0 (ResidualBlock)  (None, 128, 128)          129024    

 dropout_24 (Dropout)        (None, 128, 128)          0         

 Residual_1 (ResidualBlock)  (None, 128, 128)          129024    

 dropout_25 (Dropout)        (None, 128, 128)          0

 ... (I have shrinked this to make it more readable)

 dropout_35 (Dropout)        (None, 128, 128)          0         

 layer_normalization_38 (La  (None, 128, 128)          256       
 yerNormalization)                                               

 flatten_2 (Flatten)         (None, 16384)             0         

 dense_148 (Dense)           (None, 1024)              16778240  

 dense_149 (Dense)           (None, 1)                 1025      

=================================================================
Total params: 22234625 (84.82 MB)
Trainable params: 22234625 (84.82 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
```

ä¸ºäº†æ›´æ–¹ä¾¿åœ°å¤„ç†ï¼Œè®©æˆ‘ä»¬å…ˆå°†æ•°æ®è¿›è¡Œé¢„å¤„ç†æˆ*numpy æ•°ç»„*ï¼Œç„¶åå†è½¬æ¢ä¸º tf.data.Dataset å¯¹è±¡ï¼š

```py
train_labels, test_labels = [], []
train_ids = np.zeros((len(dataset['train']), args.seq_length))
test_ids = np.zeros((len(dataset['test']), args.seq_length))

for i, item in enumerate(tqdm(dataset['train'])):
    text = item['review']
    train_ids[i, :] = tokenizer.encode_plus(
            text, 
            max_length=args.seq_length, 
            padding='max_length', 
            return_tensors='np')['input_ids'][0][:args.seq_length]

    train_labels.append(item['label'])

for i, item in enumerate(tqdm(dataset['test'])):
    text = item['review']
    test_ids[i, :] = tokenizer.encode_plus(
            text, 
            max_length=args.seq_length, 
            padding='max_length', 
            return_tensors='np')['input_ids'][0][:args.seq_length]

    test_labels.append(item['label'])

del dataset # delete the original dataset to save some memory

BATCH_SIZE = 32
train_dataset = tf.data.Dataset.from_tensor_slices((train_ids, train_labels)).batch(BATCH_SIZE).shuffle(1000)
test_dataset = tf.data.Dataset.from_tensor_slices((test_ids, test_labels)).batch(BATCH_SIZE).shuffle(1000)
```

ç°åœ¨æ¨¡å‹å¯ä»¥è¿›è¡Œè®­ç»ƒï¼š

```py
history = model.fit(train_dataset, validation_data=test_dataset, epochs=10)
```

ä½ å¯ä»¥å°è¯•æ¨ç†ç®—æ³•ï¼š

```py
def infer(text: str, model: Model, tokenizer):
    tokens = tokenizer.encode(
            "Hello what is up", 
            max_length=args.seq_length, 
            padding='max_length', return_tensors='np')
    output = model(tokens)[0, 0]
    return output
```

è¯¥æ¨¡å‹å¯ä»¥è½¬åŒ–ä¸ºè¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨åƒ*æŸæœç´¢ã€top-k é‡‡æ ·ã€è´ªå¿ƒé‡‡æ ·*ç­‰ç®—æ³•æ¥ç”Ÿæˆè¯­è¨€ã€‚

è¿™æ®µä»£ç å¯ä»¥åœ¨æˆ‘çš„[Github](https://github.com/maxDeCoder/Mamba-tf)ä¸Šæ‰¾åˆ°ã€‚

å¾ˆå¤šä»£ç çµæ„Ÿæ¥æºäº mamba çš„å®˜æ–¹å®ç°[2]ä»¥åŠå¦ä¸€ä¸ªåä¸ºâ€˜mamba-tinyâ€™çš„ pytorch å®ç°[3]

æ„Ÿè°¢é˜…è¯»ã€‚

+   é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±æˆ‘åˆ¶ä½œã€‚

å‚è€ƒèµ„æ–™ï¼š

1.  [Mamba è®ºæ–‡](https://arxiv.org/abs/2312.00752)

1.  [Mamba åŸå§‹ä»“åº“](https://github.com/state-spaces/mamba)

1.  [Mamba çš„ä¸€ä¸ªæ›´ç®€å•çš„ Torch å®ç°ï¼šmamba-tiny](https://github.com/PeaBrane/mamba-tiny)

1.  [Letitia åœ¨ YouTube ä¸Šçš„ç®€å•è§£é‡Š](https://youtu.be/vrF3MtGwD0Y?si=st2Oipq3fli9tGhl)

1.  [Maarten Grootendorst å…³äº SSM å’Œ Mamba çš„æ–‡ç« ](https://maartengrootendorst.substack.com/p/a-visual-guide-to-mamba-and-state)

1.  [ç»´åŸºç™¾ç§‘ä¸Šçš„ SSM](https://en.wikipedia.org/wiki/SSM)

1.  [Nvidia å…³äºå¹¶è¡Œå…³è”æ‰«æçš„æ•™ç¨‹](https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda)

æƒ³è¦è”ç³»æˆ‘å—ï¼Ÿè¯·é€šè¿‡ vedantjumle@gmail.com ç»™æˆ‘å†™ä¿¡ã€‚
