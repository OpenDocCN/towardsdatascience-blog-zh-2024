<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Fine-Tune the Audio Spectrogram Transformer with Hugging Face Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Fine-Tune the Audio Spectrogram Transformer with Hugging Face Transformers</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/fine-tune-the-audio-spectrogram-transformer-with-transformers-73333c9ef717?source=collection_archive---------4-----------------------#2024-08-21">https://towardsdatascience.com/fine-tune-the-audio-spectrogram-transformer-with-transformers-73333c9ef717?source=collection_archive---------4-----------------------#2024-08-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="fbe3" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Learn how to fine-tune the Audio Spectrogram Transformer model for audio classification of your own data</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@marius_s?source=post_page---byline--73333c9ef717--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Marius Steger" class="l ep by dd de cx" src="../Images/9dff217a20fc1542eac8e52d32048114.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*GD2dbQS4yWPinTx5j46nPA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--73333c9ef717--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@marius_s?source=post_page---byline--73333c9ef717--------------------------------" rel="noopener follow">Marius Steger</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--73333c9ef717--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Aug 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/7f8ad4be45f2ff60c16110e638acb74c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PTJSzCLXR_QTuFVqRKScVw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Fine-tuning an audio classification model instead of training from scratch can be more data efficient, leading to better results on the downstream task | <em class="nc">Image by author</em></figcaption></figure><p id="74ea" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Audio classification is one of the key tasks in audio understanding with Machine Learning and serves as a building block for many AI systems. It powers industry applications for <a class="af nz" href="https://renumics.com/use-cases/test-data" rel="noopener ugc nofollow" target="_blank">test data evaluation</a> in the engineering domain, error and anomaly detection, or predictive maintenance. Pre-trained transformer models, like the Audio Spectrogram Transformer (AST)[1], provide a powerful foundation for these applications, offering robustness and flexibility.</p><p id="684f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While training an AST model from scratch would require a huge amount of data, using a pretrained model that has already learned audio-specific features can be more efficient. Fine-tuning these models with data specific to our use case is essential to enable their use for our particular application. This process adapts the model‚Äôs capabilities to the unique characteristics of our dataset, such as classes and data distribution, ensuring the relevance of the results.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk oa"><img src="../Images/895d4939ab3d1eb6e2da381a10bcfbe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NUkrXBiI255xZOTWpOe5Aw.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The Audio Spectrogram Transformer predicts a class for an audio sample based on its spectrogram | <em class="nc">Image by author</em></figcaption></figure><p id="91df" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The AST model, integrated with the Hugging Face ü§ó <a class="af nz" href="https://huggingface.co/docs/transformers/index" rel="noopener ugc nofollow" target="_blank">Transformers</a> library, has become a popular choice due to its ease of use and strong performance in audio classification tasks. This guide will take us through the entire process of fine-tuning a pretrained AST model (‚Äú<a class="af nz" href="https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593" rel="noopener ugc nofollow" target="_blank"><em class="ob">MIT/ast-finetuned-audioset-10‚Äì10‚Äì0.4593</em></a><em class="ob">‚Äù</em>) using our own data, demonstrated with the <a class="af nz" href="https://github.com/karolpiczak/ESC-50" rel="noopener ugc nofollow" target="_blank">ESC50 dataset</a>[2]. Using tools from the Hugging Face ecosystem and PyTorch as the backend, we will cover everything from data preparation and preprocessing to model configuration and training.</p><blockquote class="oc"><p id="f257" class="od oe fq bf of og oh oi oj ok ol ny dx">I am writing this guide based on my professional experience with the AST model and the Hugging Face ecosystem over the past years.</p></blockquote></div></div></div><div class="ab cb om on oo op" role="separator"><span class="oq by bm or os ot"/><span class="oq by bm or os ot"/><span class="oq by bm or os"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><blockquote class="ou ov ow"><p id="152c" class="nd ne ob nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This tutorial will guide us through the process of fine-tuning the AST on our own audio classification dataset with tooling from the Hugging Face ecosystem.<br/>We will load the data (1), preprocess the audios (2), setup audio augmentations (3), configure and initialize the AST model (4) and finally, configure and start a training (5).</p></blockquote><h1 id="f380" class="ox oy fq bf oz pa pb gq pc pd pe gt pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Step-by-Step Guide to Fine-Tune the AST</h1><p id="c0c1" class="pw-post-body-paragraph nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny fj bk">Before we start, install all the required packages with pip:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="ff27" class="qc oy fq pz b bg qd qe l qf qg">pip install transformers[torch] datasets[audio] audiomentations</span></pre><h2 id="26bc" class="qh oy fq bf oz qi qj qk pc ql qm qn pf nm qo qp qq nq qr qs qt nu qu qv qw qx bk">1. Load Our Data in the Correct Format</h2><p id="8da6" class="pw-post-body-paragraph nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny fj bk">To start, we‚Äôll use the Hugging Face ü§ó <a class="af nz" href="https://huggingface.co/docs/datasets/index" rel="noopener ugc nofollow" target="_blank">Datasets</a> library to manage our data. This library will assist us in preprocessing, storing, and accessing data during training, as well as performing waveform transformations and encoding into spectrograms on the fly.</p><p id="81c2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Our data should be loaded into a <code class="cx qy qz ra pz b">Dataset</code> object with the following structure:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="b294" class="qc oy fq pz b bg qd qe l qf qg">Dataset({<br/>    features: ['audio', 'labels'],<br/>    num_rows: 1234<br/>})</span></pre><blockquote class="ou ov ow"><p id="5d9b" class="nd ne ob nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the following two sections I will demonstrate how to load a prepared dataset from the ü§ó Hub and also create a <code class="cx qy qz ra pz b"><em class="fq">Dataset</em></code> from local audio data and labels.</p></blockquote></div></div></div><div class="ab cb om on oo op" role="separator"><span class="oq by bm or os ot"/><span class="oq by bm or os ot"/><span class="oq by bm or os"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="dc7f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Loading a Dataset from the Hugging Face Hub: </strong>If we don‚Äôt have an audio dataset locally, we can conveniently load one from the Hugging Face Hub using the <code class="cx qy qz ra pz b">load_dataset</code> function.</p><p id="15c3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this guide we will load the ESC50 Audio Classification dataset for demonstration purposes:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="33e7" class="qc oy fq pz b bg qd qe l qf qg">from datasets import load_dataset<br/><br/>esc50 = load_dataset("ashraq/esc50", split="train")</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rb"><img src="../Images/d311793e7f3a774f4c6cbf257a504f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*GUgRTdFq5BS6S1goETKa3Q.gif"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The spectrograms (top) and waveforms (bottom) of different classes from the ESC50 Dataset | <em class="nc">Image by author (created with </em><a class="af nz" href="https://github.com/Renumics/spotlight" rel="noopener ugc nofollow" target="_blank"><em class="nc">Spotlight</em></a><em class="nc">)</em></figcaption></figure></div></div></div><div class="ab cb om on oo op" role="separator"><span class="oq by bm or os ot"/><span class="oq by bm or os ot"/><span class="oq by bm or os"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4a58" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Loading Local Audio Files and Labels: </strong>We can load our audio files and associated labels into a <code class="cx qy qz ra pz b">Dataset</code> object using a dictionary or a pandas DataFrame that contains file paths and labels. If we have a mapping of class names (strings) to label indices (integers), this information can be included during dataset construction.</p><p id="e0a6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here‚Äôs a practical example:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="4f24" class="qc oy fq pz b bg qd qe l qf qg">from datasets import Dataset, Audio, ClassLabel, Features<br/><br/># Define class labels<br/>class_labels = ClassLabel(names=["bang", "dog_bark"])<br/># Define features with audio and label columns<br/>features = Features({<br/>    "audio": Audio(),  # Define the audio feature<br/>    "labels": class_labels  # Assign the class labels<br/>})<br/># Construct the dataset from a dictionary<br/>dataset = Dataset.from_dict({<br/>    "audio": ["/audio/fold1/7061-6-0-0.wav", "/audio/fold1/7383-3-0-0.wav"],<br/>    "labels": [0, 1],  # Corresponding labels for the audio files<br/>}, features=features)</span></pre><p id="c61b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this example:</p><ul class=""><li id="4087" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny rc rd re bk">The <code class="cx qy qz ra pz b">Audio</code> feature class automatically handles audio file loading and processing.</li><li id="48e5" class="nd ne fq nf b go rf nh ni gr rg nk nl nm rh no np nq ri ns nt nu rj nw nx ny rc rd re bk"><code class="cx qy qz ra pz b">ClassLabel</code> helps manage categorical labels, making it easier to handle classes during training and evaluation.</li></ul><blockquote class="ou ov ow"><p id="f06c" class="nd ne ob nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Note:</strong> For more information on loading audio with Hugging Face, have a look at the Datasets library <a class="af nz" href="https://huggingface.co/docs/datasets/audio_load" rel="noopener ugc nofollow" target="_blank">Docs</a>.</p></blockquote></div></div></div><div class="ab cb om on oo op" role="separator"><span class="oq by bm or os ot"/><span class="oq by bm or os ot"/><span class="oq by bm or os"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f041" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Inspecting the Dataset: </strong>Once the dataset is successfully loaded, each audio sample is accessible via an <code class="cx qy qz ra pz b">Audio</code> feature class, which optimizes data handling by loading it into memory only when needed. This efficient management saves computational resources and speeds up the training process.</p><p id="9348" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To get a better understanding of the data structure and ensure everything is loaded correctly, we can inspect individual samples in the dataset:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="98f7" class="qc oy fq pz b bg qd qe l qf qg">print(dataset[0])</span></pre><p id="a9b2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Output example:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="0d2d" class="qc oy fq pz b bg qd qe l qf qg">{'audio': {'path': '/audio/fold1/7061-6-0-0.wav',<br/>  'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,<br/>         1.52587891e-05, 3.05175781e-05, 0.00000000e+00]),<br/>  'sampling_rate': 44100},<br/> 'labels': 0}</span></pre><p id="e630" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This output shows the path, waveform data array, and the sampling rate for the audio file, along with its corresponding label.</p><blockquote class="oc"><p id="698c" class="od oe fq bf of og oh oi oj ok ol ny dx">For the following steps, you can either use a prepared dataset as demo like we do or continue with your own dataset.</p></blockquote><h2 id="b2c8" class="qh oy fq bf oz qi rk qk pc ql rl qn pf nm rm qp qq nq rn qs qt nu ro qv qw qx bk">2. Preprocess the audio data</h2><p id="fa2a" class="pw-post-body-paragraph nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny fj bk">If our dataset is from the Hugging Face Hub, we cast the <code class="cx qy qz ra pz b"><em class="ob">audio</em></code> and <code class="cx qy qz ra pz b"><em class="ob">labels</em></code> columns to the correct feature types:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="e208" class="qc oy fq pz b bg qd qe l qf qg">import numpy as np<br/>from datasets import Audio, ClassLabel<br/><br/># get target value - class name mappings<br/>df = esc50.select_columns(["target", "category"]).to_pandas()<br/>class_names = df.iloc[np.unique(df["target"], return_index=True)[1]]["category"].to_list()<br/># cast target and audio column<br/>esc50 = esc50.cast_column("target", ClassLabel(names=class_names))<br/>esc50 = esc50.cast_column("audio", Audio(sampling_rate=16000))<br/># rename the target feature<br/>esc50 = esc50.rename_column("target", "labels")<br/>num_labels = len(np.unique(esc50["labels"]))</span></pre><p id="30ac" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this code:</p><ul class=""><li id="3ee9" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny rc rd re bk"><strong class="nf fr">Audio Casting</strong>: The <code class="cx qy qz ra pz b">Audio</code> feature handles loading and processing audio files, resampling them to the desired sampling rate (16kHz in this case, sampling rate of the <code class="cx qy qz ra pz b">ASTFeatureExtractor</code>).</li><li id="b89c" class="nd ne fq nf b go rf nh ni gr rg nk nl nm rh no np nq ri ns nt nu rj nw nx ny rc rd re bk"><strong class="nf fr">ClassLabel Casting</strong>: The <code class="cx qy qz ra pz b">ClassLabel</code> feature maps integers to labels and vice versa.</li></ul></div></div></div><div class="ab cb om on oo op" role="separator"><span class="oq by bm or os ot"/><span class="oq by bm or os ot"/><span class="oq by bm or os"/></div><div class="fj fk fl fm fn"><div class="mr"><div class="ab cb"><div class="lm rp ln rq lo rr cf rs cg rt ci bh"><div class="mm mn mo mp mq ab ke"><figure class="lb mr ru rv rw rx ry paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/5406a4f6a58ec46f62f5875bafeab4cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*Lyxo_3J1h5QjQ4XbzBgHOQ.png"/></div></figure><figure class="lb mr rz rv rw rx ry paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/113a08208bca4a7f8e5b2e3d018054c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*VtHpi9zHtBNrNOzpOT1r6A.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx sa ed sb sc">An audio array as waveform (left) and as spectrogram (right) | <em class="nc">Image by author</em></figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="6dc3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Preparing for AST Model Inputs: </strong>The AST model requires spectrogram inputs, so we need to encode our waveforms into a format that the model can process. This is achieved using the <code class="cx qy qz ra pz b">ASTFeatureExtractor</code>, which is instantiated from the configuration of the pretrained model we intend to fine-tune on our dataset.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="9473" class="qc oy fq pz b bg qd qe l qf qg">from transformers import ASTFeatureExtractor<br/><br/># we define which pretrained model we want to use and instantiate a feature extractor<br/>pretrained_model = "MIT/ast-finetuned-audioset-10-10-0.4593"<br/>feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)<br/># we save model input name and sampling rate for later use<br/>model_input_name = feature_extractor.model_input_names[0]  # key -&gt; 'input_values'<br/>SAMPLING_RATE = feature_extractor.sampling_rate</span></pre><blockquote class="ou ov ow"><p id="d514" class="nd ne ob nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Note:</strong> It is important to set the <strong class="nf fr">mean</strong> and <strong class="nf fr">std</strong> values for <strong class="nf fr">normalization</strong> in the feature extractor to the <strong class="nf fr">values of our dataset</strong>. We can calculate the values using the following block of code:</p></blockquote><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="b931" class="qc oy fq pz b bg qd qe l qf qg"># calculate values for normalization<br/>feature_extractor.do_normalize = False  # we set normalization to False in order to calculate the mean + std of the dataset<br/>mean = []<br/>std = []<br/><br/># we use the transformation w/o augmentation on the training dataset to calculate the mean + std<br/>dataset["train"].set_transform(preprocess_audio, output_all_columns=False)<br/>for i, (audio_input, labels) in enumerate(dataset["train"]):<br/>    cur_mean = torch.mean(dataset["train"][i][audio_input])<br/>    cur_std = torch.std(dataset["train"][i][audio_input])<br/>    mean.append(cur_mean)<br/>    std.append(cur_std)<br/>feature_extractor.mean = np.mean(mean)<br/>feature_extractor.std = np.mean(std)<br/>feature_extractor.do_normalize = True</span></pre></div></div></div><div class="ab cb om on oo op" role="separator"><span class="oq by bm or os ot"/><span class="oq by bm or os ot"/><span class="oq by bm or os"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="1c22" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Applying Transforms for Preprocessing:</strong> We create a function to preprocess the audio data by encoding the audio arrays into the <code class="cx qy qz ra pz b">input_values</code> format expected by the model. This function is set up to be applied dynamically, meaning it processes the data on-the-fly as each sample is loaded from the dataset.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="8d87" class="qc oy fq pz b bg qd qe l qf qg">def preprocess_audio(batch):<br/>    wavs = [audio["array"] for audio in batch["input_values"]]<br/>    # inputs are spectrograms as torch.tensors now<br/>    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors="pt")<br/>    <br/>    output_batch = {model_input_name: inputs.get(model_input_name), "labels": list(batch["labels"])}<br/>    return output_batch<br/><br/># Apply the transformation to the dataset<br/>dataset = dataset.rename_column("audio", "input_values")  # rename audio column<br/>dataset.set_transform(preprocess_audio, output_all_columns=False)</span></pre><p id="142b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Inspecting Transformed Data</strong>: If we load a sample now, it will be transformed on the fly and the encoded audios are yielded as <code class="cx qy qz ra pz b"><em class="ob">input_values</em></code><em class="ob">:</em></p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="2292" class="qc oy fq pz b bg qd qe l qf qg">{'input_values': tensor([[-1.2776, -1.2776, -1.2776,  ..., -1.2776, -1.2776, -1.2776],<br/>         [-1.2776, -1.2776, -1.2776,  ..., -1.2776, -1.2776, -1.2776],<br/>         [-1.2776, -1.2776, -1.2776,  ..., -1.2776, -1.2776, -1.2776],<br/>         ...,<br/>         [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670],<br/>         [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670],<br/>         [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670]]),<br/> 'label': 0}</span></pre><blockquote class="ou ov ow"><p id="d6fe" class="nd ne ob nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Note</strong>: It is crucial to verify that the transformation process maintains data integrity and that the spectrograms are correctly formed to avoid any issues during model training.</p></blockquote></div></div></div><div class="ab cb om on oo op" role="separator"><span class="oq by bm or os ot"/><span class="oq by bm or os ot"/><span class="oq by bm or os"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="3f1e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Splitting the Dataset:</strong> As last data preprocessing step, we split the dataset into a <code class="cx qy qz ra pz b">train</code><em class="ob"> </em>and <code class="cx qy qz ra pz b">test</code>-set<em class="ob"> </em>while utilizing the labels for stratification. This ensures to maintain class distribution across both sets.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="a76f" class="qc oy fq pz b bg qd qe l qf qg"># split training data<br/>if "test" not in dataset:<br/>    dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=0, stratify_by_column="labels")</span></pre><h2 id="112b" class="qh oy fq bf oz qi qj qk pc ql qm qn pf nm qo qp qq nq qr qs qt nu qu qv qw qx bk">3. Add audio augmentations</h2><p id="9c1d" class="pw-post-body-paragraph nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny fj bk">Augmentations play a crucial role in increasing the robustness of machine learning models by introducing variability into the training data. This simulates different recording conditions and helps the model to better generalize to unseen data.</p><p id="e98a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Before diving into the setup, here‚Äôs a visual comparison showing the <strong class="nf fr">original</strong> spectrogram of an audio file and its augmented version using the <strong class="nf fr">AddBackgroundNoise</strong> transformation.</p></div></div><div class="mr"><div class="ab cb"><div class="lm rp ln rq lo rr cf rs cg rt ci bh"><div class="mm mn mo mp mq ab ke"><figure class="lb mr sd rv rw rx ry paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/d3f6385a1580e5879d6a036094c54a5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*GBFkrVOr5eG8A-igbZ_NeA.png"/></div></figure><figure class="lb mr se rv rw rx ry paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/b3fa731cbc376356c42569f444f90b76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*EgPlQ66wUKN4A54SCDKjog.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx sf ed sg sc">The original spectrogram of an audio file (left) and the same audio with the AddBackgroundNoise transformation from <a class="af nz" href="https://github.com/iver56/audiomentations" rel="noopener ugc nofollow" target="_blank">Audiomentations</a> library (right) | <em class="nc">Image by author</em></figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><blockquote class="ou ov ow"><p id="8671" class="nd ne ob nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Note: </strong>Augmentations are a very effective tool for increasing the robustness of training and reducing overfitting in machine learning models.</p><p id="3b69" class="nd ne ob nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">However, it‚Äôs important to <strong class="nf fr">carefully consider the potential impact of each transformation</strong>. For example, adding noise may be appropriate for speech datasets, as it can simulate real-world scenarios where background noise is present. However, for tasks such as sound classification, such enhancements could lead to class confusion, resulting in poor model performance.</p></blockquote><p id="cb70" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Setting Up Audio Augmentations:</strong> To create a set of audio augmentations, we use the <code class="cx qy qz ra pz b">Compose</code> class from the <a class="af nz" href="https://iver56.github.io/audiomentations/" rel="noopener ugc nofollow" target="_blank">Audiomentations</a> library, which allows us to chain multiple augmentations.</p><p id="e964" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here‚Äôs how to set it up:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="9c11" class="qc oy fq pz b bg qd qe l qf qg">from audiomentations import Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift<br/><br/>audio_augmentations = Compose([<br/>    AddGaussianSNR(min_snr_db=10, max_snr_db=20),<br/>    Gain(min_gain_db=-6, max_gain_db=6),<br/>    GainTransition(min_gain_db=-6, max_gain_db=6, min_duration=0.01, max_duration=0.3, duration_unit="fraction"),<br/>    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.5),<br/>    TimeStretch(min_rate=0.8, max_rate=1.2),<br/>    PitchShift(min_semitones=-4, max_semitones=4),<br/>], p=0.8, shuffle=True)</span></pre><p id="e3e8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this setup:</p><ul class=""><li id="79c0" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny rc rd re bk">The <code class="cx qy qz ra pz b">p=0.8</code> parameter specifies that each augmentation in the <code class="cx qy qz ra pz b">Compose</code> sequence has an 80% chance of being applied to any given audio sample. This probabilistic approach ensures variability in the training data, preventing the model from becoming overly dependent on any specific augmentation pattern and improving its ability to generalize.</li><li id="fb36" class="nd ne fq nf b go rf nh ni gr rg nk nl nm rh no np nq ri ns nt nu rj nw nx ny rc rd re bk">The <code class="cx qy qz ra pz b">shuffle=True</code> parameter randomizes the order in which the augmentations are applied, adding another layer of variability.</li></ul><blockquote class="ou ov ow"><p id="10cf" class="nd ne ob nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For a better understanding of these augmentations and detailed configuration options, check out the <a class="af nz" href="https://iver56.github.io/audiomentations/" rel="noopener ugc nofollow" target="_blank"><strong class="nf fr">Audiomentations‚Äô</strong> <strong class="nf fr">docs</strong></a>. Additionally, there‚Äôs a great <a class="af nz" href="https://phrasenmaeher-audio-transformat-visualize-transformation-5s1n4t.streamlit.app/" rel="noopener ugc nofollow" target="_blank">ü§ó <strong class="nf fr">Space</strong></a> where we can experiment with these audio transformations and hear and see their effects on the spectrograms.</p></blockquote></div></div></div><div class="ab cb om on oo op" role="separator"><span class="oq by bm or os ot"/><span class="oq by bm or os ot"/><span class="oq by bm or os"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="a5f6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Integrating Augmentations into the Training Pipeline: </strong>We apply these augmentations during the <code class="cx qy qz ra pz b">preprocess_audio</code> transformation where we also encode the audio data into spectrograms.</p><p id="ef98" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The new preprocessing with augmentation is given by:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="9799" class="qc oy fq pz b bg qd qe l qf qg">def preprocess_audio_with_transforms(batch):<br/>    # we apply augmentations on each waveform<br/>    wavs = [audio_augmentations(audio["array"], sample_rate=SAMPLING_RATE) for audio in batch["input_values"]]<br/>    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors="pt")<br/>    <br/>    output_batch = {model_input_name: inputs.get(model_input_name), "labels": list(batch["labels"])}<br/>    return output_batch<br/><br/># Cast the audio column to the appropriate feature type and rename it<br/>dataset = dataset.cast_column("input_values", Audio(sampling_rate=feature_extractor.sampling_rate))</span></pre><p id="71b4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This function applies the defined augmentations to each waveform and then uses the <code class="cx qy qz ra pz b">ASTFeatureExtractor</code> to encode the augmented waveforms into model inputs.</p></div></div></div><div class="ab cb om on oo op" role="separator"><span class="oq by bm or os ot"/><span class="oq by bm or os ot"/><span class="oq by bm or os"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4239" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Setting Transforms for Training and Validation Splits: </strong>Finally, we set these transformations to be applied during the training and evaluation phases:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="073a" class="qc oy fq pz b bg qd qe l qf qg"># with augmentations on the training set<br/>dataset["train"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)<br/># w/o augmentations on the test set<br/>dataset["test"].set_transform(preprocess_audio, output_all_columns=False)</span></pre><h2 id="5cac" class="qh oy fq bf oz qi qj qk pc ql qm qn pf nm qo qp qq nq qr qs qt nu qu qv qw qx bk">4. Configure and Initialize the AST for Fine-Tuning</h2><p id="bfa8" class="pw-post-body-paragraph nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny fj bk">To adapt the AST model to our specific audio classification task, we will need to adjust the model‚Äôs configuration. This is because our dataset has a different number of classes than the pretrained model, and these classes correspond to different categories. It requires replacing the pretrained classifier head with a new one for our multi-class problem.</p><p id="da13" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The weights for the new classifier head will be randomly initialized, while the rest of the model‚Äôs weights will be loaded from the pretrained version. In this way, we benefit from the learned features of the pretraining and fine-tune on our data.</p><p id="6158" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here‚Äôs how to set up and initialize the AST model with a new classification head:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="d233" class="qc oy fq pz b bg qd qe l qf qg">from transformers import ASTConfig, ASTForAudioClassification<br/><br/># Load configuration from the pretrained model<br/>config = ASTConfig.from_pretrained(pretrained_model)<br/># Update configuration with the number of labels in our dataset<br/>config.num_labels = num_labels<br/>config.label2id = label2id<br/>config.id2label = {v: k for k, v in label2id.items()}<br/># Initialize the model with the updated configuration<br/>model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)<br/>model.init_weights()</span></pre><p id="6297" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Expected Output:</strong> We will see warnings indicating that some weights, especially those in the classifier layers, are being reinitialized:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="095b" class="qc oy fq pz b bg qd qe l qf qg">Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:<br/>- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated<br/>- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated<br/>You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</span></pre><h2 id="7f2d" class="qh oy fq bf oz qi qj qk pc ql qm qn pf nm qo qp qq nq qr qs qt nu qu qv qw qx bk">5. Setup Metrics and Start Training</h2><p id="4264" class="pw-post-body-paragraph nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny fj bk">In the final step we will configure the training process with the ü§ó <a class="af nz" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">Transformers</a> library and use the ü§ó <a class="af nz" href="https://github.com/huggingface/evaluate" rel="noopener ugc nofollow" target="_blank">Evaluate</a> library to define the evaluation metrics to assess the model‚Äôs performance.</p><p id="c5bf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">1. Configure Training Arguments:</strong> The <code class="cx qy qz ra pz b">TrainingArguments</code> class helps set up various parameters for the training process, such as learning rate, batch size, and number of epochs.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="ca14" class="qc oy fq pz b bg qd qe l qf qg">from transformers import TrainingArguments<br/><br/># Configure training run with TrainingArguments class<br/>training_args = TrainingArguments(<br/>    output_dir="./runs/ast_classifier",<br/>    logging_dir="./logs/ast_classifier",<br/>    report_to="tensorboard",<br/>    learning_rate=5e-5,  # Learning rate<br/>    push_to_hub=False,<br/>    num_train_epochs=10,  # Number of epochs<br/>    per_device_train_batch_size=8,  # Batch size per device<br/>    eval_strategy="epoch",  # Evaluation strategy<br/>    save_strategy="epoch",<br/>    eval_steps=1,<br/>    save_steps=1,<br/>    load_best_model_at_end=True,<br/>    metric_for_best_model="accuracy",<br/>    logging_strategy="steps",<br/>    logging_steps=20,<br/>)</span></pre><p id="3e8a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">2. Define Evaluation Metrics:</strong> Define metrics such as accuracy, precision, recall, and F1 score to evaluate the model‚Äôs performance. The <code class="cx qy qz ra pz b">compute_metrics</code> function will handle the calculations during training.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="0201" class="qc oy fq pz b bg qd qe l qf qg">import evaluate<br/>import numpy as np<br/><br/>accuracy = evaluate.load("accuracy")<br/>recall = evaluate.load("recall")<br/>precision = evaluate.load("precision")<br/>f1 = evaluate.load("f1")<br/>AVERAGE = "macro" if config.num_labels &gt; 2 else "binary"<br/><br/>def compute_metrics(eval_pred):<br/>    logits = eval_pred.predictions<br/>    predictions = np.argmax(logits, axis=1)<br/>    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)<br/>    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))<br/>    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))<br/>    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))<br/>    return metrics</span></pre><p id="d27c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">3. Setup the Trainer:</strong> Use the <code class="cx qy qz ra pz b">Trainer</code> class from Hugging Face to handle the training process. This class integrates the model, training arguments, datasets, and metrics.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="3f71" class="qc oy fq pz b bg qd qe l qf qg">from transformers import Trainer<br/><br/># Setup the trainer<br/>trainer = Trainer(<br/>    model=model,<br/>    args=training_args,<br/>    train_dataset=dataset["train"],<br/>    eval_dataset=dataset["test"],<br/>    compute_metrics=compute_metrics,  # Use the metrics function from above<br/>)</span></pre><p id="8a5f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">With everything configured, we initiate the training process:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="fa07" class="qc oy fq pz b bg qd qe l qf qg">trainer.train()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk sh"><img src="../Images/e14d2383267ae80949cd038f1e0f21b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*W80qzPQTBZAcSgKUW4UuUg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Example log of a training with audio-augmentations applied to the train-split | <em class="nc">Image by author</em></figcaption></figure><h1 id="13b2" class="ox oy fq bf oz pa pb gq pc pd pe gt pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">(Not so optional:) Evaluate The Results</h1><p id="da8e" class="pw-post-body-paragraph nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny fj bk">To understand our model‚Äôs performance and find potential areas for improvement, it is essential to evaluate its predictions on train and test data. During training, metrics such as accuracy, precision, recall, and F1 score are logged to <a class="af nz" href="https://www.tensorflow.org/tensorboard" rel="noopener ugc nofollow" target="_blank">TensorBoard</a>, which allows us to inspect the model‚Äôs progress and performance over time.</p><p id="e725" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Starting TensorBoard</strong>: To visualize these metrics, initiate TensorBoard by running the following command in your terminal:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="5d7e" class="qc oy fq pz b bg qd qe l qf qg">tensorboard --logdir="./logs"</span></pre><p id="4a66" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This provides a graphical representation of the model‚Äôs learning curve and metric improvements over time, helping to identify potential overfitting or underperformance early in the training process.</p></div></div></div><div class="ab cb om on oo op" role="separator"><span class="oq by bm or os ot"/><span class="oq by bm or os ot"/><span class="oq by bm or os"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="d1cc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For <strong class="nf fr">more detailed insights</strong>, we can inspect the model‚Äôs predictions using <a class="af nz" href="https://renumics.com/" rel="noopener ugc nofollow" target="_blank">Renumics</a>‚Äô open-source tool, <a class="af nz" href="https://renumics.com/open-source/spotlight" rel="noopener ugc nofollow" target="_blank"><strong class="nf fr">Spotlight</strong></a>. Spotlight enables us to explore and visualize the predictions alongside the data, helping us to identify patterns, potential biases, and miss-classifications on the level of single data points.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk si"><img src="../Images/07e84100feee6c38d3dbde8ed89c2a7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qbdyCW4VE-ikzU_Q1WsnBQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The ESC50 dataset with audio embeddings and model predictions loaded in Spotlight. Try it yourself in this Hugging Face <a class="af nz" href="https://huggingface.co/spaces/renumics/spotlight-esc50-clap" rel="noopener ugc nofollow" target="_blank">Space</a> | <em class="nc">Image by author</em></figcaption></figure><p id="fd1b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Installing and Using Spotlight</strong>:</p><p id="b4af" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To get started with Spotlight, install it using pip and load your dataset for exploration:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="64b7" class="qc oy fq pz b bg qd qe l qf qg">pip install renumics-spotlight</span></pre><p id="a172" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And load the ESC50 dataset for <strong class="nf fr">interactive exploration</strong> with one line of code:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="a5bc" class="qc oy fq pz b bg qd qe l qf qg">from renumics import spotlight<br/><br/>spotlight.show(esc50, dtype={"audio": spotlight.Audio})</span></pre><blockquote class="ou ov ow"><p id="7452" class="nd ne ob nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This tutorial focuses on setting up the fine-tuning pipeline. For a comprehensive <strong class="nf fr">evaluation</strong>, including <strong class="nf fr">using Spotlight</strong>, please refer to the other tutorials and resources provided below and at the end of this guide (Useful Links).</p></blockquote><p id="9832" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here are some examples of how to use Spotlight for model evaluation:</p><ol class=""><li id="efba" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny sj rd re bk">A blog post with demo on <strong class="nf fr">Hands-On Voice Analytics with Transformers: </strong><a class="af nz" href="https://renumics.com/blog/voice-analytics-with-transformers" rel="noopener ugc nofollow" target="_blank">Blog</a> &amp; ü§ó <a class="af nz" href="https://huggingface.co/spaces/renumics/emodb-model-comparison" rel="noopener ugc nofollow" target="_blank">Space</a></li><li id="3639" class="nd ne fq nf b go rf nh ni gr rg nk nl nm rh no np nq ri ns nt nu rj nw nx ny sj rd re bk">A blog post and short example on <strong class="nf fr">Fine-tuning image classification models from image search:</strong> <a class="af nz" href="https://itnext.io/image-classification-in-2023-8ab7dc552115" rel="noopener ugc nofollow" target="_blank">Blog</a> &amp; <a class="af nz" href="https://renumics.com/next/docs/use-cases/image-fine-tuning" rel="noopener ugc nofollow" target="_blank">Use Case</a></li><li id="a73c" class="nd ne fq nf b go rf nh ni gr rg nk nl nm rh no np nq ri ns nt nu rj nw nx ny sj rd re bk">A blog post and short example on <strong class="nf fr">How to Automatically Find and Remove Issues in Your Image, Audio, and Text Classification Datasets</strong>: <a class="af nz" href="https://medium.com/@daniel-klitzke/finding-problematic-data-slices-in-unstructured-data-aeec0a3b9a2a" rel="noopener">Blog</a> &amp; <a class="af nz" href="https://renumics.com/next/docs/use-cases/audio-classification" rel="noopener ugc nofollow" target="_blank">Use Case</a></li></ol></div></div></div><div class="ab cb om on oo op" role="separator"><span class="oq by bm or os ot"/><span class="oq by bm or os ot"/><span class="oq by bm or os"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5510" class="ox oy fq bf oz pa sk gq pc pd sl gt pf pg sm pi pj pk sn pm pn po so pq pr ps bk">Conclusion</h1><p id="77e8" class="pw-post-body-paragraph nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny fj bk">By following the steps outlined in this guide, we‚Äôll be able to fine-tune the Audio Spectrogram Transformer (AST) on any audio classification dataset. This includes setting up data preprocessing, applying effective audio augmentations, and configuring the model for the specific task. After training, we can evaluate the model‚Äôs performance using the defined metrics, ensuring it meets our requirements. Once the model is fine-tuned and validated, it can be used for inference.</p><h2 id="8c60" class="qh oy fq bf oz qi qj qk pc ql qm qn pf nm qo qp qq nq qr qs qt nu qu qv qw qx bk">More on the Topic</h2><p id="f967" class="pw-post-body-paragraph nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny fj bk">This is the second in a <strong class="nf fr">series of tutorials and blog posts</strong> on the Audio Spectrogram Transformer for industrial audio classification use cases.</p><ul class=""><li id="046b" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny rc rd re bk">Have a look at <strong class="nf fr">part one</strong>: <a class="af nz" href="https://medium.com/itnext/how-to-use-ssast-model-weigths-in-the-huggingface-ecosystem-0f3fdc8d38da" rel="noopener"><em class="ob">How to Use SSAST Model Weigths in the HuggingFace Ecosystem?</em></a>,</li><li id="b871" class="nd ne fq nf b go rf nh ni gr rg nk nl nm rh no np nq ri ns nt nu rj nw nx ny rc rd re bk">and watch <a class="af nz" href="https://medium.com/@marius_s/list/audio-classification-for-industry-use-cases-cb6d169a7d80" rel="noopener">this list</a> for upcoming posts.</li></ul><p id="2d4a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Stay tuned for further posts in this series, where we will examine specific challenges from real use cases and how to adapt the AST to handle them.</p></div></div></div><div class="ab cb om on oo op" role="separator"><span class="oq by bm or os ot"/><span class="oq by bm or os ot"/><span class="oq by bm or os"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="1e75" class="qh oy fq bf oz qi qj qk pc ql qm qn pf nm qo qp qq nq qr qs qt nu qu qv qw qx bk">Useful Links</h2><ol class=""><li id="0b9a" class="nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny sj rd re bk"><strong class="nf fr">Download</strong> <strong class="nf fr">this guide </strong>as<strong class="nf fr"> </strong>notebook from the Renumics <a class="af nz" href="https://renumics.com/open-source/resources" rel="noopener ugc nofollow" target="_blank"><strong class="nf fr">Resource Page</strong></a><strong class="nf fr">.</strong></li><li id="4fb5" class="nd ne fq nf b go rf nh ni gr rg nk nl nm rh no np nq ri ns nt nu rj nw nx ny sj rd re bk">A tutorial on how to use<strong class="nf fr"> Spotlight for audio model evaluation: <br/></strong><a class="af nz" href="https://renumics.com/blog/voice-analytics-with-transformers" rel="noopener ugc nofollow" target="_blank"><strong class="nf fr">Blog</strong></a> &amp; <strong class="nf fr">ü§ó </strong><a class="af nz" href="https://huggingface.co/spaces/renumics/emodb-model-comparison" rel="noopener ugc nofollow" target="_blank"><strong class="nf fr">Space</strong></a> (Demo)</li><li id="a12b" class="nd ne fq nf b go rf nh ni gr rg nk nl nm rh no np nq ri ns nt nu rj nw nx ny sj rd re bk">A tutorial on how to <strong class="nf fr">train an acoustic event detection</strong> <strong class="nf fr">system</strong> with Spotlight: <a class="af nz" href="https://renumics.com/blog/acoustic-event-detection-annotation" rel="noopener ugc nofollow" target="_blank"><strong class="nf fr">Blog</strong></a></li><li id="4f6b" class="nd ne fq nf b go rf nh ni gr rg nk nl nm rh no np nq ri ns nt nu rj nw nx ny sj rd re bk">The <strong class="nf fr">official ü§ó audio course</strong>: <a class="af nz" href="https://huggingface.co/learn/audio-course/chapter0/introduction" rel="noopener ugc nofollow" target="_blank"><strong class="nf fr">Introduction</strong></a> &amp; <a class="af nz" href="https://huggingface.co/learn/audio-course/chapter4/fine-tuning" rel="noopener ugc nofollow" target="_blank"><strong class="nf fr">Fine-Tuning</strong></a></li></ol></div></div></div><div class="ab cb om on oo op" role="separator"><span class="oq by bm or os ot"/><span class="oq by bm or os ot"/><span class="oq by bm or os"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="d2bb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Thanks for reading! My name is <a class="af nz" href="https://www.linkedin.com/in/marius-steger/" rel="noopener ugc nofollow" target="_blank">Marius Steger</a>, I‚Äôm a Machine Learning Engineer @<a class="af nz" href="https://renumics.com/" rel="noopener ugc nofollow" target="_blank">Renumics</a> ‚Äî We have developed <a class="af nz" href="https://github.com/Renumics/spotlight" rel="noopener ugc nofollow" target="_blank">Spotlight</a>, an Open Source tool that takes your data-centric AI workflow to the next level.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk sp"><img src="../Images/cc655344170c51a9f0d18fa1a30b1164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*m4ZZDhXesXE6bymkDUPVFw.png"/></div></figure></div></div></div><div class="ab cb om on oo op" role="separator"><span class="oq by bm or os ot"/><span class="oq by bm or os ot"/><span class="oq by bm or os"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="49e4" class="qh oy fq bf oz qi qj qk pc ql qm qn pf nm qo qp qq nq qr qs qt nu qu qv qw qx bk">References</h2><p id="44fd" class="pw-post-body-paragraph nd ne fq nf b go pt nh ni gr pu nk nl nm pv no np nq pw ns nt nu px nw nx ny fj bk">[1] Yuan Gong, Yu-An Chung, James Glass: <a class="af nz" href="https://arxiv.org/abs/2104.01778" rel="noopener ugc nofollow" target="_blank">AST: Audio Spectrogram Transformer</a> (2021), arxiv</p><p id="df06" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[2] Piczak, Karol J.: <a class="af nz" href="https://dl.acm.org/doi/10.1145/2733373.2806390" rel="noopener ugc nofollow" target="_blank">ESC: Dataset for Environmental Sound Classification</a> (2015), ACM Press</p></div></div></div></div>    
</body>
</html>