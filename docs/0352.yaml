- en: Set up a local LLM on CPU with chat UI in 15 minutes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在15分钟内设置一个基于CPU的本地LLM和聊天UI
- en: 原文：[https://towardsdatascience.com/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=collection_archive---------1-----------------------#2024-02-06](https://towardsdatascience.com/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=collection_archive---------1-----------------------#2024-02-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=collection_archive---------1-----------------------#2024-02-06](https://towardsdatascience.com/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=collection_archive---------1-----------------------#2024-02-06)
- en: This blog post shows how to easily run an LLM locally and how to set up a ChatGPT-like
    GUI in 4 easy steps.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这篇博客文章展示了如何轻松地在本地运行一个大语言模型（LLM），并在4个简单步骤中设置一个类似ChatGPT的图形用户界面。
- en: '[](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--4cdc741408df--------------------------------)[![Kasper
    Groes Albin Ludvigsen](../Images/3c31c9e54fae4fd1c8f1c441379d1f10.png)](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--4cdc741408df--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4cdc741408df--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4cdc741408df--------------------------------)
    [Kasper Groes Albin Ludvigsen](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--4cdc741408df--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--4cdc741408df--------------------------------)[![Kasper
    Groes Albin Ludvigsen](../Images/3c31c9e54fae4fd1c8f1c441379d1f10.png)](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--4cdc741408df--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4cdc741408df--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4cdc741408df--------------------------------)
    [Kasper Groes Albin Ludvigsen](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--4cdc741408df--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4cdc741408df--------------------------------)
    ·5 min read·Feb 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4cdc741408df--------------------------------)
    ·阅读时间5分钟·2024年2月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4af6944f035f6558d066e724d2f69d26.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4af6944f035f6558d066e724d2f69d26.png)'
- en: Photo by Liudmila Shuvalova on Unsplash
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Liudmila Shuvalova，来自Unsplash
- en: Thanks to the global open source community, it is now easier than ever to run
    performant large language models (LLM) on consumer laptops or CPU-based servers
    and easily interact with them through well-designed graphical user interfaces.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢全球开源社区，现在在消费者笔记本电脑或基于CPU的服务器上运行高效的大型语言模型（LLM）并通过精心设计的图形用户界面与之交互比以往任何时候都更容易。
- en: This is particularly valuable to all the organizations who are not allowed or
    not willing to use services that requires sending data to a third party.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这对所有不允许或不愿使用需要将数据发送给第三方的服务的组织特别有价值。
- en: This tutorial shows how to set up a local LLM with a neat ChatGPT-like UI in
    four easy steps. If you have the prerequisite software installed, it will take
    you no more than 15 minutes of work (excluding the computer processing time used
    in some of the steps).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程展示了如何在四个简单步骤中设置一个本地LLM，并配有一个简洁的ChatGPT样式的UI。如果你已经安装了必要的软件，整个过程不超过15分钟（不包括某些步骤中计算机处理时间）。
- en: 'This tutorial assumes you have the following installed on your machine:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程假设你已经在机器上安装了以下软件：
- en: '[Ollama](https://ollama.ai/)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ollama](https://ollama.ai/)'
- en: Docker
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker
- en: React
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: React
- en: Python and common packages including transformers
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python和常用包，包括transformers
- en: Now let’s get going.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始吧。
- en: Step 1 — Decide which Huggingface LLM to use
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步 — 决定使用哪个Huggingface LLM
- en: The first step is to decide what LLM you want to run locally. Maybe you already
    have an idea. Otherwise, for English, the [instruct version](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)
    of Mistral 7b seems to be the go-to choice. For Danish, I recommend Munin-NeuralBeagle
    although its known to over-generate tokens (perhaps because it’s a merge of a
    model that was not instruction fine tuned). For other Scandinavian languages,
    see [ScandEval’s](https://scandeval.com/) evaluation of Scandinavian generative
    models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是决定你想在本地运行哪个LLM。也许你已经有了想法。如果没有，对于英语，Mistral 7b的[指令版本](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)似乎是首选。对于丹麦语，我推荐Munin-NeuralBeagle，尽管它在生成令牌时可能过度（可能是因为它合并了一个未经过指令微调的模型）。对于其他斯堪的纳维亚语言，请参见[ScandEval](https://scandeval.com/)对斯堪的纳维亚生成模型的评估。
- en: Once you’ve decided which LLM to use, copy the Huggingface “path” to the model.
    For Mistral 7b it would be “mistralai/Mistral-7B-v0.1". You’ll need it in the
    next step.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你决定了使用哪个LLM，复制Huggingface“路径”到该模型。对于Mistral 7b，它的路径是“mistralai/Mistral-7B-v0.1”。你将在下一步中用到它。
- en: '[](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----4cdc741408df--------------------------------)
    [## How to make a PyTorch Transformer for time series forecasting'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----4cdc741408df--------------------------------)
    [## 如何制作一个用于时间序列预测的PyTorch Transformer'
- en: This post will show you how to transform a time series Transformer architecture
    diagram into PyTorch code step by step.
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本文将向你展示如何一步步地将时间序列Transformer架构图转换为PyTorch代码。
- en: towardsdatascience.com](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----4cdc741408df--------------------------------)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----4cdc741408df--------------------------------)'
- en: Step 2 – Quantize the LLM
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤2 – 对LLM进行量化
- en: Next step is to quantize your chosen model unless you selected a model that
    was already quantized. If your model’s name ends in GGUF or GPTQ it’s already
    quantized.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是对你选择的模型进行量化，除非你选择了一个已经量化的模型。如果你的模型名称以GGUF或GPTQ结尾，它已经是量化过的。
- en: Quantization is a technique that converts the weights of a model (its learned
    parameters) to a smaller data type than the original, eg from fp16 to int4\. This
    makes the model take up less memory and also makes it faster to run inference
    which is a nice feature if you’re running on CPU.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种技术，它将模型的权重（即它的学习参数）转换为比原始数据类型更小的类型，例如从fp16转换为int4。这使得模型占用更少的内存，同时也加快了推理速度，如果你在CPU上运行，这是一项很有用的功能。
- en: The script `quantize.py`in my repo [local_llm](https://github.com/KasperGroesLudvigsen/local_llm)
    is adapated from [Maxime Labonne’s fantastic Colab notebook](https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu?usp=sharing)
    (see his [LLM course](https://github.com/mlabonne/llm-course) for other great
    LLM resources). You can use his notebook or my script. The method’s been tested
    on Mistral and Mistral-like models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我在我的仓库[local_llm](https://github.com/KasperGroesLudvigsen/local_llm)中的脚本`quantize.py`改编自[Maxime
    Labonne的精彩Colab笔记本](https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu?usp=sharing)（可以查看他的[LLM课程](https://github.com/mlabonne/llm-course)获取更多LLM资源）。你可以使用他的笔记本或者我的脚本。这个方法已在Mistral和类似Mistral的模型上进行过测试。
- en: 'To quantize, first clone my repo:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行量化，首先克隆我的仓库：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, change the `MODEL_ID` variable in the `quantize.py` file to reflect your
    model of choice. This is where you need the Huggingface “path” that you copied
    in the first step. So if you wanna use Mistral 7b:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在`quantize.py`文件中更改`MODEL_ID`变量，以反映你选择的模型。这就是你需要在第一步中复制的Huggingface“路径”。所以如果你想使用Mistral
    7b：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, in your terminal, run the script:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在终端中运行脚本：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will take some time. While the quantization process runs, you can proceed
    to the next step.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要一些时间。在量化过程运行时，你可以继续进行下一步。
- en: The script will produce a directory that contains the model files for the model
    you selected as well as the quantized version of the model which has the file
    extension “.gguf”.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本将生成一个目录，其中包含你选择的模型的模型文件以及量化版本的模型，量化版本的文件扩展名为“.gguf”。
- en: '[](/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----4cdc741408df--------------------------------)
    [## ChatGPT’s energy use per query'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----4cdc741408df--------------------------------)
    [## ChatGPT每次查询的能源使用'
- en: How much electricity does ChatGPT use to answer one question?
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ChatGPT回答一个问题需要多少电量？
- en: towardsdatascience.com](/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----4cdc741408df--------------------------------)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----4cdc741408df--------------------------------)'
- en: 'Step 3: Build and run Ollama version of model'
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤3：构建并运行Ollama版本的模型
- en: We will run the model with [Ollama](https://ollama.ai/). Ollama is a software
    framework that neatly wraps a model into an API. Ollama also integrates easily
    with various front ends as we’ll see in the next step.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[Ollama](https://ollama.ai/)运行该模型。Ollama是一个软件框架，它将模型封装成一个API。Ollama还可以轻松与各种前端集成，正如我们将在下一步中看到的那样。
- en: To build an Ollama image of the model, you need a so-called model file which
    is a plain text file that configures the Ollama image. If you’re acquainted with
    Dockerfiles, Ollama’s model files will look familiar.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建模型的Ollama镜像，你需要一个所谓的模型文件，这是一个配置Ollama镜像的纯文本文件。如果你熟悉Dockerfile，Ollama的模型文件会让你觉得很熟悉。
- en: In the example below, we first specify which LLM to use. We’re assuming that
    there is a folder in your repo called `mistral7b` and that the folder contains
    a model called `quantized.gguf`. Then we specify the model’s context window to
    8,000 – Mistral 7b’s max context size. In the Modelfile, you can also specify
    which prompt template to use, and you can specify stop tokens.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们首先指定要使用的 LLM。假设您的仓库中有一个名为 `mistral7b` 的文件夹，并且该文件夹包含名为 `quantized.gguf`
    的模型。接着，我们将模型的上下文窗口设置为 8,000——Mistral 7b 的最大上下文大小。在 Modelfile 文件中，您还可以指定要使用的提示模板，并可以指定停止标记。
- en: Save the model file, eg as Modelfile.txt.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 保存模型文件，例如保存为 Modelfile.txt。
- en: For more configuration options, see [Ollama’s GitHub.](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多配置选项，请参见 [Ollama 的 GitHub](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that you have made the Modelfile, build an Ollama image from the Modelfile
    by running this from your terminal. This will also take a few moments:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经创建了 Modelfile，通过在终端运行以下命令，构建一个 Ollama 镜像。这也需要几分钟时间：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When the “create” process is done, start the Ollama server by running this command.
    This will expose all your Ollama model(s) in a way that the GUI can interact with
    them.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当“创建”过程完成后，通过运行以下命令启动 Ollama 服务器。这将以一种 GUI 可以与其交互的方式暴露所有您的 Ollama 模型。
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----4cdc741408df--------------------------------)
    [## How to estimate and reduce the carbon footprint of machine learning models'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----4cdc741408df--------------------------------)
    [## 如何估算和减少机器学习模型的碳足迹'
- en: Two ways to easily estimate the carbon footprint of machine learning models
    and 17 ideas for how you might reduce it
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有两种简单的方法可以估算机器学习模型的碳足迹，以及 17 个减少碳足迹的想法
- en: towardsdatascience.com](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----4cdc741408df--------------------------------)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----4cdc741408df--------------------------------)
- en: Step 4 – Set up chat UI for Ollama
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四步 – 设置 Ollama 的聊天 UI
- en: The next step is to set up a GUI to interact with the LLM. Several options exist
    for this. In this tutorial, we’ll use “Chatbot Ollama” – a very neat GUI that
    has a ChatGPT feel to it. “Ollama WebUI” is a similar option. You can also setup
    your own chat [GUI with Streamlit](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是设置一个 GUI 与 LLM 进行交互。对此有几种选择。在本教程中，我们将使用“Chatbot Ollama”——一个非常整洁的 GUI，具有
    ChatGPT 的感觉。“Ollama WebUI”是一个类似的选择。您还可以 [使用 Streamlit 设置自己的聊天 GUI](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)。
- en: 'By running the two commands below, you’ll first clone the Chatbot Ollama GitHub
    repo, and then install React dependencies:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下两个命令，您将首先克隆 Chatbot Ollama 的 GitHub 仓库，然后安装 React 依赖：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The next step is to build a Docker image from the Dockerfile. If you’re on Linux,
    you need to change the OLLAMA_HOST environment variable in the Dockerfile from
    `hhtp://host.docker.internal:11434`to `http://localhost:11434` .
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从 Dockerfile 构建 Docker 镜像。如果您使用的是 Linux，您需要将 Dockerfile 中的 OLLAMA_HOST 环境变量从
    `hhtp://host.docker.internal:11434` 修改为 `http://localhost:11434`。
- en: Now, build the Docker image and run a container from it by executing these commands
    from a terminal. You need to stand in the root of the project.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从终端执行以下命令来构建 Docker 镜像并从中运行容器。您需要站在项目的根目录下。
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The GUI is now running inside a Docker container on your local computer. In
    the terminal, you’ll see the address at which the GUI is available (eg. “http://localhost:3000")
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: GUI 现在正在您的本地计算机上的 Docker 容器中运行。在终端中，您将看到 GUI 可用的地址（例如：“http://localhost:3000”）
- en: Visit that address in your browser, and you should now be able to chat with
    the LLM through the Ollama Chat UI.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中访问该地址，您现在应该能够通过 Ollama 聊天 UI 与 LLM 进行互动。
- en: Conclusion
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'This concludes this brief tutorial on how to easily set up chat UI that let’s
    you interact with an LLM that’s running on your local machine. Easy, right? Only
    four steps were required:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程简要介绍了如何轻松设置聊天 UI，使您能够与在本地机器上运行的 LLM 进行交互。很简单，对吧？只需要四个步骤：
- en: Select a model on Huggingface
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Huggingface 上选择一个模型
- en: (Optional) Quantize the model
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （可选）对模型进行量化
- en: Wrap model in Ollama image
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型封装在 Ollama 镜像中
- en: Build and run a Docker container that wraps the GUI
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并运行一个封装 GUI 的 Docker 容器
- en: Remember, it’s all made possible because open source is awesome 👏
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这一切都得以实现，因为开源真是太棒了 👏
- en: 'GitHub repo for this article: [https://github.com/KasperGroesLudvigsen/local_llm](https://github.com/KasperGroesLudvigsen/local_llm)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的 GitHub 仓库：[https://github.com/KasperGroesLudvigsen/local_llm](https://github.com/KasperGroesLudvigsen/local_llm)
- en: That’s it! I hope you enjoyed the story. Let me know what you think!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！希望你喜欢这个故事，告诉我你的想法！
- en: Get the benefits of Medium and support my writing by signing up for a Medium
    membership [HERE](https://kaspergroesludvigsen.medium.com/membership).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过注册 Medium 会员 [HERE](https://kaspergroesludvigsen.medium.com/membership)，你可以享受
    Medium 的福利并支持我的写作。
- en: Follow me for more on AI and sustainability and [subscribe](https://kaspergroesludvigsen.medium.com/subscribe)
    to get my stories via email when I publish.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 关注我，了解更多关于 AI 和可持续发展的内容，并 [订阅](https://kaspergroesludvigsen.medium.com/subscribe)
    以便在我发布新故事时通过电子邮件接收。
- en: I also sometimes write about [time series forecasting](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我有时也会写关于 [时间序列预测](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e)
    的文章。
- en: And feel free to connect on [LinkedIn](https://www.linkedin.com/in/kaspergroesludvigsen).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以随时在 [LinkedIn](https://www.linkedin.com/in/kaspergroesludvigsen) 上与我联系。
