- en: Set up a local LLM on CPU with chat UI in 15 minutes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨15åˆ†é’Ÿå†…è®¾ç½®ä¸€ä¸ªåŸºäºCPUçš„æœ¬åœ°LLMå’ŒèŠå¤©UI
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=collection_archive---------1-----------------------#2024-02-06](https://towardsdatascience.com/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=collection_archive---------1-----------------------#2024-02-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=collection_archive---------1-----------------------#2024-02-06](https://towardsdatascience.com/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=collection_archive---------1-----------------------#2024-02-06)
- en: This blog post shows how to easily run an LLM locally and how to set up a ChatGPT-like
    GUI in 4 easy steps.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿™ç¯‡åšå®¢æ–‡ç« å±•ç¤ºäº†å¦‚ä½•è½»æ¾åœ°åœ¨æœ¬åœ°è¿è¡Œä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¹¶åœ¨4ä¸ªç®€å•æ­¥éª¤ä¸­è®¾ç½®ä¸€ä¸ªç±»ä¼¼ChatGPTçš„å›¾å½¢ç”¨æˆ·ç•Œé¢ã€‚
- en: '[](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--4cdc741408df--------------------------------)[![Kasper
    Groes Albin Ludvigsen](../Images/3c31c9e54fae4fd1c8f1c441379d1f10.png)](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--4cdc741408df--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4cdc741408df--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4cdc741408df--------------------------------)
    [Kasper Groes Albin Ludvigsen](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--4cdc741408df--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--4cdc741408df--------------------------------)[![Kasper
    Groes Albin Ludvigsen](../Images/3c31c9e54fae4fd1c8f1c441379d1f10.png)](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--4cdc741408df--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4cdc741408df--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4cdc741408df--------------------------------)
    [Kasper Groes Albin Ludvigsen](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--4cdc741408df--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4cdc741408df--------------------------------)
    Â·5 min readÂ·Feb 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4cdc741408df--------------------------------)
    Â·é˜…è¯»æ—¶é—´5åˆ†é’ŸÂ·2024å¹´2æœˆ6æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4af6944f035f6558d066e724d2f69d26.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4af6944f035f6558d066e724d2f69d26.png)'
- en: Photo by Liudmila Shuvalova on Unsplash
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šLiudmila Shuvalovaï¼Œæ¥è‡ªUnsplash
- en: Thanks to the global open source community, it is now easier than ever to run
    performant large language models (LLM) on consumer laptops or CPU-based servers
    and easily interact with them through well-designed graphical user interfaces.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢å…¨çƒå¼€æºç¤¾åŒºï¼Œç°åœ¨åœ¨æ¶ˆè´¹è€…ç¬”è®°æœ¬ç”µè„‘æˆ–åŸºäºCPUçš„æœåŠ¡å™¨ä¸Šè¿è¡Œé«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ä¸ä¹‹äº¤äº’æ¯”ä»¥å¾€ä»»ä½•æ—¶å€™éƒ½æ›´å®¹æ˜“ã€‚
- en: This is particularly valuable to all the organizations who are not allowed or
    not willing to use services that requires sending data to a third party.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¹æ‰€æœ‰ä¸å…è®¸æˆ–ä¸æ„¿ä½¿ç”¨éœ€è¦å°†æ•°æ®å‘é€ç»™ç¬¬ä¸‰æ–¹çš„æœåŠ¡çš„ç»„ç»‡ç‰¹åˆ«æœ‰ä»·å€¼ã€‚
- en: This tutorial shows how to set up a local LLM with a neat ChatGPT-like UI in
    four easy steps. If you have the prerequisite software installed, it will take
    you no more than 15 minutes of work (excluding the computer processing time used
    in some of the steps).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹å±•ç¤ºäº†å¦‚ä½•åœ¨å››ä¸ªç®€å•æ­¥éª¤ä¸­è®¾ç½®ä¸€ä¸ªæœ¬åœ°LLMï¼Œå¹¶é…æœ‰ä¸€ä¸ªç®€æ´çš„ChatGPTæ ·å¼çš„UIã€‚å¦‚æœä½ å·²ç»å®‰è£…äº†å¿…è¦çš„è½¯ä»¶ï¼Œæ•´ä¸ªè¿‡ç¨‹ä¸è¶…è¿‡15åˆ†é’Ÿï¼ˆä¸åŒ…æ‹¬æŸäº›æ­¥éª¤ä¸­è®¡ç®—æœºå¤„ç†æ—¶é—´ï¼‰ã€‚
- en: 'This tutorial assumes you have the following installed on your machine:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹å‡è®¾ä½ å·²ç»åœ¨æœºå™¨ä¸Šå®‰è£…äº†ä»¥ä¸‹è½¯ä»¶ï¼š
- en: '[Ollama](https://ollama.ai/)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ollama](https://ollama.ai/)'
- en: Docker
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker
- en: React
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: React
- en: Python and common packages including transformers
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pythonå’Œå¸¸ç”¨åŒ…ï¼ŒåŒ…æ‹¬transformers
- en: Now letâ€™s get going.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å¼€å§‹å§ã€‚
- en: Step 1 â€” Decide which Huggingface LLM to use
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥ â€” å†³å®šä½¿ç”¨å“ªä¸ªHuggingface LLM
- en: The first step is to decide what LLM you want to run locally. Maybe you already
    have an idea. Otherwise, for English, the [instruct version](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)
    of Mistral 7b seems to be the go-to choice. For Danish, I recommend Munin-NeuralBeagle
    although its known to over-generate tokens (perhaps because itâ€™s a merge of a
    model that was not instruction fine tuned). For other Scandinavian languages,
    see [ScandEvalâ€™s](https://scandeval.com/) evaluation of Scandinavian generative
    models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ˜¯å†³å®šä½ æƒ³åœ¨æœ¬åœ°è¿è¡Œå“ªä¸ªLLMã€‚ä¹Ÿè®¸ä½ å·²ç»æœ‰äº†æƒ³æ³•ã€‚å¦‚æœæ²¡æœ‰ï¼Œå¯¹äºè‹±è¯­ï¼ŒMistral 7bçš„[æŒ‡ä»¤ç‰ˆæœ¬](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)ä¼¼ä¹æ˜¯é¦–é€‰ã€‚å¯¹äºä¸¹éº¦è¯­ï¼Œæˆ‘æ¨èMunin-NeuralBeagleï¼Œå°½ç®¡å®ƒåœ¨ç”Ÿæˆä»¤ç‰Œæ—¶å¯èƒ½è¿‡åº¦ï¼ˆå¯èƒ½æ˜¯å› ä¸ºå®ƒåˆå¹¶äº†ä¸€ä¸ªæœªç»è¿‡æŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹ï¼‰ã€‚å¯¹äºå…¶ä»–æ–¯å ªçš„çº³ç»´äºšè¯­è¨€ï¼Œè¯·å‚è§[ScandEval](https://scandeval.com/)å¯¹æ–¯å ªçš„çº³ç»´äºšç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°ã€‚
- en: Once youâ€™ve decided which LLM to use, copy the Huggingface â€œpathâ€ to the model.
    For Mistral 7b it would be â€œmistralai/Mistral-7B-v0.1". Youâ€™ll need it in the
    next step.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ å†³å®šäº†ä½¿ç”¨å“ªä¸ªLLMï¼Œå¤åˆ¶Huggingfaceâ€œè·¯å¾„â€åˆ°è¯¥æ¨¡å‹ã€‚å¯¹äºMistral 7bï¼Œå®ƒçš„è·¯å¾„æ˜¯â€œmistralai/Mistral-7B-v0.1â€ã€‚ä½ å°†åœ¨ä¸‹ä¸€æ­¥ä¸­ç”¨åˆ°å®ƒã€‚
- en: '[](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----4cdc741408df--------------------------------)
    [## How to make a PyTorch Transformer for time series forecasting'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----4cdc741408df--------------------------------)
    [## å¦‚ä½•åˆ¶ä½œä¸€ä¸ªç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹çš„PyTorch Transformer'
- en: This post will show you how to transform a time series Transformer architecture
    diagram into PyTorch code step by step.
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å°†å‘ä½ å±•ç¤ºå¦‚ä½•ä¸€æ­¥æ­¥åœ°å°†æ—¶é—´åºåˆ—Transformeræ¶æ„å›¾è½¬æ¢ä¸ºPyTorchä»£ç ã€‚
- en: towardsdatascience.com](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----4cdc741408df--------------------------------)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----4cdc741408df--------------------------------)'
- en: Step 2 â€“ Quantize the LLM
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­¥éª¤2 â€“ å¯¹LLMè¿›è¡Œé‡åŒ–
- en: Next step is to quantize your chosen model unless you selected a model that
    was already quantized. If your modelâ€™s name ends in GGUF or GPTQ itâ€™s already
    quantized.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯å¯¹ä½ é€‰æ‹©çš„æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œé™¤éä½ é€‰æ‹©äº†ä¸€ä¸ªå·²ç»é‡åŒ–çš„æ¨¡å‹ã€‚å¦‚æœä½ çš„æ¨¡å‹åç§°ä»¥GGUFæˆ–GPTQç»“å°¾ï¼Œå®ƒå·²ç»æ˜¯é‡åŒ–è¿‡çš„ã€‚
- en: Quantization is a technique that converts the weights of a model (its learned
    parameters) to a smaller data type than the original, eg from fp16 to int4\. This
    makes the model take up less memory and also makes it faster to run inference
    which is a nice feature if youâ€™re running on CPU.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: é‡åŒ–æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå®ƒå°†æ¨¡å‹çš„æƒé‡ï¼ˆå³å®ƒçš„å­¦ä¹ å‚æ•°ï¼‰è½¬æ¢ä¸ºæ¯”åŸå§‹æ•°æ®ç±»å‹æ›´å°çš„ç±»å‹ï¼Œä¾‹å¦‚ä»fp16è½¬æ¢ä¸ºint4ã€‚è¿™ä½¿å¾—æ¨¡å‹å ç”¨æ›´å°‘çš„å†…å­˜ï¼ŒåŒæ—¶ä¹ŸåŠ å¿«äº†æ¨ç†é€Ÿåº¦ï¼Œå¦‚æœä½ åœ¨CPUä¸Šè¿è¡Œï¼Œè¿™æ˜¯ä¸€é¡¹å¾ˆæœ‰ç”¨çš„åŠŸèƒ½ã€‚
- en: The script `quantize.py`in my repo [local_llm](https://github.com/KasperGroesLudvigsen/local_llm)
    is adapated from [Maxime Labonneâ€™s fantastic Colab notebook](https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu?usp=sharing)
    (see his [LLM course](https://github.com/mlabonne/llm-course) for other great
    LLM resources). You can use his notebook or my script. The methodâ€™s been tested
    on Mistral and Mistral-like models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨æˆ‘çš„ä»“åº“[local_llm](https://github.com/KasperGroesLudvigsen/local_llm)ä¸­çš„è„šæœ¬`quantize.py`æ”¹ç¼–è‡ª[Maxime
    Labonneçš„ç²¾å½©Colabç¬”è®°æœ¬](https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu?usp=sharing)ï¼ˆå¯ä»¥æŸ¥çœ‹ä»–çš„[LLMè¯¾ç¨‹](https://github.com/mlabonne/llm-course)è·å–æ›´å¤šLLMèµ„æºï¼‰ã€‚ä½ å¯ä»¥ä½¿ç”¨ä»–çš„ç¬”è®°æœ¬æˆ–è€…æˆ‘çš„è„šæœ¬ã€‚è¿™ä¸ªæ–¹æ³•å·²åœ¨Mistralå’Œç±»ä¼¼Mistralçš„æ¨¡å‹ä¸Šè¿›è¡Œè¿‡æµ‹è¯•ã€‚
- en: 'To quantize, first clone my repo:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¿›è¡Œé‡åŒ–ï¼Œé¦–å…ˆå…‹éš†æˆ‘çš„ä»“åº“ï¼š
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, change the `MODEL_ID` variable in the `quantize.py` file to reflect your
    model of choice. This is where you need the Huggingface â€œpathâ€ that you copied
    in the first step. So if you wanna use Mistral 7b:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œåœ¨`quantize.py`æ–‡ä»¶ä¸­æ›´æ”¹`MODEL_ID`å˜é‡ï¼Œä»¥åæ˜ ä½ é€‰æ‹©çš„æ¨¡å‹ã€‚è¿™å°±æ˜¯ä½ éœ€è¦åœ¨ç¬¬ä¸€æ­¥ä¸­å¤åˆ¶çš„Huggingfaceâ€œè·¯å¾„â€ã€‚æ‰€ä»¥å¦‚æœä½ æƒ³ä½¿ç”¨Mistral
    7bï¼š
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, in your terminal, run the script:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œåœ¨ç»ˆç«¯ä¸­è¿è¡Œè„šæœ¬ï¼š
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will take some time. While the quantization process runs, you can proceed
    to the next step.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†éœ€è¦ä¸€äº›æ—¶é—´ã€‚åœ¨é‡åŒ–è¿‡ç¨‹è¿è¡Œæ—¶ï¼Œä½ å¯ä»¥ç»§ç»­è¿›è¡Œä¸‹ä¸€æ­¥ã€‚
- en: The script will produce a directory that contains the model files for the model
    you selected as well as the quantized version of the model which has the file
    extension â€œ.ggufâ€.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è„šæœ¬å°†ç”Ÿæˆä¸€ä¸ªç›®å½•ï¼Œå…¶ä¸­åŒ…å«ä½ é€‰æ‹©çš„æ¨¡å‹çš„æ¨¡å‹æ–‡ä»¶ä»¥åŠé‡åŒ–ç‰ˆæœ¬çš„æ¨¡å‹ï¼Œé‡åŒ–ç‰ˆæœ¬çš„æ–‡ä»¶æ‰©å±•åä¸ºâ€œ.ggufâ€ã€‚
- en: '[](/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----4cdc741408df--------------------------------)
    [## ChatGPTâ€™s energy use per query'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----4cdc741408df--------------------------------)
    [## ChatGPTæ¯æ¬¡æŸ¥è¯¢çš„èƒ½æºä½¿ç”¨'
- en: How much electricity does ChatGPT use to answer one question?
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ChatGPTå›ç­”ä¸€ä¸ªé—®é¢˜éœ€è¦å¤šå°‘ç”µé‡ï¼Ÿ
- en: towardsdatascience.com](/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----4cdc741408df--------------------------------)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----4cdc741408df--------------------------------)'
- en: 'Step 3: Build and run Ollama version of model'
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­¥éª¤3ï¼šæ„å»ºå¹¶è¿è¡ŒOllamaç‰ˆæœ¬çš„æ¨¡å‹
- en: We will run the model with [Ollama](https://ollama.ai/). Ollama is a software
    framework that neatly wraps a model into an API. Ollama also integrates easily
    with various front ends as weâ€™ll see in the next step.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨[Ollama](https://ollama.ai/)è¿è¡Œè¯¥æ¨¡å‹ã€‚Ollamaæ˜¯ä¸€ä¸ªè½¯ä»¶æ¡†æ¶ï¼Œå®ƒå°†æ¨¡å‹å°è£…æˆä¸€ä¸ªAPIã€‚Ollamaè¿˜å¯ä»¥è½»æ¾ä¸å„ç§å‰ç«¯é›†æˆï¼Œæ­£å¦‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€æ­¥ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚
- en: To build an Ollama image of the model, you need a so-called model file which
    is a plain text file that configures the Ollama image. If youâ€™re acquainted with
    Dockerfiles, Ollamaâ€™s model files will look familiar.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ„å»ºæ¨¡å‹çš„Ollamaé•œåƒï¼Œä½ éœ€è¦ä¸€ä¸ªæ‰€è°“çš„æ¨¡å‹æ–‡ä»¶ï¼Œè¿™æ˜¯ä¸€ä¸ªé…ç½®Ollamaé•œåƒçš„çº¯æ–‡æœ¬æ–‡ä»¶ã€‚å¦‚æœä½ ç†Ÿæ‚‰Dockerfileï¼ŒOllamaçš„æ¨¡å‹æ–‡ä»¶ä¼šè®©ä½ è§‰å¾—å¾ˆç†Ÿæ‚‰ã€‚
- en: In the example below, we first specify which LLM to use. Weâ€™re assuming that
    there is a folder in your repo called `mistral7b` and that the folder contains
    a model called `quantized.gguf`. Then we specify the modelâ€™s context window to
    8,000 â€“ Mistral 7bâ€™s max context size. In the Modelfile, you can also specify
    which prompt template to use, and you can specify stop tokens.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæŒ‡å®šè¦ä½¿ç”¨çš„ LLMã€‚å‡è®¾æ‚¨çš„ä»“åº“ä¸­æœ‰ä¸€ä¸ªåä¸º `mistral7b` çš„æ–‡ä»¶å¤¹ï¼Œå¹¶ä¸”è¯¥æ–‡ä»¶å¤¹åŒ…å«åä¸º `quantized.gguf`
    çš„æ¨¡å‹ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å°†æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£è®¾ç½®ä¸º 8,000â€”â€”Mistral 7b çš„æœ€å¤§ä¸Šä¸‹æ–‡å¤§å°ã€‚åœ¨ Modelfile æ–‡ä»¶ä¸­ï¼Œæ‚¨è¿˜å¯ä»¥æŒ‡å®šè¦ä½¿ç”¨çš„æç¤ºæ¨¡æ¿ï¼Œå¹¶å¯ä»¥æŒ‡å®šåœæ­¢æ ‡è®°ã€‚
- en: Save the model file, eg as Modelfile.txt.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿å­˜æ¨¡å‹æ–‡ä»¶ï¼Œä¾‹å¦‚ä¿å­˜ä¸º Modelfile.txtã€‚
- en: For more configuration options, see [Ollamaâ€™s GitHub.](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬²äº†è§£æ›´å¤šé…ç½®é€‰é¡¹ï¼Œè¯·å‚è§ [Ollama çš„ GitHub](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that you have made the Modelfile, build an Ollama image from the Modelfile
    by running this from your terminal. This will also take a few moments:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»åˆ›å»ºäº† Modelfileï¼Œé€šè¿‡åœ¨ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œæ„å»ºä¸€ä¸ª Ollama é•œåƒã€‚è¿™ä¹Ÿéœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼š
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When the â€œcreateâ€ process is done, start the Ollama server by running this command.
    This will expose all your Ollama model(s) in a way that the GUI can interact with
    them.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å½“â€œåˆ›å»ºâ€è¿‡ç¨‹å®Œæˆåï¼Œé€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨ Ollama æœåŠ¡å™¨ã€‚è¿™å°†ä»¥ä¸€ç§ GUI å¯ä»¥ä¸å…¶äº¤äº’çš„æ–¹å¼æš´éœ²æ‰€æœ‰æ‚¨çš„ Ollama æ¨¡å‹ã€‚
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----4cdc741408df--------------------------------)
    [## How to estimate and reduce the carbon footprint of machine learning models'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----4cdc741408df--------------------------------)
    [## å¦‚ä½•ä¼°ç®—å’Œå‡å°‘æœºå™¨å­¦ä¹ æ¨¡å‹çš„ç¢³è¶³è¿¹'
- en: Two ways to easily estimate the carbon footprint of machine learning models
    and 17 ideas for how you might reduce it
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ç§ç®€å•çš„æ–¹æ³•å¯ä»¥ä¼°ç®—æœºå™¨å­¦ä¹ æ¨¡å‹çš„ç¢³è¶³è¿¹ï¼Œä»¥åŠ 17 ä¸ªå‡å°‘ç¢³è¶³è¿¹çš„æƒ³æ³•
- en: towardsdatascience.com](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----4cdc741408df--------------------------------)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----4cdc741408df--------------------------------)
- en: Step 4 â€“ Set up chat UI for Ollama
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬å››æ­¥ â€“ è®¾ç½® Ollama çš„èŠå¤© UI
- en: The next step is to set up a GUI to interact with the LLM. Several options exist
    for this. In this tutorial, weâ€™ll use â€œChatbot Ollamaâ€ â€“ a very neat GUI that
    has a ChatGPT feel to it. â€œOllama WebUIâ€ is a similar option. You can also setup
    your own chat [GUI with Streamlit](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯è®¾ç½®ä¸€ä¸ª GUI ä¸ LLM è¿›è¡Œäº¤äº’ã€‚å¯¹æ­¤æœ‰å‡ ç§é€‰æ‹©ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨â€œChatbot Ollamaâ€â€”â€”ä¸€ä¸ªéå¸¸æ•´æ´çš„ GUIï¼Œå…·æœ‰
    ChatGPT çš„æ„Ÿè§‰ã€‚â€œOllama WebUIâ€æ˜¯ä¸€ä¸ªç±»ä¼¼çš„é€‰æ‹©ã€‚æ‚¨è¿˜å¯ä»¥ [ä½¿ç”¨ Streamlit è®¾ç½®è‡ªå·±çš„èŠå¤© GUI](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)ã€‚
- en: 'By running the two commands below, youâ€™ll first clone the Chatbot Ollama GitHub
    repo, and then install React dependencies:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿è¡Œä»¥ä¸‹ä¸¤ä¸ªå‘½ä»¤ï¼Œæ‚¨å°†é¦–å…ˆå…‹éš† Chatbot Ollama çš„ GitHub ä»“åº“ï¼Œç„¶åå®‰è£… React ä¾èµ–ï¼š
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The next step is to build a Docker image from the Dockerfile. If youâ€™re on Linux,
    you need to change the OLLAMA_HOST environment variable in the Dockerfile from
    `hhtp://host.docker.internal:11434`to `http://localhost:11434` .
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯ä» Dockerfile æ„å»º Docker é•œåƒã€‚å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ Linuxï¼Œæ‚¨éœ€è¦å°† Dockerfile ä¸­çš„ OLLAMA_HOST ç¯å¢ƒå˜é‡ä»
    `hhtp://host.docker.internal:11434` ä¿®æ”¹ä¸º `http://localhost:11434`ã€‚
- en: Now, build the Docker image and run a container from it by executing these commands
    from a terminal. You need to stand in the root of the project.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä»ç»ˆç«¯æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¥æ„å»º Docker é•œåƒå¹¶ä»ä¸­è¿è¡Œå®¹å™¨ã€‚æ‚¨éœ€è¦ç«™åœ¨é¡¹ç›®çš„æ ¹ç›®å½•ä¸‹ã€‚
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The GUI is now running inside a Docker container on your local computer. In
    the terminal, youâ€™ll see the address at which the GUI is available (eg. â€œhttp://localhost:3000")
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: GUI ç°åœ¨æ­£åœ¨æ‚¨çš„æœ¬åœ°è®¡ç®—æœºä¸Šçš„ Docker å®¹å™¨ä¸­è¿è¡Œã€‚åœ¨ç»ˆç«¯ä¸­ï¼Œæ‚¨å°†çœ‹åˆ° GUI å¯ç”¨çš„åœ°å€ï¼ˆä¾‹å¦‚ï¼šâ€œhttp://localhost:3000â€ï¼‰
- en: Visit that address in your browser, and you should now be able to chat with
    the LLM through the Ollama Chat UI.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æµè§ˆå™¨ä¸­è®¿é—®è¯¥åœ°å€ï¼Œæ‚¨ç°åœ¨åº”è¯¥èƒ½å¤Ÿé€šè¿‡ Ollama èŠå¤© UI ä¸ LLM è¿›è¡Œäº’åŠ¨ã€‚
- en: Conclusion
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: 'This concludes this brief tutorial on how to easily set up chat UI that letâ€™s
    you interact with an LLM thatâ€™s running on your local machine. Easy, right? Only
    four steps were required:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹ç®€è¦ä»‹ç»äº†å¦‚ä½•è½»æ¾è®¾ç½®èŠå¤© UIï¼Œä½¿æ‚¨èƒ½å¤Ÿä¸åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œçš„ LLM è¿›è¡Œäº¤äº’ã€‚å¾ˆç®€å•ï¼Œå¯¹å§ï¼Ÿåªéœ€è¦å››ä¸ªæ­¥éª¤ï¼š
- en: Select a model on Huggingface
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ Huggingface ä¸Šé€‰æ‹©ä¸€ä¸ªæ¨¡å‹
- en: (Optional) Quantize the model
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ï¼ˆå¯é€‰ï¼‰å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–
- en: Wrap model in Ollama image
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹å°è£…åœ¨ Ollama é•œåƒä¸­
- en: Build and run a Docker container that wraps the GUI
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ„å»ºå¹¶è¿è¡Œä¸€ä¸ªå°è£… GUI çš„ Docker å®¹å™¨
- en: Remember, itâ€™s all made possible because open source is awesome ğŸ‘
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°ä½ï¼Œè¿™ä¸€åˆ‡éƒ½å¾—ä»¥å®ç°ï¼Œå› ä¸ºå¼€æºçœŸæ˜¯å¤ªæ£’äº† ğŸ‘
- en: 'GitHub repo for this article: [https://github.com/KasperGroesLudvigsen/local_llm](https://github.com/KasperGroesLudvigsen/local_llm)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„ GitHub ä»“åº“ï¼š[https://github.com/KasperGroesLudvigsen/local_llm](https://github.com/KasperGroesLudvigsen/local_llm)
- en: Thatâ€™s it! I hope you enjoyed the story. Let me know what you think!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™äº›ï¼å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªæ•…äº‹ï¼Œå‘Šè¯‰æˆ‘ä½ çš„æƒ³æ³•ï¼
- en: Get the benefits of Medium and support my writing by signing up for a Medium
    membership [HERE](https://kaspergroesludvigsen.medium.com/membership).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ³¨å†Œ Medium ä¼šå‘˜ [HERE](https://kaspergroesludvigsen.medium.com/membership)ï¼Œä½ å¯ä»¥äº«å—
    Medium çš„ç¦åˆ©å¹¶æ”¯æŒæˆ‘çš„å†™ä½œã€‚
- en: Follow me for more on AI and sustainability and [subscribe](https://kaspergroesludvigsen.medium.com/subscribe)
    to get my stories via email when I publish.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å…³æ³¨æˆ‘ï¼Œäº†è§£æ›´å¤šå…³äº AI å’Œå¯æŒç»­å‘å±•çš„å†…å®¹ï¼Œå¹¶ [è®¢é˜…](https://kaspergroesludvigsen.medium.com/subscribe)
    ä»¥ä¾¿åœ¨æˆ‘å‘å¸ƒæ–°æ•…äº‹æ—¶é€šè¿‡ç”µå­é‚®ä»¶æ¥æ”¶ã€‚
- en: I also sometimes write about [time series forecasting](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æœ‰æ—¶ä¹Ÿä¼šå†™å…³äº [æ—¶é—´åºåˆ—é¢„æµ‹](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e)
    çš„æ–‡ç« ã€‚
- en: And feel free to connect on [LinkedIn](https://www.linkedin.com/in/kaspergroesludvigsen).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå¯ä»¥éšæ—¶åœ¨ [LinkedIn](https://www.linkedin.com/in/kaspergroesludvigsen) ä¸Šä¸æˆ‘è”ç³»ã€‚
