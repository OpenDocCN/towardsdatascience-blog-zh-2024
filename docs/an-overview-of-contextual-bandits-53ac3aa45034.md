# 上下文Bandit概述

> 原文：[https://towardsdatascience.com/an-overview-of-contextual-bandits-53ac3aa45034?source=collection_archive---------1-----------------------#2024-02-02](https://towardsdatascience.com/an-overview-of-contextual-bandits-53ac3aa45034?source=collection_archive---------1-----------------------#2024-02-02)

## 一种治疗个性化的动态方法

[](https://medium.com/@uguryi?source=post_page---byline--53ac3aa45034--------------------------------)[![Ugur Yildirim](../Images/33db36531a170c9621504f466d61334b.png)](https://medium.com/@uguryi?source=post_page---byline--53ac3aa45034--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--53ac3aa45034--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--53ac3aa45034--------------------------------) [Ugur Yildirim](https://medium.com/@uguryi?source=post_page---byline--53ac3aa45034--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--53ac3aa45034--------------------------------) ·19分钟阅读·2024年2月2日

--

# 大纲

1.  [介绍](#bc08)

1.  [何时使用上下文Bandit](#0f83)

    2.1\. [上下文Bandit与多臂赌博机（MAB）与A/B测试](#9c98)

    2.2\. [上下文Bandit与多个多臂赌博机（MAB）](#0b64)

    2.3\. [上下文Bandit与多步强化学习](#fc98)

    2.4\. [上下文Bandit与提升建模](#e404)

1.  [上下文Bandit中的探索与利用](#6c33)

    3.1\. [*ε*-贪婪策略](#635e)

    3.2\. [上置信界限（UCB）](#7841)

    3.3\. [汤普森采样](#31b8)

1.  [上下文Bandit算法步骤](#49b9)

1.  [上下文Bandit中的离线策略评估](#e7aa)

    5.1\. [使用因果推断方法的OPE](#3f89)

    5.2\. [使用采样方法的OPE](#c117)

1.  [上下文Bandit实践](#6403)

1.  [结论](#92a6)

1.  [致谢](#7629)

1.  [参考文献](#5b07)

# 1\. 介绍

想象一下这样的场景：你刚刚开始了一个为期两周的A/B测试。然而，仅仅一两天后，越来越明显地发现版本A对于某些类型的用户效果更好，而版本B对于另一组用户效果更佳。你心想：也许我应该重新引导流量，让用户更多地接触到对他们更有益的版本，而少接触到另一个版本。有没有一种有原则的方法可以实现这一点？

上下文赌博者（Contextual Bandits）是一类一阶强化学习算法，专门为这种治疗个性化问题设计，在这种问题中，我们希望根据哪种治疗对谁有效来动态调整流量。尽管它们在可以实现的效果上极为强大，但它们是数据科学中较少为人知的算法之一，我希望这篇文章能为你提供一个全面的介绍，帮助你了解这个令人惊叹的话题。事不宜迟，让我们直接深入了解吧！

# 2\. 何时使用上下文赌博者

如果你刚刚开始了解上下文赌博者，可能会对上下文赌博者与其他更广为人知的方法（如A/B测试）之间的关系感到困惑，以及为什么你可能会选择使用上下文赌博者而不是其他方法。因此，我们将从讨论上下文赌博者与相关方法之间的相似性和差异性开始我们的旅程。

## 2.1\. 上下文赌博者 vs 多臂赌博者 vs A/B 测试

让我们从最基本的A/B测试设置开始，该设置将流量以*静态*的方式分配到治疗组和控制组。例如，一个数据科学家可能决定进行为期两周的A/B测试，50%的流量分配给治疗组，50%分配给控制组。这意味着无论我们处于测试的第一天还是最后一天，我们都将以50%的概率将用户分配给治疗组或控制组。

另一方面，如果数据科学家在这种情况下使用多臂赌博者（MAB）而不是A/B测试，那么流量将以*动态*的方式分配到治疗组和控制组。换句话说，MAB中的流量分配将随着时间的推移而变化。例如，如果算法在第一天判断治疗组优于控制组，那么流量分配可能会从第一天的50%治疗和50%控制调整为第二天的60%治疗和40%控制，依此类推。

尽管动态分配流量，MAB忽略了一个重要事实，那就是并不是所有用户都一样。这意味着对某一类用户有效的治疗方法可能对另一类用户无效。例如，可能出现这种情况：尽管治疗对核心用户更有效，但对休闲用户而言，控制组实际上更好。在这种情况下，即使治疗方法整体上更有效，我们如果将更多核心用户分配给治疗组，将更多休闲用户分配给控制组，实际上可以从我们的应用中获得更多价值。

这正是上下文赌博机（CB）派上用场的地方。尽管多臂赌博机（MAB）只关注治疗组或对照组整体效果如何，*总体*上是否表现更好，CB则关注治疗组或对照组对于具有特定*特征*的用户表现如何。在上下文赌博机中，“上下文”恰恰指的就是这些用户特征，这也是其与多臂赌博机的区别所在。例如，CB可能会在观察到第一天的数据后，决定将核心用户的治疗分配提高到60%，而将普通用户的治疗分配降至40%。换句话说，CB会根据用户特征（在此例中为核心用户与普通用户）动态调整流量分配。

下表总结了A/B测试、多臂赌博机和上下文赌博机的关键区别，接下来的图表将可视化这些概念。

**表1：A/B测试、多臂赌博机和上下文赌博机的区别**

![](../Images/b064c54b71fa354aa2c4b4fcaa761b23.png)

**图1：A/B测试、多臂赌博机和上下文赌博机中的流量分配**

![](../Images/dd8e19b91a7218c593729019b8ff710d.png)

## 2.2\. 上下文赌博机与多个多臂赌博机

到此，你可能会产生这样的想法：上下文赌博机（CB）不过是多个多臂赌博机（MAB）同时运行的集合。事实上，当我们关注的上下文较小（例如，我们只关心一个用户是核心用户还是普通用户）时，我们可以简单地为核心用户运行一个MAB，为普通用户运行另一个MAB。然而，当上下文变得庞大（例如核心与普通用户、年龄、国家、上次活跃时间等）时，为每个独特的上下文值运行一个单独的MAB变得不切实际。

在这种情况下，上下文赌博机的真正价值体现在通过使用*模型*来描述不同上下文中的实验条件与我们关注的结果（例如转化率）之间的关系。与逐一列举每个上下文值并将其独立处理不同，使用模型可以让我们共享来自不同上下文的信息，从而处理更大的上下文空间。这个模型的概念将在本文的多个部分进行讨论，因此请继续阅读以了解更多内容。

## 2.3\. 上下文赌博机与多步强化学习

引言中将上下文赌博机（CB）称为一种单步强化学习（RL）算法。那么，单步与多步强化学习到底有什么区别呢？是什么使得CB成为单步学习？上下文赌博机与多步强化学习的根本区别在于，在CB中，我们假设算法所采取的行动（例如，为特定用户提供治疗或对照组）不会影响系统整体的未来状态。换句话说，状态（或在CB中更适当称为“上下文”）影响我们采取的行动，但我们采取的行动*不会*反过来影响或改变状态。下图总结了这一区别。

**图2：上下文赌博机与多步强化学习**

![](../Images/99dd1e47664a6110fcef41adf27f1326.png)

图片由作者提供，灵感来源于[source](/contextual-bandits-and-reinforcement-learning-6bdfeaece72a)

一些例子应该能让这个区别更加清晰。假设我们正在构建一个系统，根据用户的年龄决定展示哪些广告。我们预期，不同年龄段的用户可能会觉得不同的广告与他们更相关，这意味着用户的年龄应该影响我们展示给他们的广告。然而，我们展示的广告并不会反过来影响他们的年龄，所以CB的单步假设似乎成立。然而，如果我们进一步发现，展示昂贵的广告会消耗我们的库存（并限制未来我们能展示的广告），或者我们今天展示的广告会影响用户是否会再次访问我们的网站，那么单步假设就间接被违反了，因此我们可能需要考虑开发一个完整的强化学习（RL）系统。

需要注意的是：虽然与上下文赌博机相比，多步强化学习更加灵活，但它的实现也更加复杂。因此，如果当前的问题能够准确地被框定为一个单步问题（即使乍一看像是多步问题），上下文赌博机可能是更实际的解决方案。

## 2.4. 上下文赌博机与提升建模

在继续讨论不同的CB算法之前，我还想简要地提及一下CB与提升建模之间的关系。提升模型通常是基于A/B测试数据构建的，用于发现处理效果（提升）与用户特征之间的关系。然后，可以使用该模型的结果来个性化未来的处理。例如，如果提升模型发现某些用户更可能从某个处理方法中受益，那么未来可能只会将该处理方法提供给这些类型的用户。

给定对提升建模的描述，应该很清楚，CB（上下文赌博机）和提升建模都是个性化问题的解决方案。它们之间的关键区别在于，CB以一种更动态的方式解决这个问题，个性化发生在*即时*的过程中，而不是等待A/B测试的结果。从概念层面上讲，CB可以非常宽泛地被看作是A/B测试和提升建模同时发生，而不是顺序发生。考虑到本文的重点，我不会进一步讨论提升建模，但有几个很好的资源可以了解更多相关内容，例如[[1]](https://www.uber.com/blog/research/uplift-modeling-for-multiple-treatments-with-cost-optimization/)。

# 3. 上下文赌博机中的探索与开发

上文我们讨论了CB如何根据某个给定时间点、特定用户群体的治疗组和对照组的表现，动态分配流量。这引出了一个重要的问题：当我们进行这些流量分配调整时，我们希望多么激进？例如，如果在一天的数据之后，我们决定治疗组对美国用户表现更好，是否应该完全停止对美国用户提供对照组？

我相信大多数人都会同意，这个做法是一个糟糕的主意，而且你们是对的。过于激进地改变流量分配的主要问题是，基于不足的数据进行推断可能导致错误的结论。例如，可能第一天的数据实际上并不代表沉睡用户的情况，实际上对照组对他们更好。如果我们在第一天之后就停止向美国用户提供对照组，我们将永远无法了解这种正确的关系。

更好的动态更新流量分配的方法是，在利用（基于目前的数据提供最佳实验条件）和探索（继续为其他实验条件提供服务）之间找到合适的平衡。延续前面的例子，如果第一天的数据表明治疗组对美国用户更好，我们可以在第二天通过更高的概率为这些用户提供治疗，同时仍然为对照组分配一个减少但非零的比例。

在CB（以及MAB）中有许多探索策略，还有一些变种试图在探索和利用之间找到合适的平衡。三种常见的策略包括ε-贪婪策略、上置信界限（UCB）和汤普森采样（Thompson sampling）。

## 3.1. ε-贪婪策略

在这个策略中，我们首先决定哪个实验条件在某个给定时间点对于某个特定用户群体表现更好。最简单的做法是通过比较这些用户在每个实验条件下的目标值（*y*）的平均值来实现。更正式地说，我们可以通过找出条件*d*，其值较高，从而决定该组用户的“获胜”条件。

![](../Images/aacbee75310f29271244345d8dbe6001.png)

其中，*n_dx*是我们到目前为止从条件*d*和上下文*x*中的用户中获取的样本数量，*y_idx*是条件*d*和上下文*x*中第*i*个样本的目标值。

在决定了哪个实验条件目前对这些用户是“最好的”之后，我们以*1-ε*的概率为他们提供该条件（其中*ε*通常是一个较小的数字，如0.05），并以*ε*的概率提供一个随机的实验条件。实际上，我们可能希望动态更新我们的*ε*，使其在实验开始时较大（此时需要更多的探索），随着我们收集更多数据，*ε*逐渐变小。

此外，上下文 *X* 可能是高维的（例如，国家、性别、平台、任期等），因此我们可能需要使用模型来获取这些 *y* 估计值，从而应对维度灾难。形式上，服务的条件可以通过找到条件 *d* 使其值更高来决定。

![](../Images/c4eed6bc6b2d15fe9be0299c6dc27931.png)

其中 *x^T* 是一个 *m* 维的行向量，表示上下文值，*θ_d* 是一个 *m* 维的列向量，表示与条件 *d* 相关的可学习参数。

## 3.2. 上置信度界限（UCB）

这个策略通过查看不仅是哪个条件具有更高的 *y* 估计值，还包括我们对该估计值的精确度（或信心），来决定下一个要服务的条件。在一个简单的 MAB 设置中，精确度可以被视为已经服务过多少次某个条件的函数。特别地，一个条件（i）具有较高的平均 *y*（所以值得利用），或者（ii）还没有被服务过很多次（所以需要更多的探索），更有可能被下一个选择来服务。

我们可以通过跟踪不同条件在不同上下文中被服务的次数，将这一思想推广到 CB 设置。假设在一个简单的设置中，上下文 *X* 是低维的，这样 CB 可以看作是多个 MAB 的组合运行，我们可以基于哪个条件 *d* 的值更高来选择下一个要服务的条件。

![](../Images/c81f49f38f4edeb81ae4e4b633191981.png)

其中 *c* 是一个常数（根据我们希望在探索时对估计精确度的重视程度来选择），*n_x* 是上下文 *x* 到目前为止被看到的次数。

然而，在大多数情况下，上下文 *X* 会是高维的，这意味着就像在 *ε*-贪婪情况下，我们需要使用模型。在这种设置中，如果某个条件 *d* 的值更高，那么它可以被选择为下一个要服务的条件。

![](../Images/56ecf5df3614fd3b09d19fbaafa735bb.png)

其中 *SE(.)* 是我们估计的标准误差（或更广泛地说，是量化我们当前对该估计的信心水平的度量）。

请注意，UCB 有多个版本，因此你可能会遇到不同的公式。一种流行的 UCB 方法是 LinUCB，它在一个线性模型框架中形式化了问题（例如，[[2]](/recommender-systems-using-linucb-a-contextual-multi-armed-bandit-approach-35a6f0eb6c4)）。

## 3.3. 汤普森采样

将要讨论的第三种也是最后一种探索策略是汤普森采样，它是一种解决探索与利用困境的贝叶斯方法。在这里，我们有一个模型*f(D, X; Θ)*，它根据实验条件*D*、上下文*X*和一些可学习的参数*Θ*返回预测的*y*值。这个函数让我们可以访问任何条件-上下文对的预期*y*值的后验分布，从而根据给定上下文下产生最高预期*y*的概率选择下一个要执行的条件。汤普森采样自然地平衡了探索与利用，因为我们是从后验分布中采样并根据观察结果更新我们的模型。为了让这些概念更加具体，以下是汤普森采样涉及的步骤：

![](../Images/6d027cd3e61c98ab31a0a13d2e6fbd3f.png)

实际上，我们可以使用不同的函数来处理每个实验条件，而不是使用单一的函数（例如，评估*f_c(X; Θ_c)*和*f_t(X; Θ_t)*，然后选择具有较高值的条件）。此外，更新步骤通常不会在每次样本之后进行，而是在看到一批样本之后进行。有关汤普森采样的更多细节，可以参考[[3]](https://arxiv.org/pdf/1707.02038.pdf) [[4]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7352306)。

# 4\. 上下文强盗算法步骤

前一节（尤其是关于汤普森采样的部分）应该已经让你对CB算法的步骤有了一个相当清晰的了解。然而，为了完整性，以下是标准CB算法的逐步描述：

1.  一个新的数据点到达，具有上下文*X*（例如，一个在美国使用iOS设备的核心用户）。

1.  给定这个数据点和选择的探索策略（例如，*ε*-贪婪策略），算法决定为该用户执行哪个条件（例如，治疗或对照）。

1.  在条件被执行后，我们观察到结果*y*（例如，用户是否进行了购买）。

1.  在看到新数据后更新（或完全重新训练）步骤2中使用的模型。（如前所述，我们通常不是在每次样本后进行更新，而是在看到一批样本后进行更新，以确保更新不那么嘈杂。）

1.  重复。

# 5\. 上下文强盗中的离线策略评估

到目前为止，我们只讨论了如何在*新*数据到来时实现CB算法。另一个同样重要的话题是如何使用*旧*（或*已记录*）数据来评估CB算法。这被称为离线评估或离线策略评估（OPE）。

## 5.1\. 使用因果推断方法进行OPE

进行OPE的一种方法是使用广为人知的因果推断技术，如逆倾向得分（IPS）或双重稳健（DR）方法。因果推断在这里是合适的，因为我们实际上是在尝试估计一个反事实，即如果采用不同的策略为用户提供不同的条件会发生什么。这个话题已经有一篇很好的Medium文章介绍[[5]](https://edoconti.medium.com/offline-policy-evaluation-run-fewer-better-a-b-tests-60ce8f93fa15)，所以这里我将简要总结文章的主要思想，并将其应用到我们的讨论中。

以IPS为例，进行OPE通常要求我们不仅知道（i）使用我们新的CB算法为样本分配给定条件的概率，还要知道（ii）在记录数据中样本被分配到给定条件的概率。假设以下是一个假设的记录数据，其中*X_1-X_3*是上下文，*D*是实验条件，*P_O(D)*是将*D*分配给该用户的概率，*y*是结果。

**表2：来自A/B测试的示例记录数据**

![](../Images/fb236a7c9b9156c8c161d4dedfef5850.png)

如你所见，在这个例子中，*P_O(D)* 对于*D=1*始终为0.6，对于*D=0*始终为0.4，无论上下文如何，因此可以假设记录的数据来自一个以0.6的概率分配处理的A/B测试。现在，如果我们想测试CB算法在我们使用CB算法而非简单A/B测试分配条件的情况下会如何表现，我们可以使用以下公式来获取CB的累积*y*的IPS估计值。

![](../Images/4a6d40fc68b5f0b3dda045a12c2a825a.png)

其中*n*是记录数据中的样本数量（此处为5），*P_N(D_i)*是如果我们使用新的CB算法，给*user_i*分配记录中的*D*的概率（此概率将取决于被评估的具体算法）。

一旦我们有了这个估计值，我们可以将其与旧A/B测试中观察到的累积*y*进行比较（在这里为1+0+0+1+1=3），以决定CB是否会产生更高的累积*y*。

有关使用因果推断方法进行OPE的更多信息，请参考本节开头链接的文章。该文章还链接了一个非常好的GitHub仓库，其中包含许多OPE实现。

这里有一个旁注，本节讨论了因果推断方法作为OPE中的一种技术。然而，实际上，人们也可以在CB算法运行时应用这些方法，以“去偏”算法在过程中收集的训练数据。我们可能希望将像IPS这样的技术应用于训练数据的原因是，生成这些数据的CB策略本质上是一个非均匀的随机策略，因此，从中估计因果效应来决定采取什么行动时，使用因果推断方法会更有帮助。如果你想了解更多关于去偏的信息，请参考[[6]](https://arxiv.org/pdf/1802.04064.pdf)。

## 5.2\. 使用采样方法进行 OPE

进行 OPE 的另一种方法是通过使用采样方法。特别地，可以使用一种非常简单的重放方法[[7]](https://arxiv.org/pdf/1003.5956.pdf)来评估 CB 算法（或任何其他算法），该方法使用来自随机化策略（如 A/B 测试）的日志数据。在最简单的形式下（假设我们使用均匀随机日志策略），该方法的工作原理如下：

1.  从日志数据中采样下一个具有上下文 *X* 的用户。

1.  使用新的 CB 算法决定分配给该用户的条件。

1.  如果选定的条件与日志数据中的实际条件匹配，则将观察到的 *y* 添加到累积 *y* 计数器中。如果不匹配，则忽略该样本。

1.  重复直到所有样本都被考虑。

如果日志策略没有均匀随机地分配处理，则需要对该方法稍作修改。作者自己提到的一个修改是使用拒绝采样（例如，[[8]](https://grail.cs.washington.edu/projects/nonstationaryeval/nonstationaryevalExtended.pdf)），在第 3 步中，我们接受来自多数处理的样本的频率会比少数处理的样本少。或者，我们可以考虑在第 3 步中将观察到的 *y* 除以倾向性，从而“下调”更频繁的处理，并“上调”较少频繁的处理。

在下一部分中，我使用了一个更简单的方法来评估，它通过引导法进行上采样和下采样，将原始的不均匀数据转化为均匀数据，然后按原样应用该方法。

# 6\. 上下文赌博机实际应用

为了演示上下文赌博机的实际应用，我整理了一个 [notebook](https://github.com/uguryi/contextual_bandit/blob/main/ab_vs_mab_vs_cb.ipynb)，该 notebook 生成一个模拟数据集并比较新 A/B、MAB 和 CB 策略在该数据集上的累积 *y*（或“奖励”）估计。该 notebook 中的许多代码来自一本关于强化学习的精彩书籍的“上下文赌博机”章节[[9]](https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python/blob/master/Chapter03/Contextual%20Bandits.ipynb)（如果你想深入了解使用 Python 的强化学习，强烈推荐），以及 James LeDoux 的两篇精彩文章[[10]](https://jamesrledoux.com/algorithms/offline-bandit-evaluation/) [[11]](https://jamesrledoux.com/algorithms/bandit-algorithms-epsilon-ucb-exp-python/)，并已根据我们在此讨论的设置进行了调整。

设置非常简单：我们拥有的原始数据来自一个 A/B 测试，该测试以 0.75 的概率分配处理给用户（即*非*均匀随机）。使用这些随机化的日志数据，我们希望基于它们的累积 *y* 来评估和比较以下三种策略：

1.  一种新的 A/B 策略，它以 0.4 的概率随机将处理分配给用户。

1.  一个 MAB 策略，它使用*ε*-贪心策略决定下一步分配什么治疗，但不考虑上下文 *X*。

1.  一个 CB 策略，它使用*ε*-贪心策略决定下一步分配什么治疗，同时考虑上下文 *X*。

我修改了 Li 等人在论文中描述的原始方法，不再直接从模拟数据中采样（在我的例子中，模拟数据是75%治疗和仅25%控制），而是首先对治疗案例进行下采样，对控制案例进行上采样（都使用替换），从而获得一个新的数据集，确保治疗和控制各占50%。

我之所以从一个*非*50%治疗和50%控制的数据集开始，是为了展示即使原始数据不是来自一个均匀随机分配治疗和控制的策略，我们仍然可以通过对数据进行上下采样，将其处理成一个50/50%的数据集，并进行离线评估。如前一节所提到的，上下采样的逻辑类似于拒绝采样以及相关的将观察到的 *y* 除以倾向的概念。

以下图表比较了上述三种策略（A/B vs MAB vs CB）在其累计 *y* 值上的表现。

**图3：累计奖励比较**

![](../Images/e6aaafba01f5022e78987f9e49c2f21b.png)

如图所示，累计 *y* 在 CB 策略下增加最快，A/B 策略最慢，而 MAB 则介于两者之间。虽然这个结果基于一个模拟数据集，但这里观察到的模式仍然可以进行概括。A/B 测试无法获得较高的累计 *y* 的原因在于，即便看到足够的证据表明治疗总体上优于控制，它仍然没有改变 60/40%的分配。另一方面，虽然 MAB 能够动态更新流量分配，但由于没有基于观察到的上下文 *X* 个性化治疗与控制的分配，它的表现仍然不如 CB。最终，CB 同时动态变化流量分配，并且个性化治疗，因此表现最好。

# 7\. 结论

恭喜你完成了这篇相对较长的文章！在这篇文章中，我们讨论了很多与上下文赌博机（CB）相关的内容，我希望你读完这篇文章后，能更加理解这种方法在在线实验中的实用性，特别是当治疗需要个性化时。

如果你对学习更多关于上下文臂（或者想深入了解多步强化学习）感兴趣，我强烈推荐阅读**Mastering Reinforcement Learning with Python**一书，作者是E. Bilgin。这本书的上下文臂章节让我最终理解这个主题的“啊哈！”时刻，我继续阅读以了解更多关于RL的知识。至于离线策略评估，我强烈推荐E. Conti和J. LeDoux的文章，两者都提供了关于所涉技术的很好的解释并附有代码示例。关于上下文臂中的去偏见问题，A. Bietti、A. Agarwal和J. Langford的论文提供了很好的技术概述。最后，虽然本文专注于在构建上下文臂时使用回归模型，但还有一种名为成本敏感分类的替代方法，你可以通过查阅A. Agarwal和S. Kakade的这些讲义开始学习[[12]](https://courses.cs.washington.edu/courses/cse599m/19sp/notes/off_policy.pdf)。

玩得开心，构建上下文臂！

# 8\. 致谢

我要感谢Colin Dickens向我介绍上下文臂，并在这篇文章中提供了宝贵的反馈，感谢Xinyi Zhang在写作过程中提供的所有有用反馈，感谢Jiaqi Gu对采样方法的富有成效的讨论，以及感谢Dennis Feehan鼓励我花时间写这篇文章。

除非另有说明，所有图片均为作者所有。

# 9\. 参考文献

[1] Z. Zhao和T. Harinen，[多治疗方案的提升建模与成本优化](https://www.uber.com/blog/research/uplift-modeling-for-multiple-treatments-with-cost-optimization/)（2019），DSAA

[2] Y. Narang，[使用LinUCB的推荐系统：上下文多臂老虎机方法](/recommender-systems-using-linucb-a-contextual-multi-armed-bandit-approach-35a6f0eb6c4)（2020），Medium

[3] D. Russo, B. Van Roy, A. Kazerouni, I. Osband, and Z. Wen，[Thompson抽样教程](https://arxiv.org/pdf/1707.02038.pdf)（2018），Foundations and Trends in Machine Learning

[4] B. Shahriari, K. Swersky, Z. Wang, R. Adams, and N. de Freitas，[摒弃人类的循环：贝叶斯优化综述](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7352306)（2015），IEEE

[5] E. Conti，[离线策略评估：减少，优化A/B测试](https://edoconti.medium.com/offline-policy-evaluation-run-fewer-better-a-b-tests-60ce8f93fa15)（2021），Medium

[6] A. Bietti, A. Agarwal, and J. Langford，[上下文臂竞赛](https://arxiv.org/pdf/1802.04064.pdf)（2021），ArXiv

[7] L. Li, W. Chu, J. Langford, and X. Wang，[无偏离线评估基于上下文臂的新闻推荐算法](https://arxiv.org/pdf/1003.5956.pdf)（2011），WSDM

[8] T. Mandel, Y. Liu, E. Brunskill, 和 Z. Popovic, [在线强化学习算法的离线评估](https://grail.cs.washington.edu/projects/nonstationaryeval/nonstationaryevalExtended.pdf) (2016), AAAI

[9] E. Bilgin, [用 Python 精通强化学习](https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python/tree/master) (2020), Packt 出版社

[10] J. LeDoux, [使用回放在 Python 中离线评估多臂老虎机算法](https://jamesrledoux.com/algorithms/offline-bandit-evaluation/) (2020), LeDoux 个人网站

[11] J. LeDoux, [Python 中的多臂老虎机：Epsilon 贪婪法、UCB1、贝叶斯 UCB 和 EXP3](https://jamesrledoux.com/algorithms/bandit-algorithms-epsilon-ucb-exp-python/) (2020), LeDoux 个人网站

[12] A. Agarwal 和 S. Kakade, [非策略评估与学习](https://courses.cs.washington.edu/courses/cse599m/19sp/notes/off_policy.pdf) (2019), 华盛顿大学计算机科学系
