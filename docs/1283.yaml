- en: Self-attentive sentence embedding for the recommendation system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/self-attentive-sentence-embedding-for-the-recommendation-system-fc8af5817035?source=collection_archive---------10-----------------------#2024-05-22](https://towardsdatascience.com/self-attentive-sentence-embedding-for-the-recommendation-system-fc8af5817035?source=collection_archive---------10-----------------------#2024-05-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is self-attentive embedding, and how do we use it in recommendation systems?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bobi_29852?source=post_page---byline--fc8af5817035--------------------------------)[![Bào
    Bùi](../Images/3b35babbfc4becf3cf45575bb7b3b26e.png)](https://medium.com/@bobi_29852?source=post_page---byline--fc8af5817035--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fc8af5817035--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fc8af5817035--------------------------------)
    [Bào Bùi](https://medium.com/@bobi_29852?source=post_page---byline--fc8af5817035--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fc8af5817035--------------------------------)
    ·6 min read·May 22, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed3ccab6483e5f59165ac277457e7f19.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Giulia Bertelli](https://unsplash.com/@giulia_bertelli) on [Unsplash](https://unsplash.com/photos/j_luAxi8fWc)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The transformer layer and its attention mechanism are some of the most impactful
    ideas in the NLP community. They play a crucial role in many large language models,
    such as [ChatGPT](https://openai.com/blog/chatgpt) and [LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/),
    which have recently taken the world by storm.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is another interesting idea that originated from the NLP community,
    but its impact is mainly realized in the recommendation field: self-attentive
    sentence embedding. In this article, I will walk us through self-attentive sentence
    embedding [1] and how to apply it to the recommendation system.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overall idea
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The paper''s main idea is to find a better way to encode a sentence into multiple
    embeddings that can capture various aspects of the sentence. Specifically, instead
    of encoding a sentence to a single embedding, the authors want to encode it into
    a 2D matrix, where each row embedding captures a different aspect of the sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the sentence embeddings, we can use them for various downstream
    tasks, such as sentence analysis, author profiling…
  prefs: []
  type: TYPE_NORMAL
