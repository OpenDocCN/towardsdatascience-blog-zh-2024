<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Graph & Geometric ML in 2024: Where We Are and What’s Next (Part II — Applications)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Graph & Geometric ML in 2024: Where We Are and What’s Next (Part II — Applications)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63?source=collection_archive---------3-----------------------#2024-01-16">https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63?source=collection_archive---------3-----------------------#2024-01-16</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="d6d0" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">State-of-the-Art Digest</h2><div/><div><h2 id="6998" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Following the tradition from previous years, we interviewed a cohort of distinguished and prolific academic and industrial experts in an attempt to summarise the highlights of the past year and predict what is in store for 2024. Past 2023 was so ripe with results that we had to break this post into two parts. This is Part II focusing on applications, see also <a class="af hi" rel="noopener" target="_blank" href="/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1">Part I</a> for theory &amp; new architectures.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hj hk hl hm hn ab"><div><div class="ab ho"><div><div class="bm" aria-hidden="false"><a href="https://mgalkin.medium.com/?source=post_page---byline--1ed786f7bf63--------------------------------" rel="noopener follow"><div class="l hp hq by hr hs"><div class="l ed"><img alt="Michael Galkin" class="l ep by dd de cx" src="../Images/c5eb13334712ca0462d8a5df4a268ad0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*R6303tLavDAf6jJAsMlaJQ.jpeg"/><div class="ht by l dd de em n hu eo"/></div></div></a></div></div><div class="hv ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--1ed786f7bf63--------------------------------" rel="noopener follow"><div class="l hw hx by hr hy"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hz cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ht by l br hz em n hu eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ia ab q"><div class="ab q ib"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ic id bk"><a class="af ag ah ai aj ak al am an ao ap aq ar ie" data-testid="authorName" href="https://mgalkin.medium.com/?source=post_page---byline--1ed786f7bf63--------------------------------" rel="noopener follow">Michael Galkin</a></p></div></div></div><span class="if ig" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ic id dx"><button class="ih ii ah ai aj ak al am an ao ap aq ar ij ik il" disabled="">Follow</button></p></div></div></span></div></div><div class="l im"><span class="bf b bg z dx"><div class="ab cn in io ip"><div class="iq ir ab"><div class="bf b bg z dx ab is"><span class="it l im">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar ie ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--1ed786f7bf63--------------------------------" rel="noopener follow"><p class="bf b bg z iu iv iw ix iy iz ja jb bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="if ig" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">42 min read</span><div class="jc jd l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 16, 2024</span></div></span></div></span></div></div></div><div class="ab cp je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt"><div class="h k w ea eb q"><div class="kj l"><div class="ab q kk kl"><div class="pw-multi-vote-icon ed it km kn ko"><div class=""><div class="kp kq kr ks kt ku kv am kw kx ky ko"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kz la lb lc ld le lf"><p class="bf b dy z dx"><span class="kq">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kp li lj ab q ee lk ll" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lh"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lg lh">5</span></p></button></div></div></div><div class="ab q ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki"><div class="lm k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ln an ao ap ij lo lp lq" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lr cn"><div class="l ae"><div class="ab cb"><div class="ls lt lu lv lw lx ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq mr"><img src="../Images/f888e845d4bdb9f34131d8b0544d71fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lz_A1l6i036AtJ-FBFOe2w.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Image by Authors with some help from DALL-E 3.</figcaption></figure></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="68d6" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><em class="om">The post is written and edited by </em><a class="af hi" href="https://twitter.com/michael_galkin" rel="noopener ugc nofollow" target="_blank"><em class="om">Michael Galkin</em></a><em class="om"> and </em><a class="af hi" href="https://twitter.com/mmbronstein" rel="noopener ugc nofollow" target="_blank"><em class="om">Michael Bronstein</em></a><em class="om"> with significant contributions from </em><a class="af hi" href="https://twitter.com/dom_beaini" rel="noopener ugc nofollow" target="_blank"><em class="om">Dominique Beaini</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/nathanbenaich" rel="noopener ugc nofollow" target="_blank"><em class="om">Nathan Benaich</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/bose_joey" rel="noopener ugc nofollow" target="_blank"><em class="om">Joey Bose</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/jo_brandstetter" rel="noopener ugc nofollow" target="_blank"><em class="om">Johannes Brandstetter</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/befcorreia" rel="noopener ugc nofollow" target="_blank"><em class="om">Bruno Correia</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/Ahmed_AI035" rel="noopener ugc nofollow" target="_blank"><em class="om">Ahmed Elhag</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/KexinHuang5" rel="noopener ugc nofollow" target="_blank"><em class="om">Kexin Huang</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/chaitjo" rel="noopener ugc nofollow" target="_blank"><em class="om">Chaitanya Joshi</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/leonklein26" rel="noopener ugc nofollow" target="_blank"><em class="om">Leon Klein</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/anoopnm007" rel="noopener ugc nofollow" target="_blank"><em class="om">N M Anoop Krishnan</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/WillLin1028" rel="noopener ugc nofollow" target="_blank"><em class="om">Chen Lin</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/loukasa_tweet" rel="noopener ugc nofollow" target="_blank"><em class="om">Andreas Loukas</em></a><em class="om">, </em><a class="af hi" href="https://www.linkedin.com/in/santiago-miret" rel="noopener ugc nofollow" target="_blank"><em class="om">Santiago Miret</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/NaefLuca" rel="noopener ugc nofollow" target="_blank"><em class="om">Luca Naef</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/LProkhorenkova" rel="noopener ugc nofollow" target="_blank"><em class="om">Liudmila Prokhorenkova</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/emaros96" rel="noopener ugc nofollow" target="_blank"><em class="om">Emanuele Rossi</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/HannesStaerk" rel="noopener ugc nofollow" target="_blank"><em class="om">Hannes Stärk</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/AlexanderTong7" rel="noopener ugc nofollow" target="_blank"><em class="om">Alex Tong</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/tsitsulin_" rel="noopener ugc nofollow" target="_blank"><em class="om">Anton Tsitsulin</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/PetarV_93" rel="noopener ugc nofollow" target="_blank"><em class="om">Petar Veličković</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/MinkaiX" rel="noopener ugc nofollow" target="_blank"><em class="om">Minkai Xu</em></a><em class="om">, and </em><a class="af hi" href="https://twitter.com/zhu_zhaocheng" rel="noopener ugc nofollow" target="_blank"><em class="om">Zhaocheng Zhu</em></a><em class="om">.</em></p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq on"><img src="../Images/187024de90ba49fdfd0f708cdc506312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Z5Ncv43RzAe1o-yI"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Geometric ML methods and applications filled the covers of high-profile journals in 2023 (Figure sources: the papers by <a class="af hi" href="https://www.nature.com/articles/s42256-023-00609-5" rel="noopener ugc nofollow" target="_blank">Wang et al.</a>, <a class="af hi" href="https://www.nature.com/articles/s42256-023-00684-8" rel="noopener ugc nofollow" target="_blank">Viñas et al.</a>, <a class="af hi" href="https://www.nature.com/articles/s42256-023-00716-3" rel="noopener ugc nofollow" target="_blank">Deng et al.</a>, <a class="af hi" href="https://www.nature.com/articles/s43588-023-00532-0" rel="noopener ugc nofollow" target="_blank">Weiss et al.</a>, <a class="af hi" href="https://www.nature.com/articles/s42256-023-00744-z" rel="noopener ugc nofollow" target="_blank">Lagemann et al.</a>, <a class="af hi" href="https://www.nature.com/articles/s43588-023-00563-7" rel="noopener ugc nofollow" target="_blank">Duan et al.</a>, and <a class="af hi" href="https://www.science.org/doi/10.1126/science.adi2336" rel="noopener ugc nofollow" target="_blank">Lam et al.</a>)</figcaption></figure></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><ol class=""><li id="69e5" class="nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol oo op oq bk"><a class="af hi" href="#2626" rel="noopener ugc nofollow">Structural Biology (Molecules &amp; Proteins)</a><br/>a. <a class="af hi" href="#2f16" rel="noopener ugc nofollow">A Structural Biologist’s Perspective</a><br/>b. <a class="af hi" href="#ade6" rel="noopener ugc nofollow">Industrial Perspective</a><br/>c. <a class="af hi" href="#7a08" rel="noopener ugc nofollow">Systems Biology</a></li><li id="76e6" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol oo op oq bk"><a class="af hi" href="#6211" rel="noopener ugc nofollow">Materials Science (Crystals)</a></li><li id="2d79" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol oo op oq bk"><a class="af hi" href="#0924" rel="noopener ugc nofollow">Molecular Dynamics &amp; ML Potentials</a></li><li id="9e7a" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol oo op oq bk"><a class="af hi" href="#34b8" rel="noopener ugc nofollow">Geometric Generative Models (Manifolds)</a></li><li id="d0af" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol oo op oq bk"><a class="af hi" href="#3f98" rel="noopener ugc nofollow">BIG Graphs, Scalability: When GNNs are too expensive</a></li><li id="8edc" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol oo op oq bk"><a class="af hi" href="#f6d7" rel="noopener ugc nofollow">Algorithmic Reasoning &amp; Alignment</a></li><li id="fe53" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol oo op oq bk"><a class="af hi" href="#5854" rel="noopener ugc nofollow">Knowledge Graphs: Inductive Reasoning is Solved?</a></li><li id="8c08" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol oo op oq bk"><a class="af hi" href="#add1" rel="noopener ugc nofollow">Temporal Graph Learning</a></li><li id="9292" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol oo op oq bk"><a class="af hi" href="#ad3d" rel="noopener ugc nofollow">LLMs + Graphs for Scientific Discovery</a></li><li id="8042" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol oo op oq bk"><a class="af hi" href="#3d00" rel="noopener ugc nofollow">Cool GNN Applications</a></li><li id="97f3" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol oo op oq bk"><a class="af hi" href="#986f" rel="noopener ugc nofollow">Geometric Wall Street Bulletin 💸</a></li></ol><p id="07c2" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">The legend we will be using throughout the text:<br/>🔥 hot topics <br/>💡 year’s highlight<br/>🏋️ challenges<br/>➡️ current/next developments <br/>🔮 predictions/speculations <br/>💰 financial transactions</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2626" class="ow ox fq bf oy oz pa gv pb pc pd gy pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Structural Biology (Molecules &amp; Proteins)</h1><p id="2b92" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk"><em class="om">Dominique Beaini (Valence), Joey Bose (Mila &amp; Dreamfold), Michael Bronstein (Oxford), Bruno Correia (EPFL), Michael Galkin (Intel), Kexin Huang (Stanford), Chaitanya Joshi (Cambridge), Andreas Loukas (Genentech), Luca Naef (VantAI), Hannes Stärk (MIT), Minkai Xu (Stanford)</em></p><blockquote class="px"><p id="d46e" class="py pz fq bf qa qb qc qd qe qf qg ol dx">Structural biology was definitely at the forefront of Geometric Deep Learning in 2023.</p></blockquote><p id="883d" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">Following the 2020 discovery of <a class="af hi" href="https://pubmed.ncbi.nlm.nih.gov/32084340/" rel="noopener ugc nofollow" target="_blank">halicin</a> as a potential new antibiotic, in 2023, two new antibiotics were discovered with the help of GNNs! First, it is <a class="af hi" href="https://www.nature.com/articles/s41589-023-01349-8" rel="noopener ugc nofollow" target="_blank">abaucin</a> (by McMaster and MIT), which targets a stubborn pathogen resistant to many drugs. Second, MIT and Harvard researchers <a class="af hi" href="https://www.nature.com/articles/s41586-023-06887-8" rel="noopener ugc nofollow" target="_blank">discovered a new structural class of antibiotics</a> where the screening process was supported by<a class="af hi" href="https://github.com/chemprop/chemprop" rel="noopener ugc nofollow" target="_blank"> ChemProp</a>, a suite of GNNs for molecular property prediction. We also observe a convergence of ML and experimental techniques (“lab-in the-loop”) in the recent work on <a class="af hi" href="https://www.science.org/doi/10.1126/science.adi1407" rel="noopener ugc nofollow" target="_blank">autonomous molecular discovery</a> (a trend we will also see in the Materials Design in the following sections).</p><p id="940c" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">Flow Matching</strong> has been one of the biggest generative ML trends of 2023, allowing for faster sampling and deterministic sampling trajectories compared to diffusion models. The most prominent examples of Flow Matching models we have seen in the biological applications are <strong class="ns ga">FoldFlow</strong> (<a class="af hi" href="https://arxiv.org/abs/2310.02391" rel="noopener ugc nofollow" target="_blank">Bose, Akhound-Sadegh, et al</a>.) for protein backbone generation, <strong class="ns ga">FlowSite</strong> (<a class="af hi" href="https://arxiv.org/abs/2310.05764" rel="noopener ugc nofollow" target="_blank">Stärk et al</a>.) for protein binding site design, and <strong class="ns ga">EquiFM</strong> (<a class="af hi" href="https://openreview.net/forum?id=hHUZ5V9XFu" rel="noopener ugc nofollow" target="_blank">Song, Gong, et al</a>.) for molecule generation.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qm"><img src="../Images/47b927a05ef39ae2fa830c5ff139d6f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UQCCpeJV1xdx7ncGx8CdZw.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx"><em class="qn">Conditional probability paths learned by different versions of FoldFlow, visualizing the rotation trajectory of a single residue by the action of SO(3) on its homogeneous space </em>𝕊²<em class="qn">. Figure source: </em><a class="af hi" href="https://arxiv.org/abs/2310.02391" rel="noopener ugc nofollow" target="_blank"><em class="qn">Bose, Akhound-Sadegh, et al</em></a><em class="qn">.</em></figcaption></figure><p id="aef9" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Efficient Flow Matching on complex geometries with necessary equivariances became possible thanks to a handful of theory papers including Riemannian Flow Matching (<a class="af hi" href="https://arxiv.org/abs/2302.03660" rel="noopener ugc nofollow" target="_blank">Chen and Lipman</a>), Minibatch Optimal Transport (<a class="af hi" href="https://arxiv.org/abs/2302.00482" rel="noopener ugc nofollow" target="_blank">Tong et al</a>), and Simulation-Free Schrödinger bridges (<a class="af hi" href="https://arxiv.org/abs/2307.03672" rel="noopener ugc nofollow" target="_blank">Tong, Malkin, Fatras, et al</a>). A great resource to learn Flow Matching with code examples and notebooks is the <a class="af hi" href="https://github.com/atong01/conditional-flow-matching" rel="noopener ugc nofollow" target="_blank">TorchCFM</a> repo on GitHub as well as talks by <a class="af hi" href="https://www.youtube.com/watch?v=5ZSwYogAxYg" rel="noopener ugc nofollow" target="_blank">Yaron Lipman</a>, <a class="af hi" href="https://www.youtube.com/watch?v=EPxDI0ytfQU" rel="noopener ugc nofollow" target="_blank">Joey Bose</a>, <a class="af hi" href="https://www.youtube.com/watch?v=Xl7YNR1-CN8" rel="noopener ugc nofollow" target="_blank">Hannes Stärk</a>, and <a class="af hi" href="https://www.youtube.com/watch?v=UhDtH7Ia9Ag" rel="noopener ugc nofollow" target="_blank">Alex Tong</a>.</p><p id="d0e7" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">Diffusion models</strong> nevertheless continue to be the main workhorse of generative modeling in structural biology. In 2023, we saw several landmark works: <strong class="ns ga">FrameDiff</strong> (<a class="af hi" href="https://arxiv.org/abs/2302.02277" rel="noopener ugc nofollow" target="_blank">Yim, Trippe, De Bortoli, Mathieu, et al</a>) for protein backbone generation, <strong class="ns ga">EvoDiff</strong> (<a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.09.11.556673v1" rel="noopener ugc nofollow" target="_blank">Alamdari et al</a>) for generating protein sequences with discrete diffusion, <strong class="ns ga">AbDiffuser</strong> (<a class="af hi" href="https://arxiv.org/abs/2308.05027" rel="noopener ugc nofollow" target="_blank">Martinkus et al</a>) for full-atom antibody design with frame averaging and discrete diffusion (and with successful wet lab experiments), <strong class="ns ga">DiffMaSIF</strong> (<a class="af hi" href="https://www.mlsb.io/papers_2023/DiffMaSIF_Surface-based_Protein-Protein_Docking_with_Diffusion_Models.pdf" rel="noopener ugc nofollow" target="_blank">Sverrison, Akdel, et al</a>) and <strong class="ns ga">DiffDock-PP</strong> (<a class="af hi" href="https://arxiv.org/abs/2304.03889" rel="noopener ugc nofollow" target="_blank">Ketata, Laue, Mammadov, Stärk, et al</a>) for protein-protein docking, <strong class="ns ga">DiffPack</strong> (<a class="af hi" href="https://arxiv.org/abs/2306.01794" rel="noopener ugc nofollow" target="_blank">Zhang, Zhang, et al</a>) for side-chain packing, and the Baker Lab published the <strong class="ns ga">RFDiffusion</strong> <strong class="ns ga">all-atom</strong> version (<a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1.full" rel="noopener ugc nofollow" target="_blank">Krishna, Wang, Ahern, et al</a>). Among latent diffusion model (like Stable Diffusion in image generation applications), <strong class="ns ga">GeoLDM</strong> (<a class="af hi" href="https://arxiv.org/abs/2305.01140" rel="noopener ugc nofollow" target="_blank">Xu et al</a>) was the first for 3D molecule conformations, followed by <a class="af hi" href="https://openreview.net/forum?id=DP4NkPZOpD" rel="noopener ugc nofollow" target="_blank">OmniProt</a> for protein sequence-structure generation.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq on"><img src="../Images/67d90c64a0bad7a57857749bd5d60f3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gzs5s5iJug9SXAyF"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">FrameDiff: parameterization of the backbone frame with rotation, translation, and torsion angle for the oxygen atom. Figure Source: <a class="af hi" href="https://arxiv.org/abs/2302.02277" rel="noopener ugc nofollow" target="_blank">Yim, Trippe, De Bortoli, Mathieu, et al</a></figcaption></figure><p id="967f" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Finally, Google DeepMind and Isomorphic Labs <a class="af hi" href="https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold" rel="noopener ugc nofollow" target="_blank">announced</a> <strong class="ns ga">AlphaFold 2.3</strong> — the latest iteration is significantly improving upon the baselines in 3 tasks: docking benchmarks (almost 2× better than DiffDock on the new <a class="af hi" href="https://arxiv.org/abs/2308.05777" rel="noopener ugc nofollow" target="_blank">PoseBusters</a> benchmark), protein-nucleic acid interactions, and antibody-antigen prediction.</p><p id="205a" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Chaitanya Joshi (Cambridge)</em></strong></p><p id="bec7" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">💡There have been two emerging trends for biomolecular modeling and design that I am very excited about in 2023:</p><p id="0990" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ Going from protein structure prediction to conformational ensemble generation. There were several interesting approaches to the problem, including <a class="af hi" href="https://www.nature.com/articles/s41586-023-06832-9" rel="noopener ugc nofollow" target="_blank">AlphaFold with MSA clustering</a>, <a class="af hi" href="https://www.nature.com/articles/s41467-023-36443-x" rel="noopener ugc nofollow" target="_blank">idpGAN</a>, <a class="af hi" href="https://arxiv.org/abs/2306.05445" rel="noopener ugc nofollow" target="_blank">Distributional Graphormer</a> (a diffusion model), and <a class="af hi" href="https://www.mlsb.io/papers_2023/AlphaFold_Meets_Flow_Matching_for_Generating_Protein_Ensembles.pdf" rel="noopener ugc nofollow" target="_blank">AlphaFold Meets Flow Matching for Generating Protein Ensembles</a>.</p><p id="cc04" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ Modelling of biomolecular complexes and design of biomolecular interactions among proteins + X: <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1.full" rel="noopener ugc nofollow" target="_blank">RFdiffusion all-atom</a> and <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.12.22.573103v1.full" rel="noopener ugc nofollow" target="_blank">Ligand MPNN</a>, both from the Baker Lab, are representative examples of the trend towards designing interactions. The new in-development <a class="af hi" href="https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold" rel="noopener ugc nofollow" target="_blank">AlphaFold report</a> claims that a unified structure prediction model can outperform or match specialised models across solo protein and protein complex structure prediction as well as protein-ligand and protein-nucleic acid co-folding.</p><blockquote class="px"><p id="84a5" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“However, for all the exciting methodology development in biomolecular modelling and design, perhaps the biggest lesson for the ML community this year should be to focus more on meaningful <strong class="al">in-silico evaluation</strong> and, if possible, <strong class="al">experimental validation</strong>.” — <strong class="al">Chaitanya Joshi </strong>(Cambridge)</p></blockquote><p id="fcba" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">1️⃣ In early 2023, Guolin Ke’s team at DP Technology released two excellent re-evaluation papers highlighting how we may have been largely overestimating the performance of prominent geometric deep learning-based methods for molecular <a class="af hi" href="https://arxiv.org/abs/2302.07061" rel="noopener ugc nofollow" target="_blank">conformation generation</a> and <a class="af hi" href="https://arxiv.org/abs/2302.07134" rel="noopener ugc nofollow" target="_blank">docking</a> w.r.t. traditional baselines.</p><p id="e150" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ <a class="af hi" href="https://arxiv.org/abs/2308.07413" rel="noopener ugc nofollow" target="_blank">PoseCheck</a> and <a class="af hi" href="https://arxiv.org/abs/2308.05777" rel="noopener ugc nofollow" target="_blank">PoseBusters</a> shed further light on the failure modes of current molecular generation and docking methods. Critically, generated molecules and their 3D poses are often ‘nonphysical’ and contain steric clashes, hydrogen placement issues, and high strain energies.</p><p id="7a5a" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">3️⃣ Very few papers attempt any experimental validation of new ML ideas. Perhaps collaborating with a wet lab is challenging for those focussed on new methodology development, but I hope that us ML-ers, as a community, will at least be a lot more cautious about the in-silico evaluation metrics we are constantly pushing as we create new models.</p><p id="5c95" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Hannes Stärk (MIT)</em></strong></p><p id="6ac5" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">💡I am reading quite some hype here about Flow Matching, stochastic interpolants, and Rectified Flows (I will call them “Bridge Matching,” or “BM”). I do not think there is much value in just replacing diffusion models with BM in all the existing applications. For pure generative modeling, the main BM advantage is simplicity.</p><p id="7ced" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">I think we should instead be excited about BM for the new capabilities it unlocks. For example, training bridges between arbitrary distributions in a simulation-free manner (what are the best applications for this? I basically only saw <a class="af hi" href="https://arxiv.org/abs/2308.16212" rel="noopener ugc nofollow" target="_blank">retrosynthesis</a> so far.) or solving OT problems as in <a class="af hi" href="https://arxiv.org/abs/2303.16852" rel="noopener ugc nofollow" target="_blank">DSBM</a> that does so for fluid flow downscaling. Maybe a lot of tools emerged in 2023 (also let us mention <a class="af hi" href="https://arxiv.org/abs/2310.03695" rel="noopener ugc nofollow" target="_blank">BM with multiple marginals</a>), and in 2024, the community will make good use of them?</p><p id="1aab" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Joey Bose (Mila &amp; Dreamfold)</em></strong></p><p id="5866" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">💡 This year we have really seen the rise of geometric generative models from theory to practice. A few standouts for me include <a class="af hi" href="https://arxiv.org/abs/2302.03660" rel="noopener ugc nofollow" target="_blank">Riemannian Flow Matching</a> — in general any paper by Ricky Chen and Yaron Lipman on these topics is a must-read — and FrameDiff from <a class="af hi" href="https://arxiv.org/abs/2302.02277" rel="noopener ugc nofollow" target="_blank">Yim et. al</a> which introduced a lot of the important machinery for protein backbone generation. Of course, standing on the shoulders of both RFM and FrameDIff, we built <a class="af hi" href="https://arxiv.org/abs/2310.02391" rel="noopener ugc nofollow" target="_blank">FoldFlow</a>, a cooler flow-matching approach to protein generative models.</p><blockquote class="px"><p id="a876" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“Looking ahead, I foresee a lot <strong class="al">more flow matching</strong>-based approaches coming into use. They are better for proteins and longer sequences and can start from any source distribution.” — Joey Bose (Mila &amp; Dreamfold)</p></blockquote><p id="d756" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">🔮 Moreover, I suspect we will soon see <strong class="ns ga">multi-modal generative models</strong> in this space, such as discrete + continuous models and also conditional models in the same vein as text-conditioned diffusion models for images. Perhaps, we might even see <strong class="ns ga">latent generative models</strong> here given that they scale so well!</p><p id="60b0" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Minkai Xu (Stanford)</em></strong></p><blockquote class="px"><p id="b590" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“This year, the community has further pushed forward the geometric generative models for 3D molecular generation in many perspectives.” — Minkai Xu (Stanford)</p></blockquote><p id="9c09" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk"><strong class="ns ga">Flow matching</strong>: Ricky and Yaron proposed the Flow Matching method as an alternative to the widely used diffusion models, and EquiFM (<a class="af hi" href="https://openreview.net/forum?id=hHUZ5V9XFu" rel="noopener ugc nofollow" target="_blank">Song et al</a> and <a class="af hi" href="https://arxiv.org/abs/2306.15030" rel="noopener ugc nofollow" target="_blank">Klein et al</a>) realize the variant for 3D molecule generation by parameterizing the flow dynamics with equivariant GNNs. In the meantime, <a class="af hi" href="https://arxiv.org/pdf/2310.05297.pdf" rel="noopener ugc nofollow" target="_blank">FrameFlow</a> and <a class="af hi" href="https://arxiv.org/abs/2310.02391" rel="noopener ugc nofollow" target="_blank">FoldFlow</a> construct FM models for protein generation.</p><p id="52c0" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮Moving forward similar to vision and text domain, people begin to explore generation in the lower-dimensional latent space instead of the complex original data space (<strong class="ns ga">latent generative models</strong>). GeoLDM (<a class="af hi" href="https://arxiv.org/abs/2305.01140" rel="noopener ugc nofollow" target="_blank">Xu et al</a>) proposed the first latent diffusion model (like Stable Diffusion in CV) for 3D molecule generation, while <a class="af hi" href="https://arxiv.org/abs/2305.04120" rel="noopener ugc nofollow" target="_blank">Fu et al</a> enjoys similar modeling formulation for large protein generation.</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2f16" class="ow ox fq bf oy oz pa gv pb pc pd gy pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">A Structural Biologist’s Perspective</h1><p id="42d0" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk"><em class="om">Bruno Correia (EPFL)</em></p><blockquote class="px"><p id="f6a1" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“Current generative models still create “garbage” outputs that violate many of the physical and chemical properties that molecules are known to have. The advantage of current generative models is, of course, their speed which affords them the possibility of generating many samples, which then brings to front and center the ability to filter the best generated samples, which in the case of protein design has benefited immensely from the transformative development of AlphaFold2.” — Bruno Correia (EPFL)</p></blockquote><p id="c6f5" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">➡️ The next challenge to the community will perhaps be how to infuse generative models with <strong class="ns ga">meaningful physical and chemical priors</strong> to enhance sampling performance and generalization. Interestingly, we have not seen the same remarkable advances (experimentally validated) in applications to small molecule design, which we hope to see during 2024.</p><p id="e8f5" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ <strong class="ns ga">The rise of multimodal models.</strong> Generally in biological-related tasks data sparsity is a given and as such strategies to extract the most signal out of the data are essential. One way to try to overcome such limitations is to improve the expressiveness of the data representations and maybe this way obtain more performant neural networks. Likely in the short term, we will be able to explore architectures that encompass several types of representations of the objects of interest and harness the best predictions for the evermore complex tasks we are facing as progressively more of the basic problems get solved. This notion of multimodality is of course intimately related to the overall aim of having models with stronger priors, that in a generative context, honour fundamental constraints of the objects of interest.</p><p id="c12b" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ <strong class="ns ga">The models that know everything</strong>. As the power of machine learning models improves we clearly tend to request a more multi-objective optimization when it comes to attempting to solve real life problems. Taking as an example small molecule generation, thinking from a biochemical perspective the drug design problem starts by having a target to which a small molecule binds, therefore one of the first and most important constraints is that the generative process ought to be conditioned to the protein pocket. However, such a constraint may not be enough to create real small molecules as many of such chemicals are simply impossible or very hard to synthesize, and, therefore, a model that has notions of chemical synthesizability and can integrate such constraints in the search space would be much more useful.</p><p id="7d01" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ <strong class="ns ga">From chemotype to phenotype</strong>. On the grounds of data representation, atomic graph structures together with vector embeddings have reached remarkable results, particularly in the search for new antibiotics. Making accurate predictions of which chemical structures have antimicrobial activity, broadly speaking, is an exercise of phenotype prediction from chemical structure. Due to the simplicity of the approaches used and the impressive results obtained, one would expect that more sophisticated data representations on the molecule end and perhaps together also with richer phenotype assignment could give critical contributions to such an important problem in drug development.</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="ade6" class="ow ox fq bf oy oz pa gv pb pc pd gy pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Industrial perspective</h1><p id="36f6" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk"><strong class="ns ga"><em class="om">Luca Naef (VantAI)</em></strong></p><p id="59f2" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔥<em class="om">What are the biggest advancements in the field you noticed in 2023?</em></p><p id="e4f6" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ <strong class="ns ga">Increasing multi-modality &amp; modularity </strong>— as shown by the emergence of initial co-folding methods for both proteins &amp; small molecules, diffusion and non-diffusion-based, to extend on AF2 success: <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2022.12.20.521309v1.full.pdf" rel="noopener ugc nofollow" target="_blank">DiffusionProteinLigand</a> in the last days of 2022 and <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1" rel="noopener ugc nofollow" target="_blank">RFDiffusion</a>, <a class="af hi" href="https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold" rel="noopener ugc nofollow" target="_blank">AlphaFold2</a> and <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.11.03.565471v1" rel="noopener ugc nofollow" target="_blank">Umol</a> by end of 2023. We are also seeing models that have sequence &amp; structure co-trained: <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.10.01.560349v2" rel="noopener ugc nofollow" target="_blank">SAProt</a>, <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.07.23.550085v1" rel="noopener ugc nofollow" target="_blank">ProstT5</a>, and sequence, structure &amp; surface co-trained with <a class="af hi" href="https://www.mlsb.io/papers_2023/Pre-training_Sequence_Structure_and_Surface_Features_for_Comprehensive_Protein_Representation_Learning.pdf" rel="noopener ugc nofollow" target="_blank">ProteinINR</a>. There is a general revival of surface-based methods after a quieter 2021 and 2022: <a class="af hi" href="https://www.mlsb.io/papers_2023/DiffMaSIF_Surface-based_Protein-Protein_Docking_with_Diffusion_Models.pdf" rel="noopener ugc nofollow" target="_blank">DiffMasif</a>, <a class="af hi" href="https://arxiv.org/abs/2311.17050" rel="noopener ugc nofollow" target="_blank">SurfDock</a>, and <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.12.03.567710v1" rel="noopener ugc nofollow" target="_blank">ShapeProt</a>.</p><p id="786b" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ <strong class="ns ga">Datasets and benchmarks</strong>. Datasets, especially synthetic/computationally derived: <a class="af hi" href="https://academic.oup.com/nar/article/52/D1/D384/7438909" rel="noopener ugc nofollow" target="_blank">ATLAS</a> and the <a class="af hi" href="https://mddbr.eu/" rel="noopener ugc nofollow" target="_blank">MDDB</a> for protein dynamics. <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.05.24.542082v1" rel="noopener ugc nofollow" target="_blank">MISATO</a>, <a class="af hi" href="https://www.nature.com/articles/s41597-022-01882-6" rel="noopener ugc nofollow" target="_blank">SPICE</a>, <a class="af hi" href="https://www.nature.com/articles/s41597-023-02443-1" rel="noopener ugc nofollow" target="_blank">Splinter</a> for protein-ligand complexes, <a class="af hi" href="https://arxiv.org/abs/2311.01135" rel="noopener ugc nofollow" target="_blank">QM1B</a> for molecular properties. PINDER: large protein-protein docking dataset with matched apo/predicted pairs and benchmark suite with retrained docking models. <a class="af hi" href="https://chanzuckerberg.github.io/cryoet-data-portal/index.html#" rel="noopener ugc nofollow" target="_blank">CryoET data portal</a> for CryoET. And a whole host of welcome benchmarks: PINDER, <a class="af hi" href="https://arxiv.org/abs/2308.05777" rel="noopener ugc nofollow" target="_blank">PoseBusters</a>, and <a class="af hi" href="https://arxiv.org/abs/2308.07413" rel="noopener ugc nofollow" target="_blank">PoseCheck</a>, with a focus on more rigorous and practically relevant settings.</p><p id="ccfd" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">3️⃣ <strong class="ns ga">Creative pre-training strategies</strong> to get around the sparsity of diverse protein-ligand complexes. Van-der-mers training (<a class="af hi" href="https://openreview.net/forum?id=UfBIxpTK10" rel="noopener ugc nofollow" target="_blank">DockGen</a>) &amp; sidechain training strategies in <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1" rel="noopener ugc nofollow" target="_blank">RF-AA</a> and pre-training on ligand-only complexes in CCD in <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1" rel="noopener ugc nofollow" target="_blank">RF-AA</a>. Multi-task pre-training <a class="af hi" href="https://openreview.net/forum?id=6K2RM6wVqKu" rel="noopener ugc nofollow" target="_blank">Unimol</a> and others.</p><p id="8414" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🏋️ <em class="om">What are the open challenges that researchers might overlook?</em></p><p id="11ae" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ <strong class="ns ga">Generalization. </strong><a class="af hi" href="https://openreview.net/forum?id=UfBIxpTK10" rel="noopener ugc nofollow" target="_blank">DockGen</a><strong class="ns ga"> </strong>showed that current state-of-the-art protein-ligand docking models completely lose predictability when asked to generalise towards novel protein domains. We see a similar phenomenon in the <a class="af hi" href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/a-glimpse-of-the-next-generation-of-alphafold/alphafold_latest_oct2023.pdf" rel="noopener ugc nofollow" target="_blank">AlphaFold-lastest report</a>, where performance on novel proteins &amp; ligands drops heavily to below biophysics-based baselines (which have access to holo structures), despite very generous definitions of novel protein &amp; ligand. This indicates that existing approaches might still largely rely on memorization, an observation that has been extensively argued over the <a class="af hi" href="https://pubs.acs.org/doi/10.1021/acs.jmedchem.2c00487" rel="noopener ugc nofollow" target="_blank">years</a></p><p id="74b8" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ <strong class="ns ga">The curse of (simple) baselines. </strong>A recurring topic over the years, 2023 has again shown what industry practitioners have long known: in many practical problems such as molecular generation, property prediction, docking, and conformer prediction, simple baselines or classical approaches often still outperform ML-based approaches in practice. This has been documented increasingly in 2023 by <a class="af hi" href="https://arxiv.org/abs/2310.09267" rel="noopener ugc nofollow" target="_blank">Tripp et al.</a><strong class="ns ga">, </strong><a class="af hi" href="https://arxiv.org/abs/2302.07134" rel="noopener ugc nofollow" target="_blank">Yu et al.</a>, <a class="af hi" href="https://arxiv.org/abs/2302.07061" rel="noopener ugc nofollow" target="_blank">Zhou et al.</a></p><p id="93cd" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮 <em class="om">Predictions for 2024!</em></p><blockquote class="px"><p id="f29c" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“In 2024, data sparsity will remain top of mind and we will see a lot of smart ways to use models to generate synthetic training data. Self-distillation in AlphaFold2 served as a big inspiration, Confidence Bootstrapping in <a class="af hi" href="https://openreview.net/forum?id=UfBIxpTK10" rel="noopener ugc nofollow" target="_blank">DockGen</a>, leveraging the insight that we now have sufficiently powerful models that can score poses but not always generate them, first realised in <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2022.03.11.484043v1" rel="noopener ugc nofollow" target="_blank">2022</a>.” — Luca Naef (VantAI)</p></blockquote><p id="9ec6" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">2️⃣ We will see more biological/chemical assays purpose-built for ML or only making sense in a machine learning context (i.e., might not lead to biological insight by themselves but be primarily useful for training models). An example from 2023 is the large-scale protein folding experiments by <a class="af hi" href="https://www.nature.com/articles/s41586-023-06328-6" rel="noopener ugc nofollow" target="_blank">Tsuboyama et al.</a> This move might be driven by techbio startups, where we have seen the first foundation models built on such ML-purpose-built assays for structural biology with e.g. <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.12.13.571579v1" rel="noopener ugc nofollow" target="_blank">ATOM-1</a>.</p><p id="4422" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Andreas Loukas (Prescient Design, part of Genentech)</em></strong></p><p id="98fc" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔥 <em class="om">What are the biggest advancements in the field you noticed in 2023?</em></p><blockquote class="px"><p id="06b5" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“In 2023, we started to see some of the challenges of equivariant generation and representation for proteins to be resolved through diffusion models.” — Andreas Loukas (Prescient Design)</p></blockquote><p id="534c" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">1️⃣ We also noticed a <strong class="ns ga">shift towards approaches that model and generate molecular systems at higher fidelity</strong>. For instance, the most recent models adopt a fully end-to-end approach by generating backbone, sequence and side-chains jointly (<a class="af hi" href="https://openreview.net/pdf?id=7GyYpomkEa" rel="noopener ugc nofollow" target="_blank">AbDiffuser</a>, <a class="af hi" href="https://arxiv.org/pdf/2302.00203.pdf" rel="noopener ugc nofollow" target="_blank">dyMEAN</a>) or at least solve the problem in two steps but with a partially joint model (<a class="af hi" href="https://www.nature.com/articles/s41586-023-06728-8" rel="noopener ugc nofollow" target="_blank">Chroma</a>); as compared to backbone generation followed by inverse folding as in <a class="af hi" href="https://www.nature.com/articles/s41586-023-06415-8" rel="noopener ugc nofollow" target="_blank">RFDiffusion</a> and <a class="af hi" href="https://openreview.net/pdf?id=m8OUBymxwv" rel="noopener ugc nofollow" target="_blank">FrameDiff</a>. Other attempts to improve the modelling fidelity can be found in the latest updates to co-folding tools like <a class="af hi" href="https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold" rel="noopener ugc nofollow" target="_blank">AlphaFold2</a> and <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1" rel="noopener ugc nofollow" target="_blank">RFDiffusion</a> which render them sensitive to non-protein components (ligands, prosthetic groups, cofactors); as well as in papers that attempt to account for conformational dynamics (see discussion above). In my view, this line of work is essential because the binding behaviour of molecular systems can be very sensitive to how atoms are placed, move, and interact.</p><p id="00cc" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ In 2023, many works also attempted to get a handle on <strong class="ns ga">binding affinity</strong> by learning to predict the effect of mutations of a known crystal by pre-training on large corpora, such as computationally predicted mutations (<a class="af hi" href="https://github.com/oxpig/Graphinity" rel="noopener ugc nofollow" target="_blank">graphinity</a>), and on side-tasks, such as <a class="af hi" href="https://openreview.net/pdf?id=_X9Yl1K2mD" rel="noopener ugc nofollow" target="_blank">rotamer density estimation</a>. The obtained results are encouraging as they can significantly outperform semi-empirical baselines like Rosetta and FoldX. However, there is still significant work to be done to render these models reliable for binding affinity prediction.</p><p id="19d2" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">3️⃣ I have further observed a growing recognition of <strong class="ns ga">protein Language Models (pLMs)<em class="om"> </em></strong>and specifically <a class="af hi" href="https://www.science.org/doi/10.1126/science.ade2574" rel="noopener ugc nofollow" target="_blank">ESM</a> as valuable tools, even among those who primarily favour geometric deep learning. These embeddings are used to help docking models, allow the construction of simple yet competitive predictive models for binding affinity prediction (<a class="af hi" href="https://www.nature.com/articles/s41467-023-39022-2" rel="noopener ugc nofollow" target="_blank">Li et al 2023</a>), and can generally offer an efficient method to create residue representations for GNNs that are informed by the extensive proteome data without the need for extensive pretraining (<a class="af hi" href="https://www.mlsb.io/papers_2023/Evaluating_Representation_Learning_on_the_Protein_Structure_Universe.pdf" rel="noopener ugc nofollow" target="_blank">Jamasb et al 2023</a>). However, I do maintain a concern regarding the use of pLMs: it is unclear whether their effectiveness is due to data leakage or genuine generalisation. This is particularly pertinent when evaluating models on tasks like amino-acid recovery in inverse folding and conditional CDR design, where distinguishing between these two factors is crucial.</p><p id="fac6" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🏋️ <em class="om">What are the open challenges that researchers might overlook?</em></p><p id="d483" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ Working with <strong class="ns ga">energetically relaxed crystal structures</strong> (and, even worse, folded structures) can significantly affect the performance of downstream predictive models. This is especially true for the prediction of protein-protein interactions (PPIs). In my experience, the performance of PPI predictors severely deteriorates when they are given a relaxed structure as opposed to the binding (holo) crystalised structure.</p><p id="83fe" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ Though successful <em class="om">in silico </em>antibody design has the capacity to revolutionise drug design, <strong class="ns ga">general protein models are not (yet?) as good at folding, docking or generating antibodies as antibody-specific models are</strong>. This is perhaps due to the low conformational variability of the antibody fold and the distinct binding mode between antibodies and antigens (loop-mediated interactions that can involve a non-negligible entropic component). Perhaps for the same reasons, the <em class="om">de novo</em> design of antibody binders (that I define as 0-shot generation of an antibody that binds to a previously unseen epitope) remains an open problem. Currently, experimentally confirmed cases of <em class="om">de novo</em> binders involve mostly stable proteins, like <a class="af hi" href="https://www.nature.com/articles/s41586-023-06415-8" rel="noopener ugc nofollow" target="_blank">alpha-helical bundles</a>, that are common in the PDB and harbour interfaces that differ substantially from epitope-paratope interactions.</p><p id="ebcf" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">3️⃣ <strong class="ns ga">We are still lacking a general-purpose proxy for binding free energy</strong><em class="om">.</em> The main issue here is the lack of high-quality data of sufficient size and diversity (esp. co-crystal structures). We should therefore be cognizant of the limitations of any such learned proxy for any model evaluation: though predicted binding scores that are out of distribution of known binders is a clear signal that something is off, we should avoid the typical pitfall of trying to demonstrate the superiority of our model in an empirical evaluation by showing how it leads to even higher scores.</p><p id="d94b" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Dominique Beaini (Valence Labs, part of Recursion)</em></strong></p><blockquote class="px"><p id="5928" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“I’m excited to see a very large community being built around the problem of drug discovery, and I feel we are on the brink of a new revolution in the speed and efficiency of discovering drugs.” — Dominique Beaini (Valence Labs)</p></blockquote><p id="c1cb" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk"><em class="om">What work got me excited in 2023?</em></p><p id="ae22" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">I am confident that machine learning will allow us to tackle rare diseases quickly, stop the next COVID-X pandemic before it can spread, and live longer and healthier. But there’s a lot of work to be done and there are a lot of challenges ahead, some bumps in the road, and some canyons on the way. Speaking of communities, you can visit the <a class="af hi" href="https://portal.valencelabs.com/" rel="noopener ugc nofollow" target="_blank">Valence Portal</a> to keep up-to-date with the 🔥 new in ML for drug discovery.</p><p id="1512" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><em class="om">What are the hard questions for 2024?</em></p><p id="d190" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">⚛️ <strong class="ns ga">A new generation of quantum mechanics.</strong> Machine learning force-fields, often based on equivariant and invariant GNNs, have been promising us a treasure. The treasure of the precision of density functional theory, but thousands of times faster and at the scale of entire proteins. Although some steps were made in this direction with <a class="af hi" href="https://link.springer.com/chapter/10.1007/978-3-031-32041-5_12" rel="noopener ugc nofollow" target="_blank">Allegro</a> and <a class="af hi" href="https://arxiv.org/pdf/2401.00096.pdf" rel="noopener ugc nofollow" target="_blank">MACE-MP</a>, current models do not generalize well to unseen settings and very large molecules, and they are still too slow to be applicable on the timescale that is needed 🐢. For the generalization, I believe that bigger and more diverse datasets are the most important stepping stones. For the computation time, I believe we will see models that are less enforcing of the equivariance, such as <a class="af hi" href="https://arxiv.org/pdf/2305.05577.pdf" rel="noopener ugc nofollow" target="_blank">FAENet</a>. But efficient sampling methods will play a bigger role: spatial-sampling such as using <a class="af hi" href="https://arxiv.org/abs/2210.01776" rel="noopener ugc nofollow" target="_blank">DiffDock</a> to get more interesting starting points and time-sampling such as <a class="af hi" href="https://www.microsoft.com/en-us/research/publication/timewarp-transferable-acceleration-of-molecular-dynamics-by-learning-time-coarsened-dynamics/" rel="noopener ugc nofollow" target="_blank">TimeWarp</a> to avoid simulating every frame. I’m really excited by the big STEBS 👣 awaiting us in 2024: Spatio-temporal equivariant Boltzmann samplers.</p><p id="dc59" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🕸️ <strong class="ns ga">Everything is connected. Biology is inherently multimodal 🙋🐁 🧫🧬🧪.</strong> One cannot simply decouple the molecule from the rest of the biological system. Of course, that’s how ML for drug discovery was done in the past: simply build a model of the molecular graph and fit it to experimental data. But we have reached a critical point 🛑, no matter how many trillion parameters are in the GNN model is, and how much data are used to train it, and how many experts are mixtured together. It is time to bring biology into the mix, and the most straightforward way is with multi-modal models. One method is to condition the output of the GNNs with the target protein sequences such as <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.09.13.557595v4.abstract" rel="noopener ugc nofollow" target="_blank">MocFormer</a>. Another is to use microscopy images or transcriptomics to better inform the model of the biological signature of molecules such as <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.11.12.566777v1.full" rel="noopener ugc nofollow" target="_blank">TranSiGen</a>. Yet another is to use LLMs to embed contextual information about the tasks such as <a class="af hi" href="https://arxiv.org/pdf/2401.04478.pdf" rel="noopener ugc nofollow" target="_blank">TwinBooster</a>. Or even better, combining all of these together 🤯, but this could take years. The main issue for the broader community seems to be the availability of large amounts of quality and standardized data, but fortunately, this is not an issue for Valence.</p><p id="f3c3" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">🔬 Relating biological knowledge and observables. </strong>Humans have been trying to map biology for a long time, building relational maps for genes 🧬, protein-protein interactions 🔄, metabolic pathways 🔀, etc. I invite you to read this <a class="af hi" href="https://academic.oup.com/bib/article/23/6/bbac404/6712301" rel="noopener ugc nofollow" target="_blank">review of knowledge graphs for drug discovery</a>. But all this knowledge often sits unused and ignored by the ML community. I feel that this is an area where GNNs for knowledge graphs could prove very useful, especially in 2024, and it could provide another modality for the 🕸️ point above. Considering that human knowledge is incomplete, we can instead recover relational maps from foundational models. This is the route taken by <a class="af hi" href="https://arxiv.org/abs/2309.16064" rel="noopener ugc nofollow" target="_blank">Phenom1 </a>when trying to recall known genetic relationships. However, having to deal with various knowledge databases is an extremely complex task that we can’t expect most ML scientists to be able to tackle alone. But with the help of artificial assistants like <a class="af hi" href="https://www.valencelabs.com/lowe" rel="noopener ugc nofollow" target="_blank">LOWE</a>, this can be done in a matter of seconds.</p><p id="d6cf" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">🏆 Benchmarks, benchmarks, benchmarks.</strong> I can’t repeat the word <strong class="ns ga"><em class="om">benchmark</em></strong> enough. Alas, benchmarks will stay the unloved kid on the ML block 🫥. But if the word benchmark is uncool, its cousin <strong class="ns ga"><em class="om">competition</em></strong> is way cooler 😎! Just as the <a class="af hi" href="https://ogb.stanford.edu/docs/lsc/" rel="noopener ugc nofollow" target="_blank">OGB-LSC</a> competition and <a class="af hi" href="https://opencatalystproject.org/challenge.html" rel="noopener ugc nofollow" target="_blank">Open Catalyst</a> challenge played a major role for the GNN community, it is now time for a new series of competitions 🥇. We even got the <a class="af hi" href="https://tgb.complexdatalab.com/" rel="noopener ugc nofollow" target="_blank">TGB (Temporal graph benchmark)</a> recently. If you were at NeurIPS’23, then you probably heard of Polaris coming up early 2024 ✨. Polaris is a consortium of multiple pharma and academic groups trying to improve the quality of available molecular benchmarks to better represent real drug discovery. Perhaps we’ll even see a benchmark suitable for molecular graph generation instead of optimizing QED and cLogP, but I wouldn’t hold my breath, I have been waiting for years. What kind of new, crazy competition will light up the GDL community this year 🤔?</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="7a08" class="ow ox fq bf oy oz pa gv pb pc pd gy pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Systems Biology</h1><p id="addb" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk"><strong class="ns ga"><em class="om">Kexin Huang (Stanford)</em></strong></p><p id="9bf7" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Biology is an interconnected, multi-scale, and multi-modal system. Effective modeling of this system can not only unravel fundamental biological questions but also significantly impact therapeutic discovery. The most natural data format for encapsulating this system is a relational database or a heterogeneous graph. This graph stores data from decades of wet lab experiments across various biological modalities, scaling up to billions of data points.</p><blockquote class="px"><p id="9b13" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“In 2023, we witnessed a range of innovative applications using GNNs on these biological system graphs. These applications have unlocked new biomedical capabilities and answered critical biological queries.” — Kexin Huang (Stanford)</p></blockquote><p id="0c60" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">1️⃣ One particularly exciting field is <strong class="ns ga">perturbative biology</strong>. Understanding the outcomes of perturbations can lead to advancements in cell reprogramming, target discovery, and synthetic lethality, among others. In 2023, <a class="af hi" href="https://www.nature.com/articles/s41587-023-01905-6" rel="noopener ugc nofollow" target="_blank">GEARS</a> applies GNN to gene perturbational relational graphs and it predicts outcomes of genetic perturbations that have not been observed before.</p><p id="1d8d" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ Another cool application concerns <strong class="ns ga">protein representation</strong>. While current protein representations are fixed and static, we recognize that the same protein can exhibit different functions in varying cellular contexts. <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.07.18.549602v1" rel="noopener ugc nofollow" target="_blank">PINNACLE</a> uses GNN on protein interaction networks to contextualize protein embeddings. This approach has shown to enhance 3D structure-based protein representations and outperform existing context-free models in identifying therapeutic targets.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qo"><img src="../Images/2a530ef72f314922fe3608ed1c120dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LdQPRd76Wnb9BDeH"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">PINNACLE has protein-, cell type-, and tissue-level attention mechanisms that enable the algorithm to generate contextualized representations of proteins, cell types, and tissues in a single unified embedding space. Source: <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2023.07.18.549602v1" rel="noopener ugc nofollow" target="_blank">Li et al</a></figcaption></figure><p id="bb1a" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">3️⃣ GNNs also have shown a vital role in <strong class="ns ga">diagnosing rare diseases</strong>. <a class="af hi" href="https://www.medrxiv.org/content/10.1101/2022.12.07.22283238v1" rel="noopener ugc nofollow" target="_blank">SHEPHERD</a> utilizes GNN over massive knowledge graph to encode extensive biological knowledge into the ML model and is shown to facilitate causal gene discovery, identify ‘patients-like-me’ with similar genes or diseases, and provide interpretable insights into novel disease manifestations.</p><p id="4902" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ Moving beyond predictions, understanding the underlying mechanisms of biological phenomena is crucial. <strong class="ns ga">Graph XAI</strong> applied to system graphs is a natural fit for identifying mechanistic pathways. <a class="af hi" href="https://www.medrxiv.org/content/10.1101/2023.03.19.23287458v2" rel="noopener ugc nofollow" target="_blank">TxGNN</a>, for example, grounds drug-disease relation predictions in the biological system graph, generating multi-hop interpretable paths. These paths rationalize the potential of a drug in treating a specific disease. TxGNN designed <a class="af hi" href="http://txgnn.org/" rel="noopener ugc nofollow" target="_blank">visualizations</a> for these interpretations and conducted user studies, proving their decision-making effectiveness for clinicians and biomedical scientists.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qp"><img src="../Images/e3f51bc7c5fb00a693d400b266dff34c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5frNFjOVtUNiUyvE"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">A web-based graphical user interface to support clinicians and scientists in exploring and analyzing the predictions and explanations generated by TxGNN. The ‘Control Panel‘ allows users to select the disease of interest and view the top-ranked TXGNN predictions for the query disease. The ‘edge threshold‘ module enables users to modify the sparsity of the explanation and thereby control the density of the multi-hop paths displayed. The ‘Drug Embedding‘ panel allows users to compare the position of a selected drug relative to the entire repurposing candidate library. The ‘Path Explanation‘ panel displays the biological relations that have been identified as crucial for TXGNN’s predictions regarding therapeutic use. Source: <a class="af hi" href="https://www.medrxiv.org/content/10.1101/2023.03.19.23287458v2" rel="noopener ugc nofollow" target="_blank">Huang, Chandar, et al</a></figcaption></figure><p id="185e" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ Foundation models in biology have predominantly been unimodal (focused on proteins, molecules, diseases, etc.), primarily due to the scarcity of paired data. <strong class="ns ga">Bridging across modalities</strong> to answer multi-modal queries is an exciting frontier. For example, <a class="af hi" href="https://openreview.net/forum?id=jJCeMiwHdH" rel="noopener ugc nofollow" target="_blank">BioBridge</a> leverages biological knowledge graphs to learn transformations across unimodal foundation models, enabling multi-modal behaviors.</p><p id="c6ae" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮 GNNs applied to system graphs have the potential to (1) encode vast biomedical knowledge, (2) bridge biological modalities, (3) provide mechanistic insights, and (4) contextualize biological entities. We anticipate even more groundbreaking applications of GNN in biology in 2024, addressing some of the most pressing questions in the field.</p><h2 id="2d81" class="qq ox fq bf oy qr qs qt pb qu qv qw pe nz qx qy qz od ra rb rc oh rd re rf fw bk"><strong class="al">Predictions from the 2023 post</strong></h2><p id="9315" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk">(1) performance improvements of diffusion models such as faster sampling and more efficient solvers;<br/>✅ yes, with flow matching</p><p id="f7a9" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">(2) more powerful conditional protein generation models;<br/>❌ Chroma and RFDiffusion are still on top</p><p id="5a83" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">(3) more successful applications of <a class="af hi" href="https://arxiv.org/abs/2111.09266" rel="noopener ugc nofollow" target="_blank">Generative Flow Networks</a> to molecules and proteins<br/>❌ yet to be seen</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6211" class="ow ox fq bf oy oz pa gv pb pc pd gy pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Materials Science (Crystals)</h1><p id="0cdd" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk"><em class="om">Michael Galkin (Intel) and Santiago Miret (Intel)</em></p><p id="39fa" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">In 2023, for a short period, all scientific news were talking only about <a class="af hi" href="https://en.wikipedia.org/wiki/LK-99" rel="noopener ugc nofollow" target="_blank">LK-99</a> — a supposed room-temperature superconductor created by a Korean team (spoiler: <a class="af hi" href="https://www.nature.com/articles/d41586-023-02585-7" rel="noopener ugc nofollow" target="_blank">it did not work as of now</a>).</p><blockquote class="px"><p id="ccad" class="py pz fq bf qa qb qc qd qe qf qg ol dx">This highlights the huge potential ML has in material science, where perhaps the biggest progress of the year has happened — we can now say that materials science and materials discovery are first-class citizens in the Geometric DL landscape.</p></blockquote><p id="26c6" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">💡The advances of Geometric DL applied to materials science and discovery saw significant advances across new modelling methods, creation of new benchmarks and datasets, automated design with generative methods, and identifying new research questions based on those advances.</p><p id="6512" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ Applications of geometric models as evaluation tools in automated discovery workflows. The <a class="af hi" href="https://github.com/IntelLabs/matsciml" rel="noopener ugc nofollow" target="_blank">Open MatSci ML Toolkit </a>consolidated all open-sourced crystal structures datasets leading to 1.5 million data points for ground-state structure calculations that are now easily available for model development. The<a class="af hi" href="https://arxiv.org/abs/2309.05934" rel="noopener ugc nofollow" target="_blank"> authors’ initial results</a> seem to indicate that merging datasets seems to improve performance if done attentively.</p><p id="e3ce" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ <a class="af hi" href="https://arxiv.org/abs/2308.14920" rel="noopener ugc nofollow" target="_blank">MatBench Discovery</a> is another good example of this integration of geometric models as an evaluation tool for crystal stability, which tests models’ predictions of the <strong class="ns ga">energy above hull</strong> for various crystal structures. The energy above hull is the most reliable approximation of crystal structure stability and also represents an improvement in metrics compared to formation energy or raw energy prediction which have practical limitations as stability metrics.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq rg"><img src="../Images/490c1c14cae95438a36595145e08625f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ryQ3qBUoPRyR4JZ_vuylOQ.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Universal potentials are more reliable classifiers because they exit the red triangle earliest. These lines show the rolling MAE on the WBM test set as the energy to the convex hull of the MP training set is varied, lower is better. The red-highlighted ’triangle of peril’ shows where the models are most likely to misclassify structures. As long as a model’s rolling MAE remains inside the triangle, its mean error is larger than the distance to the convex hull. If the model’s error for a given prediction happens to point towards the stability threshold at 0 eV from the hull (the plot’s center), its average error will change the stability classification of a material from true positive/negative to false negative/positive. The width of the ’rolling window’ box indicates the width over which errors hull distance prediction errors were averaged. Source: <a class="af hi" href="https://arxiv.org/abs/2308.14920" rel="noopener ugc nofollow" target="_blank">Riebesell et al</a></figcaption></figure><p id="66f6" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">3️⃣ In terms of new geometric models for crystal structure prediction, <strong class="ns ga">Crystal Hamiltonian Graph neural network</strong> (<a class="af hi" href="https://chgnet.lbl.gov/" rel="noopener ugc nofollow" target="_blank">CHGNet</a>, <a class="af hi" href="https://arxiv.org/abs/2302.14231" rel="noopener ugc nofollow" target="_blank">Deng et al</a>) is a new GNN trained on static and relaxation trajectories of Materials Project that shows quite competitive performance compared to prior methods. The development of CHGNet suggests that finding better training objectives will be as (if not more) important than the development of new methods as the intersection of materials science and geometric deep learning continues to grow.</p><p id="9d05" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔥 The other proof points of the further integration of Geometric DL and materials discovery are several massive works by big labs focused on crystal structure discovery with generative methods:</p><p id="f2b3" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ Google DeepMind released <a class="af hi" href="https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/" rel="noopener ugc nofollow" target="_blank"><strong class="ns ga">GNoME</strong></a> (Graph Networks for Materials Science by <a class="af hi" href="https://www.nature.com/articles/s41586-023-06735-9" rel="noopener ugc nofollow" target="_blank">Merchant et al</a>) as a successful example of an active learning pipeline for discovering new materials, and <a class="af hi" href="https://unified-materials.github.io/unimat/" rel="noopener ugc nofollow" target="_blank">UniMat</a> as an<em class="om"> ab initio</em> crystal generation model. Similar to the protein world, we see more examples of automated labs for materials science (“lab-in-the-loop”) such as the <a class="af hi" href="https://www.nature.com/articles/s41586-023-06734-w" rel="noopener ugc nofollow" target="_blank">A-Lab from UC Berkley</a>.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq on"><img src="../Images/bfe512fa260e4fa3114312d89c43d783.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AqWfkbgEvL_t02xy"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">The active learning loop of GNoME. Source: <a class="af hi" href="https://www.nature.com/articles/s41586-023-06735-9" rel="noopener ugc nofollow" target="_blank">Merchant et al.</a></figcaption></figure><p id="1cc3" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ Microsoft Research released <a class="af hi" href="https://www.microsoft.com/en-us/research/blog/mattergen-property-guided-materials-design/" rel="noopener ugc nofollow" target="_blank">MatterGen</a>, a generative model for unconditional and property-guided materials design, and <a class="af hi" href="https://distributionalgraphormer.github.io/" rel="noopener ugc nofollow" target="_blank">Distributional Graphormer</a>, a generative model trained to recover the equilibrium energy distribution of a molecule/protein/crystal.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq on"><img src="../Images/f9bb59d0a617dbbf35236c7e6d8b146c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7Pq4uOFCOICQHbEg"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Unconditional and conditional generation of MatterGen. Source: <a class="af hi" href="https://arxiv.org/abs/2312.03687" rel="noopener ugc nofollow" target="_blank">Zeni, Pinsler, Zügner, Fowler, Horton, et al.</a></figcaption></figure><p id="b3b9" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">3️⃣ Meta AI and CMU released the<a class="af hi" href="https://open-catalyst.metademolab.com/" rel="noopener ugc nofollow" target="_blank"> Open Catalyst Demo</a> where you can play around with relaxations (DFT approximations) of 11.5k catalyst materials on 86 adsorbates in 100 different configurations each (making it up to 100M combinations). The demo is powered by SOTA geometric models GemNet-OC and Equiformer-V2.</p><p id="f041" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Santiago Miret (Intel)</em></strong></p><p id="800d" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">While those works represent large-scale deployments of generative methods, there is also new work on using reinforcement learning (<a class="af hi" href="https://openreview.net/forum?id=VbjD8w2ctG" rel="noopener ugc nofollow" target="_blank">Govindarajan et al.</a>, <a class="af hi" href="https://openreview.net/forum?id=MNfVMjsL7S" rel="noopener ugc nofollow" target="_blank">Lacombe et al.</a>) and GFlowNets (<a class="af hi" href="https://openreview.net/forum?id=l167FjdPOv" rel="noopener ugc nofollow" target="_blank">Mistal et al.</a>, <a class="af hi" href="https://openreview.net/forum?id=dJuDv4MKLE" rel="noopener ugc nofollow" target="_blank">Nguyen et al.</a>) with geometric DL for crystal structure discovery as highlighted in the <a class="af hi" href="https://sites.google.com/view/ai4mat" rel="noopener ugc nofollow" target="_blank">AI for Accelerated Materials Design (AI4Mat)</a> workshop at NeurIPS’23. AI4Mat-2023 itself saw rapid expansion in participation with a 2× increase in the number of submitted and accepted papers and almost tripling in the number of attendees.</p><p id="f547" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">💡 Geometric DL and GNNs continue to be a major part of AI4Mat’s research content as we saw increased application of methods not only for property prediction but also for improving <strong class="ns ga">chemical synthesis</strong> and <strong class="ns ga">material characterization</strong>. One such promising example highlighted in the AI4Mat-2023 workshop is <strong class="ns ga">KREED</strong> (<a class="af hi" href="https://openreview.net/forum?id=jlZrTCccAb" rel="noopener ugc nofollow" target="_blank">Cheng, Lo, et al</a>), which uses equivariant diffusion to predict 3D structures of molecules based on incomplete information that can be obtained from real laboratory machines.</p><blockquote class="px"><p id="0ba2" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“Given the importance of structural data in material characterization, the discussions at AI4Mat highlighted the opportunities for Geometric DL to enter the space of real-world materials modelling in addition to their continued successes in simulations including ML-based potentials.” — Santiago Miret (Intel)</p></blockquote><p id="9061" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">🔮 In 2024, I expect to see multiple developments:</p><p id="13bc" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ More discovery architectures and workflows that directly integrate geometric models like M3GNet, CHGNet, MACE.</p><p id="32d5" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ Geometric models might also see increased competition from text-based representations and LLMs as <a class="af hi" href="https://openreview.net/forum?id=0r5DE2ZSwJ" rel="noopener ugc nofollow" target="_blank">new methods are being proposed</a> that directly generate CIF files.</p><p id="86d5" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">3️⃣ More deployment of geometric models and GNNs into real-world experimental data, likely in materials characterization such as KREED, which will likely run into regimes with less data compared to simulation-based modeling.</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0924" class="ow ox fq bf oy oz pa gv pb pc pd gy pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Molecular Dynamics &amp; ML Potentials</h1><p id="0087" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk"><em class="om">Michael Galkin (Intel), Leon Klein (FU Berlin), N M Anoop Krishnan (IIT Delhi), Santiago Miret (Intel)</em></p><blockquote class="px"><p id="5f4b" class="py pz fq bf qa qb qc qd qe qf qg ol dx">One of the pronounced trends of 2023 is going towards foundation models for ML potentials that work on a variety of compounds from small molecules to periodic crystals</p></blockquote><p id="e80b" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">For example, <strong class="ns ga">JMP</strong> (<a class="af hi" href="https://arxiv.org/abs/2310.16802" rel="noopener ugc nofollow" target="_blank">Shoghi et al</a>) from FAIR and CMU, <strong class="ns ga">DPA-2 </strong>(<a class="af hi" href="https://arxiv.org/abs/2312.15492" rel="noopener ugc nofollow" target="_blank">Zhang, Liu, et al</a>) from a large collaboration of Chinese institutions, and <strong class="ns ga">MACE-MP-0</strong> (<a class="af hi" href="https://arxiv.org/abs/2401.00096" rel="noopener ugc nofollow" target="_blank">Batatia et al</a>) from a collaboration led by Cambridge. Practically, those are geometric GNNs pre-trained in the multi-task mode to predict the energy (or forces) of a certain atomic structure. Another notable mention goes to <strong class="ns ga">Equiformer V2</strong> (<a class="af hi" href="https://arxiv.org/abs/2306.12059" rel="noopener ugc nofollow" target="_blank">Liao et al</a>) as a strong equivariant transformer that holds SOTA in many tasks including the recent <a class="af hi" href="https://opencatalystproject.org/challenge.html" rel="noopener ugc nofollow" target="_blank">OpenCatalyst 2023 Challenge</a> and <a class="af hi" href="https://open-dac.github.io/index.html" rel="noopener ugc nofollow" target="_blank">OpenDAC</a> (Direct Air Capture) challenge.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq rh"><img src="../Images/82a368628516ed1fc271de109838f4e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pbsN3z6DZbJuFfKc5-eTrg.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">A foundation model for materials modelling. Trained only on Materials Project data which consists primarily of inorganic crystals and is skewed heavily towards oxides, MACE-MP-0 is capable of molecular dynamics simulation across a wide variety of chemistries in the solid, liquid and gaseous phases. Source: <a class="af hi" href="https://arxiv.org/abs/2401.00096" rel="noopener ugc nofollow" target="_blank">Batatia et al</a></figcaption></figure><p id="b08a" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">⚛️ A common use case for ML potentials is molecular dynamics (MD) which aims to simulate a certain structure on a span of nanoseconds (10ᐨ⁹) to seconds. The main problem is that the fundamental timestep in classical methods is a femtosecond (10ᐨ¹⁵), that is, you’d need at least 1 million steps to simulate a nanosecond and that’s expensive. Modern ML-based methods for MD aim to speed it up by applying coarse-graining and other approximation tricks that accelerate simulations by large margins (30–1000x). <a class="af hi" href="https://openreview.net/forum?id=y8RZoPjEUl" rel="noopener ugc nofollow" target="_blank">Fu, Xie, et al</a> (TMLR’23) apply coarse-graining to atomic structures and run a GNN over smaller graphs to predict the next-step position. Experimentally, the method brings 1000–10.000x speedups compared to classical methods. <strong class="ns ga">TimeWarp</strong> (<a class="af hi" href="https://arxiv.org/abs/2302.01170" rel="noopener ugc nofollow" target="_blank">Klein, Foong, Fjelde, Mlodozeniec, et al</a>, NeurIPS’23) can simulate large timesteps (1⁰⁵ — 1⁰⁶ femtoseconds) in a single forward pass by using a conditional normalizing flow model that approximates a distribution of next-step positions. A trained model is used with MCMC sampling and delivers ~33x speedups.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq ri"><img src="../Images/b960ce8fc1b4928fe2965be02fbe1f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KfzwdxDqrRUSMK5BIeKmqA.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">(a) Initial state x(t) (Left) and accepted proposal state x(t+τ) (Right) sampled with Timewarp for the dipeptide HT (unseen during training). (b) TICA projections of simulation trajectories, showing transitions between metastable states, for a short MD simulation (Left) and Timewarp MCMC (Right), both run for 30 minutes of wall-clock time. Timewarp MCMC achieves a speed-up factor of ≈ 33 over MD in terms of effective sample size per second. Source: <a class="af hi" href="https://arxiv.org/abs/2302.01170" rel="noopener ugc nofollow" target="_blank">Klein, Foong, Fjelde, Mlodozeniec, et al</a></figcaption></figure><p id="fdae" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Santiago Miret (Intel)</em></strong></p><p id="e8bb" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">💡As the deployment of geometric models has seen greater success in property modelling, researchers have pushed the state-of-the-art by testing these models in real-world molecular dynamics simulations. The first work to highlight issues with training models on energy and forces alone was <a class="af hi" href="https://openreview.net/forum?id=A8pqQipwkt" rel="noopener ugc nofollow" target="_blank">Forces Are Not Enough</a> published in TMLR in early 2023. Nevertheless, advances in neighborhood-based methods such as <a class="af hi" href="https://arxiv.org/abs/2204.05249" rel="noopener ugc nofollow" target="_blank">Allegro</a> led to the successful deployment of large-scale simulations using geometric deep learning models, including a <a class="af hi" href="https://www.hpcwire.com/off-the-wire/sc23-spotlight-gordon-bell-prize-2023-finalists-showcase-diverse-supercomputing-applications/" rel="noopener ugc nofollow" target="_blank">nomination for the Gordon Bell Prize</a>.</p><blockquote class="px"><p id="d2ff" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“Much work still remains in ensuring successful, generalised deployment of machine learning potentials across a variety of physical and chemical phenomena.” — Santiago Miret (Intel)</p></blockquote><p id="0f8b" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">➡️ <a class="af hi" href="https://arxiv.org/abs/2310.02428" rel="noopener ugc nofollow" target="_blank">EGraffBench</a> highlights some new challenges, such as generalisation across temperatures and materials phase changes (i.e. <em class="om">solid-to-liquid</em> change), and proposes new metrics for evaluating the performance of machine learning potentials in real MD simulations. The AI4Mat-2023 workshop also showcased the development of new ML potentials for specialised use cases, such as <a class="af hi" href="https://openreview.net/forum?id=jtAXitX6dh" rel="noopener ugc nofollow" target="_blank">solid electrolytes for batteries</a>.</p><p id="3a45" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Leon Klein (FU Berlin)</em></strong></p><p id="0ff8" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">💡 A notable constraint in the application of generative models to sample from the equilibrium Boltzmann distribution was the requirement for retraining with each new system, thereby limiting potential advantages over traditional MD simulations. However, recent advancements have seen the emergence of transferable models across various domains. Our contribution, <a class="af hi" href="https://arxiv.org/abs/2302.01170" rel="noopener ugc nofollow" target="_blank">Timewarp</a>, presents a transferable model capable of proposing large time steps for MD simulations focused on all atom small peptide systems. Similarly,<a class="af hi" href="https://arxiv.org/abs/2204.10348" rel="noopener ugc nofollow" target="_blank"> Fu et al.</a> capture the time-coarsened dynamics of coarse-grained polymers, while<a class="af hi" href="https://arxiv.org/abs/2310.18278" rel="noopener ugc nofollow" target="_blank"> Charron et al.</a> excel in learning a transferable force field for coarse-grained proteins.</p><blockquote class="px"><p id="ed0a" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“Consequently, this year has demonstrated the feasibility of transferable generative models for MD simulations, showcasing their potential to speed up such simulations.” — Leon Klein (FU Berlin)</p></blockquote><p id="bfe3" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">🔮 In 2024, I expect that more tailored GNNs are used to improve accuracy for the transferable models, with a potential focus on encoding more information about the system. For example, Timewarp, while lacking rotational symmetry in its model, employs data augmentation. Alternatively, rotational symmetry could be incorporated using the recently proposed <a class="af hi" href="https://arxiv.org/abs/2308.10364" rel="noopener ugc nofollow" target="_blank">SE(3) Equivariant Augmented Coupling Flows</a><strong class="ns ga">. </strong>Similarly, <a class="af hi" href="https://arxiv.org/abs/2310.18278" rel="noopener ugc nofollow" target="_blank">Charron et al.</a> use a SchNet instead of a more complex GNN.</p><p id="e9b0" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">N M Anoop Krishnan (IIT Delhi)</em></strong></p><blockquote class="px"><p id="ea66" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“One of the most exciting developments for the year in the realm of ML potentials is the development of “universal” interatomic potentials that can span almost all the elements of the periodic table.” — N M Anoop Krishnan (IIT Delhi)</p></blockquote><p id="762f" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">💡 Following M3GNet in 2022, this year witnessed the developments of three such models based on CHGNet (<a class="af hi" href="https://www.nature.com/articles/s42256-023-00716-3" rel="noopener ugc nofollow" target="_blank">Deng et al</a>), NequIP (<a class="af hi" href="https://www.nature.com/articles/s41586-023-06735-9" rel="noopener ugc nofollow" target="_blank">Merchant et al</a>), and MACE (<a class="af hi" href="https://arxiv.org/abs/2401.00096" rel="noopener ugc nofollow" target="_blank">Batatia et al</a>). These models have been used to demonstrate several challenging tasks including materials discovery (<a class="af hi" href="https://www.nature.com/articles/s41586-023-06735-9" rel="noopener ugc nofollow" target="_blank">Merchant et al</a>), and diverse set of MD simulations (<a class="af hi" href="https://arxiv.org/abs/2401.00096" rel="noopener ugc nofollow" target="_blank">Batatia et al</a>) such as phase transition, amorphization, chemical reaction, 2D materials modeling, dissolution, defects, combustion to name a few. These approaches provide promising results towards the universality of these potentials, thereby allowing one to solve challenging problems including the discovery of crystals from their corresponding amorphous structure (<a class="af hi" href="https://arxiv.org/abs/2310.01117" rel="noopener ugc nofollow" target="_blank">Aykol et al</a>), a long-standing open problem in materials.</p><p id="30c6" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🏋️ While these potentials do provide a handle to attack some outstanding problems, the challenges remain in understanding the scenarios where these potentials can fail.</p><p id="317a" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">1️⃣ </strong>Testing these potentials to their limit to understand their capability is an important aspect to understand their limitations. This includes modeling extreme environments such as <strong class="ns ga">high pressure</strong> and <strong class="ns ga">radiation conditions</strong>, simulating complex multicomponent systems such as <strong class="ns ga">glasses or high-entropy alloys</strong>, or simulating <strong class="ns ga">different phases</strong> of systems such as water or silica would be interesting challenges.</p><p id="759c" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">2️⃣ </strong>While some of these models have been termed as “foundation” models, <strong class="ns ga">emergent behavior </strong>associated with FMs <strong class="ns ga">has not been demonstrated</strong> by them. Most of these models simply show extrapolation capability to potentially unseen regions in the phase space or to novel compositions. Developing truly foundational models in terms of emergent properties would be an interesting challenge.</p><p id="db5e" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">3️⃣ </strong>A third aspect that has been paid less attention to is the ability of these models to <strong class="ns ga">simulate at scale</strong>. While <a class="af hi" href="https://arxiv.org/abs/2204.05249" rel="noopener ugc nofollow" target="_blank">Allegro</a> has demonstrated some capability in terms of length scales these potentials can achieve, simulating at larger time and length scales with stability while respecting the “universality” shall still remain an open challenge for these potentials.</p><p id="7fce" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮 <strong class="ns ga">What to expect in 2024?</strong></p><p id="9e0b" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">1️⃣</strong> <strong class="ns ga">Benchmarking suite</strong>: While there exist several benchmarking studies on MD simulations, it is expected that 2024 will witness more formalized efforts in this direction both in terms of datasets and tasks. A standard set of tasks that can automatically evaluate potentials and place them on leaderboards will enable easy ranking of potentials targeted for downstream tasks on different materials such as metals, polymers, or oxides.</p><p id="1ccc" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">2️⃣ Model and dataset development</strong>: Further efforts will be made to make ML potentials more compact and efficient in terms of their architectures. Moreover, 2024 will also witness large-scale dataset development that will provide <em class="om">ab initio</em> data for training these potentials.</p><p id="f2ef" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">3️⃣ Differentiable MD/AIMD</strong>: Further, it is expected that the developments in differentiable simulations will become a major area of fusing experiments and <em class="om">ab initio</em> simulations towards automated development of interatomic potentials for targeted applications. This year may also see advances in differentiable AIMD with machine learned functionals that may allow economical simulations to scale beyond what it has been able to achieve thus far.</p><p id="aef3" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">Predictions from the 2023 post</strong></p><p id="0d8e" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">We expect to see a lot more focus on computational efficiency and scalability of GNNs. Current GNN-based force-fields are obtaining remarkable accuracy, but are still 2–3 orders of magnitude slower than classical force-fields and are typically only deployed on a few hundred atoms.</p><p id="ff79" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">✅ Allegro for the Gordon Bell Prize, Large-scale screening with GNoMe</p><p id="5fb4" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮<strong class="ns ga">What to expect in 2024</strong>:</p><p id="258e" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">1️⃣ </strong>More deployment of ML potentials into large-scale MD simulations that showcase new research opportunities and challenges and provide a better idea of what benefits ML potentials provide compared to traditional potentials.</p><p id="1875" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">2️⃣ </strong>New datasets that outline previously unexplored challenges for ML potentials, such as new materials systems and new physical phenomena for those materials such as phase changes at various temperatures and pressures.</p><p id="26c3" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">3️⃣ </strong>Exploration of multi-scale problems that might draw inspiration from classical techniques.</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="34b8" class="ow ox fq bf oy oz pa gv pb pc pd gy pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Geometric Generative Models (Manifolds)</h1><p id="a007" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk"><em class="om">Joey Bose (Mila &amp; Dreamfold) and Alex Tong (Mila &amp; Dreamfold)</em></p><p id="43c7" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">While generative ML continued to dominate the field in 2023, it was the popularization of geometric generative models that incorporate geometric priors an interesting trend of the year.</p><p id="97eb" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Joey Bose (Mila &amp; Dreamfold)</em></strong></p><blockquote class="px"><p id="3851" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“This year we saw the burgeoning subfield of geometric generative generative models really take a commanding step forward. With the success of diffusion models and flow matching in images we saw more fundamental contributions to enable Generative AI for geometric data types.“ — Joey Bose (Mila &amp; Dreamfold)</p></blockquote><p id="6088" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">While diffusion models for manifolds existed, this year we really saw them being scaled up with <strong class="ns ga">Scaling Riemannian Diffusion Models</strong> by <a class="af hi" href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=54-actIAAAAJ&amp;sortby=pubdate&amp;citation_for_view=54-actIAAAAJ%3A_FxGoFyzp5QC" rel="noopener ugc nofollow" target="_blank">Lou et. al</a> and functional approaches in <strong class="ns ga">Manifold Diffusion Fields</strong> <a class="af hi" href="https://arxiv.org/abs/2305.15586" rel="noopener ugc nofollow" target="_blank">Elhag et. al.</a></p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq on"><img src="../Images/c4c9c5ec0804e2389bd2fc37d0611062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SRn5B9QlL59Gy86O"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">(Left) Visual depiction of a training iteration for a field on the bunny manifold M. (Right) Visual depiction of the sampling process for a field on the bunny manifold. Figure source: <a class="af hi" href="https://arxiv.org/abs/2305.15586" rel="noopener ugc nofollow" target="_blank">Elhag et al.</a></figcaption></figure><p id="cf58" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">For Normalizing flow-based methods, <strong class="ns ga">Riemannian Flow matching</strong> by <a class="af hi" href="https://arxiv.org/abs/2302.03660" rel="noopener ugc nofollow" target="_blank">Chen and Lipman</a> stands at the top of the sea of papers as being the most general framework for FM.</p><p id="5d6d" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">In general, a large theme of geometric generative models involves handling symmetries. Equivariant approaches shone this year, from SE(3) models including <strong class="ns ga">EDGI</strong> (<a class="af hi" href="https://arxiv.org/abs/2303.12410" rel="noopener ugc nofollow" target="_blank">Brehmer, Bose et. al</a>), <strong class="ns ga">SE(3) augmented coupling flows</strong> (<a class="af hi" href="https://arxiv.org/abs/2308.10364" rel="noopener ugc nofollow" target="_blank">Midgley et. al</a>), to cool theoretical work on <strong class="ns ga">Geometric neural diffusion processes</strong> (<a class="af hi" href="https://arxiv.org/abs/2307.05431" rel="noopener ugc nofollow" target="_blank">Mathieu et. al</a>) and important physics-based applications with the paper by <a class="af hi" href="https://arxiv.org/abs/2305.02402" rel="noopener ugc nofollow" target="_blank">Abbot et. al</a>.</p><p id="249b" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Alex Tong (Mila &amp; Dreamfold)</em></strong></p><blockquote class="px"><p id="036f" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“In 2023 we saw advancement both in terms of modelling and the rise of a new application — Protein backbone design. Much work is still needed to understand the properties of the SE(3)<em class="qn">ᴺ</em>₀ type of product manifold, where it is still unclear how to best combine modalities” — Alex Tong (Mila &amp; Dreamfold)</p></blockquote><p id="4253" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">2023 saw new models such as <a class="af hi" href="https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1" rel="noopener ugc nofollow" target="_blank">RFDiffusion</a>, <a class="af hi" href="https://arxiv.org/abs/2302.02277" rel="noopener ugc nofollow" target="_blank">FrameDiff</a>, and <a class="af hi" href="https://arxiv.org/abs/2310.02391" rel="noopener ugc nofollow" target="_blank">FoldFlow</a> which operate over the SE(3)<em class="om">ᴺ</em>₀ manifold of protein backbones. This presents a new challenge for geometric generative models which I think we will see significant progress in the coming year.</p><p id="03ad" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">On the modelling side, generative modelling with flow and bridge matching models in Euclidean domains led to quick succession of Riemannian and equivariant extensions with Riemannian Flow Matching by <a class="af hi" href="https://arxiv.org/abs/2302.03660" rel="noopener ugc nofollow" target="_blank">Chen and Lipman</a> and Equivariant flow matching (<a class="af hi" href="https://arxiv.org/abs/2306.15030" rel="noopener ugc nofollow" target="_blank">Klein et al.</a>, <a class="af hi" href="https://arxiv.org/abs/2312.07168" rel="noopener ugc nofollow" target="_blank">Song et al.</a>) on molecule generation tasks.</p><p id="14a9" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮 <strong class="ns ga">What to expect in 2024</strong>:</p><p id="77fa" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">1️⃣ </strong>More exploration into modelling the SE(3)<em class="om">ᴺ</em>₀ manifold following successes in protein backbone design.</p><p id="197b" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">2️⃣ </strong>Further investigation and theory of how to train generative models on multimodal and product manifolds.</p><p id="38b3" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">3️⃣ </strong>Domain-specific models exploiting features of more specific manifold and equivariant structures.</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3f98" class="ow ox fq bf oy oz pa gv pb pc pd gy pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">BIG Graphs, Scalability: When GNNs are too expensive</h1><p id="56d4" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk"><strong class="ns ga"><em class="om">Anton Tsitsulin (Google)</em></strong></p><p id="92a2" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">This year has been fruitful for large graph fans.</p><blockquote class="px"><p id="6c9d" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“Learning on Very Large Graphs has always been a challenge due to the unstructured sparsity not being supported by modern accelerators, losing in the <a class="af hi" href="https://hardwarelottery.github.io/" rel="noopener ugc nofollow" target="_blank">hardware lottery</a>. <a class="af hi" href="https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains" rel="noopener ugc nofollow" target="_blank">Tensor Processing Units</a> — you can think about them as very fast GPUs with tons (multi-terabyte) of HBM memory — were the rescue of 2023.” — <strong class="al">Anton Tsitsulin</strong> (Google)</p></blockquote><p id="35c5" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">In a KDD paper (<a class="af hi" href="https://arxiv.org/abs/2307.14490" rel="noopener ugc nofollow" target="_blank">Mayer et al.</a>), we showed that TPUs can solve large-scale node embedding problems more efficiently than GPU and CPU systems at a fraction of the cost. Many industrial applications of graph machine learning are fully unsupervised; there, it is hard to evaluate embedding quality. We wrote a paper (<a class="af hi" href="https://arxiv.org/abs/2305.16562" rel="noopener ugc nofollow" target="_blank">Tsitsulin et al.</a>) that performs <strong class="ns ga">unsupervised embedding analysis</strong> at scale.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq rj"><img src="../Images/c4683b5e9dcf8a260db6773c1db23ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*d9VT0pu8UHL5gpeE"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Scale of TpuGraphs compared to other graph property prediction datasets. Source: <a class="af hi" href="https://arxiv.org/abs/2308.13490" rel="noopener ugc nofollow" target="_blank">Phothilimthana et al.</a></figcaption></figure><p id="f2d3" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ This year, TPUs helped graph machine learning, so it was time to give back. We released a new <strong class="ns ga">TpuGraphs</strong> dataset (<a class="af hi" href="https://arxiv.org/abs/2308.13490" rel="noopener ugc nofollow" target="_blank">Phothilimthana et al.</a>) and ran a <a class="af hi" href="https://www.kaggle.com/competitions/predict-ai-model-runtime" rel="noopener ugc nofollow" target="_blank">Kaggle competition</a> “Google — Fast or Slow? Predict AI Model Runtime” on it that showed <a class="af hi" href="https://blog.research.google/2023/12/advancements-in-machine-learning-for.html" rel="noopener ugc nofollow" target="_blank">how to improve</a> learning models running on TPUs with graph machine learning. It had 792 Competitors, 616 Teams, and 10,507 Entries. The dataset provides 25x more graphs than the largest graph property prediction dataset (with comparable graph sizes), and 770x larger graphs on average compared to existing performance prediction datasets on machine learning programs. This dataset is so large, a new algorithm for doing graph-level predictions on large-scale graphs had to be developed by <a class="af hi" href="https://arxiv.org/abs/2305.12322" rel="noopener ugc nofollow" target="_blank">Cao et al</a>.</p><p id="bf70" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ Large-scale graph clustering has seen significant contributions this year. A new approximation algorithm (<a class="af hi" href="https://arxiv.org/abs/2309.17243" rel="noopener ugc nofollow" target="_blank">Cohen-Addad et al.</a>) was proposed for correlation clustering improving the approximation factor from 1.994 to the whopping 1.73. <strong class="ns ga">TeraHAC </strong>(<a class="af hi" href="https://arxiv.org/abs/2308.03578" rel="noopener ugc nofollow" target="_blank">Dhulipala et al</a>) is a major improvement over last year’s <strong class="ns ga">ParHAC</strong> (that we covered in the <a class="af hi" href="https://medium.com/towards-data-science/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232#ca19" rel="noopener">2023 post</a>) — an approximate (1+𝝐) hierarchical agglomerative clustering algorithm for trillion-edge graphs. The largest graph used in the experiments is a massive Web-Query graph with 31B nodes and 8.6 trillion edges 👀. Notable mentions also go to the fastest (to date) algorithm for Euclidean minimum spanning tree (<a class="af hi" href="https://arxiv.org/abs/2308.00503" rel="noopener ugc nofollow" target="_blank">Jayaram et al</a>) and a new near-linear time algorithm for approximating the Chamfer distance between point sets (<a class="af hi" href="https://arxiv.org/abs/2307.03043" rel="noopener ugc nofollow" target="_blank">Bakshi et al.</a>).</p><p id="f3c7" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮 <strong class="ns ga">What to expect in 2024</strong>:</p><p id="d42f" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">1️⃣ </strong>Algorithmic advances will help scale other popular graph algorithms</p><p id="e117" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">2️⃣ </strong>Novel hardware usage will help scaling up different graph models</p><p id="fc9a" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">Predictions from the 2023 post</strong></p><p id="5cd5" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">(1) further reduction in compute costs and inference time for very large graphs<br/>✅ We observed order-of-magnitude speedups in clustering and node embedding.</p><p id="04e8" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">(2) Perhaps models for OGB LSC graphs could run on commodity machines instead of huge clusters?<br/>❌ solid no</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="f6d7" class="ow ox fq bf oy oz pa gv pb pc pd gy pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Algorithmic Reasoning &amp; Alignment</h1><p id="c666" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk"><em class="om">Petar Veličković (Google DeepMind) and Liudmila Prokhorenkova (Yandex Research)</em></p><p id="09c2" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Algorithmic reasoning, a class of ML techniques able to execute algorithmic computation, has continued to make stable progress during 2023.</p><p id="8b64" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Petar Veličković (Google DeepMind)</em></strong></p><blockquote class="px"><p id="2c87" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“2023 has been a year of steady progress for neural algorithmic reasoning models — it indeed remains one of the areas where GNN development gets most creative — probably because it has to be.” — <strong class="al">Petar Veličković </strong>(Google DeepMind)</p></blockquote><p id="0389" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">Aside from the already discussed <a class="af hi" href="https://openreview.net/forum?id=ba4bbZ4KoF" rel="noopener ugc nofollow" target="_blank">asynchronous algorithmic alignment</a> work, there are three results we achieved this year that I am personally proudest of:</p><p id="4b68" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ <a class="af hi" href="https://openreview.net/forum?id=tRP0Ydz5nN" rel="noopener ugc nofollow" target="_blank">DAR</a> showed that pre-trained multi-task neural algorithmic reasoners can be scalably deployed to downstream graph problems — even if they are 180,000x larger than the synthetic training distribution of the NAR. What’s more, we set the state-of-the-art in modelling mouse brain vessels 🐁🧠🩸. NAR is <strong class="ns ga">not</strong> a victim of the bitter lesson! 📈</p><p id="bc13" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ <a class="af hi" href="https://openreview.net/forum?id=kP2p67F4G7" rel="noopener ugc nofollow" target="_blank">Hint-ReLIC</a> 🗿was our response to the rich body of research in <a class="af hi" href="https://openreview.net/forum?id=xkrtvHlp3P" rel="noopener ugc nofollow" target="_blank">no-hint models</a>. We go away from the issue-ridden <em class="om">hint</em> <em class="om">autoregression</em> and instead model <em class="om">hint invariants</em> using causal reasoning. We obtain a potent hint-based NAR, which still holds state-of-the-art on broad patches of CLRS-30! <em class="om">“Hints can take you a long way, if used in the right way.”</em></p><p id="a833" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">3️⃣ Last but not least, we took the plunge and made the first in-depth analysis of the <a class="af hi" href="https://openreview.net/forum?id=tRP0Ydz5nN" rel="noopener ugc nofollow" target="_blank">latent space representations of trained NAR models</a>. What we found was not only immensely beautiful to look at 🌺 but it also taught us a great deal about how these models work.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq rk"><img src="../Images/2f82cfb2cc837b506d2bca4387e8b55c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c8vWpt6GPQmDvYVvPE_11w.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Left: Trajectory-wise PCA of eight clusters of reweighted graphs showing that they all contain a single dominant direction. Different clusters have different colors. Middle: Many embedding clusters with dominant directions overlaid in red. Right: Step-wise PCA of random graphs with the dominant cluster directions overlaid in red. Source: <a class="af hi" href="https://openreview.net/forum?id=tRP0Ydz5nN" rel="noopener ugc nofollow" target="_blank">Mirjanić, Pascanu, Veličković</a></figcaption></figure><p id="4d1e" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Beyond growing our vibrant community, I find it important to state that many of NAR’s foundational ideas are at the crux of important LLM methodologies; to name just one example, hint following is directly related to <a class="af hi" href="https://arxiv.org/abs/2201.11903" rel="noopener ugc nofollow" target="_blank">chain-of-thought</a> prompting.</p><p id="00ff" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">💡 What I am most happy about is that in 2023, this link is getting explicit recognition, and ideas from NAR are now directly or indirectly influencing the most potent AI systems in use today. Indeed, NAR is listed as a key motivation for studying <a class="af hi" href="https://arxiv.org/abs/2310.16028" rel="noopener ugc nofollow" target="_blank">length generalisation</a>, and more broadly <a class="af hi" href="https://arxiv.org/abs/2301.13105" rel="noopener ugc nofollow" target="_blank">generalisation on the unseen</a><em class="om"> (ICML’23 Best Paper Award)</em>. CLRS-30, the flagship NAR benchmark, is directly used to evaluate capabilities of LLMs in <a class="af hi" href="https://arxiv.org/abs/2302.14838" rel="noopener ugc nofollow" target="_blank">neural architecture search</a> and <a class="af hi" href="https://arxiv.org/abs/2310.03302" rel="noopener ugc nofollow" target="_blank">general AI research</a>. And, as a final cherry on top, CLRS-30 is recognised as one of only seven reasoning evaluations used by <a class="af hi" href="https://arxiv.org/abs/2312.11805" rel="noopener ugc nofollow" target="_blank">Gemini</a>, a frontier large language model from Google DeepMind. I am hopeful that this is a beacon of things to come in 2024, and that we will see even more ideas from NAR break into the design of frontier scalable AI models.</p><p id="ef7b" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Liudmila Prokhorenkova (Yandex Research)</em></strong></p><p id="1674" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Throughout the year, substantial progress has been achieved on the path towards endowing models with various algorithmic inductive biases: the use of dual problems <a class="af hi" href="https://arxiv.org/abs/2302.04496" rel="noopener ugc nofollow" target="_blank">(Numeroso et al)</a>, contrastive learning techniques (<a class="af hi" href="https://arxiv.org/abs/2302.10258" rel="noopener ugc nofollow" target="_blank">Bevilacqua et al</a>; <a class="af hi" href="https://arxiv.org/abs/2306.13411" rel="noopener ugc nofollow" target="_blank">Rodionov et al</a>), augmentation of models with data structures (<a class="af hi" href="https://arxiv.org/abs/2307.00337" rel="noopener ugc nofollow" target="_blank">Jürß et al</a>; <a class="af hi" href="https://arxiv.org/abs/2307.09660" rel="noopener ugc nofollow" target="_blank">Jain et a</a>l), and in-depth examination of computational models <a class="af hi" href="https://arxiv.org/abs/2307.04049" rel="noopener ugc nofollow" target="_blank">(Engelmayer et al)</a>. Another important direction is evaluating existing models in terms of scalability and data diversity <a class="af hi" href="https://arxiv.org/abs/2309.12253" rel="noopener ugc nofollow" target="_blank">(Minder et al)</a>.</p><blockquote class="px"><p id="ee51" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“In 2024 it would be great to see more comprehensive analysis and understanding of neural reasoners: which operations they learn, how sensitive they are to different shifts in data distributions, what types of mistakes they tend to make and why.” — <strong class="al">Liudmila Prokhorenkova </strong>(Yandex Research)</p></blockquote><p id="d9fe" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">Gaining such insights may contribute to the development of even more robust and scalable models. Furthermore, robust neural reasoners have the potential to positively impact combinatorial optimization models.</p><p id="1e68" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">Predictions from the 2023 post</strong></p><p id="651a" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">(1) Algorithmic reasoning tasks are likely to scale to graphs of thousands of nodes and practical applications like in code analysis or databases<br/>✅ yes, <a class="af hi" href="https://openreview.net/forum?id=tRP0Ydz5nN" rel="noopener ugc nofollow" target="_blank">DAR</a> scales to the OGB vessel size</p><p id="d76c" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">(2) even more algorithms in the benchmark<br/>✅ yes, <a class="af hi" href="https://arxiv.org/abs/2309.12253" rel="noopener ugc nofollow" target="_blank">SALSA-CLRS</a></p><p id="4a48" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">(3) most unlikely — there will appear a model capable of solving quickselect<br/>❌ still unsolved ;(</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5854" class="ow ox fq bf oy oz pa gv pb pc pd gy pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Knowledge Graphs: Inductive Reasoning is Solved?</h1><p id="12b7" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk"><em class="om">Michael Galkin (Intel) and Zhaocheng Zhu (Mila &amp; Google)</em></p><p id="1e01" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Since its inception in 2011, the grand challenge of KG representation learning was truly inductive reasoning when a <strong class="ns ga">single</strong> model would be able to run inference (eg, missing link prediction) on any graph without input features and without learning hard-coded entity/relation embedding matrices. <a class="af hi" href="https://arxiv.org/abs/1911.06962" rel="noopener ugc nofollow" target="_blank">GraIL</a> (ICML’20) and <a class="af hi" href="https://arxiv.org/abs/2106.06935" rel="noopener ugc nofollow" target="_blank">Neural Bellman-Ford Nets</a> (NeurIPS’21) were instrumental in extending inference to unseen entities, but generalization to both new entities and relation types at inference time remained an unsolved challenge due to the main question: what can be learned and transferred when the whole entity/relation vocabulary can change?</p><p id="7f22" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮 Our prediction for 2023 (an inductive model fully transferable to different KGs with new sets of entities and relations, e.g., training on Wikidata, and running inference on DBpedia or Freebase) came true in several works:</p><ul class=""><li id="6383" class="nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol rl op oq bk"><a class="af hi" href="https://arxiv.org/abs/2302.01313" rel="noopener ugc nofollow" target="_blank">Gao et al</a> introduced the concept of double equivariance that forces the neural net to be equivariant to permutations of both node IDs and relation IDs. The proposed ISDEA++ model employs a <a class="af hi" href="https://arxiv.org/abs/2110.02910" rel="noopener ugc nofollow" target="_blank">DSS-GNN</a>-like aggregation of a relation-induced subgraph and a subgraph induced by all other relation types.</li><li id="a036" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol rl op oq bk"><a class="af hi" href="https://github.com/DeepGraphLearning/ULTRA" rel="noopener ugc nofollow" target="_blank">ULTRA</a> introduced by <a class="af hi" href="https://arxiv.org/abs/2310.04562" rel="noopener ugc nofollow" target="_blank">Galkin et al</a> learns the invariance of relation interactions (captured by a graph of relations) and transfers to absolutely any multi-relational graph. ULTRA achieves SOTA results on dozens of transductive and inductive datasets even in the zero-shot inference setup. Besides, it enables a foundation model-like approach for KG reasoning with generic pre-training, zero-shot inference, and task-specific fine-tuning.</li></ul><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq on"><img src="../Images/849215a4796605a7efc5a4809308de67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*I7ppzlhNllzuqFmj"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Three main steps taken by ULTRA: (1) building a relation graph; (2) running conditional message passing over the relation graph to get relative relation representations; (3) use those representations for inductive link predictor GNN on the entity level. Source: <a class="af hi" href="https://arxiv.org/abs/2310.04562" rel="noopener ugc nofollow" target="_blank">Galkin et al</a></figcaption></figure><p id="08ba" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Learn more about inductive reasoning in the recent blog post:</p><div class="rm rn ro rp rq rr"><a rel="noopener follow" target="_blank" href="/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----1ed786f7bf63--------------------------------"><div class="rs ab im"><div class="rt ab co cb ru rv"><h2 class="bf ga ic z iu rw iw ix rx iz jb fz bk">ULTRA: Foundation Models for Knowledge Graph Reasoning</h2><div class="ry l"><h3 class="bf b ic z iu rw iw ix rx iz jb dx">One model to rule them all</h3></div><div class="gq l"><p class="bf b dy z iu rw iw ix rx iz jb dx">towardsdatascience.com</p></div></div><div class="rz l"><div class="sa l sb sc sd rz se lx rr"/></div></div></a></div><p id="62ca" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">As the grand challenge seems to be solved now, is there anything left for KG research, or we should call it a day, throw a party, and move on?</p><p id="677d" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Michael Galkin (Intel)</em></strong></p><blockquote class="px"><p id="addd" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“Indeed, with the grand challenge solved, it feels a bit like an existential crisis — everything important is invented, Graph ML enabled things that looked impossible just 5 years ago. Perhaps, KG community should re-invent itself and focus on practical problems that can be tackled with graph foundation models. Otherwise, the subfield would disappear from research radars like Semantic Web” — Michael Galkin (Intel)</p></blockquote><p id="94d6" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">Transductive and shallow KG embeddings are dead and nobody in 2024 should work on them, it is time to retire them for good. ULTRA-like foundation models can now work without training on any graph which is a sweet spot for many closed enterprise KGs.</p><p id="8b4a" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ The last uncharted territory is inductive reasoning beyond simple link prediction (<a class="af hi" href="https://medium.com/towards-data-science/neural-graph-databases-cc35c9e1d04f" rel="noopener">complex database-like logical queries</a>) and I think it will also be solved in 2024. Adding temporal aspects, LLM node features, or scaling GNNs for larger graphs is a question of time and presents more of an engineering task than a research question.</p><p id="c513" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Zhaocheng Zhu (Mila &amp; Google)</em></strong></p><blockquote class="px"><p id="ed63" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“With the rise of LLMs and numerous prompt-based reasoning techniques, it looks like <strong class="al">KG reasoning is coming to an end</strong>. Texts are more expressive and flexible than KGs, and meanwhile they are more available in quantity. However, I don’t think the reasoning techniques that the KG community developed are in vain.” — Zhaocheng Zhu (Mila &amp; Google)</p></blockquote><p id="8fec" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">➡️ We see that many LLM reasoning methods coincide with well-known ideas on KGs. For instance, the difference between direct prompting and chain-of-thought (CoT) shares much spirit with embedding methods and path-based methods on KGs, where the latter ones parameterize smaller steps and thereby generalize better to new combinations of steps. In fact, topics like inductive and multi-step generalization were explored on KGs several years earlier than on LLMs.</p><p id="30fb" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">When we develop new techniques for LLMs, it is essential to take a glance at similar goals and solutions on KGs. In brief, while the modality of KGs <em class="om">may fade at some point</em>, the insights we learned from KG reasoning will continue to illuminate in the era of LLMs.</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="add1" class="ow ox fq bf oy oz pa gv pb pc pd gy pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Temporal Graph Learning</h1><p id="6ced" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk">Shenyang Huang, Emanuele Rossi, Andrea Cini, Ingo Scholtes, and Michael Galkin prepared a separate overview post on temporal graph learning!</p><div class="rm rn ro rp rq rr"><a rel="noopener follow" target="_blank" href="/temporal-graph-learning-in-2024-feaa9371b8e2?source=post_page-----1ed786f7bf63--------------------------------"><div class="rs ab im"><div class="rt ab co cb ru rv"><h2 class="bf ga ic z iu rw iw ix rx iz jb fz bk">Temporal Graph Learning in 2024</h2><div class="ry l"><h3 class="bf b ic z iu rw iw ix rx iz jb dx">Continue the journey for evolving networks</h3></div><div class="gq l"><p class="bf b dy z iu rw iw ix rx iz jb dx">towardsdatascience.com</p></div></div><div class="rz l"><div class="sf l sb sc sd rz se lx rr"/></div></div></a></div></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="ad3d" class="ow ox fq bf oy oz pa gv pb pc pd gy pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">LLMs + Graphs for Scientific Discovery</h1><p id="7280" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk"><em class="om">Michael Galkin (Intel)</em></p><p id="d782" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">💡LLMs were everywhere in 2023 and it’s hard to miss the 🐘 in the room.</p><blockquote class="px"><p id="363f" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“We have seen a flurry of approaches trying to marry graphs with LLMs. The subfield is emerging and <strong class="al">making its tiny baby steps</strong> which are important to acknowledge.” — Michael Galkin (Intel)</p></blockquote><p id="db52" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">We have seen a flurry of approaches trying to marry graphs with LLMs (sometimes literally verbalizing the edges in a text prompt) where straightforward prompting with edge index does not really work for running graph algorithms with language models, so the crux is in the “text linearization” and proper prompting. Among the notable mentions, you might be interested in <strong class="ns ga">GraphText</strong> by <a class="af hi" href="https://arxiv.org/abs/2310.01089" rel="noopener ugc nofollow" target="_blank">Zhao et al</a> that devises a <em class="om">graph syntax tree</em> prompt constructed from features and labels in the ego-subgraph of a target node — GraphText works for node classification. In <strong class="ns ga">Talk Like a Graph</strong> by <a class="af hi" href="https://arxiv.org/abs/2310.04560" rel="noopener ugc nofollow" target="_blank">Fatemi et al</a> the authors study graph linearization strategies and how they impact LLM performance on basic tasks like edge existence, node count, or cycle check.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq sg"><img src="../Images/4c313a55121a3192694ee0c6d387a706.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fzM6yf61zDMpGqLE"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Standard GNNs (left) and GraphText (right). GraphText encodes the graph information into text sequences and uses LLM to perform inference. The graph-syntax tree contains both node attributes (e.g. feature and label) and relationships (e.g. center-node, 1st-hop, and 2nd-hop). Source: <a class="af hi" href="https://arxiv.org/abs/2310.01089" rel="noopener ugc nofollow" target="_blank">Zhao et al</a></figcaption></figure><p id="4844" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ Despite the early stage, there exist already 3 recent surveys (<a class="af hi" href="https://arxiv.org/abs/2311.12399" rel="noopener ugc nofollow" target="_blank">Li et al</a>, <a class="af hi" href="https://arxiv.org/abs/2312.02783" rel="noopener ugc nofollow" target="_blank">Jin et al</a>, <a class="af hi" href="https://arxiv.org/abs/2311.16534" rel="noopener ugc nofollow" target="_blank">Sun et al</a>) covering dozens of prompting approaches for graphs. Generally, it is yet to be seen <strong class="ns ga">whether</strong> <strong class="ns ga">LLMs are an appropriate hammer</strong> 🔨 for a specific <em class="om">graph</em> nail given all the limitations of the autoregressive decoding, small context sizes, and permutation-invariant nature of graph tasks. If you are broadly interested in LLM reasoning, check out <a class="af hi" rel="noopener" target="_blank" href="/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d">our recent blog post</a> covering the main areas and progress made in 2023.</p><p id="f936" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ LLMs in applied scientific tasks exhibit more promising, sometimes quite unexpected results: <strong class="ns ga">ChemCrow</strong> 🐦‍⬛ by <a class="af hi" href="https://arxiv.org/abs/2304.05376" rel="noopener ugc nofollow" target="_blank">Bran, Cox, et al</a> is an LLM agent powered with tools that can perform tasks in organic chemistry, synthesis, and material design right in natural language (without fancy equivariant GNNs). For example, with a query “<em class="om">Find and synthesize a thiourea organocatalyst which accelerates a Diels-Alder reaction</em>” ChemCrow devises a sequence of actions starting from a basic SMILES string and ending up with instructions to a synthesis platform.</p><p id="bdd1" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Similarly, <a class="af hi" href="https://openreview.net/forum?id=0r5DE2ZSwJ" rel="noopener ugc nofollow" target="_blank">Gruver et al</a> fine-tuned LLaMA-2 to generate 3D crystal structures as a plain text file with lattice parameters, atomic composition, and 3D coordinates and it is surprisingly competitive with SOTA geometric diffusion models like CDVAE.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq sh"><img src="../Images/0cf27820c04f943ef14d5fc406665457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ruE6PRORCjm8roCFA5h8fw.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Experimental validation. a) Example of the script run by a user to initiate ChemCrow. b) Query and synthesis of a thiourea organocatalyst. c) The IBM Research RoboRXN synthesis platform on which the experiments were executed (pictures reprinted courtesy of International Business Machines Corporation). d) Experimentally validated compounds. Source: <a class="af hi" href="https://arxiv.org/abs/2304.05376" rel="noopener ugc nofollow" target="_blank">Bran, Cox, et al</a></figcaption></figure><p id="3ed4" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮 In 2024, scientific applications of LLMs are likely to expand both breadth-wise and depth-wise:</p><p id="74b4" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ Reaching out to more AI4Science areas;</p><p id="d73d" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ Integration with geometric foundation models (since multi-modality is the main LLM focus for the coming year);</p><p id="957e" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">3️⃣ Hot take: LLMs will solve the <em class="om">quickselect</em> task in the CLRS-30 benchmark before GNNs do 🔥</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3d00" class="ow ox fq bf oy oz pa gv pb pc pd gy pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Cool GNN Applications</h1><p id="b36e" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk"><em class="om">Petar Veličković (Google DeepMind)</em></p><p id="21d4" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">In my standard deck motivating the use of GNNs to a broader audience, I rely on a usual “arsenal” slide of impactful GNN applications over the years. With 2023 being significantly marked by LLM developments, I was wondering — can I meaningfully update this slide, but only using models released this year?</p><blockquote class="px"><p id="d0df" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“It was the middle of the year back then, and already I was in for a nice surprise;<em class="qn"> I did not have enough space to list all the awesome things done with GNNs!” — </em><strong class="al">Petar Veličković </strong>(Google DeepMind)</p></blockquote><p id="6a81" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk">💡 While it might have gone comparatively under the radar, I confidently claim that 2023 was the <strong class="ns ga">most exciting year</strong> for cool GNN applications! The rise of LLMs just made it very clear where the limits of text-based autoregressive models are, and that for most scientific problems coming from Nature, their graph structure cannot be ignored.</p><p id="2d0d" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Here’s a handful of my personal favourite landmark results — all published in top-tier venues:</p><ul class=""><li id="85d2" class="nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol rl op oq bk"><a class="af hi" href="https://www.science.org/doi/10.1126/science.adi2336" rel="noopener ugc nofollow" target="_blank">GraphCast</a> provided us a landmark model for medium-range global weather forecasting ⛈️ and with it, more accurate foreshadowing of extreme events such as hurricanes. A highly well-deserved cover of Science!</li><li id="1f62" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol rl op oq bk">In an outstanding development in materials science, <a class="af hi" href="https://www.nature.com/articles/s41586-023-06735-9" rel="noopener ugc nofollow" target="_blank">GNoME</a> uses a GNN-based model to discover <em class="om">millions </em>of novel crystal structures 💎 — an <em class="om">“order-of-magnitude expansion in stable materials known to humanity”</em>. Published in Nature.</li><li id="b5ca" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol rl op oq bk">We’ve been treated to not just <a class="af hi" href="https://www.nature.com/articles/s41589-023-01349-8" rel="noopener ugc nofollow" target="_blank">one</a>, but <a class="af hi" href="https://www.nature.com/articles/s41586-023-06887-8" rel="noopener ugc nofollow" target="_blank">two</a> new breakthroughs in antibiotic discovery 💊 using message passing neural networks — the latter being published in Nature!</li><li id="6bee" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol rl op oq bk"><a class="af hi" href="https://www.science.org/doi/10.1126/science.ade4401" rel="noopener ugc nofollow" target="_blank">GNNs can smell</a> 👃 by observing the molecular structure emitting an odour — a result that may well revolutionise many industries, including perfumes! Published in Science.</li><li id="86b2" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol rl op oq bk">On the cover of Nature Machine Intelligence, <a class="af hi" href="https://www.nature.com/articles/s42256-023-00684-8" rel="noopener ugc nofollow" target="_blank">HYFA</a> 🍄 shows how to use hypergraph factorisation to make significant progress in gene expression imputation 🧬!</li><li id="5ec9" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol rl op oq bk">Last but not least, particle physics ⚛️ remains a natural stronghold of GNN applications. In this year’s Nature Physics Review, we have been treated to a <a class="af hi" href="https://www.nature.com/articles/s42254-023-00569-0" rel="noopener ugc nofollow" target="_blank">fascinating survey</a> elucidating the myriad of ways how graph neural networks are deployed for various data analysis tasks at the Large Hadron Collider ⚡.</li></ul><p id="8ad2" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">⚽ My own humble contribution to the space of GNN applications this year was <a class="af hi" href="https://arxiv.org/abs/2310.10553" rel="noopener ugc nofollow" target="_blank">TacticAI</a>, the <em class="om">first full AI system giving useful tactical suggestions to (association) football coaches</em>, developed in partnership with our collaborators at Liverpool FC 🔴. TacticAI is capable of both predictive modelling (<em class="om">“what will happen in this tactical scenario?”</em>), retrieving similar tactics, and conditional generative modelling (<em class="om">“how to modify player positions to make a particular outcome happen?”</em>). In my opinion, the most satisfying part of this very fun collaboration was our user study with some of LFC’s top coaching staff — directly illustrating that the outputs of our model will be of use to coaches in their work 🏃.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq si"><img src="../Images/a05d2b481488396a23817f3106efd65f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SjqNMqxXmASGyOnnG7HHtw.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">A “bird’s eye” overview of TacticAI. (A), how corner kick situations are converted to a graph representation. Each player is treated as a node in a graph, with node, edge and graph features extracted as detailed in the main text. Then, a graph neural network operates over this graph by performing message passing; each node’s representation is updated using the messages sent to it from its neighbouring nodes. (B), how TacticAI processes a given corner kick. To ensure that TacticAI’s answers are robust in the face of horizontal or vertical reflections, all possible combinations of reflections are applied to the input corner, and these four views are then fed to the core TacticAI model, where they are able to interact with each other to compute the final player representations — each “internal blue arrow” corresponds to a single message passing layer from (A). Once player representations are computed, they can be used to predict the corner’s receiver, whether a shot has been taken, as well as assistive adjustments to player positions and velocities, which increase or decrease the probability of a shot being taken. Source: <a class="af hi" href="https://arxiv.org/abs/2310.10553" rel="noopener ugc nofollow" target="_blank">Wang, Veličković, Hennes et al.</a></figcaption></figure><p id="31f1" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">This is what I’m all about — AI systems that significantly augment human abilities. I can only hope that, in my home country, Partizan catches on to these methods before Red Star does! 😅</p><p id="1743" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮 What will we see in 2024? Probably more of the same, just accelerated! ⏩</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="986f" class="ow ox fq bf oy oz pa gv pb pc pd gy pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Geometric Wall Street Bulletin 💸</h1><p id="93de" class="pw-post-body-paragraph nq nr fq ns b gt ps nu nv gw pt nx ny nz pu ob oc od pv of og oh pw oj ok ol fj bk"><em class="om">Nathan Benaich (AirStreet Capital)</em><strong class="ns ga"><em class="om">, </em></strong><em class="om">Michael Bronstein (Oxford), and Luca Naef (VantAI)</em></p><p id="7e0a" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2023 started with BioNTech (mostly known to the broad public for developing mRNA SARS-CoV-2 vaccines) <a class="af hi" href="https://www.instadeep.com/2023/01/biontech-to-acquire-instadeep-to-strengthen-pioneering-position-in-the-field-of-ai-powered-drug-discovery-design-and-development/" rel="noopener ugc nofollow" target="_blank">announcing the acquisition of InstaDeep</a>, a decade-old British company focused on AI-powered drug discovery, design and development. In May 2023, Recursion <a class="af hi" href="https://ir.recursion.com/news-releases/news-release-details/recursion-enters-agreements-acquire-cyclica-and-valence-bolster" rel="noopener ugc nofollow" target="_blank">acquired two startups</a>, Cyclica and Valence “to bolster chemistry and generative AI capabilities”. Valence ML team is well-known for multiple works in the geometric and graph ML and hosting the <strong class="ns ga">Graphs &amp; Geometry and Molecular Modeling</strong> &amp; <strong class="ns ga">Drug Discovery seminars</strong> on <a class="af hi" href="https://www.youtube.com/@valence_labs" rel="noopener ugc nofollow" target="_blank">YouTube</a>.</p><p id="a6aa" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><a class="af hi" href="https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal" rel="noopener ugc nofollow" target="_blank">💰</a>Isomorphic Labs started 2024 by announcing small molecule-focused <a class="af hi" href="https://www.isomorphiclabs.com/articles/isomorphic-labs-kicks-off-2024-with-two-pharmaceutical-collaborations" rel="noopener ugc nofollow" target="_blank">collaborations</a> with Eli Lilly and Novartis with upfront payments of $45M and $37.5M, respectively, with the potential worth of <strong class="ns ga">$3 billion</strong>.</p><p id="fd25" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><a class="af hi" href="https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal" rel="noopener ugc nofollow" target="_blank">💰</a><a class="af hi" href="https://www.businesswire.com/news/home/20240108659035/en/VantAI-Secures-Renewed-Support-from-Blueprint-Medicines-to-Chart-New-Frontiers-in-Induced-Proximity-Drug-Discovery" rel="noopener ugc nofollow" target="_blank">VantAI partnered with Blueprint Medicines</a> on innovative proximity modulating therapeutics, including molecular glue and hetero-bifunctional candidates. The deal’s potential worth is $1.25 billion.</p><p id="0249" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><a class="af hi" href="https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal" rel="noopener ugc nofollow" target="_blank">💰</a>CHARM Therapeutics raised more funding <a class="af hi" href="https://www.businesswire.com/news/home/20230515005172/en/CHARM-Therapeutics-Receives-Investment-for-Deep-Learning-Enabled-Drug-Discovery-Research-from-NVIDIA" rel="noopener ugc nofollow" target="_blank">from NVIDIA</a> and <a class="af hi" href="https://www.businesswire.com/news/home/20230320005101/en/CHARM-Therapeutics-Announces-Collaboration-with-Bristol-Myers-Squibb-to-Enable-and-Accelerate-Small-Molecule-Drug-Discovery-Programs" rel="noopener ugc nofollow" target="_blank">from Bristol Myers Squibb</a> totalling the initial funding round to $70M. The company has developed DragonFold, its proprietary algorithm for protein-ligand co-folding.</p><p id="de6f" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">💊 Monte Rosa <a class="af hi" href="https://ir.monterosatx.com/news-releases/news-release-details/monte-rosa-therapeutics-announces-interim-pkpd-and-clinical-data" rel="noopener ugc nofollow" target="_blank">announced a successful</a> Phase 1 study of MRT-2359 (orally bioavailable investigational molecular glue degrader) against MYC-driven tumors like lung cancer and neuroendocrine cancer. Monte Rosa is known to <a class="af hi" href="https://ir.monterosatx.com/static-files/8806793a-99fb-4df8-8eb7-3785b39cf210" rel="noopener ugc nofollow" target="_blank">use geometric deep learning </a>for proteins (<a class="af hi" href="https://www.nature.com/articles/s41592-019-0666-6" rel="noopener ugc nofollow" target="_blank">MaSIF</a>).</p><p id="92fd" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Nathan Benaich (AirStreet Capital, author of </em></strong><a class="af hi" href="https://www.stateof.ai/" rel="noopener ugc nofollow" target="_blank"><strong class="ns ga"><em class="om">the State of AI Report</em></strong></a><strong class="ns ga"><em class="om">)</em></strong></p><blockquote class="px"><p id="8123" class="py pz fq bf qa qb qc qd qe qf qg ol dx">“I have long been optimistic about the potential of AI-first approaches to design problems in medicine, biotech, and materials science. Graph-based models had a great year in techbio in 2023.” — Nathan Benaich (AirStreet Capital)</p></blockquote><p id="ad9b" class="pw-post-body-paragraph nq nr fq ns b gt qh nu nv gw qi nx ny nz qj ob oc od qk of og oh ql oj ok ol fj bk"><a class="af hi" href="https://www.nature.com/articles/s41586-023-06415-8" rel="noopener ugc nofollow" target="_blank">RFdiffusion</a> combines diffusion techniques with GNNs to predict protein structures. It denoises blurry or corrupted structures from the Protein Data Bank, while tapping into RoseTTAFold’s prediction capabilities. DeepMind have continued to further develop AlphaFold and build on top of it. Their <a class="af hi" href="https://www.science.org/doi/10.1126/science.adg7492" rel="noopener ugc nofollow" target="_blank">AlphaMissense </a>uses weak labels, language modeling, and AlphaFold to predict the pathogenicity of 71 million human variants. This is an important achievement, as most amino acid changes from genetic variation have unknown effects.</p><p id="1b4b" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Beyond proteins, graph-based models have been improving our understanding of genetics. Stanford’s <a class="af hi" href="https://www.nature.com/articles/s41587-023-01905-6.pdf" rel="noopener ugc nofollow" target="_blank">GEARS</a> system integrates deep learning with a gene interaction knowledge graph to predict gene expression changes from combinatorial perturbations. By leveraging prior data on single and double perturbations, GEARS can predict outcomes for thousands of gene pairs.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq on"><img src="../Images/34d2a1814ebb3bb913bd1f9cd617b0d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*A2Ftafm8dTxwfxFV"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">GEARS can predict new biologically meaningful phenotypes. (a) Workflow for predicting all pairwise combinatorial perturbation outcomes of a set of genes. (b) Low-dimensional representation of postperturbation gene expression for 102 one-gene perturbations and 128 two-gene perturbations used to train GEARS. A random selection is labeled. (c) GEARS predicts postperturbation gene expression for all 5,151 pairwise combinations of the 102 single genes seen experimentally perturbed. Predicted postperturbation phenotypes (non-black symbols) are often different from phenotypes seen experimentally (black symbols). Colors indicate Leiden clusters labeled using marker gene expression. Source: <a class="af hi" href="https://www.nature.com/articles/s41587-023-01905-6" rel="noopener ugc nofollow" target="_blank">Roohani et al</a></figcaption></figure><p id="a1b1" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮 In 2024, I put hope in two different developments.</p><p id="43f9" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">1️⃣</strong> We have seen the first two CRISPR-Cas9 therapies approved in the US and the UK. These genome editors were discovered through sequencing and random experimentation. I am excited about the use of AI models to design and create bespoke editors on demand.</p><p id="1797" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">2️⃣ </strong>We have started to see multimodality come to the AI bio world — combining DNA, RNA, protein, cellular, and imaging data to give us a more holistic understanding of biology.</p><p id="5bc1" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">Companies to watch in 2024</strong></p><ul class=""><li id="1dbb" class="nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol rl op oq bk"><a class="af hi" href="https://www.profluent.bio/" rel="noopener ugc nofollow" target="_blank">Profluent</a> — LLMs for protein design</li><li id="0ab4" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol rl op oq bk"><a class="af hi" href="https://inceptive.life/" rel="noopener ugc nofollow" target="_blank">Inceptive.bio</a> — founded by one of the authors of the Transformers paper.</li><li id="2ab6" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol rl op oq bk"><a class="af hi" href="https://www.envedabio.com/" rel="noopener ugc nofollow" target="_blank">Enveda Biosciences</a></li><li id="de31" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol rl op oq bk"><a class="af hi" href="https://orbitalmaterials.com/" rel="noopener ugc nofollow" target="_blank">Orbital Materials</a></li><li id="212e" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol rl op oq bk"><a class="af hi" href="https://kumo.ai/" rel="noopener ugc nofollow" target="_blank">Kumo.AI</a></li><li id="9e55" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol rl op oq bk"><a class="af hi" href="https://www.vant.ai/" rel="noopener ugc nofollow" target="_blank">VantAI</a> — we are biased (Michael Bronstein is Vant’s Chief Scientist and Luca Naef is a founder and CTO), but this is a cool company focused on the rational design of molecular glues using a combination of ML and proprietary experimental technology, which we believe to be the right combination for success.</li><li id="0356" class="nq nr fq ns b gt or nu nv gw os nx ny nz ot ob oc od ou of og oh ov oj ok ol rl op oq bk"><a class="af hi" href="https://www.futurehouse.org/articles/announcing-future-house" rel="noopener ugc nofollow" target="_blank">Future House</a> — a new Silicon Valley-based non-profit company in the AI4Science space funded by ex-Google CEO Eric Schmidt. Head of Science is Andrew White, known for his works on LLMs for chemistry. The self-described mission of the company is a “moonshot to build an AI scientist.”</li></ul></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="da03" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><em class="om">For additional articles about geometric and graph deep learning, see </em><a class="af hi" href="https://medium.com/@mgalkin" rel="noopener"><em class="om">Michael Galkin</em></a><em class="om">’s and </em><a class="af hi" href="https://medium.com/@michael-bronstein" rel="noopener"><em class="om">Michael Bronstein</em></a><em class="om">’s Medium posts and follow the two Michaels (</em><a class="af hi" href="https://twitter.com/michael_galkin" rel="noopener ugc nofollow" target="_blank"><em class="om">Galkin</em></a><em class="om"> and </em><a class="af hi" href="https://twitter.com/mmbronstein" rel="noopener ugc nofollow" target="_blank"><em class="om">Bronstein</em></a><em class="om">) on Twitter.</em></p></div></div></div></div>    
</body>
</html>