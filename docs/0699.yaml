- en: 'Intro to LLM Agents with Langchain: When RAG is Not Enough'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Langchain介绍LLM代理：当RAG不足以满足需求时
- en: 原文：[https://towardsdatascience.com/intro-to-llm-agents-with-langchain-when-rag-is-not-enough-7d8c08145834?source=collection_archive---------0-----------------------#2024-03-15](https://towardsdatascience.com/intro-to-llm-agents-with-langchain-when-rag-is-not-enough-7d8c08145834?source=collection_archive---------0-----------------------#2024-03-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/intro-to-llm-agents-with-langchain-when-rag-is-not-enough-7d8c08145834?source=collection_archive---------0-----------------------#2024-03-15](https://towardsdatascience.com/intro-to-llm-agents-with-langchain-when-rag-is-not-enough-7d8c08145834?source=collection_archive---------0-----------------------#2024-03-15)
- en: First-order principles of brain structure for AI assistants
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI助手的脑结构一阶原则
- en: '[](https://alexhonchar.medium.com/?source=post_page---byline--7d8c08145834--------------------------------)[![Alex
    Honchar](../Images/fd30740a0ee12701b0d8670143ba7a9b.png)](https://alexhonchar.medium.com/?source=post_page---byline--7d8c08145834--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7d8c08145834--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7d8c08145834--------------------------------)
    [Alex Honchar](https://alexhonchar.medium.com/?source=post_page---byline--7d8c08145834--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://alexhonchar.medium.com/?source=post_page---byline--7d8c08145834--------------------------------)[![Alex
    Honchar](../Images/fd30740a0ee12701b0d8670143ba7a9b.png)](https://alexhonchar.medium.com/?source=post_page---byline--7d8c08145834--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7d8c08145834--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7d8c08145834--------------------------------)
    [Alex Honchar](https://alexhonchar.medium.com/?source=post_page---byline--7d8c08145834--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7d8c08145834--------------------------------)
    ·7 min read·Mar 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7d8c08145834--------------------------------)
    ·7分钟阅读·2024年3月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Hello everyone, this article is a written form of a tutorial I conducted two
    weeks ago with [Neurons Lab](https://neurons-lab.com/). If you prefer a narrative
    walkthrough, you can find the YouTube video here:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，本文是我两周前与[Neurons Lab](https://neurons-lab.com/)进行的教程的书面形式。如果你更喜欢叙述性引导，可以在这里找到YouTube视频：
- en: 'As always, you can find [the code on GitHub](https://github.com/Rachnog/intro_to_llm_agents),
    and here are separate Colab Notebooks:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，你可以在[GitHub上找到代码](https://github.com/Rachnog/intro_to_llm_agents)，这里还有独立的Colab笔记本：
- en: '[Planning and reasoning](https://colab.research.google.com/drive/1SplDwEIbVfo9zNt6JOK0gJlV0wCNuz0F?usp=sharing)'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[规划与推理](https://colab.research.google.com/drive/1SplDwEIbVfo9zNt6JOK0gJlV0wCNuz0F?usp=sharing)'
- en: '[Different types of memories](https://colab.research.google.com/drive/13b_pD27aqcNXYI2M7fBxK1fRIO2pygZJ?usp=sharing)'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[不同类型的记忆](https://colab.research.google.com/drive/13b_pD27aqcNXYI2M7fBxK1fRIO2pygZJ?usp=sharing)'
- en: '[Various types of tools](https://colab.research.google.com/drive/1-VpwkmSvzA-zQ_iVK-xjOQf5kA-Lzwg9?usp=sharing)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[各种类型的工具](https://colab.research.google.com/drive/1-VpwkmSvzA-zQ_iVK-xjOQf5kA-Lzwg9?usp=sharing)'
- en: '[Building complete agents](https://colab.research.google.com/drive/1aC9AUNNYYz36atE8BUJH4fZIfknHa9Pk?usp=sharing)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[构建完整的代理](https://colab.research.google.com/drive/1aC9AUNNYYz36atE8BUJH4fZIfknHa9Pk?usp=sharing)'
- en: Introduction to the agents
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理介绍
- en: '![](../Images/e9d979f7bddb7cefa7b8a51fed6ea16f.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9d979f7bddb7cefa7b8a51fed6ea16f.png)'
- en: Illustration by author. LLMs are often augmented with external memory via RAG
    architecture. Agents extend this concept to memory, reasoning, tools, answers,
    and actions
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图示由作者提供。LLM通常通过RAG架构增强外部记忆。代理将这一概念扩展到记忆、推理、工具、答案和行动。
- en: 'Let’s begin the lecture by exploring various examples of LLM agents. While
    the topic is widely discussed, few are actively utilizing agents; often, what
    we perceive as agents are simply large language models. Let’s consider such a
    simple task as **searching for football game results and saving them as a CSV
    file**. We can compare several available tools:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过探索各种LLM代理的示例来开始这堂讲座。虽然这个话题被广泛讨论，但实际上很少有在积极使用代理的；我们通常认为是代理的，往往只是大型语言模型。我们可以考虑一个简单的任务——**搜索足球比赛结果并将其保存为CSV文件**。我们可以比较几种现有的工具：
- en: '**GPT-4 with search and plugins**: as you will find in the [chat history here](https://chat.openai.com/share/2ecd61a9-dbd9-4287-aa75-14618106a34c),
    GPT-4 failed to do the task due to code errors'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带有搜索和插件的GPT-4**：正如你在[这里的聊天历史](https://chat.openai.com/share/2ecd61a9-dbd9-4287-aa75-14618106a34c)中看到的，GPT-4因代码错误未能完成任务。'
- en: '**AutoGPT** through [https://evo.ninja/](https://evo.ninja/) at least could
    generate some kind of CSV (not ideal though):'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AutoGPT** 通过[https://evo.ninja/](https://evo.ninja/)至少可以生成某种形式的 CSV（尽管不理想）：'
- en: '![](../Images/bf2d41707b6fd266beb26e4f705cd0ca.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf2d41707b6fd266beb26e4f705cd0ca.png)'
- en: '**AgentGPT** through [https://agentgpt.reworkd.ai/](https://agentgpt.reworkd.ai/):
    decided to treat this task as a synthetic data generator which is not what we
    asked about, check the [chat history here](https://agentgpt.reworkd.ai/agent?id=clsyfh1t101t9jv08yaof890k)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AgentGPT** 通过[https://agentgpt.reworkd.ai/](https://agentgpt.reworkd.ai/)：决定将此任务视为合成数据生成器，但这并不是我们所要求的，查看[聊天历史记录](https://agentgpt.reworkd.ai/agent?id=clsyfh1t101t9jv08yaof890k)'
- en: '**Since the available tools are not great**, let’s learn from the first principles
    of how to build agents from scratch. I am using amazing [Lilian’s blog article](https://lilianweng.github.io/posts/2023-06-23-agent/)
    as a structure reference but adding more examples on my own.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**由于现有的工具不太理想**，让我们从构建代理的基本原理开始学习。我正在使用[丽莉安的博客文章](https://lilianweng.github.io/posts/2023-06-23-agent/)作为结构参考，但同时加入了更多我自己的例子。'
- en: 'Step 1: Planning'
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 1：规划
- en: '![](../Images/571a9bacf9092b0cc86e2cf8fa5d34ed.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/571a9bacf9092b0cc86e2cf8fa5d34ed.png)'
- en: The visual difference between simple “input-output” LLM usage and such techniques
    as a chain of thought, a chain of thought with self-consistency, a tree of thought
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的“输入-输出”LLM使用和像思维链、带自一致性的思维链、思维树等技术之间的视觉差异
- en: You might have come across various techniques aimed at improving the performance
    of large language models, such as [offering tips](https://twitter.com/literallydenis/status/1730965217125839142)
    or even jokingly threatening them. One popular technique is called “[chain of
    thought](https://www.promptingguide.ai/techniques/cot),” where the **model is
    asked to think step by step, enabling self-correction**. This approach has evolved
    into more advanced versions like the “[chain of thought with self-consistency](https://www.promptingguide.ai/techniques/consistency)”
    and the generalized “[tree of thoughts](https://medium.com/@astropomeai/implementing-the-tree-of-thoughts-in-langchains-chain-f2ebc5864fac),”
    **where multiple thoughts are created, re-evaluated, and consolidated to provide
    an output**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能遇到过各种旨在提升大型语言模型性能的技术，例如[提供建议](https://twitter.com/literallydenis/status/1730965217125839142)或甚至开玩笑地威胁它们。一种流行的技术叫做“[思维链](https://www.promptingguide.ai/techniques/cot)”，在这种方法中，**模型被要求一步步思考，从而实现自我修正**。这种方法已经发展成更高级的版本，如“[带自一致性的思维链](https://www.promptingguide.ai/techniques/consistency)”和广义的“[思维树](https://medium.com/@astropomeai/implementing-the-tree-of-thoughts-in-langchains-chain-f2ebc5864fac)”，**其中多个思维被创造、重新评估并整合，以提供输出**。
- en: 'In this tutorial, I am using heavily [Langsmith](https://www.langchain.com/langsmith),
    a platform for productionizing LLM applications. For example, while building the
    tree of thoughts prompts, I save my sub-prompts in the **prompts repository**
    and load them:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我重度使用[Langsmith](https://www.langchain.com/langsmith)，这是一个将 LLM 应用生产化的平台。例如，在构建思维树提示时，我将我的子提示保存在**提示仓库**中并加载它们：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can see in [this notebook](https://github.com/Rachnog/intro_to_llm_agents/blob/main/1_planning.ipynb)
    the result of such reasoning, the point I want to make here is the right process
    for defining your reasoning steps and versioning them in such **an LLMOps system
    like Langsmith**. Also, you can see other examples of popular reasoning techniques
    in public repositories like ReAct or Self-ask with search:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这个笔记本](https://github.com/Rachnog/intro_to_llm_agents/blob/main/1_planning.ipynb)中看到这种推理的结果，我想在这里强调的是定义推理步骤和在**像
    Langsmith 这样的 LLMOps 系统中进行版本控制的正确过程**。另外，你还可以在像 ReAct 或 Self-ask with search 这样的公开仓库中看到其他流行的推理技术示例：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Other notable approaches are:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其他值得注意的方法有：
- en: '**Reflexion** ([Shinn & Labash 2023](https://arxiv.org/abs/2303.11366)) is
    a framework to equip agents with dynamic memory and self-reflection capabilities
    to improve reasoning skills.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反思**（[Shinn & Labash 2023](https://arxiv.org/abs/2303.11366)）是一个框架，通过动态记忆和自我反思能力来装备代理，从而提高推理技能。'
- en: '**Chain of Hindsight** (CoH; [Liu et al. 2023](https://arxiv.org/abs/2302.02676))
    encourages the model to improve on its own outputs by explicitly presenting it
    with a sequence of past outputs, each annotated with feedback.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后见之明链**（CoH；[Liu et al. 2023](https://arxiv.org/abs/2302.02676)）鼓励模型通过明确地展示一系列过去的输出，每个输出都附有反馈，来改进其自身的输出。'
- en: 'Step 2: Memory'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 2：记忆
- en: '![](../Images/e3c6c61abf60034387cbdcb5f7007f75.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3c6c61abf60034387cbdcb5f7007f75.png)'
- en: We can map different types of memories in our brain to the components of the
    LLM agents' architecture
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将大脑中不同类型的记忆映射到LLM代理架构的各个组件上。
- en: '**Sensory Memory:** This component of memory captures immediate sensory inputs,
    like what we see, hear or feel. In the context of prompt engineering and AI models,
    a prompt serves as a transient input, similar to a momentary touch or sensation.
    It’s the initial stimulus that triggers the model’s processing.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**感官记忆：** 记忆的这一部分捕捉即时的感官输入，比如我们看到、听到或感受到的东西。在提示工程和AI模型的上下文中，提示作为一种瞬时输入，类似于短暂的触摸或感觉。它是触发模型处理的初步刺激。'
- en: '**Short-Term Memory:** Short-term memory holds information temporarily, typically
    related to the ongoing task or conversation. In prompt engineering, this equates
    to retaining the recent chat history. This memory enables the agent to maintain
    context and coherence throughout the interaction, ensuring that responses align
    with the current dialogue. **In code, you typically add it as conversation history**:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**短期记忆：** 短期记忆暂时保存信息，通常与当前的任务或对话相关。在提示工程中，这等同于保留最近的聊天记录。这种记忆使得代理能够保持上下文和连贯性，确保响应与当前对话一致。**在代码中，通常将其作为对话历史添加**：'
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Long-Term Memory:** Long-term memory stores both factual knowledge and procedural
    instructions. In AI models, this is represented by the data used for training
    and fine-tuning. Additionally, long-term memory supports the operation of RAG
    frameworks, allowing agents to access and integrate learned information into their
    responses. It’s like the comprehensive knowledge repository that agents draw upon
    to generate informed and relevant outputs. **In code, you typically add it as
    a vectorized database**:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长期记忆：** 长期记忆储存事实性知识和过程性指令。在AI模型中，这通过用于训练和微调的数据来表示。此外，长期记忆支持RAG框架的运作，允许代理访问并将学到的信息整合到它们的响应中。它就像是代理用来生成有根据和相关输出的综合知识库。**在代码中，通常将其作为向量化数据库添加**：'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 3: Tools'
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3步：工具
- en: '![](../Images/6585f6b90c5882221fd42d4f294545e9.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6585f6b90c5882221fd42d4f294545e9.png)'
- en: In practice, you want to augment your agent with a separate line of reasoning
    (which can be another LLM, i.e. domain-specific or another ML model for image
    classification) or with something more rule-based or API-based
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，你可能希望通过独立的推理线路增强你的代理（这可以是另一个LLM，例如领域特定的LLM，或者是用于图像分类的其他ML模型），或者通过一些更基于规则或API的方式来增强。
- en: ChatGPT [Plugins](https://openai.com/blog/chatgpt-plugins) and **OpenAI API**
    [function calling](https://platform.openai.com/docs/guides/gpt/function-calling)
    are good examples of LLMs augmented with tool use capability working in practice.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT [插件](https://openai.com/blog/chatgpt-plugins)和**OpenAI API** [函数调用](https://platform.openai.com/docs/guides/gpt/function-calling)是LLM增强工具使用能力在实际中工作的良好示例。
- en: '**Built-in Langchain tools**: Langchain has a [pleiad of built-in tools](https://python.langchain.com/docs/integrations/tools/)
    ranging from internet search and Arxiv toolkit to Zapier and Yahoo Finance. For
    this simple tutorial, we will experiment with the internet search provided by
    [Tavily](https://tavily.com/):'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内置Langchain工具：** Langchain提供了一个[内置工具库](https://python.langchain.com/docs/integrations/tools/)，包括互联网搜索、Arxiv工具包、Zapier和Yahoo
    Finance等。对于这个简单的教程，我们将尝试使用[Tavily](https://tavily.com/)提供的互联网搜索功能：'
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Custom tools**: it’s also very easy to define your own tools. Let’s dissect
    the simple example of a tool that calculates the length of the string. You need
    to use the `@tool`decorator to make Langchain know about it. Then, don’t forget
    about the type of input and the output. But the most important part will be the
    function comment between `""" """` — this is how your agent will know what this
    tool does and will compare this description to descriptions of the other tools:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义工具：** 定义自己的工具也非常简单。让我们分析一个计算字符串长度的简单工具示例。你需要使用`@tool`装饰器让Langchain知道它的存在。然后，不要忘记输入和输出的类型。但最重要的部分是`"""
    """`之间的函数注释 —— 这是你的代理知道这个工具做什么的方式，并且会将这个描述与其他工具的描述进行比较：'
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can find examples of how it works [in this script](https://github.com/Rachnog/intro_to_llm_agents/blob/main/3_tools.ipynb),
    **but you also can see an error** — it doesn't pull the correct description of
    the Neurons Lab company and despite calling the right custom function of length
    calculation, the final result is wrong. Let’s try to fix it!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这个脚本](https://github.com/Rachnog/intro_to_llm_agents/blob/main/3_tools.ipynb)中找到它是如何工作的示例，**但你也可以看到一个错误**
    —— 它没有提取到正确的Neurons Lab公司描述，尽管调用了正确的自定义长度计算函数，但最终结果还是错误的。让我们尝试修复它！
- en: 'Step 4: All together'
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤4：所有内容合并
- en: 'I am providing a clean version of combining all the pieces of architecture
    together [in this script](https://github.com/Rachnog/intro_to_llm_agents/blob/main/4_agents.ipynb).
    Notice, how we can easily decompose and define separately:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我提供了一个干净的版本，将所有架构的部分组合在一起，[请参见这个脚本](https://github.com/Rachnog/intro_to_llm_agents/blob/main/4_agents.ipynb)。注意，我们如何轻松地将其分解并分别定义：
- en: All kinds of **tools** (search, custom tools, etc)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种**工具**（搜索、自定义工具等）
- en: All kinds of **memories** (**sensory** as a prompt, **short-term** as runnable
    message history, and as a sketchpad within the prompt, and **long-term** as a
    retrieval from the vector database)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种**记忆**（**感官**作为提示，**短期**作为可运行的消息历史，以及作为提示中的草图板，**长期**作为从向量数据库中检索的内容）
- en: Any kind of **planning strategy** (as a **part of a prompt** pulled from the
    LLMOps system)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何类型的**规划策略**（作为从LLMOps系统提取的**提示**的一部分）
- en: 'The final definition of the agent will look as simple as this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的最终定义看起来将是如此简单：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see in the [outputs of the script](https://github.com/Rachnog/intro_to_llm_agents/blob/main/4_agents.ipynb)
    (or you can run it yourself), it solves the issue in the previous part related
    to tools. What changed? We defined a **complete architecture**, where short-term
    memory plays a crucial role. Our agent obtained **message history and a sketchpad
    as a part of the reasoning structure** which allowed it to pull the correct website
    description and calculate its length.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在[脚本输出](https://github.com/Rachnog/intro_to_llm_agents/blob/main/4_agents.ipynb)中看到的（或者你也可以自己运行它），它解决了前面部分中与工具相关的问题。发生了什么变化？我们定义了一个**完整的架构**，其中短期记忆起着至关重要的作用。我们的代理获得了**消息历史和草图板作为推理结构的一部分**，这使得它能够提取正确的网站描述并计算其长度。
- en: Outro
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: I hope this walkthrough through the core elements of the LLM agent architecture
    will help you design functional bots for the cognitive tasks you aim to automate.
    To complete, I would like to emphasize again the importance of having all elements
    of the agent in place. As we can see, missing short-term memory or having an incomplete
    description of a tool can mess with the agent’s reasoning and provide incorrect
    answers even for very simple tasks like summary generation and its length calculation.
    Good luck with your AI projects and don’t hesitate [to reach out](https://linktr.ee/alexhonchar)
    if you need help at your company!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这次对LLM代理架构核心元素的讲解能帮助你设计出能够自动化认知任务的功能性机器人。在此，我再次强调，确保代理的所有元素都到位的重要性。正如我们所看到的，缺失短期记忆或工具描述不完整，会干扰代理的推理，甚至导致非常简单任务（如摘要生成和长度计算）给出错误答案。祝你的AI项目好运，如果你在公司遇到问题，欢迎随时[联系我](https://linktr.ee/alexhonchar)！
