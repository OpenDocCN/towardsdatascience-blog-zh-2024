<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to build an OpenAI-compatible API</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to build an OpenAI-compatible API</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-an-openai-compatible-api-87c8edea2f06?source=collection_archive---------0-----------------------#2024-03-24">https://towardsdatascience.com/how-to-build-an-openai-compatible-api-87c8edea2f06?source=collection_archive---------0-----------------------#2024-03-24</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="9843" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Create a server to replicate OpenAI’s Chat Completions API, enabling any LLM to integrate with tools written for the OpenAI API</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@saarb?source=post_page---byline--87c8edea2f06--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Saar Berkovich" class="l ep by dd de cx" src="../Images/8a834597e8c6cce1b948f6aa17bfe8be.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*6amIrKbsTywZz-J8KaYaNg.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--87c8edea2f06--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@saarb?source=post_page---byline--87c8edea2f06--------------------------------" rel="noopener follow">Saar Berkovich</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--87c8edea2f06--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 24, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">11</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/3678fe82d7cec7940e7288da517234fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fi5nERyz9P8GLq62SyD-3Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image generated by the author using OpenAI DALL-E</figcaption></figure><p id="487d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is early 2024, and the Gen AI market is being dominated by <a class="af ny" href="https://iot-analytics.com/leading-generative-ai-companies/" rel="noopener ugc nofollow" target="_blank">OpenAI</a>. For good reasons, too — they have the first mover’s advantage, being the first to provide an easy-to-use API for an LLM, and they also offer arguably the most capable LLM to date, GPT 4. Given that this is the case, developers of all sorts of tools (<a class="af ny" href="https://docs.agpt.co/autogpt/" rel="noopener ugc nofollow" target="_blank">agents</a>, <a class="af ny" href="https://github.com/QuivrHQ/quivr" rel="noopener ugc nofollow" target="_blank">personal assistants</a>, <a class="af ny" href="https://github.com/jupyterlab/jupyter-ai" rel="noopener ugc nofollow" target="_blank">coding extensions</a>), have turned to OpenAI for their LLM needs.</p><p id="479b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While there are many reasons to fuel your Gen AI creations with OpenAI’s GPT, there are plenty of reasons to opt for an alternative. Sometimes, it might be less cost-efficient, and at other times your data privacy policy may prohibit you from using OpenAI, or maybe you’re hosting an open-source LLM (or your own).</p><p id="2685" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">OpenAI’s market dominance means that many of the tools you might want to use only support the OpenAI API. Gen AI &amp; LLM providers like OpenAI, Anthropic, and Google all seem to creating different API schemas (perhaps intentionally), which adds a lot of extra work for devs who want to support all of them.</p><p id="e44e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, as a quick weekend project, I decided to implement a Python <a class="af ny" href="https://fastapi.tiangolo.com/" rel="noopener ugc nofollow" target="_blank">FastAPI</a> server that is compatible with the OpenAI API specs, so that you can wrap virtually any LLM you like (either managed like Anthropic’s Claude, or self-hosted) to mimic the OpenAI API. Thankfully, the OpenAI API specs, have a <code class="cx nz oa ob oc b">base_url</code> parameter you can set to effectively point the client to your server, instead of OpenAI’s servers, and most of the developers of aforementioned tools allow you to set this parameter to your liking.</p><p id="1a96" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To do this, I’ve followed OpenAI’s Chat API reference openly available <a class="af ny" href="https://platform.openai.com/docs/api-reference/chat" rel="noopener ugc nofollow" target="_blank">here</a>, with some help from the code of <a class="af ny" href="https://github.com/vllm-project/vllm" rel="noopener ugc nofollow" target="_blank">vLLM</a>, an Apache-2.0 licensed inference server for LLMs that also offers OpenAI API compatibility.</p><h2 id="38b1" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Game Plan</h2><p id="dcf9" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">We will be building a mock API that mimics the way OpenAI’s Chat Completion API (<code class="cx nz oa ob oc b">/v1/chat/completions</code>) works. While this implementation is in Python and uses FastAPI, I kept it quite simple so that it can be easily transferable to another modern coding language like TypeScript or Go. We will be using the Python official <a class="af ny" href="https://github.com/openai/openai-python" rel="noopener ugc nofollow" target="_blank">OpenAI client library</a> to test it — the idea is that if we can get the library to think our server is OpenAI, we can get any program that uses it to think the same.</p></div></div></div><div class="ab cb pd pe pf pg" role="separator"><span class="ph by bm pi pj pk"/><span class="ph by bm pi pj pk"/><span class="ph by bm pi pj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="12ee" class="pl oe fq bf of pm pn gq oj po pp gt on pq pr ps pt pu pv pw px py pz qa qb qc bk">First step — chat completions API, no streaming</h1><p id="eb2a" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">We’ll start with implementing the non-streaming bit. Let’s start with modeling our request:</p><pre class="mm mn mo mp mq qd oc qe bp qf bb bk"><span id="df93" class="qg oe fq oc b bg qh qi l qj qk">from typing import List, Optional<br/><br/>from pydantic import BaseModel<br/><br/><br/>class ChatMessage(BaseModel):<br/>    role: str<br/>    content: str<br/><br/>class ChatCompletionRequest(BaseModel):<br/>    model: str = "mock-gpt-model"<br/>    messages: List[ChatMessage]<br/>    max_tokens: Optional[int] = 512<br/>    temperature: Optional[float] = 0.1<br/>    stream: Optional[bool] = False</span></pre><p id="0db9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <a class="af ny" href="https://docs.pydantic.dev/latest/" rel="noopener ugc nofollow" target="_blank">PyDantic</a> model represents the request from the client, aiming to replicate the API reference. For the sake of brevity, this model does not implement the entire specs, but rather the bare bones needed for it to work. If you’re missing a parameter that is a part of the <a class="af ny" href="https://platform.openai.com/docs/api-reference/chat" rel="noopener ugc nofollow" target="_blank">API specs</a> (like <code class="cx nz oa ob oc b">top_p</code>), you can simply add it to the model.</p><p id="d105" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <code class="cx nz oa ob oc b">ChatCompletionRequest</code> models the parameters OpenAI uses in their requests. The chat API specs require specifying a list of <code class="cx nz oa ob oc b">ChatMessage</code> (like a chat history, the client is usually in charge of keeping it and feeding back in at every request). Each chat message has a <code class="cx nz oa ob oc b">role</code> attribute (usually <code class="cx nz oa ob oc b">system</code>, <code class="cx nz oa ob oc b">assistant</code> , or <code class="cx nz oa ob oc b">user</code> ) and a <code class="cx nz oa ob oc b">content</code> attribute containing the actual message text.</p><p id="3353" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, we’ll write our FastAPI chat completions endpoint:</p><pre class="mm mn mo mp mq qd oc qe bp qf bb bk"><span id="3e5d" class="qg oe fq oc b bg qh qi l qj qk">import time<br/><br/>from fastapi import FastAPI<br/><br/>app = FastAPI(title="OpenAI-compatible API")<br/><br/>@app.post("/chat/completions")<br/>async def chat_completions(request: ChatCompletionRequest):<br/>    <br/>    if request.messages and request.messages[0].role == 'user':<br/>      resp_content = "As a mock AI Assitant, I can only echo your last message:" + request.messages[-1].content<br/>    else:<br/>      resp_content = "As a mock AI Assitant, I can only echo your last message, but there were no messages!"<br/><br/>    return {<br/>        "id": "1337",<br/>        "object": "chat.completion",<br/>        "created": time.time(),<br/>        "model": request.model,<br/>        "choices": [{<br/>            "message": ChatMessage(role="assistant", content=resp_content)<br/>        }]<br/>    }</span></pre><p id="b154" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">That simple.</p><h2 id="7211" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Testing our implementation</h2><p id="4c68" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">Assuming both code blocks are in a file called <code class="cx nz oa ob oc b">main.py</code>, we’ll install two Python libraries in our environment of choice (always best to create a new one): <code class="cx nz oa ob oc b">pip install fastapi openai</code> and launch the server from a terminal:</p><pre class="mm mn mo mp mq qd oc qe bp qf bb bk"><span id="ca4d" class="qg oe fq oc b bg qh qi l qj qk">uvicorn main:app</span></pre><p id="0cf2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using another terminal (or by launching the server in the background), we will open a Python console and copy-paste the following code, taken straight from <a class="af ny" href="https://github.com/openai/openai-python#Usage" rel="noopener ugc nofollow" target="_blank">OpenAI’s Python Client Reference</a>:</p><pre class="mm mn mo mp mq qd oc qe bp qf bb bk"><span id="9a69" class="qg oe fq oc b bg qh qi l qj qk">from openai import OpenAI<br/><br/># init client and connect to localhost server<br/>client = OpenAI(<br/>    api_key="fake-api-key",<br/>    base_url="http://localhost:8000" # change the default port if needed<br/>)<br/><br/># call API<br/>chat_completion = client.chat.completions.create(<br/>    messages=[<br/>        {<br/>            "role": "user",<br/>            "content": "Say this is a test",<br/>        }<br/>    ],<br/>    model="gpt-1337-turbo-pro-max",<br/>)<br/><br/># print the top "choice" <br/>print(chat_completion.choices[0].message.content)</span></pre><p id="cf5f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you’ve done everything correctly, the response from the server should be correctly printed. It’s also worth inspecting the <code class="cx nz oa ob oc b">chat_completion</code> object to see that all relevant attributes are as sent from our server. You should see something like this:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ql"><img src="../Images/b41ac9b9749709c7180ac0bf9d59a281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ULB-ja65pkGyZl157oZa1Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Code by the author, formatted using <a class="af ny" href="https://carbon.now.sh/" rel="noopener ugc nofollow" target="_blank">Carbon</a></figcaption></figure><h1 id="8c6e" class="pl oe fq bf of pm qm gq oj po qn gt on pq qo ps pt pu qp pw px py qq qa qb qc bk">Leveling up — supporting streaming</h1><p id="87d1" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">As LLM generation tends to be slow (computationally expensive), it’s worth streaming your generated content back to the client, so that the user can see the response as it’s being generated, without having to wait for it to finish. If you recall, we gave <code class="cx nz oa ob oc b">ChatCompletionRequest</code> a boolean <code class="cx nz oa ob oc b">stream</code> property — this lets the client request that the data be streamed back to it, rather than sent at once.</p><p id="2a7e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This makes things just a bit more complex. We will create a <a class="af ny" href="https://wiki.python.org/moin/Generators" rel="noopener ugc nofollow" target="_blank">generator function</a> to wrap our mock response (in a real-world scenario, we will want a generator that is hooked up to our LLM generation)</p><pre class="mm mn mo mp mq qd oc qe bp qf bb bk"><span id="b3e7" class="qg oe fq oc b bg qh qi l qj qk">import asyncio<br/>import json<br/><br/>async def _resp_async_generator(text_resp: str):<br/>    # let's pretend every word is a token and return it over time<br/>    tokens = text_resp.split(" ")<br/><br/>    for i, token in enumerate(tokens):<br/>        chunk = {<br/>            "id": i,<br/>            "object": "chat.completion.chunk",<br/>            "created": time.time(),<br/>            "model": "blah",<br/>            "choices": [{"delta": {"content": token + " "}}],<br/>        }<br/>        yield f"data: {json.dumps(chunk)}\n\n"<br/>        await asyncio.sleep(1)<br/>    yield "data: [DONE]\n\n"</span></pre><p id="f7d8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And now, we would modify our original endpoint to return a StreamingResponse when <code class="cx nz oa ob oc b">stream==True</code></p><pre class="mm mn mo mp mq qd oc qe bp qf bb bk"><span id="b966" class="qg oe fq oc b bg qh qi l qj qk">import time<br/><br/>from starlette.responses import StreamingResponse<br/><br/>app = FastAPI(title="OpenAI-compatible API")<br/><br/>@app.post("/chat/completions")<br/>async def chat_completions(request: ChatCompletionRequest):<br/>    <br/>    if request.messages:<br/>      resp_content = "As a mock AI Assitant, I can only echo your last message:" + request.messages[-1].content<br/>    else:<br/>      resp_content = "As a mock AI Assitant, I can only echo your last message, but there wasn't one!"<br/>    if request.stream:<br/>      return StreamingResponse(_resp_async_generator(resp_content), media_type="application/x-ndjson")<br/><br/>    return {<br/>        "id": "1337",<br/>        "object": "chat.completion",<br/>        "created": time.time(),<br/>        "model": request.model,<br/>        "choices": [{<br/>            "message": ChatMessage(role="assistant", content=resp_content)        }]<br/>    }</span></pre><h2 id="555c" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Testing the streaming implementation</h2><p id="fbd3" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">After restarting the uvicorn server, we’ll open up a Python console and put in this code (again, taken from OpenAI’s library docs)</p><pre class="mm mn mo mp mq qd oc qe bp qf bb bk"><span id="62b0" class="qg oe fq oc b bg qh qi l qj qk">from openai import OpenAI<br/><br/># init client and connect to localhost server<br/>client = OpenAI(<br/>    api_key="fake-api-key",<br/>    base_url="http://localhost:8000" # change the default port if needed<br/>)<br/><br/>stream = client.chat.completions.create(<br/>    model="mock-gpt-model",<br/>    messages=[{"role": "user", "content": "Say this is a test"}],<br/>    stream=True,<br/>)<br/>for chunk in stream:<br/>    print(chunk.choices[0].delta.content or "")</span></pre><p id="6feb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You should see each word in the server’s response being slowly printed, mimicking token generation. We can inspect the last <code class="cx nz oa ob oc b">chunk</code> object to see something like this:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ql"><img src="../Images/ad32225e29fca2ff7ea30226264ff67f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*smlnT9D-fYUIbU6pb0fg-g.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Code by the author, formatted using <a class="af ny" href="https://carbon.now.sh/" rel="noopener ugc nofollow" target="_blank">Carbon</a></figcaption></figure><h2 id="abe3" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Putting it all together</h2><p id="d079" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">Finally, in the gist below, you can see the entire code for the server.</p><figure class="mm mn mo mp mq mr"><div class="qr io l ed"><div class="qs qt l"/></div></figure></div></div></div><div class="ab cb pd pe pf pg" role="separator"><span class="ph by bm pi pj pk"/><span class="ph by bm pi pj pk"/><span class="ph by bm pi pj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2d03" class="pl oe fq bf of pm pn gq oj po pp gt on pq pr ps pt pu pv pw px py pz qa qb qc bk">Final Notes</h1><ul class=""><li id="f02c" class="nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx qu qv qw bk">There are many other interesting things we can do here, like supporting other request parameters, and other OpenAI abstractions like Function Calls and the Assistant API.</li><li id="5ee5" class="nc nd fq ne b go qx ng nh gr qy nj nk nl qz nn no np ra nr ns nt rb nv nw nx qu qv qw bk">The lack of standardization in LLM APIs makes it difficult to switch providers, for both companies and developers of LLM-wrapping packages. In the absence of any standard, the approach I’ve taken here is to abstract the LLM behind the specs of the biggest and most mature API.</li></ul></div></div></div></div>    
</body>
</html>