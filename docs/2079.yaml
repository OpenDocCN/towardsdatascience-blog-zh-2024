- en: Exploring the Strategic Capabilities of LLMs in a Risk Game Setting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/exploring-the-strategic-capabilities-of-llms-in-a-risk-game-setting-43c868d83c3b?source=collection_archive---------1-----------------------#2024-08-27](https://towardsdatascience.com/exploring-the-strategic-capabilities-of-llms-in-a-risk-game-setting-43c868d83c3b?source=collection_archive---------1-----------------------#2024-08-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[STRATEGIC AI](https://medium.com/@hc.ekne/list/strategic-ai-72a460668137)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a simulated Risk environment, large language models from Anthropic, OpenAI,
    and Meta showcase distinct strategic behaviors, with Claude Sonnet 3.5 edging
    out a narrow lead
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hc.ekne?source=post_page---byline--43c868d83c3b--------------------------------)[![Hans
    Christian Ekne](../Images/c85483d8b5dd89584b996b321b7f4a45.png)](https://medium.com/@hc.ekne?source=post_page---byline--43c868d83c3b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--43c868d83c3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--43c868d83c3b--------------------------------)
    [Hans Christian Ekne](https://medium.com/@hc.ekne?source=post_page---byline--43c868d83c3b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--43c868d83c3b--------------------------------)
    ·32 min read·Aug 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6985aa99f03d8aafc79c081e0372f271.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author using DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, large language models (LLMs) have rapidly become a part of
    our everyday lives. Since OpenAI blew our minds with GPT-3 we have witnessed a
    profound increase in the capabilities of the models. They are excelling in a myriad
    of different tests, in anything from language comprehension to reasoning and problem-solving
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: One topic that I find particularly compelling — and perhaps under-explored —
    is the ability of LLMs to reason strategically. That is, how the models will act
    if you insert them into a situation where the outcome of their decisions depends
    not only on their own actions but also on the actions of others, who are also
    making decisions based on their own goals. The LLMs’ ability to think and act
    strategically is increasingly important as we weave them into our products and
    services, and especially considering the emerging risks associated with powerful
    AIs.
  prefs: []
  type: TYPE_NORMAL
- en: A decade ago, philosopher and author Nick Bostrom brought AI risk into the spotlight
    with his influential book *Superintelligence*. He started a global conversation
    about AI, and it brought AI as an existential risk into the popular debate. Although
    the LLMs are still far from Bostrom’s superintelligence, it’s important to keep
    an eye on their strategic capabilities as we integrate them tighter into our daily
    lives.
  prefs: []
  type: TYPE_NORMAL
- en: When I was a child, I used to love playing board games, and Risk was one of
    my favorites. The game requires a great deal of strategy and if you don’t think
    through your moves, you will likely be decimated by your opponents. Risk serves
    as a good proxy for evaluating strategic behavior, because making strategic decisions
    often involves weighing potential gains against uncertain outcomes, and while
    for small troop sizes, luck clearly plays an big part, given enough time and larger
    army sizes, the luck component becomes less pronounced and the most skillful players
    emerge. So, what better arena to test the LLMs’ strategic behavior than Risk!
  prefs: []
  type: TYPE_NORMAL
- en: In this article I explore two main topics related to LLMs and strategy. Firstly,
    which of the top LLM models is the most strategic Risk player and how strategic
    is the best model in its actions? Secondly, how have the strategic capabilities
    of the models developed through the model iterations?
  prefs: []
  type: TYPE_NORMAL
- en: To answer these questions, I built a virtual Risk game engine and let the LLMs
    battle it out. The first part of this article will explore some of the details
    of the game implementation before we move on to analyzing the results. We then
    discuss how the LLMs approached the game and their strategic abilities and shortcomings,
    before we end with a section on what these results mean and what we can expect
    from future model generations.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the Stage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/a36d9b074821175cb83aa682b4ab4319.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by author using DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: '**Why Risk?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: My own experience playing Risk obviously played a part in choosing this game
    as a testbed for the LLMs. The game requires players to understand how their territories
    are linked, balance offense with defense and all while planning long-term strategies.
    Elements of uncertainty are also introduced through dice rolls and unpredictable
    opponent behavior, challenging AI models to manage risk and adapt to changing
    conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Risk simulates real-world strategic challenges, such as resource allocation,
    adaptability, and pursuing long-term goals amid immediate obstacles, making it
    a valuable proxy for evaluating AI’s strategic capabilities. By placing LLMs in
    this environment, we can observe how well they handle these complexities compared
    to human players.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Modelling Environment**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To conduct the experiments, I created a small python package creatively named
    `risk_game`. (See the appendix for how to get started running this on your own
    machine.) The package is a Risk game engine, and it allows for the simulation
    of games played by LLMs. (The non-technical reader can safely skip this part and
    continue to the section “The Flow of the Game”.)
  prefs: []
  type: TYPE_NORMAL
- en: To make it easier to conceptually keep track of the moving parts, I followed
    an object-oriented approach to the package development, where I developed a few
    different key classes to run the simulation. This includes a game master class
    to control the flow of the game, a player class to control prompts sent to the
    LLMs and a game state class to control the state of the game, including which
    player controls which territories and how many troops they hold at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: 'I tried to make it a flexible and extensible solution for AI-driven strategy
    simulations, and the package could potentially be modified to study strategic
    behavior of LLMs in other settings as well. See below for a full overview of the
    package structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To run an experiment, I would first instantiate a `GameConfig` object. This
    config objects holds all the related game configuration settings, like whether
    we played with progressive cards, whether or not capitals mode was active, and
    how high percent of the territories needed to be controlled to win, in addition
    to multiple other game settings. I would then used that to create an instance
    of the `Experiment` class and call the `run_experiment` method.
  prefs: []
  type: TYPE_NORMAL
- en: Diving deeper behind the scenes we can see how the `Experiment`class is set
    up.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: From the code above, we see that the `run_experiment()` method will run the
    number of games that are specified in the initialization of the `Experiment` object.
    The first thing that happens is to initialize a game, and the first thing we need
    to do is to create the rules and instantiate at game with the `GameMaster` class.
    Subsequently, the chosen mix of LLM player agents are added to the game. This
    concludes the necessary pre-game set-up and we use the games’ `play_game()`method
    to start playing a game.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid becoming too technical I will skip over most of the code details for
    now, and rather refer the interested reader to the Github repo below. Check out
    the `README` to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/hcekne/risk-game.git?source=post_page-----43c868d83c3b--------------------------------)
    [## GitHub — hcekne/risk-game'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to hcekne/risk-game development by creating an account on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/hcekne/risk-game.git?source=post_page-----43c868d83c3b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The Flow of the Game
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the game begins, the LLM player agents are prompted to do initial troop
    placement. The agents take turns placing their troops on their territories until
    all their initial troops have been exhausted.
  prefs: []
  type: TYPE_NORMAL
- en: 'After initial troop placement, the first player starts its turn. In Risk a
    turn is comprised of the 3 following phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Phase 1: Card trading and troop placement.** If a player agent wins an attack
    during its turn, it gains a card. Once it has three cards, it can trade those
    in for troops if has the correct combination of infantry, cavalry, artillery or
    wildcard. The player also receives troops as a function of how many territories
    it controls and also if controls any continents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Phase 2: Attack.** In this phase the player agent can attack other players
    and take over their territories. It is a good idea to attack because that allows
    the player to gain a card for that turn and also gain more territories. The player
    agent can attack as many times as it wishes during a turn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Phase 3: Fortify.** The last phase is the fortify phase, and now the player
    is allowed to move troops from one of its territories to another. However, the
    territories must be connected by territories the player controls. The player is
    only allowed one such fortify move. After this the is finished, the next player
    starts his turn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the beginning of each turn, the LLM agents receive dynamically generated
    prompts to formulate their strategy. This strategy-setting prompt provides the
    agent with the current game rules, the state of the board, and possible attack
    vectors. The agent’s response to this prompt guides its decisions throughout the
    turn, ensuring that its actions align with an overall strategic plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'The request for strategy prompt is given below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the prompt above, there are multiple dynamically generated
    elements that help the player agent better understand the game context and make
    more informed strategic decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'These dynamically produced elements include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rules:** The rules of the game such as whether capitals mode is activated,
    how many percent of the territories are needed to secure a win, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Current game state**: This is presented to the agent as the different continents
    and the'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Formatted Attack Vectors:** These are a collection of the possible territories
    the agent can launch an attack from, to which territories it can attack and the
    maximum number of troops the agent can attack with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The extra territories needed to win the game:** This represents the remaining
    territories the agent needs to capture to win the game. For example, if the total
    territories required to win the game are 28 and the agent holds 25 territories,
    this number would be 3 and would maybe encourage the agent to develop a more aggressive
    strategy for that turn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each specific action during the turn — whether it’s placing troops, attacking,
    or fortifying — the agent is given tailored prompts that reflect the current game
    situation. Thankfully, Risk’s gameplay can be simplified because it adheres to
    the Markov property, meaning that optimal moves depend only on the current game
    state, not on the history of moves. This allows for streamlined prompts that focus
    on the present conditions
  prefs: []
  type: TYPE_NORMAL
- en: The Experimental Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/dc054b42a45908a69df85bf2d0b5a9b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To explore the strategic capabilities of LLMs, I designed two main experiments.
    These experiments were crafted to address two key questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What is the top performing LLM, and how strategic is it in its actions?*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Is there a progression in the strategic capabilities of the LLMs through model
    iterations?*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both of these questions can be answered by running two different experiments,
    with a slightly different mix of AI agents.
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment-1: Evaluating the Top Models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the first question, I created an experiment using the following top LLM
    models as players:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s GPT-4o running off the OpenAI API endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anthropic’s claude-3–5-sonnet-20240620 running off the Anthropic API endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meta’s llama-3.1–70b-versatile running of the Groq API endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I obviously wanted to try Meta’s meta.llama3–1–405b-instruct-v1:0 and configured
    it to run off AWS Bedrock, however the response time was painfully slow and made
    simulating games take forever. This is why we run Meta’s 70b model on Groq. It’s
    much faster than AWS bedrock. (If anyone knows how to speed up llama3.1 405b on
    AWS please let me know!)
  prefs: []
  type: TYPE_NORMAL
- en: 'And we formulate our null and alternative hypotheses as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment-1, H0 :** There is no difference in performance among the models;
    each model has an equal probability of winning.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Experiment-1, H1​**: At least one model performs better (or worse) than the
    others, indicating that the models do not have equal performance.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Experiment-2: Analyzing the Model Generations**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second experiment aimed to evaluate how strategic capabilities have progressed
    through different iterations of OpenAI’s models. For this, I selected three models:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4o
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-4o-mini
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-3.5-turbo-0125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experiment-2** allows us to see how the strategic capabilities of the models
    have developed across model generations, and also allows us to analyze the difference
    between different size models in the same model generation (GPT-4o vs GPT-4o-mini).
    I chose OpenAI’s solutions because they didn’t have the same restrictive rate
    limits as the other providers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, as for **Experiment-1**, for this experiment we can formulate our
    null and alternative hypotheses:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment-2, H0:** There is no difference in performance among the models;
    each model has an equal probability of winning'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Experiment-2, H1A​**: GPT-4o is better than GPT-4o-mini'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Experiment-2, H1B:** GPT-4o and GPT-4o-mini are better than GPT-3.5-turbo'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Game Setup, Victory Conditions & Card Bonuses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Both experiments involved 10 games, each with the same victory conditions.
    There are multiple different victory conditions in Risk, and typical victory conditions
    that players can agree upon are:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of controlled territories required for the winner. “World domination”
    is subset of this where one player needs to control all the territories. Other
    typical territory conditions are 70% territory control.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Number of controlled continent(s) required for the winner
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Control / possession of key areas required for the winner
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Preset time / turn count: whoever controls the most territories after x hours
    or x turns wins.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the end I settled for a more pragmatic approach which was a a combination
    of victory conditions that would be easier to fulfill and progressive cards. The
    victory conditions for the games in the experiments were finally chosen to be:'
  prefs: []
  type: TYPE_NORMAL
- en: First agent to reach 65% territory dominance or
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent with the most territories after 17 game rounds of play (Making the
    full game be concluded after at most 51 turns distributed across the three players.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For those of you unfamiliar with Risk, progressive cards means that the value
    of the traded cards increase progressively as the game goes on, which is contrasted
    by fixed cards, where the troop value of traded cards are the same throughout
    the game. (4,6,8,10 for the different combinations.) Progressive is generally
    accepted to be a faster game mode.
  prefs: []
  type: TYPE_NORMAL
- en: The Results — Who Conquered the World?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/026bdd04bf3a9d830e8e4c046b982a41.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author using DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment-1: The Top Models'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The results were actually quite astounding — for both experiments. Starting
    with the first, below we show the distribution of wins amongst the three agents.
    Anthropic’s Claude is the winner with 5 wins in total, second place goes to OpenAI’s
    GPT-4o with 3 wins and last place to Meta’s llama3.1 with 2 wins.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6788b3e54ad8d81ba78591808ecfa9c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Experiment-1 wins by player, grouped by victory condition / image
    by author
  prefs: []
  type: TYPE_NORMAL
- en: Because of their long history and early success with GPT-3 I was expecting OpenAI's
    model to be the winner, but it ended up being Anthropic’s Claude which took a
    lead in overall games. I guess if we take a look at how Claude is performing on
    [benchmark tests](https://www.anthropic.com/news/claude-3-5-sonnet), it shouldn’t
    be too unexpected that they come out ahead.
  prefs: []
  type: TYPE_NORMAL
- en: Territory Control and Game Flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we dive a little deeper in the overall flow of the game and evaluate the
    distribution of territories throughout the game, we find the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa1eddf080dd03ff5fde4148032d1d50.png)![](../Images/6f94cec739c08a2a3b1883dc0577085c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Experiment-1 territory control per turn / image by author
  prefs: []
  type: TYPE_NORMAL
- en: When we examine the distribution of territories throughout the games, a clearer
    picture emerges. On average, Claude managed to gain a lead in territory control
    midway through most games and maintained that lead until the end. Interestingly,
    there was only one instance where a player was eliminated from the game entirely
    — this happened in Game 8, where Llama 3.1 was knocked out around turn 27.
  prefs: []
  type: TYPE_NORMAL
- en: In our analysis, a “turn” refers to the full set of moves made by one player
    during their turn. Since we had three agents participating, each game round typically
    involved three turns, one for each player. As players were eliminated, the number
    of turns per round naturally decreased.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Looking at the evolution of troop strength and territory control we find the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5574ac0390ea9bc95173bd127e2eb94b.png)![](../Images/72b526b75082936ab08dead7e440d04d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Experiment-1 change in troop strength throughout the game / image
    by author
  prefs: []
  type: TYPE_NORMAL
- en: The troop strength seems to be relatively even, on average, for all the models,
    so that is clearly not the reason why Claude is able to pull off the most wins.
  prefs: []
  type: TYPE_NORMAL
- en: 'Statistical Analysis: Is Claude Really the Best?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this experiment, I aimed to determine whether any of the three models demonstrated
    significantly better performance than the others based on the number of wins.
    Given that the outcome of interest was the frequency of wins across multiple categories
    (the three models), the chi-square goodness-of-fit test is a good statistical
    tool to use.
  prefs: []
  type: TYPE_NORMAL
- en: The test is often used to compare observed frequencies against expected frequencies
    under the null hypothesis, which in this case was that all models would have an
    equal probability of winning. By applying the chi-square test, I could assess
    whether the distribution of wins across the models deviated significantly from
    the expected distribution, thereby helping to identify if any model performed
    substantially better.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The chi-square goodness-of-fit test was conducted based on the observed wins
    for the three models: 5 wins for Claude, 3 wins for GPT-4o, and 2 wins for llama3.1\.
    Under the null hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment-1, H0 :** There is no difference in performance among the models;
    each model has an equal probability of winning.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: each model was expected to win approximately 3.33 games out of the 10 trials.
    The chi-square test yielded a statistic of 1.4 with a corresponding p-value of
    0.497\. Since this p-value is much larger than the conventional significance level
    of 0.05, we can’t really say with any statistical rigor that Claude is better
    than the others.
  prefs: []
  type: TYPE_NORMAL
- en: We can interpret the p-value such that there is a 49.7% chance that we would
    observe an outcome as extreme as (5,3,2) under the null hypothesis, which assumes
    each model has the same probability of winning. So this is actually quite a likely
    scenario to observe.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To make a definitive conclusion, we would need to run more experiments with
    a larger sample size. Unfortunately, rate limits — particularly with Llama 3.1
    hosted on Groq — made this impractical. I invite the eager reader to follow up
    and test themselves. See the appendix for how to run the experiments on your own
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment-2: Model Generations'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The results of Experiment-2 were equally surprising. Contrary to expectations,
    GPT-4o-mini outperformed both GPT-4o and GPT-3.5-turbo. GPT-4o-mini secured 7
    wins, while GPT-4o managed 3 wins, and GPT-3.5-turbo failed to win any games.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b907976ce49a154ef432f34e1646c86.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8\. Number of wins by player and victory condition / image by author
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4o-mini actually went off with the overall victory. This was also rather
    substantial, with 7 wins of GPT-4o’s 3 and GPT-3.5 turbo’s 0 wins. While GPT-4o
    on average had more troops GPT-4o-mini won most of the games.
  prefs: []
  type: TYPE_NORMAL
- en: Territory Control and Troop Strength
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Again, diving deeper and looking at performance in individual games, we find
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/652e25d45a39e3b1f0977b557ad2dea5.png)![](../Images/c6b8388c2167e89e5993f197a2a45306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9\. Experiment-2 Average territory control per turn, for all games /
    image by author
  prefs: []
  type: TYPE_NORMAL
- en: The charts above show territory control per turn, on average, as well as for
    all the games. These plots show a confirmation of what we saw in the overall win
    statistics, namely that GPT-4o-mini is on average coming out with the lead in
    territory control by the end of the games. GPT-4o-mini is beating its big brother
    when it actually counts, close to the end of the game!
  prefs: []
  type: TYPE_NORMAL
- en: 'Turning around and examining troop strength, a slightly different picture emerges:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d6a45f00d2db026a2cc801099fc23a5.png)![](../Images/525b3f22fd081aa7e8afc38c2e3a07ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10\. Experiment-2 Average total troop strength per turn, for all games
    / image by author
  prefs: []
  type: TYPE_NORMAL
- en: The above chart shows that on average, the assumed strongest player, GPT-4o
    manages to keep the highest troop strength throughout most of the games. Surprisingly
    it fails to use this troop strength to its advantage! Also, there is a clear trend
    between troop strength and model size and model generation.
  prefs: []
  type: TYPE_NORMAL
- en: To get some more insights we can also evaluate a few games more in detail and
    look at the heatmap of controlled territories across the turns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/043d2693f7293e2f52cc8cb6cfbc7703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11\. Experiment 2, heatmap of territory control, game 2 / image by author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9888ffe2054a2e82bc5e55e6aea482b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12\. Experiment 2, heatmap of territory control, game 7 / image by author
  prefs: []
  type: TYPE_NORMAL
- en: From the heatmaps we see how the models trade blows and grab territories from
    another. Here we have selected two games which seemed reasonably representative
    for the 10 games in the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding specific territory ownership, a trend we saw play out frequently was
    GPT-4o trying to hold North America while GPT-4o-mini often tried to get Asia.
  prefs: []
  type: TYPE_NORMAL
- en: 'Statistical Analysis: Generational Differences'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the above results, let’s again revisit our initial hypotheses:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment-2, H0 :** There is no difference in performance among the models;
    each model has an equal probability of winning.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Experiment-2, H1A​**: GPT-4o is better than GPT-4o-mini'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Experiment-2, H1B:** GPT-4o and GPT-4o-mini are better than GPT-3.5-turbo'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s start with the easy one, **H1B,** namely that GPT-4o and GPT-4o-mini are
    better than GPT-3.5-turbo. This is quite easy to see, and we can do a chi-squared
    test again, based on equal probabilities of winning for each model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This suggests that the observed distribution of wins is unlikely to have occurred
    if every model had the same probability of winning, 33.3%. In fact, a case as
    extreme as this could only be expected to have occurred in 2.5% of cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'To then evaluate our **H1A** hypothesis we should first update our null hypothesis
    adjusting for unequal probabilities of winning. For example, we can now assume
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4o-mini: Higher probability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-4o: Higher probability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-3.5-turbo: Lower probability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Putting some numbers on these, and given the results we just observed, let’s
    assume GPT-4o-mini:'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-40-mini: 45% chance of winning each game'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-4o: 45% chance of winning each game'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-3.5-turbo: 10% chance of winning each game'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, for 10 games, the expected wins would be:'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4o-mini: 0.45×10=4.5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-4o: 0.45 ×10=4.5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-3.5-turbo: 0.1×10=10 → .1×10=1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, given the fact that GPT-4o-mini won 7 out of the 10 games, we
    also revise our alternative hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment-2 Revised Hypothesis, H1AR**: GPT-4o-mini is better than GPT-4o.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Using python to calculate the chi-squared test, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: With our updated probabilities, we see from the code above that seeing a result
    as extreme as we did, (7,3,0) is in fact not very unlikely under our new updated
    expected probabilities. Interpreting the p-value tells us that a results at least
    as extreme as what we observed would be expected 23% of the time. So, we cannot
    conclude with any statistical significance that there is a difference between
    GPT-4o-mini and GPT-4o and we reject the revised alternative hypothesis, **H1AR.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Key Takeaways**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although there is only limited evidence to suggest Claude is the more strategic
    model, we can with reasonably high confidence state that there is a difference
    in performance across model generations. GPT-3.5-turbo is significantly less strategic
    than its newer iterations. Obviously this implication works in reverse, which
    means we are seeing an increase in the strategic abilities of the models as they
    improve through the generations, and this is likely to profoundly impact how these
    models will be used in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the Strategic Behavior of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/58a82733ec071f0d0f915de1aab5cfd4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author using DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: One of the first things I noticed after running some initial tests were how
    different the LLMs play than humans. The LLM games seem to have more turns than
    human games, even after I prompted them to be more aggressive and try to go after
    weak opponents.
  prefs: []
  type: TYPE_NORMAL
- en: 'While many of the observations about player strategy can be made just from
    looking at plots of territory control and troop strength; some of the more detailed
    observations below first became clear as I watched the LLMs play turn-by-turn.
    This is slightly hard to replicate in an article format, however all the data
    from both experiments are stored in .csv files in the Github repo and loaded into
    pandas dataframes in the Jupyter notebooks used for analysis. The interested reader
    can find them in the repo here: `/game_analysis/experiment1_analysis_notebook.ipynb`.
    The dataframe `experiment1_game_data_df`holds all relevant game data for Experiment-1\.
    By looking at territory ownership and troop control turn-by-turn more details
    about the playstyles emerge.'
  prefs: []
  type: TYPE_NORMAL
- en: Distinctive Winning Play Styles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What seemed to distinguish Anthropic’s model was its ability to claim a lot
    of territory in one move. This can be seen in some of the plots of territory control,
    when you look at individual games. But even though Claude had the most wins, how
    strategic was it really? Based on what I observed in the experiments, it seems
    that the LLMs are still rather immature when it comes to strategy. Below we discuss
    some of the typical behavior observed through the games.
  prefs: []
  type: TYPE_NORMAL
- en: Poor Fortifying Strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common issue across all models was a failure to adequately fortify their borders.
    Quite frequently, the times the agents were also stuck with a lot of troops inside
    their internal territory instead of guarding their borders. This made it easier
    for neighbors to attack their territories and steal continent bonuses. In addition,
    it made it more difficult for the player agents to actually do a larger land-grab
    since often their territories with large troop strengths were surrounded by other
    territories it controlled.
  prefs: []
  type: TYPE_NORMAL
- en: Failure to See Winning Moves
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another noticeable shortcoming was the models’ failure to recognize winning
    moves. They don’t seem to realize that they can win in a turn if they play correctly.
    Less so with the stronger models but still present.
  prefs: []
  type: TYPE_NORMAL
- en: For example, for all the games in the simulations we played with 65% territory
    control to win. This means you just need to acquire 28 territories. In one instance
    during Experiment-2 Game 2, OpenAI’s GPT-4o had 24 territories and 19 troops in
    Greenland. It could easily just have taken Europe which has several territories
    with just 1 troop, however, it fails to see the move. This is a move that even
    a relatively inexperienced human player would likely recognize.
  prefs: []
  type: TYPE_NORMAL
- en: Failure to Eliminate Other Players
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The models also frequently failed to eliminate opponents with only a few troops
    remaining, even when it would have been strategically advantageous. More specifically,
    they fail to remove players with only a few troops left and more than 2 cards.
    This would be considered an easy move for most human players, especially when
    playing with progressive cards. The card bonuses quickly escalate, and if an opponent
    only has 10 troops left, but 3 or more cards, taking him down for the cards is
    almost always the right move.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4o Likes North America
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the very typical strategies that I saw GPT-4o pursue was to get early
    control over North America. Because of the strong continent bonus and the fact
    that it only requires to be guarded in 3 places means that is a strategically
    good starting point. I suspect the reason that GPT-4o does is because it has read
    as part of its training data that it is a strategically good location.
  prefs: []
  type: TYPE_NORMAL
- en: Top Models Finish More Games
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overall, there is a trend amongst the top models to finish more of the games,
    and achieve the victory conditions than weaker models. Of the games played with
    the top models, only 2 games went to the max game limit, while this happened 6
    times for the weaker models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Limitations of Pre-Trained Knowledge**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A limitation of classic Risk is of course that the LLMs have read about strategies
    for playing Risk online, and then the top models are simply the best ones at executing
    on this. I think the tendency to quickly try to dominate North America highlights
    this. This limitation could be mitigated if we played with randomly generated
    maps instead. This would increase the difficulty level and would provide a higher
    bar for the models. However, given their performance on the current maps I don’t
    think we need to increase the difficulty for the current model generations.
  prefs: []
  type: TYPE_NORMAL
- en: General observations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even the strongest LLMs are still far from mastering strategic gameplay. None
    of the models exhibited behavior that could challenge even an average human player.
    I think we have to wait at least one or two model generations before we can start
    to see a substantial increase in strategic behavior.
  prefs: []
  type: TYPE_NORMAL
- en: That said, dynamically adjusting prompts to handle specific scenarios — such
    as eliminating weak opponents for card bonuses — could improve performance. With
    different and more enhanced prompting the models might be able to put up more
    of a fight. To get that to work though, you would need to manually program in
    a range of possible scenarios that typically occur and offer specialized prompts
    for each scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a concrete example where we could see this come into play: Player
    B is weak and only has 4 territories with 10 troops, however player B has 3 Risk
    cards, and you are playing progressive cards and the reward for trading in cards
    is currently 20 troops.'
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of this experiment, I didn’t want to make the prompts too specialized,
    because the goal wasn’t to optimize agent behavior in Risk, but rather to test
    their ability to do that themselves, given the game state.
  prefs: []
  type: TYPE_NORMAL
- en: What These Results Mean for the Future of AI and Strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/c4820641aa83f44cfde7286079673697.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author using DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: The results from these experiments highlight a few key considerations for the
    future of AI and its strategic applications. While LLMs have shown remarkable
    improvements in language comprehension and problem-solving, their ability to reason
    and act strategically is still in its early stages.
  prefs: []
  type: TYPE_NORMAL
- en: '**Strategic Awareness and AI Evolution**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As seen in the simulations, the current generation of LLMs struggles with basic
    strategic concepts like fortification and recognizing winning moves. This indicates
    that even though AI models have improved in many areas, the sophistication required
    for high-level strategic thinking remains underdeveloped.
  prefs: []
  type: TYPE_NORMAL
- en: However, as we clearly saw in Experiment-2, there is a trend towards improved
    strategic thinking, and if this trend keeps going for future generations we probably
    don’t have to wait too long until the models are much more capable. There are
    people claiming the LLMs have already plateaued, however I would be very careful
    assuming that.
  prefs: []
  type: TYPE_NORMAL
- en: '**Implications for Real-World Applications**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The real-world applications of a strategically aware and capable AI agent are
    obviously enormous and cannot be understated. They could be used in anything from
    business strategy to military planning and complex human interaction. A strategic
    AI that can anticipate and react to the actions of others could be incredibly
    valuable — and of course also very dangerous. Below we present three possible
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: If we consider a more positive application first, we could imagine everyone
    having a helpful strategic agent guiding them through their daily lives, helping
    make important decisions. This agent could help with anything from financial planning,
    planning daily tasks, to optimizing social interactions and behavior that involves
    the actions of other humans. It could act on your behalf and be goal oriented
    to optimize your interests and well-being.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are obviously many insidious application areas as well. Think: Autonomous
    fighter drones with on-board strategic capabilities. This might not be too far-fetched
    especially when we consider the relative strength of the smaller models compared
    to their big-brother counterparts, for example GPT-4o vs GPT-4o-mini. Smaller
    models are much easier to deploy on edge devices like drones, and when we see
    how [popular](https://www.atlanticcouncil.org/blogs/ukrainealert/fpv-drones-in-ukraine-are-changing-modern-warfare/)
    drones have become the Russian-Ukraine war, taking the step from first person
    view (FPV) drone to unmanned AI-driven drone might be considered feasible. Perhaps
    even as a back-up option if the drone operator lost contact with the drone.'
  prefs: []
  type: TYPE_NORMAL
- en: Detailed simulation of social interaction is a third way to use strategically
    aware agents. We could for example create simulations to model specific economic
    or other social phenomena, blending classic agent-based methods with LLMs. Agent
    based modelling (ABM) as a field of research and toolkit for understanding complex
    adaptive systems has existed for decades — I used in my Masters' thesis back in
    2012 — but coupled with much smarter and strategic agents this could potentially
    be game changing.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Importance of Dynamic Prompting**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Detailed dynamic prompting is probably one of the best ways to use and interact
    with LLMs in the near future — and perhaps also for the next few model generations
    (GPT-5, Claude 4, etc.). By providing more dynamic scenario-specific prompts and
    letting LLM agents execute specific plans, we might see more sophisticated strategic
    behavior in the next generation of models.
  prefs: []
  type: TYPE_NORMAL
- en: This type of “handholding” requires a lot more work from human programmers —
    than just prompting agents directly — however it could be a crucial stepping stone
    until the models become more capable of independent strategic thinking.
  prefs: []
  type: TYPE_NORMAL
- en: One could of course argue that if we provide too detailed and specific prompts
    we are working against the generalized nature of these models, and at that point
    we might as well introduce a different type of optimization algorithm, however
    I think there are many problems where the more open-ended problem-solving abilities
    of the LLMs could be paired with some form of dynamic prompting.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Need for New Benchmarks**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the LLMs continue to improve, it will also be necessary to develop new benchmarks
    to study them. The traditional benchmarks and tests are well suited to study problem-solving
    in isolated environments, but moving forward we might want to introduce more strategic
    tests, that allow us to understand how the agents behave in situations where they
    need to consider how their actions influence others over time. Games like Risk
    provide a reasonable starting point because of their strategic nature and elements
    of uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: '**Future Considerations**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Looking ahead, as AI models continue to evolve, it will be important to monitor
    their strategic capabilities closely. We need to ensure that as these models become
    more powerful, they are aligned with human values and ethical considerations.
    The risks associated with strategic AI — such as unintended consequences in high-stakes
    environments — must be carefully managed.
  prefs: []
  type: TYPE_NORMAL
- en: As smaller models like GPT-4o-mini have shown competitive performance in strategic
    tasks, there is potential for deploying highly capable AI on edge devices, such
    as drones or autonomous systems. This opens up new possibilities for decentralized
    AI applications that require real-time decision-making in dynamic environments.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I think it’s safe to say that that while the strategic capabilities of LLMs
    are improving with each new generation, they still have a long way to go before
    they can rival even a moderately skilled human player. Models like Claude and
    GPT-4o are beginning to show some level of strategic thinking, but their shortcomings
    in areas such as fortification and recognizing winning moves highlight the current
    limitations of AI in complex, multi-agent environments. Nevertheless, the trend
    toward better performance across newer models shows promise for future advancements
    in AI strategy.
  prefs: []
  type: TYPE_NORMAL
- en: As we continue to integrate AI into more aspects of life, from business to military
    strategy, understanding and refining the strategic capabilities of these systems
    will become increasingly important. While we’re not there yet, the potential for
    AI to handle complex decision-making processes in dynamic environments is incredible.
    It will be super interesting to see how the capabilities of the LLMs evolve over
    time, and if our results that show the improvements of LLMs across model generations
    continue through to GPT-5, GPT-6, Claude 4, Claude 5 etc. I think we are in for
    a wild ride!
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in developing your own AI driven tools, feel free to reach
    out! I am always happy to explore collaborative opportunities!
  prefs: []
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here I aim to provide some extra details that while interesting for the more
    technically inclined reader, might not be necessary for the full flow of the article.
    The first topic we touch on is rate limit issues. Then we describe more detailed
    analysis of errors, accumulated turn time used by the agents and parsing of responses
    from the LLMs. In addition, I provide the reader with a short description of how
    to test the code base out by cloning the Github repo and getting started with
    the docker setup.
  prefs: []
  type: TYPE_NORMAL
- en: Rate Limit Issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many actions that needs to be considered every turn, and this leads
    to quite a lot of back-and-forth interaction between the program and the LLM providers.
    One issues that turned out to be slightly problematic for running longer experiments
    was rate limiting.
  prefs: []
  type: TYPE_NORMAL
- en: Rate limits are something the LLM providers set in place to protect against
    spamming an other potentially disrupting behavior, so even though you have funds
    in the accounts, the providers still limit the amount of tokens you can query.
    For example, Anthropic does a rate limit of 1M token / per day for their best
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75e0be47ff4140dbf85276862452a009.png)'
  prefs: []
  type: TYPE_IMG
- en: Rate limits for Anthropic models, taken from Anthropic console
  prefs: []
  type: TYPE_NORMAL
- en: And when you hit your rate limit, your LLM queries are answered with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For many application areas this might not be a problem, however the simulation
    queries each of the providers multiple times per turn (for strategy evaluation,
    card choice, troop placement, multiple attacks and fortification) so this quickly
    adds up, especially in long games that go over many rounds. I was initially planning
    on doing 10 experiments on with victory condition set to World Domination (this
    means the winner would need to control all 42 territories in the game to win),
    but because of how the LLMs play the game this wasn’t feasible in my time frame.
    The victory conditions had to be adjusted so a winner could be determined at an
    earlier stage.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking Errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some of the LLMs in the experiments were also struggling with a large number
    of errors when prompted for moves, this could be anything from trying to place
    troops in territories they didn’t control to fortifying to territories that were
    not connected. I implemented a few variables to track these errors. This was way
    more common with the weaker models as the plots below suggests:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5681a68a04ca5da46fe2bc155d153f0.png)![](../Images/b2a5e49a82cee4a6f18f039b4e426397.png)'
  prefs: []
  type: TYPE_IMG
- en: Experiment-1 attack and fortify errors / image by author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a73141944e7d89c7ab68119d32f9690.png)![](../Images/d2839d2ecda849e5ae8c087df31b4f31.png)'
  prefs: []
  type: TYPE_IMG
- en: Experiment-2 attack and fortify errors / image by author
  prefs: []
  type: TYPE_NORMAL
- en: Accumulated Turn Time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last thing I tracked during the experiments was how much time each of the
    LLMs used on their actions. As expected, the largest and more complex models used
    the most time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0ecf143559382f80f1e3446b9b56e4c.png)![](../Images/050feda024ff8bef9cfc13075d75518e.png)'
  prefs: []
  type: TYPE_IMG
- en: Accumulated turn time by player / image by author
  prefs: []
  type: TYPE_NORMAL
- en: What is clear is that Claude seems to be really taking it’s time. For experiment-1
    GPT-4 is coming out better than llama3.1 70b running on Groq but this is likely
    due to the fact that there were more issues with internal server response errors
    etc., in addition to errors in the returned answers, which lead to the turn time
    going up. For pure inference, when it provides the correct response, Groq was
    marginally faster than OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Trending Towards Less Mistakes and More Robust Output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we could see from the model improved model generations, the new LLMs are
    generating far less erroneous output than the older models. This is important
    as we continue to build data products with the models and integrate them into
    pipelines. There will likely still be the need for some post-prompt error handling
    but less than before.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing Responses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A key issue in interacting with the LLMs is to parse the output that they produce.
    OpenAI recently [revealed](https://openai.com/index/introducing-structured-outputs-in-the-api/)
    that GPT-4o, can *“now reliably adhere to developer-supplied JSON Schemas.”* So,
    this is of course amazing news, but many of the other models, such as llama 3.1
    70B still struggled to consistently return JSON output in the right format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to the parsing problem ended up packing the output into special
    text strings such as `||| output 1 |||`, `+++ output 2+++` and then using regex
    to parse those output strings. I simply prompt the LLM to format the output using
    the special text strings, and also provide examples of correctly formatted output.
    I guess because of how the LLMs are inherently sequence based this type of formatting
    is easier out of the box than for example asking it to return a complex JSON object.
    For a concrete example, see below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Trying Out the Code and Running Your Own Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I developed the package for the risk_game engine in addition to the modules
    and Jupyter notebook inside a docker container, and everything is self contained.
    So for anyone interested in trying out the simulator and run your own experiments
    all the code is available and should be very easy to run from the Github repo.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/hcekne/risk-game.git?source=post_page-----43c868d83c3b--------------------------------)
    [## GitHub — hcekne/risk-game'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to hcekne/risk-game development by creating an account on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/hcekne/risk-game.git?source=post_page-----43c868d83c3b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Clone the repo and follow the instructions in the `README.md` file. It should
    be pretty straightforward. The only thing you need to change to get everything
    running on your own machine is the `.env_example` file. You need to put in your
    own API keys for the relevant LLM providers and change the name of the file to
    `.env`.
  prefs: []
  type: TYPE_NORMAL
- en: Then run the `start_container.sh` script. This is just a bash script that initializes
    some environment variables and runs a docker compose .yml file. This file configures
    the appropriate settings for the docker container, and everything should start
    up by itself. (The reason we feed these environment variables into the docker
    container is because when doing in-container development you can run into an issue
    with file permissions on the files that are created in the container. This is
    fixed if we change the container user to your user, then the files created by
    the container will have the same owner as the user on the machine running the
    container.)
  prefs: []
  type: TYPE_NORMAL
- en: '*If you enjoyed reading this article and would like to access more content
    from me please feel free to connect with me on LinkedIn at* [*https://www.linkedin.com/in/hans-christian-ekne-1760a259/*](https://www.linkedin.com/in/hans-christian-ekne-1760a259/)
    *or visit my webpage at* [*https://www.ekneconsulting.com/*](https://www.ekneconsulting.com/)
    *to explore some of the services I offer. Don’t hesitate to reach out via email
    at hce@ekneconsulting.com*'
  prefs: []
  type: TYPE_NORMAL
