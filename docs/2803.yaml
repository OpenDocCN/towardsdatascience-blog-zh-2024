- en: Multimodal Models — LLMs That Can See and Hear
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multimodal-models-llms-that-can-see-and-hear-5c6737c981d3?source=collection_archive---------7-----------------------#2024-11-19](https://towardsdatascience.com/multimodal-models-llms-that-can-see-and-hear-5c6737c981d3?source=collection_archive---------7-----------------------#2024-11-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An introduction with example Python code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/?source=post_page---byline--5c6737c981d3--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page---byline--5c6737c981d3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5c6737c981d3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5c6737c981d3--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page---byline--5c6737c981d3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5c6737c981d3--------------------------------)
    ·9 min read·Nov 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: This is the first post in a larger series on [Multimodal AI](https://shawhin.medium.com/list/multimodal-ai-fe9521d0e77a).
    A **Multimodal Model (MM)** is an **AI system capable of processing or generating
    multiple data modalities** (e.g., text, image, audio, video). In this article,
    I will discuss a particular type of MM that builds on top of a large language
    model (LLM). I’ll start with a high-level overview of such models and then share
    example code for using LLaMA 3.2 Vision to perform various image-to-text tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27ab3af36cfa4d6886cc5244b94147db.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sincerely Media](https://unsplash.com/@sincerelymedia?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[Large language models (LLMs)](/a-practical-introduction-to-llms-65194dda1148)
    have marked a fundamental shift in AI research and development. However, despite
    their broader impacts, they are still **fundamentally limited**.'
  prefs: []
  type: TYPE_NORMAL
- en: Namely, LLMs can only process and generate text, making them *blind* to other
    modalities such as images, video, audio, and more. This is a major limitation
    since **some tasks rely on non-text data,** e.g., analyzing engineering blueprints,
    reading body language or speech tonality, and interpreting plots and infographics.
  prefs: []
  type: TYPE_NORMAL
