["```py\nI am sorry, I cannot fulfill this request. \nI do not have access to real-time information, including financial data \nlike exchange rates.\n```", "```py\nvertexai==1.65.0\nlangchain==0.2.16\nlangchain-community==0.2.16\nlangchain-core==0.2.38\nlangchain-google-community==1.0.8\nlangchain-google-vertexai==1.0.6\n```", "```py\nfrom vertexai.generative_models import (\n    GenerativeModel,\n    GenerationConfig,\n    Part\n)\n\ngemini_model = GenerativeModel(\n    \"gemini-1.5-flash\",\n    generation_config=GenerationConfig(temperature=0),\n)\nchat = gemini_model.start_chat()\n```", "```py\nresponse = chat.send_message(\"What is the current exchange rate for USD vs EUR ?\")\nanswer = response.candidates[0].content.parts[0].text\n\n--- OUTPUT ---\n\"I am sorry, I cannot fulfill this request. I do not have access to real-time information, including financial data like exchange rates.\" \n```", "```py\ndef get_exchange_rate_from_api(params):\n    url = f\"https://api.frankfurter.app/latest?from={params['currency_from']}&to={params['currency_to']}\"\n    print(url)\n    api_response = requests.get(url)\n    return api_response.text\n\n# Try it out !\nget_exchange_rate_from_api({'currency_from': 'USD', 'currency_to': 'EUR'})\n---\n'{\"amount\":1.0,\"base\":\"USD\",\"date\":\"2024-11-20\",\"rates\":{\"EUR\":0.94679}}'\n```", "```py\nimport requests\n\nfrom vertexai.generative_models import FunctionDeclaration\n\nget_exchange_rate_func = FunctionDeclaration(\n    name=\"get_exchange_rate\",\n    description=\"Get the exchange rate for currencies between countries\",\n    parameters={\n    \"type\": \"object\",\n    \"properties\": {\n        \"currency_from\": {\n            \"type\": \"string\",\n            \"description\": \"The currency to convert from in ISO 4217 format\"\n        },\n        \"currency_to\": {\n            \"type\": \"string\",\n            \"description\": \"The currency to convert to in ISO 4217 format\"\n        }\n    },\n        \"required\": [\n            \"currency_from\",\n            \"currency_to\",\n      ]\n  },\n)\n```", "```py\n# Edit our function\ndef get_exchange_rate_from_api(currency_from: str, currency_to: str):\n    \"\"\"\n    Get the exchange rate for currencies   \n\n    Args:\n        currency_from (str): The currency to convert from in ISO 4217 format\n        currency_to (str): The currency to convert to in ISO 4217 format\n    \"\"\"\n    url = f\"https://api.frankfurter.app/latest?from={currency_from}&to={currency_to}\"\n    api_response = requests.get(url)\n    return api_response.text\n\n# Create the tool\nget_exchange_rate_func = FunctionDeclaration.from_func(\n  get_exchange_rate_from_api\n)\n```", "```py\nfrom vertexai.generative_models import Tool as VertexTool\n\ntool = VertexTool(\n    function_declarations=[\n        get_exchange_rate_func,\n        # add more functions here !\n    ]\n)\n```", "```py\nfrom vertexai.generative_models import GenerativeModel\n\ngemini_model = GenerativeModel(\n    \"gemini-1.5-flash\",\n    generation_config=GenerationConfig(temperature=0),\n    tools=[tool] #We add the tool here !\n)\nchat = gemini_model.start_chat()\n\nresponse = chat.send_message(prompt)\n\n# Extract the function call response\nresponse.candidates[0].content.parts[0].function_call\n\n--- OUTPUT ---\n\"\"\"\nname: \"get_exchange_rate\"\nargs {\n  fields {\n    key: \"currency_to\"\n    value {\n      string_value: \"EUR\"\n    }\n  }\n  fields {\n    key: \"currency_from\"\n    value {\n      string_value: \"USD\"\n    }\n  }\n  fields {\n    key: \"currency_date\"\n    value {\n      string_value: \"latest\"\n    }\n  }\n}\"\"\" \n```", "```py\n# mapping dictionnary to map function names and function\nfunction_handler = {\n    \"get_exchange_rate\": get_exchange_rate_from_api,\n}\n\n# Extract the function call name\nfunction_name = function_call.name\nprint(\"#### Predicted function name\")\nprint(function_name, \"\\n\")\n\n# Extract the function call parameters\nparams = {key: value for key, value in function_call.args.items()}\nprint(\"#### Predicted function parameters\")\nprint(params, \"\\n\")\n\nfunction_api_response = function_handler[function_name](params)\nprint(\"#### API response\")\nprint(function_api_response)\nresponse = chat.send_message(\n    Part.from_function_response(\n        name=function_name,\n        response={\"content\": function_api_response},\n    ),\n)   \nprint(\"\\n#### Final Answer\")\nprint(response.candidates[0].content.parts[0].text)\n\n--- OUTPUT ---\n\"\"\"\n#### Predicted function name\nget_exchange_rate \n\n#### Predicted function parameters\n{'currency_from': 'USD', 'currency_date': 'latest', 'currency_to': 'EUR'} \n\n#### API response\n{\"amount\":1.0,\"base\":\"USD\",\"date\":\"2024-11-20\",\"rates\":{\"EUR\":0.94679}}\n\n#### Final Answer\nThe current exchange rate for USD vs EUR is 0.94679\\. This means that 1 USD is equal to 0.94679 EUR. \n\"\"\"\n```", "```py\nfrom langchain_core.tools import tool\n\n@tool\ndef get_exchange_rate_from_api(currency_from: str, currency_to: str) -> str:\n    \"\"\"\n    Return the exchange rate between currencies\n    Args:\n        currency_from: str\n        currency_to: str\n    \"\"\"\n    url = f\"https://api.frankfurter.app/latest?from={currency_from}&to={currency_to}\"\n    api_response = requests.get(url)\n    return api_response.text\n```", "```py\n@tool\ndef list_tables(project: str, dataset_id: str) -> list:\n    \"\"\"\n    Return a list of Bigquery tables\n    Args:\n        project: GCP project id\n        dataset_id: ID of the dataset\n    \"\"\"\n    client = bigquery.Client(project=project)\n    try:\n        response = client.list_tables(dataset_id)\n        return [table.table_id for table in response]\n    except Exception as e:\n        return f\"The dataset {params['dataset_id']} is not found in the {params['project']} project, please specify the dataset and project\"\n```", "```py\nlangchain_tool = [\n    list_tables,\n    get_exchange_rate_from_api\n]\n```", "```py\ngemini_llm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```", "```py\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful assistant\"),\n        (\"human\", \"{input}\"),\n        # Placeholders fill up a **list** of messages\n        (\"placeholder\", \"{agent_scratchpad}\"),\n    ]\n)\n```", "```py\nagent = create_tool_calling_agent(gemini_llm, langchain_tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=langchain_tools)\nagent_executor.invoke({\n    \"input\": \"Which tables are available in the thelook_ecommerce dataset ?\"\n})\n\n--- OUTPUT ---\n\"\"\"\n{'input': 'Which tables are available in the thelook_ecommerce dataset ?',\n 'output': 'The dataset `thelook_ecommerce` is not found in the `gcp-project-id` project. \n            Please specify the correct dataset and project. \\n'}\n\"\"\" \n```", "```py\nagent_executor.invoke({\"input\": f\"Project id is bigquery-public-data\"})\n\n--- OUPTUT ---\n\"\"\"\n{'input': 'Project id is bigquery-public-data',\n 'output': 'OK. What else can I do for you? \\n'}\n\"\"\" \n```", "```py\nfrom langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\n\n# Different types of memory can be found in Langchain\nmemory = InMemoryChatMessageHistory(session_id=\"foo\")\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful assistant.\"),\n        # First put the history\n        (\"placeholder\", \"{chat_history}\"),\n        # Then the new input\n        (\"human\", \"{input}\"),\n        # Finally the scratchpad\n        (\"placeholder\", \"{agent_scratchpad}\"),\n    ]\n)\n\n# Remains unchanged\nagent = create_tool_calling_agent(gemini_llm, langchain_tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=langchain_tools)\n\n# We add the memory part and the chat history\nagent_with_chat_history = RunnableWithMessageHistory(\n    agent_executor,\n    lambda session_id: memory, #<-- NEW\n    input_messages_key=\"input\", \n    history_messages_key=\"chat_history\", #<-- NEW\n)\n\nconfig = {\"configurable\": {\"session_id\": \"foo\"}}\n```", "```py\nagent_with_chat_history.invoke({\n    \"input\": \"Which tables are available in the thelook_ecommerce dataset ?\"\n    }, \n    config\n)\n\n--- OUTPUT ---\n\"\"\"\n{'input': 'Which tables are available in the thelook_ecommerce dataset ?',\n 'chat_history': [],\n 'output': 'The dataset `thelook_ecommerce` is not found in the `gcp-project-id` project. Please specify the correct dataset and project. \\n'}\n\"\"\"\n```", "```py\nreply = \"Project id is bigquery-public-data\"\nagent_with_chat_history.invoke({\"input\": reply}, config)\n\n--- OUTPUT ---\n\"\"\"\n{'input': 'Project id is bigquery-public-data',\n 'chat_history': [HumanMessage(content='Which tables are available in the thelook_ecommerce dataset ?'),\n  AIMessage(content='The dataset `thelook_ecommerce` is not found in the `gcp-project-id` project. Please specify the correct dataset and project. \\n')],\n 'output': 'The following tables are available in the `thelook_ecommerce` dataset:\\n- distribution_centers\\n- events\\n- inventory_items\\n- order_items\\n- orders\\n- products\\n- users \\n'}\n\"\"\"\n```", "```py\n'output': 'The following tables are available in the `thelook_ecommerce` dataset:\\n- distribution_centers\\n- events\\n- inventory_items\\n- order_items\\n- orders\\n- products\\n- users \\n'}\n```", "```py\n# define the prompt with memory\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful assistant.\"),\n        # First put the history\n        (\"placeholder\", \"{chat_history}\"),\n        # Then the new input\n        (\"human\", \"{input}\"),\n        # Finally the scratchpad\n        (\"placeholder\", \"{agent_scratchpad}\"),\n    ]\n)\n\n# bind the tools to the LLM\ngemini_with_tools = gemini_llm.bind_tools(langchain_tool)\n\n# build the chain\nchain = prompt | gemini_with_tools\n```", "```py\n# With AgentExecutor\n\n# agent = create_tool_calling_agent(gemini_llm, langchain_tool, prompt)\n# agent_executor = AgentExecutor(agent=agent, tools=langchain_tool)\n\n# agent_with_chat_history = RunnableWithMessageHistory(\n#     agent_executor,\n#     lambda session_id: memory,\n#     input_messages_key=\"input\",\n#     history_messages_key=\"chat_history\",\n# )\n\nconfig = {\"configurable\": {\"session_id\": \"foo\"}}\n\n# With Chains\nmemory = InMemoryChatMessageHistory(session_id=\"foo\")\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: memory,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n)\n\nresponse = chain_with_history.invoke(\n    {\"input\": \"What is the current CHF EUR exchange rate ?\"}, config)\n\n--- OUTPUT\n\"\"\"\ncontent='', \nadditional_kwargs={\n    'function_call': {\n        'name': 'get_exchange_rate_from_api', \n        'arguments': '{\"currency_from\": \"CHF\", \"currency_to\": \"EUR\"}'\n    }\n}\n\"\"\" \n```", "```py\nfrom langchain_core.messages import AIMessage\n\ndef call_tools(msg: AIMessage) -> list[dict]:\n    \"\"\"Simple sequential tool calling helper.\"\"\"\n    tool_map = {tool.name: tool for tool in langchain_tool}\n    tool_calls = msg.tool_calls.copy()\n    for tool_call in tool_calls:\n        tool_call[\"output\"] = tool_map[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n    return tool_calls\n\nchain = prompt | gemini_with_tools | call_tools #<-- Extra step\n\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: memory,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n)\n\n# Rerun the chain \nchain_with_history.invoke({\"input\": \"What is the current CHF EUR exchange rate ?\"}, config)\n```", "```py\n[{'name': 'get_exchange_rate_from_api',\n  'args': {'currency_from': 'CHF', 'currency_to': 'EUR'},\n  'id': '81bc85ea-dfd4-4c01-85e8-f3ca592fff5b',\n  'type': 'tool_call',\n  'output': '{\"amount\":1.0,\"base\":\"USD\",\"date\":\"2024-11-20\",\"rates\":{\"EUR\":0.94679}}'\n}]\n```", "```py\ndef human_approval(msg: AIMessage) -> AIMessage:\n    \"\"\"Responsible for passing through its input or raising an exception.\n\n    Args:\n        msg: output from the chat model\n\n    Returns:\n        msg: original output from the msg\n    \"\"\"\n    for tool_call in msg.tool_calls:\n        print(f\"I want to use function [{tool_call.get('name')}] with the following parameters :\")\n        for k,v in tool_call.get('args').items():\n            print(\" {} = {}\".format(k, v))\n\n    print(\"\")\n    input_msg = (\n        f\"Do you approve (Y|y)?\\n\\n\"\n        \">>>\"\n    )\n    resp = input(input_msg)\n    if resp.lower() not in (\"yes\", \"y\"):\n        raise NotApproved(f\"Tool invocations not approved:\\n\\n{tool_strs}\")\n    return msg\n```", "```py\nchain = prompt | gemini_with_tools | human_approval | call_tools\n\nmemory = InMemoryChatMessageHistory(session_id=\"foo\")\n\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: memory,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n)\n\nchain_with_history.invoke({\"input\": \"What is the current CHF EUR exchange rate ?\"}, config)\n```", "```py\nagent_with_chat_history.invoke(\n    {\"input\": \"What was the result of Rafael Nadal's latest game ?\"}, config)\n\n--- OUTPUT ---\n\"\"\"\n{'input': \"What was the result of Rafael Nadal's latest game ?\",\n 'chat_history': [],\n 'output': \"I do not have access to real-time information, including sports results. To get the latest information on Rafael Nadal's game, I recommend checking a reliable sports website or news source. \\n\"}\n\"\"\"\n```", "```py\nfrom langchain_community.utilities import GoogleSerperAPIWrapper\n\n# Create our new search tool here\nsearch = GoogleSerperAPIWrapper(serper_api_key=\"...\")\n\n@tool\ndef google_search(query: str):\n    \"\"\"\n    Perform a search on Google\n    Args:\n        query: the information to be retrieved with google search\n    \"\"\"\n    return search.run(query)\n\n# Add it to our existing tools\nlangchain_tool = [\n    list_datasets,\n    list_tables,\n    get_exchange_rate_from_api,\n    google_search\n]\n\n# Create agent\nagent = create_tool_calling_agent(gemini_llm, langchain_tool, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=langchain_tool)\n\n# Add memory\nmemory = InMemoryChatMessageHistory()\nagent_with_chat_history = RunnableWithMessageHistory(\n    agent_executor,\n    lambda session_id: memory,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n)\n```", "```py\nagent_with_chat_history.invoke({\"input\": \"What was the result of Rafael Nadal's latest game ?\"}, config)\n\n--- OUTPUT ---\n\"\"\"\n{'input': \"What was the result of Rafael Nadal's latest game ?\",\n 'chat_history': [],\n 'output': \"Rafael Nadal's last match was a loss to Botic van de Zandschulp in the Davis Cup. Spain was eliminated by the Netherlands. \\n\"}\n\"\"\" \n```"]