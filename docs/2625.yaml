- en: 'GSM-Symbolic: Analyzing LLM Limitations in Mathematical Reasoning and Potential
    Solutions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gsm-symbolic-analyzing-llm-limitations-in-mathematical-reasoning-and-potential-solutions-363b82370a26?source=collection_archive---------8-----------------------#2024-10-28](https://towardsdatascience.com/gsm-symbolic-analyzing-llm-limitations-in-mathematical-reasoning-and-potential-solutions-363b82370a26?source=collection_archive---------8-----------------------#2024-10-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What The Paper on LLM Reasoning Got Right — And What It Missed.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@zredlined?source=post_page---byline--363b82370a26--------------------------------)[![Alexander
    Watson](../Images/aea574d9652ea8b1b91d4ec8a9c88ef8.png)](https://medium.com/@zredlined?source=post_page---byline--363b82370a26--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--363b82370a26--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--363b82370a26--------------------------------)
    [Alexander Watson](https://medium.com/@zredlined?source=post_page---byline--363b82370a26--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--363b82370a26--------------------------------)
    ·9 min read·Oct 28, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Co-authors: Alex Watson, Yev Meyer, Dane Corneil, Maarten Van Segbroeck (Gretel.ai)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a06f7b26698ac70c2f8d5922ee6ca19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Gretel.ai'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) have recently made significant strides in AI reasoning,
    including mathematical problem-solving. However, a recent paper titled “[GSM-Symbolic:
    Understanding the Limitations of Mathematical Reasoning in Large Language Models](https://arxiv.org/pdf/2410.05229)”
    by Mirzadeh et al. raises questions about the true capabilities of these models
    when it comes to mathematical reasoning. We have reviewed the paper and found
    it to be a valuable contribution to the ongoing discussion about AI capabilities
    and limitations, however, our analysis suggests that its conclusions may not fully
    capture the complexity of the issue.'
  prefs: []
  type: TYPE_NORMAL
- en: The GSM-Symbolic Benchmark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The authors introduce GSM-Symbolic, an enhanced benchmark derived from the popular
    GSM8K dataset. This new benchmark allows for the generation of diverse question
    variants, enabling a more nuanced evaluation of LLMs’ performance across various
    setups. The study’s large-scale analysis of 25 state-of-the-art open and closed
    models provides significant insights into how these models behave when faced with
    mathematical reasoning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6e00b2fb8155010c499955f50509297.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning
    in Large Language Models (Source: [Mirzadeh et al., GSM-Symbolic Paper](https://arxiv.org/abs/2410.05229))'
  prefs: []
  type: TYPE_NORMAL
- en: Performance Variability and Model Comparisons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most surprising findings is the high variability in model performance
    across different instantiations of the same question. All models exhibit “significant
    variability in accuracy” when tested on GSM-Symbolic. This variability raises
    concerns about the reliability of currently reported metrics on the [GSM8K](https://huggingface.co/datasets/openai/gsm8k)
    benchmark, which relies on single point-accuracy responses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51313010f4883f6faa35b8d5b355484b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning
    in Large Language Models (Source: [Mirzadeh et al., GSM-Symbolic Paper](https://arxiv.org/abs/2410.05229))'
  prefs: []
  type: TYPE_NORMAL
- en: '**Not all models are created equal.** `Llama-3–8b` and `GPT-4o` are clear outliers
    in that they don’t exhibit as significant of a drop on the new benchmark as other
    models like `gemma-2–9b`, `phi-3`, `phi-3.5` and `mathstral-7b`. This observations
    suggests two important points:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Llama-3–8b` and `GPT-4o` generally demonstrate a more robust understanding
    of mathematical concepts, although they are still not immune to performance variations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The training data for `Llama-3–8b` and `GPT-4o` likely has not been contaminated
    (or at least not to the same extent) with GSM8K data. In this context, data contamination
    refers to the unintentional inclusion of test or benchmark data in a model’s training
    set, leading to artificially inflated model performance during evaluation. If
    contamination had occurred, as the authors hypothesize for some models, we would
    expect to see very high performance on GSM8K but significantly lower performance
    on even slight variations of these problems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These findings highlight a opportunity for improvement through the use of synthetic
    data, where properly designed synthetic datasets can address both of these points
    for anyone training models:'
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate potential data contamination issues, there’s no need to use the
    original GSM8K data in training when high-quality synthetic versions can be generated
    ([blog link](https://gretel.ai/blog/teaching-ai-to-think-a-new-approach-with-synthetic-data-and-reflection)).
    These synthetic datasets retain the mathematical reasoning challenges of GSM8K
    without reusing the exact problems or solutions, thus preserving the integrity
    of the model’s evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even more importantly, it’s possible to generate synthetic data that surpass
    the quality of both the OpenAI GSM8K and Apple GSM-Symbolic datasets. This approach
    can lead to a more robust understanding of mathematical concepts, addressing the
    performance variability observed in current models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sensitivity to Changes and Complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The authors show that LLMs are more sensitive to changes in numerical values
    than to changes in proper names within problems, suggesting that the models’ understanding
    of the underlying mathematical concepts may not be as robust as previously thought.
    As the complexity of questions increases (measured by the number of clauses),
    the performance of all models degrades, and the variance in their performance
    increases. This highlights the importance of using diverse data in training, and
    this is something that synthetics can help with. As the authors demonstrate, there
    is logically no reason why a AI model should perform worse on a given set of problems,
    with just a simple change in numbers or a slight variation in the number of clauses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b953ae222b4dcb1370c9695ed3b319d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning
    in Large Language Models (Source: [Mirzadeh et al., GSM-Symbolic Paper](https://arxiv.org/abs/2410.05229))'
  prefs: []
  type: TYPE_NORMAL
- en: The GSM-NoOp Challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps the most concerning finding is the introduction of GSM-NoOp, a dataset
    designed to challenge the reasoning capabilities of LLMs. By adding seemingly
    relevant but ultimately inconsequential information to problems, the authors observed
    substantial performance drops across all models — up to 65% for some. The authors
    propose that this points to current LLMs relying more on a type of pattern matching
    than true logical reasoning
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1b1f43f0cd7b8ca1a64325f5456e1a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning
    in Large Language Models (Source: [Mirzadeh et al., GSM-Symbolic Paper](https://arxiv.org/abs/2410.05229))'
  prefs: []
  type: TYPE_NORMAL
- en: A Critical Perspective on the Paper’s Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the GSM-Symbolic study provides valuable insights into the performance
    of LLMs on mathematical reasoning tasks, it’s important to critically examine
    the paper’s conclusions. The authors argue that the observed limitations suggest
    LLMs are not capable of true logical reasoning. However, this interpretation may
    be oversimplifying a complex issue.
  prefs: []
  type: TYPE_NORMAL
- en: The paper’s argument for LLMs relying on pattern matching rather than reasoning
    seems less definitive when examined closely. It’s clear that these models are
    not perfect reasoners — if they were, they would achieve 100% accuracy on GSM8K.
    But the leap from imperfect performance to a lack of reasoning capability is not
    necessarily justified.
  prefs: []
  type: TYPE_NORMAL
- en: '**There are at least two potential explanations for why LLMs, like humans,
    sometimes get questions wrong**:'
  prefs: []
  type: TYPE_NORMAL
- en: The model tries to strictly pattern match a problem to something it has seen
    before, and fails if it can’t.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model tries to follow a logical program but has a certain (compounding)
    probability of making an error at each step, as expected based on the fact that
    it literally samples tokens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The paper seems to lean towards explanation (1), but doesn’t make a convincing
    case for why this should be preferred over explanation (2). In fact, (2) is more
    akin to human-like reasoning and potentially more interesting from a research
    perspective.
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s examine each main finding of the paper through this critical lens:**'
  prefs: []
  type: TYPE_NORMAL
- en: '*GSM-Symbolic Performance*'
  prefs: []
  type: TYPE_NORMAL
- en: The GSM-Symbolic approach is a valuable method for dataset expansion, validating
    the potential of synthetic data generation techniques like those used by Gretel.
    However, it’s worth noting that model performance doesn’t completely fall apart
    on these new variants — it just gets somewhat worse. If the models were strictly
    pattern matching, we might expect performance to drop to near zero on these new
    variants. The observed behavior seems more consistent with a model that can generalize
    to some degree but makes more errors on unfamiliar problem structures.
  prefs: []
  type: TYPE_NORMAL
- en: Even human experts are not infallible. On the MATH benchmark, for instance,
    former math olympians typically scored 18/20 or 19/20, making small arithmetic
    errors. This suggests that error-prone reasoning, rather than a lack of reasoning
    capability, might be a more accurate description of both human and LLM performance.
  prefs: []
  type: TYPE_NORMAL
- en: '*Varying Difficulty*'
  prefs: []
  type: TYPE_NORMAL
- en: The paper’s findings on performance degradation with increasing question complexity
    are consistent with the idea of compounding errors in a multi-step reasoning process.
    As the number of steps increases, so does the probability of making an error at
    some point in the chain. This behavior is observed in human problem-solving as
    well and doesn’t necessarily indicate a lack of reasoning ability.
  prefs: []
  type: TYPE_NORMAL
- en: '*GSM-NoOp Challenge*'
  prefs: []
  type: TYPE_NORMAL
- en: The GSM-NoOp results, may not be as directly related to reasoning capability
    as the paper suggests. In real-world scenarios, we typically assume that all information
    provided in a problem statement is relevant. For instance, in the example question
    in Figure 7, a reasonable human might infer (like the LLMs did) that the size
    of the kiwis was only mentioned because they were discarded.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b20402670fc477c1e3f36a39ecb5f8db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: GSM-Symbolic: Example GSM No-Op question. (Source: [Mirzadeh et al.,
    GSM-Symbolic Paper](https://arxiv.org/abs/2410.05229))'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to discern relevant information from irrelevant information, especially
    when the irrelevant information is inserted with the intent to be misleading (i.e.
    *seemingly* relevant), is a separate skill from pure mathematical reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors include a follow-up experiment (NoOp-NoOp) in which the models
    are implicitly “warned” of the misleading intent: they use few-shot examples that
    also contain irrelevant information. The subset of models illustrated with this
    experiment still show a drop in performance. **Several follow-up experiments could
    serve to better understand the phenomenon**:'
  prefs: []
  type: TYPE_NORMAL
- en: Expand the NoOp-NoOp experiment to more models;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure how well models perform when *explicitly* warned that some information
    may be irrelevant in the prompt;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tune models on synthetic training examples that include irrelevant information
    in addition to examples that contain entirely relevant information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Opportunities for Improvement: The Promise of Synthetic Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the paper by Mirzadeh et al. highlights important limitations in current
    LLMs, at Gretel we have developed datasets that address many of the challenges
    identified in the paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Synthetic GSM8K** Dataset: Available on HuggingFace at [gretelai/synthetic-gsm8k-reflection-405b](https://huggingface.co/datasets/gretelai/synthetic-gsm8k-reflection-405b),
    this dataset focuses on generating more complex, multi-step reasoning versions
    of problems than what existed in the original human generated dataset from OpenAI.
    It incorporates advanced prompting techniques, including Reflection and other
    cognitive models, to capture detailed reasoning processes. This approach has shown
    significant improvements, particularly for very hard problems, demonstrating its
    potential to enhance AI’s ability to handle complex, multi-step reasoning tasks.
    As covered in our blog, Gretel’s synthetic data created using these techniques
    achieved a [92.3% win-rate on problem complexity and an 82.7% win-rate for educational
    value over the standard Llama 3.1 405B parameter model outputs](https://gretel.ai/blog/teaching-ai-to-think-a-new-approach-with-synthetic-data-and-reflection),
    using these advanced techniques as judged by `GPT-4o`- demonstrating that LLM
    reasoning can further be unlocked with more sophisticated training data examples
    and prompting techniques than the basic Chain-of-Thought used in the paper.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/a28512feacedaced6552c69262035702.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [https://gretel.ai/blog/teaching-ai-to-think-a-new-approach-with-synthetic-data-and-reflection](https://gretel.ai/blog/teaching-ai-to-think-a-new-approach-with-synthetic-data-and-reflection)'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Synthetic Text-to-SQL** Dataset: Generated by Gretel to help improve
    LLMs ability to interact with SQL-based databases/warehouses & lakes, available
    at [gretelai/synthetic_text_to_sql](https://huggingface.co/datasets/gretelai/synthetic_text_to_sql),
    has proven highly effective in improving model performance on Text-to-SQL tasks.
    When used to fine-tune CodeLlama models, [it led to 36%+ improvements on the BIRD
    benchmark](https://gretel.ai/blog/fine-tuning-codellama-on-gretel-aws-sagemaker-jumpstart),
    a challenging cross-domain Text-to-SQL evaluation platform. Further supporting
    the theory about today’s LLMs being trained on data that is too simple and leading
    to memorization, a single epoch of fine-tuning the [Phi-3 and Llama 3.1 models
    on this dataset yielded a 300%+ improvement](https://youtu.be/jn6FuG4WA1c?feature=shared&t=2420)
    on BIRD benchmark problems labeled as “very hard”.'
  prefs: []
  type: TYPE_NORMAL
- en: These results demonstratethat high-quality synthetic data can be a powerful
    tool in addressing the limitations of current LLMs in complex reasoning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Future Directions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, the GSM-Symbolic paper provides valuable insights into the current
    limitations of LLMs in mathematical reasoning tasks. However, its conclusions
    should be approached critically. The observed behavior of LLMs could be interpreted
    in multiple ways, and it’s possible that the paper’s emphasis on pattern matching
    over reasoning may be oversimplifying a more complex issue.
  prefs: []
  type: TYPE_NORMAL
- en: The limitations identified by the study are real and significant. The variability
    in performance, sensitivity to numerical changes, and struggles with irrelevant
    information all point to areas where current LLMs can be improved.
  prefs: []
  type: TYPE_NORMAL
- en: However, as demonstrated by more advanced models such as GPT-4o and Llama 3.1
    above- by synthesizing diverse, challenging problem sets that push the boundaries
    of what AI models can tackle, we can develop LLMs that exhibit more robust, human-like
    reasoning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I. Mirzadeh, K. Alizadeh, H. Shahrokhi, O. Tuzel, S. Bengio, and M. Farajtabar.
    [GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large
    Language Models.](https://arxiv.org/pdf/2410.05229) 2024.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
