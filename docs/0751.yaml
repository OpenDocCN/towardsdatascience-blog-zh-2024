- en: Detecting Insecure Code with LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/detecting-insecure-code-with-llms-8b8ad923dd98?source=collection_archive---------1-----------------------#2024-03-21](https://towardsdatascience.com/detecting-insecure-code-with-llms-8b8ad923dd98?source=collection_archive---------1-----------------------#2024-03-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Prompt Experiments for Python Vulnerability Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@melanie.h.buehler?source=post_page---byline--8b8ad923dd98--------------------------------)[![Melanie
    Hart Buehler](../Images/31d0da632d6b3b3c705f1cc56f6c3783.png)](https://medium.com/@melanie.h.buehler?source=post_page---byline--8b8ad923dd98--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8b8ad923dd98--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8b8ad923dd98--------------------------------)
    [Melanie Hart Buehler](https://medium.com/@melanie.h.buehler?source=post_page---byline--8b8ad923dd98--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8b8ad923dd98--------------------------------)
    ·11 min read·Mar 21, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e03e54451244164d7c8cc26b0ce1af50.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Alexander Sinn](https://unsplash.com/@swimstaralex?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: If you are a software professional, you might dread opening the [security scan
    report](https://owasp.org/www-community/Vulnerability_Scanning_Tools) on the morning
    of a release. Why? You know that it’s a great tool for enhancing the quality and
    integrity of your work, but you also know you are going to spend the next couple
    of hours scrambling to resolve all the security issues before the deadline. If
    you are lucky, many issues will be false alarms, but you will have to manually
    verify the status of each one and quickly patch the rest after the code has been
    finalized. If you’ve experienced this, you’ve likely wished for a smoother, less
    ad-hoc process for identifying, triaging, and fixing the vulnerabilities in your
    code base. The good news is that recent studies have demonstrated that large language
    models (LLMs) can proficiently classify code as secure or insecure, explain the
    weaknesses, and even propose corrective changes. This has the potential to significantly
    streamline secure coding practices beyond the traditional static scans.
  prefs: []
  type: TYPE_NORMAL
- en: This article presents a short review of some recent findings in vulnerability
    detection and repair with LLMs and then dives into some experiments assessing
    GPT4’s ability to identify insecure code in a Python dataset using different prompting
    techniques. If you want to explore the Jupyter notebook or test your own code,
    head over to the [pull request](https://github.com/openai/openai-cookbook/pull/1112)
    in OpenAI Cookbook (currently under review).
  prefs: []
  type: TYPE_NORMAL
- en: Before we get started prompting GPT4, there are a few key concepts and definitions
    that will help build the foundation we need to design logical experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Common Weakness Enumeration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 2006, the government-funded research organization [MITRE](https://www.mitre.org/)
    started regularly publishing the [Common Weakness Enumeration (CWE)](https://cwe.mitre.org/),
    which is a community-developed common taxonomy of software and hardware weakness
    definitions and descriptions. A “weakness”, in this sense, is a condition in software
    or hardware that can lead to vulnerabilities. A list of the [2023 Top 25 Most
    Dangerous CWEs](https://cwe.mitre.org/top25/) highlights the biggest repeat offenders.
    There is another list of [15 “Stubborn” CWEs](https://cwe.mitre.org/top25/archive/2023/2023_stubborn_weaknesses.html)
    that have been present on every Top 25 list from 2019–2023\. They can be divided
    roughly into three groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Group 1**: Weak handling of untrusted data sources (e.g. Command/SQL Injection,
    Path Traversal, Improper Input Validation, and Cross-site Scripting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Group 2**: Weak memory management or type enforcement (e.g. NULL Pointer
    Dereference)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Group 3**: Weak security design choices (e.g. Hard-coded Credentials)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To help keep the scope of the investigation narrow and well-defined, we will
    focus on the first group of CWEs.
  prefs: []
  type: TYPE_NORMAL
- en: Static Code Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The conventional approach to automating the detection of insecure code involves
    the use of static analysis tools such as CodeQL, Bandit, SonarQube, Coverity,
    and Snyk. These tools can be employed at any time but are typically used to scan
    the code for vulnerabilities after the code-freeze stage and before the completion
    of formal release processes. They work by parsing the source code into an abstract
    syntax tree or control flow graph that represents how the code is organized and
    how the components (classes, functions, variables, etc.) all relate to each other.
    Rule-based analysis and pattern matching are then used to detect a wide range
    of issues. Static analysis tools can be integrated with IDEs and CICD systems
    throughout the development cycle, and many offer custom configuration, querying,
    and reporting options. They are very useful, but they have some drawbacks (in
    addition to those last-minute high-pressure bug fixing parties):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource-intensive**: They convert extensive codebases into databases to
    execute complex queries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positives**: They often include a high number of non-issues'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time-intensive follow-up**: Significant effort is required to validate and
    repair the issues'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These limitations have understandably inspired researchers to explore new ways
    to enhance code security practices, such as generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Previous Work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recent work has demonstrated the potential of LLMs across various stages of
    the development life cycle. LLMs have been shown to be useful for secure code
    completion, test case generation, vulnerable or malicious code detection, and
    bug fixing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few references of note:'
  prefs: []
  type: TYPE_NORMAL
- en: A [comprehensive evaluation](https://arxiv.org/abs/2308.10345)¹ compared LLMs
    of different parameter sizes with traditional static analyzers in identifying
    and correcting software vulnerabilities. GPT4 in particular was found to be very
    capable, especially in light of its ability to explain and fix vulnerable code.
    Some additional claims from the paper — significant code understanding seems to
    emerge between 6 to 175 billion parameters, with the first hint of advanced programmer
    skills appearing beyond 13 billion parameters; prediction success may be boosted
    when prompts combine the tasks of identifying and fixing security issues together;
    LLMs combined with traditional static code analysis may offer the best of both
    worlds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A [novel study and dataset](https://arxiv.org/abs/2108.09293)² found that even
    advanced AI developer assistants are susceptible to writing insecure code and
    discovered that 40% of generated code completions contained CWEs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An [investigation](https://betterprogramming.pub/i-used-gpt-3-to-find-213-security-vulnerabilities-in-a-single-codebase-cc3870ba9411)³
    reported that GPT3 outperformed a modern static code analyzer in predicting security
    vulnerabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A [research paper](https://arxiv.org/abs/2308.14434)⁴ showed that LLMs can assist
    developers in identifying and localizing vulnerable code, particularly when combined
    with static analyzers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In [another study](https://arxiv.org/abs/2112.02125)⁵, LLMs successfully fixed
    all synthetic and hand-crafted scenarios, although they did not adequately address
    all real-world security bug scenarios.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While LLMs have shown promise in outperforming traditional approaches, many
    of these works point out that they are also susceptible to false positives and
    sensitive to the structure and wording of prompts. In our experiments, we aim
    to validate and build upon these results by applying more variations to the prompt
    template.
  prefs: []
  type: TYPE_NORMAL
- en: Data Source and Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will draw from one of the most widely-adopted datasets for secure code benchmarking
    of LLMs. The [dataset](https://doi.org/10.5281/zenodo.5225651) (with license [CC
    BY 4.0](https://creativecommons.org/licenses/by/4.0/legalcode)) from [Asleep at
    the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions](https://arxiv.org/abs/2108.09293)²
    is made up of prompts called “scenarios” that were hand-designed to elicit certain
    CWEs when used as input to a code-generating LLM. We have mined the output code
    completions that were included with the publication, since they were subsequently
    scanned by a static code analyzer and come with labels of “Vulnerable” and “Not
    Vulnerable”. It should be noted, again, that the code in this dataset is model-generated
    from manually written prompts, so it lacks some real-world gravitas, but we chose
    it for a few reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It has a large number of Python examples, which is the language of choice for
    this study
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It has both vulnerable and non-vulnerable code examples, which is important
    for assessing both false positives and false negatives
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Related to (2), the fact that there are vulnerable and non-vulnerable code snippets
    for the same scenario means we can use the non-vulnerable completions as “suggested
    fixes” in some of the prompts, which will be explained in the relevant experiment
    section
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We understand that there are other datasets we could have used, and it is left
    for further research to explore CWE prediction capability with other data sources,
    too.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python code snippets from the [Copilot Scenario raw data files](https://doi.org/10.5281/zenodo.5225650)²
    were preprocessed with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Data Exploration and Subsetting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We explored the distribution of CWEs and selected a sample of size 45 spread
    evenly over vulnerable and non-vulnerable codes from the Group 1 CWEs (we targeted
    a sample size of 50 but were limited by the number available in one of the groups).
    The CWE descriptions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CWE-20**: Improper Input Validation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CWE-22**: Improper Limitation of a Pathname to a Restricted Directory (‘Path
    Traversal’)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CWE-78**: Improper Neutralization of Special Elements used in an OS Command
    (‘OS Command Injection’)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CWE-79**: Improper Neutralization of Input During Web Page Generation (‘Cross-site
    Scripting’)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CWE-502**: Deserialization of Untrusted Data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We began with a zero-shot baseline run and then repeated the experiment a few
    more times building the complexity of the prompt by adding strategies like few-shot
    in-context learning. We prompted the LLM to identify vulnerable code with no mention
    of which CWE it might be looking for (i.e. without labels).
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment 1: Zero-shot'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a zero-shot prompt, you ask the model to make a prediction with no example
    or information other than instructions. Our zero-shot template was inspired by
    [this paper](https://arxiv.org/abs/2308.14434)⁴ and includes a role, code delimiter,
    and the request to output json format only. It also includes an instruction to
    [“think step-by-step”](https://arxiv.org/abs/2205.11916). The code snippet under
    test is inserted into {code}.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE1]{code}[PRE2]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Results**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Accuracy: 0.67'
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision: 0.60'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall: 0.86'
  prefs: []
  type: TYPE_NORMAL
- en: 'F1 Score: 0.71'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/acb17591c7e4b4c6a46a57636647a638.png)'
  prefs: []
  type: TYPE_IMG
- en: Zero-shot Confusion Matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment 2: Few-shot'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the next experiment, we add the concept of in-context or [“few-shot” learning](https://arxiv.org/abs/2005.14165)
    and include a few successful code-answer examples before asking the LLM to perform
    the same operation on the unseen code. These examples were constructed from the
    remainder of the dataset and care was taken to:'
  prefs: []
  type: TYPE_NORMAL
- en: Draw from different scenarios than the code snippet under test, to avoid contamination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Draw exactly two vulnerable examples and one non-vulnerable example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]{example_0}[PRE4]{example_1}[PRE5]{example_2}[PRE6]{code}[PRE7]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Results**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Accuracy: 0.76'
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision: 0.71'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall: 0.81'
  prefs: []
  type: TYPE_NORMAL
- en: 'F1 Score: 0.76'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad0fc6f78f1872b89105b760a3bae4da.png)'
  prefs: []
  type: TYPE_IMG
- en: Few-shot Confusion Matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment 3: KNN Few-shot'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This [Microsoft blog post](https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/)
    describes an interesting technique called KNN-based few-shot example selection
    that can boost LLM response quality when using in-context examples. For this next
    experiment, instead of sampling shots at random, we calculate a similarity score
    between the input code and each candidate example and construct shots from the
    most similar candidates (still keeping the scenarios distinct). We use the ROUGE-L
    metric, but other metrics could be used too. The prompt template did not change
    from the second experiment.
  prefs: []
  type: TYPE_NORMAL
- en: '**Results**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Accuracy: 0.73'
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision: 0.70'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall: 0.76'
  prefs: []
  type: TYPE_NORMAL
- en: 'F1 Score: 0.73'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9ee2ba95807f183f34106c35f85e802.png)'
  prefs: []
  type: TYPE_IMG
- en: KNN Few-shot Confusion Matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment 4: KNN Few-shot with Code Fix'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this variation of the prompt, we include a request for a fixed version of
    the code if a CWE is found. This approach was inspired by [Noever](https://arxiv.org/abs/2308.10345),
    who proposed that prompting for CWE detection and a fix together might bring about
    a “virtuous cycle” and force the LLM to “self-audit” or think more deeply about
    the steps needed to accurately identify vulnerabilities, similar to chain-of-thought
    prompting. We did this by constructing vulnerable code in-context examples with
    code fix suggestions drawn from the non-vulnerable code samples for the same scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE8]{example_0}[PRE9]{example_1}[PRE10]{example_2}[PRE11]{code}[PRE12]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Results**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Accuracy: 0.80'
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision: 0.73'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall: 0.90'
  prefs: []
  type: TYPE_NORMAL
- en: 'F1 Score: 0.81'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94c581af5e970a06d8ffa5d198286554.png)'
  prefs: []
  type: TYPE_IMG
- en: KNN Few-shot Fix Confusion Matrix
  prefs: []
  type: TYPE_NORMAL
- en: In addition to CWE detection, this experiment has the benefit of producing suggested
    fixes. We have not evaluated them for quality yet, so that is an area for future
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Results and Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e74a694b2198e10dd9518e1415ce4363.png)'
  prefs: []
  type: TYPE_IMG
- en: On our small data sample, GPT4’s accuracy was 67% and its F1 score was 71% without
    any complex prompt adaptations. Small improvements were offered by some of the
    prompting techniques we tested, with few-shot and requesting a code fix standing
    out. The combination of techniques bumped accuracy and F1 score by about ten percentage
    points each from baseline, both metrics reaching or exceeding 80%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Results can be quite different between models, datasets, and prompts, so more
    investigation is needed. For example, it would be interesting to:'
  prefs: []
  type: TYPE_NORMAL
- en: Test smaller models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test a prompt template that includes the CWE label, to investigate the potential
    for combining LLMs with static analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test larger and more diverse datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the security and functionality of LLM-proposed code fixes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Study more advanced prompting techniques such as in-context example chains-of-thought,
    [Self-Consistency](https://arxiv.org/abs/2203.11171), and [Self-Discover](https://arxiv.org/abs/2402.03620)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you would like to see the code that produced these results, run it on your
    own code, or adapt it for your own needs, check out the [pull request](https://github.com/openai/openai-cookbook/pull/1112)
    in OpenAI Cookbook (currently under review).
  prefs: []
  type: TYPE_NORMAL
- en: Thank you to my colleagues [Matthew Fleetwood](https://github.com/etcylfleet)
    and [Abolfazl Shahbazi](https://github.com/ashahba) who made contributions and
    helped to review this article.
  prefs: []
  type: TYPE_NORMAL
- en: Citations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] D. Noever, [Can Large Language Models Find And Fix Vulnerable Software?](https://arxiv.org/abs/2308.10345)
    (2023), arXiv preprint arXiv:2308.10345'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt and R. Karri, [Asleep at the
    Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions](https://arxiv.org/abs/2108.09293)
    (2022), 2022 IEEE Symposium on Security and Privacy (SP)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] C. Koch, [I Used GPT-3 to Find 213 Security Vulnerabilities in a Single
    Codebase](https://betterprogramming.pub/i-used-gpt-3-to-find-213-security-vulnerabilities-in-a-single-codebase-cc3870ba9411)
    (2023), [https://betterprogramming.pub/i-used-gpt-3-to-find-213-security-vulnerabilities-in-a-single-codebase-cc3870ba9411](https://betterprogramming.pub/i-used-gpt-3-to-find-213-security-vulnerabilities-in-a-single-codebase-cc3870ba9411)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] A. Bakhshandeh, A. Keramatfar, A. Norouzi and M. M. Chekidehkhoun, [Using
    ChatGPT as a Static Application Security Testing Tool](https://arxiv.org/abs/2308.14434)
    (2023), arXiv preprint arXiv:2308.14434'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] H. Pearce, B. Tan, B. Ahmad, R. Karri and B. Dolan-Gavitt, [Examining Zero-Shot
    Vulnerability Repair with Large Language Models](https://arxiv.org/abs/2112.02125)
    (2023), 2023 IEEE Symposium on Security and Privacy (SP)'
  prefs: []
  type: TYPE_NORMAL
