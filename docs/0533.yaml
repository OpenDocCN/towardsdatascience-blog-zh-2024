- en: Vision Transformers, Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8?source=collection_archive---------1-----------------------#2024-02-27](https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8?source=collection_archive---------1-----------------------#2024-02-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Vision Transformers Explained Series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Full Walk-Through of *Vision Transformers in PyTorch*
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sjcallis?source=post_page---byline--a9d07147e4c8--------------------------------)[![Skylar
    Jean Callis](../Images/db4d07b27d7feb86bfbb73b1065aa3a0.png)](https://medium.com/@sjcallis?source=post_page---byline--a9d07147e4c8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a9d07147e4c8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a9d07147e4c8--------------------------------)
    [Skylar Jean Callis](https://medium.com/@sjcallis?source=post_page---byline--a9d07147e4c8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a9d07147e4c8--------------------------------)
    ·18 min read·Feb 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Since their introduction in 2017 with* Attention is All You Need*¹, transformers
    have established themselves as the state of the art for natural language processing
    (NLP). In 2021,* An Image is Worth 16x16 Words*² successfully adapted transformers
    for computer vision tasks. Since then, numerous transformer-based architectures
    have been proposed for computer vision.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**This article walks through the Vision Transformer (ViT) as laid out in *An
    Image is Worth 16x16 Words*². It includes open-source code for the ViT, as well
    as conceptual explanations of the components. All of the code uses the PyTorch
    Python package.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb8f3e52a6e63f4ae89de2cb443aa76d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sahand Babali](https://unsplash.com/@sahandbabali?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'This article is part of a collection examining the internal workings of Vision
    Transformers in depth. Each of these articles is also available as a Jupyter Notebook
    with executable code. The other articles in the series are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Vision Transformers, Explained**](/vision-transformers-explained-a9d07147e4c8)→
    [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/VisionTransformersExplained.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Attention for Vision Transformers, Explained](/attention-for-vision-transformers-explained-70f83984c673)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/AttentionExplained.ipynb)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Position Embeddings for Vision Transformers, Explained](/position-embeddings-for-vision-transformers-explained-a6f9add341d5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/PositionEmbeddingExplained.ipynb)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Tokens-to-Token Vision Transformers, Explained](/tokens-to-token-vision-transformers-explained-2fa4e2002daa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/TokensToTokenViTExplained.ipynb)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[GitHub Repository for Vision Transformers, Explained Series](https://github.com/lanl/vision_transformers_explained)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[What Are Vision Transformers?](#1d10)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Model Walk-Through](#bce9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: — [Image Tokenization](#691b)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — [Token Processing](#a32e)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — [Encoding Block](#962c)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — [Neural Network Module](#c80b)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — [Prediction Processing](#d3ad)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Complete Code](#3fdb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Conclusion](#caea)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: — [Further Reading](#96ea)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — [Citations](#dc9b)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What are Vision Transformers?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As introduced in *Attention is All You Need*¹, transformers are a type of machine
    learning model utilizing attention as the primary learning mechanism. Transformers
    quickly became the state of the art for sequence-to-sequence tasks such as language
    translation.
  prefs: []
  type: TYPE_NORMAL
- en: '*An Image is Worth 16x16 Words*² successfully modified the transformer put
    forth in [1] to solve image classification tasks, creating the **Vi**sion **T**ransformer
    (ViT). The ViT is based on the same attention mechanism as the transformer in
    [1]. However, while transformers for NLP tasks consist of an encoder attention
    branch and a decoder attention branch, the ViT only uses an encoder. The output
    of the encoder is then passed to a neural network “head” that makes a prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: The drawback of ViT as implemented in [2] is that it’s optimal performance requires
    pretraining on large datasets. The best models pretrained on the proprietary JFT-300M
    dataset. Models pretrained on the smaller, open source ImageNet-21k perform on
    par with the state-of-the-art convolutional ResNet models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet*³
    attempts to remove this pretraining requirement by introducing a novel pre-processing
    methodology to transform an input image into a series of tokens. More about this
    method can be found [here](/tokens-to-token-vision-transformers-explained-2fa4e2002daa).
    For this article, we’ll focus on the ViT as implemented in [2].'
  prefs: []
  type: TYPE_NORMAL
- en: Model Walk-Through
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article follows the model structure outlined in *An Image is Worth 16x16
    Words*². However, code from this paper is not publicly available. Code from the
    more recent *Tokens-to-Token ViT*³ is available on GitHub. The Tokens-to-Token
    ViT (T2T-ViT) model prepends a Tokens-to-Token (T2T) module to a vanilla ViT backbone.
    The code in this article is based on the ViT components in the *Tokens-to-Token
    ViT*³GitHub code. Modifications made for this article include, but are not limited
    to, modifying to allow for non-square input images and removing dropout layers.
  prefs: []
  type: TYPE_NORMAL
- en: A diagram of the ViT model is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e78867b93d211770cf4444ad59c6f8b5.png)'
  prefs: []
  type: TYPE_IMG
- en: ViT Model Diagram (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Image Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step of the ViT is to create tokens from the input image. Transformers
    operate on a *sequence* of *tokens*; in NLP, this is commonly a *sentence* of
    *words*. For computer vision, it is less clear how to segment the input into tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'TheViT converts an image to tokens such that each token represents a local
    area — or *patch* — of the image. They describe reshaping an image of height *H*,
    width *W*, and channels *C* into *N* tokens with patch size *P*:'
  prefs: []
  type: TYPE_NORMAL
- en: Each token is of length *P²∗C*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an example of patch tokenization on this pixel art *Mountain at
    Dusk* by Luis Zuno ([@ansimuz](http://twitter.com/ansimuz))⁴. The original artwork
    has been cropped and converted to a single channel image. This means that each
    pixel has a value between zero and one. Single channel images are typically displayed
    in grayscale; however, we’ll be displaying it in a purple color scheme because
    its easier to see.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the patch tokenization is not included in the code associated with
    [3]. All code in this section is original to the author.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bac19092e0adc6376b2a9f26c9bcc604.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: This image has *H=60* and *W=100*. We’ll set *P=20* since it divides both *H*
    and *W* evenly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/88fd07364a89357fe4edb4af137ee20d.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: By flattening these patches, we see the resulting tokens. Let’s look at patch
    12 as an example, since it has four different shades in it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d20a481555baf48c6ee77002eb4e003d.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: After extracting tokens from an image, it is common to use a linear projection
    to change the length of the tokens. This is implemented as a learnable linear
    layer. The new length of the tokens is referred to as the *latent dimension²*,
    *channel dimension³*, or the *token length.* After the projection, the tokens
    are no longer visually identifiable as a patch from the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the concept, we can look at how patch tokenization is
    implemented in code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note the two `assert` statements that ensure the image dimensions are evenly
    divisible by the patch size. The actual splitting into patches is implemented
    as a `torch.nn.Unfold`⁵ layer.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll run an example of this code using our cropped, single channel version
    of *Mountain at Dusk*⁴. We should see the values for number of tokens and initial
    token size as we did above. We’ll use *token_len=768* as the projected length,
    which is the size for the base variant of ViT².
  prefs: []
  type: TYPE_NORMAL
- en: The first line in the code block below is changing the datatype of *Mountain
    at Dusk*⁴ from a NumPy array to a Torch tensor. We also have to `unsqueeze`⁶ the
    tensor to create a channel dimension and a batch size dimension. As above, we
    have one channel. Since there is only one image, *batchsize=1*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, we’ll split the image into tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As we saw in the example, there are *N=15* tokens each of length 400\. Lastly,
    we project the tokens to be the *token_len*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have tokens, we’re ready to proceed through the ViT.
  prefs: []
  type: TYPE_NORMAL
- en: Token Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll designate the next two steps of the ViT, before the encoding blocks, as
    “token processing.” The token processing component of the ViT diagram is shown
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12ccf966ee3c533f10d3a53652cb4fdf.png)'
  prefs: []
  type: TYPE_IMG
- en: Token Processing Components of ViT Diagram (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to prepend a blank token, called the *Prediction Token,* to
    the the image tokens. This token will be used at the output of the encoding blocks
    to make a prediction. It starts off blank — equivalently zero — so that it can
    gain information from the other image tokens.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll be starting with 175 tokens. Each token has length 768, which is the size
    for the base variant of ViT². We’re using a batch size of 13 because it’s prime
    and won’t be confused for any of the other parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now, we add a position embedding for our tokens. The position embedding allows
    the transformer to understand the order of the image tokens. Note that this is
    an addition, not a concatenation. The specifics of position embeddings are a tangent
    best left for [another time](/position-embeddings-for-vision-transformers-explained-a6f9add341d5).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now, our tokens are ready to proceed to the encoding blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding Block
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The encoding block is where the model actually learns from the image tokens.
    The number of encoding blocks is a hyperparameter set by the user. A diagram of
    the encoding block is below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1200a46f21d3d72d0176d291146d2dd1.png)'
  prefs: []
  type: TYPE_IMG
- en: Encoding Block (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The code for an encoding block is below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The *num_heads, qkv_bias,* and *qk_scale* parameters define the *Attention*
    module components. A deep dive into attention for vision transformers is left
    for [another time](/attention-for-vision-transformers-explained-70f83984c673).
  prefs: []
  type: TYPE_NORMAL
- en: The *hidden_chan_mul* and *act_layer* parameters define the *Neural Network*
    module components. The activation layer can be any `torch.nn.modules.activation`⁷
    layer. We’ll look more at the *Neural Network* module [later](#c80b).
  prefs: []
  type: TYPE_NORMAL
- en: The *norm_layer* can be chosen from any `torch.nn.modules.normalization`⁸ layer.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll now step through each blue block in the diagram and its accompanying code.
    We’ll use 176 tokens of length 768\. We’ll use a batch size of 13 because it’s
    prime and won’t be confused for any of the other parameters. We’ll use 4 attention
    heads because it evenly divides token length; however, you won’t see the attention
    head dimension in the encoding block.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now, we’ll pass through a norm layer and an *Attention* module. The *Attention*
    module in the encoding block is parameterized so that it don’t change the token
    length. After the *Attention* module, we implement our first split connection.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now, we pass through another norm layer, and then the *Neural Network* module.
    We finish with the second split connection.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: That’s all for a single encoding block! Since the final dimensions are the same
    as the initial dimensions, the model can easily pass tokens through multiple encoding
    blocks, as set by the *depth* hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network Module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Neural Network* (NN) module is a sub-component of the encoding block. The
    NN module is very simple, consisting of a fully-connected layer, an activation
    layer, and another fully-connected layer. The activation layer can be any `torch.nn.modules.activation`⁷
    layer, which is passed as input to the module. The NN module can be configured
    to change the shape of an input, or to maintain the same shape. We’re not going
    to step through this code, as neural networks are common in machine learning,
    and not the focus of this article. However, the code for the NN module is presented
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Prediction Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After passing through the encoding blocks, the last thing the model must do
    is make a prediction. The “prediction processing” component of the ViT diagram
    is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d09768832b3f7457cf3cf0ac45e80310.png)'
  prefs: []
  type: TYPE_IMG
- en: Prediction Processing Components of ViT Diagram (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to look at each step of this process. We’ll continue with 176 tokens
    of length 768\. We’ll use a batch size of 1 to illustrate how a single prediction
    is made. A batch size greater than 1 would be computing this prediction in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: First, all the tokens are passed through a norm layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Next, we split off the prediction token from the rest of the tokens. Throughout
    the encoding block(s), the prediction token has become nonzero and gained information
    about our input image. We’ll use only this prediction token to make a final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the prediction token is passed through the *head* to make a prediction.
    The *head*, usually some variety of neural network, is varied based on the model.
    In *An Image is Worth 16x16 Words*², they use an MLP ([multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron))
    with one hidden layer during pretraining and a single linear layer during fine
    tuning. In *Tokens-to-Token ViT³*, they use a single linear layer as a head. This
    example proceeds with a single linear layer.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the output shape of the head is set based on the parameters of the
    learning problem. For classification, it is typically a vector of length *number
    of classes* in a [one-hot encoding](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics).
    For regression, it would be any integer number of predicted parameters. This example
    will use an output shape of 1 to represent a single estimated regression value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: And that’s all! The model has made a prediction!
  prefs: []
  type: TYPE_NORMAL
- en: Complete Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To create the complete ViT module, we use the *Patch Tokenization* module defined
    [above](#691b) and the *ViT Backbone* module. The *ViT Backbone* is defined below,
    and contains the [*Token Processing*](#a32e), [*Encoding Blocks*](#962c), and
    [*Prediction Processing*](http://d3ad)components.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: From the *ViT Backbone* module, we can define the full ViT model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In the *ViT Model*, the *img_size*, *patch_size*, and *token_len* define the
    *Patch Tokenization* module.
  prefs: []
  type: TYPE_NORMAL
- en: The *num_heads*, *Encoding_hidden_channel_mul*, *qkv_bias*, *qk_scale*, and
    *act_layer* parameters define the *Encoding Bock* modules. The *act_layer* can
    be any `torch.nn.modules.activation`⁷ layer. The *depth* parameter determines
    how many encoding blocks are in the model.
  prefs: []
  type: TYPE_NORMAL
- en: The *norm_layer* parameter sets the norm for both within and outside of the
    *Encoding Block* modules. It can be chosen from any `torch.nn.modules.normalization`⁸
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: The *_init_weights* method comes from the T2T-ViT³ code. This method could be
    deleted to initiate all learned weights and biases randomly. As implemented, the
    weights of linear layers are initialized as a truncated normal distribution; the
    biases of linear layers are initialized as zero; the weights of normalization
    layers are initialized as one; the biases of normalization layers are initialized
    as zero.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, you can go forth and train ViT models with a deep understanding of their
    mechanics! Below is a list of places to download code for ViT models. Some of
    them allow for more modifications of the model than others. Happy transforming!
  prefs: []
  type: TYPE_NORMAL
- en: '[GitHub Repository](https://github.com/lanl/vision_transformers_explained)
    for this Article Series'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GitHub Repository](https://github.com/google-research/vision_transformer)
    for *An Image is Worth 16x16 Words*²'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → Contains pretrained models and code for fine-tuning; does not contain model
    definitions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ViT as implemented in [PyTorch Image Models](https://github.com/huggingface/pytorch-image-models)
    (`timm`)⁹
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timm.create_model(''vit_base_patch16_224'', pretrained=True)`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Phil Wang’s `vit-pytorch` [package](https://github.com/lucidrains/vit-pytorch)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This article was approved for release by Los Alamos National Laboratory as LA-UR-23–33876\.
    The associated code was approved for a BSD-3 open source license under O#4693.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To learn more about transformers in NLP contexts, see
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers Explained Visually Part 1 Overview of Functionality: [https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452](/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transformers Explained Visually Part 2 How it Works, Step by Step: [https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34](/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a video lecture broadly about vision transformers, see
  prefs: []
  type: TYPE_NORMAL
- en: 'Vision Transformer and its Applications: [https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP](https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Citations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Vaswani et al (2017). *Attention Is All You Need.* [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Dosovitskiy et al (2020). *An Image is Worth 16x16 Words: Transformers
    for Image Recognition at Scale.* [https://doi.org/10.48550/arXiv.2010.11929](https://doi.org/10.48550/arXiv.2010.11929)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Yuan et al (2021). *Tokens-to-Token ViT: Training Vision Transformers from
    Scratch on ImageNet*. [https://doi.org/10.48550/arXiv.2101.11986](https://doi.org/10.48550/arXiv.2101.11986)'
  prefs: []
  type: TYPE_NORMAL
- en: '→ GitHub code: [https://github.com/yitu-opensource/T2T-ViT](https://github.com/yitu-opensource/T2T-ViT)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Luis Zuno ([@ansimuz](http://twitter.com/ansimuz)). *Mountain at Dusk Background.*
    License CC0: [https://opengameart.org/content/mountain-at-dusk-background](https://opengameart.org/content/mountain-at-dusk-background)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] PyTorch. *Unfold.* [https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold](https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] PyTorch. *Unsqueeze.* [https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] PyTorch. *Non-linear Activation (weighted sum, nonlinearity).* [https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] PyTorch. *Normalization Layers*. [https://pytorch.org/docs/stable/nn.html#normalization-layers](https://pytorch.org/docs/stable/nn.html#normalization-layers)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Ross Wightman. *PyTorch Image Models.* [https://github.com/huggingface/pytorch-image-models](https://github.com/huggingface/pytorch-image-models)'
  prefs: []
  type: TYPE_NORMAL
