- en: Vision Transformers, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Vision Transformers 解析
- en: 原文：[https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8?source=collection_archive---------1-----------------------#2024-02-27](https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8?source=collection_archive---------1-----------------------#2024-02-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8?source=collection_archive---------1-----------------------#2024-02-27](https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8?source=collection_archive---------1-----------------------#2024-02-27)
- en: Vision Transformers Explained Series
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vision Transformer 解析系列
- en: A Full Walk-Through of *Vision Transformers in PyTorch*
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整的*Vision Transformers in PyTorch*演示
- en: '[](https://medium.com/@sjcallis?source=post_page---byline--a9d07147e4c8--------------------------------)[![Skylar
    Jean Callis](../Images/db4d07b27d7feb86bfbb73b1065aa3a0.png)](https://medium.com/@sjcallis?source=post_page---byline--a9d07147e4c8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a9d07147e4c8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a9d07147e4c8--------------------------------)
    [Skylar Jean Callis](https://medium.com/@sjcallis?source=post_page---byline--a9d07147e4c8--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sjcallis?source=post_page---byline--a9d07147e4c8--------------------------------)[![Skylar
    Jean Callis](../Images/db4d07b27d7feb86bfbb73b1065aa3a0.png)](https://medium.com/@sjcallis?source=post_page---byline--a9d07147e4c8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a9d07147e4c8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a9d07147e4c8--------------------------------)
    [Skylar Jean Callis](https://medium.com/@sjcallis?source=post_page---byline--a9d07147e4c8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a9d07147e4c8--------------------------------)
    ·18 min read·Feb 27, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a9d07147e4c8--------------------------------)
    ·18分钟阅读·2024年2月27日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*Since their introduction in 2017 with* Attention is All You Need*¹, transformers
    have established themselves as the state of the art for natural language processing
    (NLP). In 2021,* An Image is Worth 16x16 Words*² successfully adapted transformers
    for computer vision tasks. Since then, numerous transformer-based architectures
    have been proposed for computer vision.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*自从2017年随着《Attention is All You Need》*¹的推出，transformer模型已成为自然语言处理（NLP）领域的最先进技术。2021年，*An
    Image is Worth 16x16 Words*²成功将transformer模型应用于计算机视觉任务。从那时起，许多基于transformer的架构已被提出，用于计算机视觉。*'
- en: '**This article walks through the Vision Transformer (ViT) as laid out in *An
    Image is Worth 16x16 Words*². It includes open-source code for the ViT, as well
    as conceptual explanations of the components. All of the code uses the PyTorch
    Python package.**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**本文详细介绍了《An Image is Worth 16x16 Words*²》中的Vision Transformer（ViT）。它包括ViT的开源代码，以及对各个组件的概念性解释。所有代码均使用PyTorch
    Python包。**'
- en: '![](../Images/cb8f3e52a6e63f4ae89de2cb443aa76d.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb8f3e52a6e63f4ae89de2cb443aa76d.png)'
- en: Photo by [Sahand Babali](https://unsplash.com/@sahandbabali?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[Sahand Babali](https://unsplash.com/@sahandbabali?utm_source=medium&utm_medium=referral)拍摄，[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'This article is part of a collection examining the internal workings of Vision
    Transformers in depth. Each of these articles is also available as a Jupyter Notebook
    with executable code. The other articles in the series are:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是系列文章的一部分，深入探讨了Vision Transformer的内部工作原理。这些文章也可以作为Jupyter Notebook下载并运行。该系列的其他文章包括：
- en: '[**Vision Transformers, Explained**](/vision-transformers-explained-a9d07147e4c8)→
    [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/VisionTransformersExplained.ipynb)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Vision Transformers 解析**](/vision-transformers-explained-a9d07147e4c8)→
    [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/VisionTransformersExplained.ipynb)'
- en: '[Attention for Vision Transformers, Explained](/attention-for-vision-transformers-explained-70f83984c673)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Vision Transformer 解析中的Attention部分](/attention-for-vision-transformers-explained-70f83984c673)'
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/AttentionExplained.ipynb)
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/AttentionExplained.ipynb)
- en: '[Position Embeddings for Vision Transformers, Explained](/position-embeddings-for-vision-transformers-explained-a6f9add341d5)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Vision Transformers位置嵌入解析](/position-embeddings-for-vision-transformers-explained-a6f9add341d5)'
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/PositionEmbeddingExplained.ipynb)
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/PositionEmbeddingExplained.ipynb)
- en: '[Tokens-to-Token Vision Transformers, Explained](/tokens-to-token-vision-transformers-explained-2fa4e2002daa)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tokens-to-Token 视觉变换器解析](/tokens-to-token-vision-transformers-explained-2fa4e2002daa)'
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/TokensToTokenViTExplained.ipynb)
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/TokensToTokenViTExplained.ipynb)
- en: '[GitHub Repository for Vision Transformers, Explained Series](https://github.com/lanl/vision_transformers_explained)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[视觉变换器系列的GitHub代码库](https://github.com/lanl/vision_transformers_explained)'
- en: Table of Contents
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: '[What Are Vision Transformers?](#1d10)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是视觉变换器？](#1d10)'
- en: '[Model Walk-Through](#bce9)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模型演示](#bce9)'
- en: — [Image Tokenization](#691b)
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — [图像Token化](#691b)
- en: — [Token Processing](#a32e)
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — [Token处理](#a32e)
- en: — [Encoding Block](#962c)
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — [编码模块](#962c)
- en: — [Neural Network Module](#c80b)
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — [神经网络模块](#c80b)
- en: — [Prediction Processing](#d3ad)
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — [预测处理](#d3ad)
- en: '[Complete Code](#3fdb)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[完整代码](#3fdb)'
- en: '[Conclusion](#caea)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[结论](#caea)'
- en: — [Further Reading](#96ea)
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — [进一步阅读](#96ea)
- en: — [Citations](#dc9b)
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — [引用](#dc9b)
- en: What are Vision Transformers?
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是视觉变换器？
- en: As introduced in *Attention is All You Need*¹, transformers are a type of machine
    learning model utilizing attention as the primary learning mechanism. Transformers
    quickly became the state of the art for sequence-to-sequence tasks such as language
    translation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如*《Attention is All You Need》*¹中介绍的，变换器是一种利用注意力机制作为主要学习方式的机器学习模型。变换器迅速成为处理序列到序列任务（如语言翻译）的最先进模型。
- en: '*An Image is Worth 16x16 Words*² successfully modified the transformer put
    forth in [1] to solve image classification tasks, creating the **Vi**sion **T**ransformer
    (ViT). The ViT is based on the same attention mechanism as the transformer in
    [1]. However, while transformers for NLP tasks consist of an encoder attention
    branch and a decoder attention branch, the ViT only uses an encoder. The output
    of the encoder is then passed to a neural network “head” that makes a prediction.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*《一张图片等于16x16个词》*²成功地修改了[1]中的变换器，以解决图像分类任务，从而创建了**视觉**变换器（ViT）。ViT基于与[1]中的变换器相同的注意力机制。然而，虽然NLP任务的变换器包含一个编码器注意力分支和一个解码器注意力分支，ViT仅使用一个编码器。编码器的输出随后传递给一个神经网络“头部”，进行预测。'
- en: The drawback of ViT as implemented in [2] is that it’s optimal performance requires
    pretraining on large datasets. The best models pretrained on the proprietary JFT-300M
    dataset. Models pretrained on the smaller, open source ImageNet-21k perform on
    par with the state-of-the-art convolutional ResNet models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]中实现的ViT的缺点是，其最佳性能需要在大型数据集上进行预训练。最佳的模型是在专有的JFT-300M数据集上进行预训练的。在较小的、开源的ImageNet-21k数据集上进行预训练的模型，其表现与最先进的卷积ResNet模型不相上下。'
- en: '*Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet*³
    attempts to remove this pretraining requirement by introducing a novel pre-processing
    methodology to transform an input image into a series of tokens. More about this
    method can be found [here](/tokens-to-token-vision-transformers-explained-2fa4e2002daa).
    For this article, we’ll focus on the ViT as implemented in [2].'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*Tokens-to-Token ViT：从头开始在ImageNet上训练视觉变换器*³试图通过引入一种新的预处理方法来消除预训练的需求， 将输入图像转换成一系列Token。有关此方法的更多信息，请参见[这里](/tokens-to-token-vision-transformers-explained-2fa4e2002daa)。本文将重点介绍[2]中实现的ViT。'
- en: Model Walk-Through
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型演示
- en: This article follows the model structure outlined in *An Image is Worth 16x16
    Words*². However, code from this paper is not publicly available. Code from the
    more recent *Tokens-to-Token ViT*³ is available on GitHub. The Tokens-to-Token
    ViT (T2T-ViT) model prepends a Tokens-to-Token (T2T) module to a vanilla ViT backbone.
    The code in this article is based on the ViT components in the *Tokens-to-Token
    ViT*³GitHub code. Modifications made for this article include, but are not limited
    to, modifying to allow for non-square input images and removing dropout layers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本文遵循了*《一张图片等于16x16个词》*²中概述的模型结构。然而，本文中所用的代码并未公开。更近期的*Tokens-to-Token ViT*³的代码已在GitHub上公开。Tokens-to-Token
    ViT（T2T-ViT）模型在普通的ViT主干网络前添加了一个Tokens-to-Token（T2T）模块。本文中的代码基于*Tokens-to-Token
    ViT*³ GitHub代码中的ViT组件。本文所做的修改包括但不限于，修改以支持非方形输入图像，并移除dropout层。
- en: A diagram of the ViT model is shown below.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了ViT模型。
- en: '![](../Images/e78867b93d211770cf4444ad59c6f8b5.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e78867b93d211770cf4444ad59c6f8b5.png)'
- en: ViT Model Diagram (image by author)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ViT模型图（图片来源：作者）
- en: Image Tokenization
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像标记化
- en: The first step of the ViT is to create tokens from the input image. Transformers
    operate on a *sequence* of *tokens*; in NLP, this is commonly a *sentence* of
    *words*. For computer vision, it is less clear how to segment the input into tokens.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ViT的第一步是从输入图像创建标记（tokens）。Transformer处理的是*标记*的*序列*；在自然语言处理（NLP）中，这通常是一个*单词*的*句子*。对于计算机视觉而言，如何将输入分割成标记并不那么明确。
- en: 'TheViT converts an image to tokens such that each token represents a local
    area — or *patch* — of the image. They describe reshaping an image of height *H*,
    width *W*, and channels *C* into *N* tokens with patch size *P*:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ViT将图像转换为标记，每个标记表示图像的一个局部区域——或称*补丁*。它们描述了如何将高度为*H*、宽度为*W*、通道数为*C*的图像重塑为*N*个补丁大小为*P*的标记：
- en: Each token is of length *P²∗C*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每个标记的长度为*P²∗C*。
- en: Let’s look at an example of patch tokenization on this pixel art *Mountain at
    Dusk* by Luis Zuno ([@ansimuz](http://twitter.com/ansimuz))⁴. The original artwork
    has been cropped and converted to a single channel image. This means that each
    pixel has a value between zero and one. Single channel images are typically displayed
    in grayscale; however, we’ll be displaying it in a purple color scheme because
    its easier to see.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在这幅由Luis Zuno创作的像素艺术*《黄昏中的山脉》*（[@ansimuz](http://twitter.com/ansimuz)）⁴上进行的补丁标记化示例。原始艺术作品已被裁剪并转换为单通道图像。这意味着每个像素的值在0和1之间。单通道图像通常以灰度显示；然而，我们将使用紫色配色方案进行展示，因为这样更容易看清。
- en: Note that the patch tokenization is not included in the code associated with
    [3]. All code in this section is original to the author.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，补丁标记化并未包含在与[3]相关的代码中。本节中的所有代码均为作者原创。
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/bac19092e0adc6376b2a9f26c9bcc604.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bac19092e0adc6376b2a9f26c9bcc604.png)'
- en: Code Output (image by author)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出（图片来源：作者）
- en: This image has *H=60* and *W=100*. We’ll set *P=20* since it divides both *H*
    and *W* evenly.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这幅图像的*H=60*和*W=100*。我们将设置*P=20*，因为它可以均匀地划分*H*和*W*。
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/88fd07364a89357fe4edb4af137ee20d.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88fd07364a89357fe4edb4af137ee20d.png)'
- en: Code Output (image by author)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出（图片来源：作者）
- en: By flattening these patches, we see the resulting tokens. Let’s look at patch
    12 as an example, since it has four different shades in it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过展开这些补丁，我们可以看到生成的标记。以补丁12为例，因为它包含四种不同的阴影。
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/d20a481555baf48c6ee77002eb4e003d.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d20a481555baf48c6ee77002eb4e003d.png)'
- en: Code Output (image by author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出（图片来源：作者）
- en: After extracting tokens from an image, it is common to use a linear projection
    to change the length of the tokens. This is implemented as a learnable linear
    layer. The new length of the tokens is referred to as the *latent dimension²*,
    *channel dimension³*, or the *token length.* After the projection, the tokens
    are no longer visually identifiable as a patch from the original image.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像中提取标记后，通常会使用线性投影来改变标记的长度。这通常是通过一个可学习的线性层来实现的。标记的新长度被称为*潜在维度²*、*通道维度³*或*标记长度*。经过投影后，标记不再能被直观地识别为原始图像中的补丁。
- en: Now that we understand the concept, we can look at how patch tokenization is
    implemented in code.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了这个概念，可以看看如何在代码中实现补丁标记化（patch tokenization）。
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note the two `assert` statements that ensure the image dimensions are evenly
    divisible by the patch size. The actual splitting into patches is implemented
    as a `torch.nn.Unfold`⁵ layer.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，两个`assert`语句确保图像尺寸能被补丁大小整除。实际的补丁拆分是通过`torch.nn.Unfold`⁵层来实现的。
- en: We’ll run an example of this code using our cropped, single channel version
    of *Mountain at Dusk*⁴. We should see the values for number of tokens and initial
    token size as we did above. We’ll use *token_len=768* as the projected length,
    which is the size for the base variant of ViT².
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用裁剪后的单通道*《黄昏中的山脉》*⁴版本运行此代码示例。我们应该能看到与之前相同的标记数和初始标记大小。我们将使用*token_len=768*作为投影后的长度，这是ViT²基础版本的大小。
- en: The first line in the code block below is changing the datatype of *Mountain
    at Dusk*⁴ from a NumPy array to a Torch tensor. We also have to `unsqueeze`⁶ the
    tensor to create a channel dimension and a batch size dimension. As above, we
    have one channel. Since there is only one image, *batchsize=1*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块的第一行将*《黄昏中的山脉》*⁴的数据类型从NumPy数组更改为Torch张量。我们还需要对张量进行`unsqueeze`⁶操作，以创建通道维度和批次大小维度。如上所述，我们有一个通道。由于只有一张图片，*batchsize=1*。
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, we’ll split the image into tokens.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将把图像拆分成标记（tokens）。
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we saw in the example, there are *N=15* tokens each of length 400\. Lastly,
    we project the tokens to be the *token_len*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在示例中看到的，每个长度为 400 的 *N=15* 个 token。最后，我们将 token 投影为 *token_len*。
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that we have tokens, we’re ready to proceed through the ViT.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了 token，准备继续处理 ViT。
- en: Token Processing
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Token 处理
- en: We’ll designate the next two steps of the ViT, before the encoding blocks, as
    “token processing.” The token processing component of the ViT diagram is shown
    below.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 ViT 的接下来的两个步骤，在编码块之前，称为“token 处理”。ViT 图中的 token 处理组件如下所示。
- en: '![](../Images/12ccf966ee3c533f10d3a53652cb4fdf.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12ccf966ee3c533f10d3a53652cb4fdf.png)'
- en: Token Processing Components of ViT Diagram (image by author)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ViT 图中的 Token 处理组件（图片由作者提供）
- en: The first step is to prepend a blank token, called the *Prediction Token,* to
    the the image tokens. This token will be used at the output of the encoding blocks
    to make a prediction. It starts off blank — equivalently zero — so that it can
    gain information from the other image tokens.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是向图像 token 前添加一个空白 token，称为 *预测 Token*。这个 token 将在编码块的输出中用于进行预测。它最初是空白的——相当于零——这样它可以从其他图像
    token 中获取信息。
- en: We’ll be starting with 175 tokens. Each token has length 768, which is the size
    for the base variant of ViT². We’re using a batch size of 13 because it’s prime
    and won’t be confused for any of the other parameters.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 175 个 token 开始。每个 token 的长度为 768，这是 ViT² 基本变种的大小。我们将使用批量大小为 13，因为它是质数，不会与其他参数混淆。
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now, we add a position embedding for our tokens. The position embedding allows
    the transformer to understand the order of the image tokens. Note that this is
    an addition, not a concatenation. The specifics of position embeddings are a tangent
    best left for [another time](/position-embeddings-for-vision-transformers-explained-a6f9add341d5).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们为我们的 token 添加一个位置嵌入。位置嵌入使 Transformer 能够理解图像 token 的顺序。请注意，这是一种加法操作，而不是拼接。位置嵌入的具体细节是一个偏题，最好留待[另一个时间](/position-embeddings-for-vision-transformers-explained-a6f9add341d5)讨论。
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, our tokens are ready to proceed to the encoding blocks.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的 token 准备好进入编码块了。
- en: Encoding Block
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码块
- en: The encoding block is where the model actually learns from the image tokens.
    The number of encoding blocks is a hyperparameter set by the user. A diagram of
    the encoding block is below.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 编码块是模型实际从图像 token 中学习的地方。编码块的数量是由用户设置的超参数。编码块的图示如下。
- en: '![](../Images/1200a46f21d3d72d0176d291146d2dd1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1200a46f21d3d72d0176d291146d2dd1.png)'
- en: Encoding Block (image by author)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 编码块（图片由作者提供）
- en: The code for an encoding block is below.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 编码块的代码如下。
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The *num_heads, qkv_bias,* and *qk_scale* parameters define the *Attention*
    module components. A deep dive into attention for vision transformers is left
    for [another time](/attention-for-vision-transformers-explained-70f83984c673).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*num_heads, qkv_bias,* 和 *qk_scale* 参数定义了 *注意力* 模块的组件。关于视觉 Transformer 中的注意力机制的深入讨论留待[另一个时间](/attention-for-vision-transformers-explained-70f83984c673)。'
- en: The *hidden_chan_mul* and *act_layer* parameters define the *Neural Network*
    module components. The activation layer can be any `torch.nn.modules.activation`⁷
    layer. We’ll look more at the *Neural Network* module [later](#c80b).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*hidden_chan_mul* 和 *act_layer* 参数定义了 *神经网络* 模块的组件。激活层可以是任何 `torch.nn.modules.activation`⁷
    层。我们稍后会更详细地介绍 *神经网络* 模块[更多内容](#c80b)。'
- en: The *norm_layer* can be chosen from any `torch.nn.modules.normalization`⁸ layer.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*norm_layer* 可以从任何 `torch.nn.modules.normalization`⁸ 层中选择。'
- en: We’ll now step through each blue block in the diagram and its accompanying code.
    We’ll use 176 tokens of length 768\. We’ll use a batch size of 13 because it’s
    prime and won’t be confused for any of the other parameters. We’ll use 4 attention
    heads because it evenly divides token length; however, you won’t see the attention
    head dimension in the encoding block.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将逐步解析图中的每个蓝色块及其伴随的代码。我们将使用 176 个长度为 768 的 token。我们将使用批量大小为 13，因为它是质数，不会与其他参数混淆。我们将使用
    4 个注意力头，因为它能均匀分配 token 长度；然而，在编码块中，你不会看到注意力头维度。
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now, we’ll pass through a norm layer and an *Attention* module. The *Attention*
    module in the encoding block is parameterized so that it don’t change the token
    length. After the *Attention* module, we implement our first split connection.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过一个归一化层和一个 *注意力* 模块。编码块中的 *注意力* 模块进行了参数化，以确保不会改变 token 长度。在 *注意力* 模块之后，我们实现第一个分割连接。
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, we pass through another norm layer, and then the *Neural Network* module.
    We finish with the second split connection.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过另一个归一化层，然后是 *神经网络* 模块。最后，我们完成第二个分割连接。
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: That’s all for a single encoding block! Since the final dimensions are the same
    as the initial dimensions, the model can easily pass tokens through multiple encoding
    blocks, as set by the *depth* hyperparameter.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是单个编码块的全部内容！由于最终的维度与初始维度相同，模型可以轻松地通过多个编码块传递标记，这由*深度*超参数设置。
- en: Neural Network Module
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络模块
- en: The *Neural Network* (NN) module is a sub-component of the encoding block. The
    NN module is very simple, consisting of a fully-connected layer, an activation
    layer, and another fully-connected layer. The activation layer can be any `torch.nn.modules.activation`⁷
    layer, which is passed as input to the module. The NN module can be configured
    to change the shape of an input, or to maintain the same shape. We’re not going
    to step through this code, as neural networks are common in machine learning,
    and not the focus of this article. However, the code for the NN module is presented
    below.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*神经网络*（NN）模块是编码块的一个子组件。NN 模块非常简单，由一个全连接层、一个激活层和另一个全连接层组成。激活层可以是任何`torch.nn.modules.activation`⁷层，并作为输入传递给模块。NN
    模块可以配置为改变输入的形状，或保持相同形状。我们不会逐步讲解这段代码，因为神经网络在机器学习中很常见，并不是本文的重点。然而，NN 模块的代码如下所示。'
- en: '[PRE24]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Prediction Processing
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测处理
- en: After passing through the encoding blocks, the last thing the model must do
    is make a prediction. The “prediction processing” component of the ViT diagram
    is shown below.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过编码块后，模型必须做的最后一件事就是进行预测。ViT 图示中的“预测处理”组件如下所示。
- en: '![](../Images/d09768832b3f7457cf3cf0ac45e80310.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d09768832b3f7457cf3cf0ac45e80310.png)'
- en: Prediction Processing Components of ViT Diagram (image by author)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ViT 图示中的预测处理组件（图片由作者提供）
- en: We’re going to look at each step of this process. We’ll continue with 176 tokens
    of length 768\. We’ll use a batch size of 1 to illustrate how a single prediction
    is made. A batch size greater than 1 would be computing this prediction in parallel.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步分析这个过程的每个步骤。我们将继续使用 176 个长度为 768 的标记。为了演示如何进行单次预测，我们将使用批量大小为 1。批量大小大于 1
    会并行计算此预测。
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: First, all the tokens are passed through a norm layer.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，所有标记都会通过一个标准化层。
- en: '[PRE27]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Next, we split off the prediction token from the rest of the tokens. Throughout
    the encoding block(s), the prediction token has become nonzero and gained information
    about our input image. We’ll use only this prediction token to make a final prediction.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将预测标记从其余标记中分离出来。在整个编码块中，预测标记变为非零，并且获得了关于输入图像的信息。我们将仅使用这个预测标记来进行最终的预测。
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Finally, the prediction token is passed through the *head* to make a prediction.
    The *head*, usually some variety of neural network, is varied based on the model.
    In *An Image is Worth 16x16 Words*², they use an MLP ([multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron))
    with one hidden layer during pretraining and a single linear layer during fine
    tuning. In *Tokens-to-Token ViT³*, they use a single linear layer as a head. This
    example proceeds with a single linear layer.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，预测标记通过*头部*进行预测。*头部*，通常是某种类型的神经网络，根据模型的不同而有所变化。在《*An Image is Worth 16x16
    Words*²》中，他们在预训练时使用一个带有隐藏层的 MLP（[多层感知器](https://en.wikipedia.org/wiki/Multilayer_perceptron)），在微调时使用一个线性层。在《*Tokens-to-Token
    ViT³*》中，他们使用一个线性层作为头部。这个例子将使用单个线性层。
- en: Note that the output shape of the head is set based on the parameters of the
    learning problem. For classification, it is typically a vector of length *number
    of classes* in a [one-hot encoding](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics).
    For regression, it would be any integer number of predicted parameters. This example
    will use an output shape of 1 to represent a single estimated regression value.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，头部的输出形状是根据学习问题的参数设置的。对于分类，通常是一个长度为*类别数*的向量，采用[独热编码](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics)。对于回归，它将是任何整数数量的预测参数。这个例子将使用一个输出形状为
    1 来表示一个单一的回归估计值。
- en: '[PRE31]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: And that’s all! The model has made a prediction!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！模型已经做出了预测！
- en: Complete Code
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整代码
- en: To create the complete ViT module, we use the *Patch Tokenization* module defined
    [above](#691b) and the *ViT Backbone* module. The *ViT Backbone* is defined below,
    and contains the [*Token Processing*](#a32e), [*Encoding Blocks*](#962c), and
    [*Prediction Processing*](http://d3ad)components.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建完整的 ViT 模块，我们使用[上面](#691b)定义的*补丁标记化*模块和*ViT 主干*模块。*ViT 主干*定义如下，并包含[*标记处理*](#a32e)、[*编码块*](#962c)和[*预测处理*](http://d3ad)组件。
- en: '[PRE33]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: From the *ViT Backbone* module, we can define the full ViT model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 从*ViT Backbone*模块，我们可以定义完整的ViT模型。
- en: '[PRE34]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the *ViT Model*, the *img_size*, *patch_size*, and *token_len* define the
    *Patch Tokenization* module.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在*ViT模型*中，*img_size*、*patch_size*和*token_len*定义了*Patch Tokenization*模块。
- en: The *num_heads*, *Encoding_hidden_channel_mul*, *qkv_bias*, *qk_scale*, and
    *act_layer* parameters define the *Encoding Bock* modules. The *act_layer* can
    be any `torch.nn.modules.activation`⁷ layer. The *depth* parameter determines
    how many encoding blocks are in the model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*num_heads*、*Encoding_hidden_channel_mul*、*qkv_bias*、*qk_scale*和*act_layer*参数定义了*Encoding
    Block*模块。*act_layer*可以是任何`torch.nn.modules.activation`⁷层。*depth*参数决定了模型中编码块的数量。'
- en: The *norm_layer* parameter sets the norm for both within and outside of the
    *Encoding Block* modules. It can be chosen from any `torch.nn.modules.normalization`⁸
    layer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*norm_layer*参数设置了*Encoding Block*模块内外的规范化层。可以从任何`torch.nn.modules.normalization`⁸层中选择。'
- en: The *_init_weights* method comes from the T2T-ViT³ code. This method could be
    deleted to initiate all learned weights and biases randomly. As implemented, the
    weights of linear layers are initialized as a truncated normal distribution; the
    biases of linear layers are initialized as zero; the weights of normalization
    layers are initialized as one; the biases of normalization layers are initialized
    as zero.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*\_init_weights*方法来源于T2T-ViT³代码。此方法可以删除，以随机初始化所有学习的权重和偏差。按实现方式，线性层的权重初始化为截断的正态分布；线性层的偏差初始化为零；归一化层的权重初始化为一；归一化层的偏差初始化为零。'
- en: Conclusion
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Now, you can go forth and train ViT models with a deep understanding of their
    mechanics! Below is a list of places to download code for ViT models. Some of
    them allow for more modifications of the model than others. Happy transforming!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以带着对ViT模型机制的深刻理解，去训练ViT模型了！下面是可以下载ViT模型代码的地方列表，其中一些比其他的允许更多的模型修改。祝您转换愉快！
- en: '[GitHub Repository](https://github.com/lanl/vision_transformers_explained)
    for this Article Series'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub 仓库](https://github.com/lanl/vision_transformers_explained)用于本系列文章'
- en: '[GitHub Repository](https://github.com/google-research/vision_transformer)
    for *An Image is Worth 16x16 Words*²'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub 仓库](https://github.com/google-research/vision_transformer)用于*An Image
    is Worth 16x16 Words*²'
- en: → Contains pretrained models and code for fine-tuning; does not contain model
    definitions
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: → 包含预训练模型和微调代码；不包含模型定义
- en: ViT as implemented in [PyTorch Image Models](https://github.com/huggingface/pytorch-image-models)
    (`timm`)⁹
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViT在[PyTorch Image Models](https://github.com/huggingface/pytorch-image-models)（`timm`）中的实现⁹
- en: '`timm.create_model(''vit_base_patch16_224'', pretrained=True)`'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`timm.create_model(''vit_base_patch16_224'', pretrained=True)`'
- en: Phil Wang’s `vit-pytorch` [package](https://github.com/lucidrains/vit-pytorch)
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phil Wang的`vit-pytorch` [包](https://github.com/lucidrains/vit-pytorch)
- en: This article was approved for release by Los Alamos National Laboratory as LA-UR-23–33876\.
    The associated code was approved for a BSD-3 open source license under O#4693.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本文已由洛斯阿拉莫斯国家实验室批准发布，批准编号为LA-UR-23–33876。相关代码已获得BSD-3开源许可证，批准编号为O#4693。
- en: Further Reading
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: To learn more about transformers in NLP contexts, see
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于NLP上下文中transformer的信息，请参见
- en: 'Transformers Explained Visually Part 1 Overview of Functionality: [https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452](/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers视觉化解析第1部分：功能概述：[https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452](/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452)
- en: 'Transformers Explained Visually Part 2 How it Works, Step by Step: [https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34](/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers视觉化解析第2部分：如何一步步工作：[https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34](/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34)
- en: For a video lecture broadly about vision transformers, see
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Vision Transformers的广泛视频讲座，请参见
- en: 'Vision Transformer and its Applications: [https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP](https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vision Transformer及其应用：[https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP](https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP)
- en: Citations
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引用
- en: '[1] Vaswani et al (2017). *Attention Is All You Need.* [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Vaswani等（2017）。*Attention Is All You Need.* [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)'
- en: '[2] Dosovitskiy et al (2020). *An Image is Worth 16x16 Words: Transformers
    for Image Recognition at Scale.* [https://doi.org/10.48550/arXiv.2010.11929](https://doi.org/10.48550/arXiv.2010.11929)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Dosovitskiy 等人（2020）。*一张图片值16x16个词：用于大规模图像识别的 Transformer。* [https://doi.org/10.48550/arXiv.2010.11929](https://doi.org/10.48550/arXiv.2010.11929)'
- en: '[3] Yuan et al (2021). *Tokens-to-Token ViT: Training Vision Transformers from
    Scratch on ImageNet*. [https://doi.org/10.48550/arXiv.2101.11986](https://doi.org/10.48550/arXiv.2101.11986)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Yuan 等人（2021）。*Tokens-to-Token ViT：从零开始在 ImageNet 上训练视觉 Transformer。* [https://doi.org/10.48550/arXiv.2101.11986](https://doi.org/10.48550/arXiv.2101.11986)'
- en: '→ GitHub code: [https://github.com/yitu-opensource/T2T-ViT](https://github.com/yitu-opensource/T2T-ViT)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '→ GitHub 代码: [https://github.com/yitu-opensource/T2T-ViT](https://github.com/yitu-opensource/T2T-ViT)'
- en: '[4] Luis Zuno ([@ansimuz](http://twitter.com/ansimuz)). *Mountain at Dusk Background.*
    License CC0: [https://opengameart.org/content/mountain-at-dusk-background](https://opengameart.org/content/mountain-at-dusk-background)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Luis Zuno ([@ansimuz](http://twitter.com/ansimuz)). *黄昏山脉背景图。* 许可 CC0:
    [https://opengameart.org/content/mountain-at-dusk-background](https://opengameart.org/content/mountain-at-dusk-background)'
- en: '[5] PyTorch. *Unfold.* [https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold](https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] PyTorch. *展开操作（Unfold）。* [https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold](https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold)'
- en: '[6] PyTorch. *Unsqueeze.* [https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] PyTorch. *扩展维度（Unsqueeze）。* [https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze)'
- en: '[7] PyTorch. *Non-linear Activation (weighted sum, nonlinearity).* [https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] PyTorch. *非线性激活（加权和，非线性）。* [https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)'
- en: '[8] PyTorch. *Normalization Layers*. [https://pytorch.org/docs/stable/nn.html#normalization-layers](https://pytorch.org/docs/stable/nn.html#normalization-layers)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] PyTorch. *归一化层（Normalization Layers）。* [https://pytorch.org/docs/stable/nn.html#normalization-layers](https://pytorch.org/docs/stable/nn.html#normalization-layers)'
- en: '[9] Ross Wightman. *PyTorch Image Models.* [https://github.com/huggingface/pytorch-image-models](https://github.com/huggingface/pytorch-image-models)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Ross Wightman. *PyTorch 图像模型。* [https://github.com/huggingface/pytorch-image-models](https://github.com/huggingface/pytorch-image-models)'
