- en: How Tiny Neural Networks Represent Basic Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-tiny-neural-networks-represent-basic-functions-8a24fce0e2d5?source=collection_archive---------4-----------------------#2024-09-10](https://towardsdatascience.com/how-tiny-neural-networks-represent-basic-functions-8a24fce0e2d5?source=collection_archive---------4-----------------------#2024-09-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A gentle introduction to mechanistic interpretability through simple algorithmic
    examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@taubenfeld9?source=post_page---byline--8a24fce0e2d5--------------------------------)[![Amir
    Taubenfeld](../Images/524631457f02f7193aeb2d4b03c5c3a4.png)](https://medium.com/@taubenfeld9?source=post_page---byline--8a24fce0e2d5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8a24fce0e2d5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8a24fce0e2d5--------------------------------)
    [Amir Taubenfeld](https://medium.com/@taubenfeld9?source=post_page---byline--8a24fce0e2d5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8a24fce0e2d5--------------------------------)
    ·9 min read·Sep 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b630b068a5309d3c370c60452a0eab18.png)'
  prefs: []
  type: TYPE_IMG
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article shows how small Artificial Neural Networks (NN) can represent basic
    functions. The goal is to provide fundamental intuition about how NNs work and
    to serve as a gentle introduction to [Mechanistic Interpretability](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)
    — a field that seeks to reverse engineer NNs.
  prefs: []
  type: TYPE_NORMAL
- en: I present three examples of elementary functions, describe each using a simple
    algorithm, and show how the algorithm can be “coded” into the weights of a neural
    network. Then, I explore if the network can learn the algorithm using backpropagation.
    I encourage readers to think about each example as a riddle and take a minute
    before reading the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning Topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article attempts to break NNs into discrete operations and describe them
    as algorithms. An alternative approach, perhaps more common and natural, is looking
    at the continuous topological interpretations of the linear transformations in
    different layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some great resources for strengthening your topological intuition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Tensorflow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.91521&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)
    — a simple tool for building basic intuition on classification tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ConvnetJS Demo](https://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html)
    — a more sophisticated tool for visualizing NNs for classification tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)
    — a great article for building topological intuition of how NNs work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three Elementary Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In all the following examples, I use the terminology “neuron” for a single
    node in the NN computation graph. Each neuron can be used only once (no cycles;
    e.g., not RNN), and it performs 3 operations in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: Inner product with the input vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding a bias term.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running a (non-linear) activation function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/e09ea44f923e70c557601eb3f9fec623.png)'
  prefs: []
  type: TYPE_IMG
- en: I provide only minimal code snippets so that reading will be fluent. This [Colab
    notebook](https://colab.research.google.com/drive/1zt9lVUH9jH2zx5nsFA_4Taq6Ic-ve09C?usp=sharing)
    includes the entire code.
  prefs: []
  type: TYPE_NORMAL
- en: The < operator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How many neurons are required to learn the function “x < 10”? Write an NN that
    returns 1 when the input is smaller than 10 and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by creating sample dataset that follows the pattern we want to learn
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ce6a710b63592e3c4a81d994798b4d0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating and visualizing the training data for “< operator”
  prefs: []
  type: TYPE_NORMAL
- en: This classification task can be solved using [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)
    and a [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) as the output
    activation. Using a single neuron, we can write the function as *Sigmoid(ax+b)*.
    *b*, the bias term, can be thought of as the neuron’s threshold. Intuitively,
    we can set *b = 10* and *a = -1* and get F=Sigmoid(10-x)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement and run F using PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6fa1dfe433dfc0d2ff2709f8d11ca509.png)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid(10-x)
  prefs: []
  type: TYPE_NORMAL
- en: Seems like the right pattern, but can we make a tighter approximation? For example,
    F(9.5) = 0.62, we prefer it to be closer to 1.
  prefs: []
  type: TYPE_NORMAL
- en: For the Sigmoid function, as the input approaches -∞ / ∞ the output approaches
    0 / 1 respectively. Therefore, we need to make our 10 — x function return large
    numbers, which can be done by multiplying it by a larger number, say 100, to get
    F=Sigmoid(100(10-x)), now we’ll get F(9.5) =~1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51a9a623419207f3eb6a22cbb29d49c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid(100(10-x))
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, when training a network with one neuron, it converges to F=Sigmoid(M(10-x)),
    where M is a scalar that keeps growing during training to make the approximation
    tighter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0dcb409478843393739eecd0584364c6.png)'
  prefs: []
  type: TYPE_IMG
- en: Tensorboard graph — the X-axis represents the number of training epochs and
    the Y-axis represents the value of the bias and the weight of the network. The
    bias and the weight increase/decrease in reverse proportion. That is, the network
    can be written as M(10-x) where M is a parameter that keeps growing during training.
  prefs: []
  type: TYPE_NORMAL
- en: To clarify, our single-neuron model is only an approximation of the “<10” function.
    We will never be able to reach a loss of zero, because the neuron is a continuous
    function while “<10” is not a continuous function.
  prefs: []
  type: TYPE_NORMAL
- en: Min(a, b)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Write a neural network that takes two numbers and returns the minimum between
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like before, let’s start by creating a test dataset and visualizing it
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/18366ae36e3e23b2e17677c371e9177b.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing the training data for Min(a, b). The two horizontal axes represent
    the coordinates of the input. The vertical axis labeled as “Ground Truth” is the
    expected output — i.e., the minimum of the two input coordinates
  prefs: []
  type: TYPE_NORMAL
- en: In this case, ReLU activation is a good candidate because it is essentially
    a maximum function (ReLU(x) = max(0, x)). Indeed, using ReLU one can write the
    min function as follows
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '***[Equation 1]***'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s build a small network that is capable of learning *Equation 1*, and
    try to train it using gradient descent
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/dc630403474bf9b5e81f7943d638e384.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualization of the MinModel computation graph. Drawing was done using the
    [Torchview](https://github.com/mert-kurttutan/torchview) library
  prefs: []
  type: TYPE_NORMAL
- en: Training for 300 epochs is enough to converge. Let’s look at the model’s parameters
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Many weights are zeroing out, and we are left with the nicely looking
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This is not the solution we expected, but it is a valid solution and even **cleaner
    than Equation 1!** By looking at the network we learned a new nicely looking formula!
    Proof:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If *a <= b: model([a,b]) = a — ReLU(a-b) = a — 0 = a*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If *a > b: a — ReLU(a-b) = a — (a-b) = b*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is even?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Create a neural network that takes an integer x as an input and returns x mod
    2\. That is, 0 if x is even, 1 if x is odd.
  prefs: []
  type: TYPE_NORMAL
- en: This one looks quite simple, but surprisingly it is impossible to create a finite-size
    network that correctly classifies each integer in (-∞, ∞) (using a standard non-periodic
    activation function such as ReLU).
  prefs: []
  type: TYPE_NORMAL
- en: '***Theorem: is_even needs at least log neurons***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*A network with ReLU activations requires at least n neurons to correctly classify
    each of 2^n consecutive natural numbers as even or odd (i.e., solving is_even).*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Proof: Using Induction***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Base: n == 2:** Intuitively, a single neuron (of the form *ReLU(ax + b)*),
    cannot solve *S = [i + 1, i + 2, i + 3, i + 4]* as it is not linearly separable.
    For example, without loss of generality, assume *a > 0* and *i + 2* is even*.*
    If *ReLU(a(i + 2) + b) = 0,* then also *ReLU(a(i + 1) + b) = 0* (monotonic function)*,*
    but *i + 1* is odd.'
  prefs: []
  type: TYPE_NORMAL
- en: More [details](https://en.wikipedia.org/wiki/Perceptrons_(book)#The_XOR_affair)
    are included in the classic Perceptrons book.
  prefs: []
  type: TYPE_NORMAL
- en: '**Assume for n, and look at n+1:** *Let S = [i + 1, …, i + 2^(n + 1)]*, and
    assume, for the sake of contradiction, that *S* can be solved using a network
    of size *n*. Take an input neuron from the first layer *f(x) = ReLU(ax + b)*,
    where *x* is the input to the network. *WLOG a > 0*. Based on the definition of
    ReLU there exists a *j* such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '*S’ = [i + 1, …, i + j], S’’ = [i + j + 1, …, i + 2^(n + 1)]'
  prefs: []
  type: TYPE_NORMAL
- en: f(x ≤ i) = 0
  prefs: []
  type: TYPE_NORMAL
- en: f(x ≥ i) = ax + b*
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two cases to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Case *|S’| ≥ 2^n*: dropping *f* and all its edges won’t change the classification
    results of the network on S’. Hence, there is a network of size *n-1* that solves
    S’. Contradiction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case *|S’’|≥ 2^n*: For each neuron *g* which takes *f* as an input *g(x) =*
    *ReLU(cf(x) + d + …) = ReLU(c ReLU(ax + b) + d + …)*, Drop the neuron *f* and
    wire *x* directly to *g*, to get *ReLU(cax + cb + d + …)*. A network of size *n
    — 1* solves *S’’*. Contradiction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logarithmic Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*How many neurons are sufficient to classify [1, 2^n]? I have proven that n
    neurons are necessary. Next, I will show that n neurons are also sufficient.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'One simple implementation is a network that constantly adds/subtracts 2, and
    checks if at some point it reaches 0\. This will require O(*2^n*) neurons. A more
    efficient algorithm is to add/subtract powers of 2, which will require only O(n)
    neurons. More formally:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f_i(x) := |x — i|'
  prefs: []
  type: TYPE_NORMAL
- en: f(x) := f_1∘ f_1∘ f_2 ∘ f_4∘ … ∘ f_(2^(n-1)) (|x|)*
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof:'
  prefs: []
  type: TYPE_NORMAL
- en: 'By definition:*∀ x ϵ[0, 2^i]: f_(2^(i-1)) (x) ≤ 2^(i-1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I.e., cuts the interval by half.*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Recursively *f_1∘ f_1∘ f_2 ∘ … ∘ f_(2^(n-1)) (|x|)* ≤ 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For every even *i: is_even(f_i(x)) = is_even(x)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly *is_even(f_1( f_1(x))) = is_even(x)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We got *f(x) ϵ {0,1}* and *is_even(x) =is_even(f(x))*. QED.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s try to implement this algorithm using a neural network over a small domain.
    We start again by defining the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/86b27d598900ae146704f82eb3407f1a.png)'
  prefs: []
  type: TYPE_IMG
- en: is_even data and labels on a small domain [0, 15]
  prefs: []
  type: TYPE_NORMAL
- en: Because the domain contains 2⁴ integers, we need to use 6 neurons. 5 for *f_1∘
    f_1∘ f_2 ∘ f_4∘ f_8,* + 1 output neuron. Let’s build the network and hardwire
    the weights
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As expected we can see that this model makes a perfect prediction on [0,15]
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/718cd603a22d3873bcdf0ec0e4fa2563.png)'
  prefs: []
  type: TYPE_IMG
- en: And, as expected, it doesn’t generalizes to new data points
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e790223ec60b4810cba422450a248582.png)'
  prefs: []
  type: TYPE_IMG
- en: We saw that we can hardwire the model, but would the model converge to the same
    solution using gradient descent?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd16479c84a320c2f4132e891dfb5f7d.png)'
  prefs: []
  type: TYPE_IMG
- en: The answer is — not so easily! Instead, it is stuck at a local minimum — predicting
    the mean.
  prefs: []
  type: TYPE_NORMAL
- en: This is a known phenomenon, where gradient descent can get stuck at a local
    minimum. It is especially prevalent for non-smooth error surfaces of highly nonlinear
    functions (such as is_even).
  prefs: []
  type: TYPE_NORMAL
- en: 'More details are beyond the scope of this article, but to get more intuition
    one can look at the many works that investigated the classic XOR problem. Even
    for such a simple problem, we can see that gradient descent can struggle to find
    a solution. In particular, I recommend Richard Bland’s short [book](https://www.cs.stir.ac.uk/~kjt/techreps/pdf/TR148.pdf)
    “Learning XOR: exploring the space of a classic problem” — a rigorous analysis
    of the error surface of the XOR problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Final Words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this article has helped you understand the basic structure of small neural
    networks. Analyzing Large Language Models is much more complex, but it’s an area
    of research that is advancing rapidly and is full of intriguing challenges.
  prefs: []
  type: TYPE_NORMAL
- en: When working with Large Language Models, it’s easy to focus on supplying data
    and computing power to achieve impressive results without understanding how they
    operate. However, interpretability offers crucial insights that can help address
    issues like fairness, inclusivity, and accuracy, which are becoming increasingly
    vital as we rely more on LLMs in decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: For further exploration, I recommend following the [AI Alignment Forum](https://www.alignmentforum.org/).
  prefs: []
  type: TYPE_NORMAL
- en: '*All the images were created by the author. The intro image was created using
    ChatGPT and the rest were created using Python libraries.'
  prefs: []
  type: TYPE_NORMAL
