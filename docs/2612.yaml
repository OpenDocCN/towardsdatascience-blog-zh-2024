- en: Understanding K-Fold Target Encoding to Handle High Cardinality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-k-fold-target-encoding-to-handle-high-cardinality-296387753e3f?source=collection_archive---------6-----------------------#2024-10-26](https://towardsdatascience.com/understanding-k-fold-target-encoding-to-handle-high-cardinality-296387753e3f?source=collection_archive---------6-----------------------#2024-10-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Balancing complexity and performance: An in-depth look at K-fold target encoding'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fhlpmah?source=post_page---byline--296387753e3f--------------------------------)[![Fhilipus
    Mahendra](../Images/84e51da9cca72313e627a8dd380ab40b.png)](https://medium.com/@fhlpmah?source=post_page---byline--296387753e3f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--296387753e3f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--296387753e3f--------------------------------)
    [Fhilipus Mahendra](https://medium.com/@fhlpmah?source=post_page---byline--296387753e3f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--296387753e3f--------------------------------)
    ·7 min read·Oct 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b01cfdfe1508b5c3c7adb5366bd3429f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mika Baumeister](https://unsplash.com/@kommumikation?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/white-printing-paper-with-numbers-Wpnoqo2plFA?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data science practitioners encounter numerous challenges when handling diverse
    data types across various projects, each demanding unique processing methods.
    A common obstacle is working with data formats that traditional machine learning
    models struggle to process effectively, resulting in subpar model performance.
    Since most machine learning algorithms are optimized for numerical data, transforming
    categorical data into numerical form is essential. However, this often oversimplifies
    complex categorical relationships, especially when the feature have high cardinality
    — meaning a large number of unique values — which complicates processing and impedes
    model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: High cardinality refers to the number of unique elements within a feature, specifically
    addressing the distinct count of categorical labels in a machine learning context.
    When a feature has many unique categorical labels, it has high cardinality, which
    can complicate model processing. To make categorical data usable in machine learning,
    these labels are often converted to numerical form using encoding methods based
    on data complexity. One popular method is One-Hot Encoding, which assigns each
    unique label a distinct binary vector. However, with high-cardinality data, One-Hot
    Encoding can dramatically increase dimensionality, leading to complex, high-dimensional
    datasets that require significant computational capacity for model training and
    potentially slow down performance.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a dataset with 2,000 unique IDs, each ID linked to one of only three
    countries. In this case, while the ID feature has a cardinality of 2,000 (since
    each ID is unique), the country feature has a cardinality of just 3\. Now, imagine
    a feature with 100,000 categorical labels that must be encoded using One-Hot Encoding.
    This would create an extremely high-dimensional dataset, leading to inefficiency
    and significant resource consumption.
  prefs: []
  type: TYPE_NORMAL
- en: A widely adopted solution among data scientists is **K-Fold Target Encoding**.
    This encoding method helps reduce feature cardinality by replacing categorical
    labels with target-mean values, based on K-Fold cross-validation. By focusing
    on individual data patterns, K-Fold Target Encoding lowers the risk of overfitting,
    helping the model learn specific relationships within the data rather than overly
    general patterns that can harm model performance.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-Fold Target Encoding involves dividing the dataset into several equally-sized
    subsets, known as “folds,” with “K” representing the number of these subsets.
    By folding the dataset into multiple groups, this method calculates the cross-subset
    weighted mean for each categorical label, enhancing the encoding’s robustness
    and reducing overfitting risks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c004e10d4c2c0c7e0ffcbbe240651a8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 1\. Indonesian Domestic Flights Dataset [1]
  prefs: []
  type: TYPE_NORMAL
- en: Using an example from **Fig 1**. of a sample dataset of Indonesian domestic
    flights emissions for each flight cycle, we can put this technique into practice.
    The base question to ask with this dataset is “What is the weighted mean for each
    categorical labels in ‘Airlines’ by looking at feature ‘HC Emission’ ?”. However,
    you might come with the same question people been asking me about. “But, if you
    just calculated them using the targeted feature, couldn’t it result as another
    high cardinality feature?”. The simple answer is “Yes, it could”.
  prefs: []
  type: TYPE_NORMAL
- en: Why?
  prefs: []
  type: TYPE_NORMAL
- en: In cases where a large dataset has a highly random target feature without identifiable
    patterns, K-Fold Target Encoding might produce a wide variety of mean values for
    each categorical label, potentially preserving high cardinality rather than reducing
    it. However, the primary goal of K-Fold Target Encoding is to address high cardinality,
    not necessarily to reduce it drastically. This method works best when there is
    a meaningful correlation between the target feature and segments of the data within
    each categorical label.
  prefs: []
  type: TYPE_NORMAL
- en: How does K-Fold Target Encoding operate? The simplest way to explain this is
    that, in each fold, you calculate the mean of the target feature from the other
    folds. This approach provides each categorical label with a unique weight, represented
    as a numerical value, making it more informative. Let’s look at an example calculation
    using our dataset for a clearer understanding.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eab2ff1f3d35d8a892639110151687f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 2\. Indonesian Domestic Flights Dataset After K-Fold Assigned [1]
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the weight of the ‘AirAsia’ label for the first observation, start
    by splitting the data into multiple folds, as shown in **Fig 2**. You can assign
    folds manually to ensure equal distribution, or automate this process using the
    following sample code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8336953e4307378b5f5c17677b7ecba4.png)![](../Images/6434adc4235962e6f83cd02c49fdd233.png)![](../Images/c7509a7f71f2fbcbb706e1bb0ff39ede.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 3\. Category-Specific Mean Calculation Process [1]
  prefs: []
  type: TYPE_NORMAL
- en: With your dataset now split into folds, the next step is to calculate the mean
    of the same label across other folds. For example, ‘AirAsia’ in Fold 1 would use
    the mean from Folds 2, 3, 4, 5, 6, and so on, resulting in a mean of 11.3\. This
    process continues across all folds, so Fold 2 would incorporate the mean from
    Folds 1, 3, 4, 5, 6, etc. The final results of these calculations are illustrated
    in **Fig 4.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3a3b50c783d067bc55689d27078d34f.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 4\. K-Fold Target Encoding (only category-specific mean) Results [1]
  prefs: []
  type: TYPE_NORMAL
- en: This calculation is known as the “category-specific mean,” which defines the
    average value for each categorical label based on similar label instances. Another
    essential calculation is the “global mean,” which defines the average intensity
    of your categorical label based on a user-defined global mean weight. The global
    mean serves as a baseline or “neutral” encoding, especially valuable for rare
    categories where the category-specific mean may rely on limited data points.
  prefs: []
  type: TYPE_NORMAL
- en: In K-Fold Target Encoding, both the category-specific and global means are typically
    combined to create a more robust and comprehensive representation. For a detailed
    illustration, refer to **Fig 5.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb730016418393663ac4f2e45b17e95c.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 5\. Mathematical Form of K-Fold Target Encoder. [1]
  prefs: []
  type: TYPE_NORMAL
- en: The mathematical formula makes it easier to understand how this calculation
    works. Here, *m* represents a user-defined weight, allowing control over the influence
    of the global mean in the final calculation. Now, we can apply this formula to
    the dataset from **Fig 2** and implement it using the code below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e94835c575331036b4660a6ad6f1b6f.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 6\. K-Fold Target Encoding Process Using Both Means. [1]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now with plotting the same formula into each of the categorical labels, the
    outcome would look like Fig 7.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/caa8f4225cb974b9f4c5cf1c46d4c84e.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 7\. K-Fold Target Encoding Final Result (Both Means). [1]
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to remember that this method can be risky if there is a significant
    difference between your training and test datasets. For example, if AirAsia consistently
    produces high volumes of HC emissions in your training data, but in your test
    data, Garuda has the highest HC emissions distributed evenly, the model may overfit
    to the training pattern, leading to lower accuracy on new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks for reading this article, hope you can get a better view of what is
    K-Fold Target Encoding and when to use it. Go check out my social media here and
    help me grow a better community for future data talents!!!:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linkedin: [https://www.linkedin.com/in/fhlpmah/](https://www.linkedin.com/in/fhlpmah/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dev.to: [https://dev.to/fhlpmah](https://dev.to/fhlpmah)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instagram: [https://www.instagram.com/fmasmoro/](https://www.instagram.com/fmasmoro/)'
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Image Made by The Author.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Datasets are Artificially Simulated by The Author. Inspired by: Organization
    ICA, 2023, *ICAO Aircraft Engine Emissions Databank* [https://www.easa.europa.eu/en/domains/environment/icao-aircraft-engine-emissions-databank](https://www.easa.europa.eu/en/domains/environment/icao-aircraft-engine-emissions-databank).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] All Codes is written by The Author.'
  prefs: []
  type: TYPE_NORMAL
