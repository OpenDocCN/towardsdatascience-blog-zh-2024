- en: A Practical Guide to Proximal Policy Optimization in JAX
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JAX ä¸­è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„å®ç”¨æŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/breaking-down-state-of-the-art-ppo-implementations-in-jax-6f102c06c149?source=collection_archive---------7-----------------------#2024-05-01](https://towardsdatascience.com/breaking-down-state-of-the-art-ppo-implementations-in-jax-6f102c06c149?source=collection_archive---------7-----------------------#2024-05-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/breaking-down-state-of-the-art-ppo-implementations-in-jax-6f102c06c149?source=collection_archive---------7-----------------------#2024-05-01](https://towardsdatascience.com/breaking-down-state-of-the-art-ppo-implementations-in-jax-6f102c06c149?source=collection_archive---------7-----------------------#2024-05-01)
- en: All the tricks and details you wish you knew about PPO
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ä½ å¸Œæœ›äº†è§£çš„å…³äº PPO çš„æŠ€å·§å’Œç»†èŠ‚
- en: '[](https://medium.com/@ryanpegoud?source=post_page---byline--6f102c06c149--------------------------------)[![Ryan
    PÃ©goud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page---byline--6f102c06c149--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6f102c06c149--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6f102c06c149--------------------------------)
    [Ryan PÃ©goud](https://medium.com/@ryanpegoud?source=post_page---byline--6f102c06c149--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryanpegoud?source=post_page---byline--6f102c06c149--------------------------------)[![Ryan
    PÃ©goud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page---byline--6f102c06c149--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6f102c06c149--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6f102c06c149--------------------------------)
    [Ryan PÃ©goud](https://medium.com/@ryanpegoud?source=post_page---byline--6f102c06c149--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6f102c06c149--------------------------------)
    Â·9 min readÂ·May 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6f102c06c149--------------------------------)
    Â·9 åˆ†é’Ÿé˜…è¯»Â·2024å¹´5æœˆ1æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9706450ef22f9e46c08b73bbee0735d6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9706450ef22f9e46c08b73bbee0735d6.png)'
- en: Photo by [Lorenzo Herrera](https://unsplash.com/@lorenzoherrera?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥è‡ª [Lorenzo Herrera](https://unsplash.com/@lorenzoherrera?utm_source=medium&utm_medium=referral)
    åœ¨ [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) ä¸Š
- en: 'Since its publication in a [2017 paper by OpenAI](https://arxiv.org/pdf/1707.06347.pdf),
    Proximal Policy Optimization (PPO) is widely regarded as one of the state-of-the-art
    algorithms in Reinforcement Learning. Indeed, PPO has demonstrated remarkable
    performances across various tasks, from [attaining superhuman performances in
    Dota 2](https://openai.com/research/openai-five) teams to solving a [Rubikâ€™s cube
    with a single robotic hand](https://openai.com/research/solving-rubiks-cube) while
    maintaining three main advantages: simplicity, stability, and sample efficiency.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªä» OpenAI åœ¨ [2017 å¹´çš„è®ºæ–‡](https://arxiv.org/pdf/1707.06347.pdf) ä¸­å‘å¸ƒä»¥æ¥ï¼Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰è¢«å¹¿æ³›è®¤ä¸ºæ˜¯å¼ºåŒ–å­¦ä¹ é¢†åŸŸæœ€å…ˆè¿›çš„ç®—æ³•ä¹‹ä¸€ã€‚å®é™…ä¸Šï¼ŒPPO
    åœ¨å„ç§ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä» [åœ¨ Dota 2 ä¸­å–å¾—è¶…äººç±»è¡¨ç°](https://openai.com/research/openai-five)
    çš„å›¢é˜Ÿåˆ°ä½¿ç”¨å•ä¸ªæœºå™¨äººæ‰‹è‡‚ [è§£ Rubikâ€™s Cube](https://openai.com/research/solving-rubiks-cube)ï¼ŒåŒæ—¶ä¿æŒä¸‰ä¸ªä¸»è¦ä¼˜åŠ¿ï¼šç®€æ´æ€§ã€ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚
- en: However, implementing RL algorithms from scratch is notoriously difficult and
    error-prone, given the numerous error sources and implementation details to be
    aware of.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä»é›¶å¼€å§‹å®ç°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•éå¸¸å›°éš¾ä¸”å®¹æ˜“å‡ºé”™ï¼Œå› ä¸ºæœ‰è®¸å¤šé”™è¯¯æºå’Œå®ç°ç»†èŠ‚éœ€è¦æ³¨æ„ã€‚
- en: In this article, weâ€™ll focus on breaking down the clever tricks and programming
    concepts used in a popular implementation of PPO in JAX. Specifically, weâ€™ll focus
    on the [implementation featured in the PureJaxRL library](https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py),
    developed by [Chris Lu](https://chrislu.page).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹åˆ†æåœ¨ JAX ä¸­æµè¡Œçš„ PPO å®ç°ä¸­ä½¿ç”¨çš„å·§å¦™æŠ€å·§å’Œç¼–ç¨‹æ¦‚å¿µã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†èšç„¦äº [PureJaxRL åº“ä¸­çš„å®ç°](https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py)ï¼Œè¯¥åº“ç”±
    [Chris Lu](https://chrislu.page) å¼€å‘ã€‚
- en: '*Disclaimer: Rather than diving too deep into theory, this article covers the
    practical implementation details and (numerous) tricks used in popular versions
    of PPO. Should you require any reminders about PPOâ€™s theory, please refer to the
    â€œ****references****â€ section at the end of this article. Additionally, all the
    code (minus the added comments) is copied directly from PureJaxRL for pedagogical
    purposes.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…è´£å£°æ˜ï¼šæœ¬æ–‡å¹¶æœªæ·±å…¥æ¢è®¨ç†è®ºï¼Œè€Œæ˜¯èšç„¦äºåœ¨PPOçš„æµè¡Œå®ç°ç‰ˆæœ¬ä¸­æ‰€ä½¿ç”¨çš„å®é™…å®ç°ç»†èŠ‚å’Œï¼ˆä¼—å¤šï¼‰æŠ€å·§ã€‚å¦‚æœä½ éœ€è¦å›é¡¾PPOçš„ç†è®ºï¼Œè¯·å‚è€ƒæœ¬æ–‡æœ«å°¾çš„â€œ****å‚è€ƒæ–‡çŒ®****â€éƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰ä»£ç ï¼ˆä¸åŒ…æ‹¬æ–°å¢çš„æ³¨é‡Šï¼‰å‡ç›´æ¥å¤åˆ¶è‡ªPureJaxRLï¼Œæ—¨åœ¨æ•™å­¦ç›®çš„ã€‚*'
- en: '[](https://github.com/luchris429/purejaxrl/tree/main?source=post_page-----6f102c06c149--------------------------------)
    [## UGitHub - luchris429/purejaxrl: Really Fast End-to-End Jax RL Implementations'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/luchris429/purejaxrl/tree/main?source=post_page-----6f102c06c149--------------------------------)
    [## UGitHub - luchris429/purejaxrl: è¶…å¿«çš„ç«¯åˆ°ç«¯ Jax å¼ºåŒ–å­¦ä¹ å®ç°'
- en: Really Fast End-to-End Jax RL Implementations. Contribute to luchris429/purejaxrl
    development by creating an account onâ€¦
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¶…å¿«çš„ç«¯åˆ°ç«¯ Jax å¼ºåŒ–å­¦ä¹ å®ç°ã€‚é€šè¿‡åˆ›å»ºä¸€ä¸ªè´¦æˆ·ï¼Œè´¡çŒ®ä»£ç è‡³luchris429/purejaxrlå¼€å‘é¡¹ç›®...
- en: github.com](https://github.com/luchris429/purejaxrl/tree/main?source=post_page-----6f102c06c149--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/luchris429/purejaxrl/tree/main?source=post_page-----6f102c06c149--------------------------------)
- en: '**Actor-Critic Architectures**'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**æ¼”å‘˜-è¯„è®ºå®¶æ¶æ„**'
- en: 'Proximal Policy Optimization is categorized within the policy gradient family
    of algorithms, a subset of which includes actor-critic methods. The designation
    â€˜actor-criticâ€™ reflects the dual components of the model:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆProximal Policy Optimizationï¼ŒPPOï¼‰è¢«å½’ç±»ä¸ºç­–ç•¥æ¢¯åº¦ç®—æ³•å®¶æ—ï¼Œå…¶ä¸­åŒ…æ‹¬æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•ã€‚â€˜æ¼”å‘˜-è¯„è®ºå®¶â€™è¿™ä¸€åç§°åæ˜ äº†æ¨¡å‹çš„åŒé‡ç»„ä»¶ï¼š
- en: The **actor network** creates a **distribution over actions** given the current
    state of the environment and returns an action sampled from this distribution.
    Here, the actor network comprises three dense layers separated by two activation
    layers (either ReLU or hyperbolic tangeant) and a final categorical layer applying
    the **softmax** function to the computed distribution.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¼”å‘˜ç½‘ç»œ**æ ¹æ®ç¯å¢ƒå½“å‰çŠ¶æ€ç”Ÿæˆä¸€ä¸ª**è¡ŒåŠ¨åˆ†å¸ƒ**ï¼Œå¹¶ä»è¯¥åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªè¡ŒåŠ¨ã€‚è¿™é‡Œï¼Œæ¼”å‘˜ç½‘ç»œåŒ…æ‹¬ä¸‰å±‚å…¨è¿æ¥å±‚ï¼Œè¿™äº›å±‚ä¹‹é—´ç”±ä¸¤å±‚æ¿€æ´»å±‚ï¼ˆå¯ä»¥æ˜¯ReLUæˆ–åŒæ›²æ­£åˆ‡ï¼‰éš”å¼€ï¼Œå¹¶ä¸”æœ€ç»ˆæœ‰ä¸€ä¸ªåº”ç”¨**softmax**å‡½æ•°çš„åˆ†ç±»å±‚æ¥è®¡ç®—åˆ†å¸ƒã€‚'
- en: The **critic network** **estimates the value function of the current state**,
    in other words, how good a particular action is at a given time. Its architecture
    is almost identical to the actor network, except for the final softmax layer.
    Indeed, the critic network doesnâ€™t apply any activation function to the final
    dense layer outputs as it performs a regression task.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¯„è®ºå®¶ç½‘ç»œ** **ä¼°ç®—å½“å‰çŠ¶æ€çš„ä»·å€¼å‡½æ•°**ï¼Œæ¢å¥è¯è¯´ï¼Œå®ƒè¯„ä¼°åœ¨æŸä¸€æ—¶åˆ»ç‰¹å®šè¡ŒåŠ¨çš„å¥½åã€‚å…¶æ¶æ„å‡ ä¹ä¸æ¼”å‘˜ç½‘ç»œç›¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯æœ€ç»ˆçš„softmaxå±‚ã€‚å®é™…ä¸Šï¼Œè¯„è®ºå®¶ç½‘ç»œåœ¨å¤„ç†å›å½’ä»»åŠ¡æ—¶ï¼Œå¹¶ä¸ä¼šå¯¹æœ€åä¸€å±‚å…¨è¿æ¥å±‚çš„è¾“å‡ºåº”ç”¨ä»»ä½•æ¿€æ´»å‡½æ•°ã€‚'
- en: '![](../Images/1e67cc67befceb9cb6015af67bab3bf3.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e67cc67befceb9cb6015af67bab3bf3.png)'
- en: Actor-critic architecture, as defined in PureJaxRL (illustration made by the
    author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ¼”å‘˜-è¯„è®ºå®¶æ¶æ„ï¼Œå¦‚PureJaxRLä¸­å®šä¹‰çš„ï¼ˆæ’å›¾ç”±ä½œè€…åˆ¶ä½œï¼‰
- en: Additionally, this implementation pays particular attention to **weight initialization**
    in dense layers. Indeed, all dense layers are initialized by **orthogonal matrices**
    with specific coefficients. This initialization strategy has been shown to **preserve
    the gradient norms** (i.e. scale) during forward passes and backpropagation, leading
    to **smoother convergence** and limiting the risks of vanishing or exploding gradients[1].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè¯¥å®ç°ç‰¹åˆ«æ³¨æ„**æƒé‡åˆå§‹åŒ–**åœ¨å…¨è¿æ¥å±‚ä¸­çš„åº”ç”¨ã€‚å®é™…ä¸Šï¼Œæ‰€æœ‰å…¨è¿æ¥å±‚éƒ½é€šè¿‡**æ­£äº¤çŸ©é˜µ**ä¸ç‰¹å®šç³»æ•°è¿›è¡Œåˆå§‹åŒ–ã€‚è¯¥åˆå§‹åŒ–ç­–ç•¥å·²è¢«è¯æ˜åœ¨å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­èƒ½å¤Ÿ**ä¿æŒæ¢¯åº¦èŒƒæ•°**ï¼ˆå³å°ºåº¦ï¼‰ï¼Œä»è€Œå®ç°**æ›´å¹³æ»‘çš„æ”¶æ•›**ï¼Œå¹¶é™åˆ¶æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸çš„é£é™©[1]ã€‚
- en: 'Orthogonal initialization is used in conjunction with specific scaling coefficients:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£äº¤åˆå§‹åŒ–ä¸ç‰¹å®šçš„ç¼©æ”¾ç³»æ•°ä¸€èµ·ä½¿ç”¨ï¼š
- en: '**Square root of 2**: Used for the first two dense layers of both networks,
    this factor aims to **compensate for the variance reduction** induced by ReLU
    activations (as inputs with negative values are set to 0). For the tanh activation,
    the Xavier initialization is a popular alternative[2].'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2çš„å¹³æ–¹æ ¹**ï¼šè¯¥å› å­ç”¨äºä¸¤ä¸ªç½‘ç»œçš„å‰ä¸¤å±‚å…¨è¿æ¥å±‚ï¼Œæ—¨åœ¨**è¡¥å¿ReLUæ¿€æ´»å¯¼è‡´çš„æ–¹å·®å‡å°‘**ï¼ˆå› ä¸ºè´Ÿå€¼è¾“å…¥ä¼šè¢«è®¾ä¸º0ï¼‰ã€‚å¯¹äºtanhæ¿€æ´»å‡½æ•°ï¼ŒXavieråˆå§‹åŒ–æ˜¯ä¸€ä¸ªæµè¡Œçš„æ›¿ä»£æ–¹æ¡ˆ[2]ã€‚'
- en: '**0.01:** Used in the last dense layer of the actor network, this factor helps
    to **minimize the initial differences in logit values** before applying the softmax
    function. This will reduce the difference in action probabilities and thus **encourage
    early exploration**.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**0.01ï¼š** ç”¨äºæ¼”å‘˜ç½‘ç»œçš„æœ€åä¸€å±‚å¯†é›†å±‚ï¼Œè¯¥å› å­æœ‰åŠ©äº**æœ€å°åŒ–logitå€¼çš„åˆå§‹å·®å¼‚**ï¼Œåœ¨åº”ç”¨softmaxå‡½æ•°ä¹‹å‰ã€‚è¿™å°†å‡å°‘åŠ¨ä½œæ¦‚ç‡çš„å·®å¼‚ï¼Œä»è€Œ**é¼“åŠ±æ—©æœŸæ¢ç´¢**ã€‚'
- en: '**1:** As the critic network is performing a regression task, we do not scale
    the initial weights.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1ï¼š** ç”±äºè¯„è®ºå‘˜ç½‘ç»œæ‰§è¡Œçš„æ˜¯å›å½’ä»»åŠ¡ï¼Œæˆ‘ä»¬ä¸ä¼šç¼©æ”¾åˆå§‹æƒé‡ã€‚'
- en: 'Actor critic network (source: PureJaxRL, Chris Lu)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ¼”å‘˜-è¯„è®ºå‘˜ç½‘ç»œï¼ˆæ¥æºï¼šPureJaxRL, Chris Luï¼‰
- en: Training Loop
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯
- en: 'The training loop is divided into 3 main blocks that share similar coding patterns,
    taking advantage of Jaxâ€™s functionalities:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯åˆ†ä¸º3ä¸ªä¸»è¦éƒ¨åˆ†ï¼Œè¿™äº›éƒ¨åˆ†å…±äº«ç±»ä¼¼çš„ç¼–ç æ¨¡å¼ï¼Œå……åˆ†åˆ©ç”¨JAXçš„åŠŸèƒ½ï¼š
- en: '**Trajectory collection:** First, weâ€™ll interact with the environment for a
    set number of steps and collect observations and rewards.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è½¨è¿¹æ”¶é›†ï¼š** é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä¸ç¯å¢ƒäº¤äº’è‹¥å¹²æ­¥éª¤ï¼Œå¹¶æ”¶é›†è§‚æµ‹å€¼å’Œå¥–åŠ±ã€‚'
- en: '**Generalized Advantage Estimation (GAE):** Then, weâ€™ll approximate the expected
    return for each trajectory by computing the generalized advantage estimation.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰ï¼š** ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡æ¥è¿‘ä¼¼æ¯ä¸ªè½¨è¿¹çš„æœŸæœ›å›æŠ¥ã€‚'
- en: '**Update step:** Finally, weâ€™ll compute the gradient of the loss and update
    the network parameters via gradient descent.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ›´æ–°æ­¥éª¤ï¼š** æœ€åï¼Œæˆ‘ä»¬å°†è®¡ç®—æŸå¤±çš„æ¢¯åº¦ï¼Œå¹¶é€šè¿‡æ¢¯åº¦ä¸‹é™æ›´æ–°ç½‘ç»œå‚æ•°ã€‚'
- en: 'Before going through each block in detail, hereâ€™s a quick reminder about the
    `jax.lax.scan`function that will show up multiple times throughout the code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯¦ç»†ä»‹ç»æ¯ä¸ªæ¨¡å—ä¹‹å‰ï¼Œè¿™é‡Œç®€è¦æé†’ä¸€ä¸‹`jax.lax.scan`å‡½æ•°ï¼Œå®ƒå°†åœ¨ä»£ç ä¸­å¤šæ¬¡å‡ºç°ï¼š
- en: Jax.lax.scan
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Jax.lax.scan
- en: A common programming pattern in JAX consists of defining a function that acts
    on a single sample and using `jax.lax.scan`to **iteratively apply it to elements
    of a sequence** or an array, while carrying along some state.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: JAXä¸­å¸¸è§çš„ç¼–ç¨‹æ¨¡å¼æ˜¯å®šä¹‰ä¸€ä¸ªä½œç”¨äºå•ä¸ªæ ·æœ¬çš„å‡½æ•°ï¼Œå¹¶ä½¿ç”¨`jax.lax.scan`æ¥**è¿­ä»£åœ°å°†å…¶åº”ç”¨äºåºåˆ—æˆ–æ•°ç»„çš„å…ƒç´ **ï¼ŒåŒæ—¶æºå¸¦æŸäº›çŠ¶æ€ã€‚
- en: For instance, weâ€™ll apply it to the `step` function to step our environment
    N consecutive times while carrying the new state of the environment through each
    iteration.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæˆ‘ä»¬å°†å…¶åº”ç”¨äº`step`å‡½æ•°ï¼Œä»¥åœ¨è¿ç»­Næ¬¡æ­¥éª¤ä¸­æ¨è¿›ç¯å¢ƒï¼ŒåŒæ—¶åœ¨æ¯æ¬¡è¿­ä»£ä¸­ä¼ é€’ç¯å¢ƒçš„æ–°çŠ¶æ€ã€‚
- en: 'In pure Python, we could proceed as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨çº¯Pythonä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼è¿›è¡Œï¼š
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'However, we avoid writing such loops in JAX for performance reasons (as pure
    Python loops are incompatible with JIT compilation). The alternative is `jax.lax.scan`which
    is equivalent to:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä¸ºäº†æ€§èƒ½è€ƒè™‘ï¼ˆå› ä¸ºçº¯Pythonå¾ªç¯ä¸JITç¼–è¯‘ä¸å…¼å®¹ï¼‰ï¼Œæˆ‘ä»¬é¿å…åœ¨JAXä¸­ç¼–å†™æ­¤ç±»å¾ªç¯ã€‚æ›¿ä»£æ–¹æ³•æ˜¯`jax.lax.scan`ï¼Œå…¶ç­‰æ•ˆäºï¼š
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using `jax.lax.scan` is more efficient than a Python loop because it allows
    the transformation to be optimized and executed as a single compiled operation
    rather than interpreting each loop iteration at runtime.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`jax.lax.scan`æ¯”ä½¿ç”¨Pythonå¾ªç¯æ›´é«˜æ•ˆï¼Œå› ä¸ºå®ƒå…è®¸å¯¹è½¬æ¢è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶ä½œä¸ºå•ä¸€çš„ç¼–è¯‘æ“ä½œæ‰§è¡Œï¼Œè€Œä¸æ˜¯åœ¨è¿è¡Œæ—¶è§£é‡Šæ¯ä¸ªå¾ªç¯è¿­ä»£ã€‚
- en: 'We can see that the `scan` function takes multiple arguments:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ`scan`å‡½æ•°æ¥å—å¤šä¸ªå‚æ•°ï¼š
- en: '**f:** A function that is applied at each step. It takes the current state
    and an element of `xs` (or a placeholder if `xs` is `None`) and returns the updated
    state and an output.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**fï¼š** åœ¨æ¯ä¸ªæ­¥éª¤åº”ç”¨çš„å‡½æ•°ã€‚å®ƒæ¥å—å½“å‰çŠ¶æ€å’Œ`xs`çš„ä¸€ä¸ªå…ƒç´ ï¼ˆå¦‚æœ`xs`ä¸º`None`ï¼Œåˆ™ä½¿ç”¨å ä½ç¬¦ï¼‰ï¼Œå¹¶è¿”å›æ›´æ–°åçš„çŠ¶æ€å’Œè¾“å‡ºã€‚'
- en: '**init:** The initial state that `f` will use in its first invocation.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**initï¼š** `f`åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ä½¿ç”¨çš„åˆå§‹çŠ¶æ€ã€‚'
- en: '**xs:** A sequence of inputs that are iteratively processed by `f`. If `xs`
    is `None`, the function simulates a loop with `length` iterations using `None`
    as the input for each iteration.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**xsï¼š** ä¸€ä¸ªè¾“å…¥åºåˆ—ï¼Œå®ƒå°†è¢«`f`é€æ­¥å¤„ç†ã€‚å¦‚æœ`xs`ä¸º`None`ï¼Œåˆ™è¯¥å‡½æ•°æ¨¡æ‹Ÿä¸€ä¸ªå…·æœ‰`length`æ¬¡è¿­ä»£çš„å¾ªç¯ï¼Œå¹¶å°†æ¯æ¬¡è¿­ä»£çš„è¾“å…¥è®¾ç½®ä¸º`None`ã€‚'
- en: '**length:** Specifies the number of iterations if `xs` is `None`, ensuring
    that the function can still operate without explicit inputs.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**lengthï¼š** å¦‚æœ`xs`ä¸º`None`ï¼ŒæŒ‡å®šè¿­ä»£çš„æ¬¡æ•°ï¼Œç¡®ä¿å‡½æ•°åœ¨æ²¡æœ‰æ˜ç¡®è¾“å…¥çš„æƒ…å†µä¸‹ä»èƒ½æ“ä½œã€‚'
- en: 'Additionally, `scan` returns:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œ`scan`è¿”å›ï¼š
- en: '**carry:** The final state after all iterations.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**carryï¼š** æ‰€æœ‰è¿­ä»£åçš„æœ€ç»ˆçŠ¶æ€ã€‚'
- en: '**ys:** An array of outputs corresponding to each stepâ€™s application of `f`,
    stacked for easy analysis or further processing.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ysï¼š** å¯¹åº”äºæ¯ä¸ªæ­¥éª¤åº”ç”¨`f`çš„è¾“å‡ºæ•°ç»„ï¼Œå †å ä»¥ä¾¿äºåˆ†ææˆ–è¿›ä¸€æ­¥å¤„ç†ã€‚'
- en: Finally, `scan` can be used in combination with `vmap` to scan a function over
    multiple dimensions in parallel. As weâ€™ll see in the next section, this allows
    us to interact with several environments in parallel to collect trajectories rapidly.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œ`scan`å¯ä»¥ä¸`vmap`ç»“åˆä½¿ç”¨ï¼Œåœ¨å¤šä¸ªç»´åº¦ä¸Šå¹¶è¡Œæ‰«æä¸€ä¸ªå‡½æ•°ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ä¸‹ä¸€èŠ‚ä¸­å°†çœ‹åˆ°çš„ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿå¹¶è¡Œä¸å¤šä¸ªç¯å¢ƒäº¤äº’ï¼Œä»è€Œå¿«é€Ÿæ”¶é›†è½¨è¿¹ã€‚
- en: '![](../Images/123e5e98f340935db435e9aab42629a7.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/123e5e98f340935db435e9aab42629a7.png)'
- en: Illustration of vmap, scan, and scan + vmap in the context of the step function
    (made by the author)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¥é•¿å‡½æ•°ä¸Šä¸‹æ–‡ä¸­ï¼Œå±•ç¤ºäº†vmapã€scanå’Œscan + vmapçš„ç¤ºæ„å›¾ï¼ˆä½œè€…åˆ¶ä½œï¼‰
- en: 1\. Trajectory Collection
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. è½¨è¿¹æ”¶é›†
- en: 'As mentioned in the previous section, the trajectory collection block consists
    of a `step` function scanned across N iterations. This `step` function successively:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰ä¸€èŠ‚æ‰€è¿°ï¼Œè½¨è¿¹æ”¶é›†å—ç”±`step`å‡½æ•°è·¨Næ¬¡è¿­ä»£æ‰«æç»„æˆã€‚è¿™ä¸ª`step`å‡½æ•°ä¾æ¬¡æ‰§è¡Œï¼š
- en: Selects an action using the actor network
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¼”å‘˜ç½‘ç»œé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
- en: Steps the environment
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰§è¡Œç¯å¢ƒæ­¥é•¿
- en: Stores transition data in a `transition` tuple
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†è½¬ç§»æ•°æ®å­˜å‚¨åœ¨`transition`å…ƒç»„ä¸­
- en: Stores the model parameters, the environment state, the current observation,
    and rng keys in a `runner_state` tuple
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹å‚æ•°ã€ç¯å¢ƒçŠ¶æ€ã€å½“å‰è§‚å¯Ÿå€¼å’Œéšæœºæ•°ç”Ÿæˆå™¨é”®å­˜å‚¨åœ¨`runner_state`å…ƒç»„ä¸­
- en: Returns `runner_state` and `transition`
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿”å›`runner_state`å’Œ`transition`
- en: Scanning this function returns the latest `runner_state` and `traj_batch`, an
    array of `transition` tuples. In practice, transitions are collected from multiple
    environments in parallel for efficiency as indicated by the use of `jax.vmap(env.step,
    â€¦)`(for more details about vectorized environments and `vmap`, refer to my [previous
    article](https://medium.com/towards-data-science/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰«æè¿™ä¸ªå‡½æ•°å°†è¿”å›æœ€æ–°çš„`runner_state`å’Œ`traj_batch`ï¼Œåè€…æ˜¯ä¸€ä¸ªåŒ…å«`transition`å…ƒç»„çš„æ•°ç»„ã€‚å®é™…ä¸Šï¼Œä¸ºäº†æé«˜æ•ˆç‡ï¼Œè½¬ç§»æ•°æ®æ˜¯ä»å¤šä¸ªç¯å¢ƒä¸­å¹¶è¡Œæ”¶é›†çš„ï¼Œå¦‚ä½¿ç”¨`jax.vmap(env.step,
    â€¦)`æ‰€ç¤ºï¼ˆæœ‰å…³å‘é‡åŒ–ç¯å¢ƒå’Œ`vmap`çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æˆ‘çš„[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5)ï¼‰ã€‚
- en: 'env step function (source: PureJaxRL, Chris Lu)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: envæ­¥é•¿å‡½æ•°ï¼ˆæ¥æºï¼šPureJaxRLï¼ŒChris Luï¼‰
- en: 2\. Generalized Advantage Estimation
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡
- en: 'After collecting trajectories, we need to compute the **advantage function,**
    a crucial component of PPOâ€™s loss function. The advantage function measures how
    much better a specific action is compared to the average action in a given state:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¶é›†å®Œè½¨è¿¹åï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—**ä¼˜åŠ¿å‡½æ•°**ï¼Œè¿™æ˜¯PPOæŸå¤±å‡½æ•°çš„ä¸€ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ã€‚ä¼˜åŠ¿å‡½æ•°è¡¡é‡ä¸€ä¸ªç‰¹å®šåŠ¨ä½œåœ¨ç»™å®šçŠ¶æ€ä¸‹ï¼Œç›¸è¾ƒäºå¹³å‡åŠ¨ä½œçš„è¡¨ç°å¦‚ä½•ï¼š
- en: '![](../Images/9025657609723998de22ebbc42479942.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9025657609723998de22ebbc42479942.png)'
- en: Where **Gt** is the return at time ***t***and **V(St) is** the value of state
    ***s***at time ***t****.*
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­**Gt**æ˜¯æ—¶é—´***t***æ—¶åˆ»çš„å›æŠ¥ï¼Œ**V(St)**æ˜¯æ—¶é—´***t***æ—¶åˆ»çŠ¶æ€***s***çš„å€¼ã€‚
- en: 'As the return is generally unknown, we have to approximate the advantage function.
    A popular solution is **generalized advantage estimation**[3], defined as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå›æŠ¥é€šå¸¸æ˜¯æœªçŸ¥çš„ï¼Œæˆ‘ä»¬å¿…é¡»å¯¹ä¼˜åŠ¿å‡½æ•°è¿›è¡Œè¿‘ä¼¼ã€‚ä¸€ä¸ªæµè¡Œçš„è§£å†³æ–¹æ¡ˆæ˜¯**å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡**[3]ï¼Œå…¶å®šä¹‰å¦‚ä¸‹ï¼š
- en: '![](../Images/0ccf1cf4804dffef16e94feee6c2eb2a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ccf1cf4804dffef16e94feee6c2eb2a.png)'
- en: 'With **Î³** the discount factor, **Î»** a parameter that controls the trade-off
    between bias and variance in the estimate, and ***Î´t***the temporal difference
    error at time ***t***:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­**Î³**æ˜¯æŠ˜æ‰£å› å­ï¼Œ**Î»**æ˜¯æ§åˆ¶åå·®ä¸æ–¹å·®æƒè¡¡çš„å‚æ•°ï¼Œ***Î´t***æ˜¯æ—¶é—´***t***æ—¶åˆ»çš„æ—¶åºå·®å¼‚è¯¯å·®ï¼š
- en: '![](../Images/298fa9ce55afb829287e91b9688fa335.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/298fa9ce55afb829287e91b9688fa335.png)'
- en: 'As we can see, the value of the GAE at time *t* depends on the GAE at future
    timesteps. Therefore, we compute it backward, starting from the end of a trajectory.
    For example, for a trajectory of 3 transitions, we would have:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼ŒGAEåœ¨æ—¶é—´*t*çš„å€¼ä¾èµ–äºæœªæ¥æ—¶é—´æ­¥çš„GAEã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»è½¨è¿¹çš„æœ«ç«¯å¼€å§‹åå‘è®¡ç®—ã€‚ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ªåŒ…å«3ä¸ªè½¬ç§»çš„è½¨è¿¹ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ï¼š
- en: '![](../Images/cbc3f0a15854dd74ba9a902e27bd939d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbc3f0a15854dd74ba9a902e27bd939d.png)'
- en: 'Which is equivalent to the following recursive form:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç­‰ä»·äºä»¥ä¸‹é€’å½’å½¢å¼ï¼š
- en: '![](../Images/8fbb9ca9d51bbddd821b3dc3747ee3bb.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fbb9ca9d51bbddd821b3dc3747ee3bb.png)'
- en: Once again, we use `jax.lax.scan` on the trajectory batch (this time in reverse
    order) to iteratively compute the GAE.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼Œæˆ‘ä»¬å¯¹è½¨è¿¹æ‰¹æ¬¡ä½¿ç”¨`jax.lax.scan`ï¼ˆè¿™æ¬¡æ˜¯é€†åºï¼‰æ¥è¿­ä»£è®¡ç®—GAEã€‚
- en: 'generalized advantage estimation (source: PureJaxRL, Chris Lu)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆæ¥æºï¼šPureJaxRLï¼ŒChris Luï¼‰
- en: Note that the function returns `advantages + traj_batch.value` as a second output,
    which is equivalent to the return according to the first equation of this section.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œè¯¥å‡½æ•°è¿”å›`advantages + traj_batch.value`ä½œä¸ºç¬¬äºŒä¸ªè¾“å‡ºï¼Œè¿™ç›¸å½“äºæœ¬èŠ‚ç¬¬ä¸€ä¸ªæ–¹ç¨‹ä¸­çš„å›æŠ¥ã€‚
- en: 3\. Update step
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. æ›´æ–°æ­¥éª¤
- en: 'The final block of the training loop defines the loss function, computes its
    gradient, and performs gradient descent on minibatches. Similarly to previous
    sections, the update step is an arrangement of several functions in a hierarchical
    order:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯çš„æœ€åä¸€éƒ¨åˆ†å®šä¹‰äº†æŸå¤±å‡½æ•°ï¼Œè®¡ç®—å…¶æ¢¯åº¦ï¼Œå¹¶åœ¨ minibatches ä¸Šæ‰§è¡Œæ¢¯åº¦ä¸‹é™ã€‚ä¸å‰é¢å‡ èŠ‚ç±»ä¼¼ï¼Œæ›´æ–°æ­¥éª¤æ˜¯æŒ‰å±‚æ¬¡é¡ºåºæ’åˆ—çš„å¤šä¸ªå‡½æ•°çš„ç»„åˆï¼š
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Letâ€™s break them down one by one, starting from the innermost function of the
    update step.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€ä¸€æ‹†è§£å®ƒä»¬ï¼Œä»æ›´æ–°æ­¥éª¤ä¸­æœ€å†…å±‚çš„å‡½æ•°å¼€å§‹ã€‚
- en: 3.1 Loss function
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 æŸå¤±å‡½æ•°
- en: 'This function aims to define and compute the PPO loss, originally defined as:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå‡½æ•°æ—¨åœ¨å®šä¹‰å’Œè®¡ç®— PPO æŸå¤±ï¼Œæœ€åˆå®šä¹‰ä¸ºï¼š
- en: '![](../Images/c0b4c98a735ce19abbf8e5090017b346.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0b4c98a735ce19abbf8e5090017b346.png)'
- en: 'Where:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼š
- en: '![](../Images/11464fd7edf1eeda50b976b7ab409280.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11464fd7edf1eeda50b976b7ab409280.png)'
- en: 'However, the PureJaxRL implementation features some tricks and differences
    compared to the original PPO paper[4]:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼ŒPureJaxRL çš„å®ç°ä¸åŸå§‹ PPO è®ºæ–‡[4]ç›¸æ¯”ï¼Œå…·æœ‰ä¸€äº›æŠ€å·§å’Œå·®å¼‚ï¼š
- en: The paper defines the PPO loss in the context of gradient ascent whereas the
    implementation performs gradient descent. Therefore, the sign of each loss component
    is reversed.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®ºæ–‡ä¸­å®šä¹‰çš„ PPO æŸå¤±æ˜¯åŸºäºæ¢¯åº¦ä¸Šå‡çš„ï¼Œè€Œå®é™…å®ç°é‡‡ç”¨äº†æ¢¯åº¦ä¸‹é™ã€‚å› æ­¤ï¼Œæ¯ä¸ªæŸå¤±ç»„ä»¶çš„ç¬¦å·è¢«åè½¬ã€‚
- en: 'The value function term is modified to include an additional clipped term.
    This could be seen as a way to make the value function updates more conservative
    (as for the clipped surrogate objective):'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å€¼å‡½æ•°é¡¹è¢«ä¿®æ”¹ï¼ŒåŒ…å«äº†ä¸€ä¸ªé¢å¤–çš„æˆªæ–­é¡¹ã€‚è¿™å¯ä»¥çœ‹ä½œæ˜¯ä½¿å€¼å‡½æ•°æ›´æ–°æ›´ä¸ºä¿å®ˆçš„ä¸€ç§æ–¹å¼ï¼ˆå°±åƒæˆªæ–­çš„æ›¿ä»£ç›®æ ‡ä¸€æ ·ï¼‰ï¼š
- en: '![](../Images/add641cf22538f261da9af6a70c8c46e.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/add641cf22538f261da9af6a70c8c46e.png)'
- en: The GAE is standardized.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAE è¢«æ ‡å‡†åŒ–ã€‚
- en: 'Hereâ€™s the complete loss function:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å®Œæ•´çš„æŸå¤±å‡½æ•°ï¼š
- en: 'PPO loss function (source: PureJaxRL, Chris Lu)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: PPO æŸå¤±å‡½æ•°ï¼ˆæ¥æºï¼šPureJaxRL, Chris Luï¼‰
- en: 3.2 Update Minibatch
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 æ›´æ–° Minibatch
- en: The `update_minibatch` function is essentially a wrapper around `loss_fn` used
    to compute its gradient over the trajectory batch and update the model parameters
    stored in `train_state`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`update_minibatch` å‡½æ•°æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŒ…è£…å™¨ï¼Œç”¨äºè®¡ç®—åœ¨è½¨è¿¹æ‰¹æ¬¡ä¸Šå¯¹ `loss_fn` çš„æ¢¯åº¦ï¼Œå¹¶æ›´æ–°å­˜å‚¨åœ¨ `train_state`
    ä¸­çš„æ¨¡å‹å‚æ•°ã€‚'
- en: 'update minibatch (source: PureJaxRL, Chris Lu)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ–° minibatchï¼ˆæ¥æºï¼šPureJaxRL, Chris Luï¼‰
- en: 3.3 Update Epoch
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 æ›´æ–° Epoch
- en: Finally, `update_epoch` wraps `update_minibatch` and applies it on minibatches.
    Once again, `jax.lax.scan` is used to apply the update function on all minibatches
    iteratively.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œ`update_epoch` å°è£…äº† `update_minibatch` å¹¶åœ¨ minibatch ä¸Šåº”ç”¨å®ƒã€‚åŒæ ·ï¼Œ`jax.lax.scan`
    è¢«ç”¨æ¥è¿­ä»£åœ°å¯¹æ‰€æœ‰ minibatch åº”ç”¨æ›´æ–°å‡½æ•°ã€‚
- en: 'update epoch (source: PureJaxRL, Chris Lu)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ–° epochï¼ˆæ¥æºï¼šPureJaxRL, Chris Luï¼‰
- en: Conclusion
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: From there, we can wrap all of the previous functions in an `update_step` function
    and use `scan` one last time for N steps to complete the training loop.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œå¼€å§‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰å…ˆå‰çš„å‡½æ•°å°è£…åœ¨ä¸€ä¸ª `update_step` å‡½æ•°ä¸­ï¼Œå¹¶ä½¿ç”¨ `scan` æœ€åä¸€æ¬¡è¿­ä»£ N æ­¥ä»¥å®Œæˆè®­ç»ƒå¾ªç¯ã€‚
- en: 'A global view of the training loop would look like this:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯çš„å…¨å±€è§†å›¾å¤§è‡´å¦‚ä¸‹ï¼š
- en: 'Summary of the training script (source: PureJaxRL, Chris Lu)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒè„šæœ¬æ€»ç»“ï¼ˆæ¥æºï¼šPureJaxRL, Chris Luï¼‰
- en: We can now run a fully compiled training loop using `jax.jit(train(rng))` or
    even train multiple agents in parallel using `jax.vmap(train(rng))`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨ `jax.jit(train(rng))` è¿è¡Œä¸€ä¸ªå®Œå…¨ç¼–è¯‘çš„è®­ç»ƒå¾ªç¯ï¼Œæˆ–è€…ä½¿ç”¨ `jax.vmap(train(rng))` å¹¶è¡Œè®­ç»ƒå¤šä¸ªä»£ç†ã€‚
- en: There we have it! We covered the essential building blocks of the PPO training
    loop as well as common programming patterns in JAX.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°æ­¤ä¸ºæ­¢ï¼æˆ‘ä»¬æ¶µç›–äº† PPO è®­ç»ƒå¾ªç¯çš„åŸºæœ¬æ„å»ºå—ä»¥åŠ JAX ä¸­å¸¸è§çš„ç¼–ç¨‹æ¨¡å¼ã€‚
- en: To go further, I highly recommend reading the [full training script](https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py)
    in detail and running example notebooks on the PureJaxRL repository.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è‹¥æƒ³æ·±å…¥äº†è§£ï¼Œå¼ºçƒˆå»ºè®®è¯¦ç»†é˜…è¯» [å®Œæ•´è®­ç»ƒè„šæœ¬](https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py)ï¼Œå¹¶åœ¨
    PureJaxRL ä»“åº“ä¸Šè¿è¡Œç¤ºä¾‹ç¬”è®°æœ¬ã€‚
- en: '[](https://github.com/luchris429/purejaxrl?source=post_page-----6f102c06c149--------------------------------)
    [## GitHub - luchris429/purejaxrl: Really Fast End-to-End Jax RL Implementations'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/luchris429/purejaxrl?source=post_page-----6f102c06c149--------------------------------)
    [## GitHub - luchris429/purejaxrl: çœŸæ­£å¿«é€Ÿçš„ç«¯åˆ°ç«¯ Jax å¼ºåŒ–å­¦ä¹ å®ç°'
- en: Really Fast End-to-End Jax RL Implementations. Contribute to luchris429/purejaxrl
    development by creating an account onâ€¦
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: çœŸæ­£å¿«é€Ÿçš„ç«¯åˆ°ç«¯ Jax å¼ºåŒ–å­¦ä¹ å®ç°ã€‚é€šè¿‡åˆ›å»ºä¸€ä¸ªè´¦æˆ·æ¥è´¡çŒ®äº luchris429/purejaxrl çš„å¼€å‘â€¦
- en: github.com](https://github.com/luchris429/purejaxrl?source=post_page-----6f102c06c149--------------------------------)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/luchris429/purejaxrl?source=post_page-----6f102c06c149--------------------------------)
- en: Thank you very much for your support, until next time ğŸ‘‹
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: éå¸¸æ„Ÿè°¢æ‚¨çš„æ”¯æŒï¼ŒæœŸå¾…ä¸‹æ¬¡å†è§ ğŸ‘‹
- en: 'References:'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®ï¼š
- en: '[Full training script](https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py),
    PureJaxRL, Chris Lu, 2023'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[å®Œæ•´è®­ç»ƒè„šæœ¬](https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py)ï¼ŒPureJaxRLï¼ŒChris
    Luï¼Œ2023'
- en: '[1] [***Explaining and illustrating orthogonal initialization for recurrent
    neural networks***](https://smerity.com/articles/2016/orthogonal_init.html), Smerity,
    2016'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [***è§£é‡Šå’Œè¯´æ˜é€’å½’ç¥ç»ç½‘ç»œçš„æ­£äº¤åˆå§‹åŒ–***](https://smerity.com/articles/2016/orthogonal_init.html)ï¼ŒSmerityï¼Œ2016'
- en: '[2] [***Initializing neural networks***](https://www.deeplearning.ai/ai-notes/initialization/index.html),
    DeepLearning.ai'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [***åˆå§‹åŒ–ç¥ç»ç½‘ç»œ***](https://www.deeplearning.ai/ai-notes/initialization/index.html)ï¼ŒDeepLearning.ai'
- en: '[3] [***Generalized Advantage Estimation in Reinforcement Learning***](/generalized-advantage-estimation-in-reinforcement-learning-bf4a957f7975),
    Siwei Causevic, Towards Data Science, 2023'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [***å¼ºåŒ–å­¦ä¹ ä¸­çš„å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡***](/generalized-advantage-estimation-in-reinforcement-learning-bf4a957f7975)ï¼ŒSiwei
    Causevicï¼ŒTowards Data Scienceï¼Œ2023'
- en: '[4] [***Proximal Policy Optimization Algorithms***](https://arxiv.org/pdf/1707.06347),
    Schulman et Al., OpenAI, 2017'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [***è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•***](https://arxiv.org/pdf/1707.06347)ï¼ŒSchulman ç­‰ï¼ŒOpenAIï¼Œ2017'
