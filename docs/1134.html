<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Sparsifying Knowledge-Graph Using Target Information</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Sparsifying Knowledge-Graph Using Target Information</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sparsifying-knowledge-graph-using-target-information-8fb1014cbb0f?source=collection_archive---------4-----------------------#2024-05-05">https://towardsdatascience.com/sparsifying-knowledge-graph-using-target-information-8fb1014cbb0f?source=collection_archive---------4-----------------------#2024-05-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="0c1a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Sparsifying knowledge graphs for supervised tasks, using PMI to remove irrelevant edges; with concrete example using medical data</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sria.louis?source=post_page---byline--8fb1014cbb0f--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sria Louis" class="l ep by dd de cx" src="../Images/d65b17e9d4ace7e0222118abc70f3954.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*E7QMAOyM0WcVeQSPJ87KGw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--8fb1014cbb0f--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@sria.louis?source=post_page---byline--8fb1014cbb0f--------------------------------" rel="noopener follow">Sria Louis</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--8fb1014cbb0f--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/a430bc1553d17b8fb14bf3bc2602599b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6hhefPsqd5qx0U7W8bNjmw.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">“Medical Knowledge Graph” — generated at deepai.org</figcaption></figure><p id="bf4d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The following is a simplified snippet from a research project I conducted while working at Microsoft Research, under the guidance of Professor Elad Yom-Tov.</p><p id="5f52" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Prerequisites</strong>: Familiarity with supervised machine learning and a basic grasp of Knowledge Graphs, including their utilization in Feature Engineering.</p><h1 id="95e4" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">TL;DR</h1><p id="bd4e" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">You have a Supervised ML task, predicting a target variable <em class="oy">y</em> from high-dimension binary feature-space, and you want to utilise a knowledge graph where each node represents one feature.</p><p id="e56f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The problem is that the knowledge graph is dense. Specifically, there are many irrelevant edges (anecdote: <a class="af oz" href="https://en.wikipedia.org/wiki/Wikipedia:Overlink_crisis" rel="noopener ugc nofollow" target="_blank">Wikipedia overlink crisis</a>).</p><p id="2ac0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">The trick</strong>: Assess the relevance of each edge by measuring Pointwise Mutual Information (PMI) between its occurrence and the target variable. Eliminate edges with low relevance. Voilà!</p><p id="a6f2" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now, let’s move into the details.</p><p id="743a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">First off, in the first chapter we will explore the need of graph sparsification. In Chapter 2 we’ll make it concrete using an example from the medical domain. Then, in the final chapter, we’ll delve into the core topic: graph sparsification using PMI. Enjoy!</p></div></div></div><div class="ab cb pa pb pc pd" role="separator"><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2621" class="nx ny fq bf nz oa pi gq oc od pj gt of og pk oi oj ok pl om on oo pm oq or os bk">First Chapter: The Need</h1><p id="0e49" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">Graphs and specifically Knowledge Graphs (KG) are ubiquitous. The technology is ripening with the advances in both Graph DB infrastructure and with the theoretical ML tools (e.g. Node Embeddings).</p><p id="3989" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">A critical challenge when working on Knowledge Graph in the real world is — and in my humble opinion forever will be — <a class="af oz" href="https://en.wikipedia.org/wiki/Dense_graph" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr">density</strong></a><strong class="nd fr">. </strong>Even algorithms with polynomial complexity might fail on dense graphs. Looking into the future, if the hardware will advance - the future Knowledge Graphs will increase both in number of nodes and in number of edges. [Moreover, I predict and hope that in this arms race we will gradationally see hierarchical hypergraphs and other monstrosities joining the game.]</p><p id="7e13" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The <strong class="nd fr">strength</strong> of KGs is the fact that they hold complex connections in large domain of knowledge, e.g. all the medical knowledge or all the known astronomical objects, and therefore you can use them to enrich simpler features, for instance, if your features are very sparse, e.g., very rare medical conditions, they can be enriched by “neighboring” pieces of information from the graph.</p><p id="2908" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The enormity comes with a drawback: Knowledge Graphs are often too large. In addition to computational complexity, there is another essential issue here. KGs cover everything in the domain and therefore most of the information is irrelevant to your specific target. In other words, they have over sensitivity (aka coverage/recall) — and therefore low specificity (aka precision). You might say that this is a common tradeoff in ML; what’s special in graphs? The answer is that in typical real-world KNs the degree distribution is heavy-tailed with huge “hubs”, i.e., nodes with extremely high-degree, namely, nodes with many edges. You might be familiar with the effect of such hubs in <a class="af oz" href="https://en.wikipedia.org/wiki/Six_degrees_of_separation" rel="noopener ugc nofollow" target="_blank">social-networks and physics</a> (“six degrees of separation”) and in Graph Theory perspective (diameter of random graphs) — in our context this means that a small number of such hubs drastically reduces the distance between most nodes in the graph and therefore takes any signal in the graph and rapidly diffuse it all over the graph, muting any subtler piece of information that might be relevant to your specific objective.</p><p id="4b84" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To be concrete, in the next chapter, we will describe the a use-case we worked on and then, in chapter 3, we will describe a method overcoming the density problem for supervised tasks by removing irrelevant edges. Feel free to jump to the 3rd chapter.</p></div></div></div><div class="ab cb pa pb pc pd" role="separator"><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="a158" class="nx ny fq bf nz oa pi gq oc od pj gt of og pk oi oj ok pl om on oo pm oq or os bk">Second Chapter: Concrete example</h1><p id="f00f" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">Let’s consider the following supervised task. We’ve simplified the details to focus on the interesting part — the PMI.</p><h2 id="6163" class="pn ny fq bf nz po pp pq oc pr ps pt of nk pu pv pw no px py pz ns qa qb qc qd bk">The Research Hypothesis</h2><p id="8507" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">we can forecast potential medical conditions based on a person’s reading history on medical Wikipedia. For example, if an individual has browsed articles on Headache, Smoking, Coughing, and Tooth Discoloration, they may be at increased risk of a particular lung disease. Once more, this is a simplified research scenario and a toy hypothesis.</p><h2 id="8f31" class="pn ny fq bf nz po pp pq oc pr ps pt of nk pu pv pw no px py pz ns qa qb qc qd bk">The Data</h2><ul class=""><li id="a070" class="nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw qe qf qg bk">The independent variables are given in a tabular data, in a design matrix X. Each entry has binary values, 0/1.</li><li id="7a1b" class="nb nc fq nd b go qh nf ng gr qi ni nj nk qj nm nn no qk nq nr ns ql nu nv nw qe qf qg bk">Each row represents one patient — i.e. the wipiedia reading history of one patient, as binary variables — one variable for each medical wikipedia article (this is similar to NLP’s bag-of-words encoding).</li><li id="0926" class="nb nc fq nd b go qh nf ng gr qi ni nj nk qj nm nn no qk nq nr ns ql nu nv nw qe qf qg bk">Each column represent a medical term from wikipedia.</li><li id="e7cb" class="nb nc fq nd b go qh nf ng gr qi ni nj nk qj nm nn no qk nq nr ns ql nu nv nw qe qf qg bk">for instance, for patient <em class="oy">p</em> and for a wikipedia article <em class="oy">f</em> (eg Pneumonia), <em class="oy">X[i, f] = 1</em> means that patient <em class="oy">p</em> had visited article <em class="oy">f</em>. namely, “The patient read the Pneumonia article”. Clearly, patients are not checking the exact wikipedia article related to their conditions, but we know from previous works that in some cases they search online for symptoms that are related to the real condition. And this exactly where the knowledge graph becomes handy: “the person read about symptoms that are related to a neighborhood in the graph that is related to specific medical condition”.</li><li id="a515" class="nb nc fq nd b go qh nf ng gr qi ni nj nk qj nm nn no qk nq nr ns ql nu nv nw qe qf qg bk">We will denote the number of rows (patients) and the number of columns (binary features) — <em class="oy">m</em> and <em class="oy">n</em>, respectively.</li><li id="ea78" class="nb nc fq nd b go qh nf ng gr qi ni nj nk qj nm nn no qk nq nr ns ql nu nv nw qe qf qg bk">The target (dependent variable) <em class="oy">y</em> is future existence of a medical condition, e.g., severe COVID-19.</li><li id="b006" class="nb nc fq nd b go qh nf ng gr qi ni nj nk qj nm nn no qk nq nr ns ql nu nv nw qe qf qg bk">In addition, we have a knowledge graph (KG), the complete medical wikipedia, where nodes are the wikipedia articles of the medical terms and edges are hyperlinks. Note that this is a directed graph.</li><li id="856a" class="nb nc fq nd b go qh nf ng gr qi ni nj nk qj nm nn no qk nq nr ns ql nu nv nw qe qf qg bk">For simplicity, assume that there is every feature (column) is represented in the graph as single node.</li></ul><p id="aedf" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Using the Knowledge Graph to enrich features is a broad topic, so we won't go into detail here. However, one method worth noting is node embedding, where binary features are transformed into a continuous, lower-dimensional vector space.</p><p id="5dfa" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now we can use the medical example scenario in order to get a better intuition for the <strong class="nd fr">need</strong> of sparsification:</p><ol class=""><li id="e3e7" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qm qf qg bk">Remember the hubs we mentioned above? While some hubs may hold significance for our target, many consist of irrelevant high-degree nodes, such as “<a class="af oz" href="https://en.wikipedia.org/wiki/Medical_specialty" rel="noopener ugc nofollow" target="_blank">List of medical specialities</a>” or “<a class="af oz" href="https://en.wikipedia.org/wiki/List_of_medical_symptoms" rel="noopener ugc nofollow" target="_blank">List of medical symptoms</a>”, such a node will amplify noise in the graph muting important information about a specific rare symptom. But how can we effectively discern these hubs and identify which of their edges are irrelevant and safe to remove from the graph?</li><li id="93d1" class="nb nc fq nd b go qh nf ng gr qi ni nj nk qj nm nn no qk nq nr ns ql nu nv nw qm qf qg bk">Furthermore, there could be a need to utilize the same database for predicting two distinct target variables, such as Female Breast Cancer and Prostate Cancer in men, which exhibit different behaviors. In such cases, employing the entire knowledge graph may not be advantageous, as certain edges may lack relevance to our specific target label. Therefore, it might be necessary to exclude edges associated with the opposite gender. But how do we systematically remove edges that are irrelevant to a specific target variable?</li></ol></div></div></div><div class="ab cb pa pb pc pd" role="separator"><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="055d" class="nx ny fq bf nz oa pi gq oc od pj gt of og pk oi oj ok pl om on oo pm oq or os bk">3rd Chapter: The PMI Trick</h1><p id="e4fd" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">Our goal is clear: we seek to remove edges within the knowledge graph that lack relevance to our target variable. While multiple mathematical definitions of relevance are available, we have opted to employ Pointwise Mutual Information (PMI) for its simplicity and intuitiveness.</p><p id="d6bb" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">PMI is a fundamental tool from Information Theory, so let’s talk about it: What exactly is PMI? We’ll begin by outlining its definition and then aim to develop a better intuition.</p><blockquote class="qn qo qp"><p id="8c3d" class="nb nc oy nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">PMI has been described as “one of the most important concepts in NLP” [<a class="af oz" href="https://web.stanford.edu/~jurafsky/slp3/6.pdf" rel="noopener ugc nofollow" target="_blank">see 6.6</a>]</p></blockquote><h2 id="eb20" class="pn ny fq bf nz po pp pq oc pr ps pt of nk pu pv pw no px py pz ns qa qb qc qd bk"><strong class="al">PMI: Pointwise Mutual Information</strong></h2><p id="fceb" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">PMI serves as a point-estimator for the well-known Mutual Information between two discrete random variables. Given observed outcomes x and y for two random variables <em class="oy">X</em> and <em class="oy">Y</em>, we define:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qq"><img src="../Images/5ec0b206c17dbed562b8d107b630e626.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/0*MjP52auWibb8307d"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Pointwise Mutual Information (from wikipedia)</figcaption></figure><p id="0739" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The equalities are immediate results of Bayes’ theorem, providing us with distinct perspectives and, ideally, intuition regarding PMI:</p><p id="fc96" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">If X and Y are independent, then p(x,y)=p(x)p(y). So, the first term might be understood as the ratio between:</p><ul class=""><li id="d8a5" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qe qf qg bk"><em class="oy">p(x, y)</em> = point-estimate of the actual joint distribution with dependency, and</li><li id="792d" class="nb nc fq nd b go qh nf ng gr qi ni nj nk qj nm nn no qk nq nr ns ql nu nv nw qe qf qg bk"><em class="oy">p(x)p(y)</em> = the joint distribution, assuming independence between the two variables.</li></ul><p id="74c1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Looking on the last term you might recognize that the PMI is quantifying “how does the probability of x changes, given knowledge of y”, and vice versa.</p><p id="9058" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Let’s do a small exercise, to get more intuition into PMI:</p><ul class=""><li id="4b70" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qe qf qg bk">assume 1% of all patients had severe covid, p(covid) = .01</li><li id="1096" class="nb nc fq nd b go qh nf ng gr qi ni nj nk qj nm nn no qk nq nr ns ql nu nv nw qe qf qg bk">Among patients who had pneumonia in the past, 4% got severe covid. p(covid|pneumonia) = .04</li><li id="996c" class="nb nc fq nd b go qh nf ng gr qi ni nj nk qj nm nn no qk nq nr ns ql nu nv nw qe qf qg bk">then the probability of covid given pneumonia is higher than without information about pneumonia, and as a result the PMI is high. PMI(covid;pneumonia) = log(.04/.01) = 2. Very intuitive, right?</li></ul><p id="e3d5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">PMI is beautiful in its simplicity, yet there’s much more to explore about its features, variations, and applications. One noteworthy variant is the normalized PMI, which ranges between -1 and 1. This feature enables comparison and filtering across numerous pairs of random variables. Keep this in mind — it will prove valuable shortly.</p><h2 id="a59a" class="pn ny fq bf nz po pp pq oc pr ps pt of nk pu pv pw no px py pz ns qa qb qc qd bk">Back to our task</h2><p id="b974" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">We have a large dense graph presenting links between out binary features and we have a target variable. How can we sparsify the graph intelligently?</p><p id="c8d7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For an edge e between two features v1 and v2, we define an indicator random variable x_e to be 1 if and only if both features have the value 1 (True), meaning the two medical terms coincide for a patient. Now, look on the edge and the target variable y. We asked the simple question: is this edge relevant for y? now we can answer simply with the PMI! if PMI[x_e,y] is very close to zero, this edge hold no information relevant to our target, otherwise <strong class="nd fr">there is some relevant information </strong>in this edge.</p><p id="283c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">So, to conclude, we remove all edges with:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qr"><img src="../Images/c88fdc9a475e66ef5074002c8f6456a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/0*xoH4E25iXV8Xm9P0"/></div></figure><p id="1a8d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Where alpha is a hyper-parameter, by tuning it you can control the sparsity of the graph (trading-off with generalization-error, aka risk of over-fit).</p><h2 id="10ab" class="pn ny fq bf nz po pp pq oc pr ps pt of nk pu pv pw no px py pz ns qa qb qc qd bk"><strong class="al">Three Caveats — and potential improvements</strong></h2><p id="3f9a" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk"><strong class="nd fr">Caveat 1)</strong> The feature space often exhibits sparsity, resulting in zero values for both the numerator and denominator of the PMI, and we better not remove such edges as we have no information about them whatsoever.</p><p id="2be0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">You might ask: if we are usually not removing edges, are we really “sparsifying” the graph? the answer is in the hubs. Remember those hubs? they will actually usually NOT be zeros BECAUSE they are hubs.</p><p id="3592" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Caveat 2)</strong> Another good question is: why define the edge-variable as “both features have a value of 1”? Alternatively, we could check if either of the features has a value of 1. Thus, instead of y = x1 and x2, we could consider y = x1 or x2. This presents a valid point. These different implementations convey slightly different narratives about your understanding of the domain and may be suitable for different datasets. I suggest exploring various versions for your specific use cases.</p><p id="3bf3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Caveat 3)</strong> Even if the probabilities are not zero, in the medical domain they are usually very-very small, so in order to add stability we can define conditional PMI:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qs"><img src="../Images/ab3b3dc3871f8b0884ed680d7f40091b.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/0*yl8BWMpqXAU6CKnb"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Conditional PMI</figcaption></figure><p id="4af8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In plain English: we calculate the PMI in a probability subspace, where third event occurs.</p><p id="d2af" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Specifically, in the Knowledge Graph, remember that the graph is directed. We will use the cPMI to check if an edge between two features e=(v1,v2) is relevant, <strong class="nd fr">given</strong> that the the first feature is positive.</p><p id="0be1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In other words, if v1 never occur, we claim that we don’t have enough information about the edge even in order to remove it.</p></div></div></div><div class="ab cb pa pb pc pd" role="separator"><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="750f" class="nx ny fq bf nz oa pi gq oc od pj gt of og pk oi oj ok pl om on oo pm oq or os bk">Conclusion</h1><p id="34a2" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">Now, when we know what’s PMI, we understand that in order to remove irrelevant edges in a knowledge graph we can check the pointwise mutual information between occurrences of each edge and the target variable and remove all irrelevant edges. Boom! 🎤</p></div></div></div></div>    
</body>
</html>