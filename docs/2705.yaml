- en: Predict Housing Price using Linear Regression in Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用线性回归预测房价（Python实现）
- en: 原文：[https://towardsdatascience.com/predict-housing-price-using-linear-regression-in-python-bfc0fcfff640?source=collection_archive---------2-----------------------#2024-11-06](https://towardsdatascience.com/predict-housing-price-using-linear-regression-in-python-bfc0fcfff640?source=collection_archive---------2-----------------------#2024-11-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/predict-housing-price-using-linear-regression-in-python-bfc0fcfff640?source=collection_archive---------2-----------------------#2024-11-06](https://towardsdatascience.com/predict-housing-price-using-linear-regression-in-python-bfc0fcfff640?source=collection_archive---------2-----------------------#2024-11-06)
- en: A walk-through of cost computation, gradient descent, and regularization using
    Boston Housing dataset
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过波士顿住房数据集，讲解成本计算、梯度下降和正则化
- en: '[](https://medium.com/@yqelisa?source=post_page---byline--bfc0fcfff640--------------------------------)[![Elisa
    Yao](../Images/bf38cf250ae51db4f9880cb907b2f854.png)](https://medium.com/@yqelisa?source=post_page---byline--bfc0fcfff640--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bfc0fcfff640--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bfc0fcfff640--------------------------------)
    [Elisa Yao](https://medium.com/@yqelisa?source=post_page---byline--bfc0fcfff640--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@yqelisa?source=post_page---byline--bfc0fcfff640--------------------------------)[![Elisa
    Yao](../Images/bf38cf250ae51db4f9880cb907b2f854.png)](https://medium.com/@yqelisa?source=post_page---byline--bfc0fcfff640--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bfc0fcfff640--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bfc0fcfff640--------------------------------)
    [Elisa Yao](https://medium.com/@yqelisa?source=post_page---byline--bfc0fcfff640--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bfc0fcfff640--------------------------------)
    ·14 min read·Nov 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bfc0fcfff640--------------------------------)
    ·14分钟阅读·2024年11月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Linear Regression seems old and naive when Large Language Models (LLMs) dominate
    people’s attention through their sophistication recently. Is there still a point
    of understanding it?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当大语言模型（LLMs）因其复杂性最近引起人们的广泛关注时，线性回归显得有些过时和简单。那么，了解它还有意义吗？
- en: My answer is “Yes”, because it’s a building block of more complex models, including
    LLMs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我的答案是“有”，因为它是更复杂模型的基石，包括大语言模型（LLMs）。
- en: 'Creating a Linear Regression model can be as easy as running 3 lines of code:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个线性回归模型可以像运行三行代码一样简单：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: However, this doesn’t show us the structure of the model. To produce optimal
    modeling results, we need to **understand what goes on behind the scenes**. In
    this article, I’ll break down the **process of implementing Linear Regression
    in Python** using a simple dataset known as “Boston Housing”, step by step.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并没有展示模型的结构。为了得到最优的建模结果，我们需要**了解背后发生了什么**。在本文中，我将逐步分解如何使用一个简单的“波士顿住房”数据集在Python中实现线性回归的**过程**。
- en: What is Linear Regression
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是线性回归
- en: '**Linear** — when plotted in a 2-dimensional space, if the dots showing the
    relationship of predictor *x* and predicted variable *y* scatter along a straight
    line, then we think this relationship can be represented by this line.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性**——当绘制在二维空间中时，如果展示预测变量 *x* 和预测结果 *y* 之间关系的点沿直线散布，那么我们认为这种关系可以用这条直线表示。'
- en: '**Regression** — a statistical method for estimating the relationship between
    one or more predictors (independent variables) and a predicted (dependent variable).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归**——一种统计方法，用于估算一个或多个预测变量（自变量）与一个预测变量（因变量）之间的关系。'
- en: '**Linear Regression** describes the predicted variable as a linear combination
    of the predictors. The line that abstracts this relationship is called **line
    of best fit**, see the red straight line in the below figure as an example.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性回归**将预测变量描述为预测变量的线性组合。抽象这种关系的直线称为**最佳拟合线**，下图中的红色直线即为示例。'
- en: '![](../Images/f5b975352a38e52b607f7ad54a6f9c56.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5b975352a38e52b607f7ad54a6f9c56.png)'
- en: Example of Linear Relationship and Line of Best Fit (Image by author)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 线性关系和最佳拟合线示例（图源：作者）
- en: Data Description
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据描述
- en: 'To keep our goal focused on illustrating the Linear Regression steps in Python,
    I picked the Boston Housing dataset, which is:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的目标集中在展示Python中线性回归的步骤上，我选择了波士顿住房数据集，数据集内容如下：
- en: '**Small** — makes debugging easy'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简洁**——便于调试'
- en: '**Simple** — so we spend less time in understanding the data or feature engineering'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单** — 让我们花更少的时间理解数据或进行特征工程'
- en: '**Clean** — requires minimum data cleaning'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清洁** — 需要最小的数据清洗'
- en: 'The dataset was first curated in [Harrison and Rubinfeld’s (1978) study of
    Hedonic Housing Prices](https://www.law.berkeley.edu/files/Hedonic.PDF). It originally
    has:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集最初由[Harrison和Rubinfeld的（1978年）《享乐住房价格研究》](https://www.law.berkeley.edu/files/Hedonic.PDF)整理。它最初包含：
- en: 13 predictors — including demographic attributes, environmental attributes,
    and economics attributes
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 13个预测变量 — 包括人口统计属性、环境属性和经济属性
- en: '- CRIM — per capita crime rate by town'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- CRIM — 每个城镇的人均犯罪率'
- en: '- ZN — proportion of residential land zoned for lots over 25,000 sq.ft.'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- ZN — 用于建造大于25,000平方英尺地块的住宅用地比例'
- en: '- INDUS — proportion of non-retail business acres per town.'
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- INDUS — 每个城镇非零售业务用地的比例'
- en: '- CHAS — Charles River dummy variable (1 if tract bounds river; 0 otherwise)'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- CHAS — 查尔斯河虚拟变量（如果区域边界为河流则为1；否则为0）'
- en: '- NOX — nitric oxides concentration (parts per 10 million)'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- NOX — 氮氧化物浓度（每1000万分之一）'
- en: '- RM — average number of rooms per dwelling'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- RM — 每个住宅的平均房间数'
- en: '- AGE — proportion of owner-occupied units built prior to 1940'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- AGE — 1940年之前建造的自有住宅单位比例'
- en: '- DIS — weighted distances to five Boston employment centres'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- DIS — 到五个波士顿就业中心的加权距离'
- en: '- RAD — index of accessibility to radial highways'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- RAD — 通往放射状高速公路的可达性指数'
- en: '- TAX — full-value property-tax rate per $10,000'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- TAX — 每$10,000的全值房产税税率'
- en: '- PTRATIO — pupil-teacher ratio by town'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- PTRATIO — 每个城镇的师生比'
- en: '- LSTAT — % lower status of the population'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- LSTAT — 低收入群体的百分比'
- en: 1 target (with variable name “MEDV”) — median value of owner-occupied homes
    in $1000's, at a specific location
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1个目标变量（变量名为“MEDV”）— 所有者自住住房的中位数价值（以$1000为单位），位于特定位置
- en: You can download the raw data [here](https://faculty.tuck.dartmouth.edu/images/uploads/faculty/business-analytics/Boston_Housing.xlsx).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以[在这里](https://faculty.tuck.dartmouth.edu/images/uploads/faculty/business-analytics/Boston_Housing.xlsx)下载原始数据。
- en: 'Load data into Python using `pandas`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pandas`加载数据到Python中：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'See the dataset’s number of rows (observations) and columns (variables):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 查看数据集的行数（观察值）和列数（变量）：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The **modeling problem** of our exercise is: given the attributes of a location,
    try to predict the **median housing price of this location**.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们练习中的**建模问题**是：给定某位置的属性，尝试预测该位置的**中位住房价格**。
- en: We store the target variable and predictors using 2 separate objects, *x* and
    *y*, following math and ML notations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个单独的对象 *x* 和 *y* 来存储目标变量和预测变量，遵循数学和机器学习的符号约定。
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Visualize** the dataset by histogram and scatter plot:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**可视化**数据集，通过直方图和散点图：'
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/55a4db43502733e11e9da189a5afd375.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55a4db43502733e11e9da189a5afd375.png)'
- en: Example output of histogram and scatter plot (Image by author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图和散点图的示例输出（图源：作者）
- en: The point of visualizing the variables is to see if any **transformation** is
    needed for the variables, and identify the **type of relationship** between individual
    variables and target. For example, the target may have a linear relationship with
    some predictors, but polynomial relationship with others. This further **infers
    which models to use** for solving the problem.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化变量的目的是查看是否需要对变量进行**转换**，并识别各个变量与目标之间的**关系类型**。例如，目标可能与一些预测变量存在线性关系，但与其他变量存在线性或多项式关系。这进一步**推断出使用哪些模型**来解决问题。
- en: Cost Computation
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本计算
- en: How well the model captures the relationship between the predictors and the
    target can be **measured by** how much the predicted results **deviate** from
    the ground truth. The function that quantifies this deviation is called **Cost
    Function**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 模型捕捉预测变量与目标之间关系的好坏，可以通过预测结果与真实值的**偏差**来**衡量**。量化这种偏差的函数被称为**成本函数**。
- en: The smaller the **cost** is, the better the model captures the relationship
    the predictors and the target. This means, mathematically, the **model training**
    process aims to **minimize** the result of cost function.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**成本**越小，模型越能更好地捕捉预测变量与目标之间的关系。这意味着，从数学角度看，**模型训练**过程的目标是**最小化**成本函数的结果。'
- en: 'There are different cost functions that can be used for regression problems:
    Sum of Squared Errors (SSE), Mean Squared Error (MSE), Mean Absolute Error (MAE)…'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，有不同的成本函数可以使用：平方误差和（SSE）、均方误差（MSE）、平均绝对误差（MAE）……
- en: '**MSE** is the most popular cost function used for Linear Regression, and is
    the default cost function in many statistical packages in R and Python. Here’s
    its math expression:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方误差（MSE）**是最常用的线性回归成本函数，也是许多R和Python统计包中的默认成本函数。其数学表达式如下：'
- en: '![](../Images/97705ac06e7953532935e51ec78ceb0e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97705ac06e7953532935e51ec78ceb0e.png)'
- en: 'Note: The 2 in the denominator is there to make calculation neater.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：分母中的2是为了使计算更加简洁。
- en: 'To use MSE as our cost function, we can create the following function in Python:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用均方误差（MSE）作为我们的成本函数，我们可以在Python中创建以下函数：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Gradient Descent
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: '**Gradient** — the slope of the tangent line at a certain point of the function.
    In multivariable calculus, gradient is a vector that points in the direction of
    the steepest ascent at a certain point.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度**——函数在某一点的切线斜率。在多变量微积分中，梯度是一个向量，指向某一点上坡度最陡的方向。'
- en: '**Descent** — moving towards the minimum of the cost function.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**下降**——朝着成本函数的最小值移动。'
- en: '**Gradient Descent** — a method that iteratively adjusts the parameters in
    small steps, guided by the gradient, to reach the lowest point of a function.
    It is a way to **numerically** reach the desired parameters for Linear Regression.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度下降**——一种迭代调整参数的小步法，通过梯度的引导，达到函数的最低点。这是一种**数值**方法，用于线性回归中获得所需的参数。'
- en: In contrast, there’s a way to **analytically** solve for the optimal parameters
    — Ordinary Least Squares (OLS). See [this GeekforGeeks article](https://www.geeksforgeeks.org/linear-regression-python-implementation/)
    for details of how to implement it in Python. In practice, it does not scale as
    well as the Gradient Descent approach because of higher computational complexity.
    Therefore, we use Gradient Descent in our case.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，还有一种方法可以**解析**求解最优参数——普通最小二乘法（OLS）。有关如何在Python中实现的详细信息，请参见[这篇GeekforGeeks文章](https://www.geeksforgeeks.org/linear-regression-python-implementation/)。实际上，由于计算复杂度较高，它不像梯度下降方法那样具有良好的可扩展性。因此，在我们的案例中，我们使用梯度下降。
- en: 'In each iteration of the Gradient Descent process:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次梯度下降迭代中：
- en: The **gradients** determine the **direction** of the descent
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度**决定了**下降的方向**'
- en: The **learning rate** determines the **scale** of the descent
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**决定了**下降的幅度**'
- en: 'To calculate the gradients, we need to understand that there are 2 parameters
    that alter the value of the cost function:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算梯度，我们需要理解有2个参数会改变成本函数的值：
- en: '***w*** — the vector of each predictor’s weight'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***w***——每个预测变量的权重向量'
- en: '***b*** — the bias term'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***b***——偏置项'
- en: 'Note: because the values of all the observations (*xⁱ*) don’t change over the
    training process, they contribute to the computation result, but are constants,
    not variables.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于所有观察值（*xⁱ*）在训练过程中保持不变，它们会对计算结果产生影响，但它们是常数，而不是变量。
- en: 'Mathematically, the gradients are:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，梯度是：
- en: '![](../Images/00ac51d4afef3f89656324302f167105.png)![](../Images/0e59a669b46d0a40d02f8220f06f2760.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00ac51d4afef3f89656324302f167105.png)![](../Images/0e59a669b46d0a40d02f8220f06f2760.png)'
- en: 'Correspondingly, we create the following function in Python:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地，我们在Python中创建以下函数：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Using this function, we get the gradients of the cost function, and with a set
    learning rate, update the parameters iteratively.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个函数，我们得到成本函数的梯度，并在设定的学习率下，迭代更新参数。
- en: 'Since it’s a loop logically, we need to define the stopping condition, which
    could be any of:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它是一个逻辑循环，我们需要定义停止条件，可能是以下任意一种：
- en: We reach the set **number of iterations**
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们达到了设定的**迭代次数**
- en: The **cost** gets to below a certain threshold
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**降低到某个阈值以下'
- en: The **improvement** drops below a certain threshold
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进**低于某个阈值'
- en: 'If we choose the number of iterations as the stopping condition, we can write
    the Gradient Descent process to be:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择迭代次数作为停止条件，我们可以将梯度下降过程写成：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Apply it to our dataset:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 将其应用于我们的数据集：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can visualize the process of cost decreases as the number iteration increases
    using the below function:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下函数可视化成本随着迭代次数增加而下降的过程：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here’s the the plot for our training process:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们训练过程的图示：
- en: '![](../Images/d76a91de9d10c5e4c3987e31c90b8dbf.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d76a91de9d10c5e4c3987e31c90b8dbf.png)'
- en: How the value of cost function changes over the iterations (Image by author)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数值随迭代次数变化的情况（图源：作者）
- en: Prediction
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测
- en: Making predictions is essentially applying the model to our dataset of interest
    to get the output values. These values are **what the model “thinks”** **the target
    value should be, given a set of predictor values**.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 进行预测本质上是将模型应用于我们关注的数据集，以获得输出值。这些值是**模型“认为”** **在给定一组预测值的情况下，目标值应该是什么**。
- en: 'In our case, we apply the linear function:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们应用线性函数：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Get the prediction results using:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码获取预测结果：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Result Evaluation
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果评估
- en: How do we get an idea of the model performance?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何评估模型的表现？
- en: 'One way is through the cost function, as stated earlier:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是通过成本函数，如前所述：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here’s the MSE on our test dataset:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在测试数据集上的均方误差（MSE）：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Another way is more intuitive — visualizing the predicted values against the
    actual values. If the model makes perfect predictions, then each element of `y_test`
    should always equal to the corresponding element of `y_pred`. If we plot `y_test`
    on *x* axis, `y_pred` on *y* axis, the dots will form a **diagonal** straight
    line.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法更为直观——将预测值与实际值进行可视化。如果模型做出完美预测，那么`y_test`的每个元素都应该等于`y_pred`的相应元素。如果我们将`y_test`绘制在*x*轴上，`y_pred`绘制在*y*轴上，这些点将形成一条**对角线**。
- en: 'Here’s our custom plotting function for the comparison:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们用于比较的自定义绘图函数：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After applying to our training result, we find that the dots look nothing like
    a straight line:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用到我们的训练结果后，我们发现这些点看起来与直线完全不符：
- en: '![](../Images/d13f49c7be107a1c26dfcda35ddf3ade.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d13f49c7be107a1c26dfcda35ddf3ade.png)'
- en: Scatter plot of predicted values against actual values (Image by author)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值与实际值的散点图（图像来源：作者）
- en: 'This should get us thinking: how can we improve the model’s performance?'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该引发我们的思考：我们如何改进模型的表现？
- en: Feature Scaling
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征缩放
- en: The Gradient Descent process is sensitive to the scale of features. As shown
    in the contour plot on the left, when the learning rate of different features
    are kept the same, then if the features are in different scales, the path of reaching
    global minimum may jump back and forth along the cost function.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降过程对特征的尺度敏感。如左侧的等高线图所示，当不同特征的学习率保持一致时，如果特征的尺度不同，达到全局最小值的路径可能会在成本函数上来回跳跃。
- en: '![](../Images/cc27bf98766041306bdc54fa915c5edd.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc27bf98766041306bdc54fa915c5edd.png)'
- en: 'The path towards global minimum of the cost function when features are not-scaled
    vs scaled (Source: [DataMListic](https://www.youtube.com/watch?v=CFA7OFYDBQY))'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征未缩放与已缩放时，成本函数达到全局最小值的路径（来源：[DataMListic](https://www.youtube.com/watch?v=CFA7OFYDBQY)）
- en: After scaling all the features to the same ranges, we can observe a smoother
    and more straight-forward path to global minimum of the cost function.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在将所有特征缩放到相同的范围后，我们可以观察到成本函数到达全局最小值的路径更加平滑和直接。
- en: There are multiple ways to conduct feature scaling, and here we choose **Standardization**
    to turn all the features to have mean of 0 and standard deviation of 1.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 进行特征缩放的方法有很多种，这里我们选择**标准化**将所有特征转化为均值为0，标准差为1。
- en: 'Here’s how to standardize features in Python:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何在Python中标准化特征：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we conduct Gradient Descent on the standardized dataset:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在标准化数据集上进行梯度下降：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We get a steeper and smoother decline of cost before 200 iterations, compared
    to the previous round of training:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 与上轮训练相比，我们在200次迭代之前获得了更陡峭且平滑的成本下降：
- en: '![](../Images/66636c59252b21acb1e9bde42290616f.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66636c59252b21acb1e9bde42290616f.png)'
- en: Cost by each iteration on the standardized dataset (Image by author)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化数据集上每次迭代的成本（图像来源：作者）
- en: 'If we plot the predicted versus actual values again, we see the dots look much
    closer to a straight line:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次绘制预测值与实际值的图像，看到的点会更加接近一条直线：
- en: '![](../Images/db67ab25d03c0b01f74ae4f4b25ecdb3.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db67ab25d03c0b01f74ae4f4b25ecdb3.png)'
- en: Scatter plot of predicted values against actual values on standardized dataset
    (Image by author)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准化数据集上，预测值与实际值的散点图（图像来源：作者）
- en: 'To quantify the model performance on the test set:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化模型在测试集上的表现：
- en: '[PRE20]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We see an improvement from MSE of 132.84 to 35.66! Can we do more to improve
    the model?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到均方误差从132.84下降到35.66！我们还可以做些什么来进一步提高模型？
- en: Regularization — Ridge Regression
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化 — 岭回归
- en: We notice that in the last round of training, the training MSE is 9.96, and
    the testing MSE is 35.66\. Can we push the test set performance to be closer to
    training set?
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，在最后一轮训练中，训练集的MSE为9.96，测试集的MSE为35.66。我们能否将测试集的表现推向更接近训练集的水平？
- en: Here comes **Regularization**. It penalizes large parameters to prevent the
    model from being too specific to the training set.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是**正则化**。它惩罚大参数，防止模型对训练集过于拟合。
- en: 'There are mainly 2 popular ways of regularization:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化主要有两种流行方式：
- en: '**L1 Regularization** — uses the L1 norm (**absolute values**, a.k.a. “Manhattan
    norm”) of the weights as the penalty term.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L1正则化** — 使用权重的L1范数（**绝对值**，也称为“曼哈顿范数”）作为惩罚项。'
- en: '**L2 Regularization** — uses the L2 norm (**squared values**, a.k.a. “Euclidean
    norm”) of the weights as the penalty term.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2正则化** — 使用权重的L2范数（**平方值**，也称为“欧几里得范数”）作为惩罚项。'
- en: Let’s first try **Ridge Regression** which uses L2 regularization as our new
    version of model. Its Gradient Descent process is easier to understand than **LASSO
    Regression**, which uses L1 regularization.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先尝试**岭回归**，它使用L2正则化作为模型的新版本。其梯度下降过程比**LASSO回归**（使用L1正则化）更容易理解。
- en: 'The cost function with L1 regularization looks like this:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 带L1正则化的损失函数如下所示：
- en: '![](../Images/30ba14f3eb405c8a1f5fbf3276207f9d.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30ba14f3eb405c8a1f5fbf3276207f9d.png)'
- en: '**Lambda** controls the degree of penalty. When lambda is high, the level of
    penalty is high, then the model leans to underfitting.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**Lambda** 控制惩罚的程度。当lambda较大时，惩罚程度较高，模型趋向于欠拟合。'
- en: 'We can turn the calculation into the following function:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将计算转化为以下函数：
- en: '[PRE22]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'For the Gradient Descent process, we use the following function to compute
    the gradients with regularization:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于梯度下降过程，我们使用以下函数来计算带正则化的梯度：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Combine the two steps together, we get the following Gradient Descent function
    for Ridge Regression:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 将这两步合并在一起，我们得到岭回归的梯度下降函数：
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Train the model on our standardized dataset:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的标准化数据集上训练模型：
- en: '[PRE25]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The training cost is slightly higher than our previous version of model.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的损失略高于我们之前的模型版本。
- en: 'The learning curve looks very similar to the one from the previous round:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线看起来与上一轮的结果非常相似：
- en: '![](../Images/1d49ce9629259ad7e2859125a46332d5.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d49ce9629259ad7e2859125a46332d5.png)'
- en: Cost by each iteration for Ridge Regression (Image by author)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归每次迭代的成本（作者提供的图片）
- en: 'The predicted vs actual values plot looks almost identical to what we got from
    the previous round:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值与实际值的图几乎与我们从上一轮得到的结果相同：
- en: '![](../Images/c3083a8f8327e8491a9dafdf20291a7a.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3083a8f8327e8491a9dafdf20291a7a.png)'
- en: Scatter plot of predicted values against actual values for Ridge Regression
    (Image by author)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归的预测值与实际值的散点图（作者提供的图片）
- en: We got test set MSE of 35.69, which is slightly higher than the one without
    regularization.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了测试集的均方误差（MSE）为35.69，稍微高于没有正则化时的值。
- en: Regularization — LASSO Regression
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化 — LASSO回归
- en: Finally, let’s try out LASSO Regression! LASSO stands for **Least Absolute Shrinkage
    and Selection Operator**.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们尝试LASSO回归！LASSO代表**最小绝对收缩和选择算子**。
- en: 'This is the cost function with L2 regularization:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是带有L2正则化的损失函数：
- en: '![](../Images/3b3bf962171d9ca08e93bb2c114abaed.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b3bf962171d9ca08e93bb2c114abaed.png)'
- en: What’s tricky about the training process of LASSO Regression, is that the derivative
    of the absolute function is undefined at *w=0*. Therefore, **Coordinate Descent**
    is used in practice for LASSO Regression. It focuses on one coordinate at a time
    to find the minimum, and then switch to the next coordinate.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: LASSO回归训练过程中复杂的地方在于，绝对值函数的导数在*w=0*时是未定义的。因此，实际上在LASSO回归中使用**坐标下降法**。它一次专注于一个坐标，找到最小值后再切换到下一个坐标。
- en: Here’s how we implement it in Python, inspired by [Sicotte (2018)](https://xavierbourretsicotte.github.io/lasso_implementation.html)
    and [D@Kg’s notebook (2022)](https://www.kaggle.com/code/ddatad/coordinate-descent-for-lasso-normal-regression/notebook).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们在Python中实现的方式，灵感来自于[Sicotte (2018)](https://xavierbourretsicotte.github.io/lasso_implementation.html)和[D@Kg的笔记本（2022）](https://www.kaggle.com/code/ddatad/coordinate-descent-for-lasso-normal-regression/notebook)。
- en: 'First, we define the soft threshold function, which is the solution to the
    single variable optimization problem:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义软阈值函数，这是单变量优化问题的解：
- en: '[PRE28]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Second, calculate the residuals of the prediction:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步，计算预测的残差：
- en: '[PRE29]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Use the residual to calculate rho, which is the subderivative:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用残差来计算rho，这是子导数：
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Put everything together:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起：
- en: '[PRE31]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Apply it to our training set:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 将其应用到我们的训练集：
- en: '[PRE32]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The training process converged drastically, compared to Gradient Descent on
    Ridge Regression:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 与岭回归的梯度下降过程相比，训练过程迅速收敛：
- en: '![](../Images/2954df2b315ee435c1882247a520c4da.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2954df2b315ee435c1882247a520c4da.png)'
- en: Cost by each iteration for LASSO Regression (Image by author)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: LASSO回归每次迭代的成本（图像来源：作者）
- en: 'However, the training result is not significantly improved:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，训练结果没有显著改善：
- en: '![](../Images/0d0a3769b84987abd051000bb9c5c56a.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d0a3769b84987abd051000bb9c5c56a.png)'
- en: Scatter plot of predicted values against actual values for LASSO Regression
    (Image by author)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: LASSO回归的预测值与实际值的散点图（图像来源：作者）
- en: Eventually, we achieved MSE of 34.40, which is the lowest among the methods
    we tried.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们实现了34.40的均方误差（MSE），这是我们尝试的所有方法中最低的。
- en: Interpreting the Results
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释结果
- en: How do we interpret the model training results using human language? Let’s use
    the result of LASSO Regression as an example, since it has the best performance
    among the model variations we tried out.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何用人类语言解释模型训练结果？让我们以LASSO回归的结果为例，因为它在我们尝试的模型变化中表现最佳。
- en: 'We can get the **weights** and the **bias** by printing the `w_out` and `b_out`
    we got in the previous section:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过打印在上一节中获得的`w_out`和`b_out`来得到**权重**和**偏差**：
- en: '[PRE33]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In our case, there are 13 predictors, so this dataset has 13 dimensions. In
    each dimension, we can plot the predictor `x_i` against the target `y` as a scatterplot.
    The regression line’s **slope** is the weight `w_i`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，有13个预测变量，因此该数据集有13个维度。在每个维度中，我们可以将预测变量`x_i`与目标变量`y`绘制成散点图。回归线的**斜率**即为权重`w_i`。
- en: In details, the first dimension is *“CRIM — per capita crime rate by town”*,
    and our `w_1` is -0.8664\. This means, each unit of **increase** in `x_i`, `y`
    is expected to **decrease** by -0.8664 unit.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，第一维度是*“CRIM — 按城镇计算的每人犯罪率”*，而我们的`w_1`是-0.8664。这意味着，`x_i`每增加一个单位，`y`预计将**减少**-0.8664个单位。
- en: Note that we have **scaled** our dataset before we run the training process,
    so now we need to **reverse** that process to get the intuitive relationship between
    the predictor *“per capita crime rate by town”* and our target variable *“median
    value of owner-occupied homes in $1000’s, at a specific location”*.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在我们运行训练过程之前，已经对数据集进行了**缩放**，因此现在我们需要**反转**这一过程，以便获得预测变量*“按城镇计算的每人犯罪率”*与目标变量*“特定位置上业主自住住房的中位数价格（单位：$1000）”*之间的直观关系。
- en: 'To reverse the scaling, we need to get the vector of scales:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了反转缩放过程，我们需要获得比例向量：
- en: '[PRE35]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here we find the scale we used for our first predictor: 8.1278\. We divide
    the weight of -0.8664 by scale or 8.1278 to get **-0.1066**.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们找到了我们第一个预测模型所用的比例：8.1278。我们将权重-0.8664除以比例8.1278，得到**-0.1066**。
- en: '**This means:** when all other factors remains the same, if the per capita
    crime rate **increases** by 1 percentage point, the medium housing price of that
    location **drops** by $1000 * (-0.1066) = $106.6 in value.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**这意味着：**当其他因素保持不变时，如果每人犯罪率**增加**1个百分点，该位置的中位数住房价格将**下降**$1000 * (-0.1066)
    = $106.6。'
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This article unveiled the details of implementing Linear Regression in Python,
    going beyond just calling high level `scikit-learn` functions.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 本文揭示了在Python中实现线性回归的详细过程，超越了仅仅调用高阶`scikit-learn`函数。
- en: We looked into the target of regression — minimizing the cost function, and
    wrote the cost function in Python.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们研究了回归的目标——最小化成本函数，并在Python中编写了成本函数。
- en: We broke down Gradient Descent process step by step.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们逐步分解了梯度下降过程。
- en: We created plotting functions to visualize the training process and assessing
    the results.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建了绘图函数来可视化训练过程并评估结果。
- en: We discussed ways to improve model performance, and found out that LASSO Regression
    achieved the lowest test MSE for our problem.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了提高模型性能的方法，并发现LASSO回归在我们尝试的模型中取得了最低的测试均方误差（MSE）。
- en: Lastly, we used one predictor as an example to illustrate how the training result
    should be interpreted.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用一个预测变量作为示例来说明如何解释训练结果。
- en: References
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Ng, *Supervised Machine Learning: Regression and Classification* (2022),
    [https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] A. Ng, *监督式机器学习：回归与分类*（2022年），[https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning)'
- en: '[2] D. Harrison and D. L. Rubinfeld, *Hedonic Housing Prices and the Demand
    for Clean Air* (1978), [https://www.law.berkeley.edu/files/Hedonic.PDF](https://www.law.berkeley.edu/files/Hedonic.PDF)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] D. Harrison 和 D. L. Rubinfeld, *居住性住房价格与对清洁空气的需求*（1978年），[https://www.law.berkeley.edu/files/Hedonic.PDF](https://www.law.berkeley.edu/files/Hedonic.PDF)'
- en: '[3] *Linear Regression (Python Implementation)* (2024), [https://www.geeksforgeeks.org/linear-regression-python-implementation/](https://www.geeksforgeeks.org/linear-regression-python-implementation/)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] *线性回归（Python实现）*（2024），[https://www.geeksforgeeks.org/linear-regression-python-implementation/](https://www.geeksforgeeks.org/linear-regression-python-implementation/)'
- en: '[4] *Why We Perform Feature Scaling In Machine Learning* (2022), [https://www.youtube.com/watch?v=CFA7OFYDBQY](https://www.youtube.com/watch?v=CFA7OFYDBQY)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] *为什么我们要在机器学习中进行特征缩放*（2022），[https://www.youtube.com/watch?v=CFA7OFYDBQY](https://www.youtube.com/watch?v=CFA7OFYDBQY)'
- en: '[5] X. Sicotte, *Lasso regression: implementation of coordinate descent* (2018),
    [https://xavierbourretsicotte.github.io/lasso_implementation.html](https://xavierbourretsicotte.github.io/lasso_implementation.html)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] X. Sicotte, *Lasso回归：坐标下降法实现*（2018），[https://xavierbourretsicotte.github.io/lasso_implementation.html](https://xavierbourretsicotte.github.io/lasso_implementation.html)'
- en: '[6] D@Kg, *Coordinate Descent for LASSO & Normal Regression* (2022), [https://www.kaggle.com/code/ddatad/coordinate-descent-for-lasso-normal-regression/notebook](https://www.kaggle.com/code/ddatad/coordinate-descent-for-lasso-normal-regression/notebook)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] D@Kg, *LASSO与普通回归的坐标下降法*（2022），[https://www.kaggle.com/code/ddatad/coordinate-descent-for-lasso-normal-regression/notebook](https://www.kaggle.com/code/ddatad/coordinate-descent-for-lasso-normal-regression/notebook)'
- en: '[7] Fairlearn, *Revisiting the Boston Housing Dataset*, [https://fairlearn.org/main/user_guide/datasets/boston_housing_data.html#revisiting-the-boston-housing-dataset](https://fairlearn.org/main/user_guide/datasets/boston_housing_data.html#revisiting-the-boston-housing-dataset)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Fairlearn，*重新审视波士顿房价数据集*，[https://fairlearn.org/main/user_guide/datasets/boston_housing_data.html#revisiting-the-boston-housing-dataset](https://fairlearn.org/main/user_guide/datasets/boston_housing_data.html#revisiting-the-boston-housing-dataset)'
- en: '[8] V. Rathod, *All about Boston Housing* (2020), [https://rpubs.com/vidhividhi/LRversusDT](https://rpubs.com/vidhividhi/LRversusDT)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] V. Rathod, *关于波士顿房价数据集的所有知识*（2020），[https://rpubs.com/vidhividhi/LRversusDT](https://rpubs.com/vidhividhi/LRversusDT)'
- en: '[9] A. Gupta, *Regularization in Machine Learning* (2023), [https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/](https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] A. Gupta, *机器学习中的正则化*（2023），[https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/](https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/)'
- en: '[10] The University of Melbourne, *Rescaling explanatory variables in linear
    regression*, [https://scc.ms.unimelb.edu.au/resources/reporting-statistical-inference/rescaling-explanatory-variables-in-linear-regression](https://scc.ms.unimelb.edu.au/resources/reporting-statistical-inference/rescaling-explanatory-variables-in-linear-regression)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 墨尔本大学，*线性回归中的解释变量重新缩放*，[https://scc.ms.unimelb.edu.au/resources/reporting-statistical-inference/rescaling-explanatory-variables-in-linear-regression](https://scc.ms.unimelb.edu.au/resources/reporting-statistical-inference/rescaling-explanatory-variables-in-linear-regression)'
