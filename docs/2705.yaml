- en: Predict Housing Price using Linear Regression in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/predict-housing-price-using-linear-regression-in-python-bfc0fcfff640?source=collection_archive---------2-----------------------#2024-11-06](https://towardsdatascience.com/predict-housing-price-using-linear-regression-in-python-bfc0fcfff640?source=collection_archive---------2-----------------------#2024-11-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A walk-through of cost computation, gradient descent, and regularization using
    Boston Housing dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@yqelisa?source=post_page---byline--bfc0fcfff640--------------------------------)[![Elisa
    Yao](../Images/bf38cf250ae51db4f9880cb907b2f854.png)](https://medium.com/@yqelisa?source=post_page---byline--bfc0fcfff640--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bfc0fcfff640--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bfc0fcfff640--------------------------------)
    [Elisa Yao](https://medium.com/@yqelisa?source=post_page---byline--bfc0fcfff640--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bfc0fcfff640--------------------------------)
    ·14 min read·Nov 6, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression seems old and naive when Large Language Models (LLMs) dominate
    people’s attention through their sophistication recently. Is there still a point
    of understanding it?
  prefs: []
  type: TYPE_NORMAL
- en: My answer is “Yes”, because it’s a building block of more complex models, including
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a Linear Regression model can be as easy as running 3 lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: However, this doesn’t show us the structure of the model. To produce optimal
    modeling results, we need to **understand what goes on behind the scenes**. In
    this article, I’ll break down the **process of implementing Linear Regression
    in Python** using a simple dataset known as “Boston Housing”, step by step.
  prefs: []
  type: TYPE_NORMAL
- en: What is Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Linear** — when plotted in a 2-dimensional space, if the dots showing the
    relationship of predictor *x* and predicted variable *y* scatter along a straight
    line, then we think this relationship can be represented by this line.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression** — a statistical method for estimating the relationship between
    one or more predictors (independent variables) and a predicted (dependent variable).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear Regression** describes the predicted variable as a linear combination
    of the predictors. The line that abstracts this relationship is called **line
    of best fit**, see the red straight line in the below figure as an example.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5b975352a38e52b607f7ad54a6f9c56.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Linear Relationship and Line of Best Fit (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Data Description
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To keep our goal focused on illustrating the Linear Regression steps in Python,
    I picked the Boston Housing dataset, which is:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Small** — makes debugging easy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simple** — so we spend less time in understanding the data or feature engineering'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clean** — requires minimum data cleaning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The dataset was first curated in [Harrison and Rubinfeld’s (1978) study of
    Hedonic Housing Prices](https://www.law.berkeley.edu/files/Hedonic.PDF). It originally
    has:'
  prefs: []
  type: TYPE_NORMAL
- en: 13 predictors — including demographic attributes, environmental attributes,
    and economics attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- CRIM — per capita crime rate by town'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- ZN — proportion of residential land zoned for lots over 25,000 sq.ft.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- INDUS — proportion of non-retail business acres per town.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- CHAS — Charles River dummy variable (1 if tract bounds river; 0 otherwise)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- NOX — nitric oxides concentration (parts per 10 million)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- RM — average number of rooms per dwelling'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- AGE — proportion of owner-occupied units built prior to 1940'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- DIS — weighted distances to five Boston employment centres'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- RAD — index of accessibility to radial highways'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- TAX — full-value property-tax rate per $10,000'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- PTRATIO — pupil-teacher ratio by town'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- LSTAT — % lower status of the population'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1 target (with variable name “MEDV”) — median value of owner-occupied homes
    in $1000's, at a specific location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can download the raw data [here](https://faculty.tuck.dartmouth.edu/images/uploads/faculty/business-analytics/Boston_Housing.xlsx).
  prefs: []
  type: TYPE_NORMAL
- en: 'Load data into Python using `pandas`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'See the dataset’s number of rows (observations) and columns (variables):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The **modeling problem** of our exercise is: given the attributes of a location,
    try to predict the **median housing price of this location**.'
  prefs: []
  type: TYPE_NORMAL
- en: We store the target variable and predictors using 2 separate objects, *x* and
    *y*, following math and ML notations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Visualize** the dataset by histogram and scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/55a4db43502733e11e9da189a5afd375.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output of histogram and scatter plot (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The point of visualizing the variables is to see if any **transformation** is
    needed for the variables, and identify the **type of relationship** between individual
    variables and target. For example, the target may have a linear relationship with
    some predictors, but polynomial relationship with others. This further **infers
    which models to use** for solving the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Cost Computation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How well the model captures the relationship between the predictors and the
    target can be **measured by** how much the predicted results **deviate** from
    the ground truth. The function that quantifies this deviation is called **Cost
    Function**.
  prefs: []
  type: TYPE_NORMAL
- en: The smaller the **cost** is, the better the model captures the relationship
    the predictors and the target. This means, mathematically, the **model training**
    process aims to **minimize** the result of cost function.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different cost functions that can be used for regression problems:
    Sum of Squared Errors (SSE), Mean Squared Error (MSE), Mean Absolute Error (MAE)…'
  prefs: []
  type: TYPE_NORMAL
- en: '**MSE** is the most popular cost function used for Linear Regression, and is
    the default cost function in many statistical packages in R and Python. Here’s
    its math expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97705ac06e7953532935e51ec78ceb0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note: The 2 in the denominator is there to make calculation neater.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use MSE as our cost function, we can create the following function in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Gradient** — the slope of the tangent line at a certain point of the function.
    In multivariable calculus, gradient is a vector that points in the direction of
    the steepest ascent at a certain point.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Descent** — moving towards the minimum of the cost function.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient Descent** — a method that iteratively adjusts the parameters in
    small steps, guided by the gradient, to reach the lowest point of a function.
    It is a way to **numerically** reach the desired parameters for Linear Regression.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, there’s a way to **analytically** solve for the optimal parameters
    — Ordinary Least Squares (OLS). See [this GeekforGeeks article](https://www.geeksforgeeks.org/linear-regression-python-implementation/)
    for details of how to implement it in Python. In practice, it does not scale as
    well as the Gradient Descent approach because of higher computational complexity.
    Therefore, we use Gradient Descent in our case.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each iteration of the Gradient Descent process:'
  prefs: []
  type: TYPE_NORMAL
- en: The **gradients** determine the **direction** of the descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **learning rate** determines the **scale** of the descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To calculate the gradients, we need to understand that there are 2 parameters
    that alter the value of the cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '***w*** — the vector of each predictor’s weight'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***b*** — the bias term'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: because the values of all the observations (*xⁱ*) don’t change over the
    training process, they contribute to the computation result, but are constants,
    not variables.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the gradients are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00ac51d4afef3f89656324302f167105.png)![](../Images/0e59a669b46d0a40d02f8220f06f2760.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Correspondingly, we create the following function in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Using this function, we get the gradients of the cost function, and with a set
    learning rate, update the parameters iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since it’s a loop logically, we need to define the stopping condition, which
    could be any of:'
  prefs: []
  type: TYPE_NORMAL
- en: We reach the set **number of iterations**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **cost** gets to below a certain threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **improvement** drops below a certain threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we choose the number of iterations as the stopping condition, we can write
    the Gradient Descent process to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply it to our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize the process of cost decreases as the number iteration increases
    using the below function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the the plot for our training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d76a91de9d10c5e4c3987e31c90b8dbf.png)'
  prefs: []
  type: TYPE_IMG
- en: How the value of cost function changes over the iterations (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Making predictions is essentially applying the model to our dataset of interest
    to get the output values. These values are **what the model “thinks”** **the target
    value should be, given a set of predictor values**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we apply the linear function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the prediction results using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Result Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How do we get an idea of the model performance?
  prefs: []
  type: TYPE_NORMAL
- en: 'One way is through the cost function, as stated earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the MSE on our test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Another way is more intuitive — visualizing the predicted values against the
    actual values. If the model makes perfect predictions, then each element of `y_test`
    should always equal to the corresponding element of `y_pred`. If we plot `y_test`
    on *x* axis, `y_pred` on *y* axis, the dots will form a **diagonal** straight
    line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s our custom plotting function for the comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying to our training result, we find that the dots look nothing like
    a straight line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d13f49c7be107a1c26dfcda35ddf3ade.png)'
  prefs: []
  type: TYPE_IMG
- en: Scatter plot of predicted values against actual values (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'This should get us thinking: how can we improve the model’s performance?'
  prefs: []
  type: TYPE_NORMAL
- en: Feature Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Gradient Descent process is sensitive to the scale of features. As shown
    in the contour plot on the left, when the learning rate of different features
    are kept the same, then if the features are in different scales, the path of reaching
    global minimum may jump back and forth along the cost function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc27bf98766041306bdc54fa915c5edd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The path towards global minimum of the cost function when features are not-scaled
    vs scaled (Source: [DataMListic](https://www.youtube.com/watch?v=CFA7OFYDBQY))'
  prefs: []
  type: TYPE_NORMAL
- en: After scaling all the features to the same ranges, we can observe a smoother
    and more straight-forward path to global minimum of the cost function.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways to conduct feature scaling, and here we choose **Standardization**
    to turn all the features to have mean of 0 and standard deviation of 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how to standardize features in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we conduct Gradient Descent on the standardized dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We get a steeper and smoother decline of cost before 200 iterations, compared
    to the previous round of training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66636c59252b21acb1e9bde42290616f.png)'
  prefs: []
  type: TYPE_IMG
- en: Cost by each iteration on the standardized dataset (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'If we plot the predicted versus actual values again, we see the dots look much
    closer to a straight line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db67ab25d03c0b01f74ae4f4b25ecdb3.png)'
  prefs: []
  type: TYPE_IMG
- en: Scatter plot of predicted values against actual values on standardized dataset
    (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'To quantify the model performance on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We see an improvement from MSE of 132.84 to 35.66! Can we do more to improve
    the model?
  prefs: []
  type: TYPE_NORMAL
- en: Regularization — Ridge Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We notice that in the last round of training, the training MSE is 9.96, and
    the testing MSE is 35.66\. Can we push the test set performance to be closer to
    training set?
  prefs: []
  type: TYPE_NORMAL
- en: Here comes **Regularization**. It penalizes large parameters to prevent the
    model from being too specific to the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are mainly 2 popular ways of regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**L1 Regularization** — uses the L1 norm (**absolute values**, a.k.a. “Manhattan
    norm”) of the weights as the penalty term.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L2 Regularization** — uses the L2 norm (**squared values**, a.k.a. “Euclidean
    norm”) of the weights as the penalty term.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s first try **Ridge Regression** which uses L2 regularization as our new
    version of model. Its Gradient Descent process is easier to understand than **LASSO
    Regression**, which uses L1 regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost function with L1 regularization looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30ba14f3eb405c8a1f5fbf3276207f9d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Lambda** controls the degree of penalty. When lambda is high, the level of
    penalty is high, then the model leans to underfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can turn the calculation into the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'For the Gradient Descent process, we use the following function to compute
    the gradients with regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Combine the two steps together, we get the following Gradient Descent function
    for Ridge Regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model on our standardized dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The training cost is slightly higher than our previous version of model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning curve looks very similar to the one from the previous round:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d49ce9629259ad7e2859125a46332d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Cost by each iteration for Ridge Regression (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The predicted vs actual values plot looks almost identical to what we got from
    the previous round:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3083a8f8327e8491a9dafdf20291a7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Scatter plot of predicted values against actual values for Ridge Regression
    (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We got test set MSE of 35.69, which is slightly higher than the one without
    regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization — LASSO Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, let’s try out LASSO Regression! LASSO stands for **Least Absolute Shrinkage
    and Selection Operator**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the cost function with L2 regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b3bf962171d9ca08e93bb2c114abaed.png)'
  prefs: []
  type: TYPE_IMG
- en: What’s tricky about the training process of LASSO Regression, is that the derivative
    of the absolute function is undefined at *w=0*. Therefore, **Coordinate Descent**
    is used in practice for LASSO Regression. It focuses on one coordinate at a time
    to find the minimum, and then switch to the next coordinate.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s how we implement it in Python, inspired by [Sicotte (2018)](https://xavierbourretsicotte.github.io/lasso_implementation.html)
    and [D@Kg’s notebook (2022)](https://www.kaggle.com/code/ddatad/coordinate-descent-for-lasso-normal-regression/notebook).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the soft threshold function, which is the solution to the
    single variable optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Second, calculate the residuals of the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the residual to calculate rho, which is the subderivative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Put everything together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply it to our training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The training process converged drastically, compared to Gradient Descent on
    Ridge Regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2954df2b315ee435c1882247a520c4da.png)'
  prefs: []
  type: TYPE_IMG
- en: Cost by each iteration for LASSO Regression (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the training result is not significantly improved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d0a3769b84987abd051000bb9c5c56a.png)'
  prefs: []
  type: TYPE_IMG
- en: Scatter plot of predicted values against actual values for LASSO Regression
    (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, we achieved MSE of 34.40, which is the lowest among the methods
    we tried.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How do we interpret the model training results using human language? Let’s use
    the result of LASSO Regression as an example, since it has the best performance
    among the model variations we tried out.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the **weights** and the **bias** by printing the `w_out` and `b_out`
    we got in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In our case, there are 13 predictors, so this dataset has 13 dimensions. In
    each dimension, we can plot the predictor `x_i` against the target `y` as a scatterplot.
    The regression line’s **slope** is the weight `w_i`.
  prefs: []
  type: TYPE_NORMAL
- en: In details, the first dimension is *“CRIM — per capita crime rate by town”*,
    and our `w_1` is -0.8664\. This means, each unit of **increase** in `x_i`, `y`
    is expected to **decrease** by -0.8664 unit.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have **scaled** our dataset before we run the training process,
    so now we need to **reverse** that process to get the intuitive relationship between
    the predictor *“per capita crime rate by town”* and our target variable *“median
    value of owner-occupied homes in $1000’s, at a specific location”*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reverse the scaling, we need to get the vector of scales:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we find the scale we used for our first predictor: 8.1278\. We divide
    the weight of -0.8664 by scale or 8.1278 to get **-0.1066**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**This means:** when all other factors remains the same, if the per capita
    crime rate **increases** by 1 percentage point, the medium housing price of that
    location **drops** by $1000 * (-0.1066) = $106.6 in value.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article unveiled the details of implementing Linear Regression in Python,
    going beyond just calling high level `scikit-learn` functions.
  prefs: []
  type: TYPE_NORMAL
- en: We looked into the target of regression — minimizing the cost function, and
    wrote the cost function in Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We broke down Gradient Descent process step by step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We created plotting functions to visualize the training process and assessing
    the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discussed ways to improve model performance, and found out that LASSO Regression
    achieved the lowest test MSE for our problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, we used one predictor as an example to illustrate how the training result
    should be interpreted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] A. Ng, *Supervised Machine Learning: Regression and Classification* (2022),
    [https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] D. Harrison and D. L. Rubinfeld, *Hedonic Housing Prices and the Demand
    for Clean Air* (1978), [https://www.law.berkeley.edu/files/Hedonic.PDF](https://www.law.berkeley.edu/files/Hedonic.PDF)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] *Linear Regression (Python Implementation)* (2024), [https://www.geeksforgeeks.org/linear-regression-python-implementation/](https://www.geeksforgeeks.org/linear-regression-python-implementation/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] *Why We Perform Feature Scaling In Machine Learning* (2022), [https://www.youtube.com/watch?v=CFA7OFYDBQY](https://www.youtube.com/watch?v=CFA7OFYDBQY)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] X. Sicotte, *Lasso regression: implementation of coordinate descent* (2018),
    [https://xavierbourretsicotte.github.io/lasso_implementation.html](https://xavierbourretsicotte.github.io/lasso_implementation.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] D@Kg, *Coordinate Descent for LASSO & Normal Regression* (2022), [https://www.kaggle.com/code/ddatad/coordinate-descent-for-lasso-normal-regression/notebook](https://www.kaggle.com/code/ddatad/coordinate-descent-for-lasso-normal-regression/notebook)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Fairlearn, *Revisiting the Boston Housing Dataset*, [https://fairlearn.org/main/user_guide/datasets/boston_housing_data.html#revisiting-the-boston-housing-dataset](https://fairlearn.org/main/user_guide/datasets/boston_housing_data.html#revisiting-the-boston-housing-dataset)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] V. Rathod, *All about Boston Housing* (2020), [https://rpubs.com/vidhividhi/LRversusDT](https://rpubs.com/vidhividhi/LRversusDT)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] A. Gupta, *Regularization in Machine Learning* (2023), [https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/](https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] The University of Melbourne, *Rescaling explanatory variables in linear
    regression*, [https://scc.ms.unimelb.edu.au/resources/reporting-statistical-inference/rescaling-explanatory-variables-in-linear-regression](https://scc.ms.unimelb.edu.au/resources/reporting-statistical-inference/rescaling-explanatory-variables-in-linear-regression)'
  prefs: []
  type: TYPE_NORMAL
