- en: Does Semi-Supervised Learning Help to Train Better Models?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/does-semi-supervised-learning-help-to-train-better-models-338283d1f4e9?source=collection_archive---------4-----------------------#2024-09-09](https://towardsdatascience.com/does-semi-supervised-learning-help-to-train-better-models-338283d1f4e9?source=collection_archive---------4-----------------------#2024-09-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Evaluating how semi-supervised learning can leverage unlabeled data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@reinhard.sellmair?source=post_page---byline--338283d1f4e9--------------------------------)[![Reinhard
    Sellmair](../Images/0aaf2ae9c27f551f9ef6921d110318b5.png)](https://medium.com/@reinhard.sellmair?source=post_page---byline--338283d1f4e9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--338283d1f4e9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--338283d1f4e9--------------------------------)
    [Reinhard Sellmair](https://medium.com/@reinhard.sellmair?source=post_page---byline--338283d1f4e9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--338283d1f4e9--------------------------------)
    ·8 min read·Sep 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91de59efdd9bf944d6eab57fe896ce4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author — created with Image Creator in Bing
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common challenges Data Scientists faces is the lack of enough
    labelled data to train a reliable and accurate model. Labelled data is essential
    for supervised learning tasks, such as classification or regression. However,
    obtaining labelled data can be costly, time-consuming, or impractical in many
    domains. On the other hand, unlabeled data is usually easy to collect, but they
    do not provide any direct input to train a model.
  prefs: []
  type: TYPE_NORMAL
- en: How can we make use of unlabeled data to improve our supervised learning models?
    This is where semi-supervised learning comes into play. Semi-supervised learning
    is a branch of machine learning that combines labelled and unlabeled data to train
    a model that can perform better than using labelled data alone. The intuition
    behind semi-supervised learning is that unlabeled data can provide useful information
    about the underlying structure, distribution, and diversity of the data, which
    can help the model generalize better to new and unseen examples.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, I present three semi-supervised learning methods that can be applied
    to different types of data and tasks. I will also evaluate their performance on
    a real-world dataset and compare them with the baseline of using only labelled
    data.
  prefs: []
  type: TYPE_NORMAL
- en: What is semi-supervised learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Semi-supervised learning is a type of machine learning that uses both labelled
    and unlabeled data to train a model. Labelled data are examples that have a known
    output or target variable, such as the class label in a classification task or
    the numerical value in a regression task. Unlabeled data are examples that do
    not have a known output or target variable. Semi-supervised learning can leverage
    the large amount of unlabeled data that is often available in real-world problems,
    while also making use of the smaller amount of labelled data that is usually more
    expensive or time-consuming to obtain.
  prefs: []
  type: TYPE_NORMAL
- en: The underlying idea to use unlabeled data to train a supervised learning method
    is to label this data via supervised or unsupervised learning methods. Although
    these labels are most likely not as accurate as actual labels, having a significant
    amount of this data can improve the performance of a supervised-learning method
    compared to training this method on labelled data only.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scikit-learn package provides three semi-supervised learning methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Self-training](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.SelfTrainingClassifier.html#sklearn.semi_supervised.SelfTrainingClassifier):
    a classifier is first trained on labelled data only to predict labels of unlabeled
    data. In the next iteration, another classifier is training on the labelled data
    and on prediction from the unlabeled data which had high confidence. This procedure
    is repeated until no new labels with high confidence are predicted or a maximum
    number of iterations is reached.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Label-propagation](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation):
    a graph is created where nodes represent data points and edges represent similarities
    between them. Labels are iteratively propagated through the graph, allowing the
    algorithm to assign labels to unlabeled data points based on their connections
    to labelled data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Label-spreading](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading):
    uses the same concept as label-propagation. The difference is that label spreading
    uses a soft assignment, where the labels are updated iteratively based on the
    similarity between data points. This method may also “overwrite” labels of the
    labelled dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To evaluate these methods I used a [diabetes prediction](https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset)
    dataset which contains features of patient data like age and BMI together with
    a label describing if the patient has diabetes. This dataset contains 100,000
    records which I randomly divided into 80,000 training, 10,000 validation and 10,000
    test data. To analyze how effective the learning methods are with respect to the
    amount of labelled data, I split the training data into a labelled and an unlabeled
    set, where the label size describes how many samples are labelled.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07314d87ce62458607a40ba79ba1f7c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Partition of dataset (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: I used the validation data to assess different parameter settings and used the
    test data to evaluate the performance of each method after parameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: I used XG Boost for prediction and F1 score to evaluate the prediction performance.
  prefs: []
  type: TYPE_NORMAL
- en: Baseline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The baseline was used to compare the self-learning algorithms against the case
    of not using any unlabeled data. Therefore, I trained XGB on labelled data sets
    of different size and calculate the F1 score on the validation data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f59973414d79ee134095a17a2129841.png)'
  prefs: []
  type: TYPE_IMG
- en: Baseline score (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: The results showed that the F1 score is quite low for training sets of less
    than 100 samples, then steadily improves to a score of 79% until a sample size
    of 1,000 is reached. Higher sample sizes hardly improved the F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: Self-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Self-training is using multiple iteration to predict labels of unlabeled data
    which will then be used in the next iteration to train another model. Two methods
    can be used to select predictions to be used as labelled data in the next iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Threshold (default): all predictions with a confidence above a threshold are
    selected'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'K best: the predictions of the k highest confidence are selected'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I evaluated the default parameters (ST Default) and tuned the threshold (ST
    Thres Tuned) and the k best (ST KB Tuned) parameter based on the validation dataset.
    The prediction results of these model were evaluated on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b49779f4c24e39f253af5763820ff94.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-learning score (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: For small sample sizes (<100) the default parameters (red line) performed worse
    than the baseline (blue line). For higher sample sizes slightly better F1 scores
    than the baseline were achieved. Tuning the threshold (green line) brought a significant
    improvement, for example at a label size of 200 the baseline F1 score was 57%
    while the algorithm with tuned thresholds achieved 70%. With one exception at
    a label size of 30, tuning the K best value (purple line) resulted in almost the
    same performance as the baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Label Propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Label propagation has two built-in kernel methods: RBF and KNN. The RBF kernel
    produces a fully connected graph using a dense matrix, which is memory intensive
    and time consuming for large datasets. To consider memory constraints, I only
    used a maximum training size of 3,000 for the RBF kernel. The KNN kernel uses
    a more memory friendly sparse matrix, which allowed me to fit on the whole training
    data of up to 80,000 samples. The results of these two kernel methods are compared
    in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22e25ebabdc3352de89144f386ad9c7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Label propagation score (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: The graph shows the F1 score on the test dataset of different label propagation
    methods as a function of the label size. The blue line represents the baseline,
    which is the same as for self-training. The red line represents the label propagation
    with default parameters, which clearly underperforms the baseline for all label
    sizes. The green line represents the label propagation with RBF kernel and tuned
    parameter gamma. Gamma defines how far the influence of a single training example
    reaches. The tuned RBF kernel performed better than the baseline for small label
    sizes (<=100) but worse for larger label sizes. The purple line represents the
    label propagation with KNN kernel and tuned parameter k, which determines the
    number of nearest neighbors to use. The KNN kernel had a similar performance as
    the RBF kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Label Spreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Label spreading is a similar approach to label propagation, but with an additional
    parameter alpha that controls how much an instance should adopt the information
    of its neighbors. Alpha can range from 0 to 1, where 0 means that the instance
    keeps its original label and 1 means that it completely adopts the labels of its
    neighbors. I also tuned the RBF and KNN kernel methods for label spreading. The
    results of label spreading are shown in the next graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/823cdd15871e1964d993702c90b6e838.png)'
  prefs: []
  type: TYPE_IMG
- en: Label spreading score (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: The results of label spreading were very similar to those of label propagation,
    with one notable exception. The RBF kernel method for label spreading has a lower
    test score than the baseline for all label sizes, not only for small ones. This
    suggests that the “overwriting” of labels by the neighbors’ labels has a rather
    negative effect for this dataset, which might have only few outliers or noisy
    labels. On the other hand, the KNN kernel method is not affected by the alpha
    parameter. It seems that this parameter is only relevant for the RBF kernel method.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison of all methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, I compared all methods with their best parameters against each other.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e608c0363175bf506fe654cba909a0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of best scores (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: The graph shows the test score of different semi-supervised learning methods
    as a function of the label size. Self-training outperforms the baseline, as it
    leverages the unlabeled data well. Label propagation and label spreading only
    beat the baseline for small label sizes and perform worse for larger label sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The results may significantly vary for different datasets, classifier methods,
    and metrics. The performance of semi-supervised learning depends on many factors,
    such as the quality and quantity of the unlabeled data, the choice of the base
    learner, and the evaluation criterion. Therefore, one should not generalize these
    findings to other settings without proper testing and validation.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in exploring more about semi-supervised learning, you
    are welcome to check out my git repo and experiment on your own. You can find
    the code and data for this project [here](https://github.com/ReinhardSellmair/ssl?tab=readme-ov-file).
  prefs: []
  type: TYPE_NORMAL
- en: One thing that I learned from this project is that parameter tuning was important
    to significantly improve the performance of these methods. With optimized parameters,
    self-training performed better than the baseline for any label size and reached
    better F1 scores of up to 13%! Label propagation and label spreading only turned
    out to improve the performance for very small sample size, but the user must be
    very careful not to get worse results compared to not using any semi-supervised
    learning method.
  prefs: []
  type: TYPE_NORMAL
