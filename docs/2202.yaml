- en: Does Semi-Supervised Learning Help to Train Better Models?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督学习是否有助于训练更好的模型？
- en: 原文：[https://towardsdatascience.com/does-semi-supervised-learning-help-to-train-better-models-338283d1f4e9?source=collection_archive---------4-----------------------#2024-09-09](https://towardsdatascience.com/does-semi-supervised-learning-help-to-train-better-models-338283d1f4e9?source=collection_archive---------4-----------------------#2024-09-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/does-semi-supervised-learning-help-to-train-better-models-338283d1f4e9?source=collection_archive---------4-----------------------#2024-09-09](https://towardsdatascience.com/does-semi-supervised-learning-help-to-train-better-models-338283d1f4e9?source=collection_archive---------4-----------------------#2024-09-09)
- en: Evaluating how semi-supervised learning can leverage unlabeled data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估半监督学习如何利用未标注数据
- en: '[](https://medium.com/@reinhard.sellmair?source=post_page---byline--338283d1f4e9--------------------------------)[![Reinhard
    Sellmair](../Images/0aaf2ae9c27f551f9ef6921d110318b5.png)](https://medium.com/@reinhard.sellmair?source=post_page---byline--338283d1f4e9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--338283d1f4e9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--338283d1f4e9--------------------------------)
    [Reinhard Sellmair](https://medium.com/@reinhard.sellmair?source=post_page---byline--338283d1f4e9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@reinhard.sellmair?source=post_page---byline--338283d1f4e9--------------------------------)[![Reinhard
    Sellmair](../Images/0aaf2ae9c27f551f9ef6921d110318b5.png)](https://medium.com/@reinhard.sellmair?source=post_page---byline--338283d1f4e9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--338283d1f4e9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--338283d1f4e9--------------------------------)
    [Reinhard Sellmair](https://medium.com/@reinhard.sellmair?source=post_page---byline--338283d1f4e9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--338283d1f4e9--------------------------------)
    ·8 min read·Sep 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--338283d1f4e9--------------------------------)
    ·阅读时长8分钟·2024年9月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/91de59efdd9bf944d6eab57fe896ce4a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91de59efdd9bf944d6eab57fe896ce4a.png)'
- en: Image by the author — created with Image Creator in Bing
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — 使用必应的图像生成器创建
- en: One of the most common challenges Data Scientists faces is the lack of enough
    labelled data to train a reliable and accurate model. Labelled data is essential
    for supervised learning tasks, such as classification or regression. However,
    obtaining labelled data can be costly, time-consuming, or impractical in many
    domains. On the other hand, unlabeled data is usually easy to collect, but they
    do not provide any direct input to train a model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家面临的最常见挑战之一是缺乏足够的标注数据来训练一个可靠且准确的模型。标注数据对于监督学习任务（如分类或回归）至关重要。然而，在许多领域，获取标注数据可能成本高昂、耗时或不可行。另一方面，未标注数据通常容易收集，但它们并不能直接用于训练模型。
- en: How can we make use of unlabeled data to improve our supervised learning models?
    This is where semi-supervised learning comes into play. Semi-supervised learning
    is a branch of machine learning that combines labelled and unlabeled data to train
    a model that can perform better than using labelled data alone. The intuition
    behind semi-supervised learning is that unlabeled data can provide useful information
    about the underlying structure, distribution, and diversity of the data, which
    can help the model generalize better to new and unseen examples.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何利用未标注数据来提高监督学习模型的性能？这正是半监督学习发挥作用的地方。半监督学习是机器学习的一个分支，它结合了标注数据和未标注数据，以训练一个比仅使用标注数据表现更好的模型。半监督学习背后的直觉是，未标注数据可以提供关于数据潜在结构、分布和多样性的有用信息，这有助于模型更好地泛化到新的和未见过的样本。
- en: In this post, I present three semi-supervised learning methods that can be applied
    to different types of data and tasks. I will also evaluate their performance on
    a real-world dataset and compare them with the baseline of using only labelled
    data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我介绍了三种可以应用于不同类型数据和任务的半监督学习方法。我还将评估它们在实际数据集上的表现，并将其与仅使用标注数据的基准进行比较。
- en: What is semi-supervised learning?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是半监督学习？
- en: Semi-supervised learning is a type of machine learning that uses both labelled
    and unlabeled data to train a model. Labelled data are examples that have a known
    output or target variable, such as the class label in a classification task or
    the numerical value in a regression task. Unlabeled data are examples that do
    not have a known output or target variable. Semi-supervised learning can leverage
    the large amount of unlabeled data that is often available in real-world problems,
    while also making use of the smaller amount of labelled data that is usually more
    expensive or time-consuming to obtain.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习是一种机器学习方法，它同时利用已标记数据和未标记数据来训练模型。已标记数据是指那些已知输出或目标变量的样本，比如分类任务中的类别标签或回归任务中的数值。未标记数据是指那些没有已知输出或目标变量的样本。半监督学习可以利用在实际问题中通常大量存在的未标记数据，同时也利用较少的已标记数据，而后者通常更加昂贵或耗时。
- en: The underlying idea to use unlabeled data to train a supervised learning method
    is to label this data via supervised or unsupervised learning methods. Although
    these labels are most likely not as accurate as actual labels, having a significant
    amount of this data can improve the performance of a supervised-learning method
    compared to training this method on labelled data only.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用未标记数据来训练监督学习方法的基本思想是通过监督或无监督学习方法给这些数据打上标签。尽管这些标签可能不像实际标签那样准确，但拥有大量此类数据可以提升监督学习方法的表现，相比只用已标记数据进行训练。
- en: 'The scikit-learn package provides three semi-supervised learning methods:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 包提供了三种半监督学习方法：
- en: '[Self-training](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.SelfTrainingClassifier.html#sklearn.semi_supervised.SelfTrainingClassifier):
    a classifier is first trained on labelled data only to predict labels of unlabeled
    data. In the next iteration, another classifier is training on the labelled data
    and on prediction from the unlabeled data which had high confidence. This procedure
    is repeated until no new labels with high confidence are predicted or a maximum
    number of iterations is reached.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自训练](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.SelfTrainingClassifier.html#sklearn.semi_supervised.SelfTrainingClassifier)：首先仅在已标记数据上训练分类器，以预测未标记数据的标签。在下一轮中，另一个分类器在已标记数据和从未标记数据中预测出高置信度标签的结果上进行训练。这个过程会重复进行，直到没有新的高置信度标签被预测，或者达到最大迭代次数为止。'
- en: '[Label-propagation](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation):
    a graph is created where nodes represent data points and edges represent similarities
    between them. Labels are iteratively propagated through the graph, allowing the
    algorithm to assign labels to unlabeled data points based on their connections
    to labelled data.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[标签传播](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation)：构建一个图，其中节点表示数据点，边表示它们之间的相似性。标签通过图进行迭代传播，从而使算法能够根据已标记数据与未标记数据之间的连接关系，将标签分配给未标记数据点。'
- en: '[Label-spreading](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading):
    uses the same concept as label-propagation. The difference is that label spreading
    uses a soft assignment, where the labels are updated iteratively based on the
    similarity between data points. This method may also “overwrite” labels of the
    labelled dataset.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[标签传播](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading)：使用与标签传播相同的概念。区别在于标签扩散使用软分配，在这种方法中，标签是根据数据点之间的相似性进行迭代更新的。这种方法还可能“覆盖”已标记数据集的标签。'
- en: To evaluate these methods I used a [diabetes prediction](https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset)
    dataset which contains features of patient data like age and BMI together with
    a label describing if the patient has diabetes. This dataset contains 100,000
    records which I randomly divided into 80,000 training, 10,000 validation and 10,000
    test data. To analyze how effective the learning methods are with respect to the
    amount of labelled data, I split the training data into a labelled and an unlabeled
    set, where the label size describes how many samples are labelled.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估这些方法，我使用了一个[糖尿病预测](https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset)数据集，该数据集包含患者的年龄、BMI等特征，以及描述患者是否患有糖尿病的标签。该数据集包含100,000条记录，我将其随机分为80,000条训练数据、10,000条验证数据和10,000条测试数据。为了分析在不同标签数据量下学习方法的有效性，我将训练数据分为标签数据集和无标签数据集，其中标签大小表示有多少样本已标记。
- en: '![](../Images/07314d87ce62458607a40ba79ba1f7c9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07314d87ce62458607a40ba79ba1f7c9.png)'
- en: Partition of dataset (image by the author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集划分（图片来源：作者）
- en: I used the validation data to assess different parameter settings and used the
    test data to evaluate the performance of each method after parameter tuning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用验证数据评估了不同参数设置，并使用测试数据评估了各方法在参数调优后的性能。
- en: I used XG Boost for prediction and F1 score to evaluate the prediction performance.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了XGBoost进行预测，并使用F1得分来评估预测性能。
- en: Baseline
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准
- en: 'The baseline was used to compare the self-learning algorithms against the case
    of not using any unlabeled data. Therefore, I trained XGB on labelled data sets
    of different size and calculate the F1 score on the validation data set:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基准模型用于将自学习算法与不使用任何无标签数据的情况进行比较。因此，我在不同大小的标签数据集上训练了XGB，并计算了验证数据集上的F1得分：
- en: '![](../Images/9f59973414d79ee134095a17a2129841.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f59973414d79ee134095a17a2129841.png)'
- en: Baseline score (image by the author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 基准分数（图片来源：作者）
- en: The results showed that the F1 score is quite low for training sets of less
    than 100 samples, then steadily improves to a score of 79% until a sample size
    of 1,000 is reached. Higher sample sizes hardly improved the F1 score.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，对于少于100个样本的训练集，F1得分相对较低，但随着样本量的增加，得分稳定提高，到达1,000个样本时达到了79%的得分。更大的样本量对F1得分的提升几乎没有作用。
- en: Self-learning
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自学习
- en: 'Self-training is using multiple iteration to predict labels of unlabeled data
    which will then be used in the next iteration to train another model. Two methods
    can be used to select predictions to be used as labelled data in the next iteration:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自学习是通过多次迭代来预测无标签数据的标签，这些标签将在下一次迭代中用于训练另一个模型。可以使用两种方法来选择将作为标签数据用于下一次迭代的预测：
- en: 'Threshold (default): all predictions with a confidence above a threshold are
    selected'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阈值（默认）：选择所有置信度超过阈值的预测
- en: 'K best: the predictions of the k highest confidence are selected'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K最优：选择置信度最高的k个预测
- en: 'I evaluated the default parameters (ST Default) and tuned the threshold (ST
    Thres Tuned) and the k best (ST KB Tuned) parameter based on the validation dataset.
    The prediction results of these model were evaluated on the test dataset:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我评估了默认参数（ST Default），并根据验证数据集调优了阈值（ST Thres Tuned）和k最优参数（ST KB Tuned）。这些模型的预测结果在测试数据集上进行了评估：
- en: '![](../Images/4b49779f4c24e39f253af5763820ff94.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b49779f4c24e39f253af5763820ff94.png)'
- en: Self-learning score (image by the author)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 自学习得分（图片来源：作者）
- en: For small sample sizes (<100) the default parameters (red line) performed worse
    than the baseline (blue line). For higher sample sizes slightly better F1 scores
    than the baseline were achieved. Tuning the threshold (green line) brought a significant
    improvement, for example at a label size of 200 the baseline F1 score was 57%
    while the algorithm with tuned thresholds achieved 70%. With one exception at
    a label size of 30, tuning the K best value (purple line) resulted in almost the
    same performance as the baseline.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于小样本量（<100），默认参数（红线）的表现不如基准（蓝线）。对于更大的样本量，模型取得的F1得分稍微优于基准。调整阈值（绿线）带来了显著的提升。例如，在标签大小为200时，基准F1得分为57%，而调整阈值后的算法F1得分为70%。除了标签大小为30时的一个例外，调整K最优值（紫线）几乎与基准表现相同。
- en: Label Propagation
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标签传播
- en: 'Label propagation has two built-in kernel methods: RBF and KNN. The RBF kernel
    produces a fully connected graph using a dense matrix, which is memory intensive
    and time consuming for large datasets. To consider memory constraints, I only
    used a maximum training size of 3,000 for the RBF kernel. The KNN kernel uses
    a more memory friendly sparse matrix, which allowed me to fit on the whole training
    data of up to 80,000 samples. The results of these two kernel methods are compared
    in the following graph:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 标签传播有两种内置的核方法：RBF和KNN。RBF核通过使用密集矩阵生成一个完全连接的图，这在大数据集上需要大量内存且耗时较长。为了考虑内存限制，我只使用了最多3,000个样本进行RBF核的训练。KNN核则使用更节省内存的稀疏矩阵，这使我能够在最多80,000个样本的整个训练数据上进行训练。以下图表展示了这两种核方法的结果比较：
- en: '![](../Images/22e25ebabdc3352de89144f386ad9c7f.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22e25ebabdc3352de89144f386ad9c7f.png)'
- en: Label propagation score (image by the author)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 标签传播得分（图片由作者提供）
- en: The graph shows the F1 score on the test dataset of different label propagation
    methods as a function of the label size. The blue line represents the baseline,
    which is the same as for self-training. The red line represents the label propagation
    with default parameters, which clearly underperforms the baseline for all label
    sizes. The green line represents the label propagation with RBF kernel and tuned
    parameter gamma. Gamma defines how far the influence of a single training example
    reaches. The tuned RBF kernel performed better than the baseline for small label
    sizes (<=100) but worse for larger label sizes. The purple line represents the
    label propagation with KNN kernel and tuned parameter k, which determines the
    number of nearest neighbors to use. The KNN kernel had a similar performance as
    the RBF kernel.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了不同标签传播方法在测试数据集上的F1得分与标签大小的关系。蓝色线表示基准线，与自我训练相同。红色线表示使用默认参数的标签传播，明显低于基准线，且所有标签大小下都表现较差。绿色线表示使用RBF核和调优后的参数gamma的标签传播。Gamma定义了单个训练样本的影响范围。调优后的RBF核在小标签大小（<=100）下表现优于基准线，但在大标签大小下则表现较差。紫色线表示使用KNN核和调优后的参数k的标签传播，k决定了使用多少个最近邻。KNN核的表现与RBF核相似。
- en: Label Spreading
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标签扩展
- en: 'Label spreading is a similar approach to label propagation, but with an additional
    parameter alpha that controls how much an instance should adopt the information
    of its neighbors. Alpha can range from 0 to 1, where 0 means that the instance
    keeps its original label and 1 means that it completely adopts the labels of its
    neighbors. I also tuned the RBF and KNN kernel methods for label spreading. The
    results of label spreading are shown in the next graph:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 标签扩展是一种类似于标签传播的方法，但增加了一个控制实例应该接受多少邻居信息的参数alpha。Alpha的取值范围为0到1，其中0表示实例保持其原始标签，1表示实例完全采用邻居的标签。我还对RBF和KNN核方法进行了调优。标签扩展的结果显示在下图中：
- en: '![](../Images/823cdd15871e1964d993702c90b6e838.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/823cdd15871e1964d993702c90b6e838.png)'
- en: Label spreading score (image by the author)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 标签扩展得分（图片由作者提供）
- en: The results of label spreading were very similar to those of label propagation,
    with one notable exception. The RBF kernel method for label spreading has a lower
    test score than the baseline for all label sizes, not only for small ones. This
    suggests that the “overwriting” of labels by the neighbors’ labels has a rather
    negative effect for this dataset, which might have only few outliers or noisy
    labels. On the other hand, the KNN kernel method is not affected by the alpha
    parameter. It seems that this parameter is only relevant for the RBF kernel method.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 标签传播的结果与标签扩展非常相似，唯一显著的例外是，RBF核方法在标签扩展中的测试得分低于基准得分，且所有标签大小下都如此，不仅仅是小标签大小。这表明，邻居标签的“覆盖”对这个数据集有相当负面的影响，该数据集可能只有少数离群点或噪声标签。另一方面，KNN核方法不受alpha参数的影响。似乎这个参数只与RBF核方法相关。
- en: Comparison of all methods
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所有方法的比较
- en: Next, I compared all methods with their best parameters against each other.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将所有方法的最佳参数进行对比。
- en: '![](../Images/3e608c0363175bf506fe654cba909a0c.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e608c0363175bf506fe654cba909a0c.png)'
- en: Comparison of best scores (image by the author)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳得分比较（图片由作者提供）
- en: The graph shows the test score of different semi-supervised learning methods
    as a function of the label size. Self-training outperforms the baseline, as it
    leverages the unlabeled data well. Label propagation and label spreading only
    beat the baseline for small label sizes and perform worse for larger label sizes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了不同半监督学习方法的测试得分与标签大小的关系。自我训练优于基线，因为它很好地利用了无标签数据。标签传播和标签扩散仅在小标签大小下超过基线，对于较大的标签大小，表现较差。
- en: Conclusion
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The results may significantly vary for different datasets, classifier methods,
    and metrics. The performance of semi-supervised learning depends on many factors,
    such as the quality and quantity of the unlabeled data, the choice of the base
    learner, and the evaluation criterion. Therefore, one should not generalize these
    findings to other settings without proper testing and validation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可能因不同的数据集、分类器方法和评估指标而显著变化。半监督学习的性能依赖于许多因素，例如无标签数据的质量和数量、基本学习器的选择以及评估标准。因此，在没有适当的测试和验证的情况下，不应将这些发现推广到其他设置中。
- en: If you are interested in exploring more about semi-supervised learning, you
    are welcome to check out my git repo and experiment on your own. You can find
    the code and data for this project [here](https://github.com/ReinhardSellmair/ssl?tab=readme-ov-file).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣进一步探索半监督学习，欢迎查看我的Git仓库并自行实验。你可以在[这里](https://github.com/ReinhardSellmair/ssl?tab=readme-ov-file)找到这个项目的代码和数据。
- en: One thing that I learned from this project is that parameter tuning was important
    to significantly improve the performance of these methods. With optimized parameters,
    self-training performed better than the baseline for any label size and reached
    better F1 scores of up to 13%! Label propagation and label spreading only turned
    out to improve the performance for very small sample size, but the user must be
    very careful not to get worse results compared to not using any semi-supervised
    learning method.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我从这个项目中学到的一点是，参数调优对于显著提升这些方法的性能至关重要。通过优化参数，自我训练在任何标签大小下都优于基线，并且F1分数提高了最多13%！标签传播和标签扩散仅在非常小的样本量下有所改进，但用户必须非常小心，以免得到比不使用任何半监督学习方法更差的结果。
