- en: TPUs Are Not for Sale, But Why?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TPU不出售，但为什么？
- en: 原文：[https://towardsdatascience.com/tpus-are-not-for-sale-but-why-5964f87f7a15?source=collection_archive---------4-----------------------#2024-04-30](https://towardsdatascience.com/tpus-are-not-for-sale-but-why-5964f87f7a15?source=collection_archive---------4-----------------------#2024-04-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tpus-are-not-for-sale-but-why-5964f87f7a15?source=collection_archive---------4-----------------------#2024-04-30](https://towardsdatascience.com/tpus-are-not-for-sale-but-why-5964f87f7a15?source=collection_archive---------4-----------------------#2024-04-30)
- en: Opinion
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 观点
- en: An analysis of Google’s unique approach to AI hardware
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google在AI硬件方面的独特做法分析
- en: '[](https://haifeng-jin.medium.com/?source=post_page---byline--5964f87f7a15--------------------------------)[![Haifeng
    Jin](../Images/705d6ecaed975b6376fac19087f2c02c.png)](https://haifeng-jin.medium.com/?source=post_page---byline--5964f87f7a15--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5964f87f7a15--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5964f87f7a15--------------------------------)
    [Haifeng Jin](https://haifeng-jin.medium.com/?source=post_page---byline--5964f87f7a15--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://haifeng-jin.medium.com/?source=post_page---byline--5964f87f7a15--------------------------------)[![Haifeng
    Jin](../Images/705d6ecaed975b6376fac19087f2c02c.png)](https://haifeng-jin.medium.com/?source=post_page---byline--5964f87f7a15--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5964f87f7a15--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5964f87f7a15--------------------------------)
    [Haifeng Jin](https://haifeng-jin.medium.com/?source=post_page---byline--5964f87f7a15--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5964f87f7a15--------------------------------)
    ·10 min read·Apr 30, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5964f87f7a15--------------------------------)
    ·10分钟阅读·2024年4月30日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/26ebc18d628a6a347ee233a287cca807.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26ebc18d628a6a347ee233a287cca807.png)'
- en: Photo by [Dollar Gill](https://unsplash.com/@dollargill?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Dollar Gill](https://unsplash.com/@dollargill?utm_source=medium&utm_medium=referral)
    via [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Nvidia’s stock price has skyrocketed because of its GPU’s dominance in the AI
    hardware market. However, at the same time, TPUs, well-known AI hardware from
    Google, are not for sale. You can only rent virtual machines on Google Cloud to
    use them. Why did Google not join the game of selling AI hardware?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia的股价飙升，因为其GPU在AI硬件市场的主导地位。然而，与此同时，Google的TPU作为知名的AI硬件，并不出售。你只能在Google Cloud上租用虚拟机来使用它们。那么，为什么Google不加入销售AI硬件的游戏呢？
- en: 'DISCLAIMER: The views expressed in this article are solely those of the author
    and do not necessarily reflect the opinions or viewpoints of Google or its affiliates.
    The entirety of the information presented within this article is sourced exclusively
    from publicly available materials.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 免责声明：本文所表达的观点仅代表作者个人意见，并不一定反映Google或其附属公司的意见或观点。本文中所提供的所有信息完全来源于公开材料。
- en: A popular theory
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个流行的理论
- en: One popular theory I heard is that Google wants to attract more customers to
    its cloud services. If they sell it to other cloud service providers, they are
    less competitive in the cloud service market.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我听到的一个流行理论是，Google希望吸引更多客户使用其云服务。如果他们将TPU卖给其他云服务提供商，他们在云服务市场中的竞争力将减弱。
- en: According to cloud service customers, this theory does not make much sense.
    No corporate-level customer wants to be locked to one specific cloud service provider.
    They want to be flexible enough to move to another whenever needed. Otherwise,
    if the provider increases the price, they can do nothing about it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 根据云服务客户的说法，这一理论并不十分合理。没有企业级客户愿意被锁定在某一特定的云服务提供商。他们希望具有足够的灵活性，能够在需要时迁移到其他提供商。否则，如果服务提供商提高价格，他们将无能为力。
- en: If they are locked to Google Cloud for using TPUs, they would rather not use
    it. This is why many customers don’t want to use TPUs. They only started to feel
    less locked in recently when OpenXLA, an intermediate software to access TPUs,
    supported more frameworks like PyTorch.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果TPU只能在Google Cloud上使用，许多企业客户宁愿不使用它。这也是为什么很多客户不愿意使用TPU的原因。直到最近，当OpenXLA（一个中介软件）支持更多框架，如PyTorch时，他们才开始感觉到不那么受限。
- en: So, using TPUs to attract customers to Google Cloud is not a valid reason for
    not selling them. Then, what is the real reason? To answer this question better,
    we must look into how Google started the TPU project.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，使用TPU吸引客户到Google Cloud并不是不出售它们的合理理由。那么，真正的原因是什么呢？为了更好地回答这个问题，我们必须探讨Google是如何启动TPU项目的。
- en: Why did Google start the TPU project?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么Google启动了TPU项目？
- en: The short answer is for proprietary usage. There was a time when GPUs could
    not meet the computing requirements for AI hardware.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的回答是为了专有用途。曾经，GPU无法满足AI硬件的计算需求。
- en: Let’s try to estimate when the TPU project was started. Given it was [first
    announced to the public in 2016](https://cloud.google.com/blog/products/ai-machine-learning/google-supercharges-machine-learning-tasks-with-custom-chip),
    it would be a fair guess that it started around 2011\. If that is true, they started
    the project pretty early since we did not see a significant improvement in computer
    vision until 2012 by AlexNet.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着估算TPU项目大约什么时候开始的。考虑到它在[2016年首次对外公布](https://cloud.google.com/blog/products/ai-machine-learning/google-supercharges-machine-learning-tasks-with-custom-chip)，我们可以合理推测它大约在2011年左右开始。如果这个推测正确，那么他们的项目启动得相当早，因为直到2012年AlexNet的出现，计算机视觉才取得了显著的进展。
- en: With this timeline, we know GPUs were less potent than today when the project
    started. Google saw this AI revolution early and wanted faster hardware for large-scale
    computing. Their only choice is to build a new solution for it.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个时间线，我们知道在项目启动时，GPU的性能不如今天。Google很早就看到了AI革命，并希望为大规模计算提供更快的硬件。他们唯一的选择是为此构建一个新的解决方案。
- en: That was why Google started this project, but there are more questions. Why
    were GPUs not good enough back in the day? What potential improvements did Google
    see that are significant enough to start their new hardware project?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Google启动该项目的原因，但还有更多的问题。为什么当时GPU不够好？Google看到了哪些潜在的改进，足够重要以至于启动了他们的新硬件项目？
- en: The answer lies in the microarchitecture of GPUs and TPUs. Let’s examine the
    design of the cores on GPUs and TPUs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 答案在于GPU和TPU的微架构。让我们来看看GPU和TPU核心的设计。
- en: The design idea of GPUs
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU的设计理念
- en: First, let’s do a quick recap of the background knowledge of CPUs. When an instruction
    comes, it is decoded by the instruction decoder and fed into the arithmetic logic
    unit (ALU) together with data from the registers. The ALU does all the computing
    and returns the results to one of the registers. If you have multiple cores in
    the CPU, they can work in parallel.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们快速回顾一下CPU的背景知识。当一个指令到来时，它会被指令解码器解码，并与寄存器中的数据一起送入算术逻辑单元（ALU）。ALU负责所有计算并将结果返回到其中一个寄存器。如果CPU有多个核心，它们可以并行工作。
- en: What is a GPU? It is short for the graphics processing unit. It was designed
    for graphics computing and later discovered suitable for machine learning. Most
    of the operations in a GPU are matrix operations, which could run in parallel.
    This also means there are not many operations they need to support compared with
    a CPU.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是GPU？它是图形处理单元（Graphics Processing Unit）的缩写。它最初是为图形计算设计的，后来发现它适用于机器学习。GPU中的大多数操作是矩阵运算，可以并行执行。这也意味着，与CPU相比，GPU需要支持的操作较少。
- en: The more specialized the chip is for a given task, the faster it is on the task.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 芯片针对特定任务越专业，完成任务的速度就越快。
- en: The key idea of the GPU’s initial design was to have a feature-reduced CPU with
    smaller but more cores for faster parallel computing. The number of instructions
    supported on a GPU is much less than on a CPU, which makes the area taken by a
    single core on a chip much smaller. This way, they can pack more cores onto the
    chip for large-scale parallel computing.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: GPU最初设计的核心理念是将CPU的功能进行简化，采用更小但更多的核心，以实现更快的并行计算。GPU上支持的指令数量远少于CPU，这使得单个核心在芯片上所占的面积更小。这样，它们可以将更多的核心集成到芯片上，进行大规模并行计算。
- en: Why do fewer features mean a smaller area on the chip? In software, more features
    mean more code. In hardware, all features are implemented using logical circuits
    instead of code. More features mean the circuit is more complex. For example,
    a CPU must implement more instructions on the chip.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么特性越少意味着芯片上占用的面积越小？在软件中，更多的特性意味着更多的代码；在硬件中，所有特性都是通过逻辑电路实现的，而不是代码。更多特性意味着电路更复杂。例如，CPU必须在芯片上实现更多的指令。
- en: Smaller also means faster. A simpler design of the logic gates leads to a shorter
    cycle time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 更小也意味着更快。更简单的逻辑门设计导致了更短的周期时间。
- en: The design idea of TPUs
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TPU的设计理念
- en: TPU further developed this idea of specialized chips for deep learning. The
    defining feature of a TPU is its matrix-multiply unit (MXU). Since matrix multiplication
    is the most frequent operation in deep learning, TPU builds a specialized core
    for it, the MXU.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: TPU进一步发展了这种专门用于深度学习的芯片理念。TPU的定义性特征是其矩阵乘法单元（MXU）。由于矩阵乘法是深度学习中最常见的操作，TPU为此构建了一个专门的核心，即MXU。
- en: 'This is even more specialized than a GPU core, capable of many matrix operations,
    while the MXU only does one thing: matrix multiplication.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这比GPU核心更为专门化，能够执行多种矩阵操作，而MXU只做一件事：矩阵乘法。
- en: It works quite differently from a traditional CPU/GPU core. All the dynamics
    and generality are removed. It has a grid of nodes all connected together. Each
    node only does multiplication and addition in a predefined manner. The results
    are directly pushed to the next node for the next multiplication and addition.
    So, everything is predefined and fixed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作方式与传统的CPU/GPU核心非常不同。所有的动态性和通用性都被去除了。它有一个网格状的节点，每个节点仅按预定义的方式进行乘法和加法。结果会直接传送到下一个节点，进行下一个乘法和加法。因此，一切都是预定义和固定的。
- en: This way, we save time by removing the need for instruction decoding since it
    just multiplies and adds whatever it receives. There is no register for writing
    and reading since we already know where the results should go, and there is no
    need to store it for arbitrary operations that come next.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们节省了时间，因为不需要进行指令解码，它只会进行接收到的乘法和加法。没有用于读写的寄存器，因为我们已经知道结果应该去哪里，也无需为随后的任意操作存储结果。
- en: Besides the MXU, the TPU has also been designed for better scalability. It has
    dedicated ports for high-bandwidth inter-chip interconnection (ICI). It is designed
    to sit on the racks in Google’s data centers and to be used in clusters. Since
    it is for proprietary usage only, they don’t need to worry about selling single
    chips or the complexity of installing the chips on the racks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了MXU，TPU还被设计为更具可扩展性。它具有用于高带宽芯片间互联（ICI）的专用端口。它被设计为能够安装在Google数据中心的机架上并用于集群。由于它仅供专有使用，因此不需要担心销售单个芯片或安装芯片到机架上的复杂性。
- en: Are TPUs still faster today?
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TPUs今天仍然更快吗？
- en: It doesn’t make sense others didn’t come up with the same simple idea of building
    dedicated cores for tensor operations (matrix multiplication). Even if they did
    not, it doesn’t make sense that they don’t copy.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其他人没有想到构建专用的张量操作核心（矩阵乘法）的简单想法是没有道理的。即使他们没有想到这一点，后来也不应该不去模仿。
- en: From the timeline, it seems Nvidia came up with the same idea at about the same
    time. A similar product from Nvidia, the Tensor Cores, was [first announced to
    the public in 2017](https://developer.nvidia.com/blog/inside-volta/), one year
    after Google’s TPU announcement.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从时间线来看，似乎Nvidia在大约相同的时间提出了相同的想法。Nvidia的类似产品——张量核心（Tensor Cores）——[首次公开宣布是在2017年](https://developer.nvidia.com/blog/inside-volta/)，也就是Google发布TPU的一年后。
- en: It is unclear whether TPUs are still faster than GPUs today. I cannot find public
    benchmarks of the latest generations of TPUs and GPUs, and it is unclear to me
    which generation and metrics should be used for benchmarking.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 目前尚不清楚TPU是否仍然比GPU更快。我无法找到最新一代TPU和GPU的公开基准测试，而且我不清楚应该使用哪个代数和度量标准来进行基准测试。
- en: 'However, we can use one universal application-oriented metric: dollars per
    epoch. I found [one interesting benchmark](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/03efaadb875e3ba0ccbf5047c308f20dcd4f93d2/community-content/vertex_model_garden/benchmarking_reports/jax_vit_benchmarking_report.md)
    from Google Cloud that aligns different hardware to the same axis: money. TPUs
    appear cheaper on Google Cloud if you have the same model, data, and number of
    epochs.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以使用一个通用的面向应用的度量标准：每轮的花费（dollars per epoch）。我发现[一个有趣的基准测试](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/03efaadb875e3ba0ccbf5047c308f20dcd4f93d2/community-content/vertex_model_garden/benchmarking_reports/jax_vit_benchmarking_report.md)来自Google
    Cloud，它将不同的硬件对齐到相同的轴心：金钱。如果你使用相同的模型、数据和轮次，在Google Cloud上TPU看起来更便宜。
- en: Large models, like Midjourney, Claude, and Gemini, are all very sensitive to
    the training cost because they consume too much computing power. As a result,
    many of them use TPUs on Google Cloud.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 大型模型，如Midjourney、Claude和Gemini，都对训练成本非常敏感，因为它们消耗大量计算资源。因此，它们中的许多都在Google Cloud上使用TPU。
- en: Why are TPUs cheaper?
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么TPU更便宜？
- en: One important reason is the software stack. You are using not only the hardware
    but also the software stack associated with it. Google has better vertical integration
    for its software stack and AI hardware than GPUs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的原因是软件堆栈。你不仅使用硬件，还使用与之相关的软件堆栈。Google在其软件堆栈和AI硬件的垂直集成上，比GPU做得更好。
- en: Google has dedicated engineering teams to build a whole software stack for it
    with strong vertical integration, from the model implementation (Vertex Model
    Garden) to the deep learning frameworks (Keras, JAX, and TensorFlow) to a compiler
    well-optimized for the TPUs (XLA).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Google拥有专门的工程团队来为其构建完整的软件堆栈，并实现强大的垂直集成，从模型实现（Vertex Model Garden）到深度学习框架（Keras、JAX和TensorFlow），再到为TPU优化的编译器（XLA）。
- en: The software stack for GPUs is very different. PyTorch is the most popular deep
    learning framework used with Nvidia GPUs, and it was mainly developed by Meta.
    The most widely used model pools with PyTorch are `transformers` and `diffusers`
    developed by HuggingFace. It is much harder to do perfect vertical integration
    for the software stack across all these companies.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: GPUs的软件堆栈非常不同。PyTorch是与Nvidia GPU一起使用的最流行的深度学习框架，主要由Meta开发。与PyTorch一起使用的最广泛应用的模型库是HuggingFace开发的`transformers`和`diffusers`。在这些公司之间实现完美的垂直集成的软件堆栈要困难得多。
- en: One caveat is that fewer models are implemented with JAX and TensorFlow. Sometimes,
    you may need to implement the model yourself or use it from PyTorch on TPUs. Depending
    on the implementation, you may experience some friction when using PyTorch on
    TPUs. So, there might be extra engineering costs besides the hardware cost itself.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一个警告是，使用JAX和TensorFlow实现的模型较少。有时，你可能需要自己实现模型，或者在TPU上使用PyTorch。根据实现的不同，在TPU上使用PyTorch时，可能会遇到一些摩擦。因此，除了硬件成本外，可能还会有额外的工程成本。
- en: Why not start selling TPUs?
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么不开始直接销售TPU呢？
- en: We understand the project was started for proprietary usage and acquired a pretty
    good user base on Google Cloud because of its lower price. Why did not Google
    just start to sell it to customers directly, just like Nvidia’s GPUs?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，该项目最初是为专有用途启动的，并由于其较低的价格，在Google Cloud上获得了相当不错的用户基础。为什么Google不直接开始将其卖给客户，就像Nvidia的GPU一样？
- en: The short answer is to stay focused. Google is in fierce competition with OpenAI
    for generative AI. At the same time, it is in the middle of multiple waves of
    tech layoffs to lower its cost. A wise strategy now would be to focus its limited
    resources on the most important projects.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的回答是保持专注。Google与OpenAI在生成式AI领域的竞争非常激烈。同时，Google正处于多轮科技裁员中，以降低成本。当前的明智策略是将有限的资源集中在最重要的项目上。
- en: If Google ever wants to start selling its TPUs, it will be competing with two
    strong opponents, Nvidia and OpenAI, at the same time, which may not be a wise
    move at the moment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Google想要开始销售TPU，它将同时与两个强大的对手Nvidia和OpenAI竞争，这在目前可能不是一个明智的举动。
- en: The huge overhead of selling hardware
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 销售硬件的巨大开销
- en: Selling hardware directly to customers creates huge overheads for the company.
    Conversely, renting TPUs on their cloud services is much more manageable.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 直接向客户销售硬件为公司带来巨大的开销。相反，租赁云服务上的TPU要更容易管理得多。
- en: When TPUs are only served on the cloud, they can have a centralized way to install
    all the TPUs and related software. There is no need to deal with various installation
    environments or the difficulty of deploying a TPU cluster.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当TPU仅在云端提供时，他们可以集中安装所有TPU和相关软件。无需处理各种安装环境或部署TPU集群的难题。
- en: They know exactly how many TPUs to make. The demands are all internal, so there
    is no uncertainty. Thus, managing the supply chain is much easier.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 他们准确知道需要生产多少TPU。需求完全是内部需求，因此没有不确定性。因此，供应链管理要容易得多。
- en: Sales also become much easier since it is just selling the cloud service. There
    is no need to build a new team experienced in selling hardware.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 销售也变得更加简便，因为这仅仅是在销售云服务。无需组建一个有经验销售硬件的新团队。
- en: The advantages of the TPU approach
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TPU方法的优势
- en: Without all the overhead of selling hardware directly to the customers, Google
    got a few advantages in return.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有直接向客户销售硬件的所有额外开销下，Google获得了几项优势。
- en: First, they can have a more aggressive TPU architecture design. The TPUs have
    a unique way of connecting the chips. Unlike multiple GPUs that connect to the
    same board, TPUs are organized in cubes. They arranged 64 TPUs in a 4 by 4 by
    4 cube to interconnect them with each other for faster inter-chip communication.
    There are 8960 chips in a single v5p Pod. They can be easily used together. This
    is the advantage of fully control your hardware installation environment.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，他们可以采用更具攻击性的TPU架构设计。TPU具有一种独特的芯片连接方式。与多个GPU连接到同一主板不同，TPU被组织成立方体的形式。他们将64个TPU排列成一个4x4x4的立方体，将它们彼此连接，以实现更快的芯片间通信。单个v5p
    Pod中有8960个芯片。它们可以轻松地一起使用。这是完全控制硬件安装环境的优势。
- en: Second, they can iterate faster to push out new generations. Since they only
    need to support a small set of use cases for proprietary usages, it drastically
    reduces their research and development cycle for every generation of the chips.
    I wonder if Nvidia came up with the TensorCore idea earlier than Google, but because
    of the overhead of selling hardware to external customers, they could only announce
    it one year later than Google.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，他们可以更快地进行迭代，推出新一代硬件。由于他们只需支持一小部分专有用途的应用场景，这大大缩短了每一代芯片的研发周期。我想知道英伟达是否早于谷歌想到了TensorCore的概念，但由于向外部客户销售硬件的额外开销，它们只能比谷歌晚一年才发布。
- en: From the perspective of serving its most important purpose, competing in GenAI,
    these advantages put Google in a very good position. Most importantly, with this
    in-house hardware solution, Google saved huge money by not buying GPUs from Nvidia
    at a monopoly price.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从竞争GenAI这个最重要目标的角度来看，这些优势使谷歌处于非常有利的地位。最重要的是，通过这种内部硬件解决方案，谷歌避免了以垄断价格购买英伟达GPU，从而节省了大量资金。
- en: The downside of the TPU approach
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TPU方法的缺点
- en: So far, we have discussed many advantages of Google’s AI hardware approach,
    but is there any downside? Indeed, there is a big one. Google became a tech island.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了谷歌AI硬件方法的许多优点，但它是否也有缺点？确实有一个大缺点。谷歌变成了一个技术孤岛。
- en: Every pioneer in tech will become an island isolated from the rest of the world,
    at least for a while. This is because they started early when the corresponding
    infrastructure was not ready. They need to build everything from scratch. Due
    to the migration cost, they will stick with their solution even if everyone else
    uses something else.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 每一个技术先锋都将在一段时间内变成一个与世界其他地方隔离的孤岛。这是因为他们在相应的基础设施尚未就绪时就开始了。他们需要从头开始建立一切。由于迁移成本，他们会坚持自己的解决方案，即使其他人都在使用其他方案。
- en: This is exactly what Google is experiencing right now. The rest of the world
    is innovating with models from HuggingFace and PyTorch. Everyone is quickly tweaking
    each other’s models to develop better ones. However, Google cannot join this process
    easily since its infra is largely built around TensorFlow and JAX. When putting
    a model from external into production, it must be re-implemented with Google’s
    framework.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是谷歌目前所面临的情况。外界正在通过HuggingFace和PyTorch的模型进行创新，大家迅速互相调整模型，以开发出更好的模型。然而，由于谷歌的基础设施主要围绕TensorFlow和JAX构建，它不能轻松地加入这个过程。当将外部模型投入生产时，必须重新用谷歌的框架进行实现。
- en: This “tech island” problem slows Google down in taking good solutions from the
    external world and further isolates it from others. Google will either start bringing
    more external solutions like HuggingFace, PyTorch, and GPUs or always ensure its
    in-house solutions are the best in the world.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“技术孤岛”问题使得谷歌在从外部世界采纳优秀解决方案时变得缓慢，进一步将其与他人隔离开来。谷歌要么开始引入更多的外部解决方案，如HuggingFace、PyTorch和GPU，要么始终确保其内部解决方案在全球范围内是最优秀的。
- en: What does the future of AI hardware look like?
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未来的AI硬件会是什么样子？
- en: Finally, let’s peek into the future of AI hardware. What would the future AI
    hardware look like? The short answer is mode collapse as the hardware becomes
    more specialized.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们展望一下AI硬件的未来。未来的AI硬件会是什么样子？简短的回答是：随着硬件变得更加专业化，出现模式崩溃现象。
- en: Hardware will be further coupled with the applications. For example, support
    more precision formats for better language model serving. Like with `bfloat16`,
    `TF32`, they may better support `int8` and `int4`. Nvidia announced their second
    generation of the [Transformer Engine](https://docs.nvidia.com/deeplearning/transformer-engine/index.html),
    which works with Blackwell GPU. This made optimizing their hardware for transformer
    models easier without changing the user code. A lot of codesign is happening.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件将进一步与应用耦合。例如，支持更多精度格式以更好地为语言模型服务。像`bfloat16`、`TF32`一样，它们可能会更好地支持`int8`和`int4`。Nvidia宣布了其第二代[变压器引擎](https://docs.nvidia.com/deeplearning/transformer-engine/index.html)，该引擎与Blackwell
    GPU兼容。这使得在不改变用户代码的情况下，更容易优化硬件以适应变压器模型。很多共同设计工作正在进行中。
- en: On the other hand, software cannot easily jump out of the transformer realm.
    If they do, they will be slow due to a lack of hardware support. On the contrary,
    they implement their models with the hardware in mind. For example, the [FlashAttention](https://arxiv.org/abs/2205.14135)
    algorithm is designed to leverage the memory hierarchy of GPUs for better performance.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，软件也难以跳出变压器领域。如果它们做到了，由于缺乏硬件支持，它们的速度将会很慢。相反，它们是在考虑硬件的前提下实施自己的模型。例如，[FlashAttention](https://arxiv.org/abs/2205.14135)算法就是为了利用GPU的内存层次结构来提高性能。
- en: We see a big mode collapse coming soon. The hardware and software are so well
    optimized for each other for the current models. Neither of them can easily leave
    the current design or algorithm. If there is a new model completely different
    from the transformers, it needs to be 10x better to get widely adopted. It must
    incentivize people to make new hardware as fast and cheap as transformers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预计很快会出现一次大的模式崩溃。当前的硬件和软件在现有模型中彼此优化得非常好。它们都无法轻易脱离现有的设计或算法。如果有一种全新、完全不同于变压器模型的模型，它需要比现有模型好10倍才能广泛采用。它必须激励人们像对变压器那样快速且廉价地制造新硬件。
- en: Summary
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In conclusion, the TPU project started for proprietary usage when the GPU’s
    computing power was insufficient. Google wants to focus on GenAI instead of competing
    in the AI hardware market to avoid slowing the iteration speed and sacrificing
    its innovative design. Faster computing at a lower cost helped Google significantly
    in doing AI research and developing AI applications. However, it also made Google
    a tech island.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，TPU项目最初是为专有用途而启动的，当时GPU的计算能力不足。谷歌希望专注于GenAI，而不是在AI硬件市场上竞争，以避免减缓迭代速度并牺牲其创新设计。更快的计算和更低的成本帮助谷歌在AI研究和应用开发上取得了显著进展。然而，这也使谷歌成为了一个技术孤岛。
- en: Looking into the future, AI hardware will be even more optimized for certain
    applications, like the transformer models. Neither the hardware nor the models
    could easily jump out of this mode collapse.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，AI硬件将会更加优化，以适应特定的应用，比如变压器模型。无论是硬件还是模型，都难以轻易跳出这种模式崩溃的局面。
