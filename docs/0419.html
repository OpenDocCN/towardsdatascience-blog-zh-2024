<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Maximizing the Utility of Scarce AI Resources: A Kubernetes Approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Maximizing the Utility of Scarce AI Resources: A Kubernetes Approach</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/maximizing-the-utility-of-scarce-ai-resources-a-kubernetes-approach-0230ba53965b?source=collection_archive---------12-----------------------#2024-02-13">https://towardsdatascience.com/maximizing-the-utility-of-scarce-ai-resources-a-kubernetes-approach-0230ba53965b?source=collection_archive---------12-----------------------#2024-02-13</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f78c" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Optimizing the use of limited AI training accelerators</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--0230ba53965b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--0230ba53965b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--0230ba53965b--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--0230ba53965b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 13, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/cffd5cd8990a848557b12e0d977c3003.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*76mGrS7mPXEHrLP3"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@okello_1?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Roman Derrick Okello</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="a8cd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the ever-evolving landscape of AI development, nothing rings truer than the old saying (attributed to Heraclitus), “the only constant in life is change”. In the case of AI, it seems that <em class="nz">change </em>is indeed constant, but the <em class="nz">pace of change</em> is forever increasing. Staying relevant in these unique and exciting times amounts to an unprecedented test of the capacity of AI teams to consistently adapt and adjust their development processes. AI development teams that fail to adapt, or are slow to adapt, may quickly become obsolete.</p><p id="6187" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One of the most challenging developments of the past few years in AI development has been the increasing difficulty to attain the hardware required to train AI models. Whether it be due to an <a class="af nc" href="https://en.wikipedia.org/wiki/2021%E2%80%932023_global_supply_chain_crisis" rel="noopener ugc nofollow" target="_blank">ongoing crisis in the global supply chain</a> or a significant increase in the demand for AI chips, getting your hands on the GPUs (or alternative training accelerators) that you need for AI development, has gotten much harder. This is evidenced by the huge <a class="af nc" href="https://www.nextplatform.com/2023/11/17/what-to-do-when-you-cant-get-nvidia-h100-gpus/" rel="noopener ugc nofollow" target="_blank">wait time</a> for new GPU orders and by the fact that cloud service providers (CSPs) that once offered virtually infinite capacity of GPU machines, now struggle to keep up with the demand.</p><p id="f9e2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The changing times are forcing AI development teams that may have once relied on endless capacity of AI accelerators to adapt to a world with reduced accessibility and, in some cases, higher costs. Development processes that once took for granted the ability to spin up a new GPU machine at will, must be modified to meet the demands of a world of scarce AI resources that are often shared by multiple projects and/or teams. Those that fail to adapt risk annihilation.</p><p id="7c11" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this post we will demonstrate the use of <a class="af nc" href="https://kubernetes.io/" rel="noopener ugc nofollow" target="_blank">Kubernetes</a> in the orchestration of AI-model training workloads in a world of scarce AI resources. We will start by specifying the goals we wish to achieve. We will then describe why Kubernetes is an appropriate tool for addressing this challenge. Last, we will provide a simple demonstration of how Kubernetes can be used to maximize the use of a scarce AI compute resource. In subsequent posts, we plan to enhance the Kubernetes-based solution and show how to apply it to a cloud-based training environment.</p><h2 id="491f" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">Disclaimers</h2><p id="82b9" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">While this post does not assume prior experience with Kubernetes, some basic familiarity would certainly be helpful. This post should not, in any way, be viewed as a Kubernetes tutorial. To learn about Kubernetes, we refer the reader to the many great online resources on the subject. Here we will discuss just a few properties of Kubernetes as they pertain to the topic of maximizing and prioritizing resource utilization.</p><p id="a96e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There are many alternative tools and techniques to the method we put forth here, each with their own pros and cons. Our intention in this post is purely educational; Please do not view any of the choices we make as an endorsement.</p><p id="1506" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Lastly, the Kubernetes platform remains under constant development, as do many of the frameworks and tools in the field of AI development. Please take into account the possibility that some of the statements, examples, and/or external links in this post may become outdated by the time you read this and be sure to take into account the most up-to-date solutions available before making your own design decisions.</p><h1 id="9d71" class="pa ob fq bf oc pb pc gq og pd pe gt ok pf pg ph pi pj pk pl pm pn po pp pq pr bk">Adapting to Scarce AI Compute Resources</h1><p id="45cd" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">To simplify our discussion, let’s assume that we have a single worker node at our disposal for training our models. This could be a local machine with a GPU or a <a class="af nc" href="https://aws.amazon.com/ec2/pricing/reserved-instances/" rel="noopener ugc nofollow" target="_blank">reserved</a> compute-accelerated instance in the cloud, such as a <a class="af nc" href="https://aws.amazon.com/ec2/instance-types/p5/" rel="noopener ugc nofollow" target="_blank">p5.48xlarge</a> instance in AWS or a <a class="af nc" href="https://cloud.google.com/tpu" rel="noopener ugc nofollow" target="_blank">TPU node</a> in GCP. In our example below we will refer to this node as “my precious”. Typically, we will have spent a lot of money on this machine. We will further assume that we have multiple training workloads all competing for our single compute resource where each workload could take anywhere from a few minutes to a few days. Naturally, we would like to maximize the utility of our compute resource by ensuring that it is in constant use and that the most important jobs get prioritized. What we need is some form of a <a class="af nc" href="https://en.wikipedia.org/wiki/Priority_queue" rel="noopener ugc nofollow" target="_blank">priority queue</a><em class="nz"> </em>and an associated priority-based <a class="af nc" href="https://en.wikipedia.org/wiki/Scheduling_(computing)" rel="noopener ugc nofollow" target="_blank">scheduling</a> algorithm. Let’s try to be a bit more specific about the behaviors that we desire.</p><h2 id="d794" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">Scheduling Requirements</h2><ol class=""><li id="0040" class="nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny ps pt pu bk"><strong class="nf fr">Maximize Utilization:</strong> We would like for our resource to be in constant use. Specifically, as soon as it completes a workload, it will promptly (and automatically) start working on a new one.</li><li id="f48f" class="nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny ps pt pu bk"><strong class="nf fr">Queue Pending Workloads: </strong>We require the existence of a <em class="nz">queue </em>of training workloads that are waiting to be processed by our unique resource. We also require associated APIs for creating and submitting new jobs to the queue, as well as monitoring and managing the state of the queue.</li><li id="2dab" class="nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny ps pt pu bk"><strong class="nf fr">Support Prioritization:</strong> We would like each training job to have an associated priority such that workloads with higher priority will be run before workloads with a lower priority.</li><li id="bffc" class="nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny ps pt pu bk"><strong class="nf fr">Preemption:</strong> Moreover, in the case that an urgent job is submitted to the queue while our resource is working on a lower priority job, we would like for the running job to be preempted and replaced by the urgent job. The preempted job should be returned to the queue.</li></ol><p id="1158" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One approach to developing a solution that satisfies these requirements could be to take an existing API for submitting jobs to a training resource and wrap it with a customized implementation of a priority queue with the desired properties. At a minimum, this approach would require a data structure for storing a list of pending jobs, a dedicated process for choosing and submitting jobs from the queue to the training resource, and some form of mechanism for identifying when a job has been completed and the resource has become available.</p><p id="da00" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">An alternative approach and the one we take in this post, is to leverage an existing solution for priority-based <a class="af nc" href="https://en.wikipedia.org/wiki/Scheduling_(computing)" rel="noopener ugc nofollow" target="_blank">scheduling</a> that fulfils our requirements and align our training development workflow to its use. The default <a class="af nc" href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/" rel="noopener ugc nofollow" target="_blank">scheduler</a> that comes with <a class="af nc" href="https://kubernetes.io/" rel="noopener ugc nofollow" target="_blank">Kubernetes</a> is an example of one such solution. In the next sections we will demonstrate how it can be used to address the problem of optimizing the use of scarce AI training resources.</p><h1 id="1316" class="pa ob fq bf oc pb pc gq og pd pe gt ok pf pg ph pi pj pk pl pm pn po pp pq pr bk">ML Orchestration with Kubernetes</h1><p id="fa97" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">In this section we will get a bit philosophical about the application of Kubernetes to the orchestration of ML training workloads. If you have no patience for such discussions (totally fair) and want to get straight to the practical examples, please feel free to skip to the next section.</p><p id="0639" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Kubernetes is (another) one of those software/technological solutions that tend to elicit strong reactions in many developers. There are some that swear by it and use it extensively, and others that find it overbearing, clumsy, and unnecessary (e.g., see <a class="af nc" href="https://cloudnativejourney.wordpress.com/2023/04/19/kubernetes-advantages-and-disadvantages/" rel="noopener ugc nofollow" target="_blank">here</a> for some of the arguments for and against using Kubernetes). As with many other heated debates, it is the author’s opinion that the truth lies somewhere in between — there are situations where Kubernetes provides an ideal framework that can significantly increase productivity, and other situations where its use borders on an insult to the SW development profession. The big question is, where on the spectrum does ML development lie? Is Kubernetes the appropriate framework for training ML models? Although a cursory online search might give the impression that the general consensus is an emphatic “yes”, we will make some arguments for why that may not be the case. But first, we need to be clear about what we mean by “ML training orchestration using Kubernetes”.</p><p id="8c98" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While there are many online resources that address the topic of ML using Kubernetes, it is important to be aware of the fact that they are not always referring to the same mode of use. Some resources (e.g., <a class="af nc" rel="noopener" target="_blank" href="/distributed-deep-learning-training-with-horovod-on-kubernetes-6b28ac1d6b5d">here</a>) use Kubernetes only for deploying a cluster; once the cluster is up and running they start the training job outside the context of Kubernetes. Others (e.g., <a class="af nc" href="https://aws.amazon.com/blogs/machine-learning/introducing-amazon-sagemaker-operators-for-kubernetes/" rel="noopener ugc nofollow" target="_blank">here</a>) use Kubernetes to define a pipeline in which a dedicated module starts up a training job (and associated resources) using a completely different system. In contrast to these two examples, many other resources define the training workload as a <a class="af nc" href="https://kubernetes.io/docs/concepts/workloads/controllers/job/" rel="noopener ugc nofollow" target="_blank">Kubernetes Job</a> artifact that runs on a <a class="af nc" href="https://kubernetes.io/docs/concepts/architecture/nodes/" rel="noopener ugc nofollow" target="_blank">Kubernetes Node</a>. However, they too vary greatly in the particular attributes on which they focus. Some (e.g., <a class="af nc" href="https://medium.com/pytorch/a-step-by-step-guide-to-building-a-distributed-spot-based-training-platform-on-aws-using-b54acd06ecb2" rel="noopener">here</a>) emphasize the auto-scaling properties and others (e.g., <a class="af nc" href="https://blog.realvarez.com/get-more-out-of-gpus-with-nvidia-multi-instance-gpu/" rel="noopener ugc nofollow" target="_blank">here</a>) the Multi-Instance GPU (<a class="af nc" href="https://www.nvidia.com/en-eu/technologies/multi-instance-gpu/" rel="noopener ugc nofollow" target="_blank">MIG</a>) support. They also vary greatly in the details of implementation, such as the precise artifact (<a class="af nc" href="https://kubernetes.io/docs/concepts/workloads/controllers/job/" rel="noopener ugc nofollow" target="_blank">Job</a> extension) for representing a training job (e.g., <a class="af nc" href="https://github.com/pytorch/elastic/tree/master/kubernetes" rel="noopener ugc nofollow" target="_blank">ElasticJob</a>, <a class="af nc" href="https://docs.run.ai/v2.15/admin/workloads/workload-overview-admin/" rel="noopener ugc nofollow" target="_blank">TrainingWorkload</a>, <a class="af nc" href="https://github.com/kubernetes-sigs/jobset" rel="noopener ugc nofollow" target="_blank">JobSet</a>, <a class="af nc" href="https://volcano.sh/en/docs/vcjob/" rel="noopener ugc nofollow" target="_blank">VolcanoJob</a>, etc.). In the context of this post, we too will assume that the training workload is defined as a <a class="af nc" href="https://kubernetes.io/docs/concepts/workloads/controllers/job/" rel="noopener ugc nofollow" target="_blank">Kubernetes Job</a>. However, in order to simplify the discussion, we will stick with the core Kubernetes objects and leave the discussion of <a class="af nc" href="https://kubernetes.io/docs/concepts/extend-kubernetes/" rel="noopener ugc nofollow" target="_blank">Kubernetes extensions</a> for ML for a future post.</p><h2 id="1d2d" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">Arguments Against Kubernetes for ML</h2><p id="e52b" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Here are some arguments that could be made against the use of Kubernetes for training ML models.</p><ol class=""><li id="804f" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny ps pt pu bk"><strong class="nf fr">Complexity:</strong> Even its greatest proponents have to admit that Kubernetes can be hard. Using Kubernetes effectively, requires a high level of expertise, has a steep learning curve, and, realistically speaking, typically requires a dedicated devops team. Designing a training solution based on Kubernetes increases dependencies on dedicated experts and by extension, increases the risk that things could go wrong, and that development could be delayed. Many alternative ML training solutions enable a greater level of developer independence and freedom and entail a reduced risk of bugs in the development process.</li><li id="a3f1" class="nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny ps pt pu bk"><strong class="nf fr">Fixed Resource Requirements:</strong> One of the most touted properties of Kubernetes is its <a class="af nc" href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/" rel="noopener ugc nofollow" target="_blank">scalability</a> — its ability to automatically and seamlessly scale its pool of compute resources up and down according to the number of jobs, the number of clients (in the case of a service application), resource capacity, etc. However, one could argue that in the case of an ML training workload, where the number of resources that are required is (usually) fixed throughout training, auto-scaling is unnecessary.</li><li id="6b10" class="nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny ps pt pu bk"><strong class="nf fr">Fixed Instance Type:</strong> Due to the fact that Kubernetes orchestrates <em class="nz">containerized</em> applications, Kubernetes enables a great deal of flexibility when it comes to the types of machines in its node pool. However, when it comes to ML, we typically require very specific machinery with dedicated accelerators (such as GPUs). Moreover, our workloads are often tuned to run optimally on one very specific instance type.</li><li id="f5e5" class="nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny ps pt pu bk"><strong class="nf fr">Monolithic Application Architecture:</strong> It is common practice in the development of modern-day applications to break them down into small elements called <a class="af nc" href="https://en.wikipedia.org/wiki/Microservices" rel="noopener ugc nofollow" target="_blank">microservices</a>. Kubernetes is often seen as a key component in this design. ML training applications tend to be quite monolithic in their design and, one could argue, that they do not lend themselves naturally to a microservice architecture.</li><li id="7c38" class="nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny ps pt pu bk"><strong class="nf fr">Resource Overhead:</strong> The dedicated processes that are required to run Kubernetes requires some system resources on each of the nodes in its pool. Consequently, it may incur a certain performance penalty on our training jobs. Given the expense of the resources required for training, we may prefer to avoid this.</li></ol><p id="e363" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Granted, we have taken a very one-sided view in the Kubernetes-for-ML debate. Based solely on the arguments above, you might conclude that we would need a darn good reason for choosing Kubernetes as a framework for ML training. It is our opinion that the challenge put forth in this post, i.e., the desire to maximize the utility of scarce AI compute resources, is exactly the type of justification that warrants the use of Kubernetes despite the arguments made above. As we will demonstrate, the default <a class="af nc" href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/" rel="noopener ugc nofollow" target="_blank">scheduler</a> that is built-in to <a class="af nc" href="https://kubernetes.io/" rel="noopener ugc nofollow" target="_blank">Kubernetes</a>, combined with its support for <a class="af nc" href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/" rel="noopener ugc nofollow" target="_blank">priority and preemption</a> makes it a front-runner for fulfilling the requirements stated above.</p><h1 id="d0b5" class="pa ob fq bf oc pb pc gq og pd pe gt ok pf pg ph pi pj pk pl pm pn po pp pq pr bk">Toy Example</h1><p id="20ad" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">In this section we will share a brief example that demonstrates the priority scheduling support that is built in to Kubernetes. For the purposes of our demonstration, we will use <a class="af nc" href="https://minikube.sigs.k8s.io/docs/" rel="noopener ugc nofollow" target="_blank">Minikube</a> (version v1.32.0). Minikube is a tool that enables you to run a Kubernetes cluster in a local environment and is an ideal playground for experimenting with Kubernetes. Please see the official documentation on <a class="af nc" href="https://minikube.sigs.k8s.io/docs/start/" rel="noopener ugc nofollow" target="_blank">installing and getting started</a> with Minikube.</p><h2 id="2f6d" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">Cluster Creation</h2><p id="8c3a" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Let’s start by creating a two-node cluster using the <a class="af nc" href="https://minikube.sigs.k8s.io/docs/commands/start/" rel="noopener ugc nofollow" target="_blank">Minikube start</a> command:</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="ae0a" class="qe ob fq qb b bg qf qg l qh qi">minikube start --nodes 2</span></pre><p id="409e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The result is a local Kubernetes cluster consisting of a master (“control-plane”) node named <em class="nz">minikube,</em> and a single worker node, named <em class="nz">minikube-m02</em>, which will simulate our single AI resource. Let’s apply the <a class="af nc" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/" rel="noopener ugc nofollow" target="_blank">label</a> <em class="nz">my-precious </em>to identify it as a unique resource type:</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="dfb2" class="qe ob fq qb b bg qf qg l qh qi">kubectl label nodes minikube-m02 node-type=my-precious</span></pre><p id="f00f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can use the <a class="af nc" href="https://minikube.sigs.k8s.io/docs/handbook/dashboard/" rel="noopener ugc nofollow" target="_blank">Minikube dashboard</a> to visualize the results. In a separate shell run the command below and open the generated browser link.</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="b755" class="qe ob fq qb b bg qf qg l qh qi">minikube dashboard</span></pre><p id="20da" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If you press on the <em class="nz">Nodes </em>tab on the left-hand pane, you should see a summary of our cluster’s nodes:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qj"><img src="../Images/77af57927c95d3cd62defaac704ecf15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uNo_XDDiC7urvbQTNyESHg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Nodes List in Minikube Dashboard (Captured by Author)</figcaption></figure><h2 id="5b15" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">PriorityClass Definitions</h2><p id="d9a2" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Next, we define two <a class="af nc" href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass" rel="noopener ugc nofollow" target="_blank">PriorityClasses</a>, <em class="nz">low-priority </em>and <em class="nz">high-priority</em>, as in the <em class="nz">priorities.yaml</em> file displayed below. New jobs will receive the <em class="nz">low-priority </em>assignment, by default.</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="9366" class="qe ob fq qb b bg qf qg l qh qi">apiVersion: scheduling.k8s.io/v1<br/>kind: PriorityClass<br/>metadata:<br/>  name: low-priority<br/>value: 0<br/>globalDefault: true<br/><br/>---<br/>apiVersion: scheduling.k8s.io/v1<br/>kind: PriorityClass<br/>metadata:<br/>  name: high-priority<br/>value: 1000000<br/>globalDefault: false</span></pre><p id="8d05" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To apply our new classes to our cluster, we run:</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="3e9c" class="qe ob fq qb b bg qf qg l qh qi">kubectl apply -f priorities.yaml</span></pre><h2 id="fb79" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">Create a Job</h2><p id="1c67" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">We define a simple job using a <em class="nz">job.yaml</em> file displayed in the code block below. For the purpose of our demonstration, we define a <a class="af nc" href="https://kubernetes.io/docs/concepts/workloads/controllers/job/" rel="noopener ugc nofollow" target="_blank">Kubernetes Job</a> that does nothing more than sleep for 100 seconds. We use <a class="af nc" href="https://hub.docker.com/_/busybox" rel="noopener ugc nofollow" target="_blank">busybox</a> as its Docker image. In practice, this would be replaced with a training script and an appropriate ML Docker image. We define the job to run on our special instance, <em class="nz">my-precious</em>, using the <a class="af nc" href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector" rel="noopener ugc nofollow" target="_blank">nodeSelector</a> field, and specify the resource requirements so that only a single instance of the job can run on the instance at a time. The priority of the job defaults to <em class="nz">low-priority</em> as defined above.</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="06ad" class="qe ob fq qb b bg qf qg l qh qi">apiVersion: batch/v1<br/>kind: Job<br/>metadata:<br/>  name: test<br/>spec:<br/>  template:<br/>    spec:<br/>      containers:<br/>        - name: test<br/>          image: busybox<br/>          command: # simple sleep command<br/>            - sleep<br/>            - '100'<br/>          resources: # require all available resources<br/>            limits:<br/>              cpu: "2"<br/>            requests:<br/>              cpu: "2"<br/>      nodeSelector: # specify our unique resource<br/>          node-type: my-precious<br/>      restartPolicy: Never</span></pre><p id="0649" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We submit the job with the following command:</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="da71" class="qe ob fq qb b bg qf qg l qh qi">kubectl apply -f job.yaml</span></pre><h2 id="4c7c" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">Create a Queue of Jobs</h2><p id="416c" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">To demonstrate the manner in which Kubernetes queues jobs for processing, we create three identical copies of the job defined above, named <em class="nz">test1</em>, <em class="nz">test2</em>, and <em class="nz">test3</em>. We group the three jobs in a single file, <em class="nz">jobs.yaml</em>, and submit them for processing:</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="8a9d" class="qe ob fq qb b bg qf qg l qh qi">kubectl apply -f jobs.yaml</span></pre><p id="7af3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The image below captures the <em class="nz">Workload Status</em> of our cluster in the <a class="af nc" href="https://minikube.sigs.k8s.io/docs/handbook/dashboard/" rel="noopener ugc nofollow" target="_blank">Minikube dashboard</a> shortly after the submission. You can see that <em class="nz">my-precious </em>has begun processing <em class="nz">test1</em>, while the other jobs are <em class="nz">pending</em> as they wait their turn.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qk"><img src="../Images/6efd9f7f525b7319dd48870d0fc7c7af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MKS3MUsziKv-KswqMNvNow.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Cluster Workload Status (Captured by Author)</figcaption></figure><p id="2b12" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Once <em class="nz">test1</em> is completed, processing of <em class="nz">test2 </em>begins:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ql"><img src="../Images/6fd10d8f89a5fef5073ae2411edeb91c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FZehiQEZSxsUUu_GLM5Nuw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Cluster Workload Status — Automated Scheduling (Captured by Author)</figcaption></figure><p id="ae30" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So long as no other jobs with higher priority are submitted, our jobs would continue to be processed one at a time until they are all completed.</p><h2 id="e8c5" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">Job Preemption</h2><p id="778b" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">We now demonstrate Kubernetes’ built-in support for <a class="af nc" href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/" rel="noopener ugc nofollow" target="_blank">job preemption</a> by showing what happens when we submit a fourth job, this time with the <em class="nz">high-priority</em> setting:</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="51cc" class="qe ob fq qb b bg qf qg l qh qi">apiVersion: batch/v1<br/>kind: Job<br/>metadata:<br/>  name: test-p1<br/>spec:<br/>  template:<br/>    spec:<br/>      containers:<br/>        - name: test-p1<br/>          image: busybox<br/>          command:<br/>            - sleep<br/>            - '100'<br/>          resources:<br/>            limits:<br/>              cpu: "2"<br/>            requests:<br/>              cpu: "2"<br/>      restartPolicy: Never<br/>      priorityClassName: high-priority # high priority job<br/>      nodeSelector:<br/>          node-type: my-precious</span></pre><p id="c70b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The impact on the <em class="nz">Workload Status</em> is displayed in the image below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qm"><img src="../Images/62ebad9df10e25effd21dbea9d2865ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H2Y7FqcjiJTPRIPQFydBpg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Cluster Workload Status — Preemption (Captured by Author)</figcaption></figure><p id="b4b4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The <em class="nz">test2</em> job has been <em class="nz">preempted </em>— its processing has been stopped and it has returned to the <em class="nz">pending</em> state. In its stead, <em class="nz">my-precious</em> has begun processing the higher priority <em class="nz">test-p1</em> job. Only once <em class="nz">test-p1 </em>is completed will processing of the lower priority jobs resume. (In the case where the preempted job is a ML training workload, we would program it to resume from the most recent saved model <a class="af nc" href="https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html" rel="noopener ugc nofollow" target="_blank">model checkpoint</a>).</p><p id="63a9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The image below displays the <em class="nz">Workload Status</em> once all jobs have been completed.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qn"><img src="../Images/f242a5d5bfa89d01de0134dba77273c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xxY65DIwADCONlp-jQskmA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Cluster Workload Status — Completion (Captured by Author)</figcaption></figure><h1 id="79d7" class="pa ob fq bf oc pb pc gq og pd pe gt ok pf pg ph pi pj pk pl pm pn po pp pq pr bk">Kubernetes Extensions</h1><p id="06eb" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The solution we demonstrated for priority-based scheduling and preemption relied only on core components of Kubernetes. In practice, you may choose to take advantage of enhancements to the basic functionality introduced by extensions such as <a class="af nc" href="https://github.com/kubernetes-sigs/kueue" rel="noopener ugc nofollow" target="_blank">Kueue</a> and/or dedicated, ML-specific features offered by platforms build on top of Kubernetes, such as <a class="af nc" href="https://www.run.ai/" rel="noopener ugc nofollow" target="_blank">Run:AI</a> or <a class="af nc" href="https://volcano.sh/en/" rel="noopener ugc nofollow" target="_blank">Volcano</a>. But keep in mind that to fulfill the basic requirements for maximizing the utility of a scarce AI compute resource all we need is the core Kubernetes.</p><h1 id="2b3a" class="pa ob fq bf oc pb pc gq og pd pe gt ok pf pg ph pi pj pk pl pm pn po pp pq pr bk">Summary</h1><p id="8742" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The reduced availability of dedicated AI silicon has forced ML teams to adjust their development processes. Unlike in the past, when developers could spin up new AI resources at will, they now face limitations on AI compute capacity. This necessitates the procurement of AI instances through means such as purchasing dedicated units and/or reserving cloud instances. Moreover, developers must come to terms with the likelihood of needing to share these resources with other users and projects. To ensure that the scarce AI compute power is appropriated towards maximum utility, dedicated scheduling algorithms must be defined that minimize idle time and prioritize critical workloads. In this post we have demonstrated how the <a class="af nc" href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/" rel="noopener ugc nofollow" target="_blank">Kubernetes scheduler</a> can be used to accomplish these goals. As emphasized above, this is just one of many approaches to address the challenge of maximizing the utility of scarce AI resources. Naturally, the approach you choose, and the details of your implementation will depend on the specific needs of your AI development.</p></div></div></div></div>    
</body>
</html>