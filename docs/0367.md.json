["```py\nuse cloud_file::{CloudFile, CloudFileError};\nuse futures_util::StreamExt; // Enables `.next()` on streams.\n\nasync fn count_lines(cloud_file: &CloudFile) -> Result<usize, CloudFileError> {\n    let mut chunks = cloud_file.stream_chunks().await?;\n    let mut newline_count: usize = 0;\n    while let Some(chunk) = chunks.next().await {\n        let chunk = chunk?;\n        newline_count += bytecount::count(&chunk, b'\\n');\n    }\n    Ok(newline_count)\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), CloudFileError> {\n    let url = \"https://raw.githubusercontent.com/fastlmm/bed-sample-files/main/toydata.5chrom.fam\";\n    let options = [(\"timeout\", \"10s\")];\n    let cloud_file = CloudFile::new_with_options(url, options)?;\n    let line_count = count_lines(&cloud_file).await?;\n    println!(\"line_count: {line_count}\");\n    Ok(())\n}\n```", "```py\nline_count: 500\n```", "```py\nuse std::fs::File;\nuse std::io::{self, BufRead, BufReader};\n\nfn main() -> io::Result<()> {\n    let path = \"examples/line_counts_local.rs\";\n    let reader = BufReader::new(File::open(path)?);\n    let mut line_count = 0;\n    for line in reader.lines() {\n        let _line = line?;\n        line_count += 1;\n    }\n    println!(\"line_count: {line_count}\");\n    Ok(())\n}\n```", "```py\nuse futures_util::StreamExt;  // Enables `.next()` on streams.\npub use object_store::path::Path as StorePath;\nuse object_store::{parse_url_opts, ObjectStore};\nuse std::sync::Arc;\nuse url::Url;\n\nasync fn count_lines(\n    object_store: &Arc<Box<dyn ObjectStore>>,\n    store_path: StorePath,\n) -> Result<usize, anyhow::Error> {\n    let mut chunks = object_store.get(&store_path).await?.into_stream();\n    let mut newline_count: usize = 0;\n    while let Some(chunk) = chunks.next().await {\n        let chunk = chunk?;\n        newline_count += bytecount::count(&chunk, b'\\n');\n    }\n    Ok(newline_count)\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), anyhow::Error> {\n    let url = \"https://raw.githubusercontent.com/fastlmm/bed-sample-files/main/toydata.5chrom.fam\";\n    let options = [(\"timeout\", \"10s\")];\n\n    let url = Url::parse(url)?;\n    let (object_store, store_path) = parse_url_opts(&url, options)?;\n    let object_store = Arc::new(object_store); // enables cloning and borrowing\n    let line_count = count_lines(&object_store, store_path).await?;\n    println!(\"line_count: {line_count}\");\n    Ok(())\n}\n```", "```py\nuse cloud_file::CloudFile;\nuse futures::StreamExt;  // Enables `.next()` on streams.\nuse std::str::from_utf8;\n\nasync fn nth_line(cloud_file: &CloudFile, n: usize) -> Result<String, anyhow::Error> {\n    // Each binary line_chunk contains one or more lines, that is, each chunk ends with a newline.\n    let mut line_chunks = cloud_file.stream_line_chunks().await?;\n    let mut index_iter = 0usize..;\n    while let Some(line_chunk) = line_chunks.next().await {\n        let line_chunk = line_chunk?;\n        let lines = from_utf8(&line_chunk)?.lines();\n        for line in lines {\n            let index = index_iter.next().unwrap(); // safe because we know the iterator is infinite\n            if index == n {\n                return Ok(line.to_string());\n            }\n        }\n    }\n    Err(anyhow::anyhow!(\"Not enough lines in the file\"))\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), anyhow::Error> {\n    let url = \"https://raw.githubusercontent.com/fastlmm/bed-sample-files/main/toydata.5chrom.fam\";\n    let n = 4;\n\n    let cloud_file = CloudFile::new(url)?;\n    let line = nth_line(&cloud_file, n).await?;\n    println!(\"line at index {n}: {line}\");\n    Ok(())\n}\n```", "```py\nline at index 4: per4 per4 0 0 2 0.452591\n```", "```py\nlet mut index_iter = 0usize..;\n...\nlet index = index_iter.next().unwrap(); // safe because we know the iterator is infinite\n```", "```py\n#[tokio::test]\nasync fn check_file_signature() -> Result<(), CloudFileError> {\n    let url = \"https://raw.githubusercontent.com/fastlmm/bed-sample-files/main/plink_sim_10s_100v_10pmiss.bed\";\n    let cloud_file = CloudFile::new(url)?;\n    let (bytes, size) = cloud_file.read_range_and_file_size(0..3).await?;\n\n    assert_eq!(bytes.len(), 3);\n    assert_eq!(bytes[0], 0x6c);\n    assert_eq!(bytes[1], 0x1b);\n    assert_eq!(bytes[2], 0x01);\n    assert_eq!(size, 303);\n    Ok(())\n}\n```", "```py\n#[tokio::main]\nasync fn main() -> Result<(), anyhow::Error> {\n    let url = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\";\n    let options = [(\"timeout\", \"30s\")];\n    let cloud_file = CloudFile::new_with_options(url, options)?;\n\n    let seed = Some(0u64);\n    let sample_count = 10_000;\n    let max_chunk_bytes = 750; // 8_000_000 is a good default when chunks are bigger.\n    let max_concurrent_requests = 10; // 10 is a good default\n\n    count_bigrams(\n        cloud_file,\n        sample_count,\n        seed,\n        max_concurrent_requests,\n        max_chunk_bytes,\n    )\n    .await?;\n\n    Ok(())\n}\n```", "```py\n#[cfg(not(target_pointer_width = \"64\"))]\ncompile_error!(\"This code requires a 64-bit target architecture.\");\n\nuse cloud_file::CloudFile;\nuse futures::pin_mut;\nuse futures_util::StreamExt; // Enables `.next()` on streams.\nuse rand::{rngs::StdRng, Rng, SeedableRng};\nuse std::{cmp::max, collections::HashMap, ops::Range};\n\nasync fn count_bigrams(\n    cloud_file: CloudFile,\n    sample_count: usize,\n    seed: Option<u64>,\n    max_concurrent_requests: usize,\n    max_chunk_bytes: usize,\n) -> Result<(), anyhow::Error> {\n    // Create a random number generator\n    let mut rng = if let Some(s) = seed {\n        StdRng::seed_from_u64(s)\n    } else {\n        StdRng::from_entropy()\n    };\n\n    // Find the document size\n    let file_size = cloud_file.read_file_size().await?;\n//...\n```", "```py\n // Randomly choose the two-byte ranges to sample\n    let range_samples: Vec<Range<usize>> = (0..sample_count)\n        .map(|_| rng.gen_range(0..file_size - 1))\n        .map(|start| start..start + 2)\n        .collect();\n```", "```py\n // Divide the ranges into chunks respecting the max_chunk_bytes limit\n    const BYTES_PER_BIGRAM: usize = 2;\n    let chunk_count = max(1, max_chunk_bytes / BYTES_PER_BIGRAM);\n    let range_chunks = range_samples.chunks(chunk_count);\n```", "```py\n // Create an iterator of future work\n    let work_chunks_iterator = range_chunks.map(|chunk| {\n        let cloud_file = cloud_file.clone(); // by design, clone is cheap\n        async move { cloud_file.read_ranges(chunk).await }\n    });\n\n    // Create a stream of futures to run out-of-order and with constrained concurrency.\n    let work_chunks_stream =\n        futures_util::stream::iter(work_chunks_iterator).buffer_unordered(max_concurrent_requests);\n    pin_mut!(work_chunks_stream); // The compiler says we need this\n```", "```py\n // Run the futures and, as result bytes come in, tabulate.\n    let mut bigram_counts = HashMap::new();\n    while let Some(result) = work_chunks_stream.next().await {\n        let bytes_vec = result?;\n        for bytes in bytes_vec.iter() {\n            let bigram = (bytes[0], bytes[1]);\n            let count = bigram_counts.entry(bigram).or_insert(0);\n            *count += 1;\n        }\n    }\n\n    // Sort the bigrams by count and print the top 10\n    let mut bigram_count_vec: Vec<(_, usize)> = bigram_counts.into_iter().collect();\n    bigram_count_vec.sort_by(|a, b| b.1.cmp(&a.1));\n    for (bigram, count) in bigram_count_vec.into_iter().take(10) {\n        let char0 = (bigram.0 as char).escape_default();\n        let char1 = (bigram.1 as char).escape_default();\n        println!(\"Bigram ('{}{}') occurs {} times\", char0, char1, count);\n    }\n    Ok(())\n}\n```", "```py\nBigram ('\\r\\n') occurs 367 times\nBigram ('e ') occurs 221 times\nBigram (' t') occurs 184 times\nBigram ('th') occurs 171 times\nBigram ('he') occurs 158 times\nBigram ('s ') occurs 143 times\nBigram ('.\\r') occurs 136 times\nBigram ('d ') occurs 133 times\nBigram (', ') occurs 127 times\nBigram (' a') occurs 121 times\n```", "```py\nuse cloud_file::CloudFile;\nuse rusoto_credential::{CredentialsError, ProfileProvider, ProvideAwsCredentials};\n\n#[tokio::main]\nasync fn main() -> Result<(), anyhow::Error> {\n    // get credentials from ~/.aws/credentials\n    let credentials = if let Ok(provider) = ProfileProvider::new() {\n        provider.credentials().await\n    } else {\n        Err(CredentialsError::new(\"No credentials found\"))\n    };\n\n    let Ok(credentials) = credentials else {\n        eprintln!(\"Skipping example because no AWS credentials found\");\n        return Ok(());\n    };\n\n    let url = \"s3://bedreader/v1/toydata.5chrom.bed\";\n    let options = [\n        (\"aws_region\", \"us-west-2\"),\n        (\"aws_access_key_id\", credentials.aws_access_key_id()),\n        (\"aws_secret_access_key\", credentials.aws_secret_access_key()),\n    ];\n    let cloud_file = CloudFile::new_with_options(url, options)?;\n\n    assert_eq!(cloud_file.read_file_size().await?, 1_250_003);\n    Ok(())\n}\n```", "```py\n let url = \"s3://bedreader/v1/toydata.5chrom.bed\";\n    let options = [\n        (\"aws_region\", \"us-west-2\"),\n        (\"aws_access_key_id\", credentials.aws_access_key_id()),\n        (\"aws_secret_access_key\", credentials.aws_secret_access_key()),\n    ];\n    let cloud_file = CloudFile::new_with_options(url, options)?;\n```", "```py\n use object_store::{aws::AmazonS3Builder, path::Path as StorePath};\n\n    let s3 = AmazonS3Builder::new()\n        .with_region(\"us-west-2\")\n        .with_bucket_name(\"bedreader\")\n        .with_access_key_id(credentials.aws_access_key_id())\n        .with_secret_access_key(credentials.aws_secret_access_key())\n        .build()?;\n    let store_path = StorePath::parse(\"v1/toydata.5chrom.bed\")?;\n    let cloud_file = CloudFile::from_structs(s3, store_path);\n```", "```py\n[tokio::test]\nasync fn cloud_file_extension() -> Result<(), CloudFileError> {\n    let url = \"https://raw.githubusercontent.com/fastlmm/bed-sample-files/main/plink_sim_10s_100v_10pmiss.bed\";\n    let mut cloud_file = CloudFile::new(url)?;\n    assert_eq!(cloud_file.read_file_size().await?, 303);\n    cloud_file.set_extension(\"fam\")?;\n    assert_eq!(cloud_file.read_file_size().await?, 130);\n    Ok(())\n}\n```", "```py\ncargo test\n```", "```py\n#[tokio::test]\nasync fn local_file() -> Result<(), CloudFileError> {\n    use std::env;\n\n    let apache_url = abs_path_to_url_string(env::var(\"CARGO_MANIFEST_DIR\").unwrap()\n             + \"/LICENSE-APACHE\")?;\n    let cloud_file = CloudFile::new(&apache_url)?;\n    assert_eq!(cloud_file.read_file_size().await?, 9898);\n    Ok(())\n}\n```", "```py\n#[test]\nfn lib_intro() -> Result<(), Box<BedErrorPlus>> {\n    let file_name = sample_bed_file(\"some_missing.bed\")?;\n\n    let mut bed = Bed::new(file_name)?;\n    println!(\"{:?}\", bed.iid()?.slice(s![..5])); // Outputs ndarray: [\"iid_0\", \"iid_1\", \"iid_2\", \"iid_3\", \"iid_4\"]\n    println!(\"{:?}\", bed.sid()?.slice(s![..5])); // Outputs ndarray: [\"sid_0\", \"sid_1\", \"sid_2\", \"sid_3\", \"sid_4\"]\n    println!(\"{:?}\", bed.chromosome()?.iter().collect::<HashSet<_>>());\n    // Outputs: {\"12\", \"10\", \"4\", \"8\", \"19\", \"21\", \"9\", \"15\", \"6\", \"16\", \"13\", \"7\", \"17\", \"18\", \"1\", \"22\", \"11\", \"2\", \"20\", \"3\", \"5\", \"14\"}\n    let _ = ReadOptions::builder()\n        .sid_index(bed.chromosome()?.map(|elem| elem == \"5\"))\n        .f64()\n        .read(&mut bed)?;\n\n    Ok(())\n}\n```", "```py\n#[tokio::test]\nasync fn cloud_lib_intro() -> Result<(), Box<BedErrorPlus>> {\n    let url = \"https://raw.githubusercontent.com/fastlmm/bed-sample-files/main/some_missing.bed\";\n    let cloud_options = [(\"timeout\", \"10s\")];\n\n    let mut bed_cloud = BedCloud::new_with_options(url, cloud_options).await?;\n    println!(\"{:?}\", bed_cloud.iid().await?.slice(s![..5])); // Outputs ndarray: [\"iid_0\", \"iid_1\", \"iid_2\", \"iid_3\", \"iid_4\"]\n    println!(\"{:?}\", bed_cloud.sid().await?.slice(s![..5])); // Outputs ndarray: [\"sid_0\", \"sid_1\", \"sid_2\", \"sid_3\", \"sid_4\"]\n    println!(\n        \"{:?}\",\n        bed_cloud.chromosome().await?.iter().collect::<HashSet<_>>()\n    );\n    // Outputs: {\"12\", \"10\", \"4\", \"8\", \"19\", \"21\", \"9\", \"15\", \"6\", \"16\", \"13\", \"7\", \"17\", \"18\", \"1\", \"22\", \"11\", \"2\", \"20\", \"3\", \"5\", \"14\"}\n    let _ = ReadOptions::builder()\n        .sid_index(bed_cloud.chromosome().await?.map(|elem| elem == \"5\"))\n        .f64()\n        .read_cloud(&mut bed_cloud)\n        .await?;\n\n    Ok(())\n}\n```", "```py\nuse tokio::runtime;\n// ...\n    #[pyfn(m)]\n    fn check_file_cloud(location: &str, options: HashMap<&str, String>) -> Result<(), PyErr> {\n        runtime::Runtime::new()?.block_on(async {\n            BedCloud::new_with_options(location, options).await?;\n            Ok(())\n        })\n    }\n```", "```py\nBedCloud::new_with_options(location, options).await?;\n```", "```py\nuse tokio::runtime;\n// ...\n\nruntime::Runtime::new()?.block_on(async {\n    BedCloud::new_with_options(location, options).await?;\n    Ok(())\n})\n```", "```py\n with open_bed(\n      \"https://raw.githubusercontent.com/fastlmm/bed-sample-files/main/some_missing.bed\",\n      cloud_options={\"timeout\": \"30s\"},\n  ) as bed:\n      print(bed.iid[:5])\n      print(bed.sid[:5])\n      print(np.unique(bed.chromosome))\n      val = bed.read(index=np.s_[:, bed.chromosome == \"5\"])\n      print(val.shape)\n```", "```py\n /// Return the `Vec` of [`Bytes`](https://docs.rs/bytes/latest/bytes/struct.Bytes.html) from specified ranges.\n    ///\n    /// # Example\n    /// ```", "```py\n```", "```py` ```", "```py`. Within the doc test, lines starting with `/// #` disappear from the documentation:\n\n![](../Images/3aa4177d13d6e88dd119ca930801c492.png)\n\nThe hidden lines, however, will still be run by `cargo test`.\n\nIn my library crates, I try to include a working example with every method. If such an example turns out overly complex or otherwise embarrassing, I try to fix the issue by improving the API.\n\nNotice that in this rule and the previous Rule 7, we added a runtime to the code. Unfortunately, including a runtime can easily double the size of your user’s programs, even if they don’t read files from the cloud. Making this extra size optional is the topic of Rule 9.\n\n# Rule 9: Include a runtime, but optionally.\n\nIf you follow Rule 6 and provide async methods, your users gain the freedom to choose their own runtime. Opting for a runtime like Tokio may significantly increase their compiled program’s size. However, if they use no async methods, selecting a runtime becomes unnecessary, keeping the compiled program lean. This embodies the “zero cost principle”, where one incurs costs only for the features one uses.\n\nOn the other hand, if you follow Rule 7 and wrap async calls inside traditional, “synchronous” methods, then you must provide a runtime. This will increase the size of the resultant program. To mitigate this cost, you should make the inclusion of any runtime optional.\n\nBed-Reader includes a runtime under two conditions. First, when used as a Python extension. Second, when testing the async methods. To handle the first condition, we create a Cargo feature called `extension-module` that pulls in optional dependencies `pyo3` and `tokio`. Here are the relevant sections of `Cargo.toml`:\n\n```", "```py\n\nAlso, because I’m using Maturin to [create a Rust extension for Python](/nine-rules-for-writing-python-extensions-in-rust-d35ea3a4ec29), I include this text in `pyproject.toml`:\n\n```", "```py\n\nI put all the Rust code related to extending Python in a file called `python_modules.rs`. It starts with this [conditional compilation attribute](https://doc.rust-lang.org/reference/conditional-compilation.html#the-cfg-attribute):\n\n```", "```py\n\nThis starting line ensures that the compiler includes the extension code only when needed.\n\nWith the Python extension code taken care of, we turn next to providing an optional runtime for testing our async methods. I again choose Tokio as the runtime. I put the tests for the async code in their own file called `tests_api_cloud.rs`. To ensure that that async tests are run only when the `tokio` dependency feature is “on”, I start the file with this line:\n\n```", "```py\n\nAs per Rule 5, we should also include examples in our documentation of the async methods. These examples also serve as “doc tests”. The doc tests need conditional compilation attributes. Below is the documentation for the method that retrieves chromosome metadata. Notice that the example includes two hidden lines that start\n`/// # #[cfg(feature = \"tokio\")]`\n\n```", "```py\n/// use ndarray as nd;\n/// use bed_reader::{BedCloud, ReadOptions};\n/// use bed_reader::assert_eq_nan;\n///\n/// # #[cfg(feature = \"tokio\")] Runtime::new().unwrap().block_on(async {\n/// let url = \"https://raw.githubusercontent.com/fastlmm/bed-sample-files/main/small.bed\";\n/// let mut bed_cloud = BedCloud::new(url).await?;\n/// let chromosome = bed_cloud.chromosome().await?;\n/// println!(\"{chromosome:?}\"); // Outputs ndarray [\"1\", \"1\", \"5\", \"Y\"]\n/// # Ok::<(), Box<BedErrorPlus>>(())}).unwrap();\n/// # #[cfg(feature = \"tokio\")] use {tokio::runtime::Runtime, bed_reader::BedErrorPlus};\n/// ```"]