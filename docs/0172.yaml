- en: Temporal Graph Learning in 2024
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2024年时序图学习
- en: 原文：[https://towardsdatascience.com/temporal-graph-learning-in-2024-feaa9371b8e2?source=collection_archive---------1-----------------------#2024-01-18](https://towardsdatascience.com/temporal-graph-learning-in-2024-feaa9371b8e2?source=collection_archive---------1-----------------------#2024-01-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/temporal-graph-learning-in-2024-feaa9371b8e2?source=collection_archive---------1-----------------------#2024-01-18](https://towardsdatascience.com/temporal-graph-learning-in-2024-feaa9371b8e2?source=collection_archive---------1-----------------------#2024-01-18)
- en: Continue the journey for evolving networks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 继续探索不断发展的网络
- en: '[](https://medium.com/@shenyanghuang1996?source=post_page---byline--feaa9371b8e2--------------------------------)[![Shenyang(Andy)
    Huang](../Images/ab63c37868db97b19480d536388930c5.png)](https://medium.com/@shenyanghuang1996?source=post_page---byline--feaa9371b8e2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--feaa9371b8e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--feaa9371b8e2--------------------------------)
    [Shenyang(Andy) Huang](https://medium.com/@shenyanghuang1996?source=post_page---byline--feaa9371b8e2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@shenyanghuang1996?source=post_page---byline--feaa9371b8e2--------------------------------)[![Shenyang(Andy)
    Huang](../Images/ab63c37868db97b19480d536388930c5.png)](https://medium.com/@shenyanghuang1996?source=post_page---byline--feaa9371b8e2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--feaa9371b8e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--feaa9371b8e2--------------------------------)
    [Shenyang(Andy) Huang](https://medium.com/@shenyanghuang1996?source=post_page---byline--feaa9371b8e2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--feaa9371b8e2--------------------------------)
    ·20 min read·Jan 18, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[数据科学前沿](https://towardsdatascience.com/?source=post_page---byline--feaa9371b8e2--------------------------------)
    ·阅读时间20分钟·2024年1月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Many complex networks evolve over time including transaction networks, traffic
    networks, social networks and more. Temporal Graph Learning (TGL) is a fast growing
    field which aims to learn, predict and understand evolving networks. See our [previous
    blog post](/temporal-graph-learning-in-2023-d28d1640dbf2) for an introduction
    to temporal graph learning and a summary of advancements last year.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 许多复杂的网络随着时间的推移而演化，包括交易网络、交通网络、社交网络等。时序图学习（TGL）是一个快速发展的领域，旨在学习、预测和理解这些不断发展的网络。请参阅我们的[上一篇博客文章](/temporal-graph-learning-in-2023-d28d1640dbf2)，了解时序图学习的介绍以及去年的一些进展。
- en: In 2023, we saw significantly increased interest from both academia and the
    industry in the development of TGL. Compared to last year, the number of submissions
    at the [temporal graph learning workshop @ NeurIPS 2023](https://sites.google.com/view/tglworkshop-2023/home)
    tripled, resulting in 35 accepted papers. In addition, the [temporal graph learning
    reading group](https://www.cs.mcgill.ca/~shuang43/rg.html), started in February
    2023, has now hosted 28 research talks (find the recordings on [YouTube](https://www.youtube.com/@TGL_RG)).
    With nearly 200 researchers signed up for the reading group, we are glad to witness
    interest in the topic and an extremely active community.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年，我们看到学术界和工业界对TGL（时序图学习）发展的兴趣显著增加。与去年相比，[NeurIPS 2023时序图学习研讨会](https://sites.google.com/view/tglworkshop-2023/home)的投稿数量增长了三倍，最终有35篇论文被接收。此外，2023年2月启动的[时序图学习读书小组](https://www.cs.mcgill.ca/~shuang43/rg.html)至今已举办了28场研究报告（可以在[YouTube](https://www.youtube.com/@TGL_RG)上找到录音）。目前，已有近200名研究人员注册加入读书小组，我们很高兴看到该主题受到了关注，并且社区极其活跃。
- en: '![](../Images/dea62afaf0827b1d3fd11fc1ef12e1ab.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dea62afaf0827b1d3fd11fc1ef12e1ab.png)'
- en: Image by authors, generated via [DALL.E 3](https://openai.com/dall-e-3)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供，通过[DALL.E 3](https://openai.com/dall-e-3)生成
- en: '*This post was co-authored with* [*Emanuele Rossi*](https://www.emanuelerossi.co.uk/),
    [*Michael Galkin*](https://migalkin.github.io/) ,[*Andrea Cini*](https://andreacini.github.io/)
    *and* [*Ingo Scholtes*](https://www.ingoscholtes.net/)*.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文由* [*Emanuele Rossi*](https://www.emanuelerossi.co.uk/), [*Michael Galkin*](https://migalkin.github.io/),
    [*Andrea Cini*](https://andreacini.github.io/) *和* [*Ingo Scholtes*](https://www.ingoscholtes.net/)
    *共同撰写*。'
- en: This blog post covers a selection of exciting developments in TGL while pointing
    out research directions for 2024\. We also ask leading researchers for their take
    on what the future holds for TGL. This blog also aims to provide references and
    act as starting points for those who want to learn more about temporal graph learning.
    Please share with us in the comment section any other advances you are excited
    about. For advancements on graph learning, checkout [Michael Galkin](https://mgalkin.medium.com/)’s
    excellent [blog post](/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文涵盖了时序图学习（TGL）领域的一些激动人心的发展，同时指出了2024年的研究方向。我们还邀请了领先的研究人员分享他们对时序图学习未来发展的看法。本文还旨在为那些希望深入了解时序图学习的读者提供参考资料，并作为学习起点。欢迎在评论区与我们分享你感兴趣的其他进展。如需了解更多图学习的进展，请查看[Michael
    Galkin](https://mgalkin.medium.com/)的精彩[博文](/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1)。
- en: '**Table of Contents:**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**目录：**'
- en: '[Temporal Graph Benchmark](#fffa)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[时序图基准](#fffa)'
- en: '[Novel Architectures for Link Prediction](#4d80)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[链接预测的创新架构](#4d80)'
- en: '[Spatiotemporal Graphs and Graph Deep Learning for Time Series Processing](#3e08)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[时空图和图深度学习在时间序列处理中的应用](#3e08)'
- en: '[Temporal Knowledge Graph](#1bf1)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[时序知识图谱](#1bf1)'
- en: '[Causality-Aware Temporal Graph Learning](#bfbb)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[因果感知的时序图学习](#bfbb)'
- en: '[Explainable Temporal Graph Methods](#4876)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[可解释的时序图方法](#4876)'
- en: '[Adversarial Attacks on Temporal Graphs](#328d)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[对时序图的对抗攻击](#328d)'
- en: '[Libraries and Benchmarks](#00ae)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[库和基准](#00ae)'
- en: '[Joining Temporal Graph Learning Community](#31ef)'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[加入时序图学习社区](#31ef)'
- en: Temporal Graph Benchmark
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时序图基准
- en: One of the driving forces of the rapid development of machine learning on graphs
    is the availability of standardized and diverse benchmarks such as the [Open Graph
    Benchmark](https://ogb.stanford.edu/) (OGB), the [Long Range Graph Benchmark](https://openreview.net/forum?id=in7XC5RcjEn)
    and [GraphWorld](https://blog.research.google/2022/05/graphworld-advances-in-graph.html).
    However, these benchmarks are designed for static graphs and lack the fine-grained
    timestamp information required for temporal graph learning. Therefore, progress
    in temporal graph learning has been held back by the lack of large high-quality
    datasets, as well as the lack of proper evaluation resulting in over-optimistic
    performances.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图学习快速发展的驱动力之一是标准化和多样化基准的出现，如[开放图基准](https://ogb.stanford.edu/)（OGB）、[长程图基准](https://openreview.net/forum?id=in7XC5RcjEn)和[GraphWorld](https://blog.research.google/2022/05/graphworld-advances-in-graph.html)。然而，这些基准是为静态图设计的，缺乏进行时序图学习所需的精细时间戳信息。因此，时序图学习的进展受限于缺乏大型高质量数据集，以及缺乏适当的评估方法，导致性能过于乐观。
- en: To address this gap, the [Temporal Graph Benchmark (TGB)](https://tgb.complexdatalab.com/)
    was presented recently, including a collection of challenging and diverse benchmark
    datasets for realistic, reproducible, and robust evaluation for machine learning
    on temporal graphs. TGB provides a [pypi package](https://pypi.org/project/py-tgb/)
    to automatically download and process nine datasets from five distinct domains
    with up to 72 million edges and 30 million timestamps. TGB also provides standardized
    evaluation motivated by real applications.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了填补这一空白，[时序图基准（TGB）](https://tgb.complexdatalab.com/)最近被提出，包括一系列具有挑战性和多样化的基准数据集，用于对时序图上的机器学习进行真实、可重现和鲁棒的评估。TGB提供了一个[pypi包](https://pypi.org/project/py-tgb/)，可以自动下载和处理来自五个不同领域的九个数据集，数据集包含最多7200万个边和3000万个时间戳。TGB还提供了基于真实应用的标准化评估。
- en: '![](../Images/2968f4a125d9de1a57a71b9aa3234353.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2968f4a125d9de1a57a71b9aa3234353.png)'
- en: 'TGB: Challenging and Realistic Benchmark for Temporal Graph Learning.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: TGB：时序图学习的挑战性和真实基准。
- en: 'Image source: [Huang et al. 2023](https://openreview.net/forum?id=qG7IkQ7IBO),
    by authors.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[黄等，2023](https://openreview.net/forum?id=qG7IkQ7IBO)，作者提供。
- en: TGB includes both link and node level tasks and an extensive empirical comparison
    of state-of-the-art TG models on all datasets. The first task is the dynamic link
    property prediction task which predicts the property (often existence) of a link
    between a pair of nodes at a future time. In TGB, this task is modeled as a ranking
    problem and evaluated with the filtered Mean Reciprocal Rank (MRR) metric. Results
    show that model rankings vary significantly across datasets with different ratios
    of test set edges which are never observed during training. In addition, model
    performance deteriorates as more negative samples (non-existence edges) are used
    in the evaluation. Interestingly, the extent of performance drop varies across
    models as well.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: TGB 包含了链接和节点级任务，并对所有数据集上的最先进 TG 模型进行了广泛的实证比较。第一个任务是动态链接属性预测任务，预测未来某一时刻一对节点之间的链接属性（通常是存在性）。在
    TGB 中，这个任务被建模为一个排序问题，并通过过滤后的平均倒数排名（MRR）指标进行评估。结果表明，模型排名在不同数据集上差异显著，数据集中的测试集边缘比例在训练过程中从未出现过。此外，随着更多负样本（不存在的边）用于评估，模型的表现会下降。有趣的是，表现下降的程度在不同模型之间也有所不同。
- en: In the dynamic node property prediction task, the goal is to predict the property
    of a node at a given time. More specifically we focus on the node affinity prediction
    task which models how the user preference towards different items shift over time.
    Here, we use the Normalized Discounted Cumulative Gain of the top 10 items (NDCG@10)
    to compare the relative order of the predicted items to that of the ground truth.
    Interesting, we found that single heuristics outperform existing TG models and
    this highlights the need for more models focusing on node level tasks in the future.
    The [TGB leaderboard](https://tgb.complexdatalab.com/docs/leader_linkprop/) is
    public and you are welcome to submit your model via a [google form](https://docs.google.com/forms/d/e/1FAIpQLSfmvBRgRPeR8bK3ubiwvJd1k26PDI_yVZDRXXRGgU7uSqJWZg/viewform).
    For more details, see the [TGB blog post](https://medium.com/towards-data-science/temporal-graph-benchmark-bb5cc26fcf11)
    by the authors of this blog.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在动态节点属性预测任务中，目标是预测给定时间节点的属性。更具体地，我们关注节点亲和力预测任务，该任务建模用户对不同物品的偏好随时间的变化。在这里，我们使用前
    10 个物品的标准化折扣累积增益（NDCG@10）来比较预测物品的相对顺序与真实值的顺序。有趣的是，我们发现单一启发式方法优于现有的 TG 模型，这突显了未来需要更多专注于节点级任务的模型。[TGB
    排行榜](https://tgb.complexdatalab.com/docs/leader_linkprop/)是公开的，欢迎通过[Google 表单](https://docs.google.com/forms/d/e/1FAIpQLSfmvBRgRPeR8bK3ubiwvJd1k26PDI_yVZDRXXRGgU7uSqJWZg/viewform)提交您的模型。更多详情，请参阅[TGB
    博客文章](https://medium.com/towards-data-science/temporal-graph-benchmark-bb5cc26fcf11)，由该博客的作者提供。
- en: Novel Architectures for Link Prediction
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链接预测的新架构
- en: “Link prediction in the realm of temporal graph learning poses a significant
    challenge. The learning algorithms must extend beyond the limited expressive power
    typically found in traditional message passing architectures like GNNs. Additionally,
    they must emphasize computational efficiency. A critical aspect of this is ensuring
    low latency in responding to link prediction queries, striking a balance between
    the expressive power of the model and the speed of its predictions in a dynamic
    and complex data environment.” — Pan Li, Assistant Professor, Georgia Institute
    of Technology
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在时序图学习领域，链接预测提出了巨大的挑战。学习算法必须超越传统信息传递架构（如 GNNs）通常所具有的有限表现力。此外，它们还必须强调计算效率。一个关键方面是确保在响应链接预测查询时具有低延迟，在动态和复杂的数据环境中平衡模型的表现力与预测速度。”
    —— 潘力，乔治亚理工学院助理教授
- en: A recent survey by [Longa et al.](https://openreview.net/forum?id=pHCdMat0gI&referrer=%5BTMLR%5D%28%2Fgroup%3Fid%3DTMLR%29)
    provides a comprehensive overview of temporal GNNs. Many approaches proposed specialized
    architectures for dynamic link prediction, often aiming to capture important structure
    properties or correlations. For example, [Luo et al.](https://arxiv.org/abs/2209.01084)
    aimed to explicitly model the joint neighborhood of a set of nodes for future
    link prediction where they designed the **N**eighborhood-**A**ware **T**emporal
    network model (NAT). The joint neighborhood is not captured by traditional Graph
    Neural Network (GNN) based approaches as the node embedding vectors are generated
    independently for each node. In the following example, node *v* and *w* have the
    same structural contexts thus indistinguishable in the eyes of GNNs. In reality,
    the link between node *u* and *v* at t₃ is more likely to form due to the triadic
    closure law while this is not sure for the link between *u* and *w* at t₃. In
    comparison, NAT adapted a novel dictionary-type neighborhood representation which
    records k-hop neighborhood information and allows fast construction of structure
    features of joint neighborhood of multiple nodes. The dictionary representation
    is maintained by an efficient cache technique named N-cache. N-caches allowed
    NAT to construct the joint neighborhood features for a batch of node pairs for
    fast link prediction.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[Longa et al.](https://openreview.net/forum?id=pHCdMat0gI&referrer=%5BTMLR%5D%28%2Fgroup%3Fid%3DTMLR%29)最近的调查提供了关于时间GNN的全面概述。许多方法提出了专门的架构用于动态链接预测，通常旨在捕获重要的结构属性或相关性。例如，[Luo
    et al.](https://arxiv.org/abs/2209.01084)旨在显式地建模一组节点的联合邻域，用于未来的链接预测，他们设计了**N**eighborhood-**A**ware
    **T**emporal网络模型（NAT）。传统的图神经网络（GNN）方法无法捕获联合邻域，因为节点嵌入向量是为每个节点独立生成的。在下面的示例中，节点*v*和*w*具有相同的结构上下文，因此在GNN眼中是不可区分的。实际上，由于三元闭合法则，节点*u*和*v*在t₃时的链接更可能形成，而节点*u*和*w*在t₃时的链接则不确定。相比之下，NAT采用了一种新的字典型邻域表示，记录k跳邻域信息，并允许快速构建多个节点的联合邻域的结构特征。字典表示通过一种名为N-cache的高效缓存技术进行维护。N-cache使NAT能够为一批节点对快速构建联合邻域特征，用于快速链接预测。'
- en: '![](../Images/613d7fba8ce836fcc6dad7fbd2dc2f51.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/613d7fba8ce836fcc6dad7fbd2dc2f51.png)'
- en: GNN embeddings of node *v* and w would be identical.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: GNN嵌入的节点*v*和w将是相同的。
- en: 'Image source: [Luo et al. 2022](https://arxiv.org/abs/2209.01084)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Luo et al. 2022](https://arxiv.org/abs/2209.01084)
- en: Second, [Yu et al.](https://openreview.net/forum?id=xHNzWHbklj) aim to capture
    long-term temporal dependencies by proposing DyGFormer, a new Transformer-based
    architecture for temporal graph learning. Given a query between node *u* and node
    *v* at time *t*, the first step is to extract historical first-hop interactions
    of node *u* and *v* before time *t*. This includes the encodings of neighbors,
    links, time intervals as well as the frequencies of every neighbor’s appearances
    of *u* and *v*. The assumption is that if *u* and *v* share more common historical
    neighbors in the past, then they are more likely to interact in the future. After
    encoding the historical interactions in a sequence, it is then divided into multiple
    patches and fed into a transformer for capturing temporal dependencies.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，[Yu et al.](https://openreview.net/forum?id=xHNzWHbklj)旨在通过提出DyGFormer，一种基于Transformer的时间图学习架构，捕获长期时间依赖性。给定时间*t*时节点*u*和节点*v*之间的查询，第一步是提取时间*t*之前节点*u*和*v*的历史第一跳交互。这包括邻居的编码、链接、时间间隔以及每个邻居在*u*和*v*中的出现频率。假设如果*u*和*v*在过去有更多共同的历史邻居，那么它们在未来更可能发生交互。将历史交互编码为一个序列后，接下来将其划分为多个补丁，并输入到Transformer中以捕获时间依赖性。
- en: '![](../Images/213d11dc93944f290c9be0ba52cd291f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/213d11dc93944f290c9be0ba52cd291f.png)'
- en: Framework of DyGFormer
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: DyGFormer框架
- en: 'Image source: [Yu et al. 2023](https://openreview.net/forum?id=xHNzWHbklj)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Yu et al. 2023](https://openreview.net/forum?id=xHNzWHbklj)
- en: 'Another question is *do we really need complicated model architectures for
    temporal networks?* In a paper with the same name, [Cong et al.](https://openreview.net/forum?id=ayPPc0SyLv1)
    examined the necessity of common modules used in temporal graph learning such
    as Recurrent Neural Network (RNN) and the self-attention mechanism. They showed
    that these modules are not always necessary for dynamic link prediction. In particular,
    their proposed GraphMixer model is based entirely on multi-layer perceptrons (MLPs)
    and neighbor mean-pooling while performing strongly against baselines with RNN
    and self-attention. GraphMixer contains three modules: a *link-encoder* summarizes
    the information from temporal links, a *node-encoder* extracts information from
    nodes and a *link-classifier* which combines the above information for prediction.
    Interestingly, [Cong et al.](https://openreview.net/forum?id=ayPPc0SyLv1) argued
    that a trainable time-encoding function could cause instability during training
    and instead opted for a fixed time-encoding function *z(t) = cos(tω)* where fixed
    features *capture* the relative difference between two timestamps as shown below.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是 *我们真的需要复杂的模型架构来处理时间网络吗？* 在一篇同名论文中，[Cong 等人](https://openreview.net/forum?id=ayPPc0SyLv1)考察了在时间图学习中常用模块（如递归神经网络（RNN）和自注意力机制）的必要性。他们表明，这些模块并不总是动态链接预测所必需的。特别地，他们提出的GraphMixer模型完全基于多层感知机（MLP）和邻居均值池化，并在与包含RNN和自注意力机制的基准模型比较时表现出色。GraphMixer包含三个模块：*链接编码器*总结来自时间链接的信息，*节点编码器*提取节点的信息，*链接分类器*结合以上信息进行预测。有趣的是，[Cong
    等人](https://openreview.net/forum?id=ayPPc0SyLv1)认为可训练的时间编码函数可能在训练过程中导致不稳定，因此选择了固定时间编码函数
    *z(t) = cos(tω)*，其中固定特征 *捕捉* 了两个时间戳之间的相对差异，如下所示。
- en: '![](../Images/f552a8d490b6e445f452de05df940622.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f552a8d490b6e445f452de05df940622.png)'
- en: Fixed time encoding function to convert *t* into a vector *cos(tω)*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 固定时间编码函数将 *t* 转换为向量 *cos(tω)*。
- en: x-axis is the vector dimension and y-axis is the cosine value.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: x轴表示向量维度，y轴表示余弦值。
- en: 'Image source: [Cong et al. 2023](https://openreview.net/forum?id=ayPPc0SyLv1)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[Cong 等人 2023](https://openreview.net/forum?id=ayPPc0SyLv1)
- en: Lastly, [Suresh et al.](https://dl.acm.org/doi/abs/10.1145/3543507.3583476)
    pointed out that existing methods maximizes accuracy independently over future
    links, ignoring the fact that future links often have dependency between each
    other. This is seen when a user selects among a list of items to purchase or a
    set of users to connect in a social network. Therefore, Suresh et al. treat dynamic
    link prediction as a ranking problem and propose **T**emporal **G**raph network
    for **RANK**ing (TGRank) to learn to rank over a list of candidates. The pipeline
    of TGRank is shown below. The task query now contains a center node *s* (in the
    example) with a set of candidate nodes (all other nodes in the subgraph) and the
    goal is to rank the most likely candidate as the destination of node *s*. To this
    end, TGRank follows three steps. First, the node *s* is labeled differently from
    other nodes. Then, GNNs are used to diffuse the center node label to every ranking
    candidate. This parametrized label diffusion step aggregates timestamps, multiplicity
    as well as features of historical interactions along the network from the center
    node to all candidates and provides provably more expressive power for link prediction.
    Lastly, a list-wise loss is used to optimize the ranking amongst candidates jointly.
    Empirically, it is also shown that with a listwise ranking loss, popular models
    such as [TGN](https://arxiv.org/abs/2006.10637) and [TGAT](https://arxiv.org/abs/2002.07962)
    also perform better than their original setup with binary classification loss.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，[Suresh 等人](https://dl.acm.org/doi/abs/10.1145/3543507.3583476)指出，现有方法在未来链接上独立最大化准确度，忽略了未来链接之间通常存在依赖关系的事实。这种情况在用户从一系列商品中选择购买物品或在社交网络中选择一组用户进行连接时尤为明显。因此，Suresh
    等人将动态链接预测视为一个排序问题，并提出了**T**emporal **G**raph网络用于**RANK**ing（TGRank），通过该方法学习对候选项进行排名。TGRank的流程图如下所示。任务查询现在包含一个中心节点
    *s*（在示例中）以及一组候选节点（子图中的所有其他节点），目标是对最可能的候选项进行排名，作为节点 *s* 的目标。为此，TGRank遵循三个步骤。首先，节点
    *s* 被与其他节点标注不同。然后，使用GNN（图神经网络）将中心节点标签传播到每个排序候选项。这个参数化的标签传播步骤聚合了时间戳、重复性以及沿着网络从中心节点到所有候选项的历史交互特征，并为链接预测提供了明显更强的表达能力。最后，使用基于列表的损失函数来联合优化候选项之间的排名。从经验上看，使用列表式排名损失后，像[TGN](https://arxiv.org/abs/2006.10637)和[TGAT](https://arxiv.org/abs/2002.07962)等流行模型的表现也优于其原始设置中的二分类损失。
- en: '![](../Images/b452c8a5094427bee68c6aa119670a18.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b452c8a5094427bee68c6aa119670a18.png)'
- en: Pipeline of TGRank. The center node is node *s*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: TGRank的流程图。中心节点是节点*s*。
- en: 'Image source: [Suresh et al. 2023](https://dl.acm.org/doi/abs/10.1145/3543507.3583476)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[Suresh et al. 2023](https://dl.acm.org/doi/abs/10.1145/3543507.3583476)
- en: Spatiotemporal Graphs and Graph Deep Learning for Time Series Processing
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时空图与图深度学习在时间序列处理中的应用
- en: “Basing our predictions primarily on the most related observations is sensible,
    yet not always straightforward, as relevant data relations often hide in plain
    sight. Unveiling them is a captivating challenge, particularly when they are dynamic
    or involve more than two entities.” — Daniele Zambon, PostDoc at The Swiss AI
    Lab IDSIA
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “将我们的预测主要基于最相关的观察结果是明智的，但并不总是简单的，因为相关的数据关系常常隐藏在显而易见的地方。揭示这些关系是一个引人入胜的挑战，尤其是当它们是动态的，或者涉及超过两个实体时。”
    — Daniele Zambon，瑞士人工智能实验室IDSIA的博士后
- en: In the temporal graph learning community, the term *spatiotemporal graph* has
    been often used to indicate a graph with fixed topology and node features that
    change over time, usually at discrete time steps corresponding to regularly sampled
    observations. More recently the problem of processing data with such a structure
    is being considered from a different perspective, i.e., by considering the dynamic
    node feature as time series and edges as functional dependencies among sequences
    of observations ([Cini et al. 2023](https://arxiv.org/abs/2310.15978), [Ming et
    al. 2023](https://arxiv.org/abs/2307.03759)). From this perspective, which significantly
    deviates from many of the settings discussed elsewhere in this blog post, spatiotemporal
    graph-based representations allow for processing collections of correlated time
    series by taking advantage of the architectural biases typical of graph neural
    networks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在时序图学习社区中，*时空图*这一术语通常用来表示具有固定拓扑结构和随时间变化的节点特征的图，通常是在离散的时间步长上，这些时间步长对应于定期采样的观测数据。最近，处理具有这种结构的数据问题正在从不同的角度进行考虑，即将动态节点特征视为时间序列，而将边视为观测序列之间的函数依赖关系（[Cini
    et al. 2023](https://arxiv.org/abs/2310.15978)，[Ming et al. 2023](https://arxiv.org/abs/2307.03759)）。从这一角度来看，时空图表示法允许通过利用图神经网络的架构偏向来处理关联时间序列集合，这一视角与本文讨论的许多其他设定有显著的偏离。
- en: '![](../Images/1f7d08d5aa14384b4ae4e0d9bcd53c9e.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f7d08d5aa14384b4ae4e0d9bcd53c9e.png)'
- en: Correlated time series with relational side information
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 具有关联侧信息的时间序列
- en: 'Image source: [Cini et al. 2023](https://arxiv.org/abs/2310.15978), by authors.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[Cini et al. 2023](https://arxiv.org/abs/2310.15978)，作者提供。
- en: Such sets of correlated time series can be generated by sensors, either physical
    or not. An example is in the traffic domain, where time series might correspond
    to readings of sensors measuring the number of vehicles passing by at a crossroads.
    Each sensor will correspond to a different node and an adjacency matrix can be
    obtained, for instance, by joining with an edge only those sensors directly connected
    by a road segment. Besides traffic forecasting ([Li et al. 2018](https://openreview.net/forum?id=SJiHXGWAZ),
    [Yu et al. 2018](https://www.ijcai.org/proceedings/2018/505)), these representations
    have been used in a wide range of time series processing applications ranging
    from air quality monitoring ([Chen et al. 2021](https://dl.acm.org/doi/10.1145/3631713))
    and energy analytics ([Cini et al. 2023](https://ojs.aaai.org/index.php/AAAI/article/view/25880))
    to biomedical data processing ([Zhang et al. 2022](https://openreview.net/forum?id=Kwm8I7dU-l5))
    and financial time series analysis ([Matsunaga et al. 2019](https://arxiv.org/abs/1909.10660)).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的关联时间序列集可以通过传感器生成，无论是物理传感器还是非物理传感器。例如，在交通领域，时间序列可能对应于传感器读取的数据，这些传感器测量的是在交叉口通过的车辆数量。每个传感器将对应一个不同的节点，邻接矩阵可以通过与仅通过路段直接连接的传感器连接边来获得。除了交通预测（[Li
    et al. 2018](https://openreview.net/forum?id=SJiHXGWAZ)，[Yu et al. 2018](https://www.ijcai.org/proceedings/2018/505)），这些表示法还广泛应用于时间序列处理的各类应用中，涵盖了从空气质量监测（[Chen
    et al. 2021](https://dl.acm.org/doi/10.1145/3631713)）和能源分析（[Cini et al. 2023](https://ojs.aaai.org/index.php/AAAI/article/view/25880)）到生物医学数据处理（[Zhang
    et al. 2022](https://openreview.net/forum?id=Kwm8I7dU-l5)）和金融时间序列分析（[Matsunaga
    et al. 2019](https://arxiv.org/abs/1909.10660)）等各个领域。
- en: '![](../Images/d48b917c91b2d348893a61a1f6ad501c.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d48b917c91b2d348893a61a1f6ad501c.png)'
- en: Example of correlated time series from the traffic domain.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 来自交通领域的关联时间序列示例。
- en: 'Image source: [tutorial at ECML PKDD 2023](https://gmlg.ch/tutorials/graph-based-processing/ecml-2023)
    by authors.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[ECML PKDD 2023教程](https://gmlg.ch/tutorials/graph-based-processing/ecml-2023)，作者提供。
- en: To process this data, the standard message-passing framework needs to be updated
    to handle sequences of observations coming from the neighborhood of each node.
    This can easily be done by replacing the proper operators (i.e., the message and
    update functions) with operators able to process the data along the temporal dimension,
    e.g., recurrent cells ([Seo et al. 2018](https://arxiv.org/abs/1612.07659)), spatiotemporal
    convolutions ([Wu et al. 2019](https://www.ijcai.org/proceedings/2019/0264)) and
    attention-based architectures ([Marisca et al. 2022](https://papers.nips.cc/paper_files/paper/2022/hash/cf70320e93c08b39b1b29a348097a376-Abstract-Conference.html)).
    The resulting models are known as spatiotemporal graph neural networks (STGNNs)
    and there has been a large amount of research dedicated to coming up with effective
    architectures (see [Ming et al. 2023](https://arxiv.org/abs/2307.03759)). One
    of the main advantages of using STGNNs is that the same set of parameters can
    be used to forecast any subset of time series, while taking dependencies into
    account throughout the processing. This is a massive advantage over standard multivariate
    time series forecasting models that usually would have to forecast each time series
    separately or give up parameter sharing. Hybrid STGNNs, with some time-series-specific
    parameters, can also be considered and, as we have shown in [a recent NeurIPS
    paper](https://openreview.net/forum?id=x2PH6q32LR), often outperform models where
    all parameters are shared. Besides the model architecture, graph-based representations,
    as shown by [Zambon et al.](https://openreview.net/forum?id=SFeKNSxect), can also
    help in assessing the optimality of a forecasting model by focusing the spatiotemporal
    correlation analysis to interconnected nodes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这些数据，标准的消息传递框架需要更新，以处理来自每个节点邻域的观测序列。这可以通过替换适当的操作符（即消息和更新函数）为能够沿时间维度处理数据的操作符来轻松实现，例如递归单元（[Seo
    et al. 2018](https://arxiv.org/abs/1612.07659)），时空卷积（[Wu et al. 2019](https://www.ijcai.org/proceedings/2019/0264)）和基于注意力的架构（[Marisca
    et al. 2022](https://papers.nips.cc/paper_files/paper/2022/hash/cf70320e93c08b39b1b29a348097a376-Abstract-Conference.html)）。由此产生的模型被称为时空图神经网络（STGNNs），并且已经有大量研究致力于提出有效的架构（见
    [Ming et al. 2023](https://arxiv.org/abs/2307.03759)）。使用STGNNs的主要优势之一是可以使用相同的参数集来预测任何子集的时间序列，同时在处理过程中考虑到依赖关系。这比标准的多元时间序列预测模型具有巨大的优势，因为后者通常需要单独预测每个时间序列或放弃参数共享。也可以考虑使用混合型STGNNs，带有一些特定于时间序列的参数，正如我们在[a
    recent NeurIPS paper](https://openreview.net/forum?id=x2PH6q32LR)中所展示的，往往优于那些共享所有参数的模型。除了模型架构之外，基于图的表示，如[Zambon
    et al.](https://openreview.net/forum?id=SFeKNSxect)所示，也可以通过将时空相关性分析聚焦于相互连接的节点，帮助评估预测模型的最优性。
- en: '![](../Images/5e4d1ef2c085ad6c04f2fe38a418f15d.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e4d1ef2c085ad6c04f2fe38a418f15d.png)'
- en: A spatiotemporal graph neural network.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个时空图神经网络。
- en: Image by authors.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: Several challenges are inherent to the field, starting from dealing with [irregularly
    sampled time series](https://arxiv.org/abs/2012.00168) and missing data; indeed,
    both are quite common phenomena when dealing with actual cyber-physical systems.
    Luckily, graph-based models are useful in this context as well, for example allowing
    to [condition the reconstruction on observations at neighboring sensors](https://openreview.net/forum?id=kOu3-S3wJ7).
    Scalability is another major concern as differently from standard GNNs, message-passing
    is often performed w.r.t. each time step. Existing scalable architectures mostly
    rely on [subsampling](https://assets.amazon.science/50/90/df9385f840c7b0363febf882a6ad/spatio-temporal-multi-graph-networks-fordemand-forecasting-in-online-marketplaces.pdf)
    and/or [pre-computed node features](https://ojs.aaai.org/index.php/AAAI/article/view/25880).
    When no prior relation information is available, the challenge becomes that of
    learning a latent graph directly from the time series. The problem has been tackled,
    for example, by directly learning an adjacency matric (e.g., [Wu et al. 2019](https://www.ijcai.org/proceedings/2019/0264))
    or, under a probabilistic framework, by relying on [reparametrization tricks](https://proceedings.mlr.press/v80/kipf18a)
    and [score-based estimators](https://www.jmlr.org/papers/v24/22-1154.html).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域固有的几个挑战，包括处理[不规则采样的时间序列](https://arxiv.org/abs/2012.00168)和缺失数据；实际上，这两种现象在处理实际的网络物理系统时非常常见。幸运的是，图模型在这一背景下也很有用，例如可以[基于相邻传感器的观察结果来条件化重建](https://openreview.net/forum?id=kOu3-S3wJ7)。可扩展性是另一个主要问题，因为与标准的图神经网络不同，消息传递通常是针对每个时间步进行的。现有的可扩展架构主要依赖于[子采样](https://assets.amazon.science/50/90/df9385f840c7b0363febf882a6ad/spatio-temporal-multi-graph-networks-fordemand-forecasting-in-online-marketplaces.pdf)和/或[预计算的节点特征](https://ojs.aaai.org/index.php/AAAI/article/view/25880)。当没有先验关系信息时，挑战就变成了如何直接从时间序列中学习潜在图。例如，问题已经被通过直接学习邻接矩阵（例如，[Wu
    等，2019](https://www.ijcai.org/proceedings/2019/0264)）或在概率框架下，依赖于[重参数化技巧](https://proceedings.mlr.press/v80/kipf18a)和[基于评分的估计器](https://www.jmlr.org/papers/v24/22-1154.html)来解决。
- en: '![](../Images/29f300758be7f7f4f941ad96bcf14338.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29f300758be7f7f4f941ad96bcf14338.png)'
- en: A scalable spatiotemporal graph neural network
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展的时空图神经网络
- en: 'Image source: [Cini et al. 2023](https://ojs.aaai.org/index.php/AAAI/article/view/25880).
    by authors'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Cini 等，2023](https://ojs.aaai.org/index.php/AAAI/article/view/25880)。由作者提供
- en: Since this topic was not covered in last year’s blog post, the objective here
    was to give a short overview of the settings and the problems that can be modeled.
    Many directions are currently being explored, from [graph state-space models](https://arxiv.org/abs/2301.01741)
    and [graph Kalman filters](https://arxiv.org/abs/2303.12021) to [diffusion-based](https://dl.acm.org/doi/10.1145/3589132.3625614)
    and [continuous space-time](https://openreview.net/forum?id=Oq5XKRVYpQ) models.
    If you are interested in knowing more and/or using these models in practice, we
    recently released a comprehensive [tutorial](https://arxiv.org/abs/2310.15978)
    paper on the topic. You can also check out [Torch Spatiotemporal (tsl)](https://torch-spatiotemporal.readthedocs.io/en/latest/),
    our library to build graph-based time series processing pipelines.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于去年博客中没有涉及这一主题，因此本文的目的是简要概述相关设置以及可以建模的问题。目前有很多方向正在探索，包括[图状态空间模型](https://arxiv.org/abs/2301.01741)、[图卡尔曼滤波器](https://arxiv.org/abs/2303.12021)、[基于扩散的模型](https://dl.acm.org/doi/10.1145/3589132.3625614)以及[连续时空模型](https://openreview.net/forum?id=Oq5XKRVYpQ)。如果你有兴趣了解更多内容和/或在实践中使用这些模型，我们最近发布了一篇关于该主题的综合[教程](https://arxiv.org/abs/2310.15978)论文。你还可以查看我们的库[Torch
    Spatiotemporal (tsl)](https://torch-spatiotemporal.readthedocs.io/en/latest/)，它用于构建基于图的时间序列处理管道。
- en: 'Tutorial paper: [Graph Deep Learning for Time Series Forecasting](https://arxiv.org/abs/2310.15978)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教程论文：[图深度学习在时间序列预测中的应用](https://arxiv.org/abs/2310.15978)
- en: 'Library: [Torch Spatiotemporal (tsl)](https://torch-spatiotemporal.readthedocs.io/en/latest/)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库：[Torch Spatiotemporal (tsl)](https://torch-spatiotemporal.readthedocs.io/en/latest/)
- en: Temporal Knowledge Graph
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时序知识图
- en: 'There were surprisingly few temporal KG papers in this year’s top ML conferences:
    **TILP** ([Xiong et al. 2023](https://openreview.net/forum?id=_X12NmQKvX)) on
    deriving temporal rule learning competitive with neural methods, and theory work
    by [Chen and Wang](https://openreview.net/forum?id=AtHJ7TLheF) to measure expressiveness
    of temporal GNNs. In fact, the most interesting (to me) papers on this topic were
    found at the TGL workshop at NeurIPS’23 (one more reason for you to follow the
    venue!), e.g., predicting future time intervals by [Pop and Kostylev](https://openreview.net/forum?id=9B8ocBg4VJ),
    or identifying leakages in standard benchmarking datasets by [Pan et al](https://openreview.net/forum?id=UMokRwWfLW).
    Finally, I’d outline the **Unified Urban KG (UUKG)** by [Ning et al](https://arxiv.org/abs/2306.11443)
    as a fresh look on temporal KG datasets that actually make practical sense and
    a use-case — modeling transportation flows in the city.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，今年在顶级机器学习会议上有很少的时序KG论文：**TILP**（[Xiong 等，2023](https://openreview.net/forum?id=_X12NmQKvX)）探讨了与神经方法竞争的时序规则学习，以及[Chen和Wang](https://openreview.net/forum?id=AtHJ7TLheF)的理论研究，旨在衡量时序GNN的表现力。事实上，最有趣的（对我来说）这类论文是在NeurIPS’23的TGL研讨会上找到的（这也是你应该关注该会议的一个原因！），例如，[Pop和Kostylev](https://openreview.net/forum?id=9B8ocBg4VJ)预测未来时间间隔，或[Pan等人](https://openreview.net/forum?id=UMokRwWfLW)在标准基准数据集中识别数据泄露。最后，我想提到[Ning等人](https://arxiv.org/abs/2306.11443)的**统一城市知识图谱（UUKG）**，它为时序KG数据集提供了一个全新的视角，真正具有实际意义并且有应用场景——建模城市中的交通流。
- en: UUKG illustrates the biggest problem of the shrinking temporal KG community
    — the lack of practically important tasks and datasets where it would be possible
    to demonstrate the utility of the data modeling paradigm in real-world tasks.
    That is, adding 1% of MRR/Hits@10 on 10-year old KG embedding benchmarks is rather
    useless these days compared to the successes of Geometric Deep Learning in biology
    or materials science (or compared to LLMs, but that’s a story for another day).
    Hopefully, we will see more UUKG-like practically useful datasets.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: UUKG展示了时序知识图谱（KG）社区萎缩的最大问题——缺乏实际重要的任务和数据集，在这些任务中可以展示数据建模范式在现实世界任务中的实用性。也就是说，与几何深度学习在生物学或材料科学中的成功相比（或者与大型语言模型相比，但那是另一个话题），在10年历史的KG嵌入基准上增加1%的MRR/Hits@10如今显得毫无意义。希望未来我们能看到更多类似UUKG的实用数据集。
- en: Perhaps another adjacent area where temporal KGs might make a difference is
    heterogeneous graphs (that usually have typed edges) that are much more used in
    industry. For example, the recent [RelBench](https://relbench.stanford.edu/) (Relational
    Deep Learning Benchmark) formulates a temporal prediction problem over relational
    databases that can be easily converted to KGs or hypergraphs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 也许时序KG可能产生影响的另一个相关领域是异构图（通常具有类型化边缘），这些图在工业界的使用频率更高。例如，最近的[RelBench](https://relbench.stanford.edu/)（关系深度学习基准）提出了一个基于关系数据库的时序预测问题，这个问题可以轻松转化为KG或超图。
- en: Causality-Aware Temporal Graph Learning
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 因果感知的时序图学习
- en: “Einstein said the arrow of time flies in only one direction. […] And who among
    us, offered the chance, would not relive the day or hour in which we first knew
    love, or ecstasy, or made a choice that forever altered our future, negating a
    life we might have had? Such chances are rarely granted.” — Greg Iles, The Quiet
    Game
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “爱因斯坦说，时间的箭头只朝一个方向飞去。[……]在我们当中，有谁愿意放弃重温那个曾让我们第一次体会到爱、狂喜，或做出一个永远改变我们未来的选择的日子或时刻，抹去可能曾有过的一种生活？这样的机会是很少有的。”——格雷格·艾尔斯，《安静的游戏》
- en: 'One reason why temporal graph learning is interesting is that — depending on
    the data at hand — it requires different perspectives. As an example, consider
    temporal graphs data with high-resolution (possibly continuous) time stamps. In
    such data, discrete-time graph learning techniques that utilize sequences of snapshot
    graphs require a coarse-graining of time, where each snapshot consists of edges
    occurring in a certain time interval. This coarse-graining allows to generalize
    (static) graph learning techniques to time series data. But it introduces a major
    issue: Each snapshots discards information on the temporal order in which edges
    occurred, which is the foundation of *causal* or *time-respecting paths* [(Kempe
    et al. 2000)](https://www.sciencedirect.com/science/article/pii/S0022000002918295).
    Like paths in static graphs, time-respecting paths are important since they tell
    us which nodes can causally influence each other indirectly. Below, we illustrate
    this in a simple temporal graph with two undirected edges *(a,b)* and *(b,c)*
    occurring at times *t₁* and *t₂* respectively. If *(a,b)* occurs before *(b,c)*,
    *a* can causally influence *c* via a time-respecting path (indicated in purple)
    passing through *b*. If the temporal order of edges is reversed, *a* cannot causally
    influence *c*, since any influence must propagate backwards in time. Note that
    the directedness of the influence from *a* to *c* is due to the directed arrow
    of time and despite the fact that both edges are undirected. Moreover, while two
    edges *(a,b)* and *(b,c)* in a static, time-aggregated graph imply a transitive
    path from *a* via *b* to *c* (purple) and vice-versa (orange), this is not true
    for temporal graphs.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 时间图学习之所以有趣，部分原因是它——根据手头的数据——需要不同的视角。例如，考虑具有高分辨率（可能是连续）时间戳的时间图数据。在这种数据中，利用快照图序列的离散时间图学习技术需要对时间进行粗粒化，其中每个快照由在某个时间间隔内发生的边缘组成。这种粗粒化使得可以将（静态）图学习技术推广到时间序列数据。但它也带来了一个主要问题：每个快照都会丢弃关于边缘发生的时间顺序的信息，而时间顺序是*因果*或*时间尊重路径*的基础[(Kempe
    et al. 2000)](https://www.sciencedirect.com/science/article/pii/S0022000002918295)。像静态图中的路径一样，时间尊重路径很重要，因为它们告诉我们哪些节点可以间接地因果影响彼此。下面，我们通过一个简单的时间图进行说明，其中包含两条无向边*(a,b)*和*(b,c)*，分别发生在时间*t₁*和*t₂*。如果*(a,b)*发生在*(b,c)*之前，则*a*可以通过一条时间尊重路径（紫色路径）因果影响*c*，该路径经过*b*。如果边的时间顺序颠倒，*a*则无法因果影响*c*，因为任何影响必须在时间上逆向传播。请注意，*a*到*c*的影响是有方向的，这是由于时间的箭头方向，尽管这两条边都是无向的。此外，虽然在静态的、时间聚合图中，边*(a,b)*和*(b,c)*表示从*a*通过*b*到*c*的传递路径（紫色）以及反向路径（橙色），但在时间图中并非如此。
- en: '![](../Images/1c51adfe45fe86ccf53bc1661740a9a0.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c51adfe45fe86ccf53bc1661740a9a0.png)'
- en: Time-respecting path from node a to node c.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从节点a到节点c的时间尊重路径。
- en: Image by authors.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者。
- en: Several works have shown that — due to the arrow of time — the *causal topology*
    *of temporal graphs*, i.e. which nodes can possibly causally influence each other
    via time-respecting paths, strongly differs from their static counterparts, with
    interesting implications for epidemic spreading ([Pfitzner et al. 2013](https://link.aps.org/doi/10.1103/PhysRevLett.110.198701)),
    diffusion speed ([Scholtes et al. 2014](https://www.nature.com/articles/ncomms6024)),
    node centralities ([Rosvall et al. 2014](https://www.nature.com/articles/ncomms5630)),
    or community detection ([Lambiotte et al. 2019](https://www.nature.com/articles/s41567-019-0459-y)).
    Can we make deep learning methods aware of patterns in the causal topology of
    temporal graphs? Advances presented at this year show that this can be achieved
    based on models that generalize commonly used static representations of temporal
    graphs. Consider a weighted time-aggregated graph, where a (directed) edge *(a,b)*
    with weight five captures that *(a,b)* occurred five times in a temporal graph.
    Such a weighted, time-aggregated graph is illustrated in the bottom of panel 2
    in the figure below.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 多项研究表明——由于时间的箭头——时间图的*因果拓扑*，即哪些节点可以通过时间尊重路径因果影响彼此，与静态图相比有很大不同，这对流行病传播（[Pfitzner
    et al. 2013](https://link.aps.org/doi/10.1103/PhysRevLett.110.198701)）、扩散速度（[Scholtes
    et al. 2014](https://www.nature.com/articles/ncomms6024)）、节点中心性（[Rosvall et al.
    2014](https://www.nature.com/articles/ncomms5630)）或社区检测（[Lambiotte et al. 2019](https://www.nature.com/articles/s41567-019-0459-y)）具有重要的影响。今年提出的进展表明，可以基于模型实现这一目标，这些模型将常用的时间图静态表示进行了一般化。考虑一个加权的时间聚合图，其中一条（有向）边*(a,b)*的权重为5，表示*(a,b)*在时间图中出现了五次。下图中2面板的底部展示了这种加权的时间聚合图。
- en: '![](../Images/c823d73a9ca3e993e9903aebfcf14986.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c823d73a9ca3e993e9903aebfcf14986.png)'
- en: Pipeline to predict temporal centralities of nodes in a temporal graph.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 预测时间图中节点时间中心性的流程图。
- en: 'Image source: [Heeg et al.](https://arxiv.org/abs/2310.15865), by authors'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Heeg 等人](https://arxiv.org/abs/2310.15865)，作者提供
- en: 'Each edge in the temporal graph is a time-respecting path with length one.
    A weighted time-aggregated graph thus corresponds to a first-order model for the
    causal topology of a temporal graph, which captures time-respecting paths of length
    one. It neglects the temporal ordering of edges, since we only count how often
    each edge occurred. A line graph transformation enables us to generalize this
    idea tocausality-aware modelsthat facilitate temporal graph learning: We simply
    replace edges in the first-order graph by nodes in a *second-order* graph, i.e.
    we turn edges *(a,b)* and *(b,c)* into nodes *“a→b”* and *“b→c”*, respectively.
    In the resulting second-order graph (see the top graph in panel 2 in figure),
    we can use edges to represent time-respecting paths of length two, i.e. edge *(a→b,
    b→c)* indicates that *a* causally influence *c* via *b*. However, the reverse
    order of edges are not included. If the edges occur in reverse order, we do not
    include *(a→b, b→c)*. Importantly, such a second-order graph is sensitive to the
    temporal ordering of edges, while a first-order graph is not! In [Scholtes, 2017](https://dl.acm.org/doi/10.1145/3097983.3098145),
    this is generalized to higher orders, which yields **k-th order De Bruijn graph
    models for the causal topology of** **temporal graphs**.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 时间图中的每条边都是长度为一的时间遵循路径。因此，经过加权时间聚合的图对应于时间图因果拓扑的一个一阶模型，能够捕捉时间遵循的长度为一的路径。它忽略了边的时间顺序，因为我们仅计算每条边出现的频率。通过图形变换，我们可以将这一思想推广到因果感知模型，以促进时间图的学习：我们只需将一阶图中的边替换为二阶图中的节点，即将边*(a,b)*和*(b,c)*转换为节点*“a→b”*和*“b→c”*。在生成的二阶图中（参见图2面板顶部图示），我们可以使用边表示长度为二的时间遵循路径，即边*(a→b,
    b→c)*表示*a*通过*b*因果地影响*c*。然而，边的反向顺序不被包含。如果边按相反顺序出现，我们就不会包含*(a→b, b→c)*。重要的是，这样的二阶图对边的时间顺序敏感，而一阶图则不敏感！在[Scholtes,
    2017](https://dl.acm.org/doi/10.1145/3097983.3098145)中，这一思想被推广到更高阶，从而产生了**k阶德·布鲁因图模型，用于时间图的因果拓扑**。
- en: '[Qarkaxhija et al.](https://proceedings.mlr.press/v198/qarkaxhija22a/qarkaxhija22a.pdf)
    have shown that neural message passing in such higher-order De Bruijn graphs yields
    a **causality-aware graph neural network architecture** for temporal graphs. Building
    on these **De Bruijn Graph Neural Networks (DBGNN)**, in a poster at this year’s
    TGL workshop, [Heeg and Scholtes](https://arxiv.org/abs/2310.15865) address the
    challenge to predict temporal betweenness and closeness centralities of nodes.
    Since they are influenced by the arrow of time, temporal node centralities can
    drastically differ from static centralities. Moreover, it is costly to compute
    them! This is addressed by training a DBGNN model on a first time interval of
    a temporal graph, then using the trained model to forecast temporal centralities
    in the remaining data. The overall approach is illustrated above. Empirically
    results are promising and showcased the potential of causality-aware graph learning.
    We also hope to see more attention from the community in learning causal structure
    on temporal graphs in 2024.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[Qarkaxhija 等人](https://proceedings.mlr.press/v198/qarkaxhija22a/qarkaxhija22a.pdf)已证明，在此类高阶德·布鲁因图中进行神经消息传递能够产生一种**因果感知图神经网络架构**，用于时间图的建模。基于这些**德·布鲁因图神经网络（DBGNN）**，在今年的
    TGL 研讨会上，[Heeg 和 Scholtes](https://arxiv.org/abs/2310.15865)提出了解决预测节点时间介入度和紧密度中心性的挑战。由于它们受到时间箭头的影响，时间节点的中心性可能与静态中心性大相径庭。而且，计算这些中心性是非常昂贵的！这个问题通过在时间图的第一个时间间隔上训练一个
    DBGNN 模型来解决，然后使用训练好的模型预测其余数据中的时间中心性。整体方法如上图所示。实证结果令人鼓舞，展示了因果感知图学习的潜力。我们也希望在 2024
    年，社区能更多关注在时间图上学习因果结构的问题。'
- en: Interested in causality-aware temporal graph learning? Then there’s good news!
    The techniques above are implemented in the Open Source library [pathpyG](https://www.pathpy.net/),
    which builds on [PyG](https://pyg.org/). There is an [introductory video and a
    tutorial](https://www.pathpy.net/dev/tutorial/) available. A [recorded talk given
    in the temporal graph reading group](https://youtu.be/IezbzMMp9QM?si=1_B8rMag5Gpk0Vqx)
    provides an in-depth introduction of the underlying research.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对因果感知时序图学习感兴趣吗？那么有好消息！上述技术已在开源库 [pathpyG](https://www.pathpy.net/) 中实现，该库建立在
    [PyG](https://pyg.org/) 之上。这里有一个 [入门视频和教程](https://www.pathpy.net/dev/tutorial/)
    可供参考。此外，还有一场 [在时序图阅读小组中进行的讲座录音](https://youtu.be/IezbzMMp9QM?si=1_B8rMag5Gpk0Vqx)，为您提供了对基础研究的深入介绍。
- en: Explainable Temporal Graph Methods
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释的时序图方法
- en: “Most important graph-structured data in real-world settings are temporal in
    nature. Explainable temporal graph models have the potential to unveil the long-standing
    questions on effective strategies and knowledge that can be leveraged to make
    temporal predictions, enabling extraction of insights from deep learning models
    and assisting scientific discovery and forecasting.” — Rex Ying, Assistant Professor,
    Yale University
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “现实世界中最重要的图结构数据本质上是时序性的。可解释的时序图模型有潜力揭示长期存在的问题，提供有效的策略和知识，以便用于时序预测，进而从深度学习模型中提取洞察，并辅助科学发现与预测。”
    — Rex Ying，耶鲁大学助理教授
- en: 2023 saw the first approaches for explaining temporal GNN methods. Explainers
    are important for high-stake applications such as fraud detection and disease
    progression prediction in healthcare. [Xia et al.](https://openreview.net/forum?id=BR_ZhvcYbGJ)
    proposed T-GNNExplainer as the first explainer designed for temporal graph models.
    T-GNNExplainer is model-agnostic and finds important events from a set of candidate
    events to best explain the model prediction. Xia et al. treat the problem of identifying
    a subset of explaining events as a combinatorial optimization problem by searching
    over a subset of the temporal graph within a given size. To tackle this, T-GNNExplainer
    employs an explorer-navigator framework. The navigator is trained from multiple
    target events to capture inductive correlations between events while the explorer
    searches out a specific combination of events based on Monte Carlo Tree Search,
    including node selection, node expansion, reward simulation and backprop. Which
    events are pruned is inferred from the navigator. The diagram below shows the
    framework of T-GNNExplainer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年首次提出了时序GNN方法的可解释性。解释器在高风险应用中非常重要，比如欺诈检测和医疗保健中的疾病进展预测。[Xia 等人](https://openreview.net/forum?id=BR_ZhvcYbGJ)提出了
    T-GNNExplainer，作为首个为时序图模型设计的解释器。T-GNNExplainer 是模型无关的，它从一组候选事件中找到最能解释模型预测的重要事件。Xia
    等人将识别解释事件子集的问题视为一个组合优化问题，通过在给定大小的时序图子集内进行搜索来解决。为了解决这个问题，T-GNNExplainer采用了探索者-导航者框架。导航者通过多个目标事件的训练，捕捉事件间的归纳相关性，而探索者则基于蒙特卡洛树搜索寻找特定的事件组合，包括节点选择、节点扩展、奖励模拟和反向传播。哪些事件被剪枝由导航者推断得出。下图展示了
    T-GNNExplainer 的框架。
- en: '![](../Images/2e8097a22cf7af9ee4686e1af17cf30e.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e8097a22cf7af9ee4686e1af17cf30e.png)'
- en: Framework of T-GNNExplainer.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: T-GNNExplainer 框架
- en: 'Image source: [Xia et al. 2023](https://openreview.net/forum?id=BR_ZhvcYbGJ)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[Xia 等人，2023](https://openreview.net/forum?id=BR_ZhvcYbGJ)
- en: Recently, [Chen et al.](https://arxiv.org/abs/2310.19324) argued that to form
    human intelligible explanations for temporal graph events requires the explanation
    to be events that are temporally proximate and spatially adjacent to that of the
    prediction, referred to as *cohesive explanations*. Utilizing *temporal motifs,*
    recurring substructures within the temporal graph, is a natural solution to form
    cohesive explanations for temporal graphs. This is because temporal motifs are
    crucial factors that guide the generative process of future events. In the following
    example, the preferential attachment rule (often facilitating influencer effect
    in e-commerce) and triadic closure rule (explains common-friend rules in social
    networks) forms cohesive and plausible explanations.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，[Chen 等人](https://arxiv.org/abs/2310.19324)认为，要为时序图事件形成人类可理解的解释，解释需要是与预测事件在时间上接近且在空间上相邻的事件，称为*紧凑解释*。利用*时序模式*，即时序图中反复出现的子结构，是为时序图形成紧凑解释的自然解决方案。这是因为时序模式是引导未来事件生成过程的重要因素。在以下示例中，优先附着规则（通常在电子商务中促进影响者效应）和三分闭合规则（解释社交网络中的共同朋友规则）形成了紧凑且可信的解释。
- en: '![](../Images/15c2737fed1e3529dffee8d9bffab28f.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15c2737fed1e3529dffee8d9bffab28f.png)'
- en: '*Cohesive explanations* are temporally approximate and spatially adjacent.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*连贯的解释*在时间上是近似的，并且在空间上是相邻的。'
- en: 'Image source: [Chen et al. 2023](https://arxiv.org/abs/2310.19324)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Chen et al. 2023](https://arxiv.org/abs/2310.19324)
- en: Therefore, [Chen et al.](https://arxiv.org/abs/2310.19324) proposed TempME,
    a novel Temporal Motif-based Explainer to identify important temporal motifs to
    explain temporal GNNs. The framework of TempME is shown in the below figure. First,
    temporal motifs surrounding the target prediction are extracted. Then these candidate
    motifs are encoded via the *Motif Encoder* which leverages event anonymization,
    message passing and graph pooling to generate an embedding for each motif. Then,
    based on the Information-bottleneck principle, TempME assigns importance scores
    to these motifs constrained by both explanation accuracy and information compression.
    Lastly, explanations are constructed by sampling from the Bernoulli distribution
    based on the importance score.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，[Chen et al.](https://arxiv.org/abs/2310.19324)提出了TempME，一种基于时间模式的解释器，用于识别重要的时间模式以解释时间GNN。TempME的框架如下图所示。首先，提取围绕目标预测的时间模式。然后，这些候选模式通过*模式编码器*进行编码，该编码器利用事件匿名化、信息传递和图池化生成每个模式的嵌入。接着，基于信息瓶颈原则，TempME为这些模式分配重要性评分，评分由解释准确性和信息压缩共同约束。最后，通过从伯努利分布中采样，基于重要性评分构建解释。
- en: '![](../Images/9baa6fe57cec7a96c033167b51ca3dec.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9baa6fe57cec7a96c033167b51ca3dec.png)'
- en: Framework of TempME, numbers on the edges denote the temporal order.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: TempME的框架，边缘上的数字表示时间顺序。
- en: 'Image credit: [Chen et al. 2023](https://arxiv.org/abs/2310.19324)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图片致谢：[Chen et al. 2023](https://arxiv.org/abs/2310.19324)
- en: Adversarial Attacks on Temporal Graphs
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间图上的对抗性攻击
- en: “As temporal graphs are adopted to be used for important tasks, like fraud detection,
    it is important to understand their failure points under adversarial attacks.
    Understanding and quantifying such blind spots is the first step towards creating
    robust and reliable temporal GNN models. Such efforts are necessary to ensure
    industry adoption of these models.” - Srijan Kumar, Assistant Professor at Georgia
    Institute of Technology
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “随着时间图被用于重要任务，如欺诈检测，理解其在对抗性攻击下的失败点变得尤为重要。理解和量化这些盲点是创建强健可靠的时间GNN模型的第一步。这类努力对于确保这些模型在行业中的应用至关重要。”
    - 佐治亚理工学院助理教授 Srijan Kumar
- en: Adversarial attacks can target the privacy of customers or affect critical decisions
    in financial systems. As temporal graph models are deployed to applications such
    as recommendation systems and fraud detection, it is important to investigate
    attacks and design defense mechanisms for TG models. [Chen et al.](https://arxiv.org/abs/1911.10561)
    proposed the first adversarial attack for dynamic link prediction called Time-aware
    Gradient Attack (TGA) for discrete time dynamic graphs. TGA rewires a limited
    number of links from the original network and the most valuable links to the predicted
    link are determined by the gradient information generated by the TG model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性攻击可以针对客户的隐私或影响金融系统中的关键决策。随着时间图模型被应用于推荐系统和欺诈检测等应用领域，研究攻击并设计针对TG模型的防御机制变得尤为重要。[Chen
    et al.](https://arxiv.org/abs/1911.10561)提出了针对离散时间动态图的第一个对抗性攻击，称为时间感知梯度攻击（TGA）。TGA通过改变原始网络中的少量链接，且通过梯度信息决定与预测链接最相关的链接。
- en: '![](../Images/ae39ba65201cfb39a0605af3c6e55601.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae39ba65201cfb39a0605af3c6e55601.png)'
- en: Overview of the Temporal Dynamics-aware Perturbation attack.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 时间动态感知扰动攻击概述。
- en: The attacker can flip the prediction of the model while evading detection.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者可以在避免检测的情况下翻转模型的预测结果。
- en: 'Image source: [Sharma et al. 2023](https://dl.acm.org/doi/pdf/10.1145/3580305.3599517)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Sharma et al. 2023](https://dl.acm.org/doi/pdf/10.1145/3580305.3599517)
- en: Recently, [Sharma et al.](https://dl.acm.org/doi/abs/10.1145/3580305.3599517)
    argued that effective attacks on temporal graphs must optimize both edge and time
    perturbations while preserving the original graph evolution. This is because drastic
    attacks that disturb the graph evolution would be easily detected by anomaly detection
    methods. Therefore, Sharma et al. formulated *evolution-preserving attacks* on
    discrete-time dynamic graphs as the **T**emporal **D**ynamics-**A**ware **P**erturbation
    (TDAP) constraint. TDAP asserts that perturbations added at a given timestamp
    must only be a small fraction of the actual number of changes with respect to
    the preceding timestamp. TDAP is shown to preserve the rate of change in both
    the structure and the embedding spaces. An overview of TDAP is shown in the figure
    below. Sharma et al. then proposes a novel attack method called **T**emporal **D**ynamics-aware
    **P**rojected **G**radient **D**escent (TD-PGD) which is shown to have a closed-form
    projection operator under the TDAP constraint. An online version of TD-PGD is
    also proposed where perturbations can be added in real time. Lastly, it is shown
    empirically that TDAP-constrained perturbations can indeed evade attacks by embedding-based
    anomaly detection methods.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，[Sharma et al.](https://dl.acm.org/doi/abs/10.1145/3580305.3599517)提出有效的时间图攻击必须同时优化边缘和时间扰动，同时保持原始图演化的完整性。这是因为，剧烈的攻击会扰乱图的演化，容易被异常检测方法发现。因此，Sharma
    et al. 将*演化保持攻击*定义为**T**emporal **D**ynamics-**A**ware **P**erturbation（TDAP）约束。TDAP主张，在给定时间戳下添加的扰动，只能是相对于前一个时间戳的实际变化数量的一小部分。TDAP被证明可以保持图的结构和嵌入空间的变化率。下图展示了TDAP的概览。Sharma
    et al. 随后提出了一种新型攻击方法——**T**emporal **D**ynamics-aware **P**rojected **G**radient
    **D**escent（TD-PGD），该方法在TDAP约束下具有封闭形式的投影算子。还提出了TD-PGD的在线版本，在该版本中，扰动可以实时添加。最后，实验证明，TDAP约束的扰动确实可以避开基于嵌入的异常检测方法的攻击。
- en: Libraries and Benchmarks
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 库和基准测试
- en: 'In 2023 saw a significant push towards standardized libraries and benchmarks
    for temporal graph learning. [TGB](https://tgb.complexdatalab.com/) provides an
    open and standardized benchmark for node and link level tasks. [DyGLib](https://github.com/yule-BUAA/DyGLib)
    is a library which includes standard training pipelines, extensible coding interfaces,
    and comprehensive evaluation strategies for temporal graph learning. [Zhang et
    al.](https://openreview.net/forum?id=zr1e15kczE) introduced the novel concept
    of [*Live Graph Lab*](https://livegraphlab.github.io/), providing live graphs
    according to blockchain transactions. With a set of tools for downloading, parsing,
    cleaning, and analyzing blockchain transactions, Live Graph Lab offers researchers
    the opportunity to extract update to date temporal graph data any time and use
    it for analysis or testing. [Zhou et al.](https://dl.acm.org/doi/10.1145/3581784.3607056)
    noticed that node memory used in TG models favors small batch sizes and needs
    to be maintained synchronously across all trainers. Therefore, they proposed [DistTGL](https://github.com/amazon-science/disttgl),
    an efficient and scalable solution to train memory-based TGNNs on distributed
    GPU clusters. Enabling multi-GPU training on large datasets is an important direction
    to deploy TG models on large datasets. We present an updated list of libraries
    and benchmarks for temporal graph learning:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年，时间图学习领域推动了标准化库和基准测试的重大进展。[TGB](https://tgb.complexdatalab.com/)为节点和链路级任务提供了一个开放且标准化的基准。[DyGLib](https://github.com/yule-BUAA/DyGLib)是一个库，包含标准化的训练流程、可扩展的编码接口和全面的时间图学习评估策略。[Zhang
    et al.](https://openreview.net/forum?id=zr1e15kczE)提出了一个新颖的概念——[*Live Graph Lab*](https://livegraphlab.github.io/)，根据区块链交易提供实时图数据。Live
    Graph Lab提供了一套工具，用于下载、解析、清理和分析区块链交易，向研究人员提供随时提取最新时间图数据的机会，以供分析或测试。[Zhou et al.](https://dl.acm.org/doi/10.1145/3581784.3607056)注意到，TG模型中使用的节点内存更适合小批量训练，并且需要在所有训练器中同步维护。因此，他们提出了[DistTGL](https://github.com/amazon-science/disttgl)，一种高效且可扩展的解决方案，用于在分布式GPU集群上训练基于内存的TGNN。启用多GPU训练大规模数据集是部署TG模型在大数据集上的一个重要方向。我们呈现了一个关于时间图学习的库和基准测试的更新列表：
- en: TGB [website](https://tgb.complexdatalab.com/) and [pypi install](https://pypi.org/project/py-tgb/)
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TGB [网站](https://tgb.complexdatalab.com/) 和 [pypi 安装](https://pypi.org/project/py-tgb/)
- en: DyGLib [Github](https://github.com/yule-BUAA/DyGLib)
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DyGLib [Github](https://github.com/yule-BUAA/DyGLib)
- en: Live Graph Lab [website](https://livegraphlab.github.io/) and [dataset](https://zenodo.org/records/8267012)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Live Graph Lab [网站](https://livegraphlab.github.io/) 和 [数据集](https://zenodo.org/records/8267012)
- en: DistTGL [Github](https://github.com/amazon-science/disttgl)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DistTGL [Github](https://github.com/amazon-science/disttgl)
- en: Torch Spatiotemporal (TSL) [website](https://torch-spatiotemporal.readthedocs.io/en/latest/)
    and [Github](https://github.com/TorchSpatiotemporal/tsl)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torch Spatiotemporal (TSL) [网站](https://torch-spatiotemporal.readthedocs.io/en/latest/)
    和 [Github](https://github.com/TorchSpatiotemporal/tsl)
- en: pathpyG [website](https://www.pathpy.net/dev/) and [Github](https://github.com/pathpy/pathpyG)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pathpyG [网站](https://www.pathpy.net/dev/) 和 [Github](https://github.com/pathpy/pathpyG)
- en: RelBench [website](https://relbench.stanford.edu/) and [Github](https://github.com/snap-stanford/relbench)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RelBench [网站](https://relbench.stanford.edu/) 和 [Github](https://github.com/snap-stanford/relbench)
- en: '[Pytorch Geometric Temporal](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pytorch Geometric Temporal](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html)'
- en: TGL [paper](https://assets.amazon.science/88/aa/0323050941cab9403763ffdde180/tgl-a-general-framework-for-temporal-gnn-training-on-billion-scale-graphs-scalable-data-science.pdf)
    and [Github](https://github.com/amazon-science/tgl)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TGL [论文](https://assets.amazon.science/88/aa/0323050941cab9403763ffdde180/tgl-a-general-framework-for-temporal-gnn-training-on-billion-scale-graphs-scalable-data-science.pdf)
    和 [Github](https://github.com/amazon-science/tgl)
- en: DGB [pypi install](https://pypi.org/project/dgb/), [paper](https://openreview.net/forum?id=1GVpwr2Tfdg)
    and [datasets](https://zenodo.org/records/7213796#.Y8QicOzMJB2)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DGB [pypi 安装](https://pypi.org/project/dgb/), [论文](https://openreview.net/forum?id=1GVpwr2Tfdg)
    和 [数据集](https://zenodo.org/records/7213796#.Y8QicOzMJB2)
- en: Chartalist Blockchain Network [website](https://www.chartalist.org/), [paper](https://openreview.net/forum?id=10iA3OowAV3)
    and [Github](https://github.com/cakcora/chartalist)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chartalist Blockchain Network [网站](https://www.chartalist.org/), [论文](https://openreview.net/forum?id=10iA3OowAV3)
    和 [Github](https://github.com/cakcora/chartalist)
- en: TKG Forecasting Evaluation [paper](https://github.com/JuliaGast/JuliaGast.github.io/blob/master/files/gastinger_evaluation_paper_TKG.pdf)
    and [Github](https://github.com/nec-research/TKG-Forecasting-Evaluation)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TKG预测评估 [论文](https://github.com/JuliaGast/JuliaGast.github.io/blob/master/files/gastinger_evaluation_paper_TKG.pdf)
    和 [Github](https://github.com/nec-research/TKG-Forecasting-Evaluation)
- en: Joining Temporal Graph Learning Community
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入时序图学习社区
- en: 'To join the fast growing TG community, sign up for the weekly temporal graph
    reading group [here](https://forms.gle/wausEzsgm7DvyLPs7). Visit the [reading
    group website](https://www.cs.mcgill.ca/~shuang43/rg.html) and [Youtube](https://www.youtube.com/@TGL_RG)
    to see all upcoming and recorded past talks. We also include the invitation link
    to the TG slack on the website (updated monthly). The second edition of [temporal
    graph learning workshop @ NeurIPS 2023](https://sites.google.com/view/tglworkshop-2023/home)
    includes an exciting lineup of [35 posters](https://openreview.net/group?id=NeurIPS.cc%2F2023%2FWorkshop%2FTGL#tab-accept-long-paper)
    for cutting edge research in TG. You can also find the talk recordings on the
    [NeurIPS virtual site](https://neurips.cc/virtual/2023/workshop/66544) (will be
    public in a month). If you want to be a reviewer for the next edition of the workshop,
    sign up [here](https://forms.gle/4UuiTUDEqkvQ4pHC8). To find more about the research
    from the authors of this post, see our websites: [Shenyang(Andy) Huang](https://cs.mcgill.ca/~shuang43/),
    [Emanuele Rossi](https://www.emanuelerossi.co.uk/), [Michael Galkin](https://migalkin.github.io/)
    ,[Andrea Cini](https://andreacini.github.io/) and [Ingo Scholtes](https://www.ingoscholtes.net/).
    Hope to see you at the reading group or the next edition of the workshop!'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入快速发展的 TG 社区，请点击 [此处](https://forms.gle/wausEzsgm7DvyLPs7) 注册每周的时序图阅读小组。访问
    [阅读小组网站](https://www.cs.mcgill.ca/~shuang43/rg.html) 和 [Youtube](https://www.youtube.com/@TGL_RG)
    查看所有即将举办和过去的录播讲座。我们还在网站上提供了 TG slack 的邀请链接（每月更新）。[2023年NeurIPS时序图学习研讨会](https://sites.google.com/view/tglworkshop-2023/home)第二版包括了激动人心的
    [35篇海报](https://openreview.net/group?id=NeurIPS.cc%2F2023%2FWorkshop%2FTGL#tab-accept-long-paper)，展示了时序图领域的前沿研究。你还可以在
    [NeurIPS虚拟网站](https://neurips.cc/virtual/2023/workshop/66544) 上找到讲座录音（一个月后公开）。如果你想成为下一版研讨会的审稿人，可以
    [在此](https://forms.gle/4UuiTUDEqkvQ4pHC8) 注册。要了解更多关于本文作者的研究，请访问我们的个人网站：[沈阳（Andy）黄](https://cs.mcgill.ca/~shuang43/)，[Emanuele
    Rossi](https://www.emanuelerossi.co.uk/)，[Michael Galkin](https://migalkin.github.io/)，[Andrea
    Cini](https://andreacini.github.io/) 和 [Ingo Scholtes](https://www.ingoscholtes.net/)。期待在阅读小组或下次研讨会中见到你！
- en: '![](../Images/63a42f043a6b9654d313b283180a13ff.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63a42f043a6b9654d313b283180a13ff.png)'
- en: Logo of the NeurIPS 2023 Temporal Graph Learning Workshop.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: NeurIPS 2023 时序图学习研讨会的 logo。
- en: Image by authors.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者。
