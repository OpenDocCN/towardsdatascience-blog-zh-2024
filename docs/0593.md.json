["```py\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\ntokenizer(\"the dog likes food\")\n>>> {'input_ids': [5984, 35433, 114022, 17304], 'attention_mask': [1, 1, 1, 1]}\n```", "```py\n{\n    \"input_ids\": [[\"the\", \"dog\", \"likes\", \"food\"]],\n    \"attention_mask\": [[1, 1, 1, 1]],\n    \"labels\": [[\"the\", \"dog\", \"likes\", \"food\"]],\n}\n```", "```py\npred_token_k = model(input_ids[:k]*attention_mask[:k]^T)\n```", "```py\npred_token_k = model(input_ids[:k])\n```", "```py\n[P(dog)=10%, P(food)=60%,P(likes)=0%, P(the)=30%]\n```", "```py\n[P(dog)=0%, P(food)=100%, P(likes)=0%, P(the)=0%]\n```"]