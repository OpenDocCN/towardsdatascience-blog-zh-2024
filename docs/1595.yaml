- en: 'Classification Loss Functions: Intuition and Applications'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类损失函数：直觉与应用
- en: 原文：[https://towardsdatascience.com/classification-loss-functions-intuition-and-applications-cb75a3b03237?source=collection_archive---------3-----------------------#2024-06-27](https://towardsdatascience.com/classification-loss-functions-intuition-and-applications-cb75a3b03237?source=collection_archive---------3-----------------------#2024-06-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/classification-loss-functions-intuition-and-applications-cb75a3b03237?source=collection_archive---------3-----------------------#2024-06-27](https://towardsdatascience.com/classification-loss-functions-intuition-and-applications-cb75a3b03237?source=collection_archive---------3-----------------------#2024-06-27)
- en: A simpler way to understand derivations of loss functions for classification
    and when/how to apply them in PyTorch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种更简洁的方式来理解分类损失函数的推导以及在PyTorch中何时/如何应用它们
- en: '[](https://medium.com/@rtdcunha?source=post_page---byline--cb75a3b03237--------------------------------)[![Ryan
    D''Cunha](../Images/7a39859e2b5e5b09ef2c60aaf6bb75ac.png)](https://medium.com/@rtdcunha?source=post_page---byline--cb75a3b03237--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cb75a3b03237--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cb75a3b03237--------------------------------)
    [Ryan D''Cunha](https://medium.com/@rtdcunha?source=post_page---byline--cb75a3b03237--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rtdcunha?source=post_page---byline--cb75a3b03237--------------------------------)[![Ryan
    D''Cunha](../Images/7a39859e2b5e5b09ef2c60aaf6bb75ac.png)](https://medium.com/@rtdcunha?source=post_page---byline--cb75a3b03237--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cb75a3b03237--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cb75a3b03237--------------------------------)
    [Ryan D''Cunha](https://medium.com/@rtdcunha?source=post_page---byline--cb75a3b03237--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cb75a3b03237--------------------------------)
    ·9 min read·Jun 27, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cb75a3b03237--------------------------------)
    ·阅读时间：9分钟·2024年6月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8799dde9d8c2ba105f0d95e98464eb61.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8799dde9d8c2ba105f0d95e98464eb61.png)'
- en: 'Source: GPT4o Generated'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：GPT4o生成
- en: Whether you are new to exploring neural networks or a seasoned pro, this should
    be a beneficial read to gain more intuition about loss functions. As someone testing
    many different loss functions during model training, I would get tripped up on
    small details between functions. I spent hours researching an intuitive depiction
    of loss functions from textbooks, research papers, and videos. I wanted to share
    not only the **derivations** that helped me grasp the concepts, but common pitfalls
    and use cases for **classification** in PyTorch.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是刚开始探索神经网络的新手，还是经验丰富的专家，这篇文章都将是一次有益的阅读，可以帮助你更好地理解损失函数。作为一个在模型训练过程中测试多种不同损失函数的人，我常常因为函数之间的小细节而卡住。我花了很多时间查阅教科书、研究论文和视频，以寻找一种直观的损失函数描述。我想与大家分享不仅是帮助我理解这些概念的**推导**过程，还有在PyTorch中使用**分类**损失函数时常见的陷阱和应用场景。
- en: Terminology
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语
- en: Before we get started, we need to define some basic terms I will be using.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我需要定义一些我将要使用的基本术语。
- en: 'Training dataset: *{xᵢ, yᵢ}*'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据集： *{xᵢ, yᵢ}*
- en: 'Loss function: *L[φ]*'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数： *L[φ]*
- en: Model prediction output *f[xᵢ, φ]* with parameters *φ*
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型预测输出 *f[xᵢ, φ]*，其中 *φ* 为参数
- en: 'Conditional probability: *Pr(y|x)*'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件概率： *Pr(y|x)*
- en: 'Parametric distribution: *Pr(y|ω)* with *ω* representing network parameters
    for distribution over *y*'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数化分布： *Pr(y|ω)*，其中 *ω* 表示网络参数，用于描述 *y* 上的分布
- en: Introduction
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Let’s first go back to the basics. A common thought is that neural networks
    compute a scalar output from the model *f[xᵢ, φ].* However, most neural networks
    these days are trained to predict parameters of a distribution *y.* (as oppose
    to to predicted the value of *y*).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先回归基础。一个常见的想法是神经网络从模型 *f[xᵢ, φ]* 计算一个标量输出。然而，现在大多数神经网络被训练用来预测一个分布的参数 *y*。（而不是预测
    *y* 的值）。
- en: In reality, a network will output a conditional probability distribution *Pr(y|x)*
    over possible outputs *y.* In other words, every input data point will lead to
    a probability distribution generated for each output. The network wants to learn
    the parameters for the probability distribution and then use the parameters and
    distribution to predict the output.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，网络将输出一个条件概率分布 *Pr(y|x)*，这个分布表示给定输入 *x* 时各个可能输出 *y* 的概率。换句话说，每个输入数据点都会生成一个针对每个输出的概率分布。网络的目标是学习这些概率分布的参数，并使用这些参数和分布来预测输出。
- en: The traditional definition of a loss function is a function that compares target
    and predicted outputs. But we just said a network raw output is a distribution
    instead of a scalar output, so how is this possible?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的损失函数定义是一个比较目标输出和预测输出的函数。但我们刚才说过，网络的原始输出是一个分布，而不是一个标量输出，那么这是如何实现的呢？
- en: Thinking about this from the view we just defined, a loss function pushes each
    *yᵢ* to have a higher probability in the distribution *Pr(yᵢ|xᵢ)*. The key part
    to remember is that our distribution is being used to predict the true output
    based on parameters from our model output. Instead of using our input *xᵢ* for
    the distribution*,* we can think of a parametric distribution *Pr(y|ω)* where
    *ω* represents probability distribution parameters. We are still considering the
    input, but there will be a different *ωᵢ = f[xᵢ, φ]* for each *xᵢ.*
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们刚才定义的视角来看，损失函数推动每个*yᵢ*在分布*Pr(yᵢ|xᵢ)*中具有更高的概率。需要记住的关键部分是，我们的分布用于根据模型输出的参数预测真实输出。我们不再使用输入*xᵢ*来表示分布*，*而是可以考虑一个参数化的分布*Pr(y|ω)*，其中*ω*代表概率分布的参数。我们仍然考虑输入，但每个*xᵢ*会有不同的*ωᵢ
    = f[xᵢ, φ]*。
- en: '**Note:** To clarify a confusing concept, φrepresents the model parameters
    and ω represents the probability distribution parameters'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 为了澄清一个容易混淆的概念，*φ*代表模型参数，而*ω*代表概率分布的参数'
- en: Deriving Negative Log-Likelihood Loss
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推导负对数似然损失
- en: '*G*oing back to the traditional definition of a loss function, we need to get
    an output we can use from the model. From our probability distribution, it seems
    logical to take *φ* that produces the greatest probability for each *xᵢ.* Thus,
    we need the overall *φ* that produces the greatest probability across all training
    points *I* (all derivations are adapted from Understanding Deep Learning [1]):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*G*回到传统的损失函数定义，我们需要从模型中获得一个可用的输出。从我们的概率分布来看，选择*φ*来产生每个*xᵢ*的最大概率似乎是合乎逻辑的。因此，我们需要得到在所有训练点*I*上产生最大概率的总体*φ*（所有推导都来自《理解深度学习》[1]）：'
- en: '![](../Images/522fceb7fdc7af4df7531a2c5537250e.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/522fceb7fdc7af4df7531a2c5537250e.png)'
- en: Maximizing parameters from output model probability distributions [1]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化输出模型概率分布的参数 [1]
- en: 'We multiply the generated probabilities from each distribution to find *φ*
    that produces the maximum probability (called **max likelihood**). In order to
    do this, we must assume the data is independent and identically distributed. But
    now we run into a problem: what if the probabilities are very small? Our multiplication
    output will approach 0 (similar to a vanishing gradient issue). Furthermore, our
    program may not be able to process such small numbers.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每个分布生成的概率相乘，以找到产生最大概率的*φ*（称为**最大似然**）。为了做到这一点，我们必须假设数据是独立且同分布的。但现在我们遇到了一个问题：如果概率非常小怎么办？我们的乘积结果将接近于0（类似于梯度消失问题）。此外，我们的程序可能无法处理如此小的数字。
- en: To fix this, we bring in a logarithmicfunction! Utilizing the properties of
    logs, we can add together our probabilities instead of multiplying them. We know
    that the logarithm is a monotonically increasing function, so our original output
    is preserved and scaled by the log.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们引入了对数函数！利用对数的特性，我们可以将概率相加，而不是相乘。我们知道，对数是一个单调递增的函数，因此我们的原始输出被保留并通过对数进行了缩放。
- en: '![](../Images/e9d290f8114504d063ca077dce4382fa.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9d290f8114504d063ca077dce4382fa.png)'
- en: Using logarithms to add probabilities [1]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对数加法概率 [1]
- en: 'The last thing we need to get our traditional negative log-likelihood is to
    minimize the output. We are currently maximizing the output, so simply multiply
    by a negative and take the minimum argument (think about some graphical examples
    to convince yourself of this):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的最后一件事是获取我们传统的负对数似然，即最小化输出。我们现在是在最大化输出，所以只需要乘以负数并取最小值（考虑一些图形示例来帮助自己理解这一点）：
- en: '![](../Images/fc1c7cee0ae2ea2d49a9ef2760b133ec.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc1c7cee0ae2ea2d49a9ef2760b133ec.png)'
- en: Negative Log-Likelihood [1]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 负对数似然 [1]
- en: Just by visualizing the model output as a probability distribution, attempting
    to maximize *φ* that creates the max probability, and applying a log, we have
    derived negative log-likelihood loss! This can be applied to many tasks by choosing
    a logical probability distribution. Common classification examples are shown below.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅通过将模型输出可视化为一个概率分布，尝试最大化生成最大概率的*φ*，并应用对数，我们就推导出了负对数似然损失！通过选择一个合适的概率分布，这可以应用于许多任务。下面展示了一些常见的分类示例。
- en: 'If you are wondering how a scalar output is generated from the model during
    **inference**, it’s just the max of the distribution:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道在**推断**过程中，模型是如何生成标量输出的，那就是分布的最大值：
- en: '![](../Images/77d8057be79a86e22f7758dac179cbec.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77d8057be79a86e22f7758dac179cbec.png)'
- en: Generating an output from inference [1]
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从推断生成输出[1]
- en: '**Note:** This is just a derivation of negative log-likelihood. In practice,
    there will most likely be regularization present in the loss function too.'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 这只是负对数似然的推导。在实践中，损失函数中很可能还会有正则化项。'
- en: Loss for Classification
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类的损失
- en: Up to this point, we derived negative log-likelihood. Important to know, but
    it can be found in most textbooks or online resources. Now, let’s apply this to
    classification to understand it’s application.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们推导了负对数似然。虽然这很重要，但它可以在大多数教科书或在线资源中找到。现在，让我们将其应用于分类问题，以理解它的应用。
- en: '**Side note:** If you are interested in seeing this applied to regression,
    Understanding Deep Learning [1] has great examples with univariate regression
    and a Gaussian Distribution to derive Mean Squared Error'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**附注：** 如果你对将其应用于回归感兴趣，《理解深度学习》[1]中有关于单变量回归和高斯分布推导均方误差的很好的例子。'
- en: Binary Classification
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二分类
- en: The goal of binary classification is to assign an input *x* to one of two class
    labels *y* ∈ {0, 1}. We are going to use the Bernoulli distribution as our probability
    distribution of choice.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类的目标是将输入*x*分配到两个类别标签之一，*y* ∈ {0, 1}。我们将使用伯努利分布作为我们选择的概率分布。
- en: '![](../Images/64d8d4c8478ea774fceb1a8d84d5c783.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64d8d4c8478ea774fceb1a8d84d5c783.png)'
- en: Mathematical Representation of Bernoulli Distribution. Image by Author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 伯努利分布的数学表示。图片来源：作者
- en: 'This is just a fancy way of saying the probability that the output is true,
    but the equation is necessary to derive our loss function. We need the model *f[x,
    φ]* to output *p* to generate the predicted output probability*.* However, before
    we can input *p* into Bernoulli, we need it to be between 0 and 1 (so it’s a probability).
    The function of choice for this is a sigmoid: σ(*z*)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个花哨的说法，表示输出为真的概率，但这个方程是推导损失函数所必需的。我们需要模型*f[x, φ]*输出*p*来生成预测的输出概率。然而，在我们将*p*输入到伯努利分布之前，我们需要确保它在0和1之间（因为它是一个概率）。用于此的函数是sigmoid：σ(*z*)
- en: '![](../Images/9716980d8305f8ba0fce144f11ee2f0d.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9716980d8305f8ba0fce144f11ee2f0d.png)'
- en: 'Source: [https://en.wikipedia.org/wiki/Sigmoid_function](https://en.wikipedia.org/wiki/Sigmoid_function)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://en.wikipedia.org/wiki/Sigmoid_function](https://en.wikipedia.org/wiki/Sigmoid_function)
- en: 'A sigmoid will compress the output *p* to between 0 and 1\. Therefore our input
    to Bernoulli will be *p =* σ(*f[x, φ]).* This makes our probability distribution:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid会将输出*p*压缩到0和1之间。因此，我们输入伯努利分布的值是*p =* σ(*f[x, φ])*。这使我们的概率分布变为：
- en: '![](../Images/32e05e638cc53e6838db0a7dcaeeb77c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32e05e638cc53e6838db0a7dcaeeb77c.png)'
- en: New Probability Distribution with Sigmoid and Bernoulli. Image by Author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 带有Sigmoid和伯努利分布的新概率分布。图片来源：作者
- en: 'Going back to negative log-likehood, we get the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 回到负对数似然，我们得到如下结果：
- en: '![](../Images/f2e36bf555c46b78ee8cac481211a9c6.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2e36bf555c46b78ee8cac481211a9c6.png)'
- en: Binary Cross Entropy. Image by Author
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 二元交叉熵。图片来源：作者
- en: Look familiar? This is the binary cross entropy (BCE) loss function! The main
    intuition with this is understanding why a sigmoid is used. We have a scalar output
    and it needs to be scaled to between 0 and 1\. There are other functions capable
    of this, but the sigmoid is the most commonly used.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很熟悉吧？这就是二元交叉熵（BCE）损失函数！理解这一点的关键是理解为什么使用sigmoid。我们有一个标量输出，它需要被缩放到0和1之间。虽然也有其他函数能做到这一点，但sigmoid是最常用的。
- en: BCE in PyTorch
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch中的BCE
- en: 'When implementing BCE in PyTorch, there are a few tricks to watch out for.
    There are two different BCE functions in PyTorch: *BCELoss()* and *BCEWithLogitsLoss()*.
    A common mistake (that I have made) is incorrectly swapping the use cases.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中实现BCE时，需要注意一些技巧。PyTorch中有两个不同的BCE函数：*BCELoss()*和*BCEWithLogitsLoss()*。一个常见的错误（我也犯过）是错误地交换了它们的使用场景。
- en: '**BCELoss():** This torch function outputs the loss WITH THE SIGMOID APPLIED.
    The output will be a **probability**.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**BCELoss()：** 这个torch函数输出的是已经应用了SIGMOID的损失值。输出将是一个**概率**。'
- en: '**BCEWithLogitsLoss():** The torch function outputs logits which are the **raw
    outputs** of the model. There is NO SIGMOID APPLIED. When using this, you will
    need to apply a *torch.sigmoid()* to the output.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**BCEWithLogitsLoss()：** 这个torch函数输出的是logits，即模型的**原始输出**。没有应用SIGMOID。当使用这个函数时，你需要对输出应用*torch.sigmoid()*。'
- en: This is especially important for Transfer Learning as the model even if you
    know the model is trained with BCE, make sure to use the right one. If not, you
    make accidentally apply a sigmoid after BCELoss() causing the network to not learn…
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这对于迁移学习尤为重要，因为即使你知道模型是使用 BCE 训练的，也要确保使用正确的损失函数。如果没有这样做，可能会在 BCELoss() 后意外地应用
    sigmoid，导致网络无法学习……
- en: Once a probability is calculated using either function, it needs to be interpreted
    during inference. The probability is the model’s prediction of the likelihood
    of being true (class label of 1). Thresholding is needed to determine the cutoff
    probability of a true label. *p = 0.5* is commonly used, but it’s important to
    test out and optimize different threshold probabilities. A good idea is to plot
    a histogram of output probabilities to see the confidence of outputs before deciding
    on a threshold.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦使用任一函数计算了概率，就需要在推理过程中进行解释。这个概率是模型对结果为真的可能性的预测（类别标签为 1）。需要通过阈值来确定真实标签的截止概率。*p
    = 0.5* 是常用的阈值，但测试并优化不同的阈值概率是很重要的。一个好主意是绘制输出概率的直方图，查看输出的置信度，再决定阈值。
- en: Multiclass Classification
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多类分类
- en: The goal of multiclass classification is to assign an input *x* to one of *K*
    > 2 class labels *y* ∈ {1, 2, …, *K*}. We are going to use the categorical distribution
    as our probability distribution of choice.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 多类分类的目标是将输入 *x* 分配给 *K* > 2 个类别标签 *y* ∈ {1, 2, …, *K*} 中的一个。我们将使用分类分布作为我们的概率分布选择。
- en: '![](../Images/0a232fbe9bf6255a4b73db74c8d240e9.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a232fbe9bf6255a4b73db74c8d240e9.png)'
- en: Categorical Distribution. Image by Author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 分类分布。图片来源：作者
- en: 'This is just assigning a probability for each class for a given output and
    all probabilities must sum to 1\. We need the model *f[x, φ]* to output *p* to
    generate the predicted output probability*.* The sum issue arises as in binary
    classification. Before we can input *p* into Bernoulli, we need it to be a probability
    between 0 and 1\. A sigmoid will no longer work as it will scale each class score
    to a probability, but there is no guarantee all probabilities will sum to 1\.
    This may not immediately be apparent, but an example is shown:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是为给定输出的每个类别分配一个概率，且所有概率的总和必须为 1。我们需要模型 *f[x, φ]* 输出 *p*，以生成预测的输出概率。和二分类一样，这里也会遇到总和问题。在将
    *p* 输入到伯努利分布之前，我们需要确保它是一个介于 0 和 1 之间的概率。sigmoid 不再适用，因为它会将每个类别的分数缩放为一个概率，但不能保证所有概率的总和为
    1。这点可能不容易发现，但可以通过一个例子来说明：
- en: '![](../Images/83affdb6b334591ba290ca2a040054c9.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83affdb6b334591ba290ca2a040054c9.png)'
- en: Sigmoid does not generate probability distribution in multiclass classification.
    Image by Author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 在多类分类中无法生成概率分布。图片来源：作者
- en: We need a function that can ensure both constraints. For this, a softmax is
    chosen. A softmax is an extension of a sigmoid, but it will ensure all the probabilities
    sum to 1.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个函数来确保满足这两个约束条件。因此，选择了 softmax。softmax 是 sigmoid 的扩展，它会确保所有概率的总和为 1。
- en: '![](../Images/94e889be5ace748c47a8fb0ce491f38a.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94e889be5ace748c47a8fb0ce491f38a.png)'
- en: Softmax Function. Image by Author
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 函数。图片来源：作者
- en: 'This means the probability distribution is a softmax applied to the model output.
    The likelihood of calculating a label *k*: *Pr*(*y = k|x*) = *S*ₖ(*f[x, φ]*).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着概率分布是对模型输出应用的 softmax。计算标签*k*的似然：*Pr*(*y = k|x*) = *S*ₖ(*f[x, φ]*）。
- en: 'To derive the loss function for multiclass classification, we can plug the
    softmax and model output into the negative log-likelihood loss:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了推导多类分类的损失函数，我们可以将 softmax 和模型输出代入负对数似然损失中：
- en: '![](../Images/bfcc77f988d2af066f4750c5d37ad78d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfcc77f988d2af066f4750c5d37ad78d.png)'
- en: Multiclass Cross Entropy. Image by Author
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 多类交叉熵。图片来源：作者
- en: This is the derivation for multiclass cross entropy. It is important to remember
    the only term contributing to the loss function is the probability of the true
    class. If you have seen cross entropy, you are more familiar with a function with
    a *p(x)* and *q(x).* This is identical to the cross entropy loss equation shown
    where *p(x) = 1* for the true class and 0 for all other classes. *q(x)* is the
    softmax of the model output. The other derivation of cross entropy comes from
    using KL Divergence, and you can reach the same loss function by treating one
    term as a Dirac-delta function where true outputs exist and the other term as
    the model output with softmax. It is important to note that both routes lead to
    the same loss function.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是多类别交叉熵的推导。需要记住，唯一对损失函数有贡献的项是正确类别的概率。如果你见过交叉熵，你会更熟悉一个包含 *p(x)* 和 *q(x)* 的函数。这个公式与交叉熵损失方程是相同的，其中
    *p(x) = 1* 表示正确类别，其他类别的值为0。*q(x)* 是模型输出的softmax。交叉熵的另一个推导来源于使用KL散度，你可以通过将一个项视为Dirac
    delta函数（其中包含真实输出）并将另一个项视为模型输出（应用softmax）来推导出相同的损失函数。需要注意的是，这两种推导方式都得出相同的损失函数。
- en: Cross Entropy in PyTorch
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch中的交叉熵
- en: Unlike binary cross entropy, there is only one loss function for cross entropy
    in PyTorch. *nn.CrossEntropyLoss* returns the model output with the softmax already
    applied. Inference can be performed by taking the largest probability softmax
    model output (taking the highest probability as would be expected).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与二元交叉熵不同，PyTorch中的交叉熵损失只有一个损失函数。*nn.CrossEntropyLoss* 返回已经应用softmax的模型输出。推断可以通过选择最大概率的softmax模型输出（选择最高概率的输出）来进行。
- en: Applications
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: These were two well studied classification examples. For a more complex task,
    it may take some time to decide on a loss function and probability distribution.
    There are a lot of charts matching probability distributions with intended tasks,
    but there is always room to explore.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是两个研究得比较深入的分类例子。对于更复杂的任务，可能需要一些时间来决定使用哪个损失函数和概率分布。有很多图表可以帮助将概率分布与目标任务匹配，但总有空间可以进一步探索。
- en: For certain tasks, it may be helpful to combine loss functions. A common use
    case for this is in a classification task where it maybe helpful to combine a
    [binary] cross entropy loss with a modified Dice coefficient loss. Most of the
    time, the loss functions will be added together and scaled by some hyperparameter
    to control each individual functions contribution to loss.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些任务，将损失函数组合起来可能会有帮助。一个常见的应用场景是在分类任务中，可能有助于将[二元]交叉熵损失与修改过的Dice系数损失结合使用。大多数时候，损失函数会被加在一起，并通过某个超参数进行缩放，以控制每个函数对总损失的贡献。
- en: Hopefully this derivation of negative log-likelihood loss and its applications
    proved to be useful!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这个关于负对数似然损失及其应用的推导对你有所帮助！
- en: References
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Prince, Simon J.D., [Understanding Deep Learning](https://mitpress.mit.edu/9780262048644/understanding-deep-learning/).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Prince, Simon J.D., [理解深度学习](https://mitpress.mit.edu/9780262048644/understanding-deep-learning/)。'
- en: '[2] [Stanford CS231n](https://cs231n.github.io/).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [斯坦福CS231n](https://cs231n.github.io/)。'
- en: '[3] P. Bosman, D. Thierens, [Negative Log–Likelihood And Statistical Hypothesis
    Testing As The Basis Of Model Selection In IDEAs](https://homepages.cwi.nl/~bosman/publications/2000_negativeloglikelihood.pdf)
    (2000).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] P. Bosman, D. Thierens, [负对数似然与统计假设检验作为IDEAs中模型选择的基础](https://homepages.cwi.nl/~bosman/publications/2000_negativeloglikelihood.pdf)（2000年）。'
