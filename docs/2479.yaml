- en: Fine-Tuning LLMs with 32-bit, 8-bit, and Paged AdamW Optimizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/fine-tuning-llms-with-32-bit-8-bit-and-paged-adamw-optimizers-1034e3105634?source=collection_archive---------9-----------------------#2024-10-10](https://towardsdatascience.com/fine-tuning-llms-with-32-bit-8-bit-and-paged-adamw-optimizers-1034e3105634?source=collection_archive---------9-----------------------#2024-10-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Finding the right trade-off between memory efficiency, accuracy, and speed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--1034e3105634--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--1034e3105634--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1034e3105634--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1034e3105634--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--1034e3105634--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1034e3105634--------------------------------)
    ·7 min read·Oct 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/812b382ba4f0564091b02974dfb093d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated with Grok
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning large language models (LLMs) has become an essential yet resource-intensive
    task, demanding considerable GPU memory — especially when using the AdamW optimizer,
    which can quickly consume available resources. For each model parameter, AdamW
    requires the storage of two additional optimizer states in memory, each typically
    in float32 format. This translates to an extra 8 bytes per parameter, meaning
    that for a model with 8 billion parameters, such as Llama 3.1, roughly 64 GB of
    memory goes solely toward managing optimizer states.
  prefs: []
  type: TYPE_NORMAL
- en: The use of quantized and paged optimizers can significantly reduce memory overhead.
    Libraries like bitsandbytes have facilitated these memory-efficient approaches,
    making them increasingly popular.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will make a comparative analysis of AdamW-32bit, its 8-bit
    counterpart, and paged AdamW optimizers, examining their impact on memory consumption,
    learning curves, and training time. Our goal is to identify when memory-efficient
    optimizers are essential and evaluate their trade-offs in training speed and model
    accuracy. In the first section, we will review AdamW 8-bit and its paged variant.
    Then, we will benchmark…
  prefs: []
  type: TYPE_NORMAL
