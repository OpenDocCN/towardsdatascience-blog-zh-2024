- en: 'Beyond Attention: How Advanced Positional Embedding Methods Improve upon the
    Original Approach in Transformer Architecture'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/beyond-attention-how-advanced-positional-embedding-methods-improve-upon-the-original-transformers-90380b74d324?source=collection_archive---------3-----------------------#2024-10-29](https://towardsdatascience.com/beyond-attention-how-advanced-positional-embedding-methods-improve-upon-the-original-transformers-90380b74d324?source=collection_archive---------3-----------------------#2024-10-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'From Sinusoidal to RoPE and ALiBi: How advanced positional encodings overcome
    limitations in Transformers'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--90380b74d324--------------------------------)[![Elahe
    Aghapour](../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png)](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--90380b74d324--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--90380b74d324--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--90380b74d324--------------------------------)
    [Elahe Aghapour](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--90380b74d324--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--90380b74d324--------------------------------)
    ·9 min read·Oct 29, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '**Authors:** [**Elahe Aghapour**](https://medium.com/u/75214fb27311?source=post_page---user_mention--90380b74d324--------------------------------)**,**
    [**Salar Rahili**](https://medium.com/u/6dff1eb2cc9f?source=post_page---user_mention--90380b74d324--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Introduction:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The exponential progress of models built in recent years is deeply connected
    with the advent of the Transformer architecture. Previously, AI scientists had
    to select architectures for each task at hand, and then optimize the hyper-parameters
    to get the best performance out of it. Another challenge limiting their potential
    was the difficulty in handling long-range dependencies of the data, surfacing
    the issues of vanishing gradients, loss of context over long sequences, and the
    inability to capture global context due to locality constraints. Additionally,
    the lack of scalability and parallelization in traditional models slowed training
    on large datasets, holding back the progress in the field.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer architecture revolutionized the field by addressing these issues
    through its self-attention mechanism. It enabled models to capture relationships
    over long sequences and efficiently understand global context, all while being
    highly parallelizable and adaptable across various modalities, such as text, images,
    and more. In the self-attention mechanism, for each token, its query is compared
    against the keys of all other tokens to compute similarity scores. These similarities
    are then used to weigh the value vectors, which ultimately decide where the current
    token should attend to. Self-attention treats all tokens as equally important
    regardless of their order, losing critical information about the sequence in which
    tokens appear, and in other words, it sees the input data as a set with no order.
    Now we need a mechanism to enforce some notion of order on the data, as natural
    language and many other types of data are inherently sequential and position-sensitive.
    This is where positional embeddings come into play. Positional embeddings encode
    the position of each token in the sequence, enabling the model to maintain awareness
    of the sequence’s structure. Various methods for encoding positional information
    have been explored, and we will cover them in this blog post.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/998a5ddc31efe8bb315ebe9eab6c558c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: '**Attention Mechanism:**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let *S = {wi}* for *i =1,…,N* be a sequence of *N* input tokens where *wi*
    represents the *i*-th token. Hence, the corresponding token embedding of *S* can
    be denoted as *E = {xi}* for *i =1,…,N* where *xi* is the *d*-dimensional token
    embedding vector for token *wi*. The self-attention mechanism incorporates position
    embedding into token embeddings and generates the query, key, and value representations
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/764b45fac9c35b4504c2338f65a6995b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, the attention weights is computed based on the similarity between query
    and key vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d9f343c6adfcbfcddf0bfb8dc532620.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The attention weights determine how important token *n* is for token *m*. In
    the other words, how much attention token *m* should pay to token *n*. The output
    for token *m* is computed as a weighted sum of the value vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5303090e8fe3e310ecf8ee7321a4e822.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, the attention mechanism token *m* to gather information from other
    tokens in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d1d99e883ab4f86c0d399614800f9ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 1\. Positional encoding in transformer architecture (image from [paper](https://arxiv.org/pdf/1706.03762)).
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Absolute Position Embedding:**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A typical choice for the equation (1) is to have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1009b706a42c879f4af02733f5d9c422.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *pi* is a *d*-dimensional vector, representing the absolute position of
    token *xi*. Sinusoidal positional encoding and learned positional encoding are
    two alternatives to generate *pi*.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.a Sinusoidal Positional Encoding**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sinusoidal positional encoding was introduced in the “[Attention is all you
    need](https://arxiv.org/pdf/1706.03762)” paper where transformer architecture
    was proposed. Sinusoidal Positional Encoding provides a unique position representation
    for each token in the input sequence. It is based on sine and cosine functions
    with different frequencies as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/108d3fd539207a2e549a52e501c3172e.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *pos* is the position of the token in the sequence, *d* is the position
    embedding dimension, and i is the dimension index (*0<=i<d*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The use of sine and cosine functions in sinusoidal positional encoding has
    a deep relationship with the **Fourier transform.** By using a range of different
    frequencies to encode positions, the Transformer creates a representation similar
    to a Fourier transform where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High-frequency components** (lower *i*) enable the model to capture local
    positional information. This is useful for understanding relationships between
    neighbor tokens in a sequence, such as word pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low-frequency components** (higher *i*) capture more global patterns over
    the entire sequence. This helps the model to focus on broader relationships between
    tokens that may be far apart, such as dependencies between words in two different
    sentences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This helps the model understand the relative positions of tokens by comparing
    their positional encodings. Sinusoidal positional encoding needs no additional
    training parameters while generalizing to larger sequence lengths at inference
    time. However, its expressiveness is limited.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.b Learned Positional Encoding**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learned positional encoding was introduced in the “[Attention is all you need](https://arxiv.org/pdf/1706.03762)”
    paper and it was applied in the [BERT](https://arxiv.org/pdf/1810.04805) and [GPT](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    models as an alternative to Sinusoidal positional encoding. In learned positional
    encoding, each position in the sequence (e.g. first token, second token, etc)
    is assigned an embedding vector. These position embeddings are learned along with
    other transformer parameters during training. For example, if the model has a
    context length of 512 with a token embedding of size 768 (i.e. *d*=768), a learnable
    tensor of size 512*768 will be added to the other trainable parameters. This means
    the model gradually learns the best way to encode positional information for the
    specific task, such as text classification or translation.
  prefs: []
  type: TYPE_NORMAL
- en: Learned positional embedding is more expressive than sinusoidal one as the model
    can learn a position embedding, effective for its specific task. However, they
    introduce more trainable parameters which increases the model size and its computational
    cost.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Relative Positional Embeddings**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both sinusoidal and learned position encodings focused on the absolute position
    of the token. However, the attention mechanism works by computing how important
    other tokens are for each specific token in the sequence. Hence, this process
    depends on the relative position of the tokens (how far apart they are from each
    other), rather than the absolute position of the tokens. To address the limitations
    of absolute position embedding, relative position encoding was introduced.
  prefs: []
  type: TYPE_NORMAL
- en: '[RelativePosEmb](https://arxiv.org/pdf/1803.02155) doesn’t add position information
    to token embeddings. Instead, it modifies the way key and value are computed at
    every layer as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72e5b379418a7f719d2e2f6566b6c4bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *r = clip(m-n, Rmin, Rmax)* represents the relative distance between position
    m and n. The maximum relative position is clipped, assuming that precise relative
    position is not useful beyond a certain distance. Clipping the maximum distance
    enables the model to extrapolate at inference time, i.e. to generalize to sequence
    length not seen during training. However, this approach may miss some useful information
    from the absolute position of the token (like the position of the first token).
  prefs: []
  type: TYPE_NORMAL
- en: You may notice that *fq* lacks position embedding. That’s because we are encoding
    the relative position. In the attention formula, the query and key values are
    used to compute attention weights as equation (2) therefore we only need either
    the query or the key to include the relative position embedding.
  prefs: []
  type: TYPE_NORMAL
- en: This encoding has been used in many models as [Transformer-XL](https://arxiv.org/pdf/1901.02860)
    and [T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf). There are different
    alternatives in applying relative positional encoding that you can find in papers
    [[7]](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf) and [[8]](https://arxiv.org/pdf/2006.03654)
    .
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Rotary Positional Embedding (RoPE)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unlike previous methods, [RoPE](https://arxiv.org/pdf/2104.09864) rotates the
    vectors in a multi-dimensional space based on the position of tokens. Instead
    of adding position information to token embeddings, it modifies the way attention
    weights are computed at every layer as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c2c747a497c0ec921bc2dc550a8ac7f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'They proposed a generalized rotation matrix to any even embedding dimensionality
    *d* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9b8dc8e3f08c5dbc3709a5c8eef9607.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *θi* is pre-defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7d44151cb5fc03ca784f80c9665741b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Applying [RoPE](https://arxiv.org/pdf/2104.09864) to attention weight yields
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d510a7dbb26896bd120fdcbb45ff781.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that [RoPE](https://arxiv.org/pdf/2104.09864) formulation doesn’t add position
    information to the values in the attention module. The output of the attention
    module is a weighted sum of the value vector and since position information isn’t
    added to values, the outputs of each transformer layer don’t have explicit position
    details.
  prefs: []
  type: TYPE_NORMAL
- en: Popular models such as [LLaMA](https://arxiv.org/pdf/2302.13971) and [GPT-NeoX](https://arxiv.org/pdf/2204.06745#cite.su2021roformer)
    are using [RoPE](https://arxiv.org/pdf/2104.09864).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/042b42f1c09ae586cd90a211c1723857.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 2: [ALiBi](https://arxiv.org/pdf/2108.12409) method visualization (image
    from [paper](https://arxiv.org/pdf/2108.12409)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Attention with Linear Biases (ALiBi)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[ALiBi](https://arxiv.org/pdf/2108.12409) also does not add positional encodings
    to word embeddings; instead, it adds a penalty to attention weight scores that
    is proportional to the distance between tokens. Therefore, the attention score
    between two tokens i and j at every layer is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Attention score = query_i . key_j — m.(i-j)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where *-m.(i-j)* is a penalty which is proportional to the distance between
    token *i* and *j*. The scalar *m* is a head-specific slope fixed before training
    and its values for different heads are chosen as a geometric sequence. For example,
    for 8 head, *m* might be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a0c79a5fea18ba0aabfa8f9b196834a.png)'
  prefs: []
  type: TYPE_IMG
- en: This means, the first head has a relatively large *m* so it penalizes far apart
    tokens more and focuses on recent tokens, while the 8th head has the smallest
    *m*, allowing it to attend to more distant tokens. Fig. 2 also offers visualization.
  prefs: []
  type: TYPE_NORMAL
- en: ALiBi is used in [BloombergGPT](https://arxiv.org/pdf/2303.17564) and [BLOOM](https://inria.hal.science/hal-03850124v1/document).
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformer Extrapolation At Inference Time:**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformer extrapolation at inference time is the model’s ability to perform
    well to input sequences that are longer than those it was trained on. The transformer
    mechanism is agnostic to input length which means at inference time, it can work
    with longer sequences. However, note that the computational cost grows quadratically
    with input length even though the transformer layers themselves are agnostic to
    it.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of [ALiBi](https://arxiv.org/pdf/2108.12409) demonstrated that the
    bottleneck for transformer extrapolation is its position embedding method. As
    shown in Fig. 3, they compared the extrapolation capabilities of different position
    embedding methods. Since learned position embedding does not have a capability
    to encode positions greater than the training length, it has no extrapolation
    ability.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/463e1fb96f99aff8dbb3dd6fad4ab1f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 3: Extrapolation: as the input sequence gets longer (x-axis), [sinusoidal](https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf),
    [RoPE](https://arxiv.org/pdf/2104.09864), and [T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)
    position encodings show degraded perplexity (y-axis, lower is better), while [ALiBi](https://arxiv.org/pdf/2108.12409)
    does not (image from [paper](https://arxiv.org/pdf/2108.12409)).'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3 shows that the sinusoidal position embedding in practice has very limited
    extrapolation capabilities. While [RoPE](https://arxiv.org/pdf/2104.09864) outperforms
    the sinusoidal one, it still does not achieve satisfactory results. The [T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)
    bias method (a version of relative position embedding) leads to better extrapolation
    than both sinusoidal and [RoPE](https://arxiv.org/pdf/2104.09864) embedding. Unfortunately,
    the [T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf) bias is computationally
    expensive (Fig. 4). [ALiBi](https://arxiv.org/pdf/2108.12409) outperforms all
    these position embeddings with negligible (0–0.7%) memory increase.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5de14ba673b2dde6cc992ccadda9135d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 4: comparison of batched training, inference speed and memory use of [sinusoidal](https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf),
    [RoPE](https://arxiv.org/pdf/2104.09864), [T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf),
    and [ALiBi](https://arxiv.org/pdf/2108.12409) position encodings (image from [paper](https://arxiv.org/pdf/2108.12409)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Conclusion:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In summary, the way positional information is being encoded in Transformer architecture
    significantly affects its ability to understand sequential data, especially its
    extrapolation at inference time. While absolute positional embedding methods provide
    positional awareness, they often struggle with Transformer extrapolation. That’s
    why newer position embeddings are proposed. Relative position encoding, RoPE,
    and ALiBi have the capability to extrapolate at inference time. As transformers
    continue to be integrated in various applications, refining position encoding
    is crucial to push the boundaries of their performance.
  prefs: []
  type: TYPE_NORMAL
- en: The opinions expressed in this blog post are solely our own and do not reflect
    those of our employer.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**References:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Vaswani, A. “Attention is all you need.” (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [BERT](https://arxiv.org/pdf/1810.04805): Devlin, Jacob. “Bert: Pre-training
    of deep bidirectional transformers for language understanding.” (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [GPT](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf):
    Radford, Alec, et al. “Language models are unsupervised multitask learners.” (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [RelativePosEmb](https://arxiv.org/pdf/1803.02155): Shaw, Peter, et al.
    “Self-attention with relative position representations.” (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [Transformer-XL](https://arxiv.org/pdf/1901.02860) Dai, Zihang. “Transformer-xl:
    Attentive language models beyond a fixed-length context.” (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] [T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf): Raffel, Colin,
    et al. “Exploring the limits of transfer learning with a unified text-to-text
    transformer.” (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Raffel, Colin, et al. “Exploring the limits of transfer learning with a
    unified text-to-text transformer.” (2020)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] He, Pengcheng, et al. “Deberta: Decoding-enhanced bert with disentangled
    attention.” (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] [RoPE](https://arxiv.org/pdf/2104.09864): Su, Jianlin, et al. “Roformer:
    Enhanced transformer with rotary position embedding.” (2024).'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] [LLaMA](https://arxiv.org/pdf/2302.13971): Touvron, Hugo, et al. “Llama:
    Open and efficient foundation language models.” (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] [GPT-NeoX](https://arxiv.org/pdf/2204.06745#cite.su2021roformer): Black,
    Sid, et al. “Gpt-neox-20b: An open-source autoregressive language model.” (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] [ALiBi](https://arxiv.org/pdf/2108.12409): Press, Ofir, et al. “Train
    short, test long: Attention with linear biases enables input length extrapolation.”
    (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] [BloombergGPT](https://arxiv.org/pdf/2303.17564): Wu, Shijie, et al. “Bloomberggpt:
    A large language model for finance.” (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] [BLOOM](https://inria.hal.science/hal-03850124v1/document): Le Scao, Teven,
    et al. “Bloom: A 176b-parameter open-access multilingual language model.” (2023).'
  prefs: []
  type: TYPE_NORMAL
