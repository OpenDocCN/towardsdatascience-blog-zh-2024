<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Scale Is All You Need for Lip-Sync?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Scale Is All You Need for Lip-Sync?</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scale-is-all-you-need-for-lip-sync-0c571423f60f?source=collection_archive---------5-----------------------#2024-06-07">https://towardsdatascience.com/scale-is-all-you-need-for-lip-sync-0c571423f60f?source=collection_archive---------5-----------------------#2024-06-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ff2f" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Alibaba’s EMO and Microsoft’s VASA-1 are crazy good. Let’s break down how they work.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@jacksaunders909?source=post_page---byline--0c571423f60f--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jack Saunders" class="l ep by dd de cx" src="../Images/00c752fe1c5bc03f52943238cb034ee4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*ZuPNDHEhDvGYCwn1jF4FVA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--0c571423f60f--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@jacksaunders909?source=post_page---byline--0c571423f60f--------------------------------" rel="noopener follow">Jack Saunders</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--0c571423f60f--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="e6fc" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">It’s no secret that the pace of AI research is exponentially accelerating. One of the biggest trends of the past couple of years has been using transformers to exploit huge-scale datasets. It looks like this trend has finally reached the field of lip-sync models. <a class="af ne" href="https://github.com/HumanAIGC/EMO?tab=readme-ov-file" rel="noopener ugc nofollow" target="_blank">The EMO release</a> by Alibaba set the precedent for this (I mean look at the 200+ GitHub issues begging for code release). But the bar has been raised even higher with <a class="af ne" href="https://www.microsoft.com/en-us/research/project/vasa-1/" rel="noopener ugc nofollow" target="_blank">Microsoft’s VASA-1 last month</a>.</p><figure class="nf ng nh ni nj nk"><div class="nl io l ed"><div class="nm nn l"/></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">Demo videos from <a class="af ne" href="https://www.microsoft.com/en-us/research/project/vasa-1/" rel="noopener ugc nofollow" target="_blank">VASA-1</a> and EMO. All image credits to the respective authors.</figcaption></figure><p id="3bbd" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">They’ve received a lot of hype, but so far no one has discussed what they’re doing. They look like almost identical works on the face of it (pun intended). Both take a single image and animate it using audio. Both use diffusion and both exploit scale to produce phenomenal results. But in actuality, there are a few differences under the hood. This article will take a sneak peek at how these models operate. We also take a look at the ethical considerations of these papers, given their obvious potential for misuse.</p><h1 id="9d1d" class="nv nw fq bf nx ny nz gq oa ob oc gt od oe of og oh oi oj ok ol om on oo op oq bk">The Data</h1><p id="0925" class="pw-post-body-paragraph mi mj fq mk b go or mm mn gr os mp mq mr ot mt mu mv ou mx my mz ov nb nc nd fj bk">A model can only be as good as the data it is trained on. Or, more succinctly, Garbage In = Garbage Out. Most existing lip sync papers make use of one or two, reasonably small datasets. The two papers we are discussing absolutely blow away the competition in this regard. Let’s break down what they use. Alibaba state in EMO:</p><blockquote class="ow ox oy"><p id="ad22" class="mi mj oz mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We collected approximately 250 hours of talking head videos from the internet and supplemented this with the HDTF [34] and VFHQ [31] datasets to train our models.</p></blockquote><p id="3788" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Exactly what they mean by the additional 250 hours of collected data is unknown. However, HDTF and VFHQ are publicly available datasets, so we can break these down. <a class="af ne" href="https://github.com/MRzzm/HDTF" rel="noopener ugc nofollow" target="_blank">HDTF</a> consists of 16 hours of data over 300 subjects of 720–1080p video. <a class="af ne" href="https://liangbinxie.github.io/projects/vfhq/" rel="noopener ugc nofollow" target="_blank">VFHQ</a> doesn’t mention the length of the dataset in terms of hours, but it has 15,000 clips and takes up 1.2TB of data. If we assume each clip is, on average, at least 10s long then this would be an additional 40 hours. This means EMO uses at least 300 hours of data. For VASA-1 Microsoft say:</p><blockquote class="ow ox oy"><p id="b155" class="mi mj oz mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The model is trained on VoxCeleb2 [13] and another high-resolution talk video dataset collected by us, which contains about 3.5K subjects.</p></blockquote><p id="d976" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Again, the authors are being secretive about a large part of the dataset. <a class="af ne" href="https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html" rel="noopener ugc nofollow" target="_blank">VoxCeleb2</a> is publicly available. Looking at the accompanying paper we can see this consists of 2442 hours of data (that’s not a typo) across 6000 subjects albeit in a lower resolution than the other datasets we mentioned (360–720p). This would be ~2TB. Microsoft uses a dataset of 3.5k additional subjects, which I suspect are far higher quality and allow the model to produce high-quality video. If we assume this is at least 1080p and some of it is 4k, with similar durations to VoxCeleb2 then we can expect another 5–10TB.</p><p id="192a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">For the following, I am making some educated guesses: </strong>Alibaba likely uses 300 hours of high-quality video (1080p or higher), while Microsoft uses ~2500 hours of low-quality video, and maybe somewhere between 100–1000 hours of very high-quality video. If we try to estimate the dataset size in terms of storage space we find that <strong class="mk fr">EMO and VASA-1 each use ~10TB of face video data to train their model. </strong>For some comparisons check out the following chart:</p><figure class="nf ng nh ni nj nk nr ns paragraph-image"><div class="nr ns pa"><img src="../Images/7e37d00962bd5486f9dcece867f90acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*MTJM-uGBaIhINgbsJm5Z9g.png"/></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">A comparison of the estimated dataset sizes of several state-of-the-art talking head generation models. Image by me. (CC-BY)</figcaption></figure><h1 id="8685" class="nv nw fq bf nx ny nz gq oa ob oc gt od oe of og oh oi oj ok ol om on oo op oq bk">The Models</h1><p id="c173" class="pw-post-body-paragraph mi mj fq mk b go or mm mn gr os mp mq mr ot mt mu mv ou mx my mz ov nb nc nd fj bk">Both models make use of diffusion and transformers to utilise the massive datasets. However, there are some key differences in how they work.</p><h1 id="7a49" class="nv nw fq bf nx ny nz gq oa ob oc gt od oe of og oh oi oj ok ol om on oo op oq bk">VASA-1</h1><p id="775e" class="pw-post-body-paragraph mi mj fq mk b go or mm mn gr os mp mq mr ot mt mu mv ou mx my mz ov nb nc nd fj bk">We can break down VASA-1 into two components. One is an image formation model that takes some latent representation of the facial expression and pose and produces a video frame. The other is a model that generates these latent pose and expression vectors from audio input. The image formation model</p><figure class="nf ng nh ni nj nk nr ns paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="nr ns pc"><img src="../Images/ac93158b0a2f00186ca3a42c6efe0e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AoJYoq4srtcPCYn8qHvN-Q.png"/></div></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">The VASA-1 model diagram. The left shows the audio-to-latent generation. The right shows the image formation model. <a class="af ne" href="https://www.microsoft.com/en-us/research/project/vasa-1/" rel="noopener ugc nofollow" target="_blank">Diagram from the VASA-1 paper</a>. (CC-BY)</figcaption></figure><h2 id="fcec" class="ph nw fq bf nx pi pj pk oa pl pm pn od mr po pp pq mv pr ps pt mz pu pv pw px bk">Image Formation Model</h2><p id="04d1" class="pw-post-body-paragraph mi mj fq mk b go or mm mn gr os mp mq mr ot mt mu mv ou mx my mz ov nb nc nd fj bk">VASA-1 relies heavily on a 3D volumetric representation of the face, building upon previous work from Samsung called <a class="af ne" href="https://arxiv.org/abs/2207.07621" rel="noopener ugc nofollow" target="_blank">MegaPortraits.</a> The idea here is to first estimate a 3D representation of a source face, warp it using the predicted source pose, make edits to the expression using knowledge of both the source and target expressions in this canonical space, and then warp it back using a target pose.</p><figure class="nf ng nh ni nj nk nr ns paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="nr ns py"><img src="../Images/af31766ad997f48e2dc3f43e8063036a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ehb39ePzOBgHGlo0XYAhMg.png"/></div></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">Model Diagram of the MegaPortraits approach. v is a volumetric representation of the face, e is an identity later descriptor, R and t are the pose (rotation and translation) and z is the facial expression code. <a class="af ne" href="https://arxiv.org/pdf/2207.07621" rel="noopener ugc nofollow" target="_blank">From the MegaPortraits paper.</a> (CC-BY)</figcaption></figure><p id="f8a5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In more detail, this process looks as follows:</p><ul class=""><li id="30ea" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pz qa qb bk">Take the source image (the man in the diagram above) and predict a simple 1D vector which represents this man.</li><li id="0621" class="mi mj fq mk b go qc mm mn gr qd mp mq mr qe mt mu mv qf mx my mz qg nb nc nd pz qa qb bk">Also predict a 4D tensor (Width, Height, Depth, RGB) as a volumetric representation of him.</li><li id="c65d" class="mi mj fq mk b go qc mm mn gr qd mp mq mr qe mt mu mv qf mx my mz qg nb nc nd pz qa qb bk">Predict for both the source and driver (the woman above) their pose and facial expression. <strong class="mk fr">Note that only the pose estimation is pre-trained, all the others are trained completely from scratch.</strong></li><li id="b79f" class="mi mj fq mk b go qc mm mn gr qd mp mq mr qe mt mu mv qf mx my mz qg nb nc nd pz qa qb bk">Create two warping fields using neural networks. One converts the man's volumetric representation into a canonical space <strong class="mk fr">(this just means a front-facing, neutral expression)</strong> using our estimate of his identity, pose and facial expression. The other converts his canonical 3D face into a posed 3D face using estimates of the woman’s pose and expression, as well as the man’s identity.</li><li id="8df0" class="mi mj fq mk b go qc mm mn gr qd mp mq mr qe mt mu mv qf mx my mz qg nb nc nd pz qa qb bk">“Render” the posed man’s face back into 2D.</li></ul><p id="f0e5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For details on how exactly they do this, that is how do you project into 3D, how is the warping achieved and how is the 2D image created from the 3D volume, please refer to the <a class="af ne" href="https://arxiv.org/abs/2207.07621" rel="noopener ugc nofollow" target="_blank">MegaPortraits paper.</a></p><p id="5ea4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This highly complex process can be simplified in our minds at this point to just imagine a model that encodes the source in some way and then takes parameters for pose and expression, creating an image based on these.</p><h2 id="e0fa" class="ph nw fq bf nx pi pj pk oa pl pm pn od mr po pp pq mv pr ps pt mz pu pv pw px bk">Audio-to-latent Generation</h2><p id="1731" class="pw-post-body-paragraph mi mj fq mk b go or mm mn gr os mp mq mr ot mt mu mv ou mx my mz ov nb nc nd fj bk">We now have a way to generate video from a sequence of expressions and pose latent codes. However, unlike MegaPortraits, we don’t want to control our videos using another person’s expressions. Instead, we want control from audio alone. To do this, we need to build a generative model that takes audio as input and outputs latent vectors. This needs to scale up to huge amounts of data, have lip-sync and also produce diverse and plausible head motions. Enter the diffusion transformer. Not familiar with these models? I don’t blame you, there are a lot of advances here to keep up with. I can recommend the following article:</p><div class="qh qi qj qk ql qm"><a rel="noopener follow" target="_blank" href="/diffusion-transformer-explained-e603c4770f7e?source=post_page-----0c571423f60f--------------------------------"><div class="qn ab ig"><div class="qo ab co cb qp qq"><h2 class="bf fr hw z io qr iq ir qs it iv fp bk">Diffusion Transformer Explained</h2><div class="qt l"><h3 class="bf b hw z io qr iq ir qs it iv dx">Exploring the architecture that brought transformers into image generation</h3></div><div class="qu l"><p class="bf b dy z io qr iq ir qs it iv dx">towardsdatascience.com</p></div></div><div class="qv l"><div class="qw l qx qy qz qv ra lq qm"/></div></div></a></div><p id="f571" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">But in a nutshell, diffusion transformers (DiTs) replace the conventional UNET in image-based diffusion models with a transformer. This switch enables learning on data with any structure, thanks to tokenization, and it is also known to scale extremely well to large datasets. For example, <a class="af ne" href="https://openai.com/index/sora/" rel="noopener ugc nofollow" target="_blank">OpenAI’s SORA</a> model is believed to be a diffusion transformer.</p><figure class="nf ng nh ni nj nk nr ns paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="nr ns rb"><img src="../Images/a05216dfd4de90fe35d5d34a65c23fc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ORtLyKEU39wc5h-fAaNR9A.png"/></div></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">The model architecture of DiT from <a class="af ne" href="https://arxiv.org/pdf/2212.09748" rel="noopener ugc nofollow" target="_blank">Scalable Diffusion Models with Transformers</a></figcaption></figure><p id="b79d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The idea then is to start from random noise in the same shape as the latent vectors and gradually denoise them to produce meaningful vectors. This process can then be conditioned on additional signals. For our purposes, this includes audio, extracted into feature vectors using <a class="af ne" href="https://huggingface.co/docs/transformers/en/model_doc/wav2vec2" rel="noopener ugc nofollow" target="_blank">Wav2Vec2 </a>(see <a class="af ne" href="https://evelynfan.github.io/audio2face/" rel="noopener ugc nofollow" target="_blank">FaceFormer </a>for how exactly this works). Additional signals are also used. We won’t go into too much detail but they include eye gaze direction and emotion. To ensure temporal stability, the previously generated motion latent codes are also used as conditioning.</p><h1 id="bea6" class="nv nw fq bf nx ny nz gq oa ob oc gt od oe of og oh oi oj ok ol om on oo op oq bk">EMO</h1><figure class="nf ng nh ni nj nk nr ns paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="nr ns rc"><img src="../Images/f2f08853244950e08272af55265967f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7wIJ3Ehs3SECCfgIyZk4xA.png"/></div></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">The model diagram from <a class="af ne" href="https://humanaigc.github.io/emote-portrait-alive/" rel="noopener ugc nofollow" target="_blank">the EMO paper.</a> (CC-BY)</figcaption></figure><p id="7670" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">EMO takes a slightly different approach with its generation process, though it still relies on diffusion at its core. The model diagram looks a bit crowded, so I think its best to break it into smaller parts.</p><h2 id="b3d7" class="ph nw fq bf nx pi pj pk oa pl pm pn od mr po pp pq mv pr ps pt mz pu pv pw px bk">Using Stable Diffusion</h2><p id="3bf4" class="pw-post-body-paragraph mi mj fq mk b go or mm mn gr os mp mq mr ot mt mu mv ou mx my mz ov nb nc nd fj bk">The first thing to notice is that EMO makes heavy use of the pretrained Stable Diffusion 1.5 model. There is a solid trend in vision at large currently towards building on top of this model. In the above diagram, the reference net and the backbone network are both instances of the SD1.5 UNET archietecture and are initialised with these weights. The detail is lacking, but presumably the VAE encoder and decoder are also taken from Stable Diffusion. The VAE components are frozen, meaning that all of the operations performed in the EMO model are done in the latent space of that VAE. The use of the same archieture and same starting weights is useful because it allows activations from intermediate layers to be easily taken from one network and used in another (they will roughly represent the same thing in both network).</p><h2 id="ad70" class="ph nw fq bf nx pi pj pk oa pl pm pn od mr po pp pq mv pr ps pt mz pu pv pw px bk">Training the First Stage</h2><p id="97e2" class="pw-post-body-paragraph mi mj fq mk b go or mm mn gr os mp mq mr ot mt mu mv ou mx my mz ov nb nc nd fj bk">The goal of the first stage is to get a single image model that can generate a novel image of a person, given a reference frame of that person. This is achieved using a diffusion model. A basic diffusion model could be used to generate random images of people. In stage one, we want to, in some way, condition this generation process on identity. The way the authors do this is by encoding a reference image of a person using the reference net, and introducing the activations in each layer into the backbone network which is doing the diffusion. See the (poorly drawn) diagram below.</p><figure class="nf ng nh ni nj nk nr ns paragraph-image"><div class="nr ns rd"><img src="../Images/f82f071f0da2819e967ce8e4b8df2eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*NViC8yvEDkYLo9u_mkDu6w.png"/></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">Basic Diagram of a simplified version of stage 1. Image by me. (CC-BY)</figcaption></figure><blockquote class="ow ox oy"><p id="d436" class="mi mj oz mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">At this stage, we now have a model that can generate random frames of a person, given a single image of that person. We now need to control it in some way.</p></blockquote><h2 id="638e" class="ph nw fq bf nx pi pj pk oa pl pm pn od mr po pp pq mv pr ps pt mz pu pv pw px bk">Training the Second Stage</h2><p id="2dfa" class="pw-post-body-paragraph mi mj fq mk b go or mm mn gr os mp mq mr ot mt mu mv ou mx my mz ov nb nc nd fj bk">We want to control the generated frames using two signals, motion and audio. The audio part is the easier to explain, so I’ll cover this first.</p><figure class="nf ng nh ni nj nk nr ns paragraph-image"><div class="nr ns re"><img src="../Images/b43d443ec51a2aecd5ff9910ac7cc6da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*v8AT6AOJMZFO_q5Z3DZtAw.png"/></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">The zoomed in backbone network of EMO. Taken from <a class="af ne" href="https://humanaigc.github.io/emote-portrait-alive/" rel="noopener ugc nofollow" target="_blank">the EMO paper</a>. (CC-BY)</figcaption></figure><ul class=""><li id="02d1" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pz qa qb bk"><strong class="mk fr">Audio: </strong>As with VASA-1, audio is encoded in the form of wav2vec2 features. These are incorporated into the backbone network using cross attention. This cross attention replaces the text prompt cross attention already present in the Stable Diffusion 1.5 model.</li><li id="9d91" class="mi mj fq mk b go qc mm mn gr qd mp mq mr qe mt mu mv qf mx my mz qg nb nc nd pz qa qb bk"><strong class="mk fr">Motion: </strong>Motion is added using motion frames, to predict the frame at time t, the previous n frames are used to provide context for the motion. The motion frames are encoded in the same way as the reference frame. The intermediate feature activations of the reference net are used to condition the backbone model. The inclusion of these motion referenece activations is done using a specially designed cross-attention layer, taken from <a class="af ne" href="https://animatediff.github.io/" rel="noopener ugc nofollow" target="_blank">AnimateDiff</a>. From these n frames, the next f are predicted using the diffusion model.</li></ul><p id="81a1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In addition to this, two other components are used. One provides a mask, taken as the union of all bounding boxes across the training video. This mask defines what region of the video is allowed to be changed. The other is a small addition of a speed condition is used. The pose velocity is divided into buckets (think slow, medium, fast) and also included. This allows us to specify the speed of the motion at inference time.</p><h2 id="3a3a" class="ph nw fq bf nx pi pj pk oa pl pm pn od mr po pp pq mv pr ps pt mz pu pv pw px bk">Inference</h2><p id="d58b" class="pw-post-body-paragraph mi mj fq mk b go or mm mn gr os mp mq mr ot mt mu mv ou mx my mz ov nb nc nd fj bk">The model is now able to take the following and produces a new set of frames:</p><ul class=""><li id="3002" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pz qa qb bk">A reference frame</li><li id="cea4" class="mi mj fq mk b go qc mm mn gr qd mp mq mr qe mt mu mv qf mx my mz qg nb nc nd pz qa qb bk">The previous n frames</li><li id="72ea" class="mi mj fq mk b go qc mm mn gr qd mp mq mr qe mt mu mv qf mx my mz qg nb nc nd pz qa qb bk">The audio</li><li id="8f74" class="mi mj fq mk b go qc mm mn gr qd mp mq mr qe mt mu mv qf mx my mz qg nb nc nd pz qa qb bk">The head motion speed</li><li id="e809" class="mi mj fq mk b go qc mm mn gr qd mp mq mr qe mt mu mv qf mx my mz qg nb nc nd pz qa qb bk">A bounding box of pixels that can be changed</li></ul><p id="488c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For the first frame, it is not stated, but I assume the reference frame is repeated and passed as the last n frames. After this point, the model is autoregressive, the outputs are then used as the previous frames for input.</p><h1 id="0db1" class="nv nw fq bf nx ny nz gq oa ob oc gt od oe of og oh oi oj ok ol om on oo op oq bk">Ethics Discussion</h1><p id="bb94" class="pw-post-body-paragraph mi mj fq mk b go or mm mn gr os mp mq mr ot mt mu mv ou mx my mz ov nb nc nd fj bk">The ethical implications of these works are, of course, very significant. They require only a single image in order to create very realistic synthetic content. This could easily be used to misrepresent people. Given the recent controversy surrounding <a class="af ne" href="https://edition.cnn.com/2024/05/22/tech/openai-scarlett-johansson-lawsuit-sam-altman/index.html" rel="noopener ugc nofollow" target="_blank">OpenAI’s use of a voice that sounds suspiciously like Scarlett Johansen without her consent</a>, the issue is particularly relevant at the moment. The two groups take rather different approaches.</p><h2 id="35e5" class="ph nw fq bf nx pi pj pk oa pl pm pn od mr po pp pq mv pr ps pt mz pu pv pw px bk"><strong class="al">EMO</strong></h2><p id="6cdd" class="pw-post-body-paragraph mi mj fq mk b go or mm mn gr os mp mq mr ot mt mu mv ou mx my mz ov nb nc nd fj bk">The discussion in the EMO paper is very much lacking. The paper does not include any discussion of the ethical implications or any proposed methods of preventing misuse. The project page says only:</p><blockquote class="ow ox oy"><p id="3479" class="mi mj oz mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">“This project is intended solely for academic research and effect demonstration”</p></blockquote><p id="e535" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This seems like a very weak attempt. Furthermore, Alibaba include a GitHub repo which (may) make the code publicly available. It’s important to consider the pros and cons of doing this, <a class="af ne" href="https://medium.com/towards-data-science/should-deepfakes-be-open-sourced-87d7644a0765" rel="noopener">as we discuss in a previous article.</a> Overall, the EMO authors have not shown too much consideration for ethics.</p><h2 id="7c37" class="ph nw fq bf nx pi pj pk oa pl pm pn od mr po pp pq mv pr ps pt mz pu pv pw px bk"><strong class="al">VASA-1</strong></h2><p id="ab6c" class="pw-post-body-paragraph mi mj fq mk b go or mm mn gr os mp mq mr ot mt mu mv ou mx my mz ov nb nc nd fj bk">VASA-1’s authors take a more comprehensive approach to preventing misuse. They include a section in the paper dedicated to this, highlighting the potential uses in deepfake detection as well as the positive benefits.</p><figure class="nf ng nh ni nj nk nr ns paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="nr ns rf"><img src="../Images/9470848109e9c0de02bb9ab05b1ff523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-m5lfBCLnt1IMELjjaPlwA.png"/></div></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">The ethics section from VASA-1. Image take from the <a class="af ne" href="https://arxiv.org/pdf/2404.10667" rel="noopener ugc nofollow" target="_blank">arxiv preprint</a>.</figcaption></figure><p id="3cfe" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In addition to this, they also include a rather interesting statement:</p><blockquote class="ow ox oy"><p id="befb" class="mi mj oz mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="fq">Note: all portrait images on this page are virtual, non-existing identities generated by StyleGAN2 or DALL·E-3 (except for Mona Lisa). We are exploring visual affective skill generation for virtual, interactive characters, NOT impersonating any person in the real world. This is only a research demonstration and there’s no product or API release plan.</em></p></blockquote><p id="d0dc" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The approach is actually one Microsoft have started to take in a few papers. They only create synthetic videos using synthetic people and do not release any of their models. Doing so prevents any possible misuse, as no real people are edited. However, it does raise issues around the fact that the power to create such videos in concentrated into the arms of big-tech companies that have the infrastructure to train such models.</p><h2 id="a379" class="ph nw fq bf nx pi pj pk oa pl pm pn od mr po pp pq mv pr ps pt mz pu pv pw px bk">Further Analysis</h2><p id="86cd" class="pw-post-body-paragraph mi mj fq mk b go or mm mn gr os mp mq mr ot mt mu mv ou mx my mz ov nb nc nd fj bk">In my opinion this line of work opens up a new set of ethical issues. While it had been previously possible to create fake videos of people, it usually required several minutes of data to train a model. This largely restricted the potential victims to people who create lots of video already. While this allowed for the creation of political misinformation, the limitations helped to stifle some other applications. For one, if someone creates a lot of video, it is possible to tell what their usual content looks like (what do they usually talk about, what their opinions are, etc.) and can learn to spot videos that are uncharacteristic. This becomes more difficult if a single image can be used. What’s more, anyone can become a victim of these models. Even a social media account with a profile picture would be enough data to build a model of a person.</p><p id="3105" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Furthermore, as a different class of “deepfake” there is not much research on how to detect these models. Methods that may have worked to catch video deepfake models would become unreliable.</p><blockquote class="ow ox oy"><p id="7a47" class="mi mj oz mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We need to ensure that the harm caused by these models is limited. Microsoft’s approach of limiting access and only using synthetic people helps for the short term. But long term we need robust regulation of the applications of these models, as well as reliable methods to detect content generated by them.</p></blockquote><h1 id="7cf7" class="nv nw fq bf nx ny nz gq oa ob oc gt od oe of og oh oi oj ok ol om on oo op oq bk">Conclusion</h1><p id="5035" class="pw-post-body-paragraph mi mj fq mk b go or mm mn gr os mp mq mr ot mt mu mv ou mx my mz ov nb nc nd fj bk">Both VASA-1 and EMO are incredible papers. They both exploit diffusion models and large-scale datasets to produce extremely high quality video from audio and a single image. A few key points stand out to me:</p><ul class=""><li id="538a" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pz qa qb bk">It’s not quite a case of scale is all you need. Both models use clever tricks (VASA-1’s use of MegaPortiats and EMO’s reference &amp; backbone nets). However, it does seem to be the case that <strong class="mk fr">“scale is something you need”.</strong></li><li id="6325" class="mi mj fq mk b go qc mm mn gr qd mp mq mr qe mt mu mv qf mx my mz qg nb nc nd pz qa qb bk"><strong class="mk fr">Diffusion is king.</strong> Both of these models, as well as most state-of-the-art generation in vision, use diffusion. It seems that VAE’s and GAN’s are almost truly dead.</li><li id="5ab7" class="mi mj fq mk b go qc mm mn gr qd mp mq mr qe mt mu mv qf mx my mz qg nb nc nd pz qa qb bk">The field of lip-sync models is likely to become the domain of the big companies only soon. If trends continue, <strong class="mk fr">there is no way academics will be able to build models </strong>that keep up with these.</li></ul></div></div></div></div>    
</body>
</html>