<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Knowledge-Enhanced Agents for Interactive Text Games</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Knowledge-Enhanced Agents for Interactive Text Games</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/knowledge-enhanced-agents-for-interactive-text-games-359e57da5de3?source=collection_archive---------17-----------------------#2024-01-10">https://towardsdatascience.com/knowledge-enhanced-agents-for-interactive-text-games-359e57da5de3?source=collection_archive---------17-----------------------#2024-01-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="cd73" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Revolutionizing Interactive Text Games with Knowledge-Enhanced AI Agents</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Prateek Chhikara" class="l ep by dd de cx" src="../Images/4cabb40cbab34038c0f762b45d58bbba.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*F6Bov2ToQ_oaiAcr8eYWpg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------" rel="noopener follow">Prateek Chhikara</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="2778" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk"><strong class="al">Introduction:</strong></h1><p id="199d" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Communication through natural language is crucial to machine intelligence [9]. The recent progress in computational language models (LMs) has enabled strong performance on tasks with limited interaction, like question-answering and procedural text understanding [10]. Recognizing that interactivity is an essential aspect of communication, the community has turned its attention towards training and evaluating agents in interactive fiction (IF) environments, like text-based games, which provide a unique testing ground for investigating the reasoning abilities of LMs and the potential for Artificial Intelligence (AI) agents to perform multi-step real-world tasks in a constrained environment. For instance, in Figure 1, an agent must pick a fruit in the living room and place it in a blue box in the kitchen. In these games, agents navigate complex environments using text-based inputs, which demands a sophisticated understanding of natural language and strategic decision-making from AI agents. To succeed in these games, agents must manage their knowledge, reason, and generate language-based actions that produce desired and predictable changes in the game world.</p><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob oc"><img src="../Images/719f0bb5fb40ca659de1eea76ddf22ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eedSThlYxuQwhvQ5rmpWLA.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 1. Illustration of an Interactive Fiction (IF) game, where an agent must perform the task of picking a fruit (e.g., an apple) then placing it in a blue box in the kitchen.</figcaption></figure><h1 id="c906" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Background and Motivation:</h1><p id="4e28" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Prior work has shown that Reinforcement Learning- and Language Model-based agents struggle to reason about or to explain science concepts in IF environments [1], which raises questions about these models’ ability to generalize to unseen situations beyond what has been observed during training [2]. For example, while tasks such as ‘<em class="ot">retrieving a known substance’s melting (or boiling) point</em>’ may be relatively simple, ‘<em class="ot">determining an unknown substance’s melting (or boiling) point in a specific environment</em>’ can be challenging for these models. To improve generalization, it may be effective to incorporate world knowledge, e.g.,<strong class="ng fr"> about object affordances</strong>, yet no prior work has investigated this direction. In addition, existing models struggle to learn effectively from environmental feedback. For instance, when examining the conductivity of a specific substance, the agent must understand that it has already obtained the necessary wires and the particular substance so that it then proceeds to locate a power source. Therefore, there is a need for a framework that can analyze and evaluate the effectiveness of different types of knowledge and knowledge-injection methods for text-based game agents.</p><p id="10b3" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">Our paper, “<strong class="ng fr">Knowledge-enhanced Agents for Interactive Text Games</strong>,” introduces a novel framework to enhance AI agents’ performance in these IF environments.</p><p id="e582" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr">Published Version:</strong> <a class="af oz" href="https://dl.acm.org/doi/10.1145/3587259.3627561" rel="noopener ugc nofollow" target="_blank">https://dl.acm.org/doi/10.1145/3587259.3627561</a></p><blockquote class="pa pb pc"><p id="79cb" class="ne nf ot ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">We are proud to announce that our paper has been awarded the <strong class="ng fr">Best Student Paper</strong> at the KCAP 2023 Conference, a testament to our team’s innovative research and dedication. 🏆🏆🏆</p></blockquote><h1 id="4617" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">The Core Innovation — Knowledge Injection Framework:</h1><p id="8464" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Our work introduces a unique framework to augment AI agents with specific knowledge. The framework comprises two key components:</p><ol class=""><li id="fc98" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pd pe pf bk"><strong class="ng fr">Memory of Correct Actions (MCA):</strong> This feature enables AI agents to remember and leverage past correct actions. The agent can formulate more effective strategies and avoid repetitive mistakes by maintaining a memory of what has worked before. MCA is determined by the environment feedback. If an action yields a reward, then it is considered correct. Therefore correct actions cannot be fed to the agent initially, but are instead stored in memory as the agent progresses through the (train/test time) episode.</li><li id="6267" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">Affordance Knowledge (Aff): </strong>Understanding the potential interactions with objects in the game world is crucial. We expect that affordances can help models learn better by listing the possible interactions with the objects around them. Unlike historical knowledge, the environment does not provide the affordances, and they need to be retrieved from external sources. For this purpose, we use ConceptNet and obtain its <em class="ot">capableOf</em> and <em class="ot">usedFor</em> relations for the objects in a given IF game episode.</li></ol><p id="4b21" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">We implemented this framework in two AI agent architectures:</p><ol class=""><li id="46f9" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pd pe pf bk">Online Policy Optimization through Rewards (RL Methods)</li><li id="c7c9" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk">Single-step Offline Prediction (LM Methods)</li></ol><h1 id="d123" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk"><strong class="al">1. Online Policy Optimization through Rewards (RL Methods)</strong></h1><p id="f625" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk"><strong class="ng fr">Pure RL-based Model — <em class="ot">DRRN </em>[3]<em class="ot"> </em>(Fig. 2)</strong></p><p id="9820" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">The baseline DRRN model uses only the inputs of observation, inventory, and task description to compute Q-values for each action. To enhance the DRRN baseline, we have injected external knowledge into the model and created three new variations of DRRN:</p><ol class=""><li id="8acf" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pd pe pf bk"><strong class="ng fr">aff:</strong> Using a distinct GRU encoding layer, we introduce the affordances of the objects presented in the inputs to the baseline model.</li><li id="a058" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">mca:</strong> A separate GRU encoding layer is utilized in this model to pass all previously correct actions to the baseline model.</li><li id="ac21" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">aff ⊕ mca:</strong> The encoding of this architecture is comprised of both the agent’s previous correct actions and the affordance as distinct components.</li></ol><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pl"><img src="../Images/53858d7b29b4a7115bd197507563c2d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tLYyCJobBZo9M4aYN1qF9Q.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 2: DRRN architecture, enhanced with the memory of previous correct actions and object affordances.</figcaption></figure><p id="787f" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr">RL-enhanced KG Model — <em class="ot">KG-A2C </em>[4] (Fig. 3)</strong></p><p id="4cdb" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">As baseline, we use a modified version of KG-A2C, where we utilize a single golden action sequence provided by the environment as the target, even though there may exist multiple possible golden sequences. We found this target to perform better than the original target of predicting a valid action. We devise the following knowledge-injection strategies to incorporate<br/>memory of correct actions and affordance knowledge for KG-A2C:</p><ol class=""><li id="55b4" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pd pe pf bk"><strong class="ng fr">mca: </strong>On top of the baseline, we incorporate all previously correct<br/>actions by using a separate GRU encoding layer and concatenate the<br/>output vector along with other output representations.</li><li id="8bd7" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">aff:</strong> The KG component in the KG-A2C model provides us with a convenient way to add more knowledge. In particular, we directly add the affordance knowledge into the KG as additional triples on top of the<br/>baseline model. For example, given the existing relation in the KG<br/>(living room, hasA, apple) we can add the affordance relation: (apple,<br/>usedFor, eating). In this way, the KG encoding network can produce<br/>a more meaningful representation of the game state and potentially<br/>guide the model to produce better actions. In our experiments, we<br/>compare this approach to adding affordance knowledge using a<br/>separate GRU encoding layer, similar to the DRRN case.</li><li id="55da" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">aff ⊕ mca:</strong> We include both affordances in the KG and the memory of all<br/>previous correction actions with a separate GRU encoding layer.</li></ol><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pm"><img src="../Images/422c7e31d5898a8c9fab79747319656e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*if8wCXlUb0wrYoeo7VgsgA.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 3: KG-A2C model architecture with integrated affordances and previous correct actions.</figcaption></figure><h1 id="235b" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk"><strong class="al">2. Single-step Offline Prediction (LM Methods)</strong></h1><p id="845b" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk"><strong class="ng fr">Pre-trained LM — <em class="ot">RoBERTa </em>[5]<em class="ot"> </em>(Fig. 4)</strong></p><p id="e01f" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">Here we view the task as multiple-choice QA. At each step, the current game state is treated as the question and must predict the next action from a set of candidates. Similar to RL agents, the model is given the environment observation (𝑜𝑏𝑣), inventory (𝑖𝑛𝑣), and task description (𝑑𝑒𝑠𝑐) at every step. Then we concatenate it with each action and let the LM select the action with the highest score. Given the large set of possible actions, we only randomly select 𝑛=4 distractor actions during training to reduce the computational burden, the LM is trained with cross-entropy loss to select the correct action. At inference time, the model assigns scores for all valid actions, and we use top-p sampling for action selection to prevent it from being stuck in an action loop. We formalize three knowledge-injection strategies for the baseline RoBERTa model.</p><ol class=""><li id="40c4" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pd pe pf bk"><strong class="ng fr">mca:</strong> Here, we enable the LM to be aware of its past correct actions by incorporating an MCA that lists them as a string, appended to the original input. Due to token limitations of RoBERTa, we use a sliding window with size 𝐴=5, i.e., at each step, the model sees at most the past<br/>𝐴 correct actions.</li><li id="2ec1" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">aff:</strong> We inject affordance knowledge into the LM by first adapting it on a subset of the Commonsense Knowledge Graph containing object utilities. We adapt the model via an auxiliary QA task following prior knowledge injection work [6]. We use pretraining instead of simple concatenation for input due to the substantial volume of affordance knowledge triples, which cannot be simply concatenated to the input of RoBERTa due to limited input length. Pre-training on affordances through an auxiliary QA task alleviates this challenge, while still enabling the model to learn the relevant knowledge. We then finetune our task model on top of the utility-enhanced model, as described in the baseline.</li><li id="981e" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">aff ⊕ mca:</strong> This variation simply combines mca and aff.</li></ol><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pn"><img src="../Images/0e09e421f92a4a224d8201706e82d999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K9NHHLmhBs2TZWVgddW_Wg.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 4: RoBERTa architecture trained using distractors.</figcaption></figure><p id="291d" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr">Instruction-tuned LM — <em class="ot">Flan T5 </em>[7][8] (Fig. 5)</strong></p><p id="6eb7" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">The Swift model inherently integrates the historical context of the preceding ten actions. Notably, in contrast to the three previously examined models that exclusively consider the history of the last ten correct actions, the Swift model adheres to its original design by encompassing the entire history of the ten previous actions. To establish a comparable baseline model to the methodology applied in the preceding three architectures, we omit the action history from the Swift model. The unaltered variation of Swift is herein denoted as the <strong class="ng fr">mca</strong> version. Additionally, incorporation of affordance into the baseline model yields the <strong class="ng fr">aff model</strong>. Similarly, integration of affordances within the mca version led to the formation of the <strong class="ng fr">aff ⊕ mca</strong> model. These affordances are introduced into the primary input sequence immediately following the inventory data and preceding information about visited rooms.</p><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob po"><img src="../Images/e50387af9e741dd9de9c7b95f6da8238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fnRfIeSPC91H1U-FfIDSNg.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 5: Swift architecture trained in a Seq2Seq manner.</figcaption></figure><h1 id="8664" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Experiment Setup</h1><p id="f266" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk"><strong class="ng fr">Environment:</strong> We have used ScienceWorld [1], a complex text-based virtual world presented in English. It features 10 interconnected locations and houses 218 unique objects, including various items from instruments and electrical components to plants, animals, and everyday objects like furniture and books. The game offers a rich array of interactions, with 25 high-level actions and up to 200,000 possible combinations per step, though only a few are practically valid. ScienceWorld has 10 tasks with a total set of 30 sub-tasks. Due to the diversity within ScienceWorld, each task functions as an individual benchmark with distinct reasoning abilities, knowledge requirements, and varying numbers of actions needed to achieve the goal state. Moreover, each sub-task has a set of mandatory objectives that need to be met by any agent (such as focusing on a non-living object and putting it in a red box in the kitchen). For experimentation purposes, we selected a single representative sub-task from each of the 10 tasks. Task details are mentioned in Appendix (at the end of this article).</p><p id="5642" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr">Rewards and Scoring System:</strong> The reward system in ScienceWorld is designed to guide the agent towards preferred solutions. The environment provides a numeric score and a boolean indicator of task completion for every action performed. An agent can take up to 100 steps (actions) in each episode. The final score, ranging between 0 and 100, reflects how well the agent achieves the episode goal and its sub-goals. An episode concludes, and the cumulative score is calculated when the agent completes the task or reaches the 100-step limit.</p><h1 id="f503" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Experimental Insights:</h1><ul class=""><li id="8146" class="ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz pp pe pf bk"><strong class="ng fr">Knowledge injection helps</strong> agents in text-based games — In 34 out of 40 cases, our knowledge injection strategies improve over the baseline models.</li><li id="2492" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pp pe pf bk"><strong class="ng fr">Affordance knowledge is more beneficial</strong> than the memory of correct actions — Affordance models obtain the best results in 15 cases, followed by including MCA (8 cases). Including both knowledge types together led to the best results in 11 cases</li><li id="156e" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pp pe pf bk">In terms of the overall impact across tasks, the LM variants, RoBERTa and Swift, benefit the most on average from including affordances, leading to a relative increase of 48% and 8% respectively, over the baselines. An example is illustrated in Fig. 6, where LM models are greatly benefitted from affordance addition.</li></ul><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pq"><img src="../Images/2754c5fc9218edca5e22c695d8c1c3df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vn3IgnBXS3eTZE1RzuWlkw.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 6: Actions taken by affordance models on Task 4. Blue = step index, green = cumulative score, and yellow = correct action.</figcaption></figure><ul class=""><li id="d0d6" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pp pe pf bk"><strong class="ng fr">Variable effect across tasks</strong> depends on the task relevance of the injected knowledge — The variable effect across tasks was frequently due to the relevance of the injected knowledge to the task at hand, with certain tasks (e.g., electricity) benefiting more from the injection.</li><li id="2826" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pp pe pf bk"><strong class="ng fr">Injecting affordances is most effective via KGs</strong>; incorporating them as raw inputs increased the learning complexity for the models — We explore multiple variations of injecting affordance knowledge into KG-A2C (Fig. 7): by adding it as input into the observation, inventory, and description, creating a separate GRU encoding layer for affordance, and adding affordance to the KG itself. We evaluate the performance of each method on three sub-tasks: easy, medium, and hard.</li></ul><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pr"><img src="../Images/17d48932521a83afbb150d9176bf0b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F_qiQNiOhk4oaCfSIUvL6g.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 7: Effect of five ways to add affordances in KG-A2C.</figcaption></figure><h1 id="574c" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Concluding Thoughts:</h1><p id="8cdf" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Our research represents a significant stride toward more sophisticated AI agents. By equipping them with the ability to learn from past actions and understand their environment deeply, we pave the way for AI that plays games and interacts intelligently and intuitively in various aspects of our lives. The framework can be extended to other AI applications, such as virtual assistants or educational tools, where understanding and interacting with the environment is crucial.</p><p id="e4fe" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">Few-shot prompting of large LMs has recently shown promise on reasoning tasks, as well as clear benefits from interactive communication and input clarification. Exploring their role in interactive tasks, either as solutions that require less training data or as components that can generate synthetic data for knowledge distillation to smaller models, is a promising future direction.</p><p id="d1f9" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">If you like our work, please cite it 😁</p><pre class="od oe of og oh ps pt pu bp pv bb bk"><span id="52c7" class="pw mj fq pt b bg px py l pz qa">@inproceedings{chhikara,<br/>author = {Chhikara, Prateek and Zhang, Jiarui and Ilievski, Filip and Francis, Jonathan and Ma, Kaixin},<br/>title = {Knowledge-Enhanced Agents for Interactive Text Games},<br/>year = {2023},<br/>doi = {10.1145/3587259.3627561},<br/>booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},<br/>pages = {157–165},<br/>numpages = {9},<br/>series = {K-CAP '23}<br/>}</span></pre><h1 id="2d86" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">References</h1><p id="16e2" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">[1] Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. 2022. ScienceWorld: Is your Agent Smarter than a 5th Grader? EMNLP (2022).</p><p id="ae02" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">[2] Peter Jansen, Kelly J. Smith, Dan Moreno, and Huitzilin Ortiz. 2021. On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings. In Proceedings of EMNLP.</p><p id="0573" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">[3] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. 2016. Deep Reinforcement Learning with a Natural Language Action Space. In Proceedings of ACL.</p><p id="996d" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">[4] Prithviraj Ammanabrolu and Matthew Hausknecht. 2020. Graph Constrained Reinforcement Learning for Natural Language Action Spaces. In ICLR.</p><p id="d605" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">[5] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. (2019).</p><p id="3d1f" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">[6] Filip Ilievski, Alessandro Oltramari, Kaixin Ma, Bin Zhang, Deborah L McGuinness, and Pedro Szekely. 2021. Dimensions of commonsense knowledge. Knowledge-Based Systems 229 (2021), 107347.</p><p id="1fea" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">[7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al . 2022. Scaling instruction-finetuned language models.</p><p id="0e51" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">[8] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2023. SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks.</p><p id="14a3" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">[9] Noam Chomsky 2014. Aspects of Theory of Syntax. Vol. 11. MIT press.</p><p id="7a7b" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">[10] Yifan Jiang, Filip Ilievski and Kaixin Ma. 2023. Transferring Procedural Knowledge across Commonsense Tasks. In ECAI</p><h1 id="bb19" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">APPENDIX</h1><p id="0ee3" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk"><strong class="ng fr">Task Descriptions</strong></p><ol class=""><li id="981b" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pd pe pf bk"><strong class="ng fr">Task 1 — Matter:</strong> Your task is to freeze water. First, focus on the substance. Then, take actions that will cause it to change its state of<br/>matter.</li><li id="d889" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">Task 2 — Measurement:</strong> Your task is to measure the melting point of chocolate, which is located around the kitchen. First, focus on the thermometer. Next, focus on the chocolate. If the melting point of chocolate is above -10.0 degrees, focus on the blue box. If the melting point of chocolate is below -10.0 degrees, focus on the orange box. The boxes are located around the kitchen.</li><li id="9b21" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">Task 3 — Electricity</strong>: Your task is to turn on the red light bulb by powering it using a renewable power source. First, focus on the red light bulb. Then, create an electrical circuit that powers it on.</li><li id="a10e" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">Task 4 — Classification:</strong> Your task is to find a(n) non-living thing. First, focus on the thing. Then, move it to the red box in the kitchen.</li><li id="5f53" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">Task 5 — Biology I:</strong> Your task is to grow a apple plant from seed. Seeds can be found in the kitchen. First, focus on a seed. Then, make changes to the environment that grow the plant until it reaches the reproduction life stage.</li><li id="32fa" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">Task 6 — Chemistry:</strong> Your task is to use chemistry to create the substance <em class="ot">‘salt water’</em>. A recipe and some of the ingredients might be found near the kitchen. When you are done, focus on the salt water.</li><li id="5e4b" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">Task 7 — Biology II:</strong> Your task is to find the animal with the longest life span, then the shortest life span. First, focus on the animal with the longest life span. Then, focus on the animal with the shortest life span. The animals are in the ’outside’ location.</li><li id="5e19" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">Task 8 — Biology III:</strong> Your task is to focus on the 4 life stages of the turtle, starting from earliest to latest.</li><li id="dcc6" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">Task 9 — Forces:</strong> Your task is to determine which of the two inclined planes (unknown material C, unknown material H) has the most<br/>friction. After completing your experiment, focus on the inclined plane with the most friction.</li><li id="68b9" class="ne nf fq ng b go pg ni nj gr ph nl nm nn pi np nq nr pj nt nu nv pk nx ny nz pd pe pf bk"><strong class="ng fr">Task 10 — Biology IV:</strong> Your task is to determine whether blue seed color is a dominant or recessive trait in the unknown E plant. If the trait is dominant, focus on the red box. If the trait is recessive, focus on the green box.</li></ol><p id="f9cc" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr">ScienceWorld Gameplay Example</strong></p><p id="014c" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr">Task:</strong> 4 (find a non-living thing)<br/><strong class="ng fr">Variation:</strong> 239 (DRRN baseline)<br/><strong class="ng fr">Description:</strong> Your task is to find a(n) non-living thing. First, focus on the thing. Then, move it to the purple box in the workshop.</p></div></div><div class="oi"><div class="ab cb"><div class="ll qb lm qc ln qd cf qe cg qf ci bh"><figure class="od oe of og oh oi qh qi paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob qg"><img src="../Images/b5bcbd114d4b75cd171d0e3e417e0034.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*XYpKCgV2o7RjNf_kcZ2luA.png"/></div></div></figure></div></div></div></div></div>    
</body>
</html>