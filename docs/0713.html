<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Mamba: SSM, Theory, and Implementation in Keras and TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Mamba: SSM, Theory, and Implementation in Keras and TensorFlow</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mamba-ssm-theory-and-implementation-in-keras-and-tensorflow-32d6d4b32546?source=collection_archive---------0-----------------------#2024-03-17">https://towardsdatascience.com/mamba-ssm-theory-and-implementation-in-keras-and-tensorflow-32d6d4b32546?source=collection_archive---------0-----------------------#2024-03-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="39e2" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Understanding how SSMs and Mamba work, along with how to get started with implementing it in Keras and TensorFlow.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@vedantjumle?source=post_page---byline--32d6d4b32546--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Vedant Jumle" class="l ep by dd de cx" src="../Images/363dbdf8564c35060b3e57cbc6e55f16.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*rcsCpcphHo2t2q_04QgFfw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--32d6d4b32546--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@vedantjumle?source=post_page---byline--32d6d4b32546--------------------------------" rel="noopener follow">Vedant Jumle</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--32d6d4b32546--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ml"><img src="../Images/2eef43d26c512f0bea0c67ca675348ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*5rdpHkscLe_gIe5S"/></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Source: AI Generate (SDXL)</figcaption></figure><p id="f27e" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Submitted on 1st December, 2023 on arXiv, the paper titled <a class="af nu" href="https://arxiv.org/abs/2312.00752" rel="noopener ugc nofollow" target="_blank">“Mamba: Linear-Time Sequence Modeling with Selective State Spaces”</a> proposed an interesting approach to sequence modeling. The authors — <a class="af nu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gu%2C+A" rel="noopener ugc nofollow" target="_blank">Albert Gu</a>, <a class="af nu" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dao%2C+T" rel="noopener ugc nofollow" target="_blank">Tri Dao</a> — introduced, ‘Mamba’ that utilized ‘selective’ <a class="af nu" href="https://en.wikipedia.org/wiki/State-space_representation" rel="noopener ugc nofollow" target="_blank">state space models (SSM)</a> to achieve results that compete with the performance of the, now ubiquitous, Transformer model.</p><h1 id="140d" class="nv nw fq bf nx ny nz gq oa ob oc gt od oe of og oh oi oj ok ol om on oo op oq bk">What’s so unique about Mamba?</h1><p id="4508" class="pw-post-body-paragraph my mz fq na b go or nc nd gr os nf ng nh ot nj nk nl ou nn no np ov nr ns nt fj bk">Transformers have seen recent popularity with the rise of Large Language Models (LLMs) like LLaMa-2, GPT-4, Claude, Gemini, etc., but it suffers from the problem of context window. The issue with transformers lies in it’s core, the multi head-attention mechanism.</p><p id="933d" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><em class="ow">The main issue with multi-head attention sprouts from the fact that for input sequence length n, the time complexity and space complexity scales by O(n²). This limits the length of the context window of an LLM. Because, to increase it by 10x, we need to scale the hardware requirement (most notably GPU VRAM) by 100x.</em></p><p id="1673" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Mamba, on the other hand, scales by <strong class="na fr"><em class="ow">O(n)!, i.e., Linearly</em></strong><em class="ow">.</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="mj mk ox"><img src="../Images/b097aae23543115f103cea34510def5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bNhLTgLlIRtbVNFCXBQmbA.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Plot taken from the Mamba paper comparing FlashAttention and Mamba approach (indicated by scan(ours) in the legends)[1]</figcaption></figure><p id="55e6" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">This linear scaling is what has taken wind for researchers to speculate that Mamba might be the future of sequence modeling.</p><h1 id="fd6b" class="nv nw fq bf nx ny nz gq oa ob oc gt od oe of og oh oi oj ok ol om on oo op oq bk">The backbone of Mamba: State Space Models</h1><p id="ad41" class="pw-post-body-paragraph my mz fq na b go or nc nd gr os nf ng nh ot nj nk nl ou nn no np ov nr ns nt fj bk">The core of the Mamba model comes from the concept of State Space Models. <strong class="na fr"><em class="ow">State Space Models, like Transformers and RNN, process sequences of information, like text, audio signals, video frames, DNA sequences, etc.</em></strong></p><p id="8403" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">State Space Models come from an idea of describing a physical system as a set of input, outputs, and variables. These variables are:<em class="ow"> A, B, C, D. </em>The process of SSM involves calculation of an <em class="ow">internal state vector h(t), given an input x(t). </em>Then, we do a weighted sum of <em class="ow">h(t)</em> and <em class="ow">x(t) </em>where the weights are <em class="ow">A, B, C, &amp; D</em>. In the simplest form (continuous time-invariant), the process formulation looks like:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pc"><img src="../Images/ebbbac8cb1eb8abaa8dbb62853c93240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*YqWwy7zhSMEboniBOELwEg.png"/></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="mj mk pd"><img src="../Images/047a1d371405fee92bb7ea7c58305c58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fSA4B4DZB5oevIXXkZHxMQ.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">source: wikipedia[6]</figcaption></figure><p id="4d1b" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><em class="ow">h(t) </em>is often called the ‘hidden’ or the ‘latent’ state, I will be sticking to calling it the ‘hidden’ state for better clarity. <strong class="na fr">It is important to note that A, B, C, and D are learnt parameters in SSM.</strong></p><h2 id="629e" class="pe nw fq bf nx pf pg ph oa pi pj pk od nh pl pm pn nl po pp pq np pr ps pt pu bk">What are the variables?</h2><p id="cba9" class="pw-post-body-paragraph my mz fq na b go or nc nd gr os nf ng nh ot nj nk nl ou nn no np ov nr ns nt fj bk"><strong class="na fr">The variables, A, B, C &amp; D, are learnt parameters,</strong> and they can be described as:</p><ul class=""><li id="5e3f" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt pv pw px bk">A: How much should the previous hidden state (h) be considered to calculate the new hidden state</li><li id="fd69" class="my mz fq na b go py nc nd gr pz nf ng nh qa nj nk nl qb nn no np qc nr ns nt pv pw px bk">B: How much should the input (x) be consider to calculate the new hidden state.</li><li id="3a29" class="my mz fq na b go py nc nd gr pz nf ng nh qa nj nk nl qb nn no np qc nr ns nt pv pw px bk">C: How much should the new hidden state be considered in calculating the output (y).</li><li id="fa77" class="my mz fq na b go py nc nd gr pz nf ng nh qa nj nk nl qb nn no np qc nr ns nt pv pw px bk">D: How much should the input (x) be consider in calculating the output (y).</li></ul><p id="2a43" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">D comes in the end of the computations and does not affect how the hidden state is calculated. Hence, it is usually considered outside of ssm, and can be thought of as a skip connection.</p><h2 id="7f68" class="pe nw fq bf nx pf pg ph oa pi pj pk od nh pl pm pn nl po pp pq np pr ps pt pu bk">Going from continuous spaces to discrete spaces</h2><p id="726a" class="pw-post-body-paragraph my mz fq na b go or nc nd gr os nf ng nh ot nj nk nl ou nn no np ov nr ns nt fj bk">The above formulation applies to a system where the input and output belong to a continuous space. But in cases, like language modeling, where the input and output belong to discrete spaces (token values in a vocabulary). Also, finding <em class="ow">h(t)</em> is analytically challenging. This can be achieved by performing a <strong class="na fr"><em class="ow">Zero-order hold</em></strong>.</p><p id="cf5d" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">In a zero-order hold, every time an input is received, the model holds its value till the next input is received. This leads to a continuous input space.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="mj mk qd"><img src="../Images/236f0dcd1ee1294b2e8064560a582f29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oeti2iSaU5G5rRqaIYEVrQ.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">How Zero order hold works</figcaption></figure><p id="4bf4" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">This length of ‘hold’ is determined by a new parameter called, <em class="ow">step size </em><strong class="na fr"><em class="ow">∆. It can be thought of as the resolution of the input. </em></strong>Ideally, ∆ should be infinitesimal.</p><p id="72e0" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Mathematically, Zero-order hold can be described as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qe"><img src="../Images/3f6f0258965a22d4970d1e76a3cf457a.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*A3VkWJI_aTyR8jM6cLbQjQ.png"/></div></figure><p id="f5cf" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Finally, we can create a discrete SSM, as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qf"><img src="../Images/dc4b655375a46584a07aed40155d829d.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*wZWyR1GNkgpIf3O1hd4NyA.png"/></div></figure><p id="c7fa" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Since, D is used with a skip connection outside of SSM, the output can be reduced to:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qg"><img src="../Images/de141e5ed6870e82fe9f71fe77beb074.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/1*dJgTDR--v3O7uE3-CKpmWw.png"/></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="mj mk qh"><img src="../Images/71c4d16a62a57ecd5732def8f4b79143.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5mI9ki4uALSefiOaF43F0g.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Involvement of DX(t) is considered as a skip connection, hence is goes from outside of SSM</figcaption></figure><h1 id="c693" class="nv nw fq bf nx ny nz gq oa ob oc gt od oe of og oh oi oj ok ol om on oo op oq bk">SSM and recurrence</h1><p id="c543" class="pw-post-body-paragraph my mz fq na b go or nc nd gr os nf ng nh ot nj nk nl ou nn no np ov nr ns nt fj bk">In SSMs, the hidden state is carried over to when the next input is received. This is similar to how Recurrent Neural Networks function.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qi"><img src="../Images/96f22333a7a3f5bd2ba29b117eef421f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*zF4z8mGCNXN2UGnVJwYa8A.png"/></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Comparison of RNN and SSM</figcaption></figure><p id="1f9d" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">This recurrent format of SSM can be unwrapped, just like RNNs. But unlike RNNs, which are iterative and slow, SSM can process the input sequence in parallel (just like transformers) and this makes the training processes faster.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="mj mk qj"><img src="../Images/22282dee4daeabed27c4669ba7ea486f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jkbPpBfPjnsNry6zrPj1Kw.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Unrolled form of SSM</figcaption></figure><blockquote class="qk ql qm"><p id="5832" class="my mz ow na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Note that ‘D’ is used in a skip connection, which is outside of SSM.</p></blockquote><p id="aea1" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The key insight in how SSM make training fast is to use the variables <em class="ow">A, B, C </em>in a pre-computed convolutional kernel. <a class="af nu" href="https://maartengrootendorst.substack.com/i/141228095/the-convolution-representation" rel="noopener ugc nofollow" target="_blank">Maarten Grootendorst</a> wrote a really good explanation on how this canonical ‘convolutional’ kernel is constructed. But here’s a simple mathematical explanation.</p><p id="c132" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Consider the output <em class="ow">y. </em>For a sequence length of <em class="ow">k</em>, the output for <em class="ow">y(k)</em> will be represented <strong class="na fr"><em class="ow">(assuming h0 = zero)</em></strong>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qn"><img src="../Images/16c7dd559c5978d453467fe139274186.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*yWnFnQgDDXkqy18u5mrFsQ.png"/></div></figure><p id="1836" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Similarly, y3 can be represented as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qo"><img src="../Images/b90cee1adec4657a3ffa7f67760b3abb.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*9RLGnIxOXPuuCtU2rSfj5w.png"/></div></figure><p id="7af8" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Extrapolating the pattern, yk can be represented as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="mj mk qp"><img src="../Images/9f7775b6a3bce55150dd393d1d9e44da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*__GzaCiyE2cbdOld5Kug5A.png"/></div></div></figure><p id="f1de" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">This formulation can be further reduced to:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qq"><img src="../Images/9d85bda2ef8d6da968ed73b8e48ace03.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*xHDgEXARTerk4rwcaE-rIw.png"/></div></figure><p id="a933" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The funny looking multiplication symbol represents a convolution operation, where the convolution kernel is K. Notice that K is not dependent on <em class="ow">x, </em>hence K can be pre-computed into a convolutional kernel, which makes the process faster.</p><h1 id="51c0" class="nv nw fq bf nx ny nz gq oa ob oc gt od oe of og oh oi oj ok ol om on oo op oq bk">Mamba and ‘Selective’ SSM</h1><p id="179a" class="pw-post-body-paragraph my mz fq na b go or nc nd gr os nf ng nh ot nj nk nl ou nn no np ov nr ns nt fj bk">As good as the computational capacity of SSM sounds, it turns out to be pretty <em class="ow">meh </em>in metrics like accuracy compared to Transformers.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="mj mk qr"><img src="../Images/0e7755efdbec8214342ce450c6900185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tVQAcULn7Ro8ycJtLdaGlg.png"/></div></div></figure><p id="e5b9" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The core issue lies with the variables, ∆, A, B, &amp; C. Turns out that since we apply the same matrices to every input, they cannot really process the context of the sequence.</p><blockquote class="qk ql qm"><p id="2845" class="my mz ow na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">SSMs are inflexible in the way they process data[4]</p></blockquote><p id="0082" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">So what’s so special about Mamba? In mamba, we use a process called ‘selective’ SSM, where the variables, ∆, B, &amp; C, are computed based on the input. 🤔. We do this by passing the current input through Linear layers, and take the output to be the ∆, B, &amp; C.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qs"><img src="../Images/d789181938ab7269cbd528b6982e3263.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*ORaq0m_xC7PcAasqN8Uvww.png"/></div></figure><p id="ee67" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">But then this makes ∆, B, &amp; C input dependent, hence meaning that they cannot be pre-computed 😢, fast convolution isn’t going to work here. But, the authors discuss a method, which is based on <em class="ow">parallel associative scan.</em></p><h2 id="c704" class="pe nw fq bf nx pf pg ph oa pi pj pk od nh pl pm pn nl po pp pq np pr ps pt pu bk">Parallel Associative Scan</h2><p id="79db" class="pw-post-body-paragraph my mz fq na b go or nc nd gr os nf ng nh ot nj nk nl ou nn no np ov nr ns nt fj bk">Parallel associative scan is a powerful technique used in parallel computing to perform a prefix sum operation, which is a cumulative operation on a sequence of numbers. This operation is “associative”, meaning the way numbers are grouped in the operation doesn’t change the result.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="mj mk qt"><img src="../Images/dbbd700bf44fb468c40690bafbc4c22c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0-Ij19HxOJQRsgyeXGvItA.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Parallel prefix sum is an example of associative scanning. (source: Nvidia)[7]</figcaption></figure><p id="d54d" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">In the context of the Mamba model, by defining an associative operator, elements and associative operators for a parallel associative scan operation are obtained. This allows for solving problems on the whole time interval in parallel, resulting in logarithmic time complexity in the number of sub-intervals.</p><h2 id="c0d8" class="pe nw fq bf nx pf pg ph oa pi pj pk od nh pl pm pn nl po pp pq np pr ps pt pu bk">Hardware aware algorithm</h2><p id="5925" class="pw-post-body-paragraph my mz fq na b go or nc nd gr os nf ng nh ot nj nk nl ou nn no np ov nr ns nt fj bk">Along with associative scan, the authors also propose a hardware aware algorithm, where they use the quirks within Nvidia GPUs related to the speed of HBM and SRAM. They argue that the computation of SSM states can be sped up by:</p><ul class=""><li id="ac77" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt pv pw px bk">keeping the hidden state and A in the faster but less capacity <strong class="na fr"><em class="ow">SRAM</em>,</strong></li><li id="3bc5" class="my mz fq na b go py nc nd gr pz nf ng nh qa nj nk nl qb nn no np qc nr ns nt pv pw px bk">while computing ∆, B, &amp; C, in the slower but larger capacity <strong class="na fr"><em class="ow">HBM</em></strong>.</li><li id="f6b9" class="my mz fq na b go py nc nd gr pz nf ng nh qa nj nk nl qb nn no np qc nr ns nt pv pw px bk">They then transfer ∆, B, &amp; C to the <strong class="na fr"><em class="ow">SRAM</em></strong>, compute the new hidden state within <strong class="na fr"><em class="ow">SRAM</em></strong>.</li><li id="a0ba" class="my mz fq na b go py nc nd gr pz nf ng nh qa nj nk nl qb nn no np qc nr ns nt pv pw px bk">And then write ∆, B &amp; C back to <strong class="na fr"><em class="ow">HBM</em></strong>.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="mj mk qu"><img src="../Images/daa8208bf094ec66ab66110f20dd51a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h6f5unmepasu1siKbNTXPA.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Illustration taken from the Mamba paper, it shows how the hardware aware algorithm works[1]</figcaption></figure><blockquote class="qk ql qm"><p id="9d8e" class="my mz ow na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">In the implementation section, I will not be discussing on how to work with the hardware aware algorithm, rather I will be only using parallel associative scan.</p></blockquote><h1 id="2424" class="nv nw fq bf nx ny nz gq oa ob oc gt od oe of og oh oi oj ok ol om on oo op oq bk">Final Mamba architecture</h1><p id="451b" class="pw-post-body-paragraph my mz fq na b go or nc nd gr os nf ng nh ot nj nk nl ou nn no np ov nr ns nt fj bk">With all of this in mind, let’s explore and implement the Mamba architecture using Keras and TensorFlow.</p><p id="204c" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The Mamba architecture, after reading the paper and analysis of the code, can be broken into a few key components which are connected as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="mj mk qv"><img src="../Images/e663ed4dbf34650f50802a442192d16d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e223lH-rHk7JsL7uLqLALw.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Breakdown of a mamba block</figcaption></figure><p id="7610" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The Mamba architecture consists of multiple stacked layers of ‘Mamba blocks’. Which, judging from the above illustration, consists of quite a few components. Another important thing to note is that the authors add the output from Selective SSM to the original input and then apply a <em class="ow">normalization </em>layer to it. This normalization can be either a Layer normalization or an <a class="af nu" href="https://arxiv.org/abs/1910.07467" rel="noopener ugc nofollow" target="_blank">RMS normalization</a>.</p><h1 id="c31c" class="nv nw fq bf nx ny nz gq oa ob oc gt od oe of og oh oi oj ok ol om on oo op oq bk">TensorFlow and Keras implementation</h1><p id="15f1" class="pw-post-body-paragraph my mz fq na b go or nc nd gr os nf ng nh ot nj nk nl ou nn no np ov nr ns nt fj bk">Lets start with coding part of Mamba. We will using the following dependencies:</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="3b6e" class="ra nw fq qx b bg rb rc l rd re">tensorflow[and-cuda]==2.15.0.post1 # if you want to use GPU or<br/>tensorflow==2.15.0.post1 # if you want to only use CPU<br/>transformers==4.36.2 # for using the bert tokenizer<br/>einops==0.7.0 # useful to make matrix manipulation faster<br/>datasets==2.16.1 # to load datasets<br/># all other modules (like numpy) will be auto installed</span></pre><p id="d413" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Imports:</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="8b46" class="ra nw fq qx b bg rb rc l rd re">import tensorflow_datasets as tfds<br/>import tensorflow as tf<br/><br/>from tensorflow import keras<br/>from tensorflow.keras import layers, Model<br/><br/>from dataclasses import dataclass<br/>from einops import rearrange, repeat<br/>from typing import Union<br/><br/>from transformers import AutoTokenizer<br/><br/>import datasets<br/>import math<br/>import numpy as np</span></pre><p id="70f9" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">To make the modeling argument processing easier, let’s create a simple <em class="ow">ModelArgs </em>dataclass as a config class. This allows us to just pass the dataclass variable in the arguments when we are initializing the model.</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="69c9" class="ra nw fq qx b bg rb rc l rd re">@dataclass<br/>class ModelArgs:<br/>    model_input_dims: int = 64<br/>    model_states: int = 64<br/>    projection_expand_factor: int = 2<br/>    conv_kernel_size: int = 4<br/>    delta_t_min: float = 0.001<br/>    delta_t_max: float = 0.1<br/>    delta_t_scale: float = 0.1<br/>    delta_t_init_floor: float = 1e-4<br/>    conv_use_bias: bool = True<br/>    dense_use_bias: bool = False<br/>    layer_id: int = -1<br/>    seq_length: int = 128<br/>    num_layers: int = 5<br/>    dropout_rate: float = 0.2<br/>    use_lm_head: float = False<br/>    num_classes: int = None<br/>    vocab_size: int = None<br/>    final_activation = None<br/>    loss:Union[str, keras.losses.Loss] = None<br/>    optimizer: Union[str, keras.optimizers.Optimizer] = keras.optimizers.AdamW()<br/>    metrics = ['accuracy']<br/><br/>    def __post_init__(self):<br/>        self.model_internal_dim: int = int(self.projection_expand_factor * self.model_input_dims)<br/><br/>        self.delta_t_rank = math.ceil(self.model_input_dims/16)<br/>        if self.layer_id == -1:<br/>            self.layer_id = np.round(np.random.randint(0, 1000), 4)<br/><br/>        if self.vocab_size == None:<br/>            raise ValueError("vocab size cannot be none")<br/><br/>        if self.use_lm_head:<br/>            self.num_classes=self.vocab_size<br/>        else:<br/>            if self.num_classes == None:<br/>                raise ValueError(f'num classes cannot be {self.num_classes}')<br/><br/>            if self.num_classes == 1:<br/>                self.final_activation = 'sigmoid'<br/>            else:<br/>                self.final_activation = 'softmax'<br/><br/>        if self.loss == None:<br/>            raise ValueError(f"loss cannot be {self.loss}")</span></pre><p id="3cd3" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Load the <em class="ow">bert-base-uncased </em>tokenizer:</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="9b9a" class="ra nw fq qx b bg rb rc l rd re">tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")<br/>vocab_size = tokenizer.vocab_size</span></pre><p id="3a00" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Before we implement our Mamba and SSM classes, we need to implement the parallel associative scan, the code looks like this:</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="be34" class="ra nw fq qx b bg rb rc l rd re">def selective_scan(u, delta, A, B, C, D):<br/>    # first step of A_bar = exp(ΔA), i.e., ΔA<br/>    dA = tf.einsum('bld,dn-&gt;bldn', delta, A) <br/>    dB_u = tf.einsum('bld,bld,bln-&gt;bldn', delta, u, B)<br/>    <br/>    dA_cumsum = tf.pad(<br/>        dA[:, 1:], [[0, 0], [1, 1], [0, 0], [0, 0]])[:, 1:, :, :]<br/>    <br/>    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip along axis 1<br/>    <br/>    # Cumulative sum along all the input tokens, parallel prefix sum, <br/>    # calculates dA for all the input tokens parallely<br/>    dA_cumsum = tf.math.cumsum(dA_cumsum, axis=1)  <br/><br/>    # second step of A_bar = exp(ΔA), i.e., exp(ΔA)<br/>    dA_cumsum = tf.exp(dA_cumsum)  <br/>    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip back along axis 1<br/><br/>    x = dB_u * dA_cumsum<br/>    # 1e-12 to avoid division by 0<br/>    x = tf.math.cumsum(x, axis=1)/(dA_cumsum + 1e-12) <br/><br/>    y = tf.einsum('bldn,bln-&gt;bld', x, C)<br/>    <br/>    return y + u * D </span></pre><p id="e792" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">With this, we can implement the MambaBlock:</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="b024" class="ra nw fq qx b bg rb rc l rd re">class MambaBlock(layers.Layer):<br/>    def __init__(self, modelargs: ModelArgs, *args, **kwargs):<br/>        super().__init__(*args, **kwargs)<br/>        self.args = modelargs<br/>        args = modelargs<br/>        self.layer_id = modelargs.layer_id<br/><br/>        self.in_projection = layers.Dense(<br/>            args.model_internal_dim * 2, <br/>            input_shape=(args.model_input_dims,), use_bias=False)<br/><br/>        self.conv1d = layers.Conv1D(<br/>            filters=args.model_internal_dim,<br/>            use_bias=args.conv_use_bias,<br/>            kernel_size=args.conv_kernel_size,<br/>            groups=args.model_internal_dim,<br/>            data_format='channels_first',<br/>            padding='causal'<br/>        )<br/><br/>        # this layer takes in current token 'x' <br/>        # and outputs the input-specific Δ, B, C (according to S6)<br/>        self.x_projection = layers.Dense(args.delta_t_rank + args.model_states * 2, use_bias=False)<br/><br/>        # this layer projects Δ from delta_t_rank to the mamba internal <br/>        # dimension<br/>        self.delta_t_projection = layers.Dense(args.model_internal_dim, <br/>                                               input_shape=(args.delta_t_rank,), use_bias=True)<br/><br/>        self.A = repeat(<br/>                tf.range(1, args.model_states+1, dtype=tf.float32), <br/>                'n -&gt; d n', d=args.model_internal_dim)<br/><br/>        self.A_log = tf.Variable(<br/>                tf.math.log(self.A), <br/>                trainable=True, dtype=tf.float32, <br/>                name=f"SSM_A_log_{args.layer_id}")<br/><br/>        self.D = tf.Variable(<br/>                np.ones(args.model_internal_dim), <br/>                trainable=True, dtype=tf.float32, <br/>                name=f"SSM_D_{args.layer_id}")<br/><br/>        self.out_projection = layers.Dense(<br/>                args.model_input_dims, <br/>                input_shape=(args.model_internal_dim,), <br/>                use_bias=args.dense_use_bias)<br/><br/>    def call(self, x):<br/>        """Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba pape.<br/>        Official Implementation:<br/>            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119<br/>            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311<br/>        """<br/><br/>        (batch_size, seq_len, dimension) = x.shape<br/><br/>        x_and_res = self.in_projection(x) # shape = (batch, seq_len, 2 * model_internal_dimension)<br/>        (x, res) = tf.split(x_and_res, <br/>                            [self.args.model_internal_dim, <br/>                             self.args.model_internal_dim], axis=-1)<br/>        <br/>        x = rearrange(x, 'b l d_in -&gt; b d_in l')<br/>        x = self.conv1d(x)[:, :, :seq_len]<br/>        x = rearrange(x, 'b d_in l -&gt; b l d_in')<br/>        <br/>        x = tf.nn.swish(x)<br/>        y = self.ssm(x)<br/>        y = y * tf.nn.swish(res)<br/>        return self.out_projection(y)<br/>    <br/>    def ssm(self, x):<br/>        """Runs the SSM. See:<br/>            - Algorithm 2 in Section 3.2 in the Mamba paper<br/>            - run_SSM(A, B, C, u) in The Annotated S4<br/>            Official Implementation:<br/>            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311<br/>        """<br/>        (d_in, n) = self.A_log.shape<br/><br/>        # Compute ∆ A B C D, the state space parameters.<br/>        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 "Interpretation of A" for why A isn't selective)<br/>        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,<br/>        #                                  and is why Mamba is called **selective** state spaces)<br/><br/>        A = -tf.exp(tf.cast(self.A_log, tf.float32)) # shape -&gt; (d_in, n)<br/>        D = tf.cast(self.D, tf.float32)<br/><br/>        x_dbl = self.x_projection(x) # shape -&gt; (batch, seq_len, delta_t_rank + 2*n)<br/><br/>        (delta, B, C) = tf.split(<br/>                x_dbl, <br/>                num_or_size_splits=[self.args.delta_t_rank, n, n], <br/>                axis=-1) # delta.shape -&gt; (batch, seq_len) &amp; B, C shape -&gt; (batch, seq_len, n)<br/><br/>        delta = tf.nn.softplus(self.delta_t_projection(delta)) # shape -&gt; (batch, seq_len, model_input_dim)<br/><br/>        return selective_scan(x, delta, A, B, C, D)</span></pre><p id="ab23" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Finally, a residual block to implement the external skip connection.</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="93d3" class="ra nw fq qx b bg rb rc l rd re">class ResidualBlock(layers.Layer):<br/>    def __init__(self, modelargs: ModelArgs, *args, **kwargs):<br/>        super().__init__(*args, **kwargs)<br/>        self.args = modelargs<br/>        self.mixer = MambaBlock(modelargs)<br/>        self.norm = layers.LayerNormalization(epsilon=1e-5)<br/><br/>    def call(self, x):<br/>        """<br/>        Official Implementation:<br/>            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297<br/>            <br/>            Note: the official repo chains residual blocks that look like<br/>                [Add -&gt; Norm -&gt; Mamba] -&gt; [Add -&gt; Norm -&gt; Mamba] -&gt; [Add -&gt; Norm -&gt; Mamba] -&gt; ...<br/>            where the first Add is a no-op. This is purely for performance reasons as this<br/>            allows them to fuse the Add-&gt;Norm.<br/><br/>            We instead implement our blocks as the more familiar, simpler, and numerically equivalent<br/>                [Norm -&gt; Mamba -&gt; Add] -&gt; [Norm -&gt; Mamba -&gt; Add] -&gt; [Norm -&gt; Mamba -&gt; Add] -&gt; ....<br/>            <br/>        """<br/>        return self.mixer(self.norm(x)) + x</span></pre><p id="7a68" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">With this, we can initialize our model. In this example, I will be demonstrating how to use the Mamba block to create a simple classification model, but it can be easily modified to become a language model. Let’s load the <em class="ow">IMDB reviews dataset</em> for a simple sentiment classifier.</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="9544" class="ra nw fq qx b bg rb rc l rd re">from datasets import load_dataset<br/>from tqdm import tqdm<br/><br/>dataset = load_dataset("ajaykarthick/imdb-movie-reviews")</span></pre><p id="3862" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">First we create a function that will take the model args and return a model.</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="974a" class="ra nw fq qx b bg rb rc l rd re">def init_model(args: ModelArgs):<br/>    input_layer = layers.Input(shape=(args.seq_length,), name='input_ids')<br/>    x = layers.Embedding(<br/>                args.vocab_size, <br/>                args.model_input_dims, <br/>                input_length=args.seq_length)(input_layer)<br/><br/>    for i in range(args.num_layers):<br/>        x = ResidualBlock(args, name=f"Residual_{i}")(x)<br/>        x = layers.Dropout(args.dropout_rate)(x) # for regularization<br/><br/>    x = layers.LayerNormalization(epsilon=1e-5)(x) # normalization layer<br/>    <br/>    # use flatten only if we are not using the model as an LM<br/>    if not args.use_lm_head: <br/>        x = layers.Flatten()(x)<br/>    x = layers.Dense(1024, activation=tf.nn.gelu)(x)<br/>    output_layer = layers.Dense(<br/>                args.num_classes, <br/>                activation=args.final_activation)(x)<br/><br/>    model = Model(<br/>                inputs=input_layer, <br/>                outputs=output_layer, name='Mamba_ka_Mamba')<br/>    model.compile(<br/>        loss=args.loss,<br/>        optimizer=args.optimizer,<br/>        metrics=args.metrics<br/>    )<br/><br/>    return model</span></pre><p id="b687" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Now we can initialize our model, and summarize it:</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="79b6" class="ra nw fq qx b bg rb rc l rd re">args = ModelArgs(<br/>    model_input_dims=128,<br/>    model_states=32,<br/>    num_layers=12,<br/>    dropout_rate=0.2,<br/>    vocab_size=vocab_size,<br/>    num_classes=1,<br/>    loss='binary_crossentropy',<br/>)<br/>model = init_model(args)<br/>model.summary()</span></pre><pre class="rf qw qx qy bp qz bb bk"><span id="3873" class="ra nw fq qx b bg rb rc l rd re">Model: "Mamba_ka_Mamba"<br/>_________________________________________________________________<br/> Layer (type)                Output Shape              Param #   <br/>=================================================================<br/> input_ids (InputLayer)      [(None, 128)]             0         <br/>                                                                 <br/> embedding_2 (Embedding)     (None, 128, 128)          3906816   <br/>                                                                 <br/> Residual_0 (ResidualBlock)  (None, 128, 128)          129024    <br/>                                                                 <br/> dropout_24 (Dropout)        (None, 128, 128)          0         <br/>                                                                 <br/> Residual_1 (ResidualBlock)  (None, 128, 128)          129024    <br/>                                                                 <br/> dropout_25 (Dropout)        (None, 128, 128)          0<br/><br/> ... (I have shrinked this to make it more readable)<br/>                                                <br/> dropout_35 (Dropout)        (None, 128, 128)          0         <br/>                                                                 <br/> layer_normalization_38 (La  (None, 128, 128)          256       <br/> yerNormalization)                                               <br/>                                                                 <br/> flatten_2 (Flatten)         (None, 16384)             0         <br/>                                                                 <br/> dense_148 (Dense)           (None, 1024)              16778240  <br/>                                                                 <br/> dense_149 (Dense)           (None, 1)                 1025      <br/>                                                                 <br/>=================================================================<br/>Total params: 22234625 (84.82 MB)<br/>Trainable params: 22234625 (84.82 MB)<br/>Non-trainable params: 0 (0.00 Byte)<br/>_________________________________________________________________</span></pre><p id="14e3" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">For easier processing, lets pre-tokenize our data into a <em class="ow">numpy arrays</em>, then convert them into tf.data.Dataset objects:</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="692f" class="ra nw fq qx b bg rb rc l rd re">train_labels, test_labels = [], []<br/>train_ids = np.zeros((len(dataset['train']), args.seq_length))<br/>test_ids = np.zeros((len(dataset['test']), args.seq_length))<br/><br/>for i, item in enumerate(tqdm(dataset['train'])):<br/>    text = item['review']<br/>    train_ids[i, :] = tokenizer.encode_plus(<br/>            text, <br/>            max_length=args.seq_length, <br/>            padding='max_length', <br/>            return_tensors='np')['input_ids'][0][:args.seq_length]<br/><br/>    train_labels.append(item['label'])<br/><br/>for i, item in enumerate(tqdm(dataset['test'])):<br/>    text = item['review']<br/>    test_ids[i, :] = tokenizer.encode_plus(<br/>            text, <br/>            max_length=args.seq_length, <br/>            padding='max_length', <br/>            return_tensors='np')['input_ids'][0][:args.seq_length]<br/><br/>    test_labels.append(item['label'])<br/><br/>del dataset # delete the original dataset to save some memory<br/><br/>BATCH_SIZE = 32<br/>train_dataset = tf.data.Dataset.from_tensor_slices((train_ids, train_labels)).batch(BATCH_SIZE).shuffle(1000)<br/>test_dataset = tf.data.Dataset.from_tensor_slices((test_ids, test_labels)).batch(BATCH_SIZE).shuffle(1000)</span></pre><p id="92f4" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Now the model can be trained:</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="a05f" class="ra nw fq qx b bg rb rc l rd re">history = model.fit(train_dataset, validation_data=test_dataset, epochs=10)</span></pre><p id="d6d7" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">You can play around with the inference algorithm:</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="6c5e" class="ra nw fq qx b bg rb rc l rd re">def infer(text: str, model: Model, tokenizer):<br/>    tokens = tokenizer.encode(<br/>            "Hello what is up", <br/>            max_length=args.seq_length, <br/>            padding='max_length', return_tensors='np')<br/>    output = model(tokens)[0, 0]<br/>    return output</span></pre><p id="1b24" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">This model can be converted into a language model and algorithms like <em class="ow">beam search, top-k sampling, greedy sampling, etc. </em>can be used to generate language.</p><p id="0df6" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">This code can be found on my <a class="af nu" href="https://github.com/maxDeCoder/Mamba-tf" rel="noopener ugc nofollow" target="_blank">Github</a>.</p><p id="b859" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">A lot of the code is inspired from the mamba’s official implementation[2] and another pytorch implementation called ‘mamba-tiny’[3]</p><p id="5a7d" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Thank you for reading.</p></div></div></div><div class="ab cb rg rh ri rj" role="separator"><span class="rk by bm rl rm rn"/><span class="rk by bm rl rm rn"/><span class="rk by bm rl rm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><ul class=""><li id="fdaf" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt pv pw px bk">Unless otherwise noted, all images are made by me.</li></ul><p id="6de7" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">References:</p><ol class=""><li id="7f91" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt ro pw px bk"><a class="af nu" href="https://arxiv.org/abs/2312.00752" rel="noopener ugc nofollow" target="_blank">Mamba paper.</a></li><li id="1918" class="my mz fq na b go py nc nd gr pz nf ng nh qa nj nk nl qb nn no np qc nr ns nt ro pw px bk"><a class="af nu" href="https://github.com/state-spaces/mamba" rel="noopener ugc nofollow" target="_blank">Mamba original repository</a></li><li id="38d3" class="my mz fq na b go py nc nd gr pz nf ng nh qa nj nk nl qb nn no np qc nr ns nt ro pw px bk"><a class="af nu" href="https://github.com/PeaBrane/mamba-tiny" rel="noopener ugc nofollow" target="_blank">A simpler Torch implementation of Mamba: mamba-tiny</a></li><li id="cfce" class="my mz fq na b go py nc nd gr pz nf ng nh qa nj nk nl qb nn no np qc nr ns nt ro pw px bk"><a class="af nu" href="https://youtu.be/vrF3MtGwD0Y?si=st2Oipq3fli9tGhl" rel="noopener ugc nofollow" target="_blank">A simple explanation by Letitia on YouTube.</a></li><li id="3a32" class="my mz fq na b go py nc nd gr pz nf ng nh qa nj nk nl qb nn no np qc nr ns nt ro pw px bk"><a class="af nu" href="https://maartengrootendorst.substack.com/p/a-visual-guide-to-mamba-and-state" rel="noopener ugc nofollow" target="_blank">Maarten Grootendorst’s article on SSMs and Mamba</a></li><li id="2710" class="my mz fq na b go py nc nd gr pz nf ng nh qa nj nk nl qb nn no np qc nr ns nt ro pw px bk"><a class="af nu" href="https://en.wikipedia.org/wiki/SSM" rel="noopener ugc nofollow" target="_blank">SSMs on wikipedia</a></li><li id="6867" class="my mz fq na b go py nc nd gr pz nf ng nh qa nj nk nl qb nn no np qc nr ns nt ro pw px bk"><a class="af nu" href="https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda" rel="noopener ugc nofollow" target="_blank">Nvidia’s tutorial on Parallel associative scan</a></li></ol></div></div></div><div class="ab cb rg rh ri rj" role="separator"><span class="rk by bm rl rm rn"/><span class="rk by bm rl rm rn"/><span class="rk by bm rl rm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="749b" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Want to connect? Please write to me at <a class="af nu" href="mailto:vedantjumle@gmail.com" rel="noopener ugc nofollow" target="_blank">vedantjumle@gmail.com</a></p></div></div></div></div>    
</body>
</html>