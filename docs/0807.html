<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Transformers: How Do They Transform Your Data?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Transformers: How Do They Transform Your Data?</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-how-do-they-transform-your-data-72d69e383e0d?source=collection_archive---------0-----------------------#2024-03-28">https://towardsdatascience.com/transformers-how-do-they-transform-your-data-72d69e383e0d?source=collection_archive---------0-----------------------#2024-03-28</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="5781" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Diving into the Transformers architecture and what makes them unbeatable at language tasks</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Maxime Wolf" class="l ep by dd de cx" src="../Images/259b3659d0e6dd1d0f0eec4ae92d02e9.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*RBpN_UoyWaIUrh-iGL-WAg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------" rel="noopener follow">Maxime Wolf</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 28, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/4dd0f4cd464558d94b73f8274580ac94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eBqke1in-L9fFZrZ6hjdRA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by the author</figcaption></figure><p id="171d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the rapidly evolving landscape of artificial intelligence and machine learning, one innovation stands out for its profound impact on how we process, understand, and generate data: <strong class="ne fr">Transformers</strong>. Transformers have revolutionized the field of natural language processing (NLP) and beyond, powering some of today’s most advanced AI applications. But what exactly are Transformers, and how do they manage to transform data in such groundbreaking ways? This article demystifies the inner workings of Transformer models, focusing on the <strong class="ne fr">encoder architecture</strong>. We will start by going through the implementation of a Transformer encoder in Python, breaking down its main components. Then, we will visualize how Transformers process and adapt input data during training.</p><p id="7cff" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While this blog doesn’t cover every architectural detail, it provides an implementation and an overall understanding of the transformative power of Transformers. For an in-depth explanation of Transformers, I suggest you look at the excellent Stanford CS224-n course.</p><p id="cec6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I also recommend following the <a class="af ny" href="https://github.com/maxime7770/Transformers-Insights" rel="noopener ugc nofollow" target="_blank">GitHub repository</a> associated with this article for additional details. 😊</p><h1 id="a5ba" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">What is a Transformer encoder architecture?</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ov"><img src="../Images/df39c5bc0e96c04388b637bb391a7fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ittgNWKSm6uejPpyGu2SdQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The Transformer model from <a class="af ny" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a></figcaption></figure><p id="39c5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This picture shows the original Transformer architecture, combining an encoder and a decoder for sequence-to-sequence language tasks.</p><p id="2da9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this article, we will focus on the encoder architecture (the red block on the picture). This is what the popular BERT model is using under the hood: the primary focus is on <strong class="ne fr">understanding and representing the data</strong>, rather than generating sequences. It can be used for a variety of applications: text classification, named-entity recognition (NER), extractive question answering, etc.</p><p id="a14c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, how is the data actually transformed by this architecture? We will explain each component in detail, but here is an overview of the process.</p><ul class=""><li id="b8f2" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ow ox oy bk">The input text is <strong class="ne fr">tokenized</strong>: the Python string is transformed into a list of tokens (numbers)</li><li id="7d06" class="nc nd fq ne b go oz ng nh gr pa nj nk nl pb nn no np pc nr ns nt pd nv nw nx ow ox oy bk">Each token is passed through an <strong class="ne fr">Embedding layer</strong> that outputs a vector representation for each token</li><li id="1a53" class="nc nd fq ne b go oz ng nh gr pa nj nk nl pb nn no np pc nr ns nt pd nv nw nx ow ox oy bk">The embeddings are then further encoded with a <strong class="ne fr">Positional Encoding layer</strong>, adding information about the position of each token in the sequence</li><li id="91c7" class="nc nd fq ne b go oz ng nh gr pa nj nk nl pb nn no np pc nr ns nt pd nv nw nx ow ox oy bk">These new embeddings are transformed by a series of <strong class="ne fr">Encoder Layers</strong>, using a self-attention mechanism</li><li id="f98d" class="nc nd fq ne b go oz ng nh gr pa nj nk nl pb nn no np pc nr ns nt pd nv nw nx ow ox oy bk">A <strong class="ne fr">task-specific head</strong> can be added. For example, we will later use a classification head to classify movie reviews as positive or negative</li></ul><p id="f5d8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">That is important to understand that the Transformer architecture transforms the embedding vectors by mapping them from one representation in a high-dimensional space to another within the same space, applying a series of complex transformations.</p><h1 id="bdc6" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Implementing an encoder architecture in Python</h1><h2 id="666a" class="pe oa fq bf ob pf pg ph oe pi pj pk oh nl pl pm pn np po pp pq nt pr ps pt pu bk">The Positional Encoder layer</h2><p id="c4c5" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk">Unlike RNN models, the attention mechanism makes no use of the order of the input sequence. The PositionalEncoder class adds positional encodings to the input embeddings, using two mathematical functions: cosine and sine.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qa"><img src="../Images/578ef5784eef3551c213f00d3bfdbdd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*971n-dZ3KwprUN0GKG-_dg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Positional encoding matrix definition from <a class="af ny" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a></figcaption></figure><p id="0a71" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that positional encodings don’t contain trainable parameters: there are the results of deterministic computations, which makes this method very tractable. Also, sine and cosine functions take values between -1 and 1 and have useful periodicity properties to help the model learn patterns about the <strong class="ne fr">relative positions of words</strong>.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="7929" class="qf oa fq qc b bg qg qh l qi qj">class PositionalEncoder(nn.Module):<br/>    def __init__(self, d_model, max_length):<br/>        super(PositionalEncoder, self).__init__()<br/>        self.d_model = d_model<br/>        self.max_length = max_length<br/>        <br/>        # Initialize the positional encoding matrix<br/>        pe = torch.zeros(max_length, d_model)<br/><br/>        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)<br/>        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))<br/>        <br/>        # Calculate and assign position encodings to the matrix<br/>        pe[:, 0::2] = torch.sin(position * div_term)<br/>        pe[:, 1::2] = torch.cos(position * div_term)<br/>        self.pe = pe.unsqueeze(0)<br/>    <br/>    def forward(self, x):<br/>        x = x + self.pe[:, :x.size(1)] # update embeddings<br/>        return x</span></pre><h2 id="2aa1" class="pe oa fq bf ob pf pg ph oe pi pj pk oh nl pl pm pn np po pp pq nt pr ps pt pu bk">Multi-Head Self-Attention</h2><p id="7a3a" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk">The self-attention mechanism is the key component of the encoder architecture. Let’s ignore the “multi-head” for now. Attention is a way to determine for each token (i.e. each embedding) the <strong class="ne fr">relevance of all other embeddings to that token</strong>, to obtain a more refined and contextually relevant encoding.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qk"><img src="../Images/cd6d84be3aae29b2a4108ac244d7dd6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*FtvndpgqfD0GHd1ocOlxTA.jpeg"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">How does“it” pay attention to other words of the sequence? (<a class="af ny" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">The Illustrated Transformer</a>)</figcaption></figure><p id="6d9b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are 3 steps in the self-attention mechanism.</p><ul class=""><li id="b3f1" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ow ox oy bk">Use matrices Q, K, and V to respectively transform the inputs “<strong class="ne fr">query</strong>”, “<strong class="ne fr">key</strong>” and “<strong class="ne fr">value</strong>”. Note that for self-attention, query, key, and values are all equal to our input embedding</li><li id="7391" class="nc nd fq ne b go oz ng nh gr pa nj nk nl pb nn no np pc nr ns nt pd nv nw nx ow ox oy bk">Compute the attention score using cosine similarity (a dot product) between the <strong class="ne fr">query</strong> and the <strong class="ne fr">key</strong>. Scores are scaled by the square root of the embedding dimension to stabilize the gradients during training</li><li id="ec14" class="nc nd fq ne b go oz ng nh gr pa nj nk nl pb nn no np pc nr ns nt pd nv nw nx ow ox oy bk">Use a softmax layer to make these scores <strong class="ne fr">probabilities</strong></li><li id="e946" class="nc nd fq ne b go oz ng nh gr pa nj nk nl pb nn no np pc nr ns nt pd nv nw nx ow ox oy bk">The output is the weighted average of the <strong class="ne fr">values</strong>, using the attention scores as the weights</li></ul><p id="379c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Mathematically, this corresponds to the following formula.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ql"><img src="../Images/c5773d753ad9db7bab34c7e7739de18c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*epJS1mkxjNqAUS0rjJhkAQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The Attention Mechanism from <a class="af ny" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a></figcaption></figure><p id="922e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">What does “multi-head” mean? Basically, we can apply the described self-attention mechanism process several times, in parallel, and concatenate and project the outputs. This allows each head to f<strong class="ne fr">ocus on different semantic aspects of the sentence</strong>.</p><p id="a820" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We start by defining the number of heads, the dimension of the embeddings (d_model), and the dimension of each head (head_dim). We also initialize the Q, K, and V matrices (linear layers), and the final projection layer.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="bdfa" class="qf oa fq qc b bg qg qh l qi qj">class MultiHeadAttention(nn.Module):<br/>    def __init__(self, d_model, num_heads):<br/>        super(MultiHeadAttention, self).__init__()<br/>        self.num_heads = num_heads<br/>        self.d_model = d_model<br/>        self.head_dim = d_model // num_heads<br/><br/>        self.query_linear = nn.Linear(d_model, d_model)<br/>        self.key_linear = nn.Linear(d_model, d_model)<br/>        self.value_linear = nn.Linear(d_model, d_model)      <br/>        self.output_linear = nn.Linear(d_model, d_model)</span></pre><p id="170d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When using multi-head attention, we apply each attention head with a reduced dimension (head_dim instead of d_model) as in the original paper, making the total computational cost similar to a one-head attention layer with full dimensionality. Note this is a logical split only. What makes multi-attention so powerful is it can still be represented via a single matrix operation, making computations very efficient on GPUs.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="5298" class="qf oa fq qc b bg qg qh l qi qj">def split_heads(self, x, batch_size):<br/>        # Split the sequence embeddings in x across the attention heads<br/>        x = x.view(batch_size, -1, self.num_heads, self.head_dim)<br/>        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)</span></pre><p id="97fd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We compute the attention scores and use a mask to avoid using attention on padded tokens. We apply a softmax activation to make these scores probabilities.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="49a2" class="qf oa fq qc b bg qg qh l qi qj">def compute_attention(self, query, key, mask=None):<br/>      # Compute dot-product attention scores<br/>      # dimensions of query and key are (batch_size * num_heads, seq_length, head_dim)<br/>      scores = query @ key.transpose(-2, -1) / math.sqrt(self.head_dim)<br/>      # Now, dimensions of scores is (batch_size * num_heads, seq_length, seq_length)<br/>      if mask is not None:<br/>          scores = scores.view(-1, scores.shape[0] // self.num_heads, mask.shape[1], mask.shape[2]) # for compatibility<br/>          scores = scores.masked_fill(mask == 0, float('-1e20')) # mask to avoid attention on padding tokens<br/>          scores = scores.view(-1, mask.shape[1], mask.shape[2]) # reshape back to original shape<br/>      # Normalize attention scores into attention weights<br/>      attention_weights = F.softmax(scores, dim=-1)<br/><br/>      return attention_weights</span></pre><p id="e31c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The forward attribute performs the multi-head logical split and computes the attention weights. Then, we get the output by multiplying these weights by the values. Finally, we reshape the output and project it with a linear layer.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="32a8" class="qf oa fq qc b bg qg qh l qi qj">def forward(self, query, key, value, mask=None):<br/>      batch_size = query.size(0)<br/><br/>      query = self.split_heads(self.query_linear(query), batch_size)<br/>      key = self.split_heads(self.key_linear(key), batch_size)<br/>      value = self.split_heads(self.value_linear(value), batch_size)<br/><br/>      attention_weights = self.compute_attention(query, key, mask)<br/>          <br/>      # Multiply attention weights by values, concatenate and linearly project outputs<br/>      output = torch.matmul(attention_weights, value)<br/>      output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)<br/>      return self.output_linear(output)</span></pre><h2 id="39fd" class="pe oa fq bf ob pf pg ph oe pi pj pk oh nl pl pm pn np po pp pq nt pr ps pt pu bk">The Encoder Layer</h2><p id="5ea9" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk">This is the main component of the architecture, which leverages multi-head self-attention. We first implement a simple class to perform a feed-forward operation through 2 dense layers.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="134b" class="qf oa fq qc b bg qg qh l qi qj">class FeedForwardSubLayer(nn.Module):<br/>    def __init__(self, d_model, d_ff):<br/>        super(FeedForwardSubLayer, self).__init__()<br/>        self.fc1 = nn.Linear(d_model, d_ff)<br/>        self.fc2 = nn.Linear(d_ff, d_model)<br/>        self.relu = nn.ReLU()<br/><br/>    def forward(self, x):<br/>        return self.fc2(self.relu(self.fc1(x)))</span></pre><p id="daed" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can now code the logic for the encoder layer. We start by applying self-attention to the input, which gives a vector of the same dimension. We then use our mini feed-forward network with Layer Norm layers. Note that we also use skip connections before applying normalization.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="30af" class="qf oa fq qc b bg qg qh l qi qj">class EncoderLayer(nn.Module):<br/>    def __init__(self, d_model, num_heads, d_ff, dropout):<br/>        super(EncoderLayer, self).__init__()<br/>        self.self_attn = MultiHeadAttention(d_model, num_heads)<br/>        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)<br/>        self.norm1 = nn.LayerNorm(d_model)<br/>        self.norm2 = nn.LayerNorm(d_model)<br/>        self.dropout = nn.Dropout(dropout)<br/><br/>    def forward(self, x, mask):<br/>        attn_output = self.self_attn(x, x, x, mask)<br/>        x = self.norm1(x + self.dropout(attn_output)) # skip connection and normalization<br/>        ff_output = self.feed_forward(x)<br/>        return self.norm2(x + self.dropout(ff_output)) # skip connection and normalization</span></pre><h2 id="c6d4" class="pe oa fq bf ob pf pg ph oe pi pj pk oh nl pl pm pn np po pp pq nt pr ps pt pu bk">Putting Everything Together</h2><p id="dba4" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk">It’s time to create our final model. We pass our data through an embedding layer. This transforms our raw tokens (integers) into a numerical vector. We then apply our positional encoder and several (num_layers) encoder layers.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="4a58" class="qf oa fq qc b bg qg qh l qi qj">class TransformerEncoder(nn.Module):<br/>    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):<br/>        super(TransformerEncoder, self).__init__()<br/>        self.embedding = nn.Embedding(vocab_size, d_model)<br/>        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)<br/>        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])<br/>    <br/>    def forward(self, x, mask):<br/>        x = self.embedding(x)<br/>        x = self.positional_encoding(x)<br/>        for layer in self.layers:<br/>            x = layer(x, mask)<br/>        return x</span></pre><p id="72cc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We also create a ClassifierHead class which is used to transform the final embedding into class probabilities for our classification task.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="4fbc" class="qf oa fq qc b bg qg qh l qi qj">class ClassifierHead(nn.Module):<br/>    def __init__(self, d_model, num_classes):<br/>        super(ClassifierHead, self).__init__()<br/>        self.fc = nn.Linear(d_model, num_classes)<br/><br/>    def forward(self, x):<br/>        logits = self.fc(x[:, 0, :]) # first token corresponds to the classification token<br/>        return F.softmax(logits, dim=-1)</span></pre><p id="82a2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that the dense and softmax layers are only applied on the first embedding (corresponding to the first token of our input sequence). This is because when tokenizing the text, the first token is the [CLS] token which stands for “classification.” The [CLS] token is designed to aggregate the entire sequence’s information into a single embedding vector, serving as a summary representation that can be used for classification tasks.</p><p id="25c8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note: the concept of including a [CLS] token originates from BERT, which was initially trained on tasks like next-sentence prediction. The [CLS] token was inserted to predict the likelihood that sentence B follows sentence A, with a [SEP] token separating the 2 sentences. For our model, the [SEP] token simply marks the end of the input sentence, as shown below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qm"><img src="../Images/d3feb19e839643803e928bfc05828ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*t4iX4ZPWJmnj42AIlTpvoA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">[CLS] Token in BERT Architecture (<a class="af ny" href="https://seunghan96.github.io/dl/nlp/28.-nlp-BERT-%EC%9D%B4%EB%A1%A0/" rel="noopener ugc nofollow" target="_blank">All About AI</a>)</figcaption></figure><p id="15cc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When you think about it, it’s really mind-blowing that this single [CLS] embedding is able to capture so much information about the entire sequence, thanks to the self-attention mechanism’s ability to weigh and synthesize the importance of every piece of the text in relation to each other.</p><h1 id="21ca" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Training and visualization</h1><p id="01b1" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk">Hopefully, the previous section gives you a better understanding of how our Transformer model transforms the input data. We will now write our training pipeline for our binary classification task using the IMDB dataset (movie reviews). Then, we will visualize the embedding of the [CLS] token during the training process to see how our model transformed it.</p><p id="8a09" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We first define our hyperparameters, as well as a BERT tokenizer. In the GitHub repository, you can see that I also coded a function to select a subset of the dataset with only 1200 train and 200 test examples.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="463a" class="qf oa fq qc b bg qg qh l qi qj">num_classes = 2 # binary classification<br/>d_model = 256 # dimension of the embedding vectors<br/>num_heads = 4 # number of heads for self-attention<br/>num_layers = 4 # number of encoder layers<br/>d_ff = 512. # dimension of the dense layers in the encoder layers<br/>sequence_length = 256 # maximum sequence length <br/>dropout = 0.4 # dropout to avoid overfitting<br/>num_epochs = 20<br/>batch_size = 32<br/><br/>loss_function = torch.nn.CrossEntropyLoss()<br/><br/>dataset = load_dataset("imdb")<br/>dataset = balance_and_create_dataset(dataset, 1200, 200) # check GitHub repo<br/><br/>tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', model_max_length=sequence_length)</span></pre><p id="b604" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can try to use the BERT tokenizer on one of the sentences:</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="1034" class="qf oa fq qc b bg qg qh l qi qj">print(tokenized_datasets['train']['input_ids'][0])</span></pre><p id="0746" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Every sequence should start with the token 101, corresponding to [CLS], followed by some non-zero integers and padded with zeros if the sequence length is smaller than 256. Note that these zeros are ignored during the self-attention computation using our “mask”.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="bb53" class="qf oa fq qc b bg qg qh l qi qj">tokenized_datasets = dataset.map(encode_examples, batched=True)<br/>tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])<br/><br/>train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=batch_size, shuffle=True)<br/>test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=batch_size, shuffle=True)<br/><br/>vocab_size = tokenizer.vocab_size<br/><br/>encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)<br/>classifier = ClassifierHead(d_model, num_classes)<br/><br/>optimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=1e-4)</span></pre><p id="4ece" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can now write our train function:</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="6651" class="qf oa fq qc b bg qg qh l qi qj">def train(dataloader, encoder, classifier, optimizer, loss_function, num_epochs):<br/>    for epoch in range(num_epochs):        <br/>        # Collect and store embeddings before each epoch starts for visualization purposes (check repo)<br/>        all_embeddings, all_labels = collect_embeddings(encoder, dataloader)<br/>        reduced_embeddings = visualize_embeddings(all_embeddings, all_labels, epoch, show=False)<br/>        dic_embeddings[epoch] = [reduced_embeddings, all_labels]<br/>        <br/>        encoder.train()<br/>        classifier.train()<br/>        correct_predictions = 0<br/>        total_predictions = 0<br/>        for batch in tqdm(dataloader, desc="Training"):<br/>            input_ids = batch['input_ids']<br/>            attention_mask = batch['attention_mask'] # indicate where padded tokens are<br/>            # These 2 lines make the attention_mask a matrix instead of a vector<br/>            attention_mask = attention_mask.unsqueeze(-1)<br/>            attention_mask = attention_mask &amp; attention_mask.transpose(1, 2) <br/>            labels = batch['label']<br/>            optimizer.zero_grad()<br/>            output = encoder(input_ids, attention_mask)<br/>            classification = classifier(output)<br/>            loss = loss_function(classification, labels)<br/>            loss.backward()<br/>            optimizer.step()<br/>            preds = torch.argmax(classification, dim=1)<br/>            correct_predictions += torch.sum(preds == labels).item()<br/>            total_predictions += labels.size(0)<br/>        <br/>        epoch_accuracy = correct_predictions / total_predictions<br/>        print(f'Epoch {epoch} Training Accuracy: {epoch_accuracy:.4f}')</span></pre><p id="c531" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can find the collect_embeddings and visualize_embeddings functions in the GitHub repo. They store the [CLS] token embedding for each sentence of the training set, apply a dimensionality reduction technique called t-SNE to make them 2D vectors (instead of 256-dimensional vectors), and save an animated plot.</p><p id="9a4c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s visualize the results.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qn"><img src="../Images/23987d3afe279c76504d4c31a47e8066.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K3bwH03hjtra_QQyY5vYGg.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Projected [CLS] embeddings for each training point (blue corresponds to positive sentences, red corresponds to negative sentences)</figcaption></figure><p id="fb07" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Observing the plot of projected [CLS] embeddings for each training point, we can see the clear distinction between positive (blue) and negative (red) sentences after a few epochs. This visual shows the remarkable capability of the Transformer architecture to adapt embeddings over time and highlights the power of the self-attention mechanism. The data is transformed in such a way that embeddings for each class are well separated, thereby significantly simplifying the task for the classifier head.</p><h1 id="2819" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion</h1><p id="7060" class="pw-post-body-paragraph nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx fj bk">As we conclude our exploration of the Transformer architecture, it’s evident that these models are adept at tailoring data to a given task. With the use of positional encoding and multi-head self-attention, Transformers go beyond mere data processing: they interpret and understand information with a level of sophistication previously unseen. The ability to dynamically weigh the relevance of different parts of the input data allows for a more nuanced understanding and representation of the input text. This enhances performance across a wide array of downstream tasks, including text classification, question answering, named entity recognition, and more.</p><p id="6353" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now that you have a better understanding of the encoder architecture, you are ready to delve into decoder and encoder-decoder models, which are very similar to what we have just explored. Decoders play a pivotal role in generative tasks and are at the core of the popular GPT models.</p></div></div></div><div class="ab cb qo qp qq qr" role="separator"><span class="qs by bm qt qu qv"/><span class="qs by bm qt qu qv"/><span class="qs by bm qt qu"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><ul class=""><li id="384b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ow ox oy bk">Feel free to connect on <a class="af ny" href="https://www.linkedin.com/in/maxime-wolf/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a></li><li id="e071" class="nc nd fq ne b go oz ng nh gr pa nj nk nl pb nn no np pc nr ns nt pd nv nw nx ow ox oy bk">Follow me on <a class="af ny" href="https://github.com/maxime7770" rel="noopener ugc nofollow" target="_blank">GitHub</a> for more content</li><li id="301b" class="nc nd fq ne b go oz ng nh gr pa nj nk nl pb nn no np pc nr ns nt pd nv nw nx ow ox oy bk">Visit my website: <a class="af ny" href="http://maximewolf.com" rel="noopener ugc nofollow" target="_blank">maximewolf.com</a></li></ul></div></div></div><div class="ab cb qo qp qq qr" role="separator"><span class="qs by bm qt qu qv"/><span class="qs by bm qt qu qv"/><span class="qs by bm qt qu"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="dccb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">References</strong></p><p id="a93f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[1] Vaswani, Ashish, et al. “Attention Is All You Need.” <em class="qw">31st Conference on Neural Information Processing Systems (NIPS 2017)</em>, Long Beach, CA, USA.</p><p id="3e2a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[2] “The Illustrated Transformer.” <em class="qw">Jay Alammar’s Blog</em>, June 2018, <a class="af ny" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-transformer/</a></p><p id="3a66" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[3] Official PyTorch Implementation of the Transformer Architecture. <em class="qw">GitHub repository</em>, PyTorch, <a class="af ny" href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py" rel="noopener ugc nofollow" target="_blank">https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py</a></p><p id="68a3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[4] Manning, Christopher, et al. “CS224n: Natural Language Processing with Deep Learning.” <em class="qw">Stanford University</em>, Stanford CS224N NLP course, <a class="af ny" href="http://web.stanford.edu/class/cs224n/" rel="noopener ugc nofollow" target="_blank">http://web.stanford.edu/class/cs224n/</a></p></div></div></div></div>    
</body>
</html>