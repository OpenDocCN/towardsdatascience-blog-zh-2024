- en: 'Speak, Don’t Type: Exploring Voice Interaction with LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/speak-dont-type-exploring-voice-interaction-with-llms-part-1-732257710e9d?source=collection_archive---------4-----------------------#2024-04-23](https://towardsdatascience.com/speak-dont-type-exploring-voice-interaction-with-llms-part-1-732257710e9d?source=collection_archive---------4-----------------------#2024-04-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Augmenting LLM Apps with a Voice Modality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@CVxTz?source=post_page---byline--732257710e9d--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--732257710e9d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--732257710e9d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--732257710e9d--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--732257710e9d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--732257710e9d--------------------------------)
    ·6 min read·Apr 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1a5f00dff88978127a9214c8aa3c074.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Quino Al](https://unsplash.com/@quinoal?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/top-view-photo-of-purple-daisy--3KfR1GVKXY?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: Many LLMs, particularly those that are open-source, have typically been limited
    to processing text or, occasionally, text with images (Large Multimodal Models
    or LMMs). But what if you want to communicate with your LLM using your voice?
    Thanks to the advancement of powerful speech-to-text open-source technologies
    in recent years, this becomes achievable.
  prefs: []
  type: TYPE_NORMAL
- en: We will go into the integration of Llama 3 with a speech-to-text model, all
    within a user-friendly interface. This fusion enables (near) real-time communication
    with an LLM through speech. Our exploration involves selecting Llama 3 8B as the
    LLM, using the Whisper speech-to-text model, and the capabilities of NiceGUI —
    a framework that uses FastAPI on the backend and Vue3 on the frontend, interconnected
    with socket.io.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this post, you will be able to augment an LLM with a new audio
    modality. This will allow you to build a full end-to-end workflow and UI that
    enables you to use your voice to command and prompt an LLM instead of typing.
    This feature can prove especially beneficial for mobile applications, where typing
    on a keyboard may not be as user-friendly as on desktops. Additionally, integrating
    this functionality can enhance the accessibility of your LLM app…
  prefs: []
  type: TYPE_NORMAL
