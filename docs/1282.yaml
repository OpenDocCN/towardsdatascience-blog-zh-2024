- en: 'LangChain’s Built-In Eval Metrics for AI Output: How Are They Different?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/langchains-built-in-eval-metrics-for-ai-output-how-are-they-different-f9dd75e2de08?source=collection_archive---------9-----------------------#2024-05-22](https://towardsdatascience.com/langchains-built-in-eval-metrics-for-ai-output-how-are-they-different-f9dd75e2de08?source=collection_archive---------9-----------------------#2024-05-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jonathan.bennion?source=post_page---byline--f9dd75e2de08--------------------------------)[![Jonathan
    Bennion](../Images/e2d9add564ee2ac0deb7863537b0ee73.png)](https://medium.com/@jonathan.bennion?source=post_page---byline--f9dd75e2de08--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f9dd75e2de08--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f9dd75e2de08--------------------------------)
    [Jonathan Bennion](https://medium.com/@jonathan.bennion?source=post_page---byline--f9dd75e2de08--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f9dd75e2de08--------------------------------)
    ·5 min read·May 22, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: I’ve created custom metrics most often for my own use cases, but have come across
    these built-in metrics for AI tools in LangChain repeatedly before I’d started
    using RAGAS and/or DeepEval for RAG evaluation, so finally was curious on how
    these metrics are created and ran a quick analysis (with all inherent bias of
    course).
  prefs: []
  type: TYPE_NORMAL
- en: '***TLDR is from the correlation matrix below:***'
  prefs: []
  type: TYPE_NORMAL
- en: '***Helpfulness and Coherence (0.46 correlation)****: This strong correlation
    suggests that the LLM (and by proxy, users) could find coherent responses more
    helpful, emphasizing the importance of logical structuring in responses. It is
    just correlation, but this relationship opens the possibility for this takeaway.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Controversiality and Criminality (0.44 correlation)****: This indicates
    that even controversial content could be deemed criminal, and vice versa, perhaps
    reflecting a user preference for engaging and thought-provoking material.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Coherence vs. Depth:*** *Despite coherence correlating with helpfulness,
    depth does not. This might suggest that users (again, assuming user preferences
    are inherent in the output of the LLM — this alone is a presumption and a bias
    that is important to be concious of) could prefer clear and concise answers over
    detailed ones, particularly in contexts where quick solutions are valued over
    comprehensive ones.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2934a7023672ad3ffd5a80c6b8bfbe3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The built-in metrics are found here (removing one that relates to ground truth
    and better handled elsewhere):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The metrics:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Conciseness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relevance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coherence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harmfulness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insensitivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helpfulness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controversiality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Criminality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creativity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, what do these mean, and why were they created?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The hypothesis:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These were created in an attempt to define metrics that could explain output
    in relation to theoretical use case goals, and any correlation could be accidental
    but was generally avoided where possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have this hypothesis after seeing [this source code here](https://api.python.langchain.com/en/latest/_modules/langchain/evaluation/criteria/eval_chain.html).
  prefs: []
  type: TYPE_NORMAL
- en: Second, some of these seem similar and/or vague — so how are these different?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I used a standard SQuAD dataset as a baseline to evaluate the differences (if
    any) between output from OpenAI’s GPT-3-Turbo model and the ground truth in this
    dataset, and compare.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I obtained a randomized set of rows for evaluation (could not afford timewise
    and compute for the whole thing), so this could be an entrypoint for more noise
    and/or bias.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: I defined an llm using ChatGPT 3.5 Turbo (to save on cost here, this is quick).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Then iterated through the sampled rows to gather a comparison — there were unknown
    thresholds that LangChain used for ‘score’ in the evaluation criteria, but the
    assumption is that they are defined the same for all metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Then I calculated means and CI at 95% confidence intervals.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: And plotted the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This is possibly intuitive that ‘Relevance’ is so much higher than the others,
    but interesting that overall they are so low (maybe thanks to GPT 3.5!), and that
    ‘Helpfulness’ is next highest metric (possibly reflecting RL techniques and optimizations).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06c0aec539a408ad7380c763433a4d69.png)'
  prefs: []
  type: TYPE_IMG
- en: To answer my question on correlation, I’d calculated a simple correlation matrix
    with the raw comparison dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Then plotted the results (p values are created [further down in my code](https://github.com/j-space-b/eval_analysis/blob/main/evaluation_metrics_corrplot.ipynb)
    and were all under .05)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Was surprising that most do not correlate, given the nature of the descriptions
    in the LangChain codebase — this lends to something a bit more thought out, and
    am glad these are built-in for use.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2934a7023672ad3ffd5a80c6b8bfbe3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the correlation matrix, notable relationships emerge:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Helpfulness and Coherence (0.46 correlation):* This strong correlation suggests
    that the LLM (as it is a proxy for users) could find coherent responses more helpful,
    emphasizing the importance of logical structuring in responses. Even though this
    is correlation, this relationship paves the way for this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Controversiality and Criminality (0.44 correlation):* This indicates that
    even controversial content could be deemed criminal, and vice versa, perhaps reflecting
    a user preference for engaging and thought-provoking material. Again, this is
    only correlation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coherence vs. Depth in Helpfulness:** Despite coherence correlating with
    helpfulness, depth does not. This might suggest that users could prefer clear
    and concise answers over detailed ones, particularly in contexts where quick solutions
    are valued over comprehensive ones.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Leveraging Controversiality:** The positive correlation between controversiality
    and criminality poses an interesting question: Can controversial topics be discussed
    in a way that is not criminal? This could potentially increase user engagement
    without compromising on content quality.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Impact of Bias and Model Choice:** The use of GPT-3.5 Turbo and the inherent
    biases in metric design could influence these correlations. Acknowledging these
    biases is essential for accurate interpretation and application of these metrics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images in this article created by the author.
  prefs: []
  type: TYPE_NORMAL
