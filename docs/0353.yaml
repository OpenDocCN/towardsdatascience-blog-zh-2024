- en: Solving Differential Equations With Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络求解微分方程
- en: 原文：[https://towardsdatascience.com/solving-differential-equations-with-neural-networks-4c6aa7b31c51?source=collection_archive---------2-----------------------#2024-02-06](https://towardsdatascience.com/solving-differential-equations-with-neural-networks-4c6aa7b31c51?source=collection_archive---------2-----------------------#2024-02-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/solving-differential-equations-with-neural-networks-4c6aa7b31c51?source=collection_archive---------2-----------------------#2024-02-06](https://towardsdatascience.com/solving-differential-equations-with-neural-networks-4c6aa7b31c51?source=collection_archive---------2-----------------------#2024-02-06)
- en: How Neural Networks are strong tools for solving differential equations without
    the use of training data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络如何成为解决微分方程的强大工具，无需使用训练数据
- en: '[](https://medium.com/@rodrigopesilva?source=post_page---byline--4c6aa7b31c51--------------------------------)[![Rodrigo
    Silva](../Images/d260f05ed9887c5072e0590db1481be2.png)](https://medium.com/@rodrigopesilva?source=post_page---byline--4c6aa7b31c51--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4c6aa7b31c51--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4c6aa7b31c51--------------------------------)
    [Rodrigo Silva](https://medium.com/@rodrigopesilva?source=post_page---byline--4c6aa7b31c51--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rodrigopesilva?source=post_page---byline--4c6aa7b31c51--------------------------------)[![Rodrigo
    Silva](../Images/d260f05ed9887c5072e0590db1481be2.png)](https://medium.com/@rodrigopesilva?source=post_page---byline--4c6aa7b31c51--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4c6aa7b31c51--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4c6aa7b31c51--------------------------------)
    [Rodrigo Silva](https://medium.com/@rodrigopesilva?source=post_page---byline--4c6aa7b31c51--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4c6aa7b31c51--------------------------------)
    ·8 min read·Feb 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4c6aa7b31c51--------------------------------)
    ·阅读时间：8分钟·2024年2月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8dd6dcc5577d527b7f03e38984b6ec26.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8dd6dcc5577d527b7f03e38984b6ec26.png)'
- en: Photo by [Linus Mimietz](https://unsplash.com/@linusmimietz?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/water-drops-macro-photography-XSQHuGGRO3g?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Linus Mimietz](https://unsplash.com/@linusmimietz?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)提供，来源于[Unsplash](https://unsplash.com/photos/water-drops-macro-photography-XSQHuGGRO3g?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: Differential equations are one of the protagonists in physical sciences, with
    vast applications in engineering, biology, economy, and even social sciences.
    Roughly speaking, they tell us how a quantity varies in time (or some other parameter,
    but usually we are interested in time variations). We can understand how a population,
    or a stock price, or even how the opinion of some society towards certain themes
    changes over time.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微分方程是自然科学中的重要工具，广泛应用于工程学、生物学、经济学，甚至社会科学。简而言之，它们告诉我们某一量如何随时间（或其他参数，通常我们关注时间变化）变化。我们可以理解某个种群、股价，甚至某个社会对于某些主题的意见如何随时间变化。
- en: 'Typically, the methods used to solve DEs are not analytical (i.e. there is
    no "closed formula" for the solution) and we have to resource to numerical methods.
    However, numerical methods can be expensive from a computational standpoint, and
    worse than that: the accumulated error can be significantly large.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，解决微分方程的方法并不是解析性的（即没有“封闭式解”），我们必须依赖数值方法。然而，从计算的角度来看，数值方法可能会非常昂贵，更糟糕的是：积累的误差可能会非常大。
- en: 'This article will showcase how a Neural Network can be a valuable ally to solve
    a differential equation, and how we can borrow concepts from Physics-Informed
    Neural Networks to tackle the question: can we use a machine learning approach
    to solve a DE?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将展示神经网络如何成为解决微分方程的有力助手，并且我们如何借鉴物理启发式神经网络的概念来解决这样一个问题：我们能否使用机器学习的方法来求解微分方程？
- en: A pinch of Physics-Informed Neural Networks
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一点点物理启发式神经网络的元素
- en: In this section, I will talk about Physics-Informed Neural Networks very briefly.
    I suppose you know the "neural network" part, but what makes them be informed
    by physics? Well, they are not exactly informed by physics, but rather by a (differential)
    equation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将简要介绍物理启发式神经网络。我猜你已经了解“神经网络”部分，但是什么使它们受物理学启发呢？嗯，它们并不是直接受物理学启发，而是受（微分）方程的启发。
- en: 'Usually, neural networks are trained to find patterns and figure out what''s
    going on with a set of training data. However, when you train a neural network
    to obey the behavior of your training data and hopefully fit unseen data, your
    model is highly dependent on the data itself, and not on the underlying nature
    of your system. It sounds almost like a philosophical matter, but it is more practical
    than that: if your data comes from measurements of ocean currents, these currents
    have to obey the physics equations that describe ocean currents. Notice, however,
    that your neural network is completely agnostic about these equations and is only
    trying to fit data points.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，神经网络被训练来发现模式，并弄清楚一组训练数据的行为。然而，当你训练一个神经网络以遵循你的训练数据的行为，并希望它能适应未见过的数据时，你的模型高度依赖于数据本身，而不是系统的基本性质。这听起来几乎像是一个哲学问题，但实际上比这更具实用性：如果你的数据来自海洋洋流的测量数据，那么这些洋流必须遵循描述洋流的物理方程。然而，请注意，你的神经网络对这些方程完全无知，只是在试图拟合数据点。
- en: This is where physics informed comes into play. If, besides learning how to
    fit your data, your model also learns how to fit the equations that govern that
    system, the predictions of your neural network will be much more precise and will
    generalize much better, just citing some advantages of physics-informed models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是物理信息化发挥作用的地方。如果除了学习如何拟合你的数据外，你的模型还学习如何拟合支配该系统的方程，那么你的神经网络的预测将更加精确，并且更好地泛化，这只是物理信息化模型的一些优势。
- en: Notice that the governing equations of your system don't have to involve physics
    at all, the "physics-informed" thing is just nomenclature (and the technique is
    most used by physicists anyway). If your system is the traffic in a city and you
    happen to have a good mathematical model that you want your neural network's predictions
    to obey, then physics-informed neural networks are a good fit for you.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，系统的控制方程不一定要涉及物理学，“物理信息化”仅仅是一种术语（而且这一技术通常由物理学家使用）。如果你的系统是一个城市的交通，并且你恰好拥有一个想让神经网络预测遵循的良好数学模型，那么物理信息化神经网络非常适合你。
- en: How do we inform these models?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何将这些信息传递给模型呢？
- en: Hopefully, I've convinced you that it is worth the trouble to make the model
    aware of the underlying equations that govern our system. However, how can we
    do this? There are several approaches to this, but the main one is to adapt the
    loss function to have a term that accounts for the governing equations, aside
    from the usual data-related part. That is, the loss function *L* will be composed
    of the sum
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 希望我已经说服你，值得费心让模型意识到支配我们系统的基本方程。然而，我们如何做到这一点呢？有几种方法，但主要的一种是调整损失函数，使其包含一个额外的项，用于表示控制方程，除了通常与数据相关的部分。也就是说，损失函数
    *L* 将由以下和构成：
- en: '![](../Images/dcc1113601f71c77b0fa9933bacd9824.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcc1113601f71c77b0fa9933bacd9824.png)'
- en: 'Here, the data loss is the usual one: a mean squared difference, or some other
    suited form of loss function; but the equation part is the charming one. Imagine
    that your system is governed by the following differential equation:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，数据损失是通常的形式：均方差，或者是其他合适的损失函数形式；但方程部分才是最吸引人的部分。假设你的系统由以下微分方程支配：
- en: '![](../Images/47f34c0f8617929070a5eca85e17a597.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47f34c0f8617929070a5eca85e17a597.png)'
- en: 'How can we fit this into the loss function? Well, since our task when training
    a neural network is to minimize the loss function, what we want is to minimize
    the following expression:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将其纳入损失函数呢？嗯，由于训练神经网络的任务是最小化损失函数，我们希望最小化以下表达式：
- en: '![](../Images/6808838b818b5eea8947e358a9851645.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6808838b818b5eea8947e358a9851645.png)'
- en: So our equation-related loss function turns out to be
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们的方程相关损失函数最终变成了
- en: '![](../Images/7c629affee0d0c244f4cac4841b471ab.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c629affee0d0c244f4cac4841b471ab.png)'
- en: that is, it is the mean difference squared of our DE. If we manage to minimize
    this (a.k.a. make this term as close to zero as possible) we automatically satisfy
    the system's governing equation. Pretty clever, right?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，它是我们微分方程的均方差。如果我们能够最小化这个（也就是让这个项尽可能接近零），我们就自动满足了系统的控制方程。相当聪明，对吧？
- en: 'Now, the extra term *L_IC* in the loss function needs to be addressed: it accounts
    for the initial conditions of the system. If a system''s initial conditions are
    not provided, there are infinitely many solutions for a differential equation.
    For instance, a ball thrown from the ground level has its trajectory governed
    by the same differential equation as a ball thrown from the 10th floor; however,
    we know for sure that the paths made by these balls will not be the same. What
    changes here are the initial conditions of the system. How does our model know
    which initial conditions we are talking about? It is natural at this point that
    we enforce it using a loss function term! For our DE, let''s impose that when
    *t = 0*, *y = 1*. Hence, we want to minimize an initial condition loss function
    that reads:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，损失函数中的额外项 *L_IC* 需要被处理：它考虑了系统的初始条件。如果一个系统的初始条件未给出，那么该微分方程将有无穷多解。例如，从地面上抛出的球的轨迹与从
    10 楼抛出的球轨迹由相同的微分方程决定；然而，我们知道这两个球的轨迹不会相同。这里变化的是系统的初始条件。我们的模型如何知道我们指的是哪些初始条件呢？此时，自然可以通过损失函数项来强制这一点！对于我们的微分方程，我们规定当
    *t = 0* 时，*y = 1*。因此，我们希望最小化一个初始条件损失函数，表达式如下：
- en: '![](../Images/5a6c9d547fb8fbe2346528fa9fad2675.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a6c9d547fb8fbe2346528fa9fad2675.png)'
- en: If we minimize this term, then we automatically satisfy the initial conditions
    of our system. Now, what is left to be understood is how to use this to solve
    a differential equation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们最小化这一项，就能自动满足系统的初始条件。现在，剩下需要理解的是如何利用这个方法来求解微分方程。
- en: Solving a differential equation
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 求解微分方程
- en: 'If a neural network can be trained either with the data-related term of the
    loss function (this is what is usually done in classical architectures), and can
    also be trained with both the data and the equation-related term (this is physics-informed
    neural networks I just mentioned), it must be true that it can be trained to minimize
    *only* the equation-related term. This is exactly what we are going to do! The
    only loss function used here will be the *L_equation*. Hopefully, this diagram
    below illustrates what I''ve just said: today we are aiming for the right-bottom
    type of model, our DE solver NN.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个神经网络既可以通过损失函数中的数据相关项进行训练（这通常是在经典架构中进行的），也可以通过数据和方程相关项同时训练（这就是我刚刚提到的物理信息神经网络），那么它一定也可以被训练来最小化*仅仅*方程相关的项。这正是我们将要做的！这里使用的唯一损失函数将是
    *L_equation*。希望下面的图示能帮助说明我刚才说的内容：今天我们目标是右下角类型的模型，即我们的微分方程求解神经网络。
- en: '![](../Images/b5397415a3249d038332a3d4300f1e2f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5397415a3249d038332a3d4300f1e2f.png)'
- en: 'Figure 1: diagram showing the kinds of neural networks with respect to their
    loss functions. In this article, we are aiming for the right-bottom one. Image
    by author.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：展示不同神经网络类型与其损失函数关系的图示。在本文中，我们的目标是右下角的模型。图片由作者提供。
- en: Code implementation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码实现
- en: To showcase the theoretical learnings we've just got, I will implement the proposed
    solution in Python code, using the PyTorch library for machine learning.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们刚刚学到的理论，我将使用 Python 代码实现提出的解决方案，并使用 PyTorch 库进行机器学习。
- en: 'The first thing to do is to create a neural network architecture:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是创建一个神经网络架构：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This one is just a simple MLP with LeakyReLU activation functions. Then, I
    will define the loss functions to calculate them later during the training loop:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个简单的 MLP，使用 LeakyReLU 激活函数。然后，我将定义损失函数，在训练循环中稍后进行计算：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we shall create a time array that will be used as train data, and instantiate
    the model, and also choose an optimization algorithm:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个时间数组，作为训练数据，并实例化模型，同时选择优化算法：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, let''s start our training loop:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们开始训练循环：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice the use of `torch.autograd.grad` function to automatically differentiate
    the output *y_pred* with respect to the input *t* to compute the loss function.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意使用 `torch.autograd.grad` 函数自动对输出 *y_pred* 关于输入 *t* 进行求导，以计算损失函数。
- en: Results
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: After training, we can see that the loss function rapidly converges. Fig. 2
    shows the loss function plotted against the epoch number, with an inset showing
    the region where the loss function has its fastest drop.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以看到损失函数迅速收敛。图 2 显示了损失函数随训练轮数变化的图像，插图展示了损失函数下降最快的区域。
- en: '![](../Images/84b3cb68272023de4d8a4d0c14124f5b.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84b3cb68272023de4d8a4d0c14124f5b.png)'
- en: 'Figure 2: Loss function by epochs. On the inset, we can see the region of most
    rapid convergence. Image by author.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：按历时（epochs）变化的损失函数。在插图中，我们可以看到收敛最为迅速的区域。图像来源：作者。
- en: You probably have noticed that this neural network is not a common one. It has
    no train data (our train data was a hand-crafted vector of timestamps, which is
    simply the time domain that we wanted to investigate), so all information it gets
    from the system comes in the form of a loss function. Its only purpose is to solve
    a differential equation within the time domain it was crafted to solve. Hence,
    to test it, it's only fair that we use the time domain it was trained on. Fig.
    3 shows a comparison between the NN prediction and the theoretical answer (that
    is, the analytical solution).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，这个神经网络并不是一个常规的神经网络。它没有训练数据（我们的训练数据是一个手工制作的时间戳向量，简单来说就是我们希望研究的时间域），因此它从系统中获得的所有信息都以损失函数的形式呈现。它唯一的目的就是在它被设计用来解决的时间域内求解微分方程。因此，要测试它，公正的方法是使用它训练时的时间域。图3显示了神经网络预测与理论答案（即解析解）之间的对比。
- en: '![](../Images/d6fa652093b89341224469a1ca0dfa2d.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6fa652093b89341224469a1ca0dfa2d.png)'
- en: 'Figure 3: Neural network prediction and the analytical solution prediction
    of the differential equation shown. Image by author.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：神经网络预测与微分方程解析解的预测对比。图像来源：作者。
- en: We can see a pretty good agreement between the two, which is very good for the
    neural network.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到两者之间的高度一致性，这对于神经网络来说非常好。
- en: One caveat of this approach is that it does not generalize well for future times.
    Fig. 4 shows what happens if we slide our time data points five steps ahead, and
    the result is simply mayhem.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个警告是，它对于未来的时间不太适用。图4显示了如果我们将时间数据点向前滑动五步会发生什么，结果就是一片混乱。
- en: '![](../Images/47df1064835a3b04232af56c64966f12.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47df1064835a3b04232af56c64966f12.png)'
- en: 'Figure 4: Neural network and analytical solution for unseen data points. Image
    by author.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：神经网络和解析解对于未见数据点的预测。图像来源：作者。
- en: Hence, the lesson here is that this approach is made to be a numerical solver
    for differential equations within a time domain, and it should not be used as
    a regular neural network to make predictions with unseen out-of-train-domain data
    and expect it to generalize well.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这里得到的教训是，这种方法是作为一个数值解法器，用于在时间域内解决微分方程，不能被当作常规神经网络用来对未见的、超出训练域的数据进行预测并期望它能很好地泛化。
- en: Conclusion
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'After all, one remaining question is:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 毕竟，剩下的一个问题是：
- en: '*Why bother to train a neural network that does not generalize well to unseen
    data, and on top of that is obviously worse than the analytical solution, since
    it has an intrinsic statistical error?*'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*为什么要训练一个不能很好地泛化到未见数据的神经网络，并且更糟糕的是，它显然比解析解更差，因为它具有内在的统计误差？*'
- en: First, the example provided here was an example of a differential equation whose
    analytical solution is known. For unknown solutions, numerical methods must be
    used nevertheless. With that being said, numerical methods for differential equation
    solving usually accumulate error. That means if you try to solve the equation
    for many time steps, the solution will lose its accuracy along the way. The neural
    network solver, on the other hand, learns how to solve the DE for all data points
    at each of its training epochs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这里提供的示例是一个已知解析解的微分方程的例子。对于未知解，仍然必须使用数值方法。然而，正如所说，微分方程求解的数值方法通常会积累误差。这意味着如果你尝试在多个时间步长中求解方程，解的准确性会随着时间的推移而丧失。另一方面，神经网络解法器学习如何在每个训练历时中为所有数据点求解微分方程。
- en: Another reason is that neural networks are good interpolators, so if you want
    to know the value of the function in unseen data (but this "unseen data" has to
    lie within the time interval you trained) the neural network will promptly give
    you a value that classic numeric methods will not be able to promptly give.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个原因是，神经网络擅长插值，因此，如果你想知道未见数据中函数的值（但这个“未见数据”必须位于你训练过的时间区间内），神经网络将迅速给出一个值，而经典的数值方法则无法迅速提供。
- en: References
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Marios Mattheakis et al., [Hamiltonian neural networks for solving equations
    of motion](https://arxiv.org/abs/2001.11107), *arXiv preprint arXiv:2001.11107v5*,
    2022.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Marios Mattheakis 等，[用于求解运动方程的哈密顿神经网络](https://arxiv.org/abs/2001.11107)，*arXiv预印本
    arXiv:2001.11107v5*，2022年。'
- en: '[2] Mario Dagrada, [Introduction to Physics-informed Neural Networks](/solving-differential-equations-with-neural-networks-afdcf7b8bcc4),
    2022.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Mario Dagrada, [介绍物理信息神经网络](/solving-differential-equations-with-neural-networks-afdcf7b8bcc4)，2022年。'
