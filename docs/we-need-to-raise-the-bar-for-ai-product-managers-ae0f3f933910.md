# 我们需要提高人工智能产品经理的标准

> 原文：[https://towardsdatascience.com/we-need-to-raise-the-bar-for-ai-product-managers-ae0f3f933910?source=collection_archive---------2-----------------------#2024-08-09](https://towardsdatascience.com/we-need-to-raise-the-bar-for-ai-product-managers-ae0f3f933910?source=collection_archive---------2-----------------------#2024-08-09)

## 如何停止怪罪“模型”，并开始构建成功的人工智能产品

[](https://medium.com/@4thewinn?source=post_page---byline--ae0f3f933910--------------------------------)[![Julia Winn](../Images/9ca44e7be7c308a0bcaf797c6fa76a8c.png)](https://medium.com/@4thewinn?source=post_page---byline--ae0f3f933910--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ae0f3f933910--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ae0f3f933910--------------------------------) [Julia Winn](https://medium.com/@4thewinn?source=post_page---byline--ae0f3f933910--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ae0f3f933910--------------------------------) ·8分钟阅读·2024年8月9日

--

![](../Images/26d8d435ece79e6a1765a675a88a6d52.png)

由作者使用Midjourney生成的图像

产品经理负责决定构建什么，并对其决策的结果负责。这适用于所有类型的产品，包括那些由人工智能驱动的产品。然而，在过去十年中，产品经理们通常将人工智能模型视为“黑箱”，将不良结果的责任推给模型开发者。

> *产品经理：我不知道模型为什么会这样做，去问模型开发者。*

这种行为就像在网站重新设计后，怪罪设计师导致用户注册量下降一样没有道理。科技公司假设负责消费产品的产品经理能够凭直觉做出关于设计变更的明智决策，并对结果负有*责任*。

那么，为什么这种不干涉的人工智能方法成为了常态呢？

问题：产品经理被激励保持与模型开发过程的距离。

# 实践型与不干涉型人工智能产品管理——概览

这种更为严格的实践型方法有助于确保模型能够成功落地，并为用户提供最佳体验。

实践型方法需要：

+   更多的技术知识和理解。

+   承担更多风险和责任，特别是对于推出时已知的问题或权衡。

+   需要2到3倍的时间和精力——创建评估数据集以系统地衡量模型行为可能需要从几小时到几周的时间。

不确定什么是“评估”（eval）吗？请查看我关于[什么是“评估”，产品经理为什么应该关注它？](https://medium.com/towards-data-science/what-exactly-is-an-eval-and-why-should-product-managers-care-b596dca275a7)的文章。

九成情况下，当一个模型的推出失败时，使用的是放手型方法。在那些具有长时间AI产品应用历史的大公司中，如Netflix、Google、Meta和Amazon，情况较少发生，但本文并非针对这些公司。

然而，克服放手型方法的惯性可能是具有挑战性的。尤其是当公司领导层没有期望更多时，PM在采用实践型方法时，甚至可能会因为“减缓”开发周期而面临反对意见。

# 实践型与放手型产品管理 — 模型开发过程

想象一下，亚马逊等市场平台上的PM负责为父母开发一个产品捆绑推荐系统。考虑这两种方法。

## **放手型AI PM — 模型要求**

目标：增加购买量。

评估：无论模型开发者认为什么是最好的。

指标：通过A/B测试来决定是否将其推送给100%的用户，如果购买率有显著的统计学意义上的提升。

## **实践型AI PM — 模型要求**

目标：帮助父母发现他们未曾意识到的优质产品，进而使他们的育儿旅程更加轻松。

指标：主要指标是推动年轻父母购买产品。我们将监控的次要长期指标是首次通过捆绑包发现的品牌的复购率，以及市场中品牌多样性随时间变化的情况。

评估：除了进行A/B测试外，我们的离线评估集将会查看多个样本用户在育儿不同关键阶段（优先考虑怀孕期、新生儿、婴儿、幼儿、儿童）以及四个收入区间的样本推荐。如果我们在这里看到任何意外情况（例如：低收入家庭推荐最昂贵的产品），我们需要更仔细地审视训练数据和模型设计。

在我们的评估集里，我们将考虑：

+   个性化 — 查看有多少人购买了相同的产品。我们预计不同收入和不同年龄段的孩子会有所不同。

+   避免冗余 — 如果捆绑包中已经有一个相同的物品，或者用户已经从我们这里购买了此类物品（如婴儿床、瓶子加热器等耐用商品），则应对重复推荐进行惩罚（不要对消耗品如尿布或收藏品如玩具进行惩罚）。

+   连贯性 — 不同阶段的产品不应混合（例如：婴儿奶瓶和2岁儿童衣物）。

+   一致性 — 避免将完全不同的产品混合，例如：超级昂贵的手工木制玩具与非常便宜的塑料玩具，带有授权角色的鲜艳印花与柔和的粉彩。

次要目标的可能驱动因素

+   考虑为重复购买的产品设置额外权重进行实验。即使我们前期卖出的捆绑包稍少一些，如果意味着购买者未来更有可能购买更多产品，这也是一种值得的权衡。

+   为了支持市场的长期健康发展，我们不希望只偏向畅销品。在进行质量检查的同时，确保至少10%的推荐产品不是该类别中的第一名品牌。如果从一开始没有做到这一点，模型可能会默认采用“最低公分母”行为，很可能没有进行适当的个性化推荐。

## **实践中的AI产品管理——模型开发者协作**

模型架构应该由模型开发者决定，但产品经理应该在以下方面有较强的发言权：

+   模型优化的目标是什么（这应该比“更多购买”或“更多点击”深入一两层）。

+   模型性能将如何评估。

+   用于[评估](https://medium.com/towards-data-science/what-exactly-is-an-eval-and-why-should-product-managers-care-b596dca275a7)的例子。

实践型方法客观上需要更多的工作！而且这还假设产品经理从一开始就参与到模型开发过程中。有时模型开发者具有良好的产品经理直觉，并能够在模型设计中考虑用户体验。然而，一家公司永远不应该指望这一点，因为在实际情况中，一个对用户体验非常敏感的模型开发者是千分之一的“独角兽”。

此外，放手型方法可能仍然在*某些*时候*有些*有效。然而，实际上这通常会导致：

+   亚最佳模型性能，可能会导致项目失败（例如：高管们得出结论，捆绑包只是个糟糕的主意）。

+   错失了显著提升的机会（例如：提升3%而不是15%）。

+   对生态系统的长期影响未加监控（例如：小品牌离开平台，导致对少数大型玩家的依赖加重）。

# 实践型与放手型产品管理——产品评审

除了前期工作更多，实践型方法还能从根本上改变产品评审流程。

## **放手型AI产品经理评审**

*领导者：为父母设计的捆绑包似乎是个好主意。让我们看看它在A/B测试中的表现。*

## **实践中的AI产品经理评审**

*领导者：我读了你的提案。如果这些畅销品真的是最好的产品，那只推荐它们有什么问题呢？我们不应该做对用户最有利的事情吗？*

*[半小时辩论后]*

*产品经理：正如你所看到的，畅销品并不一定适合每个人。以尿布为例，低收入家庭应该了解亚马逊品牌尿布，它的价格是畅销品的一半。高收入家庭则应该了解新兴的昂贵品牌，它深受富裕顾客喜爱，因为它用起来就像云一样。而且，如果我们总是偏向已有的类目赢家，从长远来看，新兴但更好的产品将难以崭露头角。*

*领导者：好吧。我只是想确保我们不会不小心推荐一个糟糕的产品。你提议的质量控制指标是什么，确保这种情况不会发生？*

*模型开发者：为了确保只展示高质量的产品，我们使用以下信号……*

## **无所作为的AI产品管理隐藏成本**

上述对比情景揭示了AI产品管理中的一个关键节点。虽然积极主动的产品经理成功应对了一个具有挑战性的对话，但这种方法并非没有风险。许多产品经理在面临快速交付的压力时，可能会选择最省力的路径。

毕竟，放手的做法承诺更顺畅的产品评审、更快速的审批流程，并且如果事情出错，还能找到一个方便的替罪羊（模型开发者）。然而，这种短期的轻松背后，却隐藏着巨大的长期成本，对产品和整个组织而言都是如此。

当产品经理不深入参与AI开发时，显而易见的问题和关键的权衡决策将被掩盖，进而导致一系列重大后果，包括：

1.  目标不一致：如果产品经理无法深入了解用户需求和业务目标，模型开发者可能会只优化那些容易衡量的指标（比如点击率），而非真正的用户价值。

1.  意外的生态系统影响：孤立优化的模型可能会带来深远的后果。例如，始终推荐畅销产品可能会逐渐将小品牌推出市场，减少多样性，甚至可能危害长期的平台健康。

1.  责任扩散：当决策“交由模型”时，就会形成一个危险的责任真空。产品经理和领导者不能为他们从未明确考虑或批准的结果负责。这种缺乏明确责任的情况，可能会导致一种文化，没人觉得有权主动解决问题，最终可能导致小问题变成大危机。

1.  劣质模型的延续：如果产品经理没有通过产品视角仔细审视模型的不足之处，最高影响的改进就无法被识别和优先处理。承认并承担这些不足，对于团队在发布时做出正确的权衡决策至关重要。如果没有这种态度，表现不佳的模型将成为常态。这种回避的循环会阻碍模型的进化，浪费AI推动真实用户和业务价值的潜力。

产品经理可以采取的第一步，变得更积极主动？问问你的模型开发者，你如何能帮助评估！有许多优秀的免费工具可以帮助这个过程，比如 [promptfoo](https://www.promptfoo.dev/)（[Shopify首席执行官的最爱](https://x.com/tobi/status/1795458221408125195)）。

# 领导力必要性：重新定义期望

产品领导在提升AI产品标准方面扮演着至关重要的角色。正如UI变更需要经过多轮评审一样，AI模型也需要同等，甚至更严格的审查，因为它们对用户体验和长期产品结果的影响深远。

促进产品经理与模型开发更深度参与的第一步，是让他们对了解自己正在交付的内容负责。

提出以下问题：

+   您使用了什么评估方法？您是如何收集示例的？我可以查看样本结果吗？

+   您认为哪些使用场景对于第一版的支持最为重要？为了实现这一点，我们是否需要做出某些权衡？

在使用不同的评估方法时，请考虑它们的适用场景：

+   对于部署在高风险场景中的模型，考虑将使用评估集作为要求。这还应该与严格的后期影响和行为分析相结合，尽可能深入到漏斗的各个环节。

+   对于部署在低风险场景中的模型，考虑允许快速首次发布，进行较不严格的评估，但在收集到用户行为数据后，推动快速的后期迭代。

+   调查模型训练和评分中的反馈循环，确保人工监督不仅仅局限于精度/召回率指标。

并且记住，迭代是关键。初始发布的模型通常不应是最终版本。确保后续工作的资源得到保障。

最终，人工智能的广泛应用带来了巨大的前景，也对产品所有权的定义带来了重大变化。为了充分发挥其潜力，我们必须超越过去常常导致次优结果的放手态度。产品领导者在这一转变中扮演着关键角色。通过要求产品经理更深入地理解AI模型，并培养一种问责制文化，我们可以确保AI产品被深思熟虑地设计、严格地测试，并真正造福用户。这需要许多团队提升技能，但相关资源已经触手可及。人工智能的未来取决于此。
