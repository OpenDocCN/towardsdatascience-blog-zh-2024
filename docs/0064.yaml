- en: Deploying LLM Apps to AWS, the Open-Source Self-Service Way
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deploying-llm-apps-to-aws-the-open-source-self-service-way-c54b8667d829?source=collection_archive---------3-----------------------#2024-01-08](https://towardsdatascience.com/deploying-llm-apps-to-aws-the-open-source-self-service-way-c54b8667d829?source=collection_archive---------3-----------------------#2024-01-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A step-by-step guide on deploying LlamaIndex RAGs to AWS ECS fargate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@wenqiglantz?source=post_page---byline--c54b8667d829--------------------------------)[![Wenqi
    Glantz](../Images/65b518863e01aaa48ecc6b8ac6d1be60.png)](https://medium.com/@wenqiglantz?source=post_page---byline--c54b8667d829--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c54b8667d829--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c54b8667d829--------------------------------)
    [Wenqi Glantz](https://medium.com/@wenqiglantz?source=post_page---byline--c54b8667d829--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c54b8667d829--------------------------------)
    ·12 min read·Jan 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac5c10a125c2b07b9900b6449740930c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by DALL-E 3 by the author
  prefs: []
  type: TYPE_NORMAL
- en: LLM applications, when developed to use third-party hosted LLMs such as OpenAI,
    do not require MLOps overhead. Such containerized LLM-powered apps or microservices
    can be deployed with DevOps practices. In this article, let’s explore how to deploy
    our LLM app to a cloud provider such as AWS, fully automated with infrastructure
    and application pipelines. LlamaIndex has a readily made [RAGs chatbot](https://github.com/run-llama/rags)
    for the community. Let’s use RAGs as the sample app to deploy.
  prefs: []
  type: TYPE_NORMAL
- en: IaC Self-Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IaC, short for Infrastructure as Code, automates infrastructure provisioning,
    ensuring that configurations are consistent and repeatable. There are many tools
    to accomplish IaC. We will focus on [HashiCorp’s Terraform](https://www.hashicorp.com/products/terraform)
    in this article, mainly because Terraform is cloud-agnostic.
  prefs: []
  type: TYPE_NORMAL
- en: The primary purpose of IaC self-service is to empower developers with more access,
    control, and ownership over their pipelines to boost productivity.
  prefs: []
  type: TYPE_NORMAL
- en: For those interested, I wrote a [5-part series](https://medium.com/@wenqiglantz/the-path-to-devops-self-service-a-five-part-series-5ea5d4552f9e?sk=e5666549c3cabbc619774f4b7b2a2747)
    on DevOps self-service model about a year ago to detail all aspects related to
    a DevOps self-service model.
  prefs: []
  type: TYPE_NORMAL
- en: High-Level Deployment Diagram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
