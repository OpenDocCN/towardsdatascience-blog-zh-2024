- en: 'Machine Learning Algorithms as a Mapping Between Spaces: From SVMs to Manifold
    Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/machine-learning-algorithms-as-a-mapping-between-spaces-from-svms-to-manifold-learning-b1dfe1046e4f?source=collection_archive---------3-----------------------#2024-02-07](https://towardsdatascience.com/machine-learning-algorithms-as-a-mapping-between-spaces-from-svms-to-manifold-learning-b1dfe1046e4f?source=collection_archive---------3-----------------------#2024-02-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring the beauty of mapping between spaces in SVMs, autoencoders, and manifold
    learning (isomaps) algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@salih.salih?source=post_page---byline--b1dfe1046e4f--------------------------------)[![Salih
    Salih](../Images/220f3c5363989d94c5593eca7ff72c67.png)](https://medium.com/@salih.salih?source=post_page---byline--b1dfe1046e4f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b1dfe1046e4f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b1dfe1046e4f--------------------------------)
    [Salih Salih](https://medium.com/@salih.salih?source=post_page---byline--b1dfe1046e4f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b1dfe1046e4f--------------------------------)
    ·12 min read·Feb 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/85f97ec0087c788a144bd88bd57fa8ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by Evgeni Tcherkasski on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, understanding how algorithms process, interpret, and classify
    data relies heavily on the concept of “spaces.” In this context, a space is a
    mathematical construct where data points are positioned based on their features.
    Each dimension in the space represents a specific attribute or feature of the
    data, allowing algorithms to navigate a structured representation.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Space and Input Space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The journey begins in the feature or input space, where each data point is a
    vector representing an instance in the dataset. To simplify, imagine an image
    where each pixel is a dimension in this space. The complexity and dimensionality
    of the space depend on the number and nature of the features. Working with high-dimensional
    spaces can be either enjoyable or frustrating for data practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in Low-dimensional Spaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In low-dimensional spaces, not all relationships or patterns in the data are
    easily identifiable. Linear separability, which is the ability to divide classes
    with a simple linear boundary, is often unachievable. This limitation becomes
    more apparent in complex datasets where the interaction of features creates non-linear
    patterns that cannot be captured by simple linear models.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore machine learning algorithms in the perspective
    of mapping and interaction between different spaces. We will start with support
    vector machines (SVMs) as an example of simplicity, then move on to autoencoders,
    and finally, we will discuss manifold learning and Isomaps.
  prefs: []
  type: TYPE_NORMAL
- en: '**Please note that the code examples in this article are for demonstration
    and may not be optimized. I encourage you to modify, improve and try the code
    with different datasets to deepen your understanding and gain further insights.**'
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Support Vector Machines (SVMs) are known machine learning algorithms that excel
    at classifying data. As we mentioned at the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: In lower dimensions, linear separability is often impossible, which means it’s
    difficult to divide classes with a simple linear boundary.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: SVMs overcomes this difficulty by transforming data into a higher-dimensional
    space, making it easier to separate and classify. To illustrate this, let’s look
    at an example. The code below generates synthetic data that is clearly not linearly
    separable in its original space.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b7e601d9fede398564838a0b0ec32f82.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: SVMs use a mapping between spaces to separate different classes. They lift the
    data from a lower dimensional space to a higher dimensional one. In this new space,
    SVMs find the optimal hyperplane, which is a decision boundary that separates
    the classes. It’s like finding the perfect line that divides groups in a two-dimensional
    graph, but in a more complex, multidimensional universe.
  prefs: []
  type: TYPE_NORMAL
- en: In the provided data, one class is close to the origin and another class is
    far from the origin. Let’s look at a typical example to understand how this data
    becomes separable when transformed into higher dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: We will transform each 2D point (x, y) to a 3D point (x, y, z), where z = x²
    + y². The transformation adds a new third dimension based on the squared distance
    from the origin in the 2D space. Points that are farther from the origin in the
    2D space will be higher in the 3D space because their squared distance is larger.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2a47086d619d1e375b7a139452c47163.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: You can notice from the output above that after this transformation, our data
    becomes linearly separable by a 2D hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is the effectiveness of drug dosages. A patient is only cured
    if the dosage falls within a certain range. Dosages that are too low or too high
    are ineffective. This scenario naturally creates a dataset that is not linearly
    separable, making it a good candidate for demonstrating how a polynomial kernel
    can help.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5f9d86f94306290f327605ff250d64af.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'In the two examples above, we take advantage of our knowledge about the data.
    For instance, in first example we know that we have two classes: one close to
    the origin and another far from the origin. This is what the algorithm does through
    training and fine-tuning — it finds a suitable space where the data can be linearly
    separated.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The great thing here is that SVMs don’t map data into higher dimensions as this
    would be very complex computationally. Instead, they compute the relationship
    between the data as if it were in higher dimensions using the dot product. This
    is called the “Kernel trick.” I will explain SVM kernels in another article.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoencoders are truly amazing and beautiful architectures that capture my imagination.
    They have a wide range of applications across various domains, utilizing diverse
    types of autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: 'They basically consist of an **encoder** and a **decoder**, the encoder takes
    your input and encode/compress it, a process in which we move from a high-dimensional
    space to a more compact, lower-dimensional one. What’s truly interesting is how
    the decoder then takes this condensed representation and reconstructs the original
    data in the higher-dimensional space. The natural question is: how is it possible
    to go back to the original space from a significantly reduced dimension?'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider an HD image with a resolution of 720x720 pixels. Storing and
    transmitting this image requires a lot of memory and bandwidth. Autoencoders solve
    this problem by compressing the image into a lower-dimensional space, like a 32x32
    representation called the ‘bottleneck’. The encoder’s job is done at this point.
    The decoder takes over, trying to rebuild the original image from this compressed
    form.
  prefs: []
  type: TYPE_NORMAL
- en: This process is similar to sharing images on platforms like WhatsApp. The image
    is encoded to a lower quality for transmission and then decoded on the receiver’s
    end. The difference in quality between the original and received image is called
    ‘reconstruction error’, which is common in autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: 'In autoencoders, we can think of it as an interaction between 3 spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: The input space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The latent representation space (the bottleneck).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The beauty here is that we can see the autoencoder as something that operates
    in these 3 spaces. It takes advantage of the latent spaces to remove any noisy
    or unnecessary information from the input space, resulting in a very compact representation
    with core information about the input space. It does this by trying to mirror
    the input space in the output space, reducing the difference between the two spaces
    or the reconstruction error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolutional Autoencoders: Encoding Complexity into Simplicity'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code below shows an example of a convolutional autoencoder, which is a type
    of autoencoders that works well with images. We will use the popular MNIST dataset[LeCun,
    Y., Cortes, C., & Burges, C.J. (1998). The MNIST Database of Handwritten Digits.
    [Retrieved from TensorFlow](https://www.tensorflow.org/datasets/catalog/mnist),
    CC BY 4.0], which contains 28x28 pixel grayscale images of handwritten digits.
    The encoder plays a crucial role by reducing the dimensionality of the data from
    784 elements to a smaller, more condensed form. The decoder then aims to reconstruct
    the original high-dimensional data from this lower-dimensional representation.
    However, this reconstruction is not perfect and some information is lost. The
    autoencoder overcomes this challenge by learning to prioritize the most important
    features of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3e836d1a5deab08a354758f18b8d0b4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'The output above shows how well the autoencoder works. It displays pairs of
    images: the original digit images and their reconstructions after encoding and
    decoding. This example proves that the encoder captures the essence of the data
    in a smaller form and the decoder can approximate the original image, even though
    some information is lost during compression.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s go further and visualize the learned latent space (the bottleneck).
    We will use PCA and t-SNE, two techniques to reduce dimensions, to show the compressed
    data points on a 2D plane. This step is important because it helps us see how
    the autoencoder organizes the data in the latent space and shows any natural clusters
    of similar digits. We used PCA and t-SNE together just to compare how well they
    work.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/849a0320906aa2409e41565a0f82a497.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the two resulted graphs, t-SNE is better than PCA at separating different
    classes of digits in the latent space visualization(it captures non-linearity).
    It creates distinct clusters with minimal overlap between classes. The autoencoder
    compresses images into a lower dimensional space but still captures enough information
    to distinguish between different digits, as shown in the t-SNE graph.
  prefs: []
  type: TYPE_NORMAL
- en: An important note here is that t-SNE is a non-linear technique used for visualizing
    high-dimensional data. It preserves local data structures, making it useful for
    identifying clusters and patterns visually. However, it is not typically used
    for feature reduction in machine learning.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**But what does this autoencoder probably learn?**'
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, one can say that an autoencoder like this learns the basic
    and simple edges and textures, moving to parts of the digits like loops and lines
    and how they are arranged, and finally understanding whole digits(hierarchical
    characteristics), all this while capturing the unique essence of each digit in
    a compact form. It can guess missing parts of an image and recognizes common patterns
    in how digits are written.
  prefs: []
  type: TYPE_NORMAL
- en: 'Manifold learning: The Blessing of Non-Uniformity'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a previous article titled [Curse of Dimensionality: An Intuitive Exploration](/curse-of-dimensionality-an-intuitive-exploration-1fbf155e1411),
    I explored the concept of the “Curse of dimensionality”, which refers to the problems
    and challenges that arises when working with data in higher dimensions, making
    the job of ML algorithms harder in many ways.'
  prefs: []
  type: TYPE_NORMAL
- en: Here come the manifold learning algorithms, driven by the blessing of non-uniformity,
    the uneven distribution or variation of data points within a given space or dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental assumption underlying manifold learning is that high-dimensional
    data actually lies on or near a lower-dimensional manifold within the high-dimensional
    space. This concept is based on the idea that although the data might exist in
    a high-dimensional space due to the way it’s measured or recorded, the intrinsic
    dimensions that effectively describe the data and its structure are much lower.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s generate the famous Swiss roll dataset and use it as an example of non-uniformity
    in higher-dimensional spaces. In its original form, this dataset looks like a
    chaotic mess of data points. But beneath this chaos, there is hidden order — a
    low-dimensional structure that includes the important features of the data. Manifold
    learning techniques, like Isomaps, take advantage of this non-uniformity. By mapping
    data points from the high-dimensional space to a lower-dimensional one, Isomap
    shows us the intrinsic shape of the Swiss roll. It keeps the richness of the original
    data while revealing the underlying structure — a 2D projection that captures
    the non-uniformity of the high-dimensional space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3a4a17ae7e0217abc2d3e0597d1dec59.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the output above:'
  prefs: []
  type: TYPE_NORMAL
- en: We have two colorful illustrations. On the left, there’s a 3D Swiss roll with
    a rainbow of colors that spiral together. It shows how each shade transitions
    into the next, marking a path through the roll.
  prefs: []
  type: TYPE_NORMAL
- en: Now, on the right. There’s a 2D spread of the same colors. Even though the shape
    has changed the order and flow of colors still tell the same story of the original
    data. The order and connections between points are preserved, as if the Swiss
    roll was carefully unrolled onto a flat surface so we can see the entire pattern
    at once.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article started by exploring the concept of spaces, which are the mathematical
    constructs where data points are positioned based on their features/attributes.
    We examined how **Support Vector Machines (SVMs)** leverage the idea of mapping
    data into higher-dimensional spaces to address the challenge of **non-linear separability**
    in lower spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Then we moved on to **autoencoders**, an elegant and truly beautiful architecture
    that maps between **3 spaces**, the **input space**, that gets compressed to a
    much lower **latent representation(the bottleneck)**, and then comes the decoder
    to take the lead aiming to reconstruct the original input from this lower representation
    **while minimizing the reconstruction error**.
  prefs: []
  type: TYPE_NORMAL
- en: We also explored **manifold learning**, and the blessing that we get from non-uniformity
    as a way to overcome the **curse of dimensionality** by simplifying complex datasets
    without losing important details.
  prefs: []
  type: TYPE_NORMAL
- en: If you made it this far, I would like to thank you for your time reading this,
    I hope you found it enjoyable and useful, please feel free to point out any mistakes
    or misconceptions in my article, your feedback and suggestions are also greatly
    appreciated.
  prefs: []
  type: TYPE_NORMAL
