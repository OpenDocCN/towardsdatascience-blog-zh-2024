- en: 'Machine Learning Algorithms as a Mapping Between Spaces: From SVMs to Manifold
    Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习算法作为空间之间的映射：从支持向量机（SVM）到流形学习
- en: 原文：[https://towardsdatascience.com/machine-learning-algorithms-as-a-mapping-between-spaces-from-svms-to-manifold-learning-b1dfe1046e4f?source=collection_archive---------3-----------------------#2024-02-07](https://towardsdatascience.com/machine-learning-algorithms-as-a-mapping-between-spaces-from-svms-to-manifold-learning-b1dfe1046e4f?source=collection_archive---------3-----------------------#2024-02-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/machine-learning-algorithms-as-a-mapping-between-spaces-from-svms-to-manifold-learning-b1dfe1046e4f?source=collection_archive---------3-----------------------#2024-02-07](https://towardsdatascience.com/machine-learning-algorithms-as-a-mapping-between-spaces-from-svms-to-manifold-learning-b1dfe1046e4f?source=collection_archive---------3-----------------------#2024-02-07)
- en: Exploring the beauty of mapping between spaces in SVMs, autoencoders, and manifold
    learning (isomaps) algorithms
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索支持向量机（SVM）、自编码器和流形学习（等距映射）算法中空间之间映射的美
- en: '[](https://medium.com/@salih.salih?source=post_page---byline--b1dfe1046e4f--------------------------------)[![Salih
    Salih](../Images/220f3c5363989d94c5593eca7ff72c67.png)](https://medium.com/@salih.salih?source=post_page---byline--b1dfe1046e4f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b1dfe1046e4f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b1dfe1046e4f--------------------------------)
    [Salih Salih](https://medium.com/@salih.salih?source=post_page---byline--b1dfe1046e4f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@salih.salih?source=post_page---byline--b1dfe1046e4f--------------------------------)[![Salih
    Salih](../Images/220f3c5363989d94c5593eca7ff72c67.png)](https://medium.com/@salih.salih?source=post_page---byline--b1dfe1046e4f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b1dfe1046e4f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b1dfe1046e4f--------------------------------)
    [Salih Salih](https://medium.com/@salih.salih?source=post_page---byline--b1dfe1046e4f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b1dfe1046e4f--------------------------------)
    ·12 min read·Feb 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b1dfe1046e4f--------------------------------)
    ·12分钟阅读·2024年2月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/85f97ec0087c788a144bd88bd57fa8ae.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85f97ec0087c788a144bd88bd57fa8ae.png)'
- en: Photo by Evgeni Tcherkasski on Unsplash
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Evgeni Tcherkasski，Unsplash
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In machine learning, understanding how algorithms process, interpret, and classify
    data relies heavily on the concept of “spaces.” In this context, a space is a
    mathematical construct where data points are positioned based on their features.
    Each dimension in the space represents a specific attribute or feature of the
    data, allowing algorithms to navigate a structured representation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，理解算法如何处理、解释和分类数据，通常依赖于“空间”这一概念。在这种情况下，空间是一个数学结构，数据点根据其特征在空间中定位。空间中的每个维度代表数据的一个特定属性或特征，使得算法能够在结构化的表示中进行导航。
- en: Feature Space and Input Space
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征空间与输入空间
- en: The journey begins in the feature or input space, where each data point is a
    vector representing an instance in the dataset. To simplify, imagine an image
    where each pixel is a dimension in this space. The complexity and dimensionality
    of the space depend on the number and nature of the features. Working with high-dimensional
    spaces can be either enjoyable or frustrating for data practitioners.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这段旅程始于特征空间或输入空间，在这个空间中，每个数据点都是一个向量，代表数据集中的一个实例。简化一下，想象一张图像，其中每个像素是这个空间中的一个维度。空间的复杂性和维度取决于特征的数量和性质。在高维空间中工作，对数据从业者来说既可以是令人愉快的，也可以是令人沮丧的。
- en: Challenges in Low-dimensional Spaces
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低维空间中的挑战
- en: In low-dimensional spaces, not all relationships or patterns in the data are
    easily identifiable. Linear separability, which is the ability to divide classes
    with a simple linear boundary, is often unachievable. This limitation becomes
    more apparent in complex datasets where the interaction of features creates non-linear
    patterns that cannot be captured by simple linear models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在低维空间中，并非所有的数据关系或模式都能容易地被识别。线性可分性，即通过简单的线性边界将不同类别分开，通常是无法实现的。这一局限性在复杂数据集中尤为明显，因为特征之间的交互产生了非线性模式，而这些模式无法通过简单的线性模型捕捉。
- en: In this article, we will explore machine learning algorithms in the perspective
    of mapping and interaction between different spaces. We will start with support
    vector machines (SVMs) as an example of simplicity, then move on to autoencoders,
    and finally, we will discuss manifold learning and Isomaps.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将从不同空间之间的映射和交互的角度来探索机器学习算法。我们将以支持向量机（SVM）作为一个简单的例子开始，然后介绍自编码器，最后讨论流形学习和等距映射（Isomaps）。
- en: '**Please note that the code examples in this article are for demonstration
    and may not be optimized. I encourage you to modify, improve and try the code
    with different datasets to deepen your understanding and gain further insights.**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**请注意，本文中的代码示例仅用于演示，可能没有经过优化。我鼓励你修改、改进并尝试使用不同的数据集运行代码，以加深理解并获得更多的见解。**'
- en: Support Vector Machines
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: 'Support Vector Machines (SVMs) are known machine learning algorithms that excel
    at classifying data. As we mentioned at the beginning:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）是已知的机器学习算法，在数据分类方面表现优异。正如我们在开头提到的：
- en: In lower dimensions, linear separability is often impossible, which means it’s
    difficult to divide classes with a simple linear boundary.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在低维空间中，线性可分性通常是不可能的，这意味着用简单的线性边界划分类别非常困难。
- en: SVMs overcomes this difficulty by transforming data into a higher-dimensional
    space, making it easier to separate and classify. To illustrate this, let’s look
    at an example. The code below generates synthetic data that is clearly not linearly
    separable in its original space.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: SVM通过将数据转换为更高维的空间，克服了这一困难，从而使得数据更容易分离和分类。为了说明这一点，下面的代码生成了在原始空间中明显无法线性分离的合成数据。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/b7e601d9fede398564838a0b0ec32f82.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7e601d9fede398564838a0b0ec32f82.png)'
- en: Image by Author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: SVMs use a mapping between spaces to separate different classes. They lift the
    data from a lower dimensional space to a higher dimensional one. In this new space,
    SVMs find the optimal hyperplane, which is a decision boundary that separates
    the classes. It’s like finding the perfect line that divides groups in a two-dimensional
    graph, but in a more complex, multidimensional universe.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）通过空间之间的映射来分离不同的类别。它们将数据从低维空间提升到高维空间。在这个新空间中，SVM找到最优的超平面，这是一个决策边界，用于分离类别。这就像是在二维图中找到完美的直线来划分群体，但在更复杂的多维宇宙中进行。
- en: In the provided data, one class is close to the origin and another class is
    far from the origin. Let’s look at a typical example to understand how this data
    becomes separable when transformed into higher dimensions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供的数据中，一类数据靠近原点，而另一类数据远离原点。让我们通过一个典型的例子来理解，当数据被转换到更高维度时，如何变得可分。
- en: We will transform each 2D point (x, y) to a 3D point (x, y, z), where z = x²
    + y². The transformation adds a new third dimension based on the squared distance
    from the origin in the 2D space. Points that are farther from the origin in the
    2D space will be higher in the 3D space because their squared distance is larger.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每个二维点(x, y)转换为三维点(x, y, z)，其中z = x² + y²。这个变换根据二维空间中到原点的平方距离，增加了一个新的第三维度。在三维空间中，距离原点更远的点将在空间中更高，因为它们的平方距离较大。
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/2a47086d619d1e375b7a139452c47163.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a47086d619d1e375b7a139452c47163.png)'
- en: Image by Author
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: You can notice from the output above that after this transformation, our data
    becomes linearly separable by a 2D hyperplane.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的输出可以看出，在此变换之后，我们的数据变得可以通过一个二维超平面线性可分。
- en: Another example is the effectiveness of drug dosages. A patient is only cured
    if the dosage falls within a certain range. Dosages that are too low or too high
    are ineffective. This scenario naturally creates a dataset that is not linearly
    separable, making it a good candidate for demonstrating how a polynomial kernel
    can help.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是药物剂量的有效性。只有当剂量处于某个特定范围内时，病人才会康复。剂量过低或过高都是无效的。这个场景自然会创建一个不可线性分离的数据集，因此它是展示多项式核如何帮助解决问题的一个好例子。
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/5f9d86f94306290f327605ff250d64af.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f9d86f94306290f327605ff250d64af.png)'
- en: Image by Author
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 'In the two examples above, we take advantage of our knowledge about the data.
    For instance, in first example we know that we have two classes: one close to
    the origin and another far from the origin. This is what the algorithm does through
    training and fine-tuning — it finds a suitable space where the data can be linearly
    separated.'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在上面的两个例子中，我们利用了关于数据的知识。例如，在第一个例子中，我们知道有两个类别：一个接近原点，另一个远离原点。这正是算法通过训练和微调所做的——它找到一个适合的空间，在这个空间中数据可以被线性分开。
- en: The great thing here is that SVMs don’t map data into higher dimensions as this
    would be very complex computationally. Instead, they compute the relationship
    between the data as if it were in higher dimensions using the dot product. This
    is called the “Kernel trick.” I will explain SVM kernels in another article.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个优点是，支持向量机（SVM）并不会将数据映射到更高维度，因为这样在计算上会非常复杂。相反，它们使用点积计算数据之间的关系，就好像数据存在于更高维度中一样。这被称为“核技巧”。我将在另一篇文章中解释SVM的核函数。
- en: Autoencoders
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器
- en: Autoencoders are truly amazing and beautiful architectures that capture my imagination.
    They have a wide range of applications across various domains, utilizing diverse
    types of autoencoders.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是非常令人惊叹且美丽的架构，深深吸引了我的想象力。它们在各个领域有广泛的应用，利用了多种类型的自编码器。
- en: 'They basically consist of an **encoder** and a **decoder**, the encoder takes
    your input and encode/compress it, a process in which we move from a high-dimensional
    space to a more compact, lower-dimensional one. What’s truly interesting is how
    the decoder then takes this condensed representation and reconstructs the original
    data in the higher-dimensional space. The natural question is: how is it possible
    to go back to the original space from a significantly reduced dimension?'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 它们基本上由**编码器**和**解码器**组成，编码器接收输入并对其进行编码/压缩，这是一个将高维空间转化为更加紧凑、低维空间的过程。真正有趣的是，解码器如何接着利用这种压缩后的表示，并在高维空间中重建原始数据。自然的疑问是：如何从显著降低维度的表示中恢复回原始空间呢？
- en: Let’s consider an HD image with a resolution of 720x720 pixels. Storing and
    transmitting this image requires a lot of memory and bandwidth. Autoencoders solve
    this problem by compressing the image into a lower-dimensional space, like a 32x32
    representation called the ‘bottleneck’. The encoder’s job is done at this point.
    The decoder takes over, trying to rebuild the original image from this compressed
    form.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一张分辨率为720x720像素的高清图片。存储和传输这张图片需要大量的内存和带宽。自编码器通过将图片压缩到一个较低维度的空间中，例如一个32x32的表示（称为“瓶颈”），来解决这个问题。此时编码器的工作已完成。接下来由解码器接手，尝试从这个压缩的形式中重建原始图片。
- en: This process is similar to sharing images on platforms like WhatsApp. The image
    is encoded to a lower quality for transmission and then decoded on the receiver’s
    end. The difference in quality between the original and received image is called
    ‘reconstruction error’, which is common in autoencoders.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程类似于在像WhatsApp这样的平台上分享图片。图片被编码成较低质量以进行传输，然后在接收端解码。原始图片和接收图片之间的质量差异被称为“重建误差”，这是自编码器中常见的现象。
- en: 'In autoencoders, we can think of it as an interaction between 3 spaces:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在自编码器中，我们可以将其视为三个空间之间的相互作用：
- en: The input space.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入空间。
- en: The latent representation space (the bottleneck).
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 潜在表示空间（瓶颈）。
- en: The output space.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出空间。
- en: The beauty here is that we can see the autoencoder as something that operates
    in these 3 spaces. It takes advantage of the latent spaces to remove any noisy
    or unnecessary information from the input space, resulting in a very compact representation
    with core information about the input space. It does this by trying to mirror
    the input space in the output space, reducing the difference between the two spaces
    or the reconstruction error.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的美妙之处在于，我们可以将自编码器视为在这三个空间中操作的东西。它利用潜在空间来去除输入空间中的任何噪声或不必要的信息，从而生成一个非常紧凑的表示，包含关于输入空间的核心信息。它通过尝试在输出空间中镜像输入空间来实现这一点，从而减少两者之间的差异或重建误差。
- en: 'Convolutional Autoencoders: Encoding Complexity into Simplicity'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积自编码器：将复杂性编码为简洁
- en: The code below shows an example of a convolutional autoencoder, which is a type
    of autoencoders that works well with images. We will use the popular MNIST dataset[LeCun,
    Y., Cortes, C., & Burges, C.J. (1998). The MNIST Database of Handwritten Digits.
    [Retrieved from TensorFlow](https://www.tensorflow.org/datasets/catalog/mnist),
    CC BY 4.0], which contains 28x28 pixel grayscale images of handwritten digits.
    The encoder plays a crucial role by reducing the dimensionality of the data from
    784 elements to a smaller, more condensed form. The decoder then aims to reconstruct
    the original high-dimensional data from this lower-dimensional representation.
    However, this reconstruction is not perfect and some information is lost. The
    autoencoder overcomes this challenge by learning to prioritize the most important
    features of the data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了一个卷积自编码器的示例，这是一种在处理图像时表现良好的自编码器。我们将使用流行的MNIST数据集[LeCun, Y., Cortes, C.,
    & Burges, C.J. (1998). 手写数字的MNIST数据库。 [来源于TensorFlow](https://www.tensorflow.org/datasets/catalog/mnist)，CC
    BY 4.0]，该数据集包含28x28像素的灰度手写数字图像。编码器在将数据的维度从784个元素压缩到更小的形式时起着至关重要的作用。然后，解码器试图从这种较低维度的表示中重建原始的高维数据。然而，这种重建并不完美，部分信息丢失。自编码器通过学习优先处理数据中最重要的特征来克服这一挑战。
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/3e836d1a5deab08a354758f18b8d0b4c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e836d1a5deab08a354758f18b8d0b4c.png)'
- en: Image by Author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'The output above shows how well the autoencoder works. It displays pairs of
    images: the original digit images and their reconstructions after encoding and
    decoding. This example proves that the encoder captures the essence of the data
    in a smaller form and the decoder can approximate the original image, even though
    some information is lost during compression.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的输出展示了自编码器的效果。它显示了成对的图像：原始数字图像及其在编码和解码后重建的结果。这个例子证明了编码器能够以更小的形式捕捉数据的本质，而解码器则能够近似原始图像，尽管在压缩过程中有些信息丢失。
- en: Now, let’s go further and visualize the learned latent space (the bottleneck).
    We will use PCA and t-SNE, two techniques to reduce dimensions, to show the compressed
    data points on a 2D plane. This step is important because it helps us see how
    the autoencoder organizes the data in the latent space and shows any natural clusters
    of similar digits. We used PCA and t-SNE together just to compare how well they
    work.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进一步可视化学习到的潜在空间（瓶颈）。我们将使用PCA和t-SNE这两种降维技术，将压缩后的数据点显示在二维平面上。这个步骤非常重要，因为它帮助我们看到自编码器是如何在潜在空间中组织数据的，并展示了相似数字的自然聚类。我们使用PCA和t-SNE一起进行比较，看看它们的效果如何。
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/849a0320906aa2409e41565a0f82a497.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/849a0320906aa2409e41565a0f82a497.png)'
- en: Image by Author
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Comparing the two resulted graphs, t-SNE is better than PCA at separating different
    classes of digits in the latent space visualization(it captures non-linearity).
    It creates distinct clusters with minimal overlap between classes. The autoencoder
    compresses images into a lower dimensional space but still captures enough information
    to distinguish between different digits, as shown in the t-SNE graph.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 比较这两个生成的图表，t-SNE在将不同类别的数字在潜在空间中分离时，比PCA更为出色（它捕捉了非线性）。它创建了明显的聚类，且不同类别之间的重叠最小。自编码器将图像压缩到较低的维度空间，但仍然捕捉到足够的信息以区分不同的数字，正如t-SNE图所示。
- en: An important note here is that t-SNE is a non-linear technique used for visualizing
    high-dimensional data. It preserves local data structures, making it useful for
    identifying clusters and patterns visually. However, it is not typically used
    for feature reduction in machine learning.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里有一个重要的说明，t-SNE是一种非线性技术，用于可视化高维数据。它保持局部数据结构，使其在视觉上有助于识别集群和模式。然而，它通常不用于机器学习中的特征降维。
- en: '**But what does this autoencoder probably learn?**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**但是这个自编码器可能学习了什么呢？**'
- en: Generally speaking, one can say that an autoencoder like this learns the basic
    and simple edges and textures, moving to parts of the digits like loops and lines
    and how they are arranged, and finally understanding whole digits(hierarchical
    characteristics), all this while capturing the unique essence of each digit in
    a compact form. It can guess missing parts of an image and recognizes common patterns
    in how digits are written.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，可以说像这样的自编码器学习基本的简单边缘和纹理，逐步学习数字的一部分，比如循环和线条以及它们如何排列，最后理解完整的数字（层次特征），在此过程中，它以紧凑的形式捕捉每个数字的独特本质。它能够猜测图像缺失的部分，并且能够识别数字书写中的常见模式。
- en: 'Manifold learning: The Blessing of Non-Uniformity'
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流形学习：非均匀性的福音
- en: 'In a previous article titled [Curse of Dimensionality: An Intuitive Exploration](/curse-of-dimensionality-an-intuitive-exploration-1fbf155e1411),
    I explored the concept of the “Curse of dimensionality”, which refers to the problems
    and challenges that arises when working with data in higher dimensions, making
    the job of ML algorithms harder in many ways.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇文章《[维度灾难：一种直观的探索](/curse-of-dimensionality-an-intuitive-exploration-1fbf155e1411)》中，我探讨了“维度灾难”这一概念，指的是在处理高维数据时遇到的问题和挑战，这使得许多机器学习算法的工作变得更加困难。
- en: Here come the manifold learning algorithms, driven by the blessing of non-uniformity,
    the uneven distribution or variation of data points within a given space or dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在出现了流形学习算法，这些算法受到非均匀性祝福的驱动，即数据点在给定空间或数据集中的不均匀分布或变化。
- en: The fundamental assumption underlying manifold learning is that high-dimensional
    data actually lies on or near a lower-dimensional manifold within the high-dimensional
    space. This concept is based on the idea that although the data might exist in
    a high-dimensional space due to the way it’s measured or recorded, the intrinsic
    dimensions that effectively describe the data and its structure are much lower.
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 流形学习的基本假设是，高维数据实际上位于高维空间内某个低维流形上或附近。这个概念基于这样的想法：尽管数据可能存在于高维空间中（因为它的测量或记录方式），但有效描述数据及其结构的内在维度要低得多。
- en: 'Let’s generate the famous Swiss roll dataset and use it as an example of non-uniformity
    in higher-dimensional spaces. In its original form, this dataset looks like a
    chaotic mess of data points. But beneath this chaos, there is hidden order — a
    low-dimensional structure that includes the important features of the data. Manifold
    learning techniques, like Isomaps, take advantage of this non-uniformity. By mapping
    data points from the high-dimensional space to a lower-dimensional one, Isomap
    shows us the intrinsic shape of the Swiss roll. It keeps the richness of the original
    data while revealing the underlying structure — a 2D projection that captures
    the non-uniformity of the high-dimensional space:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成著名的瑞士卷数据集，并将其作为高维空间中非均匀性的示例。在其原始形式中，这个数据集看起来像是一个混乱的数据点堆积。然而，在这混乱的表面下，隐藏着秩序——一个低维结构，包含了数据的重要特征。流形学习技术，如Isomap，利用了这种非均匀性。通过将数据点从高维空间映射到低维空间，Isomap展示了瑞士卷的内在形状。它保留了原始数据的丰富性，同时揭示了底层结构——一个二维投影，捕捉了高维空间的非均匀性：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/3a4a17ae7e0217abc2d3e0597d1dec59.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a4a17ae7e0217abc2d3e0597d1dec59.png)'
- en: Image by Author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来自作者
- en: 'Let’s look at the output above:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看上面的输出：
- en: We have two colorful illustrations. On the left, there’s a 3D Swiss roll with
    a rainbow of colors that spiral together. It shows how each shade transitions
    into the next, marking a path through the roll.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两幅色彩丰富的插图。在左侧，是一个3D瑞士卷，五光十色的颜色呈螺旋状交织在一起。它展示了每个颜色过渡到下一个颜色的过程，标记着穿越瑞士卷的路径。
- en: Now, on the right. There’s a 2D spread of the same colors. Even though the shape
    has changed the order and flow of colors still tell the same story of the original
    data. The order and connections between points are preserved, as if the Swiss
    roll was carefully unrolled onto a flat surface so we can see the entire pattern
    at once.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，右侧显示的是相同颜色的二维分布。尽管形状已经改变，颜色的顺序和流动仍然讲述着原始数据的相同故事。点与点之间的顺序和连接被保留，就像瑞士卷被小心地展开到平面上一样，让我们能够一次性看到整个模式。
- en: '**Conclusion**'
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: This article started by exploring the concept of spaces, which are the mathematical
    constructs where data points are positioned based on their features/attributes.
    We examined how **Support Vector Machines (SVMs)** leverage the idea of mapping
    data into higher-dimensional spaces to address the challenge of **non-linear separability**
    in lower spaces.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 本文从探讨空间的概念开始，空间是数据点根据其特征/属性位置的数学构造。我们考察了**支持向量机（SVMs）**如何利用将数据映射到高维空间的思路，来应对低维空间中**非线性可分性**的挑战。
- en: Then we moved on to **autoencoders**, an elegant and truly beautiful architecture
    that maps between **3 spaces**, the **input space**, that gets compressed to a
    much lower **latent representation(the bottleneck)**, and then comes the decoder
    to take the lead aiming to reconstruct the original input from this lower representation
    **while minimizing the reconstruction error**.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们讨论了**自编码器**，一种优雅且真正美丽的架构，它在**3个空间**之间进行映射，**输入空间**被压缩到一个更低的**潜在表示（瓶颈）**，接着是解码器发挥作用，目标是从这个较低的表示中重建原始输入，**同时最小化重建误差**。
- en: We also explored **manifold learning**, and the blessing that we get from non-uniformity
    as a way to overcome the **curse of dimensionality** by simplifying complex datasets
    without losing important details.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了**流形学习**，以及我们从非均匀性中获得的好处，它作为一种方法，通过简化复杂的数据集而不丢失重要细节，从而克服了**维度诅咒**。
- en: If you made it this far, I would like to thank you for your time reading this,
    I hope you found it enjoyable and useful, please feel free to point out any mistakes
    or misconceptions in my article, your feedback and suggestions are also greatly
    appreciated.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到这里，我想感谢你花时间阅读这篇文章，我希望你觉得它既有趣又有用，若有任何错误或误解，请随时指出，我非常感激你的反馈和建议。
