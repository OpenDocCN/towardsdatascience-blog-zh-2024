<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Bounded Kernel Density Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Bounded Kernel Density Estimation</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bounded-kernel-density-estimation-2082dff3f47f?source=collection_archive---------7-----------------------#2024-02-28">https://towardsdatascience.com/bounded-kernel-density-estimation-2082dff3f47f?source=collection_archive---------7-----------------------#2024-02-28</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e59b" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><em class="hd">Learn how Kernel Density Estimation works and how you can adjust it to better handle bounded data, like age, height, or price</em></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@thom01.rouch?source=post_page---byline--2082dff3f47f--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Thomas Rouch" class="l ep by dd de cx" src="../Images/a8440bbed59cd8d9cdd752cf1fea2831.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*bzXmGPw5ErdKoJZ6gYLBsw.jpeg"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2082dff3f47f--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@thom01.rouch?source=post_page---byline--2082dff3f47f--------------------------------" rel="noopener follow">Thomas Rouch</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2082dff3f47f--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 28, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk lb lc ab q ee ld le" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lf"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap ie li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/5227621375cf4dca482a337bcfaf6180.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6hwwlhkfyMF1T1LF"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@maxberg?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Maxim Berg</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="9326" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">H</span>istograms are widely used and easily grasped, but when it comes to estimating continuous densities, people often resort to treating it as a mysterious black box. However, understanding this concept is just as straightforward and becomes crucial, especially when dealing with bounded data like age, height, or price, where available libraries may not handle it automatically.</p><h1 id="aaa5" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">1. Kernel Density Estimation</h1><h2 id="4b8a" class="pe oj fq bf ok pf pg ph on pi pj pk oq nm pl pm pn nq po pp pq nu pr ps pt pu bk">Histogram</h2><p id="ec61" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">A</span> histogram involves partitioning the data range into bins or sub-intervals and counting the number of samples that fall within each bin. It thus approximates the continuous density function with a piecewise constant function.</p><ul class=""><li id="fc61" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qa qb qc bk"><strong class="nf fr"><em class="qd">Large bin size</em></strong>: It helps capturing the low-frequency outline of the density function, by gathering neighboring samples to avoid empty bins. However, it looses the continuity property because there could be a significant gap between the count of adjacent bins.</li><li id="3d9f" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qa qb qc bk"><strong class="nf fr"><em class="qd">Small bin size</em></strong>: It helps capturing details at higher frequencies. However, if the number of samples is too small, we’ll end up with lots of empty bins at places where the true density isn’t.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qj"><img src="../Images/fc2efc6f47353bda2c8e052fa20b6281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5c8HpfTDPnJKSnKM5NQVeA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Histogram of 100 samples drawn from a Gaussian Distribution, with increasing number of bins (5/10/50)— Image by the author</figcaption></figure><h2 id="bea8" class="pe oj fq bf ok pf pg ph on pi pj pk oq nm pl pm pn nq po pp pq nu pr ps pt pu bk">Kernel Density Estimation (KDE)</h2><p id="f053" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">An</span> intuitive idea is to assume that the density function from which the samples are drawn is smooth, and leverage it to fill-in the gaps of our high frequency histogram.</p><p id="d25f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is precisely what the Kernel Density Estimation (KDE) does. It estimates the global density as the average of local density kernels K centered around each sample. A Kernel is a non-negative function integrating to <code class="cx qk ql qm qn b">1</code>, e.g uniform, triangular, normal… Just like adjusting the bin size in a histogram, we introduce a bandwidth parameter <code class="cx qk ql qm qn b">h</code> that modulates the deviation of the kernel around each sample point. It thus controls the smoothness of the resulting density estimate.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qo"><img src="../Images/ac11595d755e2dce6d7d8cb08639a66e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*MO350O5x53kbsZmh"/></div></figure><h2 id="dd99" class="pe oj fq bf ok pf pg ph on pi pj pk oq nm pl pm pn nq po pp pq nu pr ps pt pu bk">Bandwith Selection</h2><p id="3d6b" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">F</span>inding the right balance between under- and over-smoothing isn’t straightforward. A popular and easy-to-compute heuristic is the Silverman’s rule of thumb, which is optimal when the underlying density being estimated is Gaussian.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qp"><img src="../Images/2f679c8d8847706e9df45beb35d899d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/0*GTex6oqIWWnni5EN"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Silverman’s Rule of thumb, with n the number of samples and sigma the standard deviation of the samples</figcaption></figure><blockquote class="qq qr qs"><p id="1ea9" class="nd ne qd nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Keep in mind that it may not always yield optimal results across all data distributions. I won’t discuss them in this article, but there are other alternatives and improvements available.</p></blockquote><p id="7e59" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The image below depicts a Gaussian distribution being estimated by a gaussian KDE at different bandwidth values. As we can see, Silverman’s rule of thumb is well-suited, but higher bandwidths cause over-smoothing, while lower ones introduce high-frequency oscillations around the true density.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qt"><img src="../Images/fbaf3cf096dbb269df2384b94d1ce347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S-ZWoN705w_nQS4UrY_R4A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Gaussian Kernel Density Estimation from 100 samples drawn from a true gaussian distribution, for different bandwith parameters (0.01, 0.04, 0.10) — Image by the author</figcaption></figure><h2 id="b852" class="pe oj fq bf ok pf pg ph on pi pj pk oq nm pl pm pn nq po pp pq nu pr ps pt pu bk">Examples</h2><p id="e1e8" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">T</span>he video below illustrates the convergence of a Kernel Density Estimation with a Gaussian kernel across 4 standard density distributions as the number of provided samples increases.</p><blockquote class="qq qr qs"><p id="2028" class="nd ne qd nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Although it’s not optimal, I’ve chosen to keep a small constant bandwidth <code class="cx qk ql qm qn b">h</code> over the video to better illustrate the process of kernel averaging, and to prevent excessive smoothing when the sample size is very small.</p></blockquote><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/a4e90d2578ba746cad4effc4bc365e1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xHEh_j16DQCtRNLQqqhGGw.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Convergence of Gaussian Kernel Density Estimation across 4 Standard Density Distributions (Uniform, Triangular, Gaussian, Gaussian Mixture) with Increasing Sample Size. — Video by the author</figcaption></figure><h2 id="dab6" class="pe oj fq bf ok pf pg ph on pi pj pk oq nm pl pm pn nq po pp pq nu pr ps pt pu bk">Implementation of Gaussian Kernel Density Estimation</h2><p id="7e6c" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">G</span>reat python libraries like <code class="cx qk ql qm qn b">scipy</code> and <code class="cx qk ql qm qn b">scikit-learn</code> provide public implementations for Kernel Density Estimation:</p><ul class=""><li id="deeb" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qa qb qc bk"><code class="cx qk ql qm qn b">scipy.stats.gaussian_kde</code></li><li id="9a2e" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qa qb qc bk"><code class="cx qk ql qm qn b">sklearn.neighbors.KernelDensity</code></li></ul><p id="3301" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">However, it’s valuable to note that a basic equivalent can be built in just three lines using <code class="cx qk ql qm qn b">numpy</code>. We need the samples <code class="cx qk ql qm qn b">x_data</code> drawn from the distribution to estimate and the points <code class="cx qk ql qm qn b">x_prediction</code> at which we want to evaluate the density estimate. Then, using array broadcasting we can evaluate a local gaussian kernel around each input sample and average them into the final density estimate.</p><blockquote class="qq qr qs"><p id="f144" class="nd ne qd nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">N.B. This version is fast because it’s vectorized. However it involves creating a large 2D temporary array of shape <code class="cx qk ql qm qn b">(len(x_data), len(x_prediction))</code> to store all the kernel evaluations. To have a lower memory footprint, we could re-write it using <code class="cx qk ql qm qn b">numba</code> or <code class="cx qk ql qm qn b">cython</code> (to avoid the computational burden of Python for loops) to aggregate kernel evaluations on-the-fly in a running sum for each output prediction.</p></blockquote><figure class="mm mn mo mp mq mr"><div class="qv ip l ed"><div class="qw qx l"/></div></figure></div></div></div><div class="ab cb qy qz ra rb" role="separator"><span class="rc by bm rd re rf"/><span class="rc by bm rd re rf"/><span class="rc by bm rd re"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rg"><img src="../Images/d70725ddf1f3e1a32622c0dff26ca558.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GBXu8lbviVi0i77s"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@lowmurmer?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Parker Coffman</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="98a0" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">2. Handle boundaries</h1><h2 id="a0e3" class="pe oj fq bf ok pf pg ph on pi pj pk oq nm pl pm pn nq po pp pq nu pr ps pt pu bk">Bounded Distributions</h2><p id="3668" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">R</span>eal-life data is often bounded by a given domain. For example, attributes such as age, weight, or duration are always non-negative values. In such scenarios, a standard smooth KDE may fail to accurately capture the true shape of the distribution, especially if there’s a density discontinuity at the boundary.</p><p id="aea0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In 1D, with the exception of some exotic cases, bounded distributions typically have either one-sided (e.g. positive values) or two-sided (e.g. uniform interval) bounded domains.</p><p id="a73a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As illustrated in the graph below, kernels are bad at estimating the edges of the uniform distribution and leak outside the bounded domain.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rh"><img src="../Images/512f7dee93cb2ecb811e4eede5ce19db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Ro_53eMKrGuxd9jCd0BApA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Gaussian KDE on 100 samples drawn from a uniform distribution — Image by the author</figcaption></figure><h2 id="0e0a" class="pe oj fq bf ok pf pg ph on pi pj pk oq nm pl pm pn nq po pp pq nu pr ps pt pu bk">No Clean Public Solution in Python</h2><p id="5d41" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">U</span>nfortunately, popular public Python libraries like <code class="cx qk ql qm qn b">scipy</code> and <code class="cx qk ql qm qn b">scikit-learn</code> do not currently address this issue. There are existing GitHub issues and pull requests discussing this topic, but regrettably, they have remained unresolved for quite some time.</p><ul class=""><li id="9c09" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qa qb qc bk"><a class="af nc" href="https://github.com/scikit-learn/scikit-learn/issues/10108" rel="noopener ugc nofollow" target="_blank"><em class="qd">Feature Request: KDE for Bounded Data</em></a><em class="qd"> </em>(GitHub Issue scikit-learn — 2017)</li><li id="d826" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qa qb qc bk"><a class="af nc" href="https://github.com/scipy/scipy/pull/6114" rel="noopener ugc nofollow" target="_blank"><em class="qd">ENH: improve stats.kde density estimation near boundaries</em></a><em class="qd"> </em>(GitHub Pull Request scipy — 2016)</li></ul><blockquote class="qq qr qs"><p id="4379" class="nd ne qd nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In R, <code class="cx qk ql qm qn b"><a class="af nc" href="https://search.r-project.org/CRAN/refmans/ks/html/kde.boundary.html" rel="noopener ugc nofollow" target="_blank">kde.boundary</a></code> allows Kernel density estimate for bounded data.</p></blockquote><p id="2d9d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There are various ways to take into account the bounded nature of the distribution. Let’s describe the most popular ones: Reflection, Weighting and Transformation.</p><blockquote class="qq qr qs"><p id="148b" class="nd ne qd nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Warning:<br/>For the sake of readability, we will focus on the unit bounded domain, i.e. <code class="cx qk ql qm qn b"><em class="fq">[0,1]</em></code>. Please remember to standardize the data and scale the density appropriately in the general case <code class="cx qk ql qm qn b"><em class="fq">[a,b]</em></code>.</p></blockquote><h2 id="08fe" class="pe oj fq bf ok pf pg ph on pi pj pk oq nm pl pm pn nq po pp pq nu pr ps pt pu bk"><strong class="al"><em class="hd">Solution: Reflection</em></strong></h2><p id="3f35" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">T</span>he trick is to augment the set of samples by reflecting them across the left and right boundaries. This is equivalent to reflecting the tails of the local kernels to keep them in the bounded domain. It works best when the density derivative is zero at the boundary.</p><blockquote class="qq qr qs"><p id="4740" class="nd ne qd nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The reflection technique also implies processing three times more sample points.</p></blockquote><p id="8f2c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The graphs below illustrate the reflection trick for three standard distributions: uniform, right triangle and inverse square root. It does a pretty good job at reducing the bias at the boundaries, even for the singularity of the inverse square root distribution.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/a2bee39cfbb23090555e0de197fec3c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-TC4wsVnkmh_uBaGWXhz7Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">KDE on an uniform distribution, using reflections to handle boundaries— Image by the author</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/36fe2400d8dd695f2f442e0a9afad07b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8jeMrorRS1ZFrTPeEIJqug.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">KDE on a triangle distribution, using reflections to handle boundaries — Image by the author</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/139670d395596d1addd42afb6699aa55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R0doJNRgf5Ns79GCRlE9Hg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">KDE on an inverse square root distribution, using reflections to handle boundaries — Image by the author</figcaption></figure><figure class="mm mn mo mp mq mr"><div class="qv ip l ed"><div class="qw qx l"/></div></figure><blockquote class="qq qr qs"><p id="0e68" class="nd ne qd nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">N.B. The signature of <code class="cx qk ql qm qn b">basic_kde</code> has been slightly updated to allow to optionally provide your own bandwidth parameter instead of using the Silverman’s rule of thumb.</p></blockquote><h2 id="0dfb" class="pe oj fq bf ok pf pg ph on pi pj pk oq nm pl pm pn nq po pp pq nu pr ps pt pu bk"><strong class="al"><em class="hd">Solution: Weighting</em></strong></h2><p id="bf42" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">T</span>he reflection trick presented above takes the leaking tails of the local kernel and add them back to the bounded domain, so that the information isn’t lost. However, we could also compute how much of our local kernel has been lost outside the bounded domain and leverage it to correct the bias.</p><p id="f9be" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For a very large number of samples, the KDE converges to the convolution between the kernel and the true density, truncated by the bounded domain.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ri"><img src="../Images/52befe5a923f523514183599c3049017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7bAMxAt9-DK07sAY"/></div></div></figure><p id="8e15" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If <code class="cx qk ql qm qn b">x</code> is at a boundary, then only half of the kernel area will actually be used. Intuitively, we’d like to normalize the convolution kernel to make it integrate to 1 over the bounded domain. The integral will be close to 1 at the center of the bounded interval and will fall off to 0.5 near the borders. This accounts for the lack of neighboring kernels at the boundaries.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rj"><img src="../Images/4562347079cb9001fc35e204d4832b16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yqsS5L-8wEsHwVWk"/></div></div></figure><p id="92a6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Similarly to the reflection technique, the graphs below illustrate the weighting trick for three standard distributions: uniform, right triangle and inverse square root. It performs very similarly to the reflection method.</p><p id="bcb3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">From a computational perspective, it doesn’t require to process 3 times more samples, but it needs to evaluate the normal Cumulative Density Function at the prediction points.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/b1d0f6328e6867278099f88dc392a045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YU5sE0kTDKSObJZXkGvcHw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">KDE on an uniform distribution, applying weight on the sides to handle boundaries — Image by the author</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/1b4690f9ddb54c9ba2f021d043ac7ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i2_0vGA61BzQRT81_axVSA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">KDE on a triangular distribution, applying weight on the sides to handle boundaries — Image by the author</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/81ffd559a6a60a493b75d9bca98c40e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2zTY7fFlVZp9vHT7TkPRAg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">KDE on an inverse square root distribution, applying weight on the sides to handle boundaries — Image by the author</figcaption></figure><figure class="mm mn mo mp mq mr"><div class="qv ip l ed"><div class="qw qx l"/></div></figure><h2 id="c184" class="pe oj fq bf ok pf pg ph on pi pj pk oq nm pl pm pn nq po pp pq nu pr ps pt pu bk"><strong class="al"><em class="hd">Transformation</em></strong></h2><p id="d1b2" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">T</span>he transformation trick maps the bounded data to an unbounded space, where the KDE can be safely applied. This results in using a different kernel function for each input sample.</p><p id="17ff" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The logit function leverages the logarithm to map the unit interval <code class="cx qk ql qm qn b">[0,1]</code> to the entire real axis.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rk"><img src="../Images/edaa6ce64267812567ef794b46968254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*Lrxrh7-huCGPyiHDE5VTiw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Logit function — Image by the author</figcaption></figure><p id="3462" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">When applying a transform <code class="cx qk ql qm qn b">f</code> onto a random variable <code class="cx qk ql qm qn b">X</code>, the resulting density can be obtained by dividing by the absolute value of the derivative of <code class="cx qk ql qm qn b">f</code>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rl"><img src="../Images/963bad27288d12de9ef24f023f7efbeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/0*LKHxjDAHnQVpXURw"/></div></figure><p id="37a0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can now apply it for the special case of the logit transform to retrieve the density distribution from the one estimated in the logit space.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/754b22657314764d9c7120a995fea91a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dgWspIcQ1vZZLBOU"/></div></div></figure><p id="84c0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Similarly to the reflection and weighting techniques, the graphs below illustrate the weighting trick for three standard distributions: uniform, right triangle and inverse square root. It performs quite poorly by creating large oscillations at the boundaries. However, it handles extremely well the singularity of the inverse square root.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/fadf55b1409f89d75c4e31e09b255a13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QMX45f3WPXjJOnXt2Q41yQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">KDE on an uniform distribution, computed after mapping the samples to the logit space — Image by the author</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/616669a0204e732995da1fed80645535.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LwrMYnLA3D-pvRGYikWmkw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">KDE on a triangular distribution, computed after mapping the samples to the logit space — Image by the author</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/45f638457823124a75f24495a53da1a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kbGNtNNpVsBm8hpDDmXB1A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">KDE on an inverse square root distribution, computed after mapping the samples to the logit space— Image by the author</figcaption></figure><figure class="mm mn mo mp mq mr"><div class="qv ip l ed"><div class="qw qx l"/></div></figure></div></div></div><div class="ab cb qy qz ra rb" role="separator"><span class="rc by bm rd re rf"/><span class="rc by bm rd re rf"/><span class="rc by bm rd re"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rn"><img src="../Images/bffbdc79d5aa04c66af6cb5746a72f95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SiETJhQvWAqg3o50"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@martz90?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Martin Martz</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="1143" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Conclusion</h1><h2 id="34b8" class="pe oj fq bf ok pf pg ph on pi pj pk oq nm pl pm pn nq po pp pq nu pr ps pt pu bk">Histograms and KDE</h2><p id="cebb" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">In</span> Kernel Density Estimation, each sample is assigned its own local kernel density centered around it, and then we average all these densities to obtain the global density. The bandwidth parameter defines how far the influence of each kernel extends. Intuitively, we should decrease the bandwidth as the number of samples increases, to prevent excessive smoothing.</p><p id="5d4a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Histograms can be seen as a simplified version of KDE. Indeed, the bins implicitly define a finite set of possible rectangular kernels, and each sample is assigned to the closest one. Finally, the average of all these densities result in a piecewise-constant estimate of the global density.</p><h2 id="e311" class="pe oj fq bf ok pf pg ph on pi pj pk oq nm pl pm pn nq po pp pq nu pr ps pt pu bk">Which method is the best ?</h2><p id="217e" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">R</span>eflection, weighting and transform are efficient basic methods to handle bounded data during KDE. However, bear in mind that there isn’t a one-size-fits-all solution; it heavily depends on the shape of your data.</p><p id="d1db" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The transform method handles pretty well the singularities, as we’ve seen with the inverse square root distribution. As for reflection and weighting, they are generally more suitable for a broader range of scenarios.</p><blockquote class="qq qr qs"><p id="ba55" class="nd ne qd nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Reflection introduces complexity during training, whereas weighting adds complexity during inference.</p></blockquote><h2 id="c2fe" class="pe oj fq bf ok pf pg ph on pi pj pk oq nm pl pm pn nq po pp pq nu pr ps pt pu bk">From [0,1] to [a,b], [a, +∞[ and ]-∞,b]</h2><p id="a9cf" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">C</span>ode presented above has been written for data bounded in the unit interval. Don’t forget to scale the density, when applying the affine transformation to normalize your data.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ro"><img src="../Images/b9caa79b4e4649590d40423a89ea6330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*l6mIv6eF4ntnIzqj"/></div></div></figure><p id="cf68" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It can also easily be adjusted for a one-sided bounded domain, by reflecting only on one side, integrating the kernel to infinity on one side or using the logarithm instead of logit.</p></div></div></div><div class="ab cb qy qz ra rb" role="separator"><span class="rc by bm rd re rf"/><span class="rc by bm rd re rf"/><span class="rc by bm rd re"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="fb4b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I hope you enjoyed reading this article and that it gave you more insights on how Kernel Density Estimation works and how to handle bounded domains!</p></div></div></div></div>    
</body>
</html>