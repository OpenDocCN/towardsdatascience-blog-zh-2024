["```py\n# Listing Criteria / LangChain's built-in metrics\nfrom langchain.evaluation import Criteria\nnew_criteria_list = [item for i, item in enumerate(Criteria) if i != 2]\nnew_criteria_list\n```", "```py\n# Import a standard SQUAD dataset from HuggingFace (ran in colab)\nfrom google.colab import userdata\nHF_TOKEN = userdata.get('HF_TOKEN')\n\ndataset = load_dataset(\"rajpurkar/squad\")\nprint(type(dataset))\n```", "```py\n# Slice dataset to randomized selection of 100 rows\nvalidation_data = dataset['validation']\nvalidation_df = validation_data.to_pandas()\nsample_df = validation_df.sample(n=100, replace=False)\n```", "```py\nimport os\n\n# Import OAI API key\nOPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\nos.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n# Define llm\nllm = ChatOpenAI(model_name='gpt-3.5-turbo', openai_api_key=OPENAI_API_KEY)\n```", "```py\n# Loop through each question in random sample\nfor index, row in sample_df.iterrows():\n    try:\n        prediction = \" \".join(row['answers']['text'])\n        input_text = row['question']\n\n        # Loop through each criteria\\\n        for m in new_criteria_list:\n            evaluator = load_evaluator(\"criteria\", llm=llm, criteria=m)\n\n            eval_result = evaluator.evaluate_strings(\n                prediction=prediction,\n                input=input_text,\n                reference=None,\n                other_kwarg=\"value\"  # adding more in future for compare\n            )\n            score = eval_result['score']\n            if m not in results:\n                results[m] = []\n            results[m].append(score)\n    except KeyError as e:\n        print(f\"KeyError: {e} in row {index}\")\n    except TypeError as e:\n        print(f\"TypeError: {e} in row {index}\")\n```", "```py\n# Calculate means and confidence intervals at 95%\nmean_scores = {}\nconfidence_intervals = {}\n\nfor m, scores in results.items():\n    mean_score = np.mean(scores)\n    mean_scores[m] = mean_score\n    # Standard error of the mean * t-value for 95% confidence\n    ci = sem(scores) * t.ppf((1 + 0.95) / 2., len(scores)-1)\n    confidence_intervals[m] = (mean_score - ci, mean_score + ci)\n```", "```py\n# Plotting results by metric\nfig, ax = plt.subplots()\nm_labels = list(mean_scores.keys())\nmeans = list(mean_scores.values())\ncis = [confidence_intervals[m] for m in m_labels]\nerror = [(mean - ci[0], ci[1] - mean) for mean, ci in zip(means, cis)]]\n\nax.bar(m_labels, means, yerr=np.array(error).T, capsize=5, color='lightblue', label='Mean Scores with 95% CI')\nax.set_xlabel('Criteria')\nax.set_ylabel('Average Score')\nax.set_title('Evaluation Scores by Criteria')\nplt.xticks(rotation=90)\nplt.legend()\nplt.show()\n```", "```py\n# Convert results to dataframe\nmin_length = min(len(v) for v in results.values())\ndfdata = {k.name: v[:min_length] for k, v in results.items()}\ndf = pd.DataFrame(dfdata)\n\n# Filtering out null values\nfiltered_df = df.drop(columns=[col for col in df.columns if 'MALICIOUSNESS' in col or 'MISOGYNY' in col])\n\n# Create corr matrix\ncorrelation_matrix = filtered_df.corr()\n```", "```py\n# Plot corr matrix\nmask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',\n            cbar_kws={\"shrink\": .8})\nplt.title('Correlation Matrix - Built-in Metrics from LangChain')\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.show()\n```"]