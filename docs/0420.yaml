- en: Lexicon-Based Sentiment Analysis Using R
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于词典的情感分析使用R语言
- en: 原文：[https://towardsdatascience.com/lexicon-based-sentiment-analysis-using-r-5c1db85984a1?source=collection_archive---------13-----------------------#2024-02-13](https://towardsdatascience.com/lexicon-based-sentiment-analysis-using-r-5c1db85984a1?source=collection_archive---------13-----------------------#2024-02-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/lexicon-based-sentiment-analysis-using-r-5c1db85984a1?source=collection_archive---------13-----------------------#2024-02-13](https://towardsdatascience.com/lexicon-based-sentiment-analysis-using-r-5c1db85984a1?source=collection_archive---------13-----------------------#2024-02-13)
- en: An empirical analysis of sentiments conveyed through media briefings during
    the COVID-19 pandemic
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一项关于COVID-19大流行期间媒体简报所传达情感的实证分析
- en: '[](https://drokanbulut.medium.com/?source=post_page---byline--5c1db85984a1--------------------------------)[![Okan
    Bulut](../Images/555a4b1818ac0b5d0766f3ad7ab71a6f.png)](https://drokanbulut.medium.com/?source=post_page---byline--5c1db85984a1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5c1db85984a1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5c1db85984a1--------------------------------)
    [Okan Bulut](https://drokanbulut.medium.com/?source=post_page---byline--5c1db85984a1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://drokanbulut.medium.com/?source=post_page---byline--5c1db85984a1--------------------------------)[![Okan
    Bulut](../Images/555a4b1818ac0b5d0766f3ad7ab71a6f.png)](https://drokanbulut.medium.com/?source=post_page---byline--5c1db85984a1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5c1db85984a1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5c1db85984a1--------------------------------)
    [Okan Bulut](https://drokanbulut.medium.com/?source=post_page---byline--5c1db85984a1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5c1db85984a1--------------------------------)
    ·14 min read·Feb 13, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5c1db85984a1--------------------------------)
    ·14分钟阅读·2024年2月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5eb08923969d0e60e8b3c64f98ab28eb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5eb08923969d0e60e8b3c64f98ab28eb.png)'
- en: Image by [Gino Crescoli](https://pixabay.com/users/absolutvision-6158753/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2979107)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2979107)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Gino Crescoli](https://pixabay.com/users/absolutvision-6158753/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2979107)提供，来源于[Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2979107)
- en: 'During the COVID-19 pandemic, I decided to learn a new statistical technique
    to keep my mind occupied rather than constantly immersing myself in pandemic-related
    news. After evaluating several options, I found the concepts related to natural
    language processing (NLP) particularly captivating. So, I opted to delve deeper
    into this field and explore one specific technique: sentiment analysis, also known
    as “opinion mining” in academic literature. This analytical method empowers researchers
    to extract and interpret the emotions conveyed toward a specific subject within
    the written text. Through sentiment analysis, one can discern the polarity (positive
    or negative), nature, and intensity of sentiments expressed across various textual
    formats such as documents, customer reviews, and social media posts.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在COVID-19大流行期间，我决定学习一项新的统计技术，以便让自己不至于总是沉浸在与大流行相关的新闻中。经过评估多个选项后，我发现与自然语言处理（NLP）相关的概念特别吸引人。因此，我决定深入研究这个领域，并探索其中一种具体的技术：情感分析，也被学术文献称为“意见挖掘”。这种分析方法使研究人员能够提取并解释写作文本中传达的情感，针对特定主题进行情感的分析。通过情感分析，人们可以辨别情感的极性（正面或负面）、性质以及在各种文本格式（如文档、客户评价和社交媒体帖子）中的强度。
- en: Amidst the pandemic, I observed a significant trend among researchers who turned
    to sentiment analysis as a tool to measure public responses to news and developments
    surrounding the virus. This involved analyzing user-generated content on popular
    social media platforms such as Twitter, YouTube, and Instagram. Intrigued by this
    methodology, my colleagues and I endeavored to contribute to the existing body
    of research by scrutinizing the daily briefings provided by public health authorities.
    In Alberta, Dr. Deena Hinshaw, who used to be the province’s chief medical officer
    of health, regularly delivered [updates on the region’s response](https://www.youtube.com/watch?v=fvw_USRfXgY)
    to the ongoing pandemic. Through our analysis of these public health announcements,
    we aimed to assess Alberta’s effectiveness in implementing communication strategies
    during this intricate public health crisis. Our investigation, conducted through
    the lenses of sentiment analysis, sought to shed light on the efficacy of communication
    strategies employed during this challenging period in public health [1, 2].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在疫情期间，我观察到许多研究人员将情感分析作为衡量公众对与病毒相关的新闻和发展反应的工具。这包括分析用户生成的内容，主要来自Twitter、YouTube和Instagram等流行社交媒体平台。由于对这种方法产生了兴趣，我和我的同事们努力通过分析公共卫生机构提供的每日简报，来为现有的研究贡献力量。在阿尔伯塔省，曾担任该省首席公共卫生官的**迪娜·辛肖**博士定期发布[关于该地区应对疫情的更新](https://www.youtube.com/watch?v=fvw_USRfXgY)。通过分析这些公共卫生公告，我们旨在评估阿尔伯塔省在这一复杂公共卫生危机中实施沟通策略的有效性。我们的研究采用情感分析的方法，旨在揭示在这个充满挑战的公共卫生时期中，所采取的沟通策略的效果[1,
    2]。
- en: In this post, I aim to walk you through the process of performing sentiment
    analysis using R. Specifically, I’ll focus on “lexicon-based sentiment analysis,”
    which I’ll discuss in more detail in the next section. I’ll provide examples of
    lexicon-based sentiment analysis that we’ve integrated into the publications referenced
    earlier. Additionally, in future posts, I’ll delve into more advanced forms of
    sentiment analysis, making use of state-of-the-art pre-trained models accessible
    on [Hugging Face](https://huggingface.co/docs/transformers/en/index).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将引导你们了解如何使用R进行情感分析。具体来说，我将重点介绍“基于词典的情感分析”，并在下一节中对此进行更详细的讨论。我将提供一些我们之前引用的出版物中整合的基于词典的情感分析的例子。此外，在未来的文章中，我还将深入探讨更先进的情感分析方法，利用[Hugging
    Face](https://huggingface.co/docs/transformers/en/index)上可访问的最先进的预训练模型。
- en: Lexicon-Based Sentiment Analysis
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于词典的情感分析
- en: As I learned more about sentiment analysis, I discovered that the predominant
    method for extracting sentiments is lexicon-based sentiment analysis. This approach
    entails utilizing a specific lexicon, essentially the vocabulary of a language
    or subject, to discern the direction and intensity of sentiments conveyed within
    a given text. Some lexicons, like the Bing lexicon [3], classify words as either
    positive or negative. Conversely, other lexicons provide more detailed sentiment
    labels, such as the NRC Emotion Lexicon [4], which categorizes words based on
    both positive and negative sentiments, as well as Plutchik’s [5] psych evolutionary
    theory of basic emotions (e.g., anger, fear, anticipation, trust, surprise, sadness,
    joy, and disgust).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我深入了解情感分析的过程中，我发现提取情感的主要方法是基于词典的情感分析。这种方法涉及使用特定的词典，即语言或主题的词汇，来识别给定文本中所表达的情感的方向和强度。有些词典，如Bing词典[3]，将单词分类为正面或负面。相反，其他词典则提供更详细的情感标签，例如NRC情感词典[4]，它根据正面和负面情感对单词进行分类，还包括普鲁奇克的[5]基本情感的心理进化理论（例如，愤怒、恐惧、期待、信任、惊讶、悲伤、喜悦和厌恶）。
- en: Lexicon-based sentiment analysis operates by aligning words within a given text
    with those found in widely used lexicons such as NRC and Bing. Each word receives
    an assigned sentiment, typically categorized as positive or negative. The text’s
    collective sentiment score is subsequently derived by summing the individual sentiment
    scores of its constituent words. For instance, in a scenario where a text incorporates
    50 positive and 30 negative words according to the Bing lexicon, the resulting
    sentiment score would be 20\. This value indicates a predominance of positive
    sentiments within the text. Conversely, a negative total would imply a prevalence
    of negative sentiments.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Performing lexicon-based sentiment analysis using R can be both fun and tricky
    at the same time. While analyzing public health announcements in terms of sentiments,
    I found Julia Silge and David Robinson’s book, [*Text Mining with R*](https://www.tidytextmining.com/),
    to be very helpful. The book has [a chapter dedicated to sentiment analysis](https://www.tidytextmining.com/sentiment),
    where the authors demonstrate how to conduct sentiment analysis using general-purpose
    lexicons like Bing and NRC. However, Julia and David also highlight a major limitation
    of lexicon-based sentiment analysis. The analysis considers only single words
    (i.e., unigrams) and does not consider qualifiers before a word. For instance,
    negation words like “not” in “not true” are ignored, and sentiment analysis processes
    them as two separate words, “not” and “true”. Furthermore, if a particular word
    (either positive or negative) is repeatedly used throughout the text, this may
    skew the results depending on the polarity (positive or negative) of this word.
    Therefore, the results of lexicon-based sentiment analysis should be interpreted
    carefully.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move to our example, where we will conduct lexicon-based sentiment
    analysis using Dr. Deena Hinshaw’s media briefings during the COVID-19 pandemic.
    My goal is to showcase two R packages capable of running sentiment analysis 📉.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Example
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the sake of simplicity, we will focus on the first wave of the pandemic
    (March 2020 — June 2020). The transcripts of all media briefings were publicly
    available on the government of Alberta’s COVID-19 pandemic website ([https://www.alberta.ca/covid](https://www.alberta.ca/covid)).
    This dataset comes with an [open data license](https://open.alberta.ca/licence)
    that allows the public to access and use the information, including for commercial
    purposes. After importing these transcripts into R, I turned all the text into
    lowercase and then applied word tokenization using the **tidytext** and **tokenizers**
    packages. Word tokenization split the sentences in the media briefings into individual
    words for each entry (i.e., day of media briefings). Next, I applied lemmatization
    to the tokens to resolve each word into its canonical form using the **textstem**
    package. Finally, I removed common stopwords, such as “my,” “for,” “that,” “with,”
    and “for, using the stopwords package. The final dataset is available [**here**](https://github.com/okanbulut/blog/raw/master/data_and_codes/wave1_alberta.RData).
    Now, let’s import the data into R and then review its content.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们将专注于疫情的第一波（2020年3月 — 2020年6月）。所有媒体简报的文字记录在阿尔伯塔省政府的COVID-19疫情网站上公开发布（[https://www.alberta.ca/covid](https://www.alberta.ca/covid)）。该数据集附带一个[开放数据许可证](https://open.alberta.ca/licence)，允许公众访问和使用这些信息，包括用于商业目的。在将这些文字记录导入R后，我将所有文本转换为小写字母，然后使用**tidytext**和**tokenizers**包进行词元化。词元化将媒体简报中的句子拆分为每个条目的单个单词（即，媒体简报的每一天）。接下来，我使用**textstem**包对词元进行了词形还原，将每个单词转化为其规范形式。最后，我使用stopwords包移除了常见的停用词，如“my”、“for”、“that”、“with”和“for”。最终的数据集可以在[**此处**](https://github.com/okanbulut/blog/raw/master/data_and_codes/wave1_alberta.RData)获得。现在，让我们将数据导入R并查看其内容。
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/d1603ea9ba4436bfd2d51e226a839b7b.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1603ea9ba4436bfd2d51e226a839b7b.png)'
- en: A preview of the dataset (Image by author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集预览（作者提供的图像）
- en: 'The dataset has three columns:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集有三列：
- en: month (the month of the media briefing)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月份（媒体简报的月份）
- en: date (the exact date of the media briefing), and
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期（媒体简报的确切日期），以及
- en: word (words or tokens used in media briefing)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词（媒体简报中使用的单词或词元）
- en: Descriptive Analysis
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描述性分析
- en: Now, we can calculate some descriptive statistics to better understand the content
    of our dataset. We will begin by finding the top 5 words (based on their frequency)
    for each month.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算一些描述性统计数据，以更好地理解数据集的内容。我们将首先按月份（基于词频）找出前5个单词。
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/759fd7f9b972d9892bc76ad34db081be.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/759fd7f9b972d9892bc76ad34db081be.png)'
- en: Top 5 words by months (Image by author)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 按月份排序的前5个单词（作者提供的图像）
- en: 'The output shows that words such as health, continue, and test were commonly
    used in the media briefings across this 4-month period. We can also expand our
    list to the most common 10 words and view the results visually:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示，在这4个月的媒体简报中，“health”、“continue”和“test”等词语被频繁使用。我们还可以扩展我们的列表，查看最常用的10个单词，并以可视化方式呈现结果：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/2b8308d5610aef09dbe4729f628a162d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b8308d5610aef09dbe4729f628a162d.png)'
- en: Most common words based on frequency (Image by author)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 基于频率的最常用单词（作者提供的图像）
- en: Since some words are common across all four months, the plot above may not necessarily
    show us the important words that are unique to each month. To find such important
    words, we can use Term Frequency — Inverse Document Frequency (TF-IDF)–a widely
    used technique in NLP for measuring how important a term is within a document
    relative to a collection of documents (for more detailed information about TF-IDF,
    check out [my previous blog post](https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/#tf-idf)).
    In our example, we will treat media briefings for each month as a document and
    calculate TF-IDF for the tokens (i.e., words) within each document. The first
    part of the R codes below creates a new dataset, *wave1_tf_idf*, by calculating
    TF-IDF for all tokens and selecting the tokens with the highest TF-IDF values
    within each month. Next, we use this dataset to create a bar plot with the TF-IDF
    values to view the common words unique to each month.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一些词汇在四个月内都是通用的，上面的图表可能无法直接展示每个月特有的重要词汇。为了找出这些重要的词汇，我们可以使用词频-逆文档频率（TF-IDF）技术——这是一种在自然语言处理（NLP）中广泛使用的技术，用于衡量一个词在文档中相对于一组文档的重要性（有关TF-IDF的详细信息，请查看[我之前的博客文章](https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/#tf-idf)）。在我们的例子中，我们将每个月的媒体简报视为一个文档，并计算文档中每个词汇的TF-IDF。下面的R代码第一部分通过计算所有词汇的TF-IDF并选择每个月内TF-IDF值最高的词汇，创建了一个新的数据集，*wave1_tf_idf*。接下来，我们使用该数据集创建一个条形图，显示每个月特有的常见词汇。
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/d6e77561244b9eb493176747ea3cac6b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6e77561244b9eb493176747ea3cac6b.png)'
- en: Most common words based on TIF-IDF (Image by author)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基于TF-IDF的最常见词汇（图片来自作者）
- en: These results are more informative because the tokens shown in the figure reflect
    unique topics discussed each month. For example, in March 2020, the media briefings
    were mostly about limiting travel, returning from crowded conferences, and COVID-19
    cases on cruise ships. In June 2020, the focus of the media briefings shifted
    towards mask requirements, people protesting pandemic-related restrictions, and
    so on.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果提供了更多的信息，因为图中显示的词汇反映了每个月讨论的独特主题。例如，在2020年3月，媒体简报主要讨论了限制旅行、从人群密集的会议返回以及邮轮上的COVID-19病例。到了2020年6月，媒体简报的重点转向了口罩要求、人们抗议与疫情相关的限制等等。
- en: 'Before we switch back to the sentiment analysis, let’s take a look at another
    descriptive variable: the length of each media briefing. This will show us whether
    the media briefings became longer or shorter over time.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们切换回情感分析之前，让我们看看另一个描述性变量：每个媒体简报的长度。这将帮助我们了解媒体简报随着时间的推移是变得更长还是更短。
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/8c46ac6ce65bdb88d89a63b92a501b6f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c46ac6ce65bdb88d89a63b92a501b6f.png)'
- en: Number of words in the media briefings by day (Image by author)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每日媒体简报中的词汇数（图片来自作者）
- en: The figure above shows that the length of media briefings varied quite substantially
    over time. Especially in March and May, there are larger fluctuations (i.e., very
    long or short briefings), whereas, in June, the daily media briefings are quite
    similar in terms of length.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表显示了媒体简报的长度随时间变化较大。特别是在3月和5月，媒体简报的长度波动较大（即，极长或极短的简报），而在6月，媒体简报的日常长度较为一致。
- en: Sentiment Analysis with tidytext
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用tidytext进行情感分析
- en: After analyzing the dataset descriptively, we are ready to begin with the sentiment
    analysis. In the first part, we will use the **tidytext** package for performing
    sentiment analysis and computing sentiment scores. We will first import the lexicons
    into R and then merge them with our dataset. Using the Bing lexicon, we need to
    find the difference between the number of positive and negative words to produce
    a sentiment score (i.e., sentiment = the number of positive words — the number
    of negative words).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据集进行描述性分析后，我们准备开始进行情感分析。在第一部分中，我们将使用**tidytext**包来执行情感分析并计算情感得分。我们首先将词典导入R中，然后将它们与我们的数据集合并。使用Bing词典时，我们需要计算正面和负面词汇的差异，以产生情感得分（即，情感
    = 正面词汇的数量 — 负面词汇的数量）。
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/52c36e29a5fde07cefdd40594b2fc537.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52c36e29a5fde07cefdd40594b2fc537.png)'
- en: Sentiment scores based on the Bing lexicon (Image by author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Bing词典的情感得分（图片来自作者）
- en: The figure above shows that the sentiments delivered in the media briefings
    were generally negative, which is not necessarily surprising since the media briefings
    were all about how many people passed away, hospitalization rates, potential outbreaks,
    etc. On certain days (e.g., March 24, 2020 and May 4, 2020), the media briefings
    were particularly more negative in terms of sentiments.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示，媒体简报中的情感通常是负面的，这并不令人惊讶，因为这些简报主要讲述了有多少人去世、住院率、潜在爆发等问题。在某些日期（例如 2020 年 3
    月 24 日和 2020 年 5 月 4 日），媒体简报的情感尤为负面。
- en: Next, we will use the AFINN lexicon. Unlike Bing that labels words as positive
    or negative, AFINN assigns a numerical weight to each word. The sign of the weight
    indicates the polarity of sentiments (i.e., positive or negative), while the value
    indicates the intensity of sentiments. Now, let’s see if these weighted values
    produce different sentiment scores.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 AFINN 词典。与将词语标记为积极或消极的 Bing 不同，AFINN 为每个词语分配一个数值权重。权重的符号表示情感的极性（即积极或消极），而数值则表示情感的强度。现在，让我们看看这些加权值是否会产生不同的情感得分。
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/fd3bcd5f5955b4a3ae40a0156952837f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd3bcd5f5955b4a3ae40a0156952837f.png)'
- en: Sentiment scores based on the AFINN lexicon (Image by author)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 AFINN 词典的情感得分（图片来源：作者）
- en: The results based on the AFINN lexicon seem to be quite different! Once we take
    the “weight” of the tokens into account, most media briefings turn out to be positive
    (see the green bars), although there are still some days with negative sentiments
    (see the red bars). The two analyses we have done so far have yielded very different
    for two reasons. First, as I mentioned above, the Bing lexicon focuses on the
    polarity of the words but ignores the intensity of the words (dislike and hate
    are considered negative words with equal intensity). Unlike the Bing lexicon,
    the AFINN lexicon takes the intensity into account, which impacts the calculation
    of the sentiment scores. Second, the Bing lexicon (6786 words) is fairly larger
    than the AFINN lexicon (2477 words). Therefore, it is likely that some tokens
    in the media briefings are included in the Bing lexicon, but not in the AFINN
    lexicon. Disregarding those tokens might have impacted the results.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 AFINN 词典的结果似乎大不相同！一旦我们考虑到词语的“权重”，大多数媒体简报的情感被认为是积极的（见绿色条形图），尽管仍然有一些日子存在负面情感（见红色条形图）。到目前为止，我们做的两项分析结果差异很大，原因有二。首先，正如我之前提到的，Bing
    词典专注于词语的极性，而忽略了词语的强度（“不喜欢”和“仇恨”被认为是强度相同的消极词语）。与 Bing 词典不同，AFINN 词典考虑了词语的强度，这影响了情感得分的计算。其次，Bing
    词典（6786 个词语）比 AFINN 词典（2477 个词语）要大得多。因此，媒体简报中的一些标记可能被包含在 Bing 词典中，但不在 AFINN 词典中。忽略这些标记可能影响了结果。
- en: The final lexicon we are going to try using the **tidytext** package is NRC.
    As I mentioned earlier, this lexicon uses Plutchik’s psych-evolutionary theory
    to label the tokens based on basic emotions such as anger, fear, and anticipation.
    We are going to count the number of words or tokens associated with each emotion
    and then visualize the results.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将尝试使用 **tidytext** 包中的最终词典是 NRC。正如我之前提到的，这个词典基于 Plutchik 的心理进化理论，对标记的词语进行分类，主要依据基本情感，如愤怒、恐惧和预期。我们将统计与每种情感相关的单词或标记数量，并可视化结果。
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/e8024c832941740e0f097412e6fe3ab3.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8024c832941740e0f097412e6fe3ab3.png)'
- en: Sentiment scores based on the NRC lexicon (Image by author)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 NRC 词典的情感得分（图片来源：作者）
- en: The figure shows that the media briefings are mostly positive each month. Dr.
    Hinshaw used words associated with “trust”, “anticipation”, and “fear”. Overall,
    the pattern of these emotions seems to remain very similar over time, indicating
    the consistency of the media briefings in terms of the type and intensity of the
    emotions delivered.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示每月的媒体简报大多是积极的。Hinshaw 博士使用了与“信任”、“预期”和“恐惧”相关的词语。总体而言，这些情感的模式似乎随着时间的推移保持非常相似，表明媒体简报在传递情感的类型和强度上具有一致性。
- en: Sentiment Analysis with sentimentr
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 sentimentr 进行情感分析
- en: 'Another package for lexicon-based sentiment analysis is **sentimentr** ([Rinker,
    2021](https://okan.cloud/posts/2024-02-09-lexicon-based-sentiment-analysis-using-r/#ref-R-sentiment)).
    Unlike the **tidytext** package, this package takes valence shifters (e.g., negation)
    into account, which can easily flip the polarity of a sentence with one word.
    For example, the sentence “I am not unhappy” is actually positive, but if we analyze
    it word by word, the sentence may seem to have a negative sentiment due to the
    words “not” and “unhappy”. Similarly, “I hardly like this book” is a negative
    sentence but the analysis of individual words, “hardly” and “like”, may yield
    a positive sentiment score. The **sentimentr** package addresses the limitations
    around sentiment detection with valence shifters (see the package author Tyler
    Rinker’s Github page for further details on **sentimentr**: [https://github.com/trinker/sentimentr](https://github.com/trinker/sentimentr)).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个基于词汇的情感分析包是**sentimentr**（[Rinker, 2021](https://okan.cloud/posts/2024-02-09-lexicon-based-sentiment-analysis-using-r/#ref-R-sentiment)）。与**tidytext**包不同，这个包考虑了情感转移因素（例如，否定词），这些因素可以通过一个词轻松地翻转句子的情感极性。例如，句子“I
    am not unhappy”实际上是正面的，但如果我们逐字分析，句子可能因为“not”和“unhappy”这两个词而看起来有负面情感。类似地，“I hardly
    like this book”是负面句子，但单独分析“hardly”和“like”这两个词时，可能会得到一个正面的情感评分。**sentimentr**包解决了情感检测中情感转移因素的限制（有关**sentimentr**的更多详细信息，请参见包作者Tyler
    Rinker的Github页面：[https://github.com/trinker/sentimentr](https://github.com/trinker/sentimentr)）。
- en: To benefit from the **sentimentr** package, we need the actual sentences in
    the media briefings rather than the individual tokens. Therefore, I had to create
    an untokenized version of the dataset, which is available [**here**](https://github.com/okanbulut/blog/raw/master/data_and_codes/wave1_alberta_sentence.RData).
    We will first import this dataset into R, get individual sentences for each media
    briefing using the `get_sentences()` function, and then calculate sentiment scores
    by day and month via `sentiment_by()`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用**sentimentr**包，我们需要媒体简报中的实际句子，而不是单独的词汇。因此，我不得不创建一个未分词的数据集版本，该版本可通过[**此处**](https://github.com/okanbulut/blog/raw/master/data_and_codes/wave1_alberta_sentence.RData)下载。我们将首先将这个数据集导入R，使用`get_sentences()`函数获取每个媒体简报的单独句子，然后通过`sentiment_by()`按天和月计算情感评分。
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/1ac1b34b41d6208e8255af9454d93fea.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ac1b34b41d6208e8255af9454d93fea.png)'
- en: A preview of the dataset (Image by author)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集预览（作者提供的图片）
- en: In the dataset we created, “ave_sentiment” is the average sentiment score for
    each day in March, April, May, and June (i.e., days where a media briefing was
    made). Using this dataset, we can visualize the sentiment scores.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建的数据集中，“ave_sentiment”是3月、4月、5月和6月每一天的平均情感评分（即有媒体简报的日子）。利用这个数据集，我们可以可视化情感评分。
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/b571e9a77534d4efa810382622d45a36.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b571e9a77534d4efa810382622d45a36.png)'
- en: Sentiment scores based on sentiment (Image by author)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基于情感的情感评分（作者提供的图片）
- en: In the figure above, the blue bars represent highly positive sentiment scores,
    while the red bars depict comparatively lower sentiment scores. The patterns observed
    in the sentiment scores generated by **sentimentr** closely resemble those derived
    from the AFINN lexicon. Notably, this analysis is based on the original media
    briefings rather than solely tokens, with consideration given to valence shifters
    in the computation of sentiment scores. The convergence between the sentiment
    patterns identified by **sentimentr** and those from AFINN is not entirely unexpected.
    Both approaches incorporate similar weighting systems and mechanisms that account
    for word intensity. This alignment reinforces our confidence in the initial findings
    obtained through AFINN, validating the consistency and reliability of our analyses
    with **sentiment**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图中，蓝色条形表示高度正面的情感评分，而红色条形则表示相对较低的情感评分。**sentimentr**生成的情感评分模式与AFINN词汇表得出的模式非常相似。值得注意的是，这个分析是基于原始的媒体简报，而不仅仅是单独的词汇，情感评分的计算考虑了情感转移因素。**sentimentr**和AFINN在情感模式上的一致性并不令人意外。两种方法都采用了类似的加权系统和机制，考虑了词汇强度。这种一致性增强了我们对通过AFINN得出的初步结论的信心，验证了我们使用**sentiment**进行分析的可靠性和一致性。
- en: Concluding Remarks
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, lexicon-based sentiment analysis in R offers a powerful tool
    for uncovering the emotional nuances within textual data. Throughout this post,
    we have explored the fundamental concepts of lexicon-based sentiment analysis
    and provided a practical demonstration of its implementation using R. By leveraging
    packages such as **sentimentr** and **tidytext**, we have illustrated how sentiment
    analysis can be seamlessly integrated into your data analysis workflow. As you
    embark on your journey into sentiment analysis, remember that the insights gained
    from this technique extend far beyond the surface of the text. They provide valuable
    perspectives on public opinion, consumer sentiment, and beyond. I encourage you
    to delve deeper into lexicon-based sentiment analysis, experiment with the examples
    presented here, and unlock the rich insights waiting to be discovered within your
    own data. Happy analyzing!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，基于词典的情感分析在R语言中提供了一种强大的工具，用于揭示文本数据中的情感细微差别。在这篇文章中，我们探讨了基于词典的情感分析的基本概念，并提供了使用R实现这一方法的实践示范。通过利用如**sentimentr**和**tidytext**等包，我们展示了如何将情感分析无缝地集成到数据分析工作流中。当你踏上情感分析的旅程时，请记住，从这项技术中获得的洞察远远超越了文本的表面。它们为公众舆论、消费者情感等提供了宝贵的视角。我鼓励你深入探讨基于词典的情感分析，尝试这里呈现的示例，解锁你自己数据中等待发现的丰富洞察。祝你分析愉快！
- en: References
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Bulut, O., & Poth, C. N. (2022). Rapid assessment of communication consistency:
    Sentiment analysis of public health briefings during the COVID-19 pandemic. *AIMS
    Public Health*, *9*(2), 293–306\. [https://doi.org/10.3934/publichealth.2022020](https://doi.org/10.3934/publichealth.2022020)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bulut, O., & Poth, C. N. (2022). 快速评估沟通一致性：COVID-19疫情期间公共卫生简报的情感分析。*AIMS公共卫生*，*9*(2)，293–306。
    [https://doi.org/10.3934/publichealth.2022020](https://doi.org/10.3934/publichealth.2022020)'
- en: '[2] Poth, C. N., Bulut, O., Aquilina, A. M., & Otto, S. J. G. (2021). Using
    data mining for rapid complex case study descriptions: Example of public health
    briefings during the onset of the COVID-19 pandemic. *Journal of Mixed Methods
    Research*, *15*(3), 348–373\. [https://doi.org/10.1177/15586898211013925](https://doi.org/10.1177/15586898211013925)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Poth, C. N., Bulut, O., Aquilina, A. M., & Otto, S. J. G. (2021). 使用数据挖掘进行快速复杂案例研究描述：以COVID-19疫情初期的公共卫生简报为例。*混合方法研究杂志*，*15*(3)，348–373。
    [https://doi.org/10.1177/15586898211013925](https://doi.org/10.1177/15586898211013925)'
- en: '[3] Hu, M., & Liu, B. (2004). Mining and summarizing customer reviews. *Proceedings
    of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining*, 168–177.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Hu, M., & Liu, B. (2004). 挖掘和总结客户评价。*第十届ACM SIGKDD国际知识发现与数据挖掘大会论文集*，168–177。'
- en: '[4] Mohammad, S. M., & Turney, P. D. (2013). Crowdsourcing a word–emotion association
    lexicon. *Computational Intelligence*, *29*(3), 436–465.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Mohammad, S. M., & Turney, P. D. (2013). 众包构建词语–情感联想词典。*计算智能*，*29*(3)，436–465。'
- en: '[5] Plutchik, R. (1980). A general psychoevolutionary theory of emotion. In
    *Theories of emotion* (pp. 3–33). Elsevier.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Plutchik, R. (1980). 情感的通用心理进化理论。收录于*情感理论*（第3–33页）。Elsevier。'
