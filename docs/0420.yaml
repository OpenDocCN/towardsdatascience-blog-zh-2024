- en: Lexicon-Based Sentiment Analysis Using R
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºäºè¯å…¸çš„æƒ…æ„Ÿåˆ†æä½¿ç”¨Rè¯­è¨€
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/lexicon-based-sentiment-analysis-using-r-5c1db85984a1?source=collection_archive---------13-----------------------#2024-02-13](https://towardsdatascience.com/lexicon-based-sentiment-analysis-using-r-5c1db85984a1?source=collection_archive---------13-----------------------#2024-02-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/lexicon-based-sentiment-analysis-using-r-5c1db85984a1?source=collection_archive---------13-----------------------#2024-02-13](https://towardsdatascience.com/lexicon-based-sentiment-analysis-using-r-5c1db85984a1?source=collection_archive---------13-----------------------#2024-02-13)
- en: An empirical analysis of sentiments conveyed through media briefings during
    the COVID-19 pandemic
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€é¡¹å…³äºCOVID-19å¤§æµè¡ŒæœŸé—´åª’ä½“ç®€æŠ¥æ‰€ä¼ è¾¾æƒ…æ„Ÿçš„å®è¯åˆ†æ
- en: '[](https://drokanbulut.medium.com/?source=post_page---byline--5c1db85984a1--------------------------------)[![Okan
    Bulut](../Images/555a4b1818ac0b5d0766f3ad7ab71a6f.png)](https://drokanbulut.medium.com/?source=post_page---byline--5c1db85984a1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5c1db85984a1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5c1db85984a1--------------------------------)
    [Okan Bulut](https://drokanbulut.medium.com/?source=post_page---byline--5c1db85984a1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://drokanbulut.medium.com/?source=post_page---byline--5c1db85984a1--------------------------------)[![Okan
    Bulut](../Images/555a4b1818ac0b5d0766f3ad7ab71a6f.png)](https://drokanbulut.medium.com/?source=post_page---byline--5c1db85984a1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5c1db85984a1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5c1db85984a1--------------------------------)
    [Okan Bulut](https://drokanbulut.medium.com/?source=post_page---byline--5c1db85984a1--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5c1db85984a1--------------------------------)
    Â·14 min readÂ·Feb 13, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5c1db85984a1--------------------------------)
    Â·14åˆ†é’Ÿé˜…è¯»Â·2024å¹´2æœˆ13æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5eb08923969d0e60e8b3c64f98ab28eb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5eb08923969d0e60e8b3c64f98ab28eb.png)'
- en: Image by [Gino Crescoli](https://pixabay.com/users/absolutvision-6158753/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2979107)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2979107)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Gino Crescoli](https://pixabay.com/users/absolutvision-6158753/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2979107)æä¾›ï¼Œæ¥æºäº[Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2979107)
- en: 'During the COVID-19 pandemic, I decided to learn a new statistical technique
    to keep my mind occupied rather than constantly immersing myself in pandemic-related
    news. After evaluating several options, I found the concepts related to natural
    language processing (NLP) particularly captivating. So, I opted to delve deeper
    into this field and explore one specific technique: sentiment analysis, also known
    as â€œopinion miningâ€ in academic literature. This analytical method empowers researchers
    to extract and interpret the emotions conveyed toward a specific subject within
    the written text. Through sentiment analysis, one can discern the polarity (positive
    or negative), nature, and intensity of sentiments expressed across various textual
    formats such as documents, customer reviews, and social media posts.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨COVID-19å¤§æµè¡ŒæœŸé—´ï¼Œæˆ‘å†³å®šå­¦ä¹ ä¸€é¡¹æ–°çš„ç»Ÿè®¡æŠ€æœ¯ï¼Œä»¥ä¾¿è®©è‡ªå·±ä¸è‡³äºæ€»æ˜¯æ²‰æµ¸åœ¨ä¸å¤§æµè¡Œç›¸å…³çš„æ–°é—»ä¸­ã€‚ç»è¿‡è¯„ä¼°å¤šä¸ªé€‰é¡¹åï¼Œæˆ‘å‘ç°ä¸è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç›¸å…³çš„æ¦‚å¿µç‰¹åˆ«å¸å¼•äººã€‚å› æ­¤ï¼Œæˆ‘å†³å®šæ·±å…¥ç ”ç©¶è¿™ä¸ªé¢†åŸŸï¼Œå¹¶æ¢ç´¢å…¶ä¸­ä¸€ç§å…·ä½“çš„æŠ€æœ¯ï¼šæƒ…æ„Ÿåˆ†æï¼Œä¹Ÿè¢«å­¦æœ¯æ–‡çŒ®ç§°ä¸ºâ€œæ„è§æŒ–æ˜â€ã€‚è¿™ç§åˆ†ææ–¹æ³•ä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿæå–å¹¶è§£é‡Šå†™ä½œæ–‡æœ¬ä¸­ä¼ è¾¾çš„æƒ…æ„Ÿï¼Œé’ˆå¯¹ç‰¹å®šä¸»é¢˜è¿›è¡Œæƒ…æ„Ÿçš„åˆ†æã€‚é€šè¿‡æƒ…æ„Ÿåˆ†æï¼Œäººä»¬å¯ä»¥è¾¨åˆ«æƒ…æ„Ÿçš„ææ€§ï¼ˆæ­£é¢æˆ–è´Ÿé¢ï¼‰ã€æ€§è´¨ä»¥åŠåœ¨å„ç§æ–‡æœ¬æ ¼å¼ï¼ˆå¦‚æ–‡æ¡£ã€å®¢æˆ·è¯„ä»·å’Œç¤¾äº¤åª’ä½“å¸–å­ï¼‰ä¸­çš„å¼ºåº¦ã€‚
- en: Amidst the pandemic, I observed a significant trend among researchers who turned
    to sentiment analysis as a tool to measure public responses to news and developments
    surrounding the virus. This involved analyzing user-generated content on popular
    social media platforms such as Twitter, YouTube, and Instagram. Intrigued by this
    methodology, my colleagues and I endeavored to contribute to the existing body
    of research by scrutinizing the daily briefings provided by public health authorities.
    In Alberta, Dr. Deena Hinshaw, who used to be the provinceâ€™s chief medical officer
    of health, regularly delivered [updates on the regionâ€™s response](https://www.youtube.com/watch?v=fvw_USRfXgY)
    to the ongoing pandemic. Through our analysis of these public health announcements,
    we aimed to assess Albertaâ€™s effectiveness in implementing communication strategies
    during this intricate public health crisis. Our investigation, conducted through
    the lenses of sentiment analysis, sought to shed light on the efficacy of communication
    strategies employed during this challenging period in public health [1, 2].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç–«æƒ…æœŸé—´ï¼Œæˆ‘è§‚å¯Ÿåˆ°è®¸å¤šç ”ç©¶äººå‘˜å°†æƒ…æ„Ÿåˆ†æä½œä¸ºè¡¡é‡å…¬ä¼—å¯¹ä¸ç—…æ¯’ç›¸å…³çš„æ–°é—»å’Œå‘å±•ååº”çš„å·¥å…·ã€‚è¿™åŒ…æ‹¬åˆ†æç”¨æˆ·ç”Ÿæˆçš„å†…å®¹ï¼Œä¸»è¦æ¥è‡ªTwitterã€YouTubeå’ŒInstagramç­‰æµè¡Œç¤¾äº¤åª’ä½“å¹³å°ã€‚ç”±äºå¯¹è¿™ç§æ–¹æ³•äº§ç”Ÿäº†å…´è¶£ï¼Œæˆ‘å’Œæˆ‘çš„åŒäº‹ä»¬åŠªåŠ›é€šè¿‡åˆ†æå…¬å…±å«ç”Ÿæœºæ„æä¾›çš„æ¯æ—¥ç®€æŠ¥ï¼Œæ¥ä¸ºç°æœ‰çš„ç ”ç©¶è´¡çŒ®åŠ›é‡ã€‚åœ¨é˜¿å°”ä¼¯å¡”çœï¼Œæ›¾æ‹…ä»»è¯¥çœé¦–å¸­å…¬å…±å«ç”Ÿå®˜çš„**è¿ªå¨œÂ·è¾›è‚–**åšå£«å®šæœŸå‘å¸ƒ[å…³äºè¯¥åœ°åŒºåº”å¯¹ç–«æƒ…çš„æ›´æ–°](https://www.youtube.com/watch?v=fvw_USRfXgY)ã€‚é€šè¿‡åˆ†æè¿™äº›å…¬å…±å«ç”Ÿå…¬å‘Šï¼Œæˆ‘ä»¬æ—¨åœ¨è¯„ä¼°é˜¿å°”ä¼¯å¡”çœåœ¨è¿™ä¸€å¤æ‚å…¬å…±å«ç”Ÿå±æœºä¸­å®æ–½æ²Ÿé€šç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶é‡‡ç”¨æƒ…æ„Ÿåˆ†æçš„æ–¹æ³•ï¼Œæ—¨åœ¨æ­ç¤ºåœ¨è¿™ä¸ªå……æ»¡æŒ‘æˆ˜çš„å…¬å…±å«ç”Ÿæ—¶æœŸä¸­ï¼Œæ‰€é‡‡å–çš„æ²Ÿé€šç­–ç•¥çš„æ•ˆæœ[1,
    2]ã€‚
- en: In this post, I aim to walk you through the process of performing sentiment
    analysis using R. Specifically, Iâ€™ll focus on â€œlexicon-based sentiment analysis,â€
    which Iâ€™ll discuss in more detail in the next section. Iâ€™ll provide examples of
    lexicon-based sentiment analysis that weâ€™ve integrated into the publications referenced
    earlier. Additionally, in future posts, Iâ€™ll delve into more advanced forms of
    sentiment analysis, making use of state-of-the-art pre-trained models accessible
    on [Hugging Face](https://huggingface.co/docs/transformers/en/index).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å¼•å¯¼ä½ ä»¬äº†è§£å¦‚ä½•ä½¿ç”¨Rè¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘å°†é‡ç‚¹ä»‹ç»â€œåŸºäºè¯å…¸çš„æƒ…æ„Ÿåˆ†æâ€ï¼Œå¹¶åœ¨ä¸‹ä¸€èŠ‚ä¸­å¯¹æ­¤è¿›è¡Œæ›´è¯¦ç»†çš„è®¨è®ºã€‚æˆ‘å°†æä¾›ä¸€äº›æˆ‘ä»¬ä¹‹å‰å¼•ç”¨çš„å‡ºç‰ˆç‰©ä¸­æ•´åˆçš„åŸºäºè¯å…¸çš„æƒ…æ„Ÿåˆ†æçš„ä¾‹å­ã€‚æ­¤å¤–ï¼Œåœ¨æœªæ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘è¿˜å°†æ·±å…¥æ¢è®¨æ›´å…ˆè¿›çš„æƒ…æ„Ÿåˆ†ææ–¹æ³•ï¼Œåˆ©ç”¨[Hugging
    Face](https://huggingface.co/docs/transformers/en/index)ä¸Šå¯è®¿é—®çš„æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚
- en: Lexicon-Based Sentiment Analysis
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºäºè¯å…¸çš„æƒ…æ„Ÿåˆ†æ
- en: As I learned more about sentiment analysis, I discovered that the predominant
    method for extracting sentiments is lexicon-based sentiment analysis. This approach
    entails utilizing a specific lexicon, essentially the vocabulary of a language
    or subject, to discern the direction and intensity of sentiments conveyed within
    a given text. Some lexicons, like the Bing lexicon [3], classify words as either
    positive or negative. Conversely, other lexicons provide more detailed sentiment
    labels, such as the NRC Emotion Lexicon [4], which categorizes words based on
    both positive and negative sentiments, as well as Plutchikâ€™s [5] psych evolutionary
    theory of basic emotions (e.g., anger, fear, anticipation, trust, surprise, sadness,
    joy, and disgust).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘æ·±å…¥äº†è§£æƒ…æ„Ÿåˆ†æçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘å‘ç°æå–æƒ…æ„Ÿçš„ä¸»è¦æ–¹æ³•æ˜¯åŸºäºè¯å…¸çš„æƒ…æ„Ÿåˆ†æã€‚è¿™ç§æ–¹æ³•æ¶‰åŠä½¿ç”¨ç‰¹å®šçš„è¯å…¸ï¼Œå³è¯­è¨€æˆ–ä¸»é¢˜çš„è¯æ±‡ï¼Œæ¥è¯†åˆ«ç»™å®šæ–‡æœ¬ä¸­æ‰€è¡¨è¾¾çš„æƒ…æ„Ÿçš„æ–¹å‘å’Œå¼ºåº¦ã€‚æœ‰äº›è¯å…¸ï¼Œå¦‚Bingè¯å…¸[3]ï¼Œå°†å•è¯åˆ†ç±»ä¸ºæ­£é¢æˆ–è´Ÿé¢ã€‚ç›¸åï¼Œå…¶ä»–è¯å…¸åˆ™æä¾›æ›´è¯¦ç»†çš„æƒ…æ„Ÿæ ‡ç­¾ï¼Œä¾‹å¦‚NRCæƒ…æ„Ÿè¯å…¸[4]ï¼Œå®ƒæ ¹æ®æ­£é¢å’Œè´Ÿé¢æƒ…æ„Ÿå¯¹å•è¯è¿›è¡Œåˆ†ç±»ï¼Œè¿˜åŒ…æ‹¬æ™®é²å¥‡å…‹çš„[5]åŸºæœ¬æƒ…æ„Ÿçš„å¿ƒç†è¿›åŒ–ç†è®ºï¼ˆä¾‹å¦‚ï¼Œæ„¤æ€’ã€ææƒ§ã€æœŸå¾…ã€ä¿¡ä»»ã€æƒŠè®¶ã€æ‚²ä¼¤ã€å–œæ‚¦å’ŒåŒæ¶ï¼‰ã€‚
- en: Lexicon-based sentiment analysis operates by aligning words within a given text
    with those found in widely used lexicons such as NRC and Bing. Each word receives
    an assigned sentiment, typically categorized as positive or negative. The textâ€™s
    collective sentiment score is subsequently derived by summing the individual sentiment
    scores of its constituent words. For instance, in a scenario where a text incorporates
    50 positive and 30 negative words according to the Bing lexicon, the resulting
    sentiment score would be 20\. This value indicates a predominance of positive
    sentiments within the text. Conversely, a negative total would imply a prevalence
    of negative sentiments.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Performing lexicon-based sentiment analysis using R can be both fun and tricky
    at the same time. While analyzing public health announcements in terms of sentiments,
    I found Julia Silge and David Robinsonâ€™s book, [*Text Mining with R*](https://www.tidytextmining.com/),
    to be very helpful. The book has [a chapter dedicated to sentiment analysis](https://www.tidytextmining.com/sentiment),
    where the authors demonstrate how to conduct sentiment analysis using general-purpose
    lexicons like Bing and NRC. However, Julia and David also highlight a major limitation
    of lexicon-based sentiment analysis. The analysis considers only single words
    (i.e., unigrams) and does not consider qualifiers before a word. For instance,
    negation words like â€œnotâ€ in â€œnot trueâ€ are ignored, and sentiment analysis processes
    them as two separate words, â€œnotâ€ and â€œtrueâ€. Furthermore, if a particular word
    (either positive or negative) is repeatedly used throughout the text, this may
    skew the results depending on the polarity (positive or negative) of this word.
    Therefore, the results of lexicon-based sentiment analysis should be interpreted
    carefully.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Now, letâ€™s move to our example, where we will conduct lexicon-based sentiment
    analysis using Dr. Deena Hinshawâ€™s media briefings during the COVID-19 pandemic.
    My goal is to showcase two R packages capable of running sentiment analysis ğŸ“‰.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Example
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the sake of simplicity, we will focus on the first wave of the pandemic
    (March 2020 â€” June 2020). The transcripts of all media briefings were publicly
    available on the government of Albertaâ€™s COVID-19 pandemic website ([https://www.alberta.ca/covid](https://www.alberta.ca/covid)).
    This dataset comes with an [open data license](https://open.alberta.ca/licence)
    that allows the public to access and use the information, including for commercial
    purposes. After importing these transcripts into R, I turned all the text into
    lowercase and then applied word tokenization using the **tidytext** and **tokenizers**
    packages. Word tokenization split the sentences in the media briefings into individual
    words for each entry (i.e., day of media briefings). Next, I applied lemmatization
    to the tokens to resolve each word into its canonical form using the **textstem**
    package. Finally, I removed common stopwords, such as â€œmy,â€ â€œfor,â€ â€œthat,â€ â€œwith,â€
    and â€œfor, using the stopwords package. The final dataset is available [**here**](https://github.com/okanbulut/blog/raw/master/data_and_codes/wave1_alberta.RData).
    Now, letâ€™s import the data into R and then review its content.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºç–«æƒ…çš„ç¬¬ä¸€æ³¢ï¼ˆ2020å¹´3æœˆ â€” 2020å¹´6æœˆï¼‰ã€‚æ‰€æœ‰åª’ä½“ç®€æŠ¥çš„æ–‡å­—è®°å½•åœ¨é˜¿å°”ä¼¯å¡”çœæ”¿åºœçš„COVID-19ç–«æƒ…ç½‘ç«™ä¸Šå…¬å¼€å‘å¸ƒï¼ˆ[https://www.alberta.ca/covid](https://www.alberta.ca/covid)ï¼‰ã€‚è¯¥æ•°æ®é›†é™„å¸¦ä¸€ä¸ª[å¼€æ”¾æ•°æ®è®¸å¯è¯](https://open.alberta.ca/licence)ï¼Œå…è®¸å…¬ä¼—è®¿é—®å’Œä½¿ç”¨è¿™äº›ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç”¨äºå•†ä¸šç›®çš„ã€‚åœ¨å°†è¿™äº›æ–‡å­—è®°å½•å¯¼å…¥Råï¼Œæˆ‘å°†æ‰€æœ‰æ–‡æœ¬è½¬æ¢ä¸ºå°å†™å­—æ¯ï¼Œç„¶åä½¿ç”¨**tidytext**å’Œ**tokenizers**åŒ…è¿›è¡Œè¯å…ƒåŒ–ã€‚è¯å…ƒåŒ–å°†åª’ä½“ç®€æŠ¥ä¸­çš„å¥å­æ‹†åˆ†ä¸ºæ¯ä¸ªæ¡ç›®çš„å•ä¸ªå•è¯ï¼ˆå³ï¼Œåª’ä½“ç®€æŠ¥çš„æ¯ä¸€å¤©ï¼‰ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä½¿ç”¨**textstem**åŒ…å¯¹è¯å…ƒè¿›è¡Œäº†è¯å½¢è¿˜åŸï¼Œå°†æ¯ä¸ªå•è¯è½¬åŒ–ä¸ºå…¶è§„èŒƒå½¢å¼ã€‚æœ€åï¼Œæˆ‘ä½¿ç”¨stopwordsåŒ…ç§»é™¤äº†å¸¸è§çš„åœç”¨è¯ï¼Œå¦‚â€œmyâ€ã€â€œforâ€ã€â€œthatâ€ã€â€œwithâ€å’Œâ€œforâ€ã€‚æœ€ç»ˆçš„æ•°æ®é›†å¯ä»¥åœ¨[**æ­¤å¤„**](https://github.com/okanbulut/blog/raw/master/data_and_codes/wave1_alberta.RData)è·å¾—ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†æ•°æ®å¯¼å…¥Rå¹¶æŸ¥çœ‹å…¶å†…å®¹ã€‚
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/d1603ea9ba4436bfd2d51e226a839b7b.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1603ea9ba4436bfd2d51e226a839b7b.png)'
- en: A preview of the dataset (Image by author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†é¢„è§ˆï¼ˆä½œè€…æä¾›çš„å›¾åƒï¼‰
- en: 'The dataset has three columns:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†æœ‰ä¸‰åˆ—ï¼š
- en: month (the month of the media briefing)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœˆä»½ï¼ˆåª’ä½“ç®€æŠ¥çš„æœˆä»½ï¼‰
- en: date (the exact date of the media briefing), and
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼ˆåª’ä½“ç®€æŠ¥çš„ç¡®åˆ‡æ—¥æœŸï¼‰ï¼Œä»¥åŠ
- en: word (words or tokens used in media briefing)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•è¯ï¼ˆåª’ä½“ç®€æŠ¥ä¸­ä½¿ç”¨çš„å•è¯æˆ–è¯å…ƒï¼‰
- en: Descriptive Analysis
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æè¿°æ€§åˆ†æ
- en: Now, we can calculate some descriptive statistics to better understand the content
    of our dataset. We will begin by finding the top 5 words (based on their frequency)
    for each month.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ä¸€äº›æè¿°æ€§ç»Ÿè®¡æ•°æ®ï¼Œä»¥æ›´å¥½åœ°ç†è§£æ•°æ®é›†çš„å†…å®¹ã€‚æˆ‘ä»¬å°†é¦–å…ˆæŒ‰æœˆä»½ï¼ˆåŸºäºè¯é¢‘ï¼‰æ‰¾å‡ºå‰5ä¸ªå•è¯ã€‚
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/759fd7f9b972d9892bc76ad34db081be.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/759fd7f9b972d9892bc76ad34db081be.png)'
- en: Top 5 words by months (Image by author)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰æœˆä»½æ’åºçš„å‰5ä¸ªå•è¯ï¼ˆä½œè€…æä¾›çš„å›¾åƒï¼‰
- en: 'The output shows that words such as health, continue, and test were commonly
    used in the media briefings across this 4-month period. We can also expand our
    list to the most common 10 words and view the results visually:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºæ˜¾ç¤ºï¼Œåœ¨è¿™4ä¸ªæœˆçš„åª’ä½“ç®€æŠ¥ä¸­ï¼Œâ€œhealthâ€ã€â€œcontinueâ€å’Œâ€œtestâ€ç­‰è¯è¯­è¢«é¢‘ç¹ä½¿ç”¨ã€‚æˆ‘ä»¬è¿˜å¯ä»¥æ‰©å±•æˆ‘ä»¬çš„åˆ—è¡¨ï¼ŒæŸ¥çœ‹æœ€å¸¸ç”¨çš„10ä¸ªå•è¯ï¼Œå¹¶ä»¥å¯è§†åŒ–æ–¹å¼å‘ˆç°ç»“æœï¼š
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/2b8308d5610aef09dbe4729f628a162d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b8308d5610aef09dbe4729f628a162d.png)'
- en: Most common words based on frequency (Image by author)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºé¢‘ç‡çš„æœ€å¸¸ç”¨å•è¯ï¼ˆä½œè€…æä¾›çš„å›¾åƒï¼‰
- en: Since some words are common across all four months, the plot above may not necessarily
    show us the important words that are unique to each month. To find such important
    words, we can use Term Frequency â€” Inverse Document Frequency (TF-IDF)â€“a widely
    used technique in NLP for measuring how important a term is within a document
    relative to a collection of documents (for more detailed information about TF-IDF,
    check out [my previous blog post](https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/#tf-idf)).
    In our example, we will treat media briefings for each month as a document and
    calculate TF-IDF for the tokens (i.e., words) within each document. The first
    part of the R codes below creates a new dataset, *wave1_tf_idf*, by calculating
    TF-IDF for all tokens and selecting the tokens with the highest TF-IDF values
    within each month. Next, we use this dataset to create a bar plot with the TF-IDF
    values to view the common words unique to each month.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºä¸€äº›è¯æ±‡åœ¨å››ä¸ªæœˆå†…éƒ½æ˜¯é€šç”¨çš„ï¼Œä¸Šé¢çš„å›¾è¡¨å¯èƒ½æ— æ³•ç›´æ¥å±•ç¤ºæ¯ä¸ªæœˆç‰¹æœ‰çš„é‡è¦è¯æ±‡ã€‚ä¸ºäº†æ‰¾å‡ºè¿™äº›é‡è¦çš„è¯æ±‡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡ï¼ˆTF-IDFï¼‰æŠ€æœ¯â€”â€”è¿™æ˜¯ä¸€ç§åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­å¹¿æ³›ä½¿ç”¨çš„æŠ€æœ¯ï¼Œç”¨äºè¡¡é‡ä¸€ä¸ªè¯åœ¨æ–‡æ¡£ä¸­ç›¸å¯¹äºä¸€ç»„æ–‡æ¡£çš„é‡è¦æ€§ï¼ˆæœ‰å…³TF-IDFçš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[æˆ‘ä¹‹å‰çš„åšå®¢æ–‡ç« ](https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/#tf-idf)ï¼‰ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªæœˆçš„åª’ä½“ç®€æŠ¥è§†ä¸ºä¸€ä¸ªæ–‡æ¡£ï¼Œå¹¶è®¡ç®—æ–‡æ¡£ä¸­æ¯ä¸ªè¯æ±‡çš„TF-IDFã€‚ä¸‹é¢çš„Rä»£ç ç¬¬ä¸€éƒ¨åˆ†é€šè¿‡è®¡ç®—æ‰€æœ‰è¯æ±‡çš„TF-IDFå¹¶é€‰æ‹©æ¯ä¸ªæœˆå†…TF-IDFå€¼æœ€é«˜çš„è¯æ±‡ï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œ*wave1_tf_idf*ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨è¯¥æ•°æ®é›†åˆ›å»ºä¸€ä¸ªæ¡å½¢å›¾ï¼Œæ˜¾ç¤ºæ¯ä¸ªæœˆç‰¹æœ‰çš„å¸¸è§è¯æ±‡ã€‚
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/d6e77561244b9eb493176747ea3cac6b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6e77561244b9eb493176747ea3cac6b.png)'
- en: Most common words based on TIF-IDF (Image by author)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºTF-IDFçš„æœ€å¸¸è§è¯æ±‡ï¼ˆå›¾ç‰‡æ¥è‡ªä½œè€…ï¼‰
- en: These results are more informative because the tokens shown in the figure reflect
    unique topics discussed each month. For example, in March 2020, the media briefings
    were mostly about limiting travel, returning from crowded conferences, and COVID-19
    cases on cruise ships. In June 2020, the focus of the media briefings shifted
    towards mask requirements, people protesting pandemic-related restrictions, and
    so on.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç»“æœæä¾›äº†æ›´å¤šçš„ä¿¡æ¯ï¼Œå› ä¸ºå›¾ä¸­æ˜¾ç¤ºçš„è¯æ±‡åæ˜ äº†æ¯ä¸ªæœˆè®¨è®ºçš„ç‹¬ç‰¹ä¸»é¢˜ã€‚ä¾‹å¦‚ï¼Œåœ¨2020å¹´3æœˆï¼Œåª’ä½“ç®€æŠ¥ä¸»è¦è®¨è®ºäº†é™åˆ¶æ—…è¡Œã€ä»äººç¾¤å¯†é›†çš„ä¼šè®®è¿”å›ä»¥åŠé‚®è½®ä¸Šçš„COVID-19ç—…ä¾‹ã€‚åˆ°äº†2020å¹´6æœˆï¼Œåª’ä½“ç®€æŠ¥çš„é‡ç‚¹è½¬å‘äº†å£ç½©è¦æ±‚ã€äººä»¬æŠ—è®®ä¸ç–«æƒ…ç›¸å…³çš„é™åˆ¶ç­‰ç­‰ã€‚
- en: 'Before we switch back to the sentiment analysis, letâ€™s take a look at another
    descriptive variable: the length of each media briefing. This will show us whether
    the media briefings became longer or shorter over time.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬åˆ‡æ¢å›æƒ…æ„Ÿåˆ†æä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦ä¸€ä¸ªæè¿°æ€§å˜é‡ï¼šæ¯ä¸ªåª’ä½“ç®€æŠ¥çš„é•¿åº¦ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬äº†è§£åª’ä½“ç®€æŠ¥éšç€æ—¶é—´çš„æ¨ç§»æ˜¯å˜å¾—æ›´é•¿è¿˜æ˜¯æ›´çŸ­ã€‚
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/8c46ac6ce65bdb88d89a63b92a501b6f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c46ac6ce65bdb88d89a63b92a501b6f.png)'
- en: Number of words in the media briefings by day (Image by author)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ—¥åª’ä½“ç®€æŠ¥ä¸­çš„è¯æ±‡æ•°ï¼ˆå›¾ç‰‡æ¥è‡ªä½œè€…ï¼‰
- en: The figure above shows that the length of media briefings varied quite substantially
    over time. Especially in March and May, there are larger fluctuations (i.e., very
    long or short briefings), whereas, in June, the daily media briefings are quite
    similar in terms of length.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢çš„å›¾è¡¨æ˜¾ç¤ºäº†åª’ä½“ç®€æŠ¥çš„é•¿åº¦éšæ—¶é—´å˜åŒ–è¾ƒå¤§ã€‚ç‰¹åˆ«æ˜¯åœ¨3æœˆå’Œ5æœˆï¼Œåª’ä½“ç®€æŠ¥çš„é•¿åº¦æ³¢åŠ¨è¾ƒå¤§ï¼ˆå³ï¼Œæé•¿æˆ–æçŸ­çš„ç®€æŠ¥ï¼‰ï¼Œè€Œåœ¨6æœˆï¼Œåª’ä½“ç®€æŠ¥çš„æ—¥å¸¸é•¿åº¦è¾ƒä¸ºä¸€è‡´ã€‚
- en: Sentiment Analysis with tidytext
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨tidytextè¿›è¡Œæƒ…æ„Ÿåˆ†æ
- en: After analyzing the dataset descriptively, we are ready to begin with the sentiment
    analysis. In the first part, we will use the **tidytext** package for performing
    sentiment analysis and computing sentiment scores. We will first import the lexicons
    into R and then merge them with our dataset. Using the Bing lexicon, we need to
    find the difference between the number of positive and negative words to produce
    a sentiment score (i.e., sentiment = the number of positive words â€” the number
    of negative words).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¯¹æ•°æ®é›†è¿›è¡Œæè¿°æ€§åˆ†æåï¼Œæˆ‘ä»¬å‡†å¤‡å¼€å§‹è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚åœ¨ç¬¬ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨**tidytext**åŒ…æ¥æ‰§è¡Œæƒ…æ„Ÿåˆ†æå¹¶è®¡ç®—æƒ…æ„Ÿå¾—åˆ†ã€‚æˆ‘ä»¬é¦–å…ˆå°†è¯å…¸å¯¼å…¥Rä¸­ï¼Œç„¶åå°†å®ƒä»¬ä¸æˆ‘ä»¬çš„æ•°æ®é›†åˆå¹¶ã€‚ä½¿ç”¨Bingè¯å…¸æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æ­£é¢å’Œè´Ÿé¢è¯æ±‡çš„å·®å¼‚ï¼Œä»¥äº§ç”Ÿæƒ…æ„Ÿå¾—åˆ†ï¼ˆå³ï¼Œæƒ…æ„Ÿ
    = æ­£é¢è¯æ±‡çš„æ•°é‡ â€” è´Ÿé¢è¯æ±‡çš„æ•°é‡ï¼‰ã€‚
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/52c36e29a5fde07cefdd40594b2fc537.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52c36e29a5fde07cefdd40594b2fc537.png)'
- en: Sentiment scores based on the Bing lexicon (Image by author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºBingè¯å…¸çš„æƒ…æ„Ÿå¾—åˆ†ï¼ˆå›¾ç‰‡æ¥è‡ªä½œè€…ï¼‰
- en: The figure above shows that the sentiments delivered in the media briefings
    were generally negative, which is not necessarily surprising since the media briefings
    were all about how many people passed away, hospitalization rates, potential outbreaks,
    etc. On certain days (e.g., March 24, 2020 and May 4, 2020), the media briefings
    were particularly more negative in terms of sentiments.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šå›¾æ˜¾ç¤ºï¼Œåª’ä½“ç®€æŠ¥ä¸­çš„æƒ…æ„Ÿé€šå¸¸æ˜¯è´Ÿé¢çš„ï¼Œè¿™å¹¶ä¸ä»¤äººæƒŠè®¶ï¼Œå› ä¸ºè¿™äº›ç®€æŠ¥ä¸»è¦è®²è¿°äº†æœ‰å¤šå°‘äººå»ä¸–ã€ä½é™¢ç‡ã€æ½œåœ¨çˆ†å‘ç­‰é—®é¢˜ã€‚åœ¨æŸäº›æ—¥æœŸï¼ˆä¾‹å¦‚ 2020 å¹´ 3
    æœˆ 24 æ—¥å’Œ 2020 å¹´ 5 æœˆ 4 æ—¥ï¼‰ï¼Œåª’ä½“ç®€æŠ¥çš„æƒ…æ„Ÿå°¤ä¸ºè´Ÿé¢ã€‚
- en: Next, we will use the AFINN lexicon. Unlike Bing that labels words as positive
    or negative, AFINN assigns a numerical weight to each word. The sign of the weight
    indicates the polarity of sentiments (i.e., positive or negative), while the value
    indicates the intensity of sentiments. Now, letâ€™s see if these weighted values
    produce different sentiment scores.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ AFINN è¯å…¸ã€‚ä¸å°†è¯è¯­æ ‡è®°ä¸ºç§¯ææˆ–æ¶ˆæçš„ Bing ä¸åŒï¼ŒAFINN ä¸ºæ¯ä¸ªè¯è¯­åˆ†é…ä¸€ä¸ªæ•°å€¼æƒé‡ã€‚æƒé‡çš„ç¬¦å·è¡¨ç¤ºæƒ…æ„Ÿçš„ææ€§ï¼ˆå³ç§¯ææˆ–æ¶ˆæï¼‰ï¼Œè€Œæ•°å€¼åˆ™è¡¨ç¤ºæƒ…æ„Ÿçš„å¼ºåº¦ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™äº›åŠ æƒå€¼æ˜¯å¦ä¼šäº§ç”Ÿä¸åŒçš„æƒ…æ„Ÿå¾—åˆ†ã€‚
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/fd3bcd5f5955b4a3ae40a0156952837f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd3bcd5f5955b4a3ae40a0156952837f.png)'
- en: Sentiment scores based on the AFINN lexicon (Image by author)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäº AFINN è¯å…¸çš„æƒ…æ„Ÿå¾—åˆ†ï¼ˆå›¾ç‰‡æ¥æºï¼šä½œè€…ï¼‰
- en: The results based on the AFINN lexicon seem to be quite different! Once we take
    the â€œweightâ€ of the tokens into account, most media briefings turn out to be positive
    (see the green bars), although there are still some days with negative sentiments
    (see the red bars). The two analyses we have done so far have yielded very different
    for two reasons. First, as I mentioned above, the Bing lexicon focuses on the
    polarity of the words but ignores the intensity of the words (dislike and hate
    are considered negative words with equal intensity). Unlike the Bing lexicon,
    the AFINN lexicon takes the intensity into account, which impacts the calculation
    of the sentiment scores. Second, the Bing lexicon (6786 words) is fairly larger
    than the AFINN lexicon (2477 words). Therefore, it is likely that some tokens
    in the media briefings are included in the Bing lexicon, but not in the AFINN
    lexicon. Disregarding those tokens might have impacted the results.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäº AFINN è¯å…¸çš„ç»“æœä¼¼ä¹å¤§ä¸ç›¸åŒï¼ä¸€æ—¦æˆ‘ä»¬è€ƒè™‘åˆ°è¯è¯­çš„â€œæƒé‡â€ï¼Œå¤§å¤šæ•°åª’ä½“ç®€æŠ¥çš„æƒ…æ„Ÿè¢«è®¤ä¸ºæ˜¯ç§¯æçš„ï¼ˆè§ç»¿è‰²æ¡å½¢å›¾ï¼‰ï¼Œå°½ç®¡ä»ç„¶æœ‰ä¸€äº›æ—¥å­å­˜åœ¨è´Ÿé¢æƒ…æ„Ÿï¼ˆè§çº¢è‰²æ¡å½¢å›¾ï¼‰ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åšçš„ä¸¤é¡¹åˆ†æç»“æœå·®å¼‚å¾ˆå¤§ï¼ŒåŸå› æœ‰äºŒã€‚é¦–å…ˆï¼Œæ­£å¦‚æˆ‘ä¹‹å‰æåˆ°çš„ï¼ŒBing
    è¯å…¸ä¸“æ³¨äºè¯è¯­çš„ææ€§ï¼Œè€Œå¿½ç•¥äº†è¯è¯­çš„å¼ºåº¦ï¼ˆâ€œä¸å–œæ¬¢â€å’Œâ€œä»‡æ¨â€è¢«è®¤ä¸ºæ˜¯å¼ºåº¦ç›¸åŒçš„æ¶ˆæè¯è¯­ï¼‰ã€‚ä¸ Bing è¯å…¸ä¸åŒï¼ŒAFINN è¯å…¸è€ƒè™‘äº†è¯è¯­çš„å¼ºåº¦ï¼Œè¿™å½±å“äº†æƒ…æ„Ÿå¾—åˆ†çš„è®¡ç®—ã€‚å…¶æ¬¡ï¼ŒBing
    è¯å…¸ï¼ˆ6786 ä¸ªè¯è¯­ï¼‰æ¯” AFINN è¯å…¸ï¼ˆ2477 ä¸ªè¯è¯­ï¼‰è¦å¤§å¾—å¤šã€‚å› æ­¤ï¼Œåª’ä½“ç®€æŠ¥ä¸­çš„ä¸€äº›æ ‡è®°å¯èƒ½è¢«åŒ…å«åœ¨ Bing è¯å…¸ä¸­ï¼Œä½†ä¸åœ¨ AFINN è¯å…¸ä¸­ã€‚å¿½ç•¥è¿™äº›æ ‡è®°å¯èƒ½å½±å“äº†ç»“æœã€‚
- en: The final lexicon we are going to try using the **tidytext** package is NRC.
    As I mentioned earlier, this lexicon uses Plutchikâ€™s psych-evolutionary theory
    to label the tokens based on basic emotions such as anger, fear, and anticipation.
    We are going to count the number of words or tokens associated with each emotion
    and then visualize the results.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ¥ä¸‹æ¥å°†å°è¯•ä½¿ç”¨ **tidytext** åŒ…ä¸­çš„æœ€ç»ˆè¯å…¸æ˜¯ NRCã€‚æ­£å¦‚æˆ‘ä¹‹å‰æåˆ°çš„ï¼Œè¿™ä¸ªè¯å…¸åŸºäº Plutchik çš„å¿ƒç†è¿›åŒ–ç†è®ºï¼Œå¯¹æ ‡è®°çš„è¯è¯­è¿›è¡Œåˆ†ç±»ï¼Œä¸»è¦ä¾æ®åŸºæœ¬æƒ…æ„Ÿï¼Œå¦‚æ„¤æ€’ã€ææƒ§å’Œé¢„æœŸã€‚æˆ‘ä»¬å°†ç»Ÿè®¡ä¸æ¯ç§æƒ…æ„Ÿç›¸å…³çš„å•è¯æˆ–æ ‡è®°æ•°é‡ï¼Œå¹¶å¯è§†åŒ–ç»“æœã€‚
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/e8024c832941740e0f097412e6fe3ab3.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8024c832941740e0f097412e6fe3ab3.png)'
- en: Sentiment scores based on the NRC lexicon (Image by author)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäº NRC è¯å…¸çš„æƒ…æ„Ÿå¾—åˆ†ï¼ˆå›¾ç‰‡æ¥æºï¼šä½œè€…ï¼‰
- en: The figure shows that the media briefings are mostly positive each month. Dr.
    Hinshaw used words associated with â€œtrustâ€, â€œanticipationâ€, and â€œfearâ€. Overall,
    the pattern of these emotions seems to remain very similar over time, indicating
    the consistency of the media briefings in terms of the type and intensity of the
    emotions delivered.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾è¡¨æ˜¾ç¤ºæ¯æœˆçš„åª’ä½“ç®€æŠ¥å¤§å¤šæ˜¯ç§¯æçš„ã€‚Hinshaw åšå£«ä½¿ç”¨äº†ä¸â€œä¿¡ä»»â€ã€â€œé¢„æœŸâ€å’Œâ€œææƒ§â€ç›¸å…³çš„è¯è¯­ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™äº›æƒ…æ„Ÿçš„æ¨¡å¼ä¼¼ä¹éšç€æ—¶é—´çš„æ¨ç§»ä¿æŒéå¸¸ç›¸ä¼¼ï¼Œè¡¨æ˜åª’ä½“ç®€æŠ¥åœ¨ä¼ é€’æƒ…æ„Ÿçš„ç±»å‹å’Œå¼ºåº¦ä¸Šå…·æœ‰ä¸€è‡´æ€§ã€‚
- en: Sentiment Analysis with sentimentr
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ sentimentr è¿›è¡Œæƒ…æ„Ÿåˆ†æ
- en: 'Another package for lexicon-based sentiment analysis is **sentimentr** ([Rinker,
    2021](https://okan.cloud/posts/2024-02-09-lexicon-based-sentiment-analysis-using-r/#ref-R-sentiment)).
    Unlike the **tidytext** package, this package takes valence shifters (e.g., negation)
    into account, which can easily flip the polarity of a sentence with one word.
    For example, the sentence â€œI am not unhappyâ€ is actually positive, but if we analyze
    it word by word, the sentence may seem to have a negative sentiment due to the
    words â€œnotâ€ and â€œunhappyâ€. Similarly, â€œI hardly like this bookâ€ is a negative
    sentence but the analysis of individual words, â€œhardlyâ€ and â€œlikeâ€, may yield
    a positive sentiment score. The **sentimentr** package addresses the limitations
    around sentiment detection with valence shifters (see the package author Tyler
    Rinkerâ€™s Github page for further details on **sentimentr**: [https://github.com/trinker/sentimentr](https://github.com/trinker/sentimentr)).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªåŸºäºè¯æ±‡çš„æƒ…æ„Ÿåˆ†æåŒ…æ˜¯**sentimentr**ï¼ˆ[Rinker, 2021](https://okan.cloud/posts/2024-02-09-lexicon-based-sentiment-analysis-using-r/#ref-R-sentiment)ï¼‰ã€‚ä¸**tidytext**åŒ…ä¸åŒï¼Œè¿™ä¸ªåŒ…è€ƒè™‘äº†æƒ…æ„Ÿè½¬ç§»å› ç´ ï¼ˆä¾‹å¦‚ï¼Œå¦å®šè¯ï¼‰ï¼Œè¿™äº›å› ç´ å¯ä»¥é€šè¿‡ä¸€ä¸ªè¯è½»æ¾åœ°ç¿»è½¬å¥å­çš„æƒ…æ„Ÿææ€§ã€‚ä¾‹å¦‚ï¼Œå¥å­â€œI
    am not unhappyâ€å®é™…ä¸Šæ˜¯æ­£é¢çš„ï¼Œä½†å¦‚æœæˆ‘ä»¬é€å­—åˆ†æï¼Œå¥å­å¯èƒ½å› ä¸ºâ€œnotâ€å’Œâ€œunhappyâ€è¿™ä¸¤ä¸ªè¯è€Œçœ‹èµ·æ¥æœ‰è´Ÿé¢æƒ…æ„Ÿã€‚ç±»ä¼¼åœ°ï¼Œâ€œI hardly
    like this bookâ€æ˜¯è´Ÿé¢å¥å­ï¼Œä½†å•ç‹¬åˆ†æâ€œhardlyâ€å’Œâ€œlikeâ€è¿™ä¸¤ä¸ªè¯æ—¶ï¼Œå¯èƒ½ä¼šå¾—åˆ°ä¸€ä¸ªæ­£é¢çš„æƒ…æ„Ÿè¯„åˆ†ã€‚**sentimentr**åŒ…è§£å†³äº†æƒ…æ„Ÿæ£€æµ‹ä¸­æƒ…æ„Ÿè½¬ç§»å› ç´ çš„é™åˆ¶ï¼ˆæœ‰å…³**sentimentr**çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§åŒ…ä½œè€…Tyler
    Rinkerçš„Githubé¡µé¢ï¼š[https://github.com/trinker/sentimentr](https://github.com/trinker/sentimentr)ï¼‰ã€‚
- en: To benefit from the **sentimentr** package, we need the actual sentences in
    the media briefings rather than the individual tokens. Therefore, I had to create
    an untokenized version of the dataset, which is available [**here**](https://github.com/okanbulut/blog/raw/master/data_and_codes/wave1_alberta_sentence.RData).
    We will first import this dataset into R, get individual sentences for each media
    briefing using the `get_sentences()` function, and then calculate sentiment scores
    by day and month via `sentiment_by()`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åˆ©ç”¨**sentimentr**åŒ…ï¼Œæˆ‘ä»¬éœ€è¦åª’ä½“ç®€æŠ¥ä¸­çš„å®é™…å¥å­ï¼Œè€Œä¸æ˜¯å•ç‹¬çš„è¯æ±‡ã€‚å› æ­¤ï¼Œæˆ‘ä¸å¾—ä¸åˆ›å»ºä¸€ä¸ªæœªåˆ†è¯çš„æ•°æ®é›†ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬å¯é€šè¿‡[**æ­¤å¤„**](https://github.com/okanbulut/blog/raw/master/data_and_codes/wave1_alberta_sentence.RData)ä¸‹è½½ã€‚æˆ‘ä»¬å°†é¦–å…ˆå°†è¿™ä¸ªæ•°æ®é›†å¯¼å…¥Rï¼Œä½¿ç”¨`get_sentences()`å‡½æ•°è·å–æ¯ä¸ªåª’ä½“ç®€æŠ¥çš„å•ç‹¬å¥å­ï¼Œç„¶åé€šè¿‡`sentiment_by()`æŒ‰å¤©å’Œæœˆè®¡ç®—æƒ…æ„Ÿè¯„åˆ†ã€‚
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/1ac1b34b41d6208e8255af9454d93fea.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ac1b34b41d6208e8255af9454d93fea.png)'
- en: A preview of the dataset (Image by author)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†é¢„è§ˆï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰
- en: In the dataset we created, â€œave_sentimentâ€ is the average sentiment score for
    each day in March, April, May, and June (i.e., days where a media briefing was
    made). Using this dataset, we can visualize the sentiment scores.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬åˆ›å»ºçš„æ•°æ®é›†ä¸­ï¼Œâ€œave_sentimentâ€æ˜¯3æœˆã€4æœˆã€5æœˆå’Œ6æœˆæ¯ä¸€å¤©çš„å¹³å‡æƒ…æ„Ÿè¯„åˆ†ï¼ˆå³æœ‰åª’ä½“ç®€æŠ¥çš„æ—¥å­ï¼‰ã€‚åˆ©ç”¨è¿™ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥å¯è§†åŒ–æƒ…æ„Ÿè¯„åˆ†ã€‚
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/b571e9a77534d4efa810382622d45a36.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b571e9a77534d4efa810382622d45a36.png)'
- en: Sentiment scores based on sentiment (Image by author)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæƒ…æ„Ÿçš„æƒ…æ„Ÿè¯„åˆ†ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰
- en: In the figure above, the blue bars represent highly positive sentiment scores,
    while the red bars depict comparatively lower sentiment scores. The patterns observed
    in the sentiment scores generated by **sentimentr** closely resemble those derived
    from the AFINN lexicon. Notably, this analysis is based on the original media
    briefings rather than solely tokens, with consideration given to valence shifters
    in the computation of sentiment scores. The convergence between the sentiment
    patterns identified by **sentimentr** and those from AFINN is not entirely unexpected.
    Both approaches incorporate similar weighting systems and mechanisms that account
    for word intensity. This alignment reinforces our confidence in the initial findings
    obtained through AFINN, validating the consistency and reliability of our analyses
    with **sentiment**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„å›¾ä¸­ï¼Œè“è‰²æ¡å½¢è¡¨ç¤ºé«˜åº¦æ­£é¢çš„æƒ…æ„Ÿè¯„åˆ†ï¼Œè€Œçº¢è‰²æ¡å½¢åˆ™è¡¨ç¤ºç›¸å¯¹è¾ƒä½çš„æƒ…æ„Ÿè¯„åˆ†ã€‚**sentimentr**ç”Ÿæˆçš„æƒ…æ„Ÿè¯„åˆ†æ¨¡å¼ä¸AFINNè¯æ±‡è¡¨å¾—å‡ºçš„æ¨¡å¼éå¸¸ç›¸ä¼¼ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªåˆ†ææ˜¯åŸºäºåŸå§‹çš„åª’ä½“ç®€æŠ¥ï¼Œè€Œä¸ä»…ä»…æ˜¯å•ç‹¬çš„è¯æ±‡ï¼Œæƒ…æ„Ÿè¯„åˆ†çš„è®¡ç®—è€ƒè™‘äº†æƒ…æ„Ÿè½¬ç§»å› ç´ ã€‚**sentimentr**å’ŒAFINNåœ¨æƒ…æ„Ÿæ¨¡å¼ä¸Šçš„ä¸€è‡´æ€§å¹¶ä¸ä»¤äººæ„å¤–ã€‚ä¸¤ç§æ–¹æ³•éƒ½é‡‡ç”¨äº†ç±»ä¼¼çš„åŠ æƒç³»ç»Ÿå’Œæœºåˆ¶ï¼Œè€ƒè™‘äº†è¯æ±‡å¼ºåº¦ã€‚è¿™ç§ä¸€è‡´æ€§å¢å¼ºäº†æˆ‘ä»¬å¯¹é€šè¿‡AFINNå¾—å‡ºçš„åˆæ­¥ç»“è®ºçš„ä¿¡å¿ƒï¼ŒéªŒè¯äº†æˆ‘ä»¬ä½¿ç”¨**sentiment**è¿›è¡Œåˆ†æçš„å¯é æ€§å’Œä¸€è‡´æ€§ã€‚
- en: Concluding Remarks
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In conclusion, lexicon-based sentiment analysis in R offers a powerful tool
    for uncovering the emotional nuances within textual data. Throughout this post,
    we have explored the fundamental concepts of lexicon-based sentiment analysis
    and provided a practical demonstration of its implementation using R. By leveraging
    packages such as **sentimentr** and **tidytext**, we have illustrated how sentiment
    analysis can be seamlessly integrated into your data analysis workflow. As you
    embark on your journey into sentiment analysis, remember that the insights gained
    from this technique extend far beyond the surface of the text. They provide valuable
    perspectives on public opinion, consumer sentiment, and beyond. I encourage you
    to delve deeper into lexicon-based sentiment analysis, experiment with the examples
    presented here, and unlock the rich insights waiting to be discovered within your
    own data. Happy analyzing!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼ŒåŸºäºè¯å…¸çš„æƒ…æ„Ÿåˆ†æåœ¨Rè¯­è¨€ä¸­æä¾›äº†ä¸€ç§å¼ºå¤§çš„å·¥å…·ï¼Œç”¨äºæ­ç¤ºæ–‡æœ¬æ•°æ®ä¸­çš„æƒ…æ„Ÿç»†å¾®å·®åˆ«ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†åŸºäºè¯å…¸çš„æƒ…æ„Ÿåˆ†æçš„åŸºæœ¬æ¦‚å¿µï¼Œå¹¶æä¾›äº†ä½¿ç”¨Rå®ç°è¿™ä¸€æ–¹æ³•çš„å®è·µç¤ºèŒƒã€‚é€šè¿‡åˆ©ç”¨å¦‚**sentimentr**å’Œ**tidytext**ç­‰åŒ…ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†æƒ…æ„Ÿåˆ†ææ— ç¼åœ°é›†æˆåˆ°æ•°æ®åˆ†æå·¥ä½œæµä¸­ã€‚å½“ä½ è¸ä¸Šæƒ…æ„Ÿåˆ†æçš„æ—…ç¨‹æ—¶ï¼Œè¯·è®°ä½ï¼Œä»è¿™é¡¹æŠ€æœ¯ä¸­è·å¾—çš„æ´å¯Ÿè¿œè¿œè¶…è¶Šäº†æ–‡æœ¬çš„è¡¨é¢ã€‚å®ƒä»¬ä¸ºå…¬ä¼—èˆ†è®ºã€æ¶ˆè´¹è€…æƒ…æ„Ÿç­‰æä¾›äº†å®è´µçš„è§†è§’ã€‚æˆ‘é¼“åŠ±ä½ æ·±å…¥æ¢è®¨åŸºäºè¯å…¸çš„æƒ…æ„Ÿåˆ†æï¼Œå°è¯•è¿™é‡Œå‘ˆç°çš„ç¤ºä¾‹ï¼Œè§£é”ä½ è‡ªå·±æ•°æ®ä¸­ç­‰å¾…å‘ç°çš„ä¸°å¯Œæ´å¯Ÿã€‚ç¥ä½ åˆ†ææ„‰å¿«ï¼
- en: References
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Bulut, O., & Poth, C. N. (2022). Rapid assessment of communication consistency:
    Sentiment analysis of public health briefings during the COVID-19 pandemic. *AIMS
    Public Health*, *9*(2), 293â€“306\. [https://doi.org/10.3934/publichealth.2022020](https://doi.org/10.3934/publichealth.2022020)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bulut, O., & Poth, C. N. (2022). å¿«é€Ÿè¯„ä¼°æ²Ÿé€šä¸€è‡´æ€§ï¼šCOVID-19ç–«æƒ…æœŸé—´å…¬å…±å«ç”Ÿç®€æŠ¥çš„æƒ…æ„Ÿåˆ†æã€‚*AIMSå…¬å…±å«ç”Ÿ*ï¼Œ*9*(2)ï¼Œ293â€“306ã€‚
    [https://doi.org/10.3934/publichealth.2022020](https://doi.org/10.3934/publichealth.2022020)'
- en: '[2] Poth, C. N., Bulut, O., Aquilina, A. M., & Otto, S. J. G. (2021). Using
    data mining for rapid complex case study descriptions: Example of public health
    briefings during the onset of the COVID-19 pandemic. *Journal of Mixed Methods
    Research*, *15*(3), 348â€“373\. [https://doi.org/10.1177/15586898211013925](https://doi.org/10.1177/15586898211013925)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Poth, C. N., Bulut, O., Aquilina, A. M., & Otto, S. J. G. (2021). ä½¿ç”¨æ•°æ®æŒ–æ˜è¿›è¡Œå¿«é€Ÿå¤æ‚æ¡ˆä¾‹ç ”ç©¶æè¿°ï¼šä»¥COVID-19ç–«æƒ…åˆæœŸçš„å…¬å…±å«ç”Ÿç®€æŠ¥ä¸ºä¾‹ã€‚*æ··åˆæ–¹æ³•ç ”ç©¶æ‚å¿—*ï¼Œ*15*(3)ï¼Œ348â€“373ã€‚
    [https://doi.org/10.1177/15586898211013925](https://doi.org/10.1177/15586898211013925)'
- en: '[3] Hu, M., & Liu, B. (2004). Mining and summarizing customer reviews. *Proceedings
    of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining*, 168â€“177.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Hu, M., & Liu, B. (2004). æŒ–æ˜å’Œæ€»ç»“å®¢æˆ·è¯„ä»·ã€‚*ç¬¬åå±ŠACM SIGKDDå›½é™…çŸ¥è¯†å‘ç°ä¸æ•°æ®æŒ–æ˜å¤§ä¼šè®ºæ–‡é›†*ï¼Œ168â€“177ã€‚'
- en: '[4] Mohammad, S. M., & Turney, P. D. (2013). Crowdsourcing a wordâ€“emotion association
    lexicon. *Computational Intelligence*, *29*(3), 436â€“465.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Mohammad, S. M., & Turney, P. D. (2013). ä¼—åŒ…æ„å»ºè¯è¯­â€“æƒ…æ„Ÿè”æƒ³è¯å…¸ã€‚*è®¡ç®—æ™ºèƒ½*ï¼Œ*29*(3)ï¼Œ436â€“465ã€‚'
- en: '[5] Plutchik, R. (1980). A general psychoevolutionary theory of emotion. In
    *Theories of emotion* (pp. 3â€“33). Elsevier.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Plutchik, R. (1980). æƒ…æ„Ÿçš„é€šç”¨å¿ƒç†è¿›åŒ–ç†è®ºã€‚æ”¶å½•äº*æƒ…æ„Ÿç†è®º*ï¼ˆç¬¬3â€“33é¡µï¼‰ã€‚Elsevierã€‚'
