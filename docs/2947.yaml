- en: Combining Large and Small LLMs to Boost Inference Time and Quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/combining-large-and-small-llms-for-inference-time-and-quality-boosts-1779b6b5100b?source=collection_archive---------9-----------------------#2024-12-05](https://towardsdatascience.com/combining-large-and-small-llms-for-inference-time-and-quality-boosts-1779b6b5100b?source=collection_archive---------9-----------------------#2024-12-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Implementing Speculative and Contrastive Decoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mlsys.medium.com/?source=post_page---byline--1779b6b5100b--------------------------------)[![Richa
    Gadgil](../Images/c39ace5df0438240647ea751e8f6ba9e.png)](https://mlsys.medium.com/?source=post_page---byline--1779b6b5100b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1779b6b5100b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1779b6b5100b--------------------------------)
    [Richa Gadgil](https://mlsys.medium.com/?source=post_page---byline--1779b6b5100b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1779b6b5100b--------------------------------)
    ·8 min read·Dec 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Large Language models are comprised of billions of parameters (weights). For
    each word it generates, the model has to perform computationally expensive calculations
    across all of these parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language models accept a sentence, or sequence of tokens, and generate
    a probability distribution of the next most likely token.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, typically decoding **n** tokens (or generating **n** words from the model)
    requires running the model **n** number of times. At each iteration, the new token
    is appended to the input sentence and passed to the model again. This can be costly.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, decoding strategy can influence the quality of the generated words.
    Generating tokens in a simple way, by just taking the token with the highest probability
    in the output distribution, can result in repetitive text. Random sampling from
    the distribution can result in unintended drift.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, a solid decoding strategy is required to ensure both:'
  prefs: []
  type: TYPE_NORMAL
- en: High Quality Outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast Inference Time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Both requirements can be addressed by using a combination of a large and
    small language model, as long as the amateur and expert models are similar (e.g.,
    same architecture but different sizes).**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Target/Large Model:** Main LM with larger number of parameters (e.g. OPT-13B)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amateur/Small Model:** Smaller version of Main LM with fewer parameters (e.g.
    OPT-125M)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speculative** and **contrastive** decoding leverage large and small LLMs
    to achieve reliable and efficient text generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0108a15ac5b5abb54cf65b964db6f0d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Contrastive Decoding for High Quality Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Contrastive Decoding](https://arxiv.org/abs/2210.15097) is a strategy that
    exploits the fact that that failures in large LLMs (such as repetition, incoherence)
    are even more pronounced in small LLMs. Thus, this strategy optimizes for the
    tokens with the highest probability difference between the small and large model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a single prediction, contrastive decoding generates two probability distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*q =* logit probabilities for amateur model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p =* logit probabilities for expert model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next token is chosen based on the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: Discard all tokens that do not have sufficiently high probability under the
    expert model (discard *p(x) < alpha * max(p)*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the remaining tokens, select the one the with the largest difference between
    large model and small model log probabilities, *max(p(x) - q(x)).*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/4e99947389dc73464dbe732a72555688.png)'
  prefs: []
  type: TYPE_IMG
- en: Implementing Contrastive Decoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Speculative Decoding For Fast Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Speculative decoding](https://arxiv.org/abs/2211.17192) is based on the principle
    that the smaller model must sample from the same distribution as the larger model.
    Thus, this strategy aims to accept as many predictions from the smaller model
    as possible, provided they align with the distribution of the larger model.'
  prefs: []
  type: TYPE_NORMAL
- en: The smaller model generates **n** tokens in sequence, as possible guesses. However,
    all **n** sequences are fed into the larger expert model as a single batch, which
    is faster than sequential generation.
  prefs: []
  type: TYPE_NORMAL
- en: This results in a cache for each model, with **n** probability distributions
    in each cache.
  prefs: []
  type: TYPE_NORMAL
- en: '*q =* logit probabilities for amateur model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p =* logit probabilities for expert model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, the sampled tokens from the amateur model are accepted or rejected based
    on the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: If probability of the token is higher in expert distribution (p) than amateur
    distribution (q), or *p(x) > q(x),* accept token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If probability of token is lower in expert distribution (p) than amateur distribution
    (q), or *p(x) < q(x)*, reject token with probability *1 - p(x) / q(x)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a token is rejected, the next token is sampled from the expert distribution
    or adjusted distribution. Additionally, the amateur and expert model reset the
    cache and re-generate **n** guesses and probability distributions *p* and *q*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/460e2c2954e742063edb26a642582e2d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the blue signifies accepted tokens, and red/green signify tokens rejected
    and then sampled from the expert or adjusted distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Speculative Decoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can evaluate both decoding approaches by comparing them to a naive decoding
    method, where we randomly pick the next token from the probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To evaluate contrastive decoding, we can use the following metrics for lexical
    richness.
  prefs: []
  type: TYPE_NORMAL
- en: '**n-gram Entropy**: Measures the unpredictability or diversity of n-grams in
    the generated text. High entropy indicates more diverse text, while low entropy
    suggests repetition or predictability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**distinct-n**: Measures the proportion of unique n-grams in the generated
    text. Higher distinct-n values indicate more lexical diversity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The following results show us that contrastive decoding outperforms naive sampling
    for these metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Naive Sampling:**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Average Entropy (n=1): 4.990499826537679'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Average Entropy (n=2): 5.174765791328267'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Average Entropy (n=3): 5.14373124004409'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Average Distinct-1: 0.8949694135740648'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Average Distinct-2: 0.9951219512195122'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Contrastive Decoding:**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Average Entropy (n=1): 5.182773920916605'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Average Entropy (n=2): 5.3495681172235665'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Average Entropy (n=3): 5.313720275712986'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Average Distinct-1: 0.9028425204970866'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Average Distinct-2: 1.0'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To evaluate speculative decoding, we can look at the average runtime for a set
    of prompts for different **n** values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the average runtime for the naive decoding is much higher than
    for speculative decoding across **n** values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/475bc024a1d7d059c360c87cc8293ad3.png)'
  prefs: []
  type: TYPE_IMG
- en: Combining large and small language models for decoding strikes a balance between
    quality and efficiency. While these approaches introduce additional complexity
    in system design and resource management, their benefits apply to conversational
    AI, real-time translation, and content creation.
  prefs: []
  type: TYPE_NORMAL
- en: These approaches require careful consideration of deployment constraints. For
    instance, the additional memory and compute demands of running dual models may
    limit feasibility on edge devices, though this can be mitigated through techniques
    like model quantization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Unless otherwise noted, all images are by the author.**'
  prefs: []
  type: TYPE_NORMAL
