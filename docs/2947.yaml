- en: Combining Large and Small LLMs to Boost Inference Time and Quality
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合大型和小型LLM以提升推理时间和质量
- en: 原文：[https://towardsdatascience.com/combining-large-and-small-llms-for-inference-time-and-quality-boosts-1779b6b5100b?source=collection_archive---------9-----------------------#2024-12-05](https://towardsdatascience.com/combining-large-and-small-llms-for-inference-time-and-quality-boosts-1779b6b5100b?source=collection_archive---------9-----------------------#2024-12-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/combining-large-and-small-llms-for-inference-time-and-quality-boosts-1779b6b5100b?source=collection_archive---------9-----------------------#2024-12-05](https://towardsdatascience.com/combining-large-and-small-llms-for-inference-time-and-quality-boosts-1779b6b5100b?source=collection_archive---------9-----------------------#2024-12-05)
- en: Implementing Speculative and Contrastive Decoding
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现推测性解码和对比性解码
- en: '[](https://mlsys.medium.com/?source=post_page---byline--1779b6b5100b--------------------------------)[![Richa
    Gadgil](../Images/c39ace5df0438240647ea751e8f6ba9e.png)](https://mlsys.medium.com/?source=post_page---byline--1779b6b5100b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1779b6b5100b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1779b6b5100b--------------------------------)
    [Richa Gadgil](https://mlsys.medium.com/?source=post_page---byline--1779b6b5100b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mlsys.medium.com/?source=post_page---byline--1779b6b5100b--------------------------------)[![Richa
    Gadgil](../Images/c39ace5df0438240647ea751e8f6ba9e.png)](https://mlsys.medium.com/?source=post_page---byline--1779b6b5100b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1779b6b5100b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1779b6b5100b--------------------------------)
    [Richa Gadgil](https://mlsys.medium.com/?source=post_page---byline--1779b6b5100b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1779b6b5100b--------------------------------)
    ·8 min read·Dec 5, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1779b6b5100b--------------------------------)
    ·阅读时间8分钟·2024年12月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Large Language models are comprised of billions of parameters (weights). For
    each word it generates, the model has to perform computationally expensive calculations
    across all of these parameters.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型由数十亿个参数（权重）组成。对于每个生成的单词，模型必须在所有这些参数上执行计算密集型操作。
- en: Large Language models accept a sentence, or sequence of tokens, and generate
    a probability distribution of the next most likely token.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型接受一个句子或标记序列，并生成下一个最可能的标记的概率分布。
- en: Thus, typically decoding **n** tokens (or generating **n** words from the model)
    requires running the model **n** number of times. At each iteration, the new token
    is appended to the input sentence and passed to the model again. This can be costly.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通常解码**n**个标记（或从模型生成**n**个单词）需要运行模型**n**次。在每次迭代中，新的标记会附加到输入句子中，并再次传递给模型。这可能是昂贵的。
- en: Additionally, decoding strategy can influence the quality of the generated words.
    Generating tokens in a simple way, by just taking the token with the highest probability
    in the output distribution, can result in repetitive text. Random sampling from
    the distribution can result in unintended drift.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，解码策略会影响生成单词的质量。以简单的方式生成标记，通过仅取输出分布中概率最高的标记，可能会导致重复的文本。从分布中进行随机采样可能会导致意外的漂移。
- en: 'Thus, a solid decoding strategy is required to ensure both:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，需要一个可靠的解码策略，以确保两者的实现：
- en: High Quality Outputs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高质量输出
- en: Fast Inference Time
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速推理时间
- en: '**Both requirements can be addressed by using a combination of a large and
    small language model, as long as the amateur and expert models are similar (e.g.,
    same architecture but different sizes).**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**这两项要求可以通过结合大型和小型语言模型来解决，只要业余模型和专家模型相似（例如，相同架构但不同大小）。**'
- en: '**Target/Large Model:** Main LM with larger number of parameters (e.g. OPT-13B)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标/大型模型：** 具有更多参数的主语言模型（例如，OPT-13B）'
- en: '**Amateur/Small Model:** Smaller version of Main LM with fewer parameters (e.g.
    OPT-125M)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**业余/小型模型：** 主语言模型的较小版本，具有较少的参数（例如，OPT-125M）'
- en: '**Speculative** and **contrastive** decoding leverage large and small LLMs
    to achieve reliable and efficient text generation.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**推测性**和**对比性**解码利用大型和小型LLM实现可靠和高效的文本生成。'
- en: '![](../Images/0108a15ac5b5abb54cf65b964db6f0d5.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0108a15ac5b5abb54cf65b964db6f0d5.png)'
- en: Contrastive Decoding for High Quality Inference
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高质量推理的对比性解码
- en: '[Contrastive Decoding](https://arxiv.org/abs/2210.15097) is a strategy that
    exploits the fact that that failures in large LLMs (such as repetition, incoherence)
    are even more pronounced in small LLMs. Thus, this strategy optimizes for the
    tokens with the highest probability difference between the small and large model.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[对比解码](https://arxiv.org/abs/2210.15097)是一种策略，利用了大规模语言模型（如重复、无 coherency 等问题）在小规模语言模型中表现得更加显著的事实。因此，这种策略优化了小模型和大模型之间概率差异最大的词元。'
- en: 'For a single prediction, contrastive decoding generates two probability distributions:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单次预测，对比解码生成两个概率分布：
- en: '*q =* logit probabilities for amateur model'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*q =* 业余模型的logit概率'
- en: '*p =* logit probabilities for expert model'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p =* 专家模型的logit概率'
- en: 'The next token is chosen based on the following criteria:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 下一词元的选择依据以下标准：
- en: Discard all tokens that do not have sufficiently high probability under the
    expert model (discard *p(x) < alpha * max(p)*)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃所有在专家模型下概率不够高的词元（丢弃*p(x) < alpha * max(p)*）。
- en: From the remaining tokens, select the one the with the largest difference between
    large model and small model log probabilities, *max(p(x) - q(x)).*
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从剩余的词元中，选择与大模型和小模型log概率差异最大的那个，*max(p(x) - q(x))。*
- en: '![](../Images/4e99947389dc73464dbe732a72555688.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e99947389dc73464dbe732a72555688.png)'
- en: Implementing Contrastive Decoding
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现对比解码
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Speculative Decoding For Fast Inference
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速推理的推测性解码
- en: '[Speculative decoding](https://arxiv.org/abs/2211.17192) is based on the principle
    that the smaller model must sample from the same distribution as the larger model.
    Thus, this strategy aims to accept as many predictions from the smaller model
    as possible, provided they align with the distribution of the larger model.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[推测性解码](https://arxiv.org/abs/2211.17192)基于这样一个原理：小模型必须从与大模型相同的分布中采样。因此，这一策略旨在接受尽可能多的小模型预测，只要这些预测与大模型的分布一致。'
- en: The smaller model generates **n** tokens in sequence, as possible guesses. However,
    all **n** sequences are fed into the larger expert model as a single batch, which
    is faster than sequential generation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 小模型依次生成**n**个词元作为可能的猜测。然而，所有**n**个序列作为一个批次输入到更大的专家模型中，这比顺序生成更快。
- en: This results in a cache for each model, with **n** probability distributions
    in each cache.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这会为每个模型生成一个缓存，每个缓存中包含**n**个概率分布。
- en: '*q =* logit probabilities for amateur model'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*q =* 业余模型的logit概率'
- en: '*p =* logit probabilities for expert model'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p =* 专家模型的logit概率'
- en: 'Next, the sampled tokens from the amateur model are accepted or rejected based
    on the following conditions:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，基于以下条件，业余模型采样的词元会被接受或拒绝：
- en: If probability of the token is higher in expert distribution (p) than amateur
    distribution (q), or *p(x) > q(x),* accept token
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在专家分布中（p）该词元的概率高于在业余分布中（q）的概率，或是*p(x) > q(x)*，则接受该词元。
- en: If probability of token is lower in expert distribution (p) than amateur distribution
    (q), or *p(x) < q(x)*, reject token with probability *1 - p(x) / q(x)*
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在专家分布中（p）该词元的概率低于在业余分布中（q）的概率，或是*p(x) < q(x)*，则以概率*1 - p(x) / q(x)*拒绝该词元。
- en: If a token is rejected, the next token is sampled from the expert distribution
    or adjusted distribution. Additionally, the amateur and expert model reset the
    cache and re-generate **n** guesses and probability distributions *p* and *q*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个词元被拒绝，下一个词元会从专家分布或调整后的分布中采样。此外，业余和专家模型会重置缓存并重新生成**n**个猜测和概率分布*p*与*q*。
- en: '![](../Images/460e2c2954e742063edb26a642582e2d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/460e2c2954e742063edb26a642582e2d.png)'
- en: Here, the blue signifies accepted tokens, and red/green signify tokens rejected
    and then sampled from the expert or adjusted distribution.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，蓝色表示被接受的词元，红色/绿色表示被拒绝的词元，随后从专家或调整后的分布中重新采样。
- en: Implementing Speculative Decoding
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现推测性解码
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Evaluation
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估
- en: We can evaluate both decoding approaches by comparing them to a naive decoding
    method, where we randomly pick the next token from the probability distribution.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将两种解码方法与一种简单的解码方法进行比较来评估它们，其中我们从概率分布中随机选择下一个词元。
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To evaluate contrastive decoding, we can use the following metrics for lexical
    richness.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估对比解码，我们可以使用以下词汇丰富度的指标。
- en: '**n-gram Entropy**: Measures the unpredictability or diversity of n-grams in
    the generated text. High entropy indicates more diverse text, while low entropy
    suggests repetition or predictability.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n-gram 熵**：衡量生成文本中n-gram的不可预测性或多样性。高熵表明文本更具多样性，而低熵则表明文本有重复性或可预测性。'
- en: '**distinct-n**: Measures the proportion of unique n-grams in the generated
    text. Higher distinct-n values indicate more lexical diversity.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**distinct-n**：衡量生成文本中唯一n-gram的比例。较高的distinct-n值表示更多的词汇多样性。'
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The following results show us that contrastive decoding outperforms naive sampling
    for these metrics.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果显示，对比解码在这些指标上优于朴素采样。
- en: '**Naive Sampling:**'
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**朴素采样：**'
- en: 'Average Entropy (n=1): 4.990499826537679'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 平均熵（n=1）：4.990499826537679
- en: 'Average Entropy (n=2): 5.174765791328267'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 平均熵（n=2）：5.174765791328267
- en: 'Average Entropy (n=3): 5.14373124004409'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 平均熵（n=3）：5.14373124004409
- en: 'Average Distinct-1: 0.8949694135740648'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 平均Distinct-1：0.8949694135740648
- en: 'Average Distinct-2: 0.9951219512195122'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 平均Distinct-2：0.9951219512195122
- en: ''
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Contrastive Decoding:**'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**对比解码：**'
- en: 'Average Entropy (n=1): 5.182773920916605'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 平均熵（n=1）：5.182773920916605
- en: 'Average Entropy (n=2): 5.3495681172235665'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 平均熵（n=2）：5.3495681172235665
- en: 'Average Entropy (n=3): 5.313720275712986'
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 平均熵（n=3）：5.313720275712986
- en: 'Average Distinct-1: 0.9028425204970866'
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 平均Distinct-1：0.9028425204970866
- en: 'Average Distinct-2: 1.0'
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 平均Distinct-2：1.0
- en: To evaluate speculative decoding, we can look at the average runtime for a set
    of prompts for different **n** values.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估推测解码，我们可以查看一组提示的平均运行时间，针对不同的**n**值。
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can see that the average runtime for the naive decoding is much higher than
    for speculative decoding across **n** values.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，朴素解码的平均运行时间远高于在不同**n**值下的推测解码。
- en: '![](../Images/475bc024a1d7d059c360c87cc8293ad3.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/475bc024a1d7d059c360c87cc8293ad3.png)'
- en: Combining large and small language models for decoding strikes a balance between
    quality and efficiency. While these approaches introduce additional complexity
    in system design and resource management, their benefits apply to conversational
    AI, real-time translation, and content creation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 将大语言模型和小语言模型结合进行解码，在质量和效率之间达到了平衡。尽管这些方法在系统设计和资源管理中增加了额外的复杂性，但它们的好处适用于对话AI、实时翻译和内容创作。
- en: These approaches require careful consideration of deployment constraints. For
    instance, the additional memory and compute demands of running dual models may
    limit feasibility on edge devices, though this can be mitigated through techniques
    like model quantization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法需要仔细考虑部署约束。例如，运行双模型所需的额外内存和计算需求可能会限制在边缘设备上的可行性，尽管可以通过诸如模型量化等技术来缓解这一问题。
- en: '**Unless otherwise noted, all images are by the author.**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**除非另有说明，所有图片均为作者所作。**'
