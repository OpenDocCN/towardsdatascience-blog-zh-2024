<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Rise of Pallas: Unlocking TPU Potential with Custom Kernels</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Rise of Pallas: Unlocking TPU Potential with Custom Kernels</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a?source=collection_archive---------5-----------------------#2024-10-06">https://towardsdatascience.com/the-rise-of-pallas-unlocking-tpu-potential-with-custom-kernels-67be10ab846a?source=collection_archive---------5-----------------------#2024-10-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="41f2" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Accelerating AI/ML Model Training with Custom Operators — Part 3</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--67be10ab846a--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--67be10ab846a--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--67be10ab846a--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--67be10ab846a--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/b7b6e80b948d3befa2dd1c167781b063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hBR-QV6y0kVsgrdu"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@hendrikmorkel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Hendrik Morkel</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="8b78" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is the third part of a <a class="af nc" href="https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12" rel="noopener">series of posts</a> on the topic of building custom operators for optimizing AI/ML workloads. In our <a class="af nc" href="https://chaimrand.medium.com/unleashing-the-power-of-triton-mastering-gpu-kernel-optimization-in-python-160a3f52701e" rel="noopener">previous post</a> we demonstrated the simplicity and accessibility of Triton. Named for the <a class="af nc" href="https://en.wikipedia.org/wiki/Triton_(mythology)" rel="noopener ugc nofollow" target="_blank">Greek god of the sea</a>, Triton empowers Python developers to increase their control over the GPU and optimize its use for the specific workload at hand. In this post we move one step down the lineage of Greek mythology to Triton’s daughter, <a class="af nc" href="https://en.wikipedia.org/wiki/Pallas_(daughter_of_Triton)" rel="noopener ugc nofollow" target="_blank">Pallas</a> and discuss her namesake, the <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/index.html" rel="noopener ugc nofollow" target="_blank">JAX extension</a> for writing custom kernels for GPU and TPU.</p><p id="37a4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One of the most important features of NVIDIA GPUs — and a significant factor in their rise to prominence — is their programmability. A key ingredient of the GPU offering are frameworks for creating General-Purpose GPU (GPGPU) operators, such as <a class="af nc" href="https://developer.nvidia.com/cuda-toolkit" rel="noopener ugc nofollow" target="_blank">CUDA</a> and <a class="af nc" href="https://triton-lang.org/main/index.html" rel="noopener ugc nofollow" target="_blank">Triton</a>.</p><p id="f69c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In previous posts (e.g., <a class="af nc" rel="noopener" target="_blank" href="/tpu-training-6eb84100d138">here</a>) we discussed the opportunity for running ML workloads on <a class="af nc" rel="noopener" target="_blank" href="/tpu-training-6eb84100d138">Google TPUs</a> and the potential for a meaningful increase in price performance and a reduction in training costs. One of the disadvantages that we noted at the time was the absence of tools for creating custom operators. As a result, models requiring unique operators that were either unsupported by the underlying ML framework (e.g., TensorFlow/XLA) or implemented in a suboptimal manner, would underperform on TPU compared to GPU. This development gap was particularly noticeable over the past few years with the frequent introduction of newer and faster solutions for computing <a class="af nc" href="https://en.wikipedia.org/wiki/Attention_(machine_learning)" rel="noopener ugc nofollow" target="_blank">attention</a> on GPU. Enabled by GPU kernel development frameworks, these led to a significant improvement in the efficiency of <a class="af nc" href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)" rel="noopener ugc nofollow" target="_blank">transformer models</a>.</p><p id="7ef7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">On TPUs, on the other hand, the lack of appropriate tooling prevented this innovation and transformer models were stuck with the attention mechanisms that were supported by the official SW stack. Fortunately, with the advent of <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/index.html" rel="noopener ugc nofollow" target="_blank">Pallas</a> this gap has been addressed. Built as an extension to <a class="af nc" href="https://jax.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">JAX</a> and with <a class="af nc" href="https://github.com/pytorch/xla/blob/3c59087e894013559b58dcb147869c4a81ca07d3/docs/pallas.md#adopt-the-above-kernel-to-be-compatible-with-pytorchxla" rel="noopener ugc nofollow" target="_blank">dedicated support</a> for <a class="af nc" href="https://pytorch.org/xla/release/r2.4/index.html" rel="noopener ugc nofollow" target="_blank">PyTorch/XLA</a>, Pallas enables the creation of custom kernels for GPU and TPU. For its GPU support Pallas utilizes Triton, and for its TPU support it uses a library called Mosaic. Although we will focus on custom kernels for TPU, it is worth noting that when developing in JAX, GPU kernel customization with Pallas offers some advantages over Triton (e.g., see <a class="af nc" href="https://www.youtube.com/watch?v=OR8NZyTz-yo" rel="noopener ugc nofollow" target="_blank">here</a>).</p><p id="c767" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Our intention in this post is to draw attention to Pallas and demonstrate its potential. Please do not view this post as a replacement for the official <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/index.html#" rel="noopener ugc nofollow" target="_blank">Pallas documentation</a>. The examples we will share were chosen for demonstrative purposes, only. We have made no effort to optimize these or verify their robustness, durability, or accuracy.</p><p id="6e42" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Importantly, at the time of this writing Pallas is an <em class="nz">experimental</em> feature and still under active development. The samples we share (which are based on <a class="af nc" href="https://pypi.org/project/jax/" rel="noopener ugc nofollow" target="_blank">JAX</a> version 0.4.32 and <a class="af nc" href="https://pypi.org/project/torch/" rel="noopener ugc nofollow" target="_blank">PyTorch</a> version 2.4.1) may become outdated by the time you read this. Be sure to use the most up-to-date APIs and resources available for your Pallas development.</p><p id="382b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Many thanks to <a class="af nc" href="https://www.linkedin.com/in/yitzhak-levi-49a217201/" rel="noopener ugc nofollow" target="_blank">Yitzhak Levi</a> for his contributions to this post.</p><h2 id="1978" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">Environment Setup</h2><p id="6601" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">For the experiments described below we use the following <a class="af nc" href="https://cloud.google.com/tpu/docs/v5e-training#train-resnet-using-the-pjrt-runtime" rel="noopener ugc nofollow" target="_blank">environment setup</a> commands:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="7596" class="pe ob fq pb b bg pf pg l ph pi"># create TPU node<br/>gcloud alpha compute tpus queued-resources create v5litepod-1-resource \<br/>     --node-id v5litepod \<br/>     --project &lt;project-id&gt; \<br/>     --zone us-central1-a \<br/>     --accelerator-type v5litepod-1 \<br/>     --runtime-version v2-alpha-tpuv5-lite \<br/>     --valid-until-duration 1d \<br/>     --service-account &lt;service-account&gt; \<br/><br/># check TPU node status (wait for state to be ACTIVE)<br/>gcloud alpha compute tpus queued-resources describe v5litepod-1-resource \<br/>     --project &lt;project-id&gt; \<br/>     --zone us-central1-a<br/><br/># SSH to TPU node<br/>gcloud alpha compute tpus tpu-vm ssh v5litepod \<br/>     --project &lt;project-id&gt; \<br/>     --zone  us-central1-a<br/><br/># install dependencies<br/>pip install torch_xla[tpu] \<br/>     -f https://storage.googleapis.com/libtpu-releases/index.html<br/>pip install torch_xla[pallas]<br/>pip install timm<br/><br/># run tests<br/>python train.py<br/><br/>#exit ssh<br/>exit<br/><br/># delete TPU node<br/>gcloud alpha compute tpus queued-resources delete v5litepod-1-resource \<br/>     --project &lt;project-id&gt; \<br/>     --zone us-central1-a --force --quiet</span></pre><h1 id="3074" class="pj ob fq bf oc pk pl gq og pm pn gt ok po pp pq pr ps pt pu pv pw px py pz qa bk">Pallas Kernels for TPU</h1><p id="32a1" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">In the toy example of our first post in this series, we distinguished between two different ways in which custom kernel development can potentially boost performance. The first is by combining (fusing) together multiple operations in a manner that reduces the overhead of: 1) loading multiple individual kernels, and 2) reading and writing intermediate values (e.g., see <a class="af nc" href="https://pytorch.org/tutorials/advanced/cpp_custom_ops.html#cpp-custom-ops-tutorial" rel="noopener ugc nofollow" target="_blank">PyTorch’s tutorial on multiply-add fusion</a>). The second is by meticulously applying the resources of the underlying accelerator in manner that optimizes the function at hand. We briefly discuss these two opportunities as they pertain to developing custom TPU kernels and make note of the limitations of the Pallas support.</p><h2 id="b531" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">Operator Fusion on TPU</h2><p id="836f" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The TPU is an <a class="af nc" href="https://openxla.org/xla" rel="noopener ugc nofollow" target="_blank">XLA</a> (Accelerated Linear Algebra) device, i.e., it runs code that has been generated by the <a class="af nc" href="https://cloud.google.com/tpu/docs/intro-to-tpu#xla_compiler" rel="noopener ugc nofollow" target="_blank">XLA compiler</a>. When training an AI model in a frameworks such as <a class="af nc" href="https://jax.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">JAX</a> or <a class="af nc" href="https://pytorch.org/xla/release/r2.4/index.html" rel="noopener ugc nofollow" target="_blank">PyTorch/XLA</a>, the training step is first transformed into an intermediate graph representation (IR). This computation graph is then fed to the XLA compiler which converts it into machine code that can run on the TPU. Contrary to <em class="nz">eager</em> execution mode, in which operations are executed individually, this mode of running models enables XLA to identify and implement opportunities for operator fusion during compilation. And, in fact, operator <a class="af nc" href="https://openxla.org/xla/tf2xla" rel="noopener ugc nofollow" target="_blank">fusion</a> is the XLA compiler’s most important optimization. Naturally, no compiler is perfect and we are certain to come across additional opportunities for fusion through custom kernels. But, generally speaking, we might expect the opportunity for boosting runtime performance in this manner to be lower than in the case of <em class="nz">eager</em> execution.</p><h2 id="aaf6" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">Optimizing TPU Utilization</h2><p id="b542" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Creating optimal kernels for TPU requires a comprehensive and intimate understanding of the <a class="af nc" href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm" rel="noopener ugc nofollow" target="_blank">TPU system architecture</a>. Importantly, TPUs are <a class="af nc" href="https://cloud.google.com/tpu/docs/intro-to-tpu" rel="noopener ugc nofollow" target="_blank">very different</a> from GPUs: expertise in GPUs and CUDA does not immediately carry over to TPU development. For example, while GPUs contain a large number of processors and draw their strength from their ability to perform massive parallelization, TPUs are primarily <em class="nz">sequential </em>with dedicated engines for running highly vectorized operations and <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.html" rel="noopener ugc nofollow" target="_blank">support for asynchronous scheduling and memory loading</a>.</p><p id="7068" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The differences between the underlying architectures of the GPU and TPU can have significant implications on how custom kernels should be designed. Mastering TPU kernel development requires 1) appropriate overlapping of memory and compute operations via <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.html" rel="noopener ugc nofollow" target="_blank">pipelining</a>, 2) knowing how to mix between the use of the scalar, vector (VPU) and matrix (MXU) compute units and their associated scalar and vector registers (SREG and VREG) and memory caches (SMEM and VMEM), 3) a comprehension of the <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/details.html#elementwise-operations" rel="noopener ugc nofollow" target="_blank">costs of different low-level operations</a>, 4) appropriate <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.html#tpus-in-megacore-configuration" rel="noopener ugc nofollow" target="_blank">megacore configuration</a> (on supporting TPU generations), 5) a grasp of the different types of <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/distributed.html#tpu-topologies" rel="noopener ugc nofollow" target="_blank">TPU topologies</a> and their implications on how to support <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/distributed.html#" rel="noopener ugc nofollow" target="_blank">distributed computing</a>, and more.</p><h2 id="6643" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">Framework Limitations</h2><p id="aa19" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">While the ability to create custom operators in <em class="nz">Python</em> using JAX functions and APIs greatly increases the simplicity and accessibility of Pallas kernel development, it also limits its expressivity. Additionally, (as of the time of this writing) there are some JAX APIs that are not supported by Pallas on TPU (e.g., see <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/details.html#supported-operations" rel="noopener ugc nofollow" target="_blank">here</a>). As a result, you may approach Pallas with the intention of implementing a particular operation only to discover that the framework does not support the APIs that you need. This is in contrast to frameworks such as CUDA which enable a great deal of flexibility when developing custom kernels (for GPU).</p><p id="4eff" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html" rel="noopener ugc nofollow" target="_blank">matrix multiplication</a> tutorial in the Pallas documentation provides an excellent introduction to Pallas kernel development, highlighting the potential for operator fusion and customization alongside the challenges involved in optimizing performance (e.g., appropriate tuning of the input <em class="nz">block size</em>). The tutorial clearly illustrates that maximizing the <em class="nz">full</em> potential of the TPU requires a certain degree of specialization. However, as we intend to demonstrate, even the novice ML developer can benefit from Pallas kernels.</p><h1 id="ee27" class="pj ob fq bf oc pk pl gq og pm pn gt ok po pp pq pr ps pt pu pv pw px py pz qa bk">Integrating the Use of Existing Pallas Kernels</h1><p id="b3db" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">To benefit from custom Pallas kernels you do not necessarily need to know how to build them. In our first example we demonstrate how you can leverage existing Pallas kernels from dedicated public repositories.</p><h2 id="3af9" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">Example — Flash Attention in Torch/XLA</h2><p id="c31e" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The JAX github repository includes implementations of a number of Pallas kernels, including <a class="af nc" href="https://github.com/jax-ml/jax/blob/jaxlib-v0.4.32/jax/experimental/pallas/ops/tpu/flash_attention.py" rel="noopener ugc nofollow" target="_blank">flash attention</a>. Here we will demonstrate its use in a Torch/XLA <a class="af nc" href="https://en.wikipedia.org/wiki/Vision_transformer" rel="noopener ugc nofollow" target="_blank">Vision Transformer</a> (ViT) model. Although Pallas kernels are developed in JAX, they can be adopted into Torch/XLA, e.g., via the <a class="af nc" href="https://github.com/pytorch/xla/blob/v2.4.0/torch_xla/experimental/custom_kernel.py#L132" rel="noopener ugc nofollow" target="_blank">make_kernel_from_pallas</a> utility (see the <a class="af nc" href="https://github.com/pytorch/xla/blob/3c59087e894013559b58dcb147869c4a81ca07d3/docs/pallas.md" rel="noopener ugc nofollow" target="_blank">documentation</a> for details). In the case of <a class="af nc" href="https://github.com/pytorch/xla/blob/v2.4.0/torch_xla/experimental/custom_kernel.py#L425" rel="noopener ugc nofollow" target="_blank">flash attention</a> the adoption is implemented by Torch/XLA.</p><p id="13cd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the following code block we define a stripped down version of the classic <em class="nz">timm </em><a class="af nc" href="https://github.com/huggingface/pytorch-image-models/blob/v1.0.9/timm/models/vision_transformer.py#L124" rel="noopener ugc nofollow" target="_blank">attention block</a> with an option to define the underlying attention operator in the constructor. We will use this option to compare the performance of the <a class="af nc" href="https://github.com/pytorch/xla/blob/v2.4.0/torch_xla/experimental/custom_kernel.py#L425" rel="noopener ugc nofollow" target="_blank">flash attention</a> Pallas kernel to its alternatives.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="b9d0" class="pe ob fq pb b bg pf pg l ph pi"># general imports<br/>import os, time, functools<br/># torch imports<br/>import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>from torch.utils.data import Dataset, DataLoader<br/>import torch_xla.core.xla_model as xm<br/># custom kernel import<br/>from torch_xla.experimental.custom_kernel import flash_attention<br/># timm imports<br/>from timm.layers import Mlp<br/>from timm.models.vision_transformer import VisionTransformer<br/><br/>class TPUAttentionBlock(nn.Module):<br/>    def __init__(<br/>            self,<br/>            dim: int = 768,<br/>            num_heads: int = 12,<br/>            attn_fn = None,<br/>            **kwargs<br/>    ) -&gt; None:<br/>        super().__init__()<br/>        self.attn_fn = attn_fn<br/>        self.num_heads = num_heads<br/>        self.head_dim = dim // num_heads<br/>        self.norm1 = nn.LayerNorm(dim)<br/>        self.norm2 = nn.LayerNorm(dim)<br/>        self.qkv = nn.Linear(dim, dim * 3, bias=False)<br/>        self.proj = nn.Linear(dim, dim)<br/>        self.mlp = Mlp(<br/>            in_features=dim,<br/>            hidden_features=dim * 4,<br/>        )<br/><br/>    def forward(self, x_in: torch.Tensor) -&gt; torch.Tensor:<br/>        x = self.norm1(x_in)<br/><br/>        B, N, C = x.shape<br/>        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)<br/>        qkv = qkv.permute(2, 0, 3, 1, 4)<br/>        q, k, v = qkv.unbind(0)<br/><br/>        if self.attn_fn is None:<br/>            attn = q @ k.transpose(-2, -1)<br/>            attn = attn.softmax(dim=-1)<br/>            x = attn @ v<br/>        else:<br/>            x = self.attn_fn(q, k, v)<br/><br/>        x = x.transpose(1, 2).reshape(B, N, C)<br/>        x = self.proj(x)<br/>        x = x + x_in<br/>        x = x + self.mlp(self.norm2(x))<br/>        return x</span></pre><p id="c2e7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the following block we train a simple ViT-backed classification model using the input dataset and attention function (<em class="nz">attn_fn</em>) of choice.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="6001" class="pe ob fq pb b bg pf pg l ph pi">def train(dataset, attn_fn=None):<br/>    device = xm.xla_device()<br/><br/>    train_loader = DataLoader(<br/>        dataset,<br/>        batch_size=128,<br/>        num_workers=os.cpu_count(),<br/>        pin_memory=True<br/>    )<br/><br/>    # configure the VisionTranformer in a manner that complies with the <br/>    # Pallas flash_attention kernel constraints<br/>    model = VisionTransformer(<br/>        block_fn=functools.partial(TPUAttentionBlock, attn_fn=attn_fn),<br/>        img_size=256,<br/>        class_token=False,<br/>        global_pool="avg"<br/>    )<br/><br/>    optimizer = torch.optim.SGD(model.parameters())<br/>    loss_fn = torch.nn.CrossEntropyLoss()<br/><br/>    # copy the model to the TPU<br/>    model = model.to(device)<br/><br/>    model.train()<br/><br/>    t0 = time.perf_counter()<br/>    summ = 0<br/>    count = 0<br/><br/><br/>    for step, data in enumerate(train_loader):<br/>        # copy data to TPU<br/>        inputs = data[0].to(device=device, non_blocking=True)<br/>        label = data[1].to(device=device, non_blocking=True)<br/><br/>        optimizer.zero_grad(set_to_none=True)<br/>        with torch.autocast('xla', dtype=torch.bfloat16):<br/>            output = model(inputs)<br/>            loss = loss_fn(output, label)<br/>        loss.backward()<br/>        optimizer.step()<br/>        xm.mark_step()<br/><br/>        # capture step time<br/>        batch_time = time.perf_counter() - t0<br/>        if step &gt; 20:  # skip first steps<br/>            summ += batch_time<br/>            count += 1<br/>        t0 = time.perf_counter()<br/>        if step &gt; 100:<br/>            break<br/><br/>    print(f'average step time: {summ / count}')</span></pre><p id="46fb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note the specific configuration we chose for the <a class="af nc" href="https://github.com/huggingface/pytorch-image-models/blob/v1.0.9/timm/models/vision_transformer.py#L414" rel="noopener ugc nofollow" target="_blank">VisionTransformer</a>. This is to comply with certain restrictions (as of the time of this writing) of the custom flash attention kernel (e.g., on tensor shapes).</p><p id="6c53" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Finally, we define a dataset and compare the runtimes of training with three different attention routines, 1. using native PyTorch functions, 2. using PyTorch’s built in <a class="af nc" href="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html" rel="noopener ugc nofollow" target="_blank">SDPA</a> function, and 3. using the custom Pallas operator:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="ba68" class="pe ob fq pb b bg pf pg l ph pi"># use random data<br/>class FakeDataset(Dataset):<br/>    def __len__(self):<br/>        return 1000000<br/><br/>    def __getitem__(self, index):<br/>        rand_image = torch.randn([3, 256, 256], dtype=torch.float32)<br/>        label = torch.tensor(data=index % 1024, dtype=torch.int64)<br/>        return rand_image, label<br/><br/>ds = FakeDataset()<br/><br/>print('PyTorch native')<br/>train(ds, attn_fn=None)<br/><br/>print('PyTorch SDPA')<br/>train(ds, attn_fn=functools.partial(F.scaled_dot_product_attention, scale=1.0))<br/><br/>print('Pallas flash_attention')<br/>train(ds, attn_fn=flash_attention)</span></pre><p id="fcd1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The comparative results are captured in the table below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qb"><img src="../Images/6850e69e43e23d9a1964fe7da0551f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*QVgVOAhVlNa5SXYFPeocdw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Step time for different attention blocks (lower is better) — by Author</figcaption></figure><p id="8b3e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Although our Pallas kernel clearly underperforms when compared to its alternatives, we should not be discouraged:</p><ol class=""><li id="1672" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qc qd qe bk">It is likely that these results could be improved with appropriate tuning.</li><li id="823e" class="nd ne fq nf b go qf nh ni gr qg nk nl nm qh no np nq qi ns nt nu qj nw nx ny qc qd qe bk">These results are specific to the model and runtime environment that we chose. The Pallas kernel may exhibit wholly different comparative results in other use cases.</li><li id="e050" class="nd ne fq nf b go qf nh ni gr qg nk nl nm qh no np nq qi ns nt nu qj nw nx ny qc qd qe bk">The real power of Pallas is in the ability to create and adjust low level operators to our specific needs. Although runtime performance is important, a 23% performance penalty (as in our example) may be a small price to pay for this flexibility. Moreover, the opportunity for customization may open up possibilities for optimizations that are not supported by the native framework operations.</li></ol><h1 id="ba23" class="pj ob fq bf oc pk pl gq og pm pn gt ok po pp pq pr ps pt pu pv pw px py pz qa bk">Enhancing Existing Kernels</h1><p id="20c4" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Oftentimes it may be easier to tweak an existing Pallas kernel to your specific needs, rather than creating one from scratch. This is especially recommended if the kernel has already been optimized as performance tuning can be tedious and time-consuming. The official <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html" rel="noopener ugc nofollow" target="_blank">matrix multiplication</a> tutorial includes a few examples of how to <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#templating-the-matrix-multiplication" rel="noopener ugc nofollow" target="_blank">extend</a> and <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#fused-activation-function" rel="noopener ugc nofollow" target="_blank">enhance</a> an existing kernel. Here we undertake one of the <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#conclusion" rel="noopener ugc nofollow" target="_blank">suggested exercises</a>: we implement <code class="cx qk ql qm pb b">int8</code> matrix multiplication and assess its performance advantage over its <code class="cx qk ql qm pb b">bfloat16</code> alternative.</p><h2 id="dc26" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">Example — Int8 Matrix Multiplication</h2><p id="7021" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">In the code block below we implement an <code class="cx qk ql qm pb b">int8</code> version of the <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html" rel="noopener ugc nofollow" target="_blank">matrix multiplication</a> example.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="3991" class="pe ob fq pb b bg pf pg l ph pi">import functools, timeit<br/>import jax<br/>import jax.numpy as jnp<br/>from jax.experimental import pallas as pl<br/>from jax.experimental.pallas import tpu as pltpu<br/><br/><br/># set to True to develop/debug on CPU<br/>interpret = False<br/><br/><br/>def matmul_kernel_int8(x_ref, y_ref, z_ref, acc_ref, *, nsteps):<br/>    @pl.when(pl.program_id(2) == 0)<br/>    def _():<br/>        acc_ref[...] = jnp.zeros_like(acc_ref)<br/><br/>    acc_ref[...] += jnp.dot(<br/>        x_ref[...], y_ref[...], preferred_element_type=jnp.int32<br/>    )<br/><br/>    @pl.when(pl.program_id(2) == nsteps - 1)<br/>    def _():<br/>        z_ref[...] = acc_ref[...]<br/><br/><br/>@functools.partial(jax.jit, static_argnames=['bm', 'bk', 'bn'])<br/>def matmul_int8(<br/>        x: jax.Array,<br/>        y: jax.Array,<br/>        *,<br/>        bm: int = 128,<br/>        bk: int = 128,<br/>        bn: int = 128,<br/>):<br/>    m, k = x.shape<br/>    _, n = y.shape<br/>    return pl.pallas_call(<br/>        functools.partial(matmul_kernel_int8, nsteps=k // bk),<br/>        grid_spec=pltpu.PrefetchScalarGridSpec(<br/>            num_scalar_prefetch=0,<br/>            in_specs=[<br/>                pl.BlockSpec(block_shape=(bm, bk), <br/>                             index_map=lambda i, j, k: (i, k)),<br/>                pl.BlockSpec(block_shape=(bk, bn),<br/>                             index_map=lambda i, j, k: (k, j)),<br/>            ],<br/>            out_specs=pl.BlockSpec(block_shape=(bm, bn), <br/>                                   index_map=lambda i, j, k: (i, j)),<br/>            scratch_shapes=[pltpu.VMEM((bm, bn), jnp.int32)],<br/>            grid=(m // bm, n // bn, k // bk),<br/>        ),<br/>        out_shape=jax.ShapeDtypeStruct((m, n), jnp.int32),<br/>        compiler_params=dict(mosaic=dict(<br/>            dimension_semantics=("parallel", "parallel", "arbitrary"))),<br/>        interpret=interpret<br/>    )(x, y)<br/></span></pre><p id="f1fb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note our use of an <code class="cx qk ql qm pb b">int32</code> accumulation matrix for addressing the possibility of overflow. Also note our use of the <a class="af nc" href="https://github.com/jax-ml/jax/blob/jaxlib-v0.4.32/jax/_src/pallas/pallas_call.py#L1238" rel="noopener ugc nofollow" target="_blank"><em class="nz">interpret</em></a><em class="nz"> </em>flag for debugging of Pallas kernels on CPU (as recommended <a class="af nc" href="https://youtu.be/OR8NZyTz-yo?t=855" rel="noopener ugc nofollow" target="_blank">here</a>).</p><p id="e541" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To assess our kernel, we introduce a slight modification to the benchmarking utilities defined in the <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#performance-of-pipelined-kernels" rel="noopener ugc nofollow" target="_blank">tutorial</a> and compare the runtime results to both the jnp.float16 Pallas matmul<em class="nz"> </em>kernel and the built-in JAX <a class="af nc" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.matmul.html" rel="noopener ugc nofollow" target="_blank">matmul</a> API:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="20fe" class="pe ob fq pb b bg pf pg l ph pi">def benchmark(f, ntrials: int = 100):<br/>    def run(*args, **kwargs):<br/>        # Compile function first<br/>        jax.block_until_ready(f(*args, **kwargs))<br/>        # Time function<br/>        res=timeit.timeit(lambda: jax.block_until_ready(f(*args, **kwargs)),<br/>                             number=ntrials<br/>                              )<br/>        time = res/ntrials<br/>        # print(f"Time: {time}")<br/>        return time<br/><br/>    return run<br/><br/><br/>def analyze_matmul(m: int, k: int, n: int, dtype: jnp.dtype,<br/>                   mm_func):<br/>    x = jnp.ones((m, k), dtype=dtype)<br/>    y = jnp.ones((k, n), dtype=dtype)<br/>    time = benchmark(mm_func)(x, y)<br/>    print("Matmul time: ", time)<br/>    mm_ops = 2*m*k*n/time<br/>    v5e_ops = 394e12 if dtype == jnp.int8 else 197e12<br/>    print(f"OP/s utilization: {mm_ops / v5e_ops * 100:.4f}%")<br/>    print()<br/><br/><br/>print("bfloat16 Pallas matmul")<br/>mm = functools.partial(matmul, bm=512, bk=1024, bn=1024)<br/>analyze_matmul(8192, 8192, 8192, jnp.bfloat16, mm)<br/><br/><br/>print("int8 Pallas matmul")<br/>mm = functools.partial(matmul_int8, bm=512, bk=1024, bn=1024)<br/>analyze_matmul(8192, 8192, 8192, jnp.int8, mm)<br/><br/>print("XLA int8 matmul")<br/>mm = functools.partial(jnp.matmul, preferred_element_type=jnp.int32)<br/>analyze_matmul(8192, 8192, 8192, jnp.int8, mm)</span></pre><p id="ecde" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The results of our experiment are captured in the table below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qn"><img src="../Images/3f08becc69e1ae67e2b6baa93d5f7da2.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*tFsDAOlQJsu4C0S_W-M1QA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Matmul time and utilization (by Author)</figcaption></figure><p id="36f6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">By using <code class="cx qk ql qm pb b">int8</code> matrices (rather than <code class="cx qk ql qm pb b">bfloat16</code>matrices) on tpuv5e we can boost the runtime performance of our custom matrix multiplication kernel by 71%. However, as in the case of the <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html#performance-of-pipelined-kernels" rel="noopener ugc nofollow" target="_blank">bfloat16 example</a>, additional tuning is required to match the performance of the built-in matmul operator. The potential for improvement is highlighted by the drop in system utilization when compared to <code class="cx qk ql qm pb b">bfloat16</code>.</p><h1 id="3f35" class="pj ob fq bf oc pk pl gq og pm pn gt ok po pp pq pr ps pt pu pv pw px py pz qa bk">Creating a Kernel from Scratch</h1><p id="34fc" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">While leveraging existing kernels can be greatly beneficial, it is unlikely to solve all of your problems. Inevitably, you may need to implement an operation that is either unsupported on TPU or exhibits suboptimal performance. Here we demonstrate the creation of a relatively simple pixel-wise kernel. For the sake of continuity, we choose the same <a class="af nc" href="https://giou.stanford.edu/" rel="noopener ugc nofollow" target="_blank">Generalized Intersection Over Union (GIOU)</a> operation as in our <a class="af nc" rel="noopener" target="_blank" href="/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12">previous posts</a>.</p><h2 id="1656" class="oa ob fq bf oc od oe of og oh oi oj ok nm ol om on nq oo op oq nu or os ot ou bk">Example — A GIOU Pallas Kernel</h2><p id="876e" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">In the code block below we define a Pallas kernel that implements GIOU on pairs of batches of bounding boxes, each of dimension <em class="nz">BxNx4</em> (where we denote the batch size by <em class="nz">B </em>and the number of boxes per sample by <em class="nz">N</em>)<em class="nz"> </em>. The function returns a tensor of scores of dimension <em class="nz">BxN</em>. We choose a block size of 128 on both the <em class="nz">batch</em> axis and the <em class="nz">boxes</em> axis, i.e., we divide each of the tensors into blocks of <em class="nz">128x128x4 </em>and pass them to our kernel function. The <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#grid-a-k-a-kernels-in-a-loop" rel="noopener ugc nofollow" target="_blank"><em class="nz">grid</em></a><em class="nz"> </em>and <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#blockspec-a-k-a-how-to-chunk-up-inputs" rel="noopener ugc nofollow" target="_blank">BlockSpec <em class="nz">index_map</em></a> are defined accordingly.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="b290" class="pe ob fq pb b bg pf pg l ph pi">import timeit<br/>import jax<br/>from jax.experimental import pallas as pl<br/>import jax.numpy as jnp<br/><br/># set to True to develop/debug on CPU<br/>interpret = False<br/><br/># perform giou on a single block<br/>def giou_kernel(preds_left_ref,<br/>                preds_top_ref,<br/>                preds_right_ref,<br/>                preds_bottom_ref,<br/>                targets_left_ref,<br/>                targets_top_ref,<br/>                targets_right_ref,<br/>                targets_bottom_ref,<br/>                output_ref):<br/>    epsilon = 1e-5<br/><br/>    # copy tensors into local memory<br/>    preds_left = preds_left_ref[...]<br/>    preds_top = preds_top_ref[...]<br/>    preds_right = preds_right_ref[...]<br/>    preds_bottom = preds_bottom_ref[...]<br/><br/>    gt_left = targets_left_ref[...]<br/>    gt_top = targets_top_ref[...]<br/>    gt_right = targets_right_ref[...]<br/>    gt_bottom = targets_bottom_ref[...]<br/><br/>    # Compute the area of each box<br/>    area1 = (preds_right - preds_left) * (preds_bottom - preds_top)<br/>    area2 = (gt_right - gt_left) * (gt_bottom - gt_top)<br/><br/>    # Compute the intersection<br/>    left = jnp.maximum(preds_left, gt_left)<br/>    top = jnp.maximum(preds_top, gt_top)<br/>    right = jnp.minimum(preds_right, gt_right)<br/>    bottom = jnp.minimum(preds_bottom, gt_bottom)<br/><br/>    # intersection width and height<br/>    inter_w = jnp.maximum(right - left, 0)<br/>    inter_h = jnp.maximum(bottom - top, 0)<br/><br/>    # intersection area<br/>    inter_area = inter_w * inter_h<br/><br/>    # union of two boxes<br/>    union_area = area1 + area2 - inter_area<br/><br/>    iou_val = inter_area / jnp.maximum(union_area, epsilon)<br/><br/>    # Compute the smallest enclosing box<br/>    enclose_left = jnp.minimum(preds_left, gt_left)<br/>    enclose_top = jnp.minimum(preds_top, gt_top)<br/>    enclose_right = jnp.maximum(preds_right, gt_right)<br/>    enclose_bottom = jnp.maximum(preds_bottom, gt_bottom)<br/><br/>    # enclosing box width and height<br/>    enclose_w = jnp.maximum(enclose_right - enclose_left, 0)<br/>    enclose_h = jnp.maximum(enclose_bottom - enclose_top, 0)<br/><br/>    # enclosing box area<br/>    enclose_area = enclose_w * enclose_h<br/><br/>    # Compute GIOU<br/>    delta_area = (enclose_area - union_area)<br/>    enclose_area = jnp.maximum(enclose_area, epsilon)<br/>    output_ref[...] = iou_val - delta_area / enclose_area<br/><br/><br/>@jax.jit<br/>def batch_giou(preds, targets):<br/>    m, n, _ = preds.shape<br/>    output = pl.pallas_call(<br/>        giou_kernel,<br/>        out_shape=jax.ShapeDtypeStruct((m, n), preds.dtype),<br/>        in_specs=[pl.BlockSpec(block_shape=(128, 128),<br/>                               index_map=lambda i, j: (i, j))]*8,<br/>        out_specs=pl.BlockSpec(block_shape=(128, 128),<br/>                                index_map=lambda i, j: (i, j)),<br/>        grid=(m // 128, n // 128),<br/>        compiler_params=dict(mosaic=dict(<br/>            dimension_semantics=("parallel", "parallel"))),<br/>        interpret=interpret<br/>    )(*jnp.unstack(preds, axis=-1), *jnp.unstack(targets, axis=-1))<br/>    return output</span></pre><p id="df03" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Although the creation of a new TPU kernel is certainly cause for celebration (especially if it enables a previously blocked ML workload) our work is not done. A critical part of Pallas kernel development is tuning the operator, (e.g. the <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#blockspec-a-k-a-how-to-chunk-up-inputs" rel="noopener ugc nofollow" target="_blank"><em class="nz">block size</em></a>) for optimal runtime performance. We omit this stage in the interest of brevity.</p><p id="bb1e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To asses the performance of our kernel, we compare it to the following native JAX GIOU implementation:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="9d30" class="pe ob fq pb b bg pf pg l ph pi">def batched_box_iou(boxes1, boxes2):<br/>    epsilon = 1e-5<br/><br/>    # Compute areas of both sets of boxes<br/>    area1 = (boxes1[..., 2]-boxes1[..., 0])*(boxes1[..., 3]-boxes1[..., 1])<br/>    area2 = (boxes2[..., 2]-boxes2[..., 0])*(boxes2[..., 3]-boxes2[..., 1])<br/><br/>    # corners of intersection<br/>    lt = jnp.maximum(boxes1[..., :2], boxes2[..., :2])<br/>    rb = jnp.minimum(boxes1[..., 2:], boxes2[..., 2:])<br/><br/>    # width and height of intersection<br/>    wh = jnp.clip(rb - lt, a_min=0)<br/><br/>    # area of the intersection<br/>    inter = wh[..., 0] * wh[..., 1]<br/><br/>    # union of the two boxes<br/>    union = area1 + area2 - inter<br/>    iou = inter / jnp.clip(union, a_min=epsilon)<br/><br/>    # corners of enclosing box<br/>    lti = jnp.minimum(boxes1[..., :2], boxes2[..., :2])<br/>    rbi = jnp.maximum(boxes1[..., 2:], boxes2[..., 2:])<br/><br/>    # Width and height of the enclosing box<br/>    whi = jnp.clip(rbi - lti, a_min=0)<br/><br/>    # Area of the enclosing box<br/>    areai = jnp.clip(whi[..., 0] * whi[..., 1], a_min=epsilon)<br/><br/>    # Generalized IoU<br/>    return iou - (areai - union) / areai</span></pre><p id="0e7f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We generate two batches of randomly generated bounding boxes and measure the performance of our functions using the <em class="nz">benchmark</em> function defined above.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="9517" class="pe ob fq pb b bg pf pg l ph pi">from jax import random<br/><br/>batch_size = 1024<br/>n_boxes = 256<br/>img_size = 256<br/>boxes = []<br/>for i in range(2):<br/>    k1, k2 = random.split(random.key(i), 2)<br/><br/>    # Randomly generate box sizes and positions<br/>    box_sizes = random.randint(k1, shape=(batch_size, n_boxes, 2), minval=1, maxval=img_size)<br/>    top_left = random.randint(k2, shape=(batch_size, n_boxes, 2), minval=0, maxval=img_size - 1)<br/>    bottom_right = jnp.clip(top_left + box_sizes, 0, img_size - 1)<br/><br/>    # Concatenate top-left and bottom-right coordinates<br/>    rand_boxes = jnp.concatenate((top_left, bottom_right), axis=2)<br/><br/>    boxes.append(rand_boxes.astype(jnp.float32))<br/><br/><br/>time = benchmark(batch_giou)(boxes[0], boxes[1])<br/>print(f'Pallas kernel: {time}')<br/>time = benchmark(batched_box_iou)(boxes[0], boxes[1])<br/>print(f'JAX function: {time}')<br/>time = benchmark(jax.jit(batched_box_iou))(boxes[0], boxes[1])<br/>print(f'Jitted function: {time}')</span></pre><p id="fd9c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The comparative results appear in the table below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qo"><img src="../Images/955378e825c927199dbea52380144e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*3z5bznK42OtBd2qTPE3VSQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Avg time of different GIOU implementations (lower is better) — by Author</figcaption></figure><p id="a193" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can see that JIT-compiling our naive JAX implementation results in slightly better performance than our Pallas kernel. Once again, we can see that matching or surpassing the performance results of JIT compilation (and its inherent kernel fusion) would require fine-tuning of our custom kernel.</p><h1 id="5c34" class="pj ob fq bf oc pk pl gq og pm pn gt ok po pp pq pr ps pt pu pv pw px py pz qa bk">Utilizing the Sequential Nature of TPUs</h1><p id="984b" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">While the ability to develop custom kernels for TPU offers great potential, our examples thus far have demonstrated that reaching optimal runtime performance could be challenging. One way to overcome this is to seek opportunities to utilize the unique properties of the TPU architecture. One example of this is the <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/details.html#what-is-a-tpu" rel="noopener ugc nofollow" target="_blank">sequential nature of the TPU processor</a>. Although deep learning workloads tend to rely on operations that are easily parallelizable (e.g., matrix multiplication), on occasion they require algorithms that are inherently sequential. These can pose a serious challenge for the <a class="af nc" href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads" rel="noopener ugc nofollow" target="_blank">SIMT</a> (single instruction multi thread) model of GPUs and can sometimes have a disproportionate impact on runtime performance. In a <a class="af nc" href="https://chaimrand.medium.com/implementing-sequential-algorithms-on-tpu-41d75c6aaa95" rel="noopener">sequel to this post</a>, we demonstrate how we can implement sequential algorithms in a way that takes advantage of the TPUs sequential processor and in a manner that minimizes their performance penalty.</p><h1 id="0abd" class="pj ob fq bf oc pk pl gq og pm pn gt ok po pp pq pr ps pt pu pv pw px py pz qa bk">Summary</h1><p id="0752" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The introduction of Pallas marks an important milestone in the evolution of TPUs. By enabling customization of TPU operations it can potentially unlock new opportunities for TPU programmability, particularly in the world of ML. Our intention in this post was to demonstrate the accessibility of this powerful new feature. While our examples have indeed shown this, they have also highlighted the effort required to reach optimal runtime performance.</p><p id="f5ed" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This post has merely scratched the surface of Pallas kernel development. Be sure to see the official documentation to learn more about <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/design.html#grad-of-pallas-call" rel="noopener ugc nofollow" target="_blank">automatic differentiation in Pallas</a>, <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/tpu/sparse.html" rel="noopener ugc nofollow" target="_blank">developing sparse kernels</a>, and more.</p></div></div></div></div>    
</body>
</html>