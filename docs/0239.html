<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Estimate Depth from a Single Image</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Estimate Depth from a Single Image</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/how-to-estimate-depth-from-a-single-image-7f421d86b22d?source=collection_archive---------0-----------------------#2024-01-25">https://towardsdatascience.com/how-to-estimate-depth-from-a-single-image-7f421d86b22d?source=collection_archive---------0-----------------------#2024-01-25</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="76e4" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Run and evaluate monocular depth estimation models with Hugging Face and FiftyOne</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@jacob_marks?source=post_page---byline--7f421d86b22d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jacob Marks, Ph.D." class="l ep by dd de cx" src="../Images/94d9832b8706d1044e3195386613bfab.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*yeOWVwqab7MArjMlb2x4jw@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--7f421d86b22d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@jacob_marks?source=post_page---byline--7f421d86b22d--------------------------------" rel="noopener follow">Jacob Marks, Ph.D.</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--7f421d86b22d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Jan 25, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/0d3b08d99c0c36b77d0d0c0bb16c53f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RCU--Cw4NJwks7o16rCyZg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Monocular Depth heat maps generated with Marigold on NYU depth v2 images. Image courtesy of the author.</figcaption></figure><p id="63f8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Humans view the world through two eyes. One of the primary benefits of this <em class="ny">binocular </em>vision is the ability to perceive <em class="ny">depth</em> ‚Äî how near or far objects are. The human brain infers object depths by comparing the pictures captured by left and right eyes at the same time and interpreting the disparities. This process is known as <a class="af nz" href="https://en.wikipedia.org/wiki/Stereopsis" rel="noopener ugc nofollow" target="_blank">stereopsis</a>.</p><p id="0304" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Just as depth perception plays a crucial role in human vision and navigation, the ability to estimate depth is critical for a wide range of computer vision applications, from autonomous driving to robotics, and even augmented reality. Yet a slew of practical considerations from spatial limitations to budgetary constraints often limit these applications to a single camera.</p><p id="9cd9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af nz" href="https://paperswithcode.com/task/monocular-depth-estimation" rel="noopener ugc nofollow" target="_blank">Monocular depth estimation</a> (MDE) is the task of predicting the depth of a scene from a single image. Depth computation from a single image is inherently ambiguous, as there are multiple ways to project the same 3D scene onto the 2D plane of an image. As a result, MDE is a challenging task that requires (either explicitly or implicitly) factoring in many cues such as object size, occlusion, and perspective.</p><p id="7895" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this post, we will illustrate how to load and visualize depth map data, run monocular depth estimation models, and evaluate depth predictions. We will do so using data from the <a class="af nz" href="https://rgbd.cs.princeton.edu/" rel="noopener ugc nofollow" target="_blank">SUN RGB-D</a> dataset.</p><p id="377f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In particular, we will cover the following:</p><ul class=""><li id="88ba" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oa ob oc bk"><a class="af nz" href="#15fc" rel="noopener ugc nofollow">Loading and visualizing SUN-RGBD ground truth depth maps</a></li><li id="4e24" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk"><a class="af nz" href="#fd8c" rel="noopener ugc nofollow">Running inference with Marigold and DPT</a></li><li id="54a1" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk"><a class="af nz" href="#1924" rel="noopener ugc nofollow">Evaluating relative depth predictions</a></li></ul><p id="e152" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We will use the Hugging Face <a class="af nz" href="https://huggingface.co/docs/transformers/index" rel="noopener ugc nofollow" target="_blank">transformers</a> and <a class="af nz" href="https://huggingface.co/docs/diffusers/index" rel="noopener ugc nofollow" target="_blank">diffusers</a> libraries for inference, <a class="af nz" href="https://github.com/voxel51/fiftyone" rel="noopener ugc nofollow" target="_blank">FiftyOne</a> for data management and visualization, and <a class="af nz" href="https://scikit-image.org/" rel="noopener ugc nofollow" target="_blank">scikit-image</a> for evaluation metrics. All of these libraries are open source and free to use. <em class="ny">Disclaimer: I work at Voxel51, the lead maintainers of one of these libraries (FiftyOne).</em></p><p id="55cf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Before we get started, make sure you have all of the necessary libraries installed:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="cd19" class="om on fq oj b bg oo op l oq or">pip install -U torch fiftyone diffusers transformers scikit-image</span></pre><p id="a67c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then we‚Äôll import the modules we‚Äôll be using throughout the post:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="a08f" class="om on fq oj b bg oo op l oq or">from glob import glob<br/>import numpy as np<br/>from PIL import Image<br/>import torch<br/><br/>import fiftyone as fo<br/>import fiftyone.zoo as foz<br/>import fiftyone.brain as fob<br/>from fiftyone import ViewField as F</span></pre><h1 id="15fc" class="os on fq bf ot ou ov gq ow ox oy gt oz pa pb pc pd pe pf pg ph pi pj pk pl pm bk">Loading and Visualizing SUN-RGBD Depth Data</h1><p id="134c" class="pw-post-body-paragraph nc nd fq ne b go pn ng nh gr po nj nk nl pp nn no np pq nr ns nt pr nv nw nx fj bk">The <a class="af nz" href="https://rgbd.cs.princeton.edu/" rel="noopener ugc nofollow" target="_blank">SUN RGB-D dataset</a> contains 10,335 RGB-D images, each of which has a corresponding RGB image, depth image, and camera intrinsics. It contains images from the <a class="af nz" href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" rel="noopener ugc nofollow" target="_blank">NYU depth v2</a>, Berkeley <a class="af nz" href="http://kinectdata.com/" rel="noopener ugc nofollow" target="_blank">B3DO</a>, and <a class="af nz" href="https://sun3d.cs.princeton.edu/" rel="noopener ugc nofollow" target="_blank">SUN3D</a> datasets. SUN RGB-D is <a class="af nz" href="https://paperswithcode.com/dataset/sun-rgb-d" rel="noopener ugc nofollow" target="_blank">one of the most popular</a> datasets for monocular depth estimation and semantic segmentation tasks!</p><p id="a001" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">üí°For this walkthrough, we will only use the NYU depth v2 portions. NYU depth v2 is <a class="af nz" href="https://github.com/dwofk/fast-depth/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank">permissively licensed for commercial use</a> (MIT), and can be <a class="af nz" href="https://huggingface.co/datasets/sayakpaul/nyu_depth_v2" rel="noopener ugc nofollow" target="_blank">downloaded from Hugging Face</a> directly.</p><h2 id="f298" class="ps on fq bf ot pt pu pv ow pw px py oz nl pz qa qb np qc qd qe nt qf qg qh qi bk">Downloading the Raw Data</h2><p id="ff4a" class="pw-post-body-paragraph nc nd fq ne b go pn ng nh gr po nj nk nl pp nn no np pq nr ns nt pr nv nw nx fj bk">First, download the SUN RGB-D dataset from <a class="af nz" href="https://rgbd.cs.princeton.edu/" rel="noopener ugc nofollow" target="_blank">here</a> and unzip it, or use the following command to download it directly:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="56e9" class="om on fq oj b bg oo op l oq or">curl -o sunrgbd.zip https://rgbd.cs.princeton.edu/data/SUNRGBD.zip</span></pre><p id="3f0b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And then unzip it:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="c1d9" class="om on fq oj b bg oo op l oq or">unzip sunrgbd.zip</span></pre><p id="e239" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you want to use the dataset for other tasks, you can fully convert the annotations and load them into your <code class="cx qj qk ql oj b">fiftyone.Dataset</code>. However, for this tutorial, we will only be using the depth images, so we will only use the RGB images and the depth images (stored in the <code class="cx qj qk ql oj b">depth_bfx</code> sub-directories).</p><h2 id="e647" class="ps on fq bf ot pt pu pv ow pw px py oz nl pz qa qb np qc qd qe nt qf qg qh qi bk">Creating the Dataset</h2><p id="327f" class="pw-post-body-paragraph nc nd fq ne b go pn ng nh gr po nj nk nl pp nn no np pq nr ns nt pr nv nw nx fj bk">Because we are just interested in getting the point across, we‚Äôll restrict ourselves to the first 20 samples, which are all from the NYU Depth v2 portion of the dataset:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="22d3" class="om on fq oj b bg oo op l oq or">## create, name, and persist the dataset<br/>dataset = fo.Dataset(name="SUNRGBD-20", persistent=True)<br/><br/>## pick out first 20 scenes<br/>scene_dirs = glob("SUNRGBD/kv1/NYUdata/*")[:20]<br/><br/>samples = []<br/><br/>for scene_dir in scene_dirs:<br/>    ## Get image file path from scene directory<br/>    image_path = glob(f"{scene_dir}/image/*")[0]<br/><br/>    ## Get depth map file path from scene directory<br/>    depth_path = glob(f"{scene_dir}/depth_bfx/*")[0]<br/><br/>    depth_map = np.array(Image.open(depth_path))<br/>    depth_map = (depth_map * 255 / np.max(depth_map)).astype("uint8")<br/><br/>    ## Create sample<br/>    sample = fo.Sample(<br/>        filepath=image_path,<br/>        gt_depth=fo.Heatmap(map=depth_map),<br/>    )<br/>    <br/>    samples.append(sample)<br/><br/>## Add samples to dataset<br/>dataset.add_samples(samples);</span></pre><p id="c4f9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here we are storing the depth maps as <a class="af nz" href="https://docs.voxel51.com/user_guide/using_datasets.html#heatmaps" rel="noopener ugc nofollow" target="_blank">heatmaps</a>. Everything is represented in terms of normalized, <em class="ny">relative</em> distances, where 255 represents the maximum distance in the scene and 0 represents the minimum distance in the scene. This is a common way to represent depth maps, although it is far from the only way to do so. If we were interested in <em class="ny">absolute</em> distances, we could store sample-wise parameters for the minimum and maximum distances in the scene, and use these to reconstruct the absolute distances from the relative distances.</p><h2 id="710f" class="ps on fq bf ot pt pu pv ow pw px py oz nl pz qa qb np qc qd qe nt qf qg qh qi bk">Visualizing Ground Truth Data</h2><p id="77da" class="pw-post-body-paragraph nc nd fq ne b go pn ng nh gr po nj nk nl pp nn no np pq nr ns nt pr nv nw nx fj bk">With heatmaps stored on our samples, we can visualize the ground truth data:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="7ad9" class="om on fq oj b bg oo op l oq or">session = fo.launch_app(dataset, auto=False)<br/>## then open tab to localhost:5151 in browser</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/e13d2736bd24bfe61ef3629acc934b4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gfYdMuqhPCRaU1lOPndGIQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Ground truth depth maps for samples from the SUN RGB-D dataset. Image courtesy of the author.</figcaption></figure><p id="863f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When working with depth maps, the color scheme and opacity of the heatmap are important. I‚Äôm colorblind, so I find that the <a class="af nz" href="https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html" rel="noopener ugc nofollow" target="_blank">viridis</a> colormap with opacity turned all the way up works best for me.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qm"><img src="../Images/bcf625a36ee7d97a1afe4d32d72d2046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_YoCd12vrkLIGMGq_GMokQ.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Visibility settings for heatmaps. Image courtesy of the author.</figcaption></figure><h2 id="7c8a" class="ps on fq bf ot pt pu pv ow pw px py oz nl pz qa qb np qc qd qe nt qf qg qh qi bk">Ground Truth?</h2><p id="54b9" class="pw-post-body-paragraph nc nd fq ne b go pn ng nh gr po nj nk nl pp nn no np pq nr ns nt pr nv nw nx fj bk">Inspecting these RGB images and depth maps, we can see that there are some inaccuracies in the ground truth depth maps. For example, in this image, the dark rift through the center of the image is actually the <em class="ny">farthest</em> part of the scene, but the ground truth depth map shows it as the <em class="ny">closest</em> part of the scene:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/5089c29d7fffe32d2eb44bb9fba5f018.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dl2pS_gLxPma2L3-U7Uvdg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Issue in ground truth depth data for sample from the SUN RGB-D dataset. Image courtesy of the author.</figcaption></figure><p id="bdd0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is one of the key challenges for MDE tasks: ground truth data is hard to come by, and is often noisy! It‚Äôs essential to be aware of this while evaluating your MDE models.</p><h1 id="fd8c" class="os on fq bf ot ou ov gq ow ox oy gt oz pa pb pc pd pe pf pg ph pi pj pk pl pm bk">Running Monocular Depth Estimation Models</h1><p id="c927" class="pw-post-body-paragraph nc nd fq ne b go pn ng nh gr po nj nk nl pp nn no np pq nr ns nt pr nv nw nx fj bk">Now that we have our dataset loaded in, we can run monocular depth estimation models on our RGB images!</p><p id="6246" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For a long time, the state-of-the-art models for monocular depth estimation such as <a class="af nz" href="https://github.com/hufu6371/DORN" rel="noopener ugc nofollow" target="_blank">DORN</a> and <a class="af nz" href="https://github.com/ialhashim/DenseDepth" rel="noopener ugc nofollow" target="_blank">DenseDepth</a> were built with convolutional neural networks. Recently, however, both transformer-based models such as <a class="af nz" href="https://huggingface.co/docs/transformers/model_doc/dpt" rel="noopener ugc nofollow" target="_blank">DPT</a> and <a class="af nz" href="https://huggingface.co/docs/transformers/model_doc/glpn" rel="noopener ugc nofollow" target="_blank">GLPN</a>, and diffusion-based models like <a class="af nz" href="https://huggingface.co/Bingxin/Marigold" rel="noopener ugc nofollow" target="_blank">Marigold</a> have achieved remarkable results!</p><p id="bd46" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this section, we‚Äôll show you how to generate MDE depth map predictions with both DPT and Marigold. In both cases, you can optionally run the model locally with the respective Hugging Face library, or run remotely with <a class="af nz" href="https://replicate.com/" rel="noopener ugc nofollow" target="_blank">Replicate</a>.</p><p id="e29f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To run via Replicate, install the Python client:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="a56a" class="om on fq oj b bg oo op l oq or">pip install replicate</span></pre><p id="2b7b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And export your Replicate API token:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="631b" class="om on fq oj b bg oo op l oq or">export REPLICATE_API_TOKEN=r8_&lt;your_token_here&gt;</span></pre><p id="da9b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">üí° With Replicate, it might take a minute for the model to load into memory on the server (cold-start problem), but once it does the prediction should only take a few seconds. Depending on your local compute resources, running on server may give you massive speedups compared to running locally, especially for Marigold and other diffusion-based depth-estimation approaches.</p><h2 id="44b6" class="ps on fq bf ot pt pu pv ow pw px py oz nl pz qa qb np qc qd qe nt qf qg qh qi bk">Monocular Depth Estimation with DPT</h2><p id="a1a8" class="pw-post-body-paragraph nc nd fq ne b go pn ng nh gr po nj nk nl pp nn no np pq nr ns nt pr nv nw nx fj bk">The first model we will run is a dense-prediction transformer (DPT). DPT models have found utility in both MDE and semantic segmentation ‚Äî tasks that require ‚Äúdense‚Äù, pixel-level predictions.</p><p id="e514" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The checkpoint below uses <a class="af nz" href="https://github.com/isl-org/MiDaS/tree/master" rel="noopener ugc nofollow" target="_blank">MiDaS</a>, which returns the <a class="af nz" href="https://pyimagesearch.com/2022/01/17/torch-hub-series-5-midas-model-on-depth-estimation/" rel="noopener ugc nofollow" target="_blank">inverse depth map</a>, so we have to invert it back to get a comparable depth map.</p><p id="d8d1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To run locally with <code class="cx qj qk ql oj b">transformers</code>, first we load the model and image processor:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="7b28" class="om on fq oj b bg oo op l oq or">from transformers import AutoImageProcessor, AutoModelForDepthEstimation<br/><br/>## swap for "Intel/dpt-large" if you'd like<br/>pretrained = "Intel/dpt-hybrid-midas"<br/><br/>image_processor = AutoImageProcessor.from_pretrained(pretrained)<br/>dpt_model = AutoModelForDepthEstimation.from_pretrained(pretrained)</span></pre><p id="b8be" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, we encapsulate the code for inference on a sample, including pre and post processing:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="6f15" class="om on fq oj b bg oo op l oq or">def apply_dpt_model(sample, model, label_field):<br/>    image = Image.open(sample.filepath)<br/>    inputs = image_processor(images=image, return_tensors="pt")<br/><br/>    with torch.no_grad():<br/>        outputs = model(**inputs)<br/>        predicted_depth = outputs.predicted_depth<br/><br/>    prediction = torch.nn.functional.interpolate(<br/>        predicted_depth.unsqueeze(1),<br/>        size=image.size[::-1],<br/>        mode="bicubic",<br/>        align_corners=False,<br/>    )<br/><br/>    output = prediction.squeeze().cpu().numpy()<br/>    ## flip b/c MiDaS returns inverse depth<br/>    formatted = (255 - output * 255 / np.max(output)).astype("uint8")<br/><br/>    sample[label_field] = fo.Heatmap(map=formatted)<br/>    sample.save()</span></pre><p id="1d83" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here, we are storing predictions in a <code class="cx qj qk ql oj b">label_field</code> field on our samples, represented with a heatmap just like the ground truth labels.</p><p id="bcc9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that in the <code class="cx qj qk ql oj b">apply_dpt_model()</code> function, between the model's forward pass and the heatmap generation, notice that we make a call to <code class="cx qj qk ql oj b">torch.nn.functional.interpolate()</code>. This is because the model's forward pass is run on a downsampled version of the image, and we want to return a heatmap that is the same size as the original image.</p><p id="94d6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Why do we need to do this? If we just want to *look* at the heatmaps, this would not matter. But if we want to compare the ground truth depth maps to the model‚Äôs predictions on a per-pixel basis, we need to make sure that they are the same size.</p><p id="71ce" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All that is left to do is iterate through the dataset:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="5927" class="om on fq oj b bg oo op l oq or">for sample in dataset.iter_samples(autosave=True, progress=True):<br/>    apply_dpt_model(sample, dpt_model, "dpt")<br/><br/>session = fo.launch_app(dataset)</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/92fca2436322f33052cdabbccf4c1a59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X9gAGoxSjl1M9cXyTCQlPg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Relative depth maps predicted by a hybrid MiDaS DPT model on SUN RGB-D sample images. Image courtesy of the author.</figcaption></figure><p id="70f8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To run with Replicate, you can use <a class="af nz" href="https://replicate.com/cjwbw/midas" rel="noopener ugc nofollow" target="_blank">this</a> model. Here is what the API looks like:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="7619" class="om on fq oj b bg oo op l oq or">import replicate<br/><br/>## example application to first sample<br/>rgb_fp = dataset.first().filepath<br/><br/>output = replicate.run(<br/>    "cjwbw/midas:a6ba5798f04f80d3b314de0f0a62277f21ab3503c60c84d4817de83c5edfdae0",<br/>    input={<br/>        "model_type": "dpt_beit_large_512",<br/>        "image":open(rgb_fp, "rb")<br/>    }<br/>)<br/>print(output)</span></pre><h2 id="e147" class="ps on fq bf ot pt pu pv ow pw px py oz nl pz qa qb np qc qd qe nt qf qg qh qi bk">Monocular Depth Estimation with Marigold</h2><p id="8805" class="pw-post-body-paragraph nc nd fq ne b go pn ng nh gr po nj nk nl pp nn no np pq nr ns nt pr nv nw nx fj bk">Stemming from their tremendous success in text-to-image contexts, diffusion models are being applied to an ever-broadening range of problems. <a class="af nz" href="https://huggingface.co/Bingxin/Marigold" rel="noopener ugc nofollow" target="_blank">Marigold</a> ‚Äúrepurposes‚Äù diffusion-based image generation models for monocular depth estimation.</p><p id="7e66" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To run Marigold locally, you will need to clone the git repository:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="4554" class="om on fq oj b bg oo op l oq or">git clone https://github.com/prs-eth/Marigold.git</span></pre><p id="eef1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This repository introduces a new diffusers pipeline, <code class="cx qj qk ql oj b">MarigoldPipeline</code>, which makes applying Marigold easy:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="a5f7" class="om on fq oj b bg oo op l oq or">## load model<br/>from Marigold.marigold import MarigoldPipeline<br/>pipe = MarigoldPipeline.from_pretrained("Bingxin/Marigold")<br/><br/>## apply to first sample, as example<br/>rgb_image = Image.open(dataset.first().filepath)<br/>output = pipe(rgb_image)<br/>depth_image = output['depth_colored']</span></pre><p id="15a3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Post-processing of the output depth image is then needed.</p><p id="6129" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To instead run via Replicate, we can create an <code class="cx qj qk ql oj b">apply_marigold_model()</code> function in analogy with the DPT case above and iterate over the samples in our dataset:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="3014" class="om on fq oj b bg oo op l oq or">import replicate<br/>import requests<br/>import io<br/><br/>def marigold_model(rgb_image):<br/>    output = replicate.run(<br/>        "adirik/marigold:1a363593bc4882684fc58042d19db5e13a810e44e02f8d4c32afd1eb30464818",<br/>        input={<br/>            "image":rgb_image<br/>        }<br/>    )<br/>    ## get the black and white depth map<br/>    response = requests.get(output[1]).content<br/>    return response<br/><br/>def apply_marigold_model(sample, model, label_field):<br/>    rgb_image = open(sample.filepath, "rb")<br/>    response = model(rgb_image)<br/>    depth_image = np.array(Image.open(io.BytesIO(response)))[:, :, 0] ## all channels are the same<br/>    formatted = (255 - depth_image).astype("uint8")<br/>    sample[label_field] = fo.Heatmap(map=formatted)<br/>    sample.save()<br/><br/>for sample in dataset.iter_samples(autosave=True, progress=True):<br/>    apply_marigold_model(sample, marigold_model, "marigold")<br/><br/>session = fo.launch_app(dataset)</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/0d3b08d99c0c36b77d0d0c0bb16c53f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RCU--Cw4NJwks7o16rCyZg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Relative depth maps predicted with Marigold endpoint on SUN RGB-D sample images. Image courtesy of the author.</figcaption></figure><h1 id="1924" class="os on fq bf ot ou ov gq ow ox oy gt oz pa pb pc pd pe pf pg ph pi pj pk pl pm bk">Evaluating Monocular Depth Estimation Models</h1><p id="0258" class="pw-post-body-paragraph nc nd fq ne b go pn ng nh gr po nj nk nl pp nn no np pq nr ns nt pr nv nw nx fj bk">Now that we have predictions from multiple models, let‚Äôs evaluate them! We will leverage <code class="cx qj qk ql oj b">scikit-image</code> to apply three simple metrics commonly used for monocular depth estimation: <a class="af nz" href="https://en.wikipedia.org/wiki/Root-mean-square_deviation" rel="noopener ugc nofollow" target="_blank">root mean squared error</a> (RMSE), <a class="af nz" href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio" rel="noopener ugc nofollow" target="_blank">peak signal to noise ratio</a> (PSNR), and <a class="af nz" href="https://en.wikipedia.org/wiki/Structural_similarity" rel="noopener ugc nofollow" target="_blank">structural similarity index</a> (SSIM).</p><p id="e979" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">üí°Higher PSNR and SSIM scores indicate better predictions, while lower RMSE scores indicate better predictions.</p><p id="ff37" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that the specific values I arrive at are a consequence of the specific pre-and-post processing steps I performed along the way. What matters is the relative performance!</p><p id="cec2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We will define the evaluation routine:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="1b67" class="om on fq oj b bg oo op l oq or">from skimage.metrics import peak_signal_noise_ratio, mean_squared_error, structural_similarity<br/><br/>def rmse(gt, pred):<br/>    """Compute root mean squared error between ground truth and prediction"""<br/>    return np.sqrt(mean_squared_error(gt, pred))<br/><br/>def evaluate_depth(dataset, prediction_field, gt_field):<br/>  """Run 3 evaluation metrics for all samples for `prediction_field`<br/>     with respect to `gt_field`"""<br/>    for sample in dataset.iter_samples(autosave=True, progress=True):<br/>        gt_map = sample[gt_field].map<br/>        pred = sample[prediction_field]<br/>        pred_map = pred.map<br/>        pred["rmse"] = rmse(gt_map, pred_map)<br/>        pred["psnr"] = peak_signal_noise_ratio(gt_map, pred_map)<br/>        pred["ssim"] = structural_similarity(gt_map, pred_map)<br/>        sample[prediction_field] = pred<br/>    <br/>    ## add dynamic fields to dataset so we can view them in the App<br/>    dataset.add_dynamic_sample_fields()</span></pre><p id="d4ab" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And then apply the evaluation to the predictions from both models:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="d1d3" class="om on fq oj b bg oo op l oq or">evaluate_depth(dataset, "dpt", "gt_depth")<br/>evaluate_depth(dataset, "marigold", "gt_depth")</span></pre><p id="cdc1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Computing average performance for a certain model/metric is as simple as calling the dataset‚Äôs <code class="cx qj qk ql oj b">mean()</code>method on that field:</p><pre class="mm mn mo mp mq oi oj ok bp ol bb bk"><span id="72c9" class="om on fq oj b bg oo op l oq or">print("Mean Error Metrics")<br/>for model in ["dpt", "marigold"]:<br/>    print("-"*50)<br/>    for metric in ["rmse", "psnr", "ssim"]:<br/>        mean_metric_value = dataset.mean(f"{model}.{metric}")<br/>        print(f"Mean {metric} for {model}: {mean_metric_value}")</span></pre><pre class="qn oi oj ok bp ol bb bk"><span id="3ddd" class="om on fq oj b bg oo op l oq or">Mean Error Metrics<br/>--------------------------------------------------<br/>Mean rmse for dpt: 49.8915828817003<br/>Mean psnr for dpt: 14.805904629602551<br/>Mean ssim for dpt: 0.8398022368184576<br/>--------------------------------------------------<br/>Mean rmse for marigold: 104.0061165272178<br/>Mean psnr for marigold: 7.93015537185192<br/>Mean ssim for marigold: 0.42766803372861134</span></pre><p id="6d53" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All of the metrics seem to agree that DPT outperforms Marigold. However, it is important to note that these metrics are not perfect. For example, RMSE is very sensitive to outliers, and SSIM is not very sensitive to small errors. For a more thorough evaluation, we can filter by these metrics in the app in order to visualize what the model is doing well and what it is doing poorly ‚Äî or where the metrics are failing to capture the model‚Äôs performance.</p><p id="7dbb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Finally, toggling masks on and off is a great way to visualize the differences between the ground truth and the model‚Äôs predictions:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qm"><img src="../Images/9cc8c71a8a38c238f74bdcfea80a543b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8hJkafTQL3t-b2xkWczL6g.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Visual comparison of heatmaps predicted by the two MDE models and the ground truth. Image courtesy of the author.</figcaption></figure><h1 id="8fad" class="os on fq bf ot ou ov gq ow ox oy gt oz pa pb pc pd pe pf pg ph pi pj pk pl pm bk">Conclusion</h1><p id="db10" class="pw-post-body-paragraph nc nd fq ne b go pn ng nh gr po nj nk nl pp nn no np pq nr ns nt pr nv nw nx fj bk">To recap, we learned how to run monocular depth estimation models on our data, how to evaluate the predictions using common metrics, and how to visualize the results. We also learned that monocular depth estimation is a notoriously difficult task.</p><p id="01a4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Data quality and quantity are severely limiting factors; models often struggle to generalize to new environments; and metrics are not always good indicators of model performance. The specific numeric values quantifying model performance can depend greatly on your processing pipeline. And even your qualitative assessment of predicted depth maps can be heavily influenced by your color schemes and opacity scales.</p><p id="7f11" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If there‚Äôs one thing you take away from this post, I hope it is this: it is mission-critical that you look at the depth maps themselves, and not just the metrics!</p></div></div></div><div class="ab cb qo qp qq qr" role="separator"><span class="qs by bm qt qu qv"/><span class="qs by bm qt qu qv"/><span class="qs by bm qt qu"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="1020" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note: All images are courtesy of the author. The sub-dataset used for all workflows and visuals presented in this post is NYU depth v2, which is <a class="af nz" href="https://github.com/dwofk/fast-depth/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank">permissively licensed for commercial use</a> (MIT).</p></div></div></div></div>    
</body>
</html>