- en: Benchmarking Hallucination Detection Methods in RAG
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG 中的幻觉检测方法基准测试
- en: 原文：[https://towardsdatascience.com/benchmarking-hallucination-detection-methods-in-rag-6a03c555f063?source=collection_archive---------0-----------------------#2024-09-09](https://towardsdatascience.com/benchmarking-hallucination-detection-methods-in-rag-6a03c555f063?source=collection_archive---------0-----------------------#2024-09-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/benchmarking-hallucination-detection-methods-in-rag-6a03c555f063?source=collection_archive---------0-----------------------#2024-09-09](https://towardsdatascience.com/benchmarking-hallucination-detection-methods-in-rag-6a03c555f063?source=collection_archive---------0-----------------------#2024-09-09)
- en: Evaluating methods to enhance reliability in LLM-generated responses.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估提升 LLM 生成响应可靠性的方法。
- en: '[](https://medium.com/@hwgoh?source=post_page---byline--6a03c555f063--------------------------------)[![Hui
    Wen Goh](../Images/69b52a340d39f5ebf79eab673187e220.png)](https://medium.com/@hwgoh?source=post_page---byline--6a03c555f063--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6a03c555f063--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6a03c555f063--------------------------------)
    [Hui Wen Goh](https://medium.com/@hwgoh?source=post_page---byline--6a03c555f063--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hwgoh?source=post_page---byline--6a03c555f063--------------------------------)[![Hui
    Wen Goh](../Images/69b52a340d39f5ebf79eab673187e220.png)](https://medium.com/@hwgoh?source=post_page---byline--6a03c555f063--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6a03c555f063--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6a03c555f063--------------------------------)
    [Hui Wen Goh](https://medium.com/@hwgoh?source=post_page---byline--6a03c555f063--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6a03c555f063--------------------------------)
    ·9 min read·Sep 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6a03c555f063--------------------------------)
    ·9 分钟阅读·2024年9月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Unchecked hallucination remains a big problem in today’s Retrieval-Augmented
    Generation applications. This study evaluates popular hallucination detectors
    across 4 public RAG datasets. Using AUROC and precision/recall, we report *how
    well* methods like G-eval, Ragas, and the Trustworthy Language Model are able
    to **automatically flag incorrect LLM responses**.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉问题依然是当前检索增强生成（RAG）应用中的一个重大问题。本研究评估了流行的幻觉检测器在 4 个公开 RAG 数据集上的表现。通过使用 AUROC
    以及精确度/召回率，我们报告了像 G-eval、Ragas 和可信语言模型等方法在**自动标记不正确的 LLM 响应**方面的表现，*效果如何*。
- en: '![](../Images/1fa197d377a413d9ee2388719a78e333.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fa197d377a413d9ee2388719a78e333.png)'
- en: Using various hallucination detection methods to identify LLM errors in RAG
    systems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用各种幻觉检测方法识别 RAG 系统中的 LLM 错误。
- en: I am currently working as a Machine Learning Engineer at Cleanlab, where I have
    contributed to the development of the Trustworthy Language Model discussed in
    this article. I am excited to present this method and evaluate it alongside others
    in the following benchmarks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我目前在 Cleanlab 担任机器学习工程师，参与了本文所讨论的可信语言模型的开发工作。我很高兴在接下来的基准测试中介绍这一方法，并将其与其他方法进行评估。
- en: 'The Problem: Hallucinations and Errors in RAG Systems'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题：RAG 系统中的幻觉与错误
- en: Large Language Models (LLM) are known to *hallucinate* incorrect answers when
    asked questions not well-supported within their training data. Retrieval Augmented
    Generation (RAG) systems mitigate this by augmenting the LLM with the ability
    to *retrieve* context and information from a specific knowledge database. While
    organizations are quickly adopting RAG to pair the power of LLMs with their own
    proprietary data, hallucinations and logical errors remain a big problem. In one
    highly publicized case, a major airline (Air Canada) lost a court case after their
    RAG chatbot hallucinated important details of their refund policy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）在面对未充分支持其训练数据中的问题时，常常会产生*幻觉*，即错误的回答。检索增强生成（RAG）系统通过为 LLM 增加从特定知识数据库中*检索*上下文和信息的能力来缓解这一问题。尽管组织正在迅速采用
    RAG，将 LLM 的强大功能与他们自己的专有数据结合使用，但幻觉和逻辑错误仍然是一个大问题。在一起广泛报道的案例中，一家大型航空公司（加拿大航空）因其 RAG
    聊天机器人出现了关于退款政策的关键信息幻觉而输掉了官司。
- en: 'To understand this issue, let’s first revisit how a RAG system works. When
    a user asks a question (`"Is this is refund eligible?"`), the *retrieval* component
    searches the knowledge database for relevant information needed to respond accurately.
    The most relevant search results are formatted into a *context* which is fed along
    with the user’s question into a LLM that *generates* the response presented to
    the user. Because enterprise RAG systems are often complex, the final response
    might be incorrect for many reasons including:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个问题，让我们首先回顾一下RAG系统是如何工作的。当用户提出问题（例如“这个能退吗？”），*检索*组件会在知识数据库中搜索相关信息，以便准确地作出回应。最相关的搜索结果被格式化为*上下文*，然后与用户的问题一起输入到LLM中，由LLM*生成*最终呈现给用户的回应。由于企业级RAG系统通常比较复杂，最终的回应可能因多种原因不准确，包括：
- en: LLMs are brittle and prone to hallucination. Even when the retrieved context
    contains the correct answer within it, the LLM may fail to generate an accurate
    response, especially if synthesizing the response requires reasoning across different
    facts within the context.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是脆弱的，容易产生幻觉。即使检索到的上下文包含正确答案，LLM也可能无法生成准确的回应，尤其是在生成回应时需要对上下文中的不同事实进行推理时。
- en: The retrieved context may not contain information required to accurately respond,
    due to suboptimal search, poor document chunking/formatting, or the absence of
    this information within the knowledge database. In such cases, the LLM may still
    attempt to answer the question and hallucinate an incorrect response.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于检索不充分、文档分块/格式不良或知识数据库中缺乏相关信息，检索到的上下文可能不包含准确回应所需的信息。在这种情况下，LLM仍然可能尝试回答问题，并产生不正确的回应。
- en: 'While some use the term *hallucination* to refer only to specific types of
    LLM errors, here we use this term synonymously with *incorrect response.* What
    matters to the users of your RAG system is the accuracy of its answers and being
    able to trust them. Unlike RAG benchmarks that assess many system properties,
    we exclusively study: **how effectively different detectors could alert your RAG
    users when the answers are incorrect**.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有些人使用“幻觉”一词仅指某些类型的LLM错误，但在这里我们将其与*不正确回应*同义使用。对RAG系统的用户而言，重要的是答案的准确性以及能够信任这些答案。与评估系统多种属性的RAG基准不同，我们只研究：**不同检测器在回应不准确时，能多有效地提醒RAG用户**。
- en: A RAG answer might be incorrect due to problems during *retrieval* or *generation*.
    Our study focuses on the latter issue, which stems from the fundamental unreliability
    of LLMs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个RAG（检索增强生成）答案可能因*检索*或*生成*过程中的问题而不正确。我们的研究侧重于后者的问题，这源于LLM的根本不可靠性。
- en: 'The Solution: Hallucination Detection Methods'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决方案：幻觉检测方法
- en: Assuming an existing retrieval system has fetched the *context* most relevant
    to a user’s question, we consider algorithms to detect when **the LLM response
    generated based on this context should not be trusted**. Such hallucination detection
    algorithms are critical in high-stakes applications spanning medicine, law, or
    finance. Beyond flagging untrustworthy responses for more careful human review,
    such methods can be used to determine when it is worth executing more expensive
    retrieval steps (e.g. searching additional data sources, rewriting queries, etc).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 假设现有的检索系统已经提取了与用户问题最相关的*上下文*，我们考虑使用算法来检测**基于这个上下文生成的LLM回应不应被信任**的情况。这类幻觉检测算法在医学、法律或金融等高风险应用中至关重要。除了标记不可信的回应供人工进一步审查外，这些方法还可以用来判断何时值得执行更昂贵的检索步骤（例如，搜索额外的数据源、重写查询等）。
- en: 'Here are the hallucination detection methods considered in our study, all based
    on using LLMs to evaluate a generated response:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们研究中考虑的幻觉检测方法，所有方法都基于使用LLM来评估生成的回应：
- en: '**Self-evaluation (”Self-eval”)** is a simple technique whereby the LLM is
    asked to evaluate the generated answer and rate its confidence on a scale of 1–5
    (Likert scale). We utilize *chain-of-thought* (CoT) prompting to improve this
    technique, asking the LLM to explain its confidence before outputting a final
    score. Here is the specific prompt template used:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**自我评估（"Self-eval"）**是一种简单的技术，LLM会被要求评估生成的答案，并按1到5的等级评定其信心。我们利用*思维链*（CoT）提示来改进这一技术，要求LLM在给出最终评分之前，先解释其信心。以下是使用的具体提示模板：'
- en: '*Question: {question}'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*问题：{question}'
- en: 'Answer: {response}*'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 答案：{response}*
- en: ''
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Evaluate how confident you are that the given Answer is a good and accurate
    response to the Question.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 评估你对给定答案是否是一个良好且准确的回应的信心程度。
- en: 'Please assign a Score using the following 5-point scale:'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请使用以下5分制评分标准进行评分：
- en: '1: You are not confident that the Answer addresses the Question at all, the
    Answer may be entirely off-topic or irrelevant to the Question.'
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '1: 你对答案是否能回答问题完全没有信心，答案可能完全跑题或与问题无关。'
- en: '2: You have low confidence that the Answer addresses the Question, there are
    doubts and uncertainties about the accuracy of the Answer.'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '2: 你对答案能否回答问题的信心较低，对于答案的准确性有疑问和不确定性。'
- en: '3: You have moderate confidence that the Answer addresses the Question, the
    Answer seems reasonably accurate and on-topic, but with room for improvement.'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '3: 你对答案是否能够回答问题有中等信心，答案看起来合理准确并且与主题相关，但仍有改进的空间。'
- en: '4: You have high confidence that the Answer addresses the Question, the Answer
    provides accurate information that addresses most of the Question.'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '4: 你对答案能回答问题有较高的信心，答案提供了准确的信息，解答了大部分问题。'
- en: '5: You are extremely confident that the Answer addresses the Question, the
    Answer is highly accurate, relevant, and effectively addresses the Question in
    its entirety.'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '5: 你非常自信答案能够回答问题，答案极其准确、相关，并有效地全面解答了问题。'
- en: ''
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The output should strictly use the following template: Explanation: [provide
    a brief reasoning you used to derive the rating Score] and then write ‘Score:
    <rating>’ on the last line.'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '输出应严格使用以下模板：解释：[提供你用来推导评分的简短理由]，然后在最后一行写上“评分: <rating>”。'
- en: '[**G-Eval**](https://docs.confident-ai.com/docs/metrics-llm-evals) (from the
    DeepEval package) is a method that uses CoT to automatically develop multi-step
    criteria for assessing the quality of a given response. In the G-Eval paper (Liu
    et al.), this technique was found to correlate with Human Judgement on several
    benchmark datasets. Quality can be measured in various ways specified as a LLM
    prompt, here we specify it should be assessed based on the factual correctness
    of the response. Here is the criteria that was used for the G-Eval evaluation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[**G-Eval**](https://docs.confident-ai.com/docs/metrics-llm-evals)（来自DeepEval包）是一种使用CoT（链式推理）自动制定多步骤标准来评估给定响应质量的方法。在G-Eval论文（Liu等人）中，发现该技术与多个基准数据集上的人工判断具有相关性。质量可以通过多种方式进行衡量，这些方式在LLM提示中有所规定，在这里我们指定应根据响应的事实正确性进行评估。以下是用于G-Eval评估的标准：'
- en: '*Determine whether the output is factually correct given the context.*'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*判断输出在给定上下文下是否事实正确。*'
- en: '[**Hallucination Metric**](https://docs.confident-ai.com/docs/metrics-hallucination)
    (from the DeepEval package) estimates the likelihood of hallucination as the degree
    to which the LLM response contradicts/disagrees with the context, as assessed
    by another LLM.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[**幻觉度量**](https://docs.confident-ai.com/docs/metrics-hallucination)（来自DeepEval包）估计幻觉的可能性，即LLM响应与上下文的矛盾/不一致的程度，评估由另一个LLM进行。'
- en: '[**RAGAS**](https://docs.ragas.io/en/stable/) is a RAG-specific, LLM-powered
    evaluation suite that provides various scores which can be used to detect hallucination.
    We consider each of the following RAGAS scores, which are produced by using LLMs
    to estimate the requisite quantities:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[**RAGAS**](https://docs.ragas.io/en/stable/)是一个专为RAG（检索增强生成）设计的、由LLM驱动的评估工具套件，提供多种评分，可以用来检测幻觉现象。我们会考虑以下每一项RAGAS评分，这些评分通过使用LLM估算所需的数量来生成：'
- en: '[Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html)
    — The fraction of claims in the answer that are supported by the provided context.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[忠实度](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html) —
    答案中由提供的上下文支持的陈述的比例。'
- en: '[Answer Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html)
    is the mean cosine similarity of the vector representation to the original question
    with the vector representations of three LLM-generated questions from the answer.
    Vector representations here are embeddings from the `BAAI/bge-base-en encoder`.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[答案相关性](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html)是指答案中的三个LLM生成问题的向量表示与原始问题向量表示的平均余弦相似度。这里的向量表示是来自`BAAI/bge-base-en编码器`的嵌入向量。'
- en: '[Context Utilization](https://docs.ragas.io/en/latest/concepts/metrics/context_utilization.html)
    measures to what extent the context was relied on in the LLM response.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[上下文利用率](https://docs.ragas.io/en/latest/concepts/metrics/context_utilization.html)衡量在
    LLM（大语言模型）响应中对上下文的依赖程度。'
- en: '[**Trustworthy Language Model (TLM)**](https://cleanlab.ai/blog/trustworthy-language-model/)
    is a model uncertainty-estimation technique that evaluates the trustworthiness
    of LLM responses. It uses a combination of self-reflection, consistency across
    multiple sampled responses, and probabilistic measures to identify errors, contradictions
    and hallucinations. Here is the prompt template used to prompt TLM:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[**可信语言模型（TLM）**](https://cleanlab.ai/blog/trustworthy-language-model/)是一种模型不确定性评估技术，用于评估LLM回应的可信度。它通过自我反思、多次采样回应的一致性和概率度量的组合来识别错误、矛盾和幻觉。以下是用于提示TLM的提示模板：'
- en: '*Answer the QUESTION using information only from'
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*仅使用以下信息回答问题'
- en: 'CONTEXT: {context}'
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'CONTEXT: {context}'
- en: 'QUESTION: {question}*'
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题：{question}*
- en: Evaluation Methodology
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估方法
- en: We will compare the hallucination detection methods stated above across 4 public
    Context-Question-Answer datasets spanning different RAG applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在4个公共的上下文-问题-回答数据集上比较上述幻觉检测方法，这些数据集涵盖了不同的RAG应用。
- en: For each user *question* in our benchmark, an existing retrieval system returns
    some relevant *context*. The user query and context are then input into a *generator*
    LLM (often along with an application-specific system prompt) in order to generate
    a response for the user. Each detection method takes in the {user query, retrieved
    context, LLM response} and returns a score between 0–1, indicating the likelihood
    of hallucination.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们基准中的每个用户*问题*，现有的检索系统会返回一些相关的*上下文*。然后，用户查询和上下文会输入到一个*生成器* LLM（通常还会包含特定应用的系统提示）中，以生成用户的回答。每种检测方法都会输入{用户查询，检索到的上下文，LLM回应}，并返回一个介于0和1之间的分数，表示幻觉的可能性。
- en: To evaluate these hallucination detectors, we consider how reliably these scores
    take lower values when the LLM responses are incorrect vs. being correct. In each
    of our benchmarks, there exist ground-truth annotations regarding the correctness
    of each LLM response, which we solely reserve for evaluation purposes. We evaluate
    hallucination detectors based on **AUROC**, defined as the probability that their
    score will be lower for an example drawn from the subset where the LLM responded
    incorrectly than for one drawn from the subset where the LLM responded correctly.
    Detectors with greater AUROC values can be used to **catch RAG errors in your
    production system with greater precision/recall**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估这些幻觉检测器，我们考虑当LLM的回答错误与正确时，这些分数在何种程度上可靠地取较低值。在我们的每个基准测试中，都有关于每个LLM响应正确性的真实标注，这些标注仅供评估使用。我们基于**AUROC**来评估幻觉检测器，AUROC被定义为在从LLM错误响应子集抽取的示例中，其分数低于从LLM正确响应子集抽取的示例的概率。AUROC值较大的检测器可用于**更精确/更高召回率地捕捉生产系统中的RAG错误**。
- en: All of the considered hallucination detection methods are themselves powered
    by a LLM. For fair comparison, we fix this LLM model to be `gpt-4o-mini` across
    all of the methods.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所有考虑的幻觉检测方法本身都由LLM提供支持。为了公平比较，我们将所有方法中的LLM模型固定为`gpt-4o-mini`。
- en: Benchmark Results
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准结果
- en: We describe each benchmark dataset and the corresponding results below. These
    datasets stem from the popular [HaluBench](https://huggingface.co/datasets/PatronusAI/HaluBench)
    benchmark suite (we do not include the other two datasets from this suite, as
    we discovered significant errors in their ground truth annotations).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下文描述每个基准数据集及其相应结果。这些数据集来自流行的[HaluBench](https://huggingface.co/datasets/PatronusAI/HaluBench)基准套件（我们没有包括该套件中的其他两个数据集，因为我们发现它们的真实标注存在重大错误）。
- en: PubMedQA
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PubMedQA
- en: '[PubMedQA](https://huggingface.co/datasets/PatronusAI/HaluBench/viewer/default/test?f%5Bsource_ds%5D%5Bvalue%5D=%27pubmedQA%27)
    is a biomedical Q&A dataset based on PubMed abstracts. Each instance in the dataset
    contains a passage from a PubMed (medical publication) abstract, a question derived
    from passage, for example: `Is a 9-month treatment sufficient in tuberculous enterocolitis?`,
    and a generated answer.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[PubMedQA](https://huggingface.co/datasets/PatronusAI/HaluBench/viewer/default/test?f%5Bsource_ds%5D%5Bvalue%5D=%27pubmedQA%27)是一个基于PubMed摘要的生物医学问答数据集。数据集中的每个实例包含一段来自PubMed（医学出版物）摘要的文本，一个基于该段文本提出的问题，例如：`9个月的治疗是否足够治疗结核性肠炎？`，以及一个生成的答案。'
- en: '![](../Images/096854c297c06c74338c7090ef3bf870.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/096854c297c06c74338c7090ef3bf870.png)'
- en: ROC Curve for PubMedQA Dataset
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: PubMedQA数据集的ROC曲线
- en: In this benchmark, TLM is the most effective method for discerning hallucinations,
    followed by the Hallucination Metric, Self-Evaluation and RAGAS Faithfulness.
    Of the latter three methods, RAGAS Faithfulness and the Hallucination Metric were
    more effective for catching incorrect answers with high precision (RAGAS Faithfulness
    had an average precision of `0.762`, Hallucination Metric had an average precision
    of `0.761`, and Self-Evaluation had an average precision of`0.702`).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个基准测试中，TLM是最有效的幻觉识别方法，其次是Hallucination Metric、Self-Evaluation和RAGAS Faithfulness。在这三种方法中，RAGAS
    Faithfulness和Hallucination Metric在高精度地捕捉错误答案方面更为有效（RAGAS Faithfulness的平均精度为`0.762`，Hallucination
    Metric的平均精度为`0.761`，Self-Evaluation的平均精度为`0.702`）。
- en: DROP
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DROP
- en: '[DROP](https://huggingface.co/datasets/PatronusAI/HaluBench/viewer/default/test?f%5Bsource_ds%5D%5Bvalue%5D=%27DROP%27),
    or “Discrete Reasoning Over Paragraphs”, is an advanced Q&A dataset based on Wikipedia
    articles. DROP is difficult in that the questions require reasoning over context
    in the articles as opposed to simply extracting facts. For example, given context
    containing a Wikipedia passage describing touchdowns in a Seahawks vs. 49ers Football
    game, a sample question is: `How many touchdown runs measured 5-yards or less
    in total yards?`, requiring the LLM to read each touchdown run and then compare
    the length against the 5-yard requirement.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[DROP](https://huggingface.co/datasets/PatronusAI/HaluBench/viewer/default/test?f%5Bsource_ds%5D%5Bvalue%5D=%27DROP%27)，即“段落上的离散推理”，是一个基于维基百科文章的高级问答数据集。DROP的难度在于问题要求对文章中的上下文进行推理，而不仅仅是提取事实。例如，给定一个描述“海鹰队与49人队”足球比赛中达阵的维基百科段落，问题可能是：`有多少次达阵跑的总距离为5码或更短？`
    这要求LLM读取每次达阵的跑动并将长度与5码的要求进行比较。'
- en: '![](../Images/a5a1e1b1fa00095774d16c2a1066956d.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5a1e1b1fa00095774d16c2a1066956d.png)'
- en: ROC Curve for DROP Dataset
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: DROP数据集的ROC曲线
- en: Most methods faced challenges in detecting hallucinations in this DROP dataset
    due to the complexity of the reasoning required. TLM emerges as the most effective
    method for this benchmark, followed by Self-Evaluation and RAGAS Faithfulness.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DROP数据集需要复杂的推理，大多数方法在检测幻觉时面临挑战。TLM在这个基准测试中表现最为有效，其次是Self-Evaluation和RAGAS
    Faithfulness。
- en: COVID-QA
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: COVID-QA
- en: '[COVID-QA](https://huggingface.co/datasets/PatronusAI/HaluBench/viewer/default/test?f%5Bsource_ds%5D%5Bvalue%5D=%27covidQA%27)
    is a Q&A dataset based on scientific articles related to COVID-19\. Each instance
    in the dataset includes a scientific passage related to COVID-19 and a question
    derived from the passage, for example: `How much similarity the SARS-COV-2 genome
    sequence has with SARS-COV?`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[COVID-QA](https://huggingface.co/datasets/PatronusAI/HaluBench/viewer/default/test?f%5Bsource_ds%5D%5Bvalue%5D=%27covidQA%27)是一个基于与COVID-19相关的科学文章的问答数据集。数据集中的每个实例包括一段与COVID-19相关的科学内容和一个基于该内容提出的问题，例如：`SARS-COV-2基因组序列与SARS-COV的相似性有多少？`'
- en: Compared to DROP, this is a simpler dataset as it only requires basic synthesis
    of information from the passage to answer more straightforward questions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与DROP相比，这是一个更简单的数据集，因为它只需要从文章中提取基本信息来回答更直接的问题。
- en: '![](../Images/134cf33f02394565be73a184f1541a97.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/134cf33f02394565be73a184f1541a97.png)'
- en: ROC Curve for COVID-QA Dataset
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: COVID-QA数据集的ROC曲线
- en: In the COVID-QA dataset, TLM and RAGAS Faithfulness both exhibited strong performance
    in detecting hallucinations. Self-Evaluation also performed well, however other
    methods, including RAGAS Answer Relevancy, G-Eval, and the Hallucination Metric,
    had mixed results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在COVID-QA数据集中，TLM和RAGAS Faithfulness在检测幻觉方面表现强劲。Self-Evaluation也表现良好，但其他方法，包括RAGAS
    Answer Relevancy、G-Eval和Hallucination Metric，结果不一。
- en: FinanceBench
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FinanceBench
- en: '[FinanceBench](https://huggingface.co/datasets/PatronusAI/HaluBench/viewer/default/test?f%5Bsource_ds%5D%5Bvalue%5D=%27FinanceBench%27)
    is a dataset containing information about public financial statements and publicly
    traded companies. Each instance in the dataset contains a large retrieved context
    of plaintext financial information, a question regarding that information, for
    example: `What is FY2015 net working capital for Kraft Heinz?`, and a numeric
    answer like: `$2850.00`.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[FinanceBench](https://huggingface.co/datasets/PatronusAI/HaluBench/viewer/default/test?f%5Bsource_ds%5D%5Bvalue%5D=%27FinanceBench%27)是一个包含有关公共财务报表和上市公司信息的数据集。数据集中的每个实例都包含大量提取的纯文本财务信息、与该信息相关的问题，例如：`Kraft
    Heinz 2015财年的净营运资本是多少？`，以及像这样的数字答案：`$2850.00`。'
- en: '![](../Images/df7119c2bfbed1f2c9ef34e37d2bd304.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df7119c2bfbed1f2c9ef34e37d2bd304.png)'
- en: ROC Curve for FinanceBench Dataset
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: FinanceBench数据集的ROC曲线
- en: For this benchmark, TLM was the most effective in identifying hallucinations,
    followed closely by Self-Evaluation. Most other methods struggled to provide significant
    improvements over random guessing, highlighting the challenges in this dataset
    that contains large amounts of context and numerical data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个基准测试中，TLM 在识别幻觉方面最为有效，其次是自我评估。其他大多数方法在提供比随机猜测更显著的改进方面表现不佳，这突显了该数据集中的挑战，数据集包含大量的上下文和数值数据。
- en: Discussion
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Our evaluation of hallucination detection methods across various RAG benchmarks
    reveals the following key insights:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对各种 RAG 基准测试中幻觉检测方法的评估揭示了以下关键见解：
- en: '**Trustworthy Language Model (TLM)** consistently performed well, showing strong
    capabilities in identifying hallucinations through a blend of self-reflection,
    consistency, and probabilistic measures.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可信语言模型（TLM）**始终表现优异，通过自我反思、一致性和概率性度量，展现了在识别幻觉方面的强大能力。'
- en: '**Self-Evaluation** showed consistent effectiveness in detecting hallucinations,
    particularly effective in simpler contexts where the LLM’s self-assessment can
    be accurately gauged. While it may not always match the performance of TLM, it
    remains a straightforward and useful technique for evaluating response quality.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自我评估**在检测幻觉方面表现出一致的有效性，尤其在较简单的上下文中，LLM 的自我评估可以准确衡量。虽然它的表现可能不总是与 TLM 相匹配，但它仍然是评估响应质量的一个直接而有用的技术。'
- en: '**RAGAS Faithfulness** demonstrated robust performance in datasets where the
    accuracy of responses is closely linked to the retrieved context, such as in PubMedQA
    and COVID-QA. It is particularly effective in identifying when claims in the answer
    are not supported by the provided context. However, its effectiveness was variable
    depending on the complexity of the questions. By default, RAGAS uses `gpt-3.5-turbo-16k`
    for generation and `gpt-4` for the critic LLM, which produced worse results than
    the RAGAS with `gpt-4o-mini` results we reported here. RAGAS failed to run on
    certain examples in our benchmark due to its sentence parsing logic, which we
    fixed by appending a period (.) to the end of answers that did not end in punctuation.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**RAGAS 忠诚度**在回答准确性与检索上下文密切相关的数据集（如 PubMedQA 和 COVID-QA）中展现了强大的性能。它特别擅长识别答案中的声明是否得到了提供的上下文支持。然而，其效果会根据问题的复杂性有所不同。默认情况下，RAGAS
    使用 `gpt-3.5-turbo-16k` 进行生成，使用 `gpt-4` 作为评论 LLM，这比我们在此报告的使用 `gpt-4o-mini` 结果要差。由于句子解析逻辑的问题，RAGAS
    无法在我们基准测试中的某些例子上运行，我们通过在没有标点符号的答案末尾添加句号（.）来解决此问题。'
- en: '**Other Methods** like G-Eval and Hallucination Metric had mixed results, and
    exhibited varied performance across different benchmarks. Their performance was
    less consistent, indicating that further refinement and adaptation may be needed.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**其他方法**，如 G-Eval 和幻觉度量，结果参差不齐，在不同的基准测试中表现差异较大。它们的表现不够稳定，表明可能需要进一步的完善和适应。'
- en: Overall, TLM, RAGAS Faithfulness, and Self-Evaluation stand out as more reliable
    methods to detect hallucinations in RAG applications. For high-stakes applications,
    combining these methods could offer the best results. Future work could explore
    hybrid approaches and targeted refinements to better conduct hallucination detection
    with specific use cases. By integrating these methods, RAG systems can achieve
    greater reliability and ensure more accurate and trustworthy responses.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，TLM、RAGAS 忠诚度和自我评估在 RAG 应用中是更可靠的幻觉检测方法。对于高风险应用，结合这些方法可能会提供最佳的结果。未来的工作可以探索混合方法和有针对性的改进，以便在特定用例中更好地进行幻觉检测。通过整合这些方法，RAG
    系统可以实现更高的可靠性，确保更加准确和可信的响应。
- en: '*Unless otherwise noted, all images are by the author.*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者提供。*'
