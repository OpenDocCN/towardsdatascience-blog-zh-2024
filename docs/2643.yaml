- en: 'Data Leakage in Preprocessing, Explained: A Visual Guide with Code Examples'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7?source=collection_archive---------2-----------------------#2024-10-30](https://towardsdatascience.com/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7?source=collection_archive---------2-----------------------#2024-10-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DATA PREPROCESSING
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 10 sneaky ways your preprocessing pipeline leaks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--33cbf07507b7--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--33cbf07507b7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--33cbf07507b7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--33cbf07507b7--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--33cbf07507b7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--33cbf07507b7--------------------------------)
    Â·14 min readÂ·Oct 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '`â›³ï¸ More [DATA PREPROCESSING](https://medium.com/@samybaladram/list/data-preprocessing-17a2c49b44e4),
    explained: Â· [Missing Value Imputation](/missing-value-imputation-explained-a-visual-guide-with-code-examples-for-beginners-93e0726284eb)
    Â· [Categorical Encoding](/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae)
    Â· [Data Scaling](/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb)
    Â· [Discretization](/discretization-explained-a-visual-guide-with-code-examples-for-beginners-f056af9102fa?gi=c1bf25229f86)
    Â· [Oversampling & Undersampling](/oversampling-and-undersampling-explained-a-visual-guide-with-mini-2d-dataset-1155577d3091)
    â–¶ [Data Leakage in Preprocessing](/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'In my experience teaching machine learning, students often come to me with
    this same problem: â€œMy model was performing great â€” over 90% accuracy! But when
    I submitted it for testing on the hidden dataset, it is not as good now. What
    went wrong?â€ This situation almost always points to data leakage.'
  prefs: []
  type: TYPE_NORMAL
- en: Data leakage happens when information from test data sneaks (or leaks) into
    your training data during data preparation steps. This often happens during routine
    data processing tasks without you noticing it. When this happens, the model **learns
    from test data it wasnâ€™t supposed to see**, making the test results misleading.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s look at common preprocessing steps and see exactly what happens when data
    leaksâ€” hopefully, you can avoid these â€œpipeline issuesâ€ in your own projects.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9b656af3dc4c870b634e61dfd631791.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data leakage is a common problem in machine learning that occurs when data thatâ€™s
    not supposed to be seen by a model (like test data or future data) is accidentally
    used to train the model. This can lead to the model overfitting and not performing
    well on new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, letâ€™s focus on data leakage during the following data preprocessing steps.
    Further, weâ€™ll also see these steps with specific `scikit-learn` preprocessing
    method names and we will see the code examples at the very end of this article.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04fe8556bee3eb3b7108de42fc432543.png)'
  prefs: []
  type: TYPE_IMG
- en: Missing Value Imputation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with real data, you often run into missing values. Rather than
    removing these incomplete data points, we can fill them in with reasonable estimates.
    This helps us keep more data for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simple ways to fill missing values include:'
  prefs: []
  type: TYPE_NORMAL
- en: Using `SimpleImputer(strategy='mean')` or `SimpleImputer(strategy='median')`
    to fill with the average or middle value from that column
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `KNNImputer()` to look at similar data points and use their values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `SimpleImputer(strategy='ffill')` or `SimpleImputer(strategy='bfill')`
    to fill with the value that comes before or after in the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `SimpleImputer(strategy='constant', fill_value=value)` to replace all
    missing spots with the same number or text
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process is called imputation, and while itâ€™s useful, we need to be careful
    about how we calculate these replacement values to avoid data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Leakage Case: Simple Imputation (Mean)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you fill missing values using the mean from all your data, the mean value
    itself contains information from both training and test sets. This combined mean
    value is different from what you would get using just the training data. Since
    this different mean value goes into your training data, your model learns from
    test data information it wasnâ€™t supposed to see. To summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš¨ **THE ISSUE** Computing mean values using complete dataset
  prefs: []
  type: TYPE_NORMAL
- en: âŒ **What Weâ€™re Doing Wrong** Calculating fill values using both training and
    test set statistics
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ’¥ **The Consequence** Training data contains averaged values influenced by test
    data
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6efef35923d84701a67951a2c693a10.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean imputation leakage occurs when filling missing values using the average
    (4) calculated from all data rows, instead of correctly using only the training
    dataâ€™s average (3), leading to wrong fill values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Leakage Case: KNN Imputation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you fill missing values using KNN on all your data, the algorithm finds
    similar data points from both training and test sets. The replacement values it
    creates are based on these nearby points, which means test set values directly
    influence what goes into your training data. Since KNN looks at actual nearby
    values, this mixing of training and test information is even more direct than
    using simple mean imputation. To summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš¨ **THE ISSUE** Finding neighbors across complete dataset
  prefs: []
  type: TYPE_NORMAL
- en: âŒ **What Weâ€™re Doing Wrong** Using test set samples as potential neighbors for
    imputation
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ’¥ **The Consequence** Missing values filled using direct test set information
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9fafdbf52962bef41e3fb5bec8e2325c.png)'
  prefs: []
  type: TYPE_IMG
- en: KNN imputation leakage occurs when finding nearest neighbors using both training
    and test data (resulting in values 3.5 and 4.5), instead of correctly using only
    training data patterns to impute missing values (resulting in values 6 and 6).
  prefs: []
  type: TYPE_NORMAL
- en: Categorical Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some data comes as categories instead of numbers â€” like colors, names, or types.
    Since models can only work with numbers, we need to convert these categories into
    numerical values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common ways to convert categories include:'
  prefs: []
  type: TYPE_NORMAL
- en: Using `OneHotEncoder()` to create separate columns of 1s and 0s for each category
    (also known as dummy variables)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `OrdinalEncoder()` or `LabelEncoder()` to assign each category a number
    (like 1, 2, 3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `OrdinalEncoder(categories=[ordered_list])` with custom category orders
    to reflect natural hierarchy (like small=1, medium=2, large=3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `TargetEncoder()` to convert categories to numbers based on their relationship
    with the target variable we're trying to predict
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The way we convert these categories can affect how well our model learns, and
    we need to be careful about using information from test data during this process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Leakage Case: **Target Encoding**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you convert categorical values using target encoding on all your data,
    the encoded values are calculated using the target information from both training
    and test sets. The numbers that replace each category are averages of target values
    that include test data. This means your training data gets assigned values that
    already contain information about the target values from the test set that it
    wasnâ€™t supposed to know about. To summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš¨ **THE ISSUE** Computing category means using complete dataset
  prefs: []
  type: TYPE_NORMAL
- en: âŒ **What Weâ€™re Doing Wrong** Calculating category replacements using all target
    values
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ’¥ **The Consequence** Training features contain future target information
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57834471ff029388e93763c630f6e268.png)'
  prefs: []
  type: TYPE_IMG
- en: Target encoding leakage occurs when replacing categories with their average
    target values (A=3, B=4, C=2) using all the data, instead of correctly using only
    training data averages (A=2, B=5, C=1), leading to wrong category values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Leakage Case: One-Hot Encoding'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you convert categories into binary columns using all your data and then
    select which columns to keep, the selection is based on patterns found in both
    training and test sets. The decision to keep or remove certain binary columns
    is influenced by how well they predict the target in the test data, not just the
    training data. This means your chosen set of columns is partially determined by
    test set relationships you werenâ€™t supposed to use. To summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš¨ **THE ISSUE** Determining categories from complete dataset
  prefs: []
  type: TYPE_NORMAL
- en: âŒ **What Weâ€™re Doing Wrong** Creating binary columns based on all unique values
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ’¥ **The Consequence** Feature selection influenced by test set patterns
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de6c9ace481f7e7d863aebe762b2940a.png)'
  prefs: []
  type: TYPE_IMG
- en: One-hot encoding leakage occurs when creating category columns using all unique
    values (A,B,C,D) from the full dataset, instead of correctly using only categories
    present in training data (A,B,C), leading to wrong encoding patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Data Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Different features in your data often have very different ranges â€” some might
    be in thousands while others are tiny decimals. We adjust these ranges so all
    features have similar scales, which helps models work better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common ways to adjust scales include:'
  prefs: []
  type: TYPE_NORMAL
- en: Using `StandardScaler()` to make values center around 0 with most falling between
    -1 and 1 (mean=0, variance=1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `MinMaxScaler()` to squeeze all values between 0 and 1, or `MinMaxScaler(feature_range=(min,
    max))` for a custom range
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `FunctionTransformer(np.log1p)` or `PowerTransformer(method='box-cox')`
    to handle very large numbers and make distributions more normal
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `RobustScaler()` to adjust scales using statistics that aren't affected
    by outliers (using quartiles instead of mean/variance)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While scaling helps models compare different features fairly, we need to calculate
    these adjustments using only training data to avoid leakage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Leakage Case: Standard Scaling'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you standardize features using all your data, the average and spread values
    used in the calculation come from both training and test sets. These values are
    different from what you would get using just the training data. This means every
    standardized value in your training data is adjusted using information about the
    distribution of values in your test set. To summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš¨ **THE ISSUE** Computing statistics using complete dataset
  prefs: []
  type: TYPE_NORMAL
- en: âŒ **What Weâ€™re Doing Wrong** Calculating mean and standard deviation using all
    values
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ’¥ **The Consequence** Training features scaled using test set distribution
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07e43284950a668a081b2a90dd6c0512.png)'
  prefs: []
  type: TYPE_IMG
- en: Standard scaling leakage occurs when using the full datasetâ€™s average (Î¼=0)
    and spread (Ïƒ=3) to normalize data, instead of correctly using only training dataâ€™s
    statistics (Î¼=2, Ïƒ=2), leading to wrong standardized values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Leakage Case: **Min-Max Scaling**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you scale features using minimum and maximum values from all your data,
    these boundary values might come from your test set. The scaled values in your
    training data are calculated using these bounds, which could be different from
    what youâ€™d get using just training data. This means every scaled value in your
    training data is adjusted using the full range of values from your test set. To
    summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš¨ **THE ISSUE** Finding bounds using complete dataset
  prefs: []
  type: TYPE_NORMAL
- en: âŒ **What Weâ€™re Doing Wrong** Determining min/max values from all data points
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ’¥ **The Consequence** Training features normalized using test set ranges
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f904f4f392adbf5fa4711e4a90bfaa9c.png)'
  prefs: []
  type: TYPE_IMG
- en: Min-max scaling leakage occurs when using the full datasetâ€™s minimum (-5) and
    maximum (5) values to scale data, instead of correctly using only training dataâ€™s
    range (min=-1, max=5), leading to wrong scaling of values.
  prefs: []
  type: TYPE_NORMAL
- en: Discretization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes itâ€™s better to group numbers into categories rather than use exact
    values. This helps machine learning models to process and analyze the data more
    easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common ways to create these groups include:'
  prefs: []
  type: TYPE_NORMAL
- en: Using `KBinsDiscretizer(strategy='uniform')` to make each group cover the same
    size range of values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `KBinsDiscretizer(strategy='quantile')` to make each group contain the
    same number of data points
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `KBinsDiscretizer(strategy='kmeans')` to find natural groupings in the
    data using clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `QuantileTransformer(n_quantiles=n, output_distribution='uniform')` to
    create groups based on percentiles in your data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While grouping values can help models find patterns better, the way we decide
    group boundaries needs to use only training data to avoid leakage.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Leakage Case: Equal Frequency Binning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you create bins with equal numbers of data points using all your data,
    the cutoff points between bins are determined using both training and test sets.
    These cutoff values are different from what youâ€™d get using just training data.
    This means when you assign data points to bins in your training data, youâ€™re using
    dividing points that were influenced by your test set values. To summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš¨ **THE ISSUE** Setting thresholds using complete dataset
  prefs: []
  type: TYPE_NORMAL
- en: âŒ **What Weâ€™re Doing Wrong** Determining bin boundaries using all data points
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ’¥ **The Consequence** Training data binned using test set distributions
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c350933b8e9d11b558836a45c596c4b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Equal frequency binning leakage occurs when setting bin cutoff points (-0.5,
    2.5) using all the data, instead of correctly using only training data to set
    boundaries (-0.5, 2.0), leading to wrong grouping of values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Leakage Case: Equal Width Binning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you create bins of equal size using all your data, the range used to determine
    bin widths comes from both training and test sets. This total range could be wider
    or narrower than what youâ€™d get using just training data. This means when you
    assign data points to bins in your training data, youâ€™re using bin boundaries
    that were calculated based on the full spread of your test set values. To summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš¨ **THE ISSUE** Calculating ranges using complete dataset
  prefs: []
  type: TYPE_NORMAL
- en: âŒ **What Weâ€™re Doing Wrong** Setting bin widths based on full data spread
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ’¥ **The Consequence** Training data binned using test set boundaries
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9942d147b1bda767bd112c2d616cec43.png)'
  prefs: []
  type: TYPE_IMG
- en: Equal width binning leakage occurs when splitting data into equal-size groups
    using the full datasetâ€™s range (-3 to 6), instead of correctly using only the
    training dataâ€™s range (-3 to 3), leading to wrong groupings.
  prefs: []
  type: TYPE_NORMAL
- en: Resampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When some categories in your data have many more examples than others, we can
    balance them using resampling techniques from `imblearn` by either creating new
    samples or removing existing ones. This helps models learn all categories fairly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common ways to add samples (Oversampling):'
  prefs: []
  type: TYPE_NORMAL
- en: Using `RandomOverSampler()` to make copies of existing examples from smaller
    categories
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `SMOTE()` to create new, synthetic examples for smaller categories using
    interpolation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `ADASYN()` to create more examples in areas where the model struggles
    most, focusing on decision boundaries
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Common ways to remove samples (Undersampling):'
  prefs: []
  type: TYPE_NORMAL
- en: Using `RandomUnderSampler()` to randomly remove examples from larger categories
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `NearMiss(version=1)` or `NearMiss(version=2)` to remove examples from
    larger categories based on their distance to smaller categories
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `TomekLinks()` or `EditedNearestNeighbours()` to carefully select which
    examples to remove based on their similarity to other categories
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While balancing your data helps models learn better, the process of creating
    or removing samples should only use information from training data to avoid leakage.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Leakage Case: Oversampling (SMOTE)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you create synthetic data points using SMOTE on all your data, the algorithm
    picks nearby points from both training and test sets to create new samples. These
    new points are created by mixing values from test set samples with training data.
    This means your training data gets new samples that were directly created using
    information from your test set values. To summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš¨ **THE ISSUE** Generating samples using complete dataset
  prefs: []
  type: TYPE_NORMAL
- en: âŒ **What Weâ€™re Doing Wrong** Creating synthetic points using test set neighbors
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ’¥ **The Consequence** Training augmented with test-influenced samples
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9faf04e494faa2e8a948243bc52ae55b.png)'
  prefs: []
  type: TYPE_IMG
- en: Oversampling leakage occurs when duplicating data points based on class counts
    from the entire dataset (AÃ—4, BÃ—3, CÃ—2), instead of correctly using only the training
    data (AÃ—1, BÃ—2, CÃ—2) to decide how many times to duplicate each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Leakage Case: Undersampling (Tomek Links)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you remove data points using Tomek Links on all your data, the algorithm
    finds pairs of points from both training and test sets that are closest to each
    other but have different labels. The decision to remove points from your training
    data is based on how close they are to test set points. This means your final
    training data is shaped by its relationship with test set values. To summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš¨ **THE ISSUE** Removing samples using complete dataset
  prefs: []
  type: TYPE_NORMAL
- en: âŒ **What Weâ€™re Doing Wrong** Identifying pairs using test set relationships
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ’¥ **The Consequence** Training reduced based on test set patterns
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d56e90a957fc6d7a249a9d990da10385.png)'
  prefs: []
  type: TYPE_IMG
- en: Undersampling leakage occurs when removing data points based on class ratios
    from the entire dataset (AÃ—4, BÃ—3, CÃ—2), instead of correctly using only the training
    data (AÃ—1, BÃ—2, CÃ—2) to decide how many samples to keep from each class.
  prefs: []
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When preprocessing data, you need to keep training and test data completely
    separate. Any time you use information from all your data to transform values
    â€” whether youâ€™re filling missing values, converting categories to numbers, scaling
    features, creating bins, or balancing classes â€” you risk mixing test data information
    into your training data. This makes your modelâ€™s test results unreliable because
    the model already learned from patterns it wasnâ€™t supposed to see.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution is simple: **always transform your training data first, save those
    calculations, and then apply them to your test data.**'
  prefs: []
  type: TYPE_NORMAL
- en: ğŸŒŸ Data Preprocessing + Classification (with Leakage) Code Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us see how leakage could happen in predicting a simple golf play dataset.
    This is the bad example and should not be followed. Just for demonstration and
    education purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The code above is using `ColumnTransformer`, which is a utility in scikit-learn
    that allows us to apply different preprocessing steps to different columns in
    a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hereâ€™s a breakdown of the preprocessing strategy for each column in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**Temperature**`**:'
  prefs: []
  type: TYPE_NORMAL
- en: -** Mean imputation to handle any missing values
  prefs: []
  type: TYPE_NORMAL
- en: '- Standard scaling to normalize the values (mean=0, std=1)'
  prefs: []
  type: TYPE_NORMAL
- en: '- Equal-width discretization into 4 bins, meaning continuous values are categorized
    into 4 equal-width intervals'
  prefs: []
  type: TYPE_NORMAL
- en: '`**Humidity**`**:'
  prefs: []
  type: TYPE_NORMAL
- en: '-** Same strategy as Temperature: Mean imputation â†’ Standard scaling â†’ Equal-width
    discretization (4 bins)'
  prefs: []
  type: TYPE_NORMAL
- en: '`**Outlook**`**(categorical):**'
  prefs: []
  type: TYPE_NORMAL
- en: '- Ordinal encoding: converts categorical values into numerical ones'
  prefs: []
  type: TYPE_NORMAL
- en: '- Unknown values are handled by setting them to -1'
  prefs: []
  type: TYPE_NORMAL
- en: '`**Wind**` **(binary):**'
  prefs: []
  type: TYPE_NORMAL
- en: '- Constant imputation with False for missing values'
  prefs: []
  type: TYPE_NORMAL
- en: '- Standard scaling to normalize the 0/1 values'
  prefs: []
  type: TYPE_NORMAL
- en: '`**Play**` **(target):**'
  prefs: []
  type: TYPE_NORMAL
- en: '- Label encoding to convert Yes/No to 1/0'
  prefs: []
  type: TYPE_NORMAL
- en: '- SMOTE applied after preprocessing to balance classes by creating synthetic
    examples of the minority class'
  prefs: []
  type: TYPE_NORMAL
- en: '- A simple decision tree is used to predict the target'
  prefs: []
  type: TYPE_NORMAL
- en: The entire pipeline demonstrates data leakage because **all transformations
    see the entire dataset during fitting**, which would be inappropriate in a real
    machine learning scenario where we need to keep test data completely separate
    from the training process.
  prefs: []
  type: TYPE_NORMAL
- en: This approach will also likely show artificially higher test accuracy because
    the test data characteristics were used in the preprocessing steps!
  prefs: []
  type: TYPE_NORMAL
- en: ğŸŒŸ Data Preprocessing + Classification (without leakage) Code Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hereâ€™s the version without data leakage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Key differences from the leakage version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Split data first, before any processing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All transformations (preprocessing, SMOTE) are inside the pipeline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pipeline ensures:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '- Preprocessing parameters learned only from training data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- SMOTE applies only to training data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Test data remains completely unseen until prediction'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This approach gives more realistic performance estimates as it maintains proper
    separation between training and test data.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 , scikit-learn 1.5, and imblearn 0.12\. While the
    concepts discussed are generally applicable, specific code implementations may
    vary slightly with different versions
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¿ğ™–ğ™©ğ™– ğ™‹ğ™§ğ™šğ™¥ğ™§ğ™¤ğ™˜ğ™šğ™¨ğ™¨ğ™ğ™£ğ™œ ğ™¢ğ™šğ™©ğ™ğ™¤ğ™™ğ™¨ ğ™ğ™šğ™§ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----33cbf07507b7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Data Preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/data-preprocessing-17a2c49b44e4?source=post_page-----33cbf07507b7--------------------------------)6
    stories![](../Images/f7ead0fb9a8dc2823d7a43d67a1c6932.png)![Cartoon illustration
    of two figures embracing, with letters â€˜Aâ€™, â€˜Bâ€™, â€˜Câ€™ and numbers â€˜1â€™, â€˜2â€™, â€˜3â€™
    floating around them. A pink heart hovers above, symbolizing affection. The background
    is a pixelated pattern of blue and green squares, representing data or encoding.
    This image metaphorically depicts the concept of encoding categorical data, where
    categories (ABC) are transformed into numerical representations (123).](../Images/72bb3a287a9ca4c5e7a3871e234bcc4b.png)![A
    cartoon illustration representing data scaling in machine learning. A tall woman
    (representing a numerical feature with a large range) is shown shrinking into
    a child (representing the same feature after scaling to a smaller range). A red
    arrow indicates the shrinking process, and yellow sparkles around the child signify
    the positive impact of scaling.](../Images/d261b2c52a3cafe266d1962d4dbabdbd.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----33cbf07507b7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----33cbf07507b7--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----33cbf07507b7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----33cbf07507b7--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
  prefs: []
  type: TYPE_NORMAL
