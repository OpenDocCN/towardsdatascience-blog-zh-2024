- en: 'Data Leakage in Preprocessing, Explained: A Visual Guide with Code Examples'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据泄露在预处理中的解释：带代码示例的视觉指南
- en: 原文：[https://towardsdatascience.com/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7?source=collection_archive---------2-----------------------#2024-10-30](https://towardsdatascience.com/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7?source=collection_archive---------2-----------------------#2024-10-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7?source=collection_archive---------2-----------------------#2024-10-30](https://towardsdatascience.com/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7?source=collection_archive---------2-----------------------#2024-10-30)
- en: DATA PREPROCESSING
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 10 sneaky ways your preprocessing pipeline leaks
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10种隐蔽的预处理管道泄露方式
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--33cbf07507b7--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--33cbf07507b7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--33cbf07507b7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--33cbf07507b7--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--33cbf07507b7--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samybaladram?source=post_page---byline--33cbf07507b7--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--33cbf07507b7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--33cbf07507b7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--33cbf07507b7--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--33cbf07507b7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--33cbf07507b7--------------------------------)
    ·14 min read·Oct 30, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--33cbf07507b7--------------------------------)
    ·阅读时间14分钟·2024年10月30日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '`⛳️ More [DATA PREPROCESSING](https://medium.com/@samybaladram/list/data-preprocessing-17a2c49b44e4),
    explained: · [Missing Value Imputation](/missing-value-imputation-explained-a-visual-guide-with-code-examples-for-beginners-93e0726284eb)
    · [Categorical Encoding](/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae)
    · [Data Scaling](/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb)
    · [Discretization](/discretization-explained-a-visual-guide-with-code-examples-for-beginners-f056af9102fa?gi=c1bf25229f86)
    · [Oversampling & Undersampling](/oversampling-and-undersampling-explained-a-visual-guide-with-mini-2d-dataset-1155577d3091)
    ▶ [Data Leakage in Preprocessing](/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7)`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '`⛳️ 更多[数据预处理](https://medium.com/@samybaladram/list/data-preprocessing-17a2c49b44e4)解释：·
    [缺失值填补](/missing-value-imputation-explained-a-visual-guide-with-code-examples-for-beginners-93e0726284eb)
    · [类别编码](/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae)
    · [数据缩放](/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb)
    · [离散化](/discretization-explained-a-visual-guide-with-code-examples-for-beginners-f056af9102fa?gi=c1bf25229f86)
    · [过采样与欠采样](/oversampling-and-undersampling-explained-a-visual-guide-with-mini-2d-dataset-1155577d3091)
    ▶ [数据泄露在预处理中的应用](/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7)`'
- en: 'In my experience teaching machine learning, students often come to me with
    this same problem: “My model was performing great — over 90% accuracy! But when
    I submitted it for testing on the hidden dataset, it is not as good now. What
    went wrong?” This situation almost always points to data leakage.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我教授机器学习的经验中，学生们经常遇到这样的问题：“我的模型表现得很好——准确率超过90%！但是当我提交给隐藏数据集进行测试时，结果不如预期。哪里出了问题？”这种情况几乎总是指向数据泄露。
- en: Data leakage happens when information from test data sneaks (or leaks) into
    your training data during data preparation steps. This often happens during routine
    data processing tasks without you noticing it. When this happens, the model **learns
    from test data it wasn’t supposed to see**, making the test results misleading.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据泄露发生在测试数据的某些信息在数据准备步骤中悄悄地（或泄露）进入训练数据时。这种情况通常发生在常规的数据处理任务中，而你未曾察觉。当这种情况发生时，模型**从不该看到的测试数据中学习**，使得测试结果具有误导性。
- en: Let’s look at common preprocessing steps and see exactly what happens when data
    leaks— hopefully, you can avoid these “pipeline issues” in your own projects.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看常见的数据预处理步骤，并准确了解数据泄漏时会发生什么——希望你可以在自己的项目中避免这些“管道问题”。
- en: '![](../Images/f9b656af3dc4c870b634e61dfd631791.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9b656af3dc4c870b634e61dfd631791.png)'
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所有视觉图：作者使用 Canva Pro 创建。优化为移动端显示；在桌面端可能会显得过大。
- en: Definition
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义
- en: Data leakage is a common problem in machine learning that occurs when data that’s
    not supposed to be seen by a model (like test data or future data) is accidentally
    used to train the model. This can lead to the model overfitting and not performing
    well on new, unseen data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据泄漏是机器学习中一个常见的问题，它发生在不应该被模型看到的数据（例如测试数据或未来数据）被意外用来训练模型时。这可能导致模型过拟合，在新的、未见过的数据上表现不佳。
- en: Now, let’s focus on data leakage during the following data preprocessing steps.
    Further, we’ll also see these steps with specific `scikit-learn` preprocessing
    method names and we will see the code examples at the very end of this article.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们集中讨论以下数据预处理步骤中的数据泄漏问题。进一步，我们还将看到这些步骤对应的 `scikit-learn` 预处理方法名称，并且在文章的最后会看到代码示例。
- en: '![](../Images/04fe8556bee3eb3b7108de42fc432543.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04fe8556bee3eb3b7108de42fc432543.png)'
- en: Missing Value Imputation
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失值填充
- en: When working with real data, you often run into missing values. Rather than
    removing these incomplete data points, we can fill them in with reasonable estimates.
    This helps us keep more data for analysis.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理真实数据时，你经常会遇到缺失值。与其删除这些不完整的数据点，不如用合理的估计值来填充它们。这可以帮助我们保留更多的数据进行分析。
- en: 'Simple ways to fill missing values include:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 填充缺失值的简单方法包括：
- en: Using `SimpleImputer(strategy='mean')` or `SimpleImputer(strategy='median')`
    to fill with the average or middle value from that column
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `SimpleImputer(strategy='mean')` 或 `SimpleImputer(strategy='median')` 用该列的平均值或中值来填充
- en: Using `KNNImputer()` to look at similar data points and use their values
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `KNNImputer()` 查看相似数据点并使用它们的值
- en: Using `SimpleImputer(strategy='ffill')` or `SimpleImputer(strategy='bfill')`
    to fill with the value that comes before or after in the data
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `SimpleImputer(strategy='ffill')` 或 `SimpleImputer(strategy='bfill')` 用数据中前一个或后一个值来填充
- en: Using `SimpleImputer(strategy='constant', fill_value=value)` to replace all
    missing spots with the same number or text
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `SimpleImputer(strategy='constant', fill_value=value)` 用相同的数字或文本填充所有缺失的位置
- en: This process is called imputation, and while it’s useful, we need to be careful
    about how we calculate these replacement values to avoid data leakage.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为填充，虽然它很有用，但我们需要小心如何计算这些替代值，以避免数据泄漏。
- en: '**Data Leakage Case: Simple Imputation (Mean)**'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数据泄漏案例：简单填充（均值）**'
- en: 'When you fill missing values using the mean from all your data, the mean value
    itself contains information from both training and test sets. This combined mean
    value is different from what you would get using just the training data. Since
    this different mean value goes into your training data, your model learns from
    test data information it wasn’t supposed to see. To summarize:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用所有数据的均值来填充缺失值时，均值本身包含了来自训练集和测试集的信息。这个组合均值不同于仅使用训练数据计算的均值。由于这个不同的均值进入了训练数据，你的模型从它本不该看到的测试数据中学习。总结一下：
- en: 🚨 **THE ISSUE** Computing mean values using complete dataset
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 🚨 **问题所在** 使用完整数据集计算均值
- en: ❌ **What We’re Doing Wrong** Calculating fill values using both training and
    test set statistics
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ **我们做错了什么** 使用训练集和测试集的统计数据来计算填充值
- en: 💥 **The Consequence** Training data contains averaged values influenced by test
    data
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 💥 **后果** 训练数据包含受测试数据影响的平均值
- en: '![](../Images/d6efef35923d84701a67951a2c693a10.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6efef35923d84701a67951a2c693a10.png)'
- en: Mean imputation leakage occurs when filling missing values using the average
    (4) calculated from all data rows, instead of correctly using only the training
    data’s average (3), leading to wrong fill values.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 均值填充泄漏发生在使用从所有数据行计算得到的平均值（4）填充缺失值，而不是正确地只使用训练数据的平均值（3），导致错误的填充值。
- en: '**Data Leakage Case: KNN Imputation**'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数据泄漏案例：KNN 填充**'
- en: 'When you fill missing values using KNN on all your data, the algorithm finds
    similar data points from both training and test sets. The replacement values it
    creates are based on these nearby points, which means test set values directly
    influence what goes into your training data. Since KNN looks at actual nearby
    values, this mixing of training and test information is even more direct than
    using simple mean imputation. To summarize:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用KNN填充缺失值时，算法会从训练集和测试集找到相似的数据点。它创建的替代值是基于这些邻近点的，这意味着测试集的值直接影响训练数据中的内容。由于KNN会查看实际的邻近值，因此这种训练和测试信息的混合比简单的均值填充更为直接。总结来说：
- en: 🚨 **THE ISSUE** Finding neighbors across complete dataset
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 🚨 **问题** 在完整数据集上寻找邻居
- en: ❌ **What We’re Doing Wrong** Using test set samples as potential neighbors for
    imputation
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ **我们做错了什么** 使用测试集样本作为潜在的邻居进行填充
- en: 💥 **The Consequence** Missing values filled using direct test set information
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 💥 **后果** 使用直接的测试集信息填充缺失值
- en: '![](../Images/9fafdbf52962bef41e3fb5bec8e2325c.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fafdbf52962bef41e3fb5bec8e2325c.png)'
- en: KNN imputation leakage occurs when finding nearest neighbors using both training
    and test data (resulting in values 3.5 and 4.5), instead of correctly using only
    training data patterns to impute missing values (resulting in values 6 and 6).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: KNN填充泄漏发生在使用训练集和测试集数据共同寻找最近邻（生成3.5和4.5的值），而不是正确地只使用训练数据的模式来填充缺失值（生成6和6的值）。
- en: Categorical Encoding
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类别编码
- en: Some data comes as categories instead of numbers — like colors, names, or types.
    Since models can only work with numbers, we need to convert these categories into
    numerical values.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有些数据以类别的形式出现，而不是数字——比如颜色、名称或类型。由于模型只能处理数字，我们需要将这些类别转换为数值。
- en: 'Common ways to convert categories include:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 转换类别的常见方法包括：
- en: Using `OneHotEncoder()` to create separate columns of 1s and 0s for each category
    (also known as dummy variables)
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`OneHotEncoder()`为每个类别创建单独的1和0的列（也叫虚拟变量）
- en: Using `OrdinalEncoder()` or `LabelEncoder()` to assign each category a number
    (like 1, 2, 3)
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`OrdinalEncoder()`或`LabelEncoder()`为每个类别分配一个数字（如1，2，3）
- en: Using `OrdinalEncoder(categories=[ordered_list])` with custom category orders
    to reflect natural hierarchy (like small=1, medium=2, large=3)
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`OrdinalEncoder(categories=[ordered_list])`与自定义类别顺序反映自然层级（如small=1，medium=2，large=3）
- en: Using `TargetEncoder()` to convert categories to numbers based on their relationship
    with the target variable we're trying to predict
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`TargetEncoder()`根据类别与我们试图预测的目标变量的关系将类别转换为数字
- en: The way we convert these categories can affect how well our model learns, and
    we need to be careful about using information from test data during this process.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们转换这些类别的方式会影响模型学习的效果，因此在这个过程中需要小心不要使用测试数据的信息。
- en: 'Data Leakage Case: **Target Encoding**'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据泄露案例：**目标编码**
- en: 'When you convert categorical values using target encoding on all your data,
    the encoded values are calculated using the target information from both training
    and test sets. The numbers that replace each category are averages of target values
    that include test data. This means your training data gets assigned values that
    already contain information about the target values from the test set that it
    wasn’t supposed to know about. To summarize:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用目标编码在所有数据上转换类别值时，编码值是基于来自训练集和测试集的目标信息计算的。替换每个类别的数字是包含测试数据的目标值的平均值。这意味着你的训练数据会被赋予已经包含来自测试集的目标值信息，这些信息它本不应该知道。总结来说：
- en: 🚨 **THE ISSUE** Computing category means using complete dataset
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 🚨 **问题** 计算类别时使用了完整的数据集
- en: ❌ **What We’re Doing Wrong** Calculating category replacements using all target
    values
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ **我们做错了什么** 使用所有目标值计算类别替换
- en: 💥 **The Consequence** Training features contain future target information
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 💥 **后果** 训练特征包含未来目标信息
- en: '![](../Images/57834471ff029388e93763c630f6e268.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57834471ff029388e93763c630f6e268.png)'
- en: Target encoding leakage occurs when replacing categories with their average
    target values (A=3, B=4, C=2) using all the data, instead of correctly using only
    training data averages (A=2, B=5, C=1), leading to wrong category values.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 目标编码泄漏发生在使用所有数据替换类别的平均目标值（A=3，B=4，C=2），而不是正确地只使用训练数据的平均值（A=2，B=5，C=1），导致类别值错误。
- en: 'Data Leakage Case: One-Hot Encoding'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据泄露案例：独热编码
- en: 'When you convert categories into binary columns using all your data and then
    select which columns to keep, the selection is based on patterns found in both
    training and test sets. The decision to keep or remove certain binary columns
    is influenced by how well they predict the target in the test data, not just the
    training data. This means your chosen set of columns is partially determined by
    test set relationships you weren’t supposed to use. To summarize:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用所有数据将类别转换为二进制列，然后选择要保留的列时，选择是基于训练集和测试集中的模式。这意味着你决定保留或删除某些二进制列时，受到了测试数据中目标预测能力的影响，而不仅仅是训练数据。这意味着你选择的列集部分受到了本不应该使用的测试集关系的影响。总结：
- en: 🚨 **THE ISSUE** Determining categories from complete dataset
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 🚨 **问题所在** 从完整数据集中确定类别
- en: ❌ **What We’re Doing Wrong** Creating binary columns based on all unique values
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ **我们做错了什么** 基于所有唯一值创建二进制列
- en: 💥 **The Consequence** Feature selection influenced by test set patterns
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 💥 **后果** 特征选择受到测试集模式的影响
- en: '![](../Images/de6c9ace481f7e7d863aebe762b2940a.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de6c9ace481f7e7d863aebe762b2940a.png)'
- en: One-hot encoding leakage occurs when creating category columns using all unique
    values (A,B,C,D) from the full dataset, instead of correctly using only categories
    present in training data (A,B,C), leading to wrong encoding patterns.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用所有唯一值（A、B、C、D）从完整数据集创建类别列，而不是仅正确使用训练数据中存在的类别（A、B、C）时，会发生独热编码泄漏，导致错误的编码模式。
- en: Data Scaling
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据缩放
- en: Different features in your data often have very different ranges — some might
    be in thousands while others are tiny decimals. We adjust these ranges so all
    features have similar scales, which helps models work better.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中的不同特征往往具有非常不同的范围——有些可能在千位，而其他则可能是很小的小数。我们调整这些范围，使所有特征具有相似的尺度，从而帮助模型更好地工作。
- en: 'Common ways to adjust scales include:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的缩放调整方法包括：
- en: Using `StandardScaler()` to make values center around 0 with most falling between
    -1 and 1 (mean=0, variance=1)
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`StandardScaler()`使得值集中在0附近，大多数值在-1和1之间（均值=0，方差=1）
- en: Using `MinMaxScaler()` to squeeze all values between 0 and 1, or `MinMaxScaler(feature_range=(min,
    max))` for a custom range
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`MinMaxScaler()`将所有值缩放到0和1之间，或者使用`MinMaxScaler(feature_range=(min, max))`来设置自定义范围
- en: Using `FunctionTransformer(np.log1p)` or `PowerTransformer(method='box-cox')`
    to handle very large numbers and make distributions more normal
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`FunctionTransformer(np.log1p)`或`PowerTransformer(method='box-cox')`来处理非常大的数字，使数据分布更加接近正态分布
- en: Using `RobustScaler()` to adjust scales using statistics that aren't affected
    by outliers (using quartiles instead of mean/variance)
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`RobustScaler()`通过不受异常值影响的统计量调整缩放（使用四分位数而非均值/方差）
- en: While scaling helps models compare different features fairly, we need to calculate
    these adjustments using only training data to avoid leakage.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然缩放有助于模型公平地比较不同的特征，但我们需要仅使用训练数据来计算这些调整，以避免数据泄漏。
- en: 'Data Leakage Case: Standard Scaling'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据泄漏案例：标准化缩放
- en: 'When you standardize features using all your data, the average and spread values
    used in the calculation come from both training and test sets. These values are
    different from what you would get using just the training data. This means every
    standardized value in your training data is adjusted using information about the
    distribution of values in your test set. To summarize:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用所有数据标准化特征时，用于计算的平均值和分布值来自训练集和测试集。这些值与仅使用训练数据时得到的不同。这意味着你训练数据中的每个标准化值都使用了关于测试集分布的信息来进行调整。总结：
- en: 🚨 **THE ISSUE** Computing statistics using complete dataset
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 🚨 **问题所在** 使用完整数据集计算统计量
- en: ❌ **What We’re Doing Wrong** Calculating mean and standard deviation using all
    values
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ **我们做错了什么** 使用所有值计算均值和标准差
- en: 💥 **The Consequence** Training features scaled using test set distribution
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 💥 **后果** 使用测试集分布缩放训练特征
- en: '![](../Images/07e43284950a668a081b2a90dd6c0512.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07e43284950a668a081b2a90dd6c0512.png)'
- en: Standard scaling leakage occurs when using the full dataset’s average (μ=0)
    and spread (σ=3) to normalize data, instead of correctly using only training data’s
    statistics (μ=2, σ=2), leading to wrong standardized values.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化泄漏发生在使用完整数据集的平均值（μ=0）和分布（σ=3）来规范化数据时，而不是仅正确使用训练数据的统计量（μ=2，σ=2），导致错误的标准化值。
- en: 'Data Leakage Case: **Min-Max Scaling**'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据泄漏案例：**最小-最大缩放**
- en: 'When you scale features using minimum and maximum values from all your data,
    these boundary values might come from your test set. The scaled values in your
    training data are calculated using these bounds, which could be different from
    what you’d get using just training data. This means every scaled value in your
    training data is adjusted using the full range of values from your test set. To
    summarize:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用所有数据的最小值和最大值进行特征缩放时，这些边界值可能来自测试集。你训练数据中的缩放值是根据这些边界计算的，这些边界可能与仅使用训练数据时得到的边界不同。这意味着训练数据中的每个缩放值都使用了来自测试集的完整值范围进行调整。总结如下：
- en: 🚨 **THE ISSUE** Finding bounds using complete dataset
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 🚨 **问题所在** 使用完整数据集查找边界
- en: ❌ **What We’re Doing Wrong** Determining min/max values from all data points
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ **我们做错了什么** 从所有数据点中确定最小/最大值
- en: 💥 **The Consequence** Training features normalized using test set ranges
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 💥 **后果** 使用测试集范围对训练特征进行归一化
- en: '![](../Images/f904f4f392adbf5fa4711e4a90bfaa9c.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f904f4f392adbf5fa4711e4a90bfaa9c.png)'
- en: Min-max scaling leakage occurs when using the full dataset’s minimum (-5) and
    maximum (5) values to scale data, instead of correctly using only training data’s
    range (min=-1, max=5), leading to wrong scaling of values.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用完整数据集的最小值（-5）和最大值（5）来进行缩放，而不是仅正确使用训练数据的范围（最小值=-1，最大值=5）进行缩放，从而导致值的缩放错误。
- en: Discretization
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离散化
- en: Sometimes it’s better to group numbers into categories rather than use exact
    values. This helps machine learning models to process and analyze the data more
    easily.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，将数字分组为类别比使用精确的值更好。这有助于机器学习模型更容易地处理和分析数据。
- en: 'Common ways to create these groups include:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这些分组的常见方法包括：
- en: Using `KBinsDiscretizer(strategy='uniform')` to make each group cover the same
    size range of values
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`KBinsDiscretizer(strategy='uniform')`使每个组覆盖相同大小的值范围
- en: Using `KBinsDiscretizer(strategy='quantile')` to make each group contain the
    same number of data points
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`KBinsDiscretizer(strategy='quantile')`使每个组包含相同数量的数据点
- en: Using `KBinsDiscretizer(strategy='kmeans')` to find natural groupings in the
    data using clustering
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`KBinsDiscretizer(strategy='kmeans')`通过聚类找到数据中的自然分组
- en: Using `QuantileTransformer(n_quantiles=n, output_distribution='uniform')` to
    create groups based on percentiles in your data
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`QuantileTransformer(n_quantiles=n, output_distribution='uniform')`基于数据中的分位数创建分组
- en: While grouping values can help models find patterns better, the way we decide
    group boundaries needs to use only training data to avoid leakage.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然分组值可以帮助模型更好地发现模式，但我们决定分组边界的方式需要仅使用训练数据，以避免泄漏。
- en: '**Data Leakage Case: Equal Frequency Binning**'
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数据泄漏案例：等频分箱**'
- en: 'When you create bins with equal numbers of data points using all your data,
    the cutoff points between bins are determined using both training and test sets.
    These cutoff values are different from what you’d get using just training data.
    This means when you assign data points to bins in your training data, you’re using
    dividing points that were influenced by your test set values. To summarize:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用所有数据创建具有相等数据点数量的箱时，箱之间的分割点是通过训练集和测试集共同确定的。这些分割值与仅使用训练数据时得到的值不同。这意味着，当你将数据点分配到训练数据的箱中时，你使用的是受测试集值影响的分割点。总结如下：
- en: 🚨 **THE ISSUE** Setting thresholds using complete dataset
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 🚨 **问题所在** 使用完整数据集设置阈值
- en: ❌ **What We’re Doing Wrong** Determining bin boundaries using all data points
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ **我们做错了什么** 使用所有数据点确定箱的边界
- en: 💥 **The Consequence** Training data binned using test set distributions
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 💥 **后果** 使用测试集分布对训练数据进行分箱
- en: '![](../Images/c350933b8e9d11b558836a45c596c4b1.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c350933b8e9d11b558836a45c596c4b1.png)'
- en: Equal frequency binning leakage occurs when setting bin cutoff points (-0.5,
    2.5) using all the data, instead of correctly using only training data to set
    boundaries (-0.5, 2.0), leading to wrong grouping of values.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 等频分箱泄漏发生在使用所有数据设置箱的分割点（-0.5，2.5）时，而不是仅正确使用训练数据来设置边界（-0.5，2.0），从而导致值的分组错误。
- en: 'Data Leakage Case: Equal Width Binning'
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据泄漏案例：等宽分箱
- en: 'When you create bins of equal size using all your data, the range used to determine
    bin widths comes from both training and test sets. This total range could be wider
    or narrower than what you’d get using just training data. This means when you
    assign data points to bins in your training data, you’re using bin boundaries
    that were calculated based on the full spread of your test set values. To summarize:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用所有数据创建相等大小的箱时，确定箱宽度所使用的范围来自训练集和测试集。这一总范围可能比仅使用训练数据得到的范围更宽或更窄。这意味着当你将数据点分配到训练数据中的箱时，使用的箱边界是基于整个测试集的范围计算得出的。总结如下：
- en: 🚨 **THE ISSUE** Calculating ranges using complete dataset
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 🚨 **问题** 使用完整数据集计算范围
- en: ❌ **What We’re Doing Wrong** Setting bin widths based on full data spread
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ **我们做错了什么** 基于完整数据分布设置箱宽
- en: 💥 **The Consequence** Training data binned using test set boundaries
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 💥 **后果** 使用测试集边界对训练数据进行分箱
- en: '![](../Images/9942d147b1bda767bd112c2d616cec43.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9942d147b1bda767bd112c2d616cec43.png)'
- en: Equal width binning leakage occurs when splitting data into equal-size groups
    using the full dataset’s range (-3 to 6), instead of correctly using only the
    training data’s range (-3 to 3), leading to wrong groupings.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 等宽分箱泄漏发生在使用完整数据集范围（-3到6）将数据划分为相等大小的组时，而不是仅正确使用训练数据的范围（-3到3），从而导致错误的分组。
- en: Resampling
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重采样
- en: When some categories in your data have many more examples than others, we can
    balance them using resampling techniques from `imblearn` by either creating new
    samples or removing existing ones. This helps models learn all categories fairly.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的数据中某些类别的示例比其他类别多得多时，我们可以使用`imblearn`中的重采样技术，通过创建新样本或删除现有样本来平衡它们。这有助于模型公平地学习所有类别。
- en: 'Common ways to add samples (Oversampling):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 添加样本的常见方法（过采样）：
- en: Using `RandomOverSampler()` to make copies of existing examples from smaller
    categories
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`RandomOverSampler()`从较小类别中复制现有示例
- en: Using `SMOTE()` to create new, synthetic examples for smaller categories using
    interpolation
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`SMOTE()`为较小类别创建新的合成示例，通过插值法生成
- en: Using `ADASYN()` to create more examples in areas where the model struggles
    most, focusing on decision boundaries
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ADASYN()`在模型最难处理的区域创建更多示例，重点关注决策边界
- en: 'Common ways to remove samples (Undersampling):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 删除样本的常见方法（欠采样）：
- en: Using `RandomUnderSampler()` to randomly remove examples from larger categories
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`RandomUnderSampler()`随机删除较大类别中的示例
- en: Using `NearMiss(version=1)` or `NearMiss(version=2)` to remove examples from
    larger categories based on their distance to smaller categories
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`NearMiss(version=1)`或`NearMiss(version=2)`根据与较小类别的距离从较大类别中删除示例
- en: Using `TomekLinks()` or `EditedNearestNeighbours()` to carefully select which
    examples to remove based on their similarity to other categories
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`TomekLinks()`或`EditedNearestNeighbours()`根据与其他类别的相似度仔细选择要删除的示例
- en: While balancing your data helps models learn better, the process of creating
    or removing samples should only use information from training data to avoid leakage.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡数据有助于模型更好地学习，但创建或删除样本的过程应该仅使用来自训练数据的信息，以避免数据泄漏。
- en: '**Data Leakage Case: Oversampling (SMOTE)**'
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数据泄漏案例：过采样（SMOTE）**'
- en: 'When you create synthetic data points using SMOTE on all your data, the algorithm
    picks nearby points from both training and test sets to create new samples. These
    new points are created by mixing values from test set samples with training data.
    This means your training data gets new samples that were directly created using
    information from your test set values. To summarize:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在所有数据上使用SMOTE创建合成数据点时，算法会从训练集和测试集中挑选附近的点来创建新样本。这些新点是通过将测试集样本的值与训练数据混合生成的。这意味着你的训练数据得到了直接使用测试集信息创建的新样本。总结如下：
- en: 🚨 **THE ISSUE** Generating samples using complete dataset
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 🚨 **问题** 使用完整数据集生成样本
- en: ❌ **What We’re Doing Wrong** Creating synthetic points using test set neighbors
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ **我们做错了什么** 使用测试集邻居创建合成点
- en: 💥 **The Consequence** Training augmented with test-influenced samples
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 💥 **后果** 训练数据被包含测试集影响的样本增强
- en: '![](../Images/9faf04e494faa2e8a948243bc52ae55b.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9faf04e494faa2e8a948243bc52ae55b.png)'
- en: Oversampling leakage occurs when duplicating data points based on class counts
    from the entire dataset (A×4, B×3, C×2), instead of correctly using only the training
    data (A×1, B×2, C×2) to decide how many times to duplicate each class.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 过采样泄漏发生在基于整个数据集的类别计数（A×4, B×3, C×2）重复数据点时，而不是仅根据训练数据（A×1, B×2, C×2）来决定每个类别重复的次数。
- en: 'Data Leakage Case: Undersampling (Tomek Links)'
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据泄漏案例：欠采样（Tomek链接）
- en: 'When you remove data points using Tomek Links on all your data, the algorithm
    finds pairs of points from both training and test sets that are closest to each
    other but have different labels. The decision to remove points from your training
    data is based on how close they are to test set points. This means your final
    training data is shaped by its relationship with test set values. To summarize:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用Tomek链接在所有数据中删除数据点时，算法会找到来自训练集和测试集的最近邻成对数据点，它们的标签却不同。根据这些数据点与测试集数据点的距离来决定是否从训练集中移除这些点。这意味着你的最终训练数据是由它与测试集值之间的关系决定的。总结如下：
- en: 🚨 **THE ISSUE** Removing samples using complete dataset
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 🚨 **问题** 使用完整数据集移除样本
- en: ❌ **What We’re Doing Wrong** Identifying pairs using test set relationships
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ **我们做错了什么** 使用测试集关系识别成对数据
- en: 💥 **The Consequence** Training reduced based on test set patterns
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 💥 **后果** 基于测试集模式减少训练
- en: '![](../Images/d56e90a957fc6d7a249a9d990da10385.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d56e90a957fc6d7a249a9d990da10385.png)'
- en: Undersampling leakage occurs when removing data points based on class ratios
    from the entire dataset (A×4, B×3, C×2), instead of correctly using only the training
    data (A×1, B×2, C×2) to decide how many samples to keep from each class.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 欠采样泄漏发生在基于整个数据集的类别比例移除数据点时（A×4，B×3，C×2），而不是正确地仅使用训练数据（A×1，B×2，C×2）来决定从每个类别中保留多少样本。
- en: Final Remarks
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的评论
- en: When preprocessing data, you need to keep training and test data completely
    separate. Any time you use information from all your data to transform values
    — whether you’re filling missing values, converting categories to numbers, scaling
    features, creating bins, or balancing classes — you risk mixing test data information
    into your training data. This makes your model’s test results unreliable because
    the model already learned from patterns it wasn’t supposed to see.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理数据时，你需要确保训练数据和测试数据完全分开。任何时候你使用所有数据的信息来转换值——无论是填补缺失值、将类别转换为数字、特征缩放、创建区间，还是平衡类别——都会有将测试数据的信息混入训练数据的风险。这会使得模型的测试结果不可靠，因为模型已经从它本不该看到的模式中学习了。
- en: 'The solution is simple: **always transform your training data first, save those
    calculations, and then apply them to your test data.**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案很简单：**始终先转换你的训练数据，保存这些计算结果，然后将其应用到你的测试数据上。**
- en: 🌟 Data Preprocessing + Classification (with Leakage) Code Summary
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🌟 数据预处理 + 分类（带有泄漏）代码总结
- en: Let us see how leakage could happen in predicting a simple golf play dataset.
    This is the bad example and should not be followed. Just for demonstration and
    education purposes.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在预测一个简单的高尔夫比赛数据集时，数据泄漏是如何发生的。这是一个不好的示例，不应当被遵循，仅用于演示和教育目的。
- en: '[PRE0]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The code above is using `ColumnTransformer`, which is a utility in scikit-learn
    that allows us to apply different preprocessing steps to different columns in
    a dataset.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码使用了`ColumnTransformer`，这是scikit-learn中的一个工具，允许我们对数据集中的不同列应用不同的预处理步骤。
- en: 'Here’s a breakdown of the preprocessing strategy for each column in the dataset:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据集中每一列预处理策略的详细说明：
- en: '`**Temperature**`**:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`**温度**`**：'
- en: -** Mean imputation to handle any missing values
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: -** 均值填充处理任何缺失值
- en: '- Standard scaling to normalize the values (mean=0, std=1)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '- 标准化缩放以规范化值（均值=0，标准差=1）'
- en: '- Equal-width discretization into 4 bins, meaning continuous values are categorized
    into 4 equal-width intervals'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '- 等宽离散化为4个区间，意味着将连续值分类为4个等宽区间'
- en: '`**Humidity**`**:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`**湿度**`**：'
- en: '-** Same strategy as Temperature: Mean imputation → Standard scaling → Equal-width
    discretization (4 bins)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: -** 与温度相同的策略：均值填充 → 标准化缩放 → 等宽离散化（4个区间）
- en: '`**Outlook**`**(categorical):**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`**展望**`**（类别）**：'
- en: '- Ordinal encoding: converts categorical values into numerical ones'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '- 序数编码：将类别值转换为数值'
- en: '- Unknown values are handled by setting them to -1'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '- 通过将未知值设置为-1来处理'
- en: '`**Wind**` **(binary):**'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`**风速**` **（二元）**：'
- en: '- Constant imputation with False for missing values'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对缺失值进行常数填充（填充为 False）'
- en: '- Standard scaling to normalize the 0/1 values'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '- 标准化缩放以规范化 0/1 值'
- en: '`**Play**` **(target):**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`**比赛**` **（目标）**：'
- en: '- Label encoding to convert Yes/No to 1/0'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '- 标签编码将“是/否”转换为 1/0'
- en: '- SMOTE applied after preprocessing to balance classes by creating synthetic
    examples of the minority class'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在预处理后应用SMOTE通过创建少数类别的合成示例来平衡类别'
- en: '- A simple decision tree is used to predict the target'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '- 使用简单的决策树来预测目标'
- en: The entire pipeline demonstrates data leakage because **all transformations
    see the entire dataset during fitting**, which would be inappropriate in a real
    machine learning scenario where we need to keep test data completely separate
    from the training process.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 整个流水线演示了数据泄漏，因为**所有的转换在拟合过程中都看到整个数据集**，在实际的机器学习场景中这将是不合适的，我们需要确保测试数据与训练过程完全分开。
- en: This approach will also likely show artificially higher test accuracy because
    the test data characteristics were used in the preprocessing steps!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法也可能会显示出人为提高的测试准确度，因为在预处理步骤中使用了测试数据的特征！
- en: 🌟 Data Preprocessing + Classification (without leakage) Code Summary
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🌟 数据预处理 + 分类（无泄漏）代码总结
- en: 'Here’s the version without data leakage:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这是没有数据泄漏的版本：
- en: '[PRE1]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Key differences from the leakage version
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与泄漏版本的主要区别
- en: Split data first, before any processing
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先拆分数据，然后再进行任何处理
- en: All transformations (preprocessing, SMOTE) are inside the pipeline
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有的转换（预处理，SMOTE）都在流水线内
- en: 'Pipeline ensures:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流水线确保：
- en: '- Preprocessing parameters learned only from training data'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 预处理参数仅从训练数据中学习'
- en: '- SMOTE applies only to training data'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- SMOTE仅适用于训练数据'
- en: '- Test data remains completely unseen until prediction'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 测试数据在预测之前完全不可见'
- en: This approach gives more realistic performance estimates as it maintains proper
    separation between training and test data.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法提供了更现实的性能估计，因为它保持了训练数据和测试数据之间的正确分离。
- en: Technical Environment
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术环境
- en: This article uses Python 3.7 , scikit-learn 1.5, and imblearn 0.12\. While the
    concepts discussed are generally applicable, specific code implementations may
    vary slightly with different versions
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用Python 3.7，scikit-learn 1.5和imblearn 0.12。虽然讨论的概念通常适用，但具体的代码实现可能会因版本不同略有差异。
- en: About the Illustrations
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于插图
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有插图均由作者创建，并结合了Canva Pro授权的设计元素。
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘿𝙖𝙩𝙖 𝙋𝙧𝙚𝙥𝙧𝙤𝙘𝙚𝙨𝙨𝙞𝙣𝙜 𝙢𝙚𝙩𝙝𝙤𝙙𝙨 𝙝𝙚𝙧𝙚:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘿𝙖𝙩𝙖 𝙋𝙧𝙚𝙥𝙧𝙤𝙘𝙚𝙨𝙨𝙞𝙣𝙜 𝙢𝙚𝙩𝙝𝙤𝙙𝙨 𝙝𝙚𝙧𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----33cbf07507b7--------------------------------)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----33cbf07507b7--------------------------------)'
- en: Data Preprocessing
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: '[View list](https://medium.com/@samybaladram/list/data-preprocessing-17a2c49b44e4?source=post_page-----33cbf07507b7--------------------------------)6
    stories![](../Images/f7ead0fb9a8dc2823d7a43d67a1c6932.png)![Cartoon illustration
    of two figures embracing, with letters ‘A’, ‘B’, ‘C’ and numbers ‘1’, ‘2’, ‘3’
    floating around them. A pink heart hovers above, symbolizing affection. The background
    is a pixelated pattern of blue and green squares, representing data or encoding.
    This image metaphorically depicts the concept of encoding categorical data, where
    categories (ABC) are transformed into numerical representations (123).](../Images/72bb3a287a9ca4c5e7a3871e234bcc4b.png)![A
    cartoon illustration representing data scaling in machine learning. A tall woman
    (representing a numerical feature with a large range) is shown shrinking into
    a child (representing the same feature after scaling to a smaller range). A red
    arrow indicates the shrinking process, and yellow sparkles around the child signify
    the positive impact of scaling.](../Images/d261b2c52a3cafe266d1962d4dbabdbd.png)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/data-preprocessing-17a2c49b44e4?source=post_page-----33cbf07507b7--------------------------------)6个故事！[](../Images/f7ead0fb9a8dc2823d7a43d67a1c6932.png)![两个人物拥抱的卡通插图，字母‘A’，‘B’，‘C’和数字‘1’，‘2’，‘3’在它们周围漂浮。一个粉色的心形图标悬浮在上方，象征着感情。背景是蓝色和绿色方块的像素化图案，代表数据或编码。这个插图隐喻地描绘了对类别数据进行编码的概念，其中类别（ABC）被转化为数字表示（123）。](../Images/72bb3a287a9ca4c5e7a3871e234bcc4b.png)![一张卡通插图，表示机器学习中的数据缩放。一个高大的女性（代表一个具有大范围的数值特征）正在缩小成一个小孩（代表相同特征在缩放后变为较小范围）。一个红色箭头指示缩小过程，孩子周围的黄色闪光象征着缩放的积极影响。](../Images/d261b2c52a3cafe266d1962d4dbabdbd.png)'
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----33cbf07507b7--------------------------------)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----33cbf07507b7--------------------------------)'
- en: Classification Algorithms
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类算法
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----33cbf07507b7--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----33cbf07507b7--------------------------------)8篇文章![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----33cbf07507b7--------------------------------)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----33cbf07507b7--------------------------------)'
- en: Regression Algorithms
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归算法
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----33cbf07507b7--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This “dummy” doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----33cbf07507b7--------------------------------)5篇文章![一个扎着辫子、戴着粉色帽子的卡通娃娃。这个“傀儡”娃娃，凭借其简单的设计和心形装饰的衬衫，在视觉上代表了机器学习中的傀儡回归器概念。就像这个玩具般的形象是一个简化、静态的人物代表，傀儡回归器是作为基准的基本模型，供更复杂的分析使用。](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
