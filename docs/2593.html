<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How AlphaFold 3 Is Like DALLE 2 and Other Learnings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How AlphaFold 3 Is Like DALLE 2 and Other Learnings</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-alphafold-3-is-like-dalle-2-and-other-learnings-1f809010afc7?source=collection_archive---------6-----------------------#2024-10-24">https://towardsdatascience.com/how-alphafold-3-is-like-dalle-2-and-other-learnings-1f809010afc7?source=collection_archive---------6-----------------------#2024-10-24</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/481d90a32bacb1ec7467f45b59cb4b3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mIpF_t6JuMJaJl8x3KryaA.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Diffusion (literally) from <a class="af gi" href="https://unsplash.com/photos/three-drinking-glasses-Y1ge0B9_oGE" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div/><div><h2 id="0948" class="pw-subtitle-paragraph hi gk gl bf b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx cq dx">Understanding AI applications in bio for machine learning engineers</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hy hz ia ib ic ab"><div><div class="ab id"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@meghanheintz?source=post_page---byline--1f809010afc7--------------------------------" rel="noopener follow"><div class="l ie if by ig ih"><div class="l ed"><img alt="Meghan Heintz" class="l ep by dd de cx" src="../Images/9eaae6d3d8168086d83ff7100329c51f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Tespb9SFbU5QAxy8f7bhnA.png"/><div class="ii by l dd de em n ij eo"/></div></div></a></div></div><div class="ik ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--1f809010afc7--------------------------------" rel="noopener follow"><div class="l il im by ig in"><div class="l ed"><img alt="Towards Data Science" class="l ep by br io cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ii by l br io em n ij eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ip ab q"><div class="ab q iq"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ir is bk"><a class="af ag ah ai aj ak al am an ao ap aq ar it" data-testid="authorName" href="https://medium.com/@meghanheintz?source=post_page---byline--1f809010afc7--------------------------------" rel="noopener follow">Meghan Heintz</a></p></div></div></div><span class="iu iv" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ir is dx"><button class="iw ix ah ai aj ak al am an ao ap aq ar iy iz ja" disabled="">Follow</button></p></div></div></span></div></div><div class="l jb"><span class="bf b bg z dx"><div class="ab cn jc jd je"><div class="jf jg ab"><div class="bf b bg z dx ab jh"><span class="ji l jb">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar it ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--1f809010afc7--------------------------------" rel="noopener follow"><p class="bf b bg z jj jk jl jm jn jo jp jq bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="iu iv" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="jr js l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 24, 2024</span></div></span></div></span></div></div></div><div class="ab cp jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki"><div class="h k w ea eb q"><div class="ky l"><div class="ab q kz la"><div class="pw-multi-vote-icon ed ji lb lc ld"><div class=""><div class="le lf lg lh li lj lk am ll lm ln ld"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l lo lp lq lr ls lt lu"><p class="bf b dy z dx"><span class="lf">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao le lx ly ab q ee lz ma" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lw"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lv lw">2</span></p></button></div></div></div><div class="ab q kj kk kl km kn ko kp kq kr ks kt ku kv kw kx"><div class="mb k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mc an ao ap iy md me mf" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mg cn"><div class="l ae"><div class="ab cb"><div class="mh mi mj mk ml gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="df7a" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In our last article, we explored how <a class="af gi" href="https://medium.com/towards-data-science/alphafold-2-through-the-context-of-bert-78c9494e99af" rel="noopener">AlphaFold 2 and BERT</a> were connected through transformer architecture. In this piece, we’ll learn how the most recent update, <a class="af gi" href="https://www.nature.com/articles/s41586-024-07487-w" rel="noopener ugc nofollow" target="_blank">AlphaFold 3</a> (hereafter AlphaFold) is more similar to DALLE 2 (hereafter DALLE) and then dive into other changes to its architecture and training.</p><h1 id="94e5" class="nz oa gl bf ob oc od hl oe of og ho oh oi oj ok ol om on oo op oq or os ot ou bk">What’s the connection?</h1><p id="6bef" class="pw-post-body-paragraph nd ne gl nf b hj ov nh ni hm ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">AlphaFold and DALLE are another example of how vastly different use cases can benefit from architectural learning across domains. DALLE is a text-to-image model that generates images from text prompts. AlphaFold 3 is a model for predicting biomolecular interactions. The applications of these two models sound like they couldn’t be any more different but both rely on diffusion model architecture.</p><p id="7b96" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Because reasoning about images and text is more intuitive than biomolecular interactions, we’ll first explore DALLE’s application. Then we’ll learn about how the same concepts are applied by AlphaFold.</p><h1 id="f3e1" class="nz oa gl bf ob oc od hl oe of og ho oh oi oj ok ol om on oo op oq or os ot ou bk">Diffusion Models</h1><p id="e627" class="pw-post-body-paragraph nd ne gl nf b hj ov nh ni hm ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">A metaphor for understanding the diffusion model: consider tracing the origin of a drop of dye in a glass of water. As the dye disperses, it moves randomly through the liquid until it is evenly spread. To backtrack to the initial drop’s location, you must reconstruct its path step by step since each movement depends on the one before. If you repeat this experiment over and over, you’ll be able to build a model to predict the dye movement.</p><p id="ec7f" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">More concretely, diffusion models are trained to predict and remove noise from a dataset. Then upon inference, the model generates a new sample using random noise. The architecture comprises three core components: the forward process, the reverse process, and the sampling procedure. The forward process takes the training data and adds noise at each time step. As you might expect, the reverse process removes noise at each step. The sampling procedure (or inference) executes the reverse process using the trained model and a noise schedule, transforming an initial random noise input back into a structured data sample.</p><figure class="pb pc pd pe pf fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pa"><img src="../Images/d85e8c619f3ce77d041fed1db3db2f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UAo096IVNUIGtVzJ-pu-Uw.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Simplified illustration of the forward and reverse processes where a pixelated heart has noise added and removed back to its original shape. (Created by author)</figcaption></figure><h1 id="d51b" class="nz oa gl bf ob oc od hl oe of og ho oh oi oj ok ol om on oo op oq or os ot ou bk">DALLE and Diffusion</h1><p id="5765" class="pw-post-body-paragraph nd ne gl nf b hj ov nh ni hm ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">DALLE incorporates diffusion model architecture in two major components, the prior and the decoder, and removes its predecessor’s autoregressive module. The prior model takes text embeddings generated by <a class="af gi" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">CLIP</a> (a model trained on a dataset of images and captions known as Contrastive Language-Image Pre-training) and creates an image embedding. During training, the prior is given a text embedding and a noised version of the image embedding. The prior learns to denoise the image embedding step by step. This process allows the model to learn a distribution over image embeddings representing the variability in possible images for a given text prompt.</p><p id="e2ad" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The decoder generates an image from the resulting image embedding starting with a random noise image. The reverse diffusion process iteratively removes noise from the image using the image embedding (from the prior) at each timestep according to the noise schedule. Time step embeddings tell the model about the current stage of the denoising process, helping it adjust the noise level removed based on how close it is to the final step.</p><p id="7370" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While DALLE 2 utilizes a diffusion model for generating images, its predecessor, DALLE 1, relied on an autoregressive approach, sequentially predicting image tokens based on the text prompt. This approach was much less computationally efficient, required a more complex training and inference process, struggled to produce high-resolution images, and often resulted in artifacts.</p><p id="936b" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Its predecessor also did not make use of CLIP. Instead, DALLE 1 learned the text-image representations directly. The introduction of CLIP embeddings unified these representations making more robust text-to-image representations.</p><figure class="pb pc pd pe pf fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pg"><img src="../Images/b373b945018f0a708e706e5f399225a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JVzZMmO9HC0crWSsw0i2hA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">High-level overview of DALLE 2 architecture from Hierarchical Text-Conditional Image Generation with CLIP Latents by the OpenAI team from<a class="af gi" href="https://arxiv.org/pdf/2204.06125" rel="noopener ugc nofollow" target="_blank"> Hierarchical Text-Conditional Image Generation with CLIP Latents</a></figcaption></figure><h1 id="96fb" class="nz oa gl bf ob oc od hl oe of og ho oh oi oj ok ol om on oo op oq or os ot ou bk">How AlphaFold uses Diffusion</h1><p id="6b5b" class="pw-post-body-paragraph nd ne gl nf b hj ov nh ni hm ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">While DALLE’s use of diffusion helps generate detailed visual content, AlphaFold leverages similar principles in biomolecular structure prediction (no longer just protein folding!).</p><p id="4e29" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">AlphaFold 2 was not a generative model, as it predicted structures directly from given input sequences. Due to the introduction of the diffusion module, AlphaFold 3 IS a generative model. Just like with DALLE, noise is sampled and then recurrently denoised to produce a final structure.</p><p id="2f7c" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The diffusion module is incorporated by replacing the structure module. This architecture change greatly simplifies the model because the structure module predicts amino-acid-specific frames and side-chain torsion angles whereas the diffusion module predicts the raw atom coordinates. This eliminates several intermediate steps in the inference process.</p><figure class="pb pc pd pe pf fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp ph"><img src="../Images/b946e3b418bc804980575b2c419aad76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*95sH3sIyWIfv9n6i2EFc0w.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">AF3 architecture for inference showing where the diffusion module residues. From <a class="af gi" href="https://www.nature.com/articles/s41586-024-07487-w" rel="noopener ugc nofollow" target="_blank">Accurate structure prediction of biomolecular interactions with AlphaFold 3</a></figcaption></figure><p id="c6e3" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The impetus behind removing these intermediate steps was that the scope of training data for this iteration of the model grew substantially. AlphaFold 2 was only trained on protein structures, whereas AlphaFold 3 is a “multi-modal” model capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. If the model still used the structure module, it would have required an excessive number of complex rules about chemical bonds and stereochemistry to create valid structures.</p><p id="a421" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The reason why diffusion did not require these rules is because it can be applied at coarse and fine-grained levels. For high noise levels, the model is focused on capturing the global structure, while at low noise levels, it fine-tunes the details. When the noise is minimal, the model refines the local details of the structure, such as the precise positions of atoms and their orientations, which are crucial for accurate molecular modeling. This means the model can easily work with different types of chemical components, not just standard amino acids or protein structures.</p><p id="39e2" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The benefit of working with different types of chemical components appears to be that the model can learn more about protein structures from other types of structures such as protein-ligand interfaces. It appears that integrating diverse data types helps models generalize better across different tasks. This improvement is similar to how <a class="af gi" href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/" rel="noopener ugc nofollow" target="_blank">Gemini’s</a> text comprehension abilities became better the model became multi-modal with the incorporation of image and video data.</p><h1 id="bddd" class="nz oa gl bf ob oc od hl oe of og ho oh oi oj ok ol om on oo op oq or os ot ou bk">Other Important Changes to AlphaFold</h1><p id="37f8" class="pw-post-body-paragraph nd ne gl nf b hj ov nh ni hm ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk"><strong class="nf gm">The role of MSA (Multiple Sequence Alignment) was significantly downgraded.</strong> The AF2 evoformer is replaced with the simpler pairformer module (a reduction of 48 blocks to 4 blocks). As you may recall from my previous article, the MSA was thought to help the model learn what parts of the amino acid sequence were important evolutionarily. Experimental changes showed that reducing the importance of the MSA had a limited impact on model accuracy.</p><p id="22bb" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf gm">Hallucination had to be countered.</strong> Generative models are very exciting but they come with the baggage of hallucination. Researchers found the model would invent plausible-looking structures in unstructured regions. To overcome this, a cross-distillation method was used to augment the training data with predicted structures <a class="af gi" href="https://deepmind.google/discover/blog/a-glimpse-of-the-next-generation-of-alphafold/" rel="noopener ugc nofollow" target="_blank">AlphaFold-Multimer (v.2.3)</a>. The cross-distillation approach teaches the model to differentiate between structured and unstructured regions better. This helps the model to understand when to avoid adding artificial details.</p><p id="ca8f" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf gm">Some interactions were easier to predict than others.</strong> Sampling probabilities were adjusted for each class of interaction i.e. fewer samples from simple types that can be learned in relatively few training steps and visa versa for complex ones. This helps avoid under and overfitting across types.</p><figure class="pb pc pd pe pf fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pi"><img src="../Images/337acecc2d6ca2b5ef8c357a8543239f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rryN70o7CLlqxi6WlByLdA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Training curves for initial training and fine-tuning stages illustrate how different classes reached their best performance at varying training steps. For this reason, the training data was subsampled to prevent under and overfitting by class. From <a class="af gi" href="https://www.nature.com/articles/s41586-024-07487-w" rel="noopener ugc nofollow" target="_blank">Accurate structure prediction of biomolecular interactions with AlphaFold 3</a></figcaption></figure><h1 id="f1ce" class="nz oa gl bf ob oc od hl oe of og ho oh oi oj ok ol om on oo op oq or os ot ou bk">High-Level Learnings</h1><ul class=""><li id="a499" class="nd ne gl nf b hj ov nh ni hm ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny pj pk pl bk">DALLE 2 and AlphaFold 3 made improvements to their predecessors by using diffusion modules which simultaneously simplified their architectures.</li><li id="51c3" class="nd ne gl nf b hj pm nh ni hm pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Training on a wider range of data types makes generative models more robust. Diversifying the types of structures in the AlphaFold training dataset allowed the model to improve protein folding predictions and generalize to other biomolecular interactions. Similarly, the diversity of text-image pairs used to train CLIP improved DALLE.</li><li id="d15c" class="nd ne gl nf b hj pm nh ni hm pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">The noise schedule is an important knob when training diffusion models. Turning it up or down affects the model’s ability to learn both coarse and fine details. Doing so allowed for a significant simplification of AlphaFold because it eliminated the need to make intermediate predictions about side-chain torsion angles etc.</li></ul><p id="17ab" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Thank you again for reading, and stay tuned for the next installment. Until then, keep learning and keep exploring.</p></div></div></div></div>    
</body>
</html>