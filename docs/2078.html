<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Missing Value Imputation, Explained: A Visual Guide with Code Examples for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Missing Value Imputation, Explained: A Visual Guide with Code Examples for Beginners</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/missing-value-imputation-explained-a-visual-guide-with-code-examples-for-beginners-93e0726284eb?source=collection_archive---------0-----------------------#2024-08-27">https://towardsdatascience.com/missing-value-imputation-explained-a-visual-guide-with-code-examples-for-beginners-93e0726284eb?source=collection_archive---------0-----------------------#2024-08-27</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="d6be" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">DATA PREPROCESSING</h2><div/><div><h2 id="b7b1" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">One (tiny) dataset, six imputation methods?</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--93e0726284eb--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--93e0726284eb--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--93e0726284eb--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--93e0726284eb--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 27, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">6</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/f42d507518dfcdd0f8722ea80b8fec0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7puOj70FGSJ_bSG49cJMUw.png"/></div></div></figure><p id="f855" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><code class="cx ny nz oa ob b">⛳️ More <a class="af oc" href="https://medium.com/@samybaladram/list/data-preprocessing-17a2c49b44e4" rel="noopener">DATA PREPROCESSING</a>, explained: <br/> ▶ <a class="af oc" rel="noopener" target="_blank" href="/missing-value-imputation-explained-a-visual-guide-with-code-examples-for-beginners-93e0726284eb">Missing Value Imputation</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae">Categorical Encoding</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb">Data Scaling</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/discretization-explained-a-visual-guide-with-code-examples-for-beginners-f056af9102fa?gi=c1bf25229f86">Discretization</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/oversampling-and-undersampling-explained-a-visual-guide-with-mini-2d-dataset-1155577d3091">Oversampling &amp; Undersampling</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7">Data Leakage in Preprocessing</a></code></p><p id="0243" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s talk about something that every data scientist, analyst, or curious number-cruncher has to deal with sooner or later: missing values. Now, I know what you’re thinking — “Oh great, another missing value guide.” But hear me out. I’m going to show you how to tackle this problem using not one, not two, but six different imputation methods, all on a single dataset (with helpful visuals as well!). By the end of this, you’ll see why domain knowledge is worth its weight in gold (something even our AI friends might struggle to replicate).</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/29c4e53115cb3858f193adc68a724447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T1bcJ8sv5Rc1lsOyGS1nig.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="82df" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">What Are Missing Values and Why Do They Occur?</h1><p id="88e0" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Before we get into our dataset and imputation methods, let’s take a moment to understand what missing values are and why they’re such a common headache in data science.</p><h2 id="d399" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py fw bk">What Are Missing Values?</h2><p id="87de" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Missing values, often represented as NaN (Not a Number) in pandas or NULL in databases, are essentially <em class="pz">holes in your dataset</em>. They’re the empty cells in your spreadsheet, the blanks in your survey responses, the data points that got away. In the world of data, not all absences are created equal, and understanding the nature of your missing values is crucial for deciding how to handle them.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qa"><img src="../Images/77e04b185445651e16249a07fbddc447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8q4lX67ocYMFXgFcIr5SFA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Image by author.</figcaption></figure><h2 id="a728" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py fw bk">Why Do Missing Values Occur?</h2><p id="f898" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Missing values can sneak into your data for a variety of reasons. Here are some common reasons:</p><ol class=""><li id="6fbd" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qb qc qd bk"><strong class="ne ga">Data Entry Errors</strong>: Sometimes, it’s just human error. Someone might forget to input a value or accidentally delete one.</li><li id="5995" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Sensor Malfunctions</strong>: In IoT or scientific experiments, a faulty sensor might fail to record data at certain times.</li><li id="7c6f" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Survey Non-Response</strong>: In surveys, respondents might skip questions they’re uncomfortable answering or don’t understand.</li><li id="b560" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Merged Datasets</strong>: When combining data from multiple sources, some entries might not have corresponding values in all datasets.</li><li id="c8e7" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Data Corruption</strong>: During data transfer or storage, some values might get corrupted and become unreadable.</li><li id="0fd2" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Intentional Omissions</strong>: Some data might be intentionally left out due to privacy concerns or irrelevance.</li><li id="2e9b" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Sampling Issues</strong>: The data collection method might systematically miss certain types of data.</li><li id="6460" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Time-Sensitive Data</strong>: In time series data, values might be missing for periods when data wasn’t collected (e.g., weekends, holidays).</li></ol><h2 id="8532" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py fw bk">Types of Missing Data</h2><p id="71b7" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Understanding the type of missing data you’re dealing with can help you choose the most appropriate imputation method. Statisticians generally categorize missing data into three types:</p><ol class=""><li id="4603" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qb qc qd bk"><strong class="ne ga">Missing Completely at Random (MCAR)</strong>: The missingness is <em class="pz">totally random</em> and doesn’t depend on any other variable. For example, if a lab sample was accidentally dropped.</li><li id="7c7e" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Missing at Random (MAR)</strong>: The probability of missing data <em class="pz">depends on other observed variables</em> but not on the missing data itself. For example, men might be less likely to answer questions about emotions in a survey.</li><li id="1a8c" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Missing Not at Random (MNAR)</strong>: The missingness <em class="pz">depends on the value of the missing data itself</em>. For example, people with high incomes might be less likely to report their income in a survey.</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/4cc65a492703a3c1624d09b257c15bbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wa4AktijA1qzVrTS2Wx4Ow.png"/></div></div></figure><h2 id="c469" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py fw bk">Why Care About Missing Values?</h2><p id="70e3" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Missing values can significantly impact your analysis:</p><ol class=""><li id="b9f3" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qb qc qd bk">They can introduce bias if not handled properly.</li><li id="d4e7" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk">Many machine learning algorithms can’t handle missing values out of the box.</li><li id="be6d" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk">They can lead to loss of important information if instances with missing values are simply discarded.</li><li id="110d" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk">Improperly handled missing values can lead to incorrect conclusions or predictions.</li></ol><p id="6a70" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">That’s why it’s crucial to have a solid strategy for dealing with missing values. And that’s exactly what we’re going to explore in this article!</p><h1 id="ed9d" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">The Dataset</h1><p id="5a42" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">First things first, let’s introduce our dataset. We’ll be working with a golf course dataset that tracks various factors affecting the crowdedness of the course. This dataset has a bit of everything — numerical data, categorical data, and yes, plenty of missing values.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qj"><img src="../Images/8c57995a7970551a87f83242506716ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cnOzAbSwKrqvT3kIt-jK0Q.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">This dataset is artificially made by the author (inspired by [1]) to promote learning.</figcaption></figure><pre class="mr ms mt mu mv qk ob ql bp qm bb bk"><span id="5327" class="qn oj fq ob b bg qo qp l qq qr">import pandas as pd<br/>import numpy as np<br/><br/># Create the dataset as a dictionary<br/>data = {<br/>    'Date': ['08-01', '08-02', '08-03', '08-04', '08-05', '08-06', '08-07', '08-08', '08-09', '08-10',<br/>             '08-11', '08-12', '08-13', '08-14', '08-15', '08-16', '08-17', '08-18', '08-19', '08-20'],<br/>    'Weekday': [0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5],<br/>    'Holiday': [0.0, 0.0, 0.0, 0.0, np.nan, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, np.nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],<br/>    'Temp': [25.1, 26.4, np.nan, 24.1, 24.7, 26.5, 27.6, 28.2, 27.1, 26.7, np.nan, 24.3, 23.1, 22.4, np.nan, 26.5, 28.6, np.nan, 27.0, 26.9],<br/>    'Humidity': [99.0, np.nan, 96.0, 68.0, 98.0, 98.0, 78.0, np.nan, 70.0, 75.0, np.nan, 77.0, 77.0, 89.0, 80.0, 88.0, 76.0, np.nan, 73.0, 73.0],<br/>    'Wind': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, np.nan, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, np.nan, 1.0, 0.0],<br/>    'Outlook': ['rainy', 'sunny', 'rainy', 'overcast', 'rainy', np.nan, 'rainy', 'rainy', 'overcast', 'sunny', np.nan, 'overcast', 'sunny', 'rainy', 'sunny', 'rainy', np.nan, 'rainy', 'overcast', 'sunny'],<br/>    'Crowdedness': [0.14, np.nan, 0.21, 0.68, 0.20, 0.32, 0.72, 0.61, np.nan, 0.54, np.nan, 0.67, 0.66, 0.38, 0.46, np.nan, 0.52, np.nan, 0.62, 0.81]<br/>}<br/><br/># Create a DataFrame from the dictionary<br/>df = pd.DataFrame(data)<br/><br/># Display basic information about the dataset<br/>print(df.info())<br/><br/># Display the first few rows of the dataset<br/>print(df.head())<br/><br/># Display the count of missing values in each column<br/>print(df.isnull().sum())</span></pre><p id="073e" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Output:</p><pre class="mr ms mt mu mv qk ob ql bp qm bb bk"><span id="cf1d" class="qn oj fq ob b bg qo qp l qq qr">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 20 entries, 0 to 19<br/>Data columns (total 8 columns):<br/> #   Column       Non-Null Count  Dtype  <br/>---  ------       --------------  -----  <br/> 0   Date         20 non-null     object <br/> 1   Weekday      20 non-null     int64  <br/> 2   Holiday      19 non-null     float64<br/> 3   Temp         16 non-null     float64<br/> 4   Humidity     17 non-null     float64<br/> 5   Wind         19 non-null     float64<br/> 6   Outlook      17 non-null     object <br/> 7   Crowdedness  15 non-null     float64<br/>dtypes: float64(5), int64(1), object(2)<br/>memory usage: 1.3+ KB<br/><br/>     Date  Weekday  Holiday  Temp  Humidity  Wind Outlook  Crowdedness<br/>0  08-01        0      0.0  25.1      99.0   0.0   rainy         0.14<br/>1  08-02        1      0.0  26.4       NaN   0.0   sunny          NaN<br/>2  08-03        2      0.0   NaN      96.0   0.0   rainy         0.21<br/>3  08-04        3      0.0  24.1      68.0   0.0   overcast      0.68<br/>4  08-05        4      NaN  24.7      98.0   0.0   rainy         0.20<br/><br/>Date           0<br/>Weekday        0<br/>Holiday        1<br/>Temp           4<br/>Humidity       3<br/>Wind           1<br/>Outlook        3<br/>Crowdedness    5<br/>dtype: int64</span></pre><p id="6fdd" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As we can see, our dataset contains 20 rows and 8 columns:</p><ul class=""><li id="86d0" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qs qc qd bk">Date: The date of the observation</li><li id="c22c" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qs qc qd bk">Weekday: Day of the week (0–6, where 0 is Monday)</li><li id="9c5c" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qs qc qd bk">Holiday: Boolean indicating if it’s a holiday (0 or 1)</li><li id="7d85" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qs qc qd bk">Temp: Temperature in Celsius</li><li id="ac15" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qs qc qd bk">Humidity: Humidity percentage</li><li id="58c7" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qs qc qd bk">Wind: Wind condition (0 or 1, possibly indicating calm or windy)</li><li id="46c0" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qs qc qd bk">Outlook: Weather outlook (sunny, overcast, or rainy)</li><li id="3454" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qs qc qd bk">Crowdedness: Percentage of course occupancy</li></ul><p id="2ee7" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And look at that! We’ve got missing values in every column except Date and Weekday. Perfect for our imputation party.</p><p id="1ef4" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now that we have our dataset loaded, let’s tackle these missing values with six different imputation methods. We’ll use a different strategy for each type of data.</p><h1 id="1262" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Method 1: Listwise Deletion</h1><p id="fa17" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Listwise deletion, also known as complete case analysis, involves removing entire rows that contain any missing values. This method is simple and preserves the distribution of the data, but it can lead to a significant loss of information if many rows contain missing values.</p><p id="2dea" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">👍 Common Use</strong>: Listwise deletion is often used when the number of missing values is small and the data is missing completely at random (MCAR). It’s also useful when you need a complete dataset for certain analyses that can’t handle missing values.</p><p id="94ab" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">In Our Case</strong>: We’re using listwise deletion for rows that have at least 4 missing values. These rows might not provide enough reliable information, and removing them can help us focus on the more complete data points. However, we’re being cautious and only removing rows with significant missing data to preserve as much information as possible.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qt"><img src="../Images/ac3d3f78bea999b4a77b406489df405b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bCzj355kcYp24366JT28tg.png"/></div></div></figure><pre class="mr ms mt mu mv qk ob ql bp qm bb bk"><span id="624b" class="qn oj fq ob b bg qo qp l qq qr"># Count missing values in each row<br/>missing_count = df.isnull().sum(axis=1)<br/><br/># Keep only rows with less than 4 missing values<br/>df_clean = df[missing_count &lt; 4].copy()</span></pre><p id="dbd7" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We’ve removed 2 rows that had too many missing values. Now let’s move on to imputing the remaining missing data.</p><h1 id="7390" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Method 2: Simple Imputation — Mean and Mode</h1><p id="78da" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Simple imputation involves replacing missing values with a summary statistic of the observed values. Common approaches include using the mean, median, or mode of the non-missing values in a column.</p><p id="d865" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">👍 Common Use</strong>: Mean imputation is often used for continuous variables when the data is missing at random and the distribution is roughly symmetric. Mode imputation is typically used for categorical variables.</p><p id="df73" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">In Our Case</strong>: We’re using mean imputation for Humidity and mode imputation for Holiday. For Humidity, assuming the missing values are random, the mean provides a reasonable estimate of the typical humidity. For Holiday, since it’s a binary variable (holiday or not), the mode gives us the most common state, which is a sensible guess for missing values.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qu"><img src="../Images/cb3cbf0666616f5ece9d7ebafe1801bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1h9pqdW0sy7CZF95kWW7CQ.png"/></div></div></figure><pre class="mr ms mt mu mv qk ob ql bp qm bb bk"><span id="7d02" class="qn oj fq ob b bg qo qp l qq qr"># Mean imputation for Humidity<br/>df_clean['Humidity'] = df_clean['Humidity'].fillna(df_clean['Humidity'].mean())<br/><br/># Mode imputation for Holiday<br/>df_clean['Holiday'] = df_clean['Holiday'].fillna(df_clean['Holiday'].mode()[0])</span></pre><h1 id="e0aa" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Method 3: Linear Interpolation</h1><p id="3fb3" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Linear interpolation estimates missing values by assuming a linear relationship between known data points. It’s particularly useful for time series data or data with a natural ordering.</p><p id="543e" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">👍 Common Use</strong>: Linear interpolation is often used for time series data, where missing values can be estimated based on the values before and after them. It’s also useful for any data where there’s expected to be a roughly linear relationship between adjacent points.</p><p id="9004" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">In Our Case</strong>: We’re using linear interpolation for Temperature. Since temperature tends to change gradually over time and our data is ordered by date, linear interpolation can provide reasonable estimates for the missing temperature values based on the temperatures recorded on nearby days.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qu"><img src="../Images/99ad8cebf6fd2d63877900c2f3b71339.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8njqYGKR6JxnIktGXAGG5w.png"/></div></div></figure><pre class="mr ms mt mu mv qk ob ql bp qm bb bk"><span id="dd08" class="qn oj fq ob b bg qo qp l qq qr">df_clean['Temp'] = df_clean['Temp'].interpolate(method='linear')</span></pre><h1 id="61fb" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Method 4: Forward/Backward Fill</h1><p id="040d" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Forward fill (or “last observation carried forward”) propagates the last known value forward to fill gaps, while backward fill does the opposite. This method assumes that the missing value is likely to be similar to the nearest known value.</p><p id="15b0" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">👍 Common Use</strong>: Forward/backward fill is often used for time series data, especially when the value is likely to remain constant until changed (like in financial data) or when the most recent known value is the best guess for the current state.</p><p id="9b0f" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">In Our Case</strong>: We’re using a combination of forward and backward fill for Outlook. Weather conditions often persist for several days, so it’s reasonable to assume that a missing Outlook value might be similar to the Outlook of the previous or following day.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qu"><img src="../Images/f76370e6ad62f0c847556ff0e1e83395.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YN4bTBtnjtXqrISXpPgGRg.png"/></div></div></figure><pre class="mr ms mt mu mv qk ob ql bp qm bb bk"><span id="c9c9" class="qn oj fq ob b bg qo qp l qq qr">df_clean['Outlook'] = df_clean['Outlook'].fillna(method='ffill').fillna(method='bfill')</span></pre><h1 id="5d15" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Method 5: Constant Value Imputation</h1><p id="5c37" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">This method involves replacing all missing values in a variable with a specific constant value. This constant could be chosen based on domain knowledge or a safe default value.</p><p id="676a" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">👍 Common Use</strong>: Constant value imputation is often used when there’s a logical default value for missing data, or when you want to explicitly flag that a value was missing (by using a value outside the normal range of the data).</p><p id="94ed" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">In Our Case</strong>: We’re using constant value imputation for the Wind column, replacing missing values with -1. This approach explicitly flags imputed values (since -1 is outside the normal 0–1 range for Wind) and it preserves the information that these values were originally missing.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qu"><img src="../Images/bc42e0bc88d6142a226e70ec4a52840e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S4ViI8_kiKIXgGV8FMFHuQ.png"/></div></div></figure><pre class="mr ms mt mu mv qk ob ql bp qm bb bk"><span id="0389" class="qn oj fq ob b bg qo qp l qq qr">df_clean['Wind'] = df_clean['Wind'].fillna(-1)</span></pre><h1 id="8da5" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Method 6: KNN Imputation</h1><p id="02a9" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk"><a class="af oc" rel="noopener" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1">K-Nearest Neighbors</a> (KNN) imputation estimates missing values by finding the K most similar samples in the dataset (just like KNN as Classification Algorithm) and using their values to impute the missing data. This method can capture complex relationships between variables.</p><p id="2730" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">👍 Common Use</strong>: KNN imputation is versatile and can be used for both continuous and categorical variables. It’s particularly useful when there are expected to be complex relationships between variables that simpler methods might miss.</p><p id="0cfd" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">In Our Case</strong>: We’re using KNN imputation for Crowdedness. Crowdedness likely depends on a combination of factors (like temperature, holiday status, etc.), and KNN can capture these complex relationships to provide more accurate estimates of missing crowdedness values.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qu"><img src="../Images/a053cca0cd994e3d358188a79e82229d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iRmxAVBhZcNZ186xx95zOA.png"/></div></div></figure><pre class="mr ms mt mu mv qk ob ql bp qm bb bk"><span id="51ad" class="qn oj fq ob b bg qo qp l qq qr">from sklearn.impute import KNNImputer<br/><br/># One-hot encode the 'Outlook' column<br/>outlook_encoded = pd.get_dummies(df_clean['Outlook'], prefix='Outlook')<br/><br/># Prepare features for KNN imputation<br/>features_for_knn = ['Weekday', 'Holiday', 'Temp', 'Humidity', 'Wind']<br/>knn_features = pd.concat([df_clean[features_for_knn], outlook_encoded], axis=1)<br/><br/># Apply KNN imputation<br/>knn_imputer = KNNImputer(n_neighbors=3)<br/>df_imputed = pd.DataFrame(knn_imputer.fit_transform(pd.concat([knn_features, df_clean[['Crowdedness']]], axis=1)),<br/>                          columns=list(knn_features.columns) + ['Crowdedness'])<br/><br/># Update the original dataframe with the imputed Crowdedness values<br/>df_clean['Crowdedness'] = df_imputed['Crowdedness']</span></pre><h1 id="37f7" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Conclusion: The Power of Choice (and Knowledge)</h1><p id="34a4" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">So, there you have it! Six different ways to handle missing values, all applied to our golf course dataset.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qv"><img src="../Images/512ea00ed33319c250f4dc0e98dc11f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D9d4d2JaqscJ_XKegqfUeg.png"/></div></div></figure><p id="c389" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s recap how each method tackled our data:</p><ol class=""><li id="8384" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qb qc qd bk"><strong class="ne ga">Listwise Deletion</strong>: Helped us focus on more complete data points by removing rows with extensive missing values.</li><li id="44b9" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Simple Imputation</strong>: Filled in Humidity with average values and Holiday with the most common occurrence.</li><li id="0c00" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Linear Interpolation</strong>: Estimated missing Temperature values based on the trend of surrounding days.</li><li id="a82f" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Forward/Backward Fill</strong>: Guessed missing Outlook values from adjacent days, reflecting the persistence of weather patterns.</li><li id="a7b4" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Constant Value Imputation</strong>: Flagged missing Wind data with -1, preserving the fact that these values were originally unknown.</li><li id="5854" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">KNN Imputation</strong>: Estimated Crowdedness based on similar days, capturing complex relationships between variables.</li></ol><p id="596d" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Each method tells a different story about our missing data, and the “right” choice depends on what we know about our golf course operations and what questions we’re trying to answer.</p><p id="9f4e" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The key takeaway? Don’t just blindly apply imputation methods. Understand your data, consider the context, and choose the method that makes the most sense for your specific situation.</p></div></div></div><div class="ab cb qw qx qy qz" role="separator"><span class="ra by bm rb rc rd"/><span class="ra by bm rb rc rd"/><span class="ra by bm rb rc"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="abaf" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py fw bk">⚠️ Warning: The Purpose and Limitations of Missing Value Imputation</h2><p id="1c14" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">While we’ve explored various imputation techniques, we need to understand their purpose and limitations:</p><ol class=""><li id="82b5" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qb qc qd bk"><strong class="ne ga">Not a Magic Solution</strong>: Imputation is not a cure-all for missing data. It’s a tool to make your data usable, <strong class="ne ga">not to create perfect data</strong>.</li><li id="693e" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Potential for Bias</strong>: Imputed values are educated guesses. They can introduce bias if not done carefully, especially if the data is Not Missing At Random (NMAR).</li><li id="e6e2" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Loss of Uncertainty</strong>: Most simple imputation methods don’t account for the uncertainty in the missing values, which can lead to overconfident models.</li><li id="21f7" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Data Distortion</strong>: Aggressive imputation can distort relationships in your data. Always check if imputation has significantly altered your data’s distribution or correlations.</li><li id="c6cb" class="nc nd fq ne b gt qe ng nh gw qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><strong class="ne ga">Document Your Process</strong>: Always clearly document your imputation methods. This transparency is crucial for reproducibility and for others to understand potential biases in your results.</li></ol><p id="876f" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Again, the goal of imputation is to make your data usable while minimizing bias and information loss. It’s not about creating perfect data, but about making the best use of the information you have. Always approach imputation with caution and critical thinking.</p><h1 id="dd41" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">🌟 Missing Value Imputation Summarized</h1><pre class="mr ms mt mu mv qk ob ql bp qm bb bk"><span id="e379" class="qn oj fq ob b bg qo qp l qq qr">import pandas as pd<br/>import numpy as np<br/>from sklearn.impute import KNNImputer<br/><br/># Create the dataset as a dictionary<br/>data = {<br/>    'Date': ['08-01', '08-02', '08-03', '08-04', '08-05', '08-06', '08-07', '08-08', '08-09', '08-10',<br/>             '08-11', '08-12', '08-13', '08-14', '08-15', '08-16', '08-17', '08-18', '08-19', '08-20'],<br/>    'Weekday': [0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5],<br/>    'Holiday': [0.0, 0.0, 0.0, 0.0, np.nan, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, np.nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],<br/>    'Temp': [25.1, 26.4, np.nan, 24.1, 24.7, 26.5, 27.6, 28.2, 27.1, 26.7, np.nan, 24.3, 23.1, 22.4, np.nan, 26.5, 28.6, np.nan, 27.0, 26.9],<br/>    'Humidity': [99.0, np.nan, 96.0, 68.0, 98.0, 98.0, 78.0, np.nan, 70.0, 75.0, np.nan, 77.0, 77.0, 89.0, 80.0, 88.0, 76.0, np.nan, 73.0, 73.0],<br/>    'Wind': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, np.nan, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, np.nan, 1.0, 0.0],<br/>    'Outlook': ['rainy', 'sunny', 'rainy', 'overcast', 'rainy', np.nan, 'rainy', 'rainy', 'overcast', 'sunny', np.nan, 'overcast', 'sunny', 'rainy', 'sunny', 'rainy', np.nan, 'rainy', 'overcast', 'sunny'],<br/>    'Crowdedness': [0.14, np.nan, 0.21, 0.68, 0.20, 0.32, 0.72, 0.61, np.nan, 0.54, np.nan, 0.67, 0.66, 0.38, 0.46, np.nan, 0.52, np.nan, 0.62, 0.81]<br/>}<br/><br/># Create a DataFrame from the dictionary<br/>df = pd.DataFrame(data)<br/><br/># 1. Listwise Deletion<br/>df_clean = df[df.isnull().sum(axis=1) &lt; 4].reset_index(drop=True).copy()<br/><br/># 2. Simple Imputation<br/>df_clean['Humidity'] = df_clean['Humidity'].fillna(df_clean['Humidity'].mean())<br/>df_clean['Holiday'] = df_clean['Holiday'].fillna(df_clean['Holiday'].mode()[0])<br/><br/># 3. Linear Interpolation<br/>df_clean['Temp'] = df_clean['Temp'].interpolate(method='linear')<br/><br/># 4. Forward/Backward Fill<br/>df_clean['Outlook'] = df_clean['Outlook'].ffill()<br/><br/># 5. Constant Value Imputation<br/>df_clean['Wind'] = df_clean['Wind'].fillna(-1)<br/><br/># 6. KNN Imputation<br/># One-hot encode the 'Outlook' column<br/>outlook_encoded = pd.get_dummies(df_clean['Outlook'], prefix='Outlook')<br/><br/># Prepare features for KNN imputation<br/>features_for_knn = ['Weekday', 'Holiday', 'Temp', 'Humidity', 'Wind']<br/>knn_features = pd.concat([df_clean[features_for_knn], outlook_encoded], axis=1)<br/><br/># Apply KNN imputation<br/>knn_imputer = KNNImputer(n_neighbors=3)<br/>df_imputed = pd.DataFrame(knn_imputer.fit_transform(pd.concat([knn_features, df_clean[['Crowdedness']]], axis=1)),<br/>                          columns=list(knn_features.columns) + ['Crowdedness'])<br/><br/># Update the original dataframe with the imputed Crowdedness values<br/>df_clean['Crowdedness'] = df_imputed['Crowdedness'].round(2)<br/><br/>print("Before:")<br/>print(df)<br/><br/>print("\n\nAfter:")<br/>print(df_clean)</span></pre></div></div></div><div class="ab cb qw qx qy qz" role="separator"><span class="ra by bm rb rc rd"/><span class="ra by bm rb rc rd"/><span class="ra by bm rb rc"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="d2c3" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py fw bk">Further Reading</h2><p id="18b3" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">For a detailed explanation of the <a class="af oc" href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html" rel="noopener ugc nofollow" target="_blank">KNNImputer</a> and its implementation in scikit-learn, readers can refer to the official documentation, which provides comprehensive information on its usage and parameters.</p><h2 id="a1b0" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py fw bk">Technical Environment</h2><p id="311d" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions.</p><h2 id="d319" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py fw bk">About the Illustrations</h2><p id="96db" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p></div></div><div class="mw"><div class="ab cb"><div class="lr re ls rf lt rg cf rh cg ri ci bh"><figure class="mr ms mt mu mv mw rk rl paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp rj"><img src="../Images/81e368a2e5f6c346e3d721bd30cf1575.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*uvcFHNK3dWAqEjcbLsZCRQ.jpeg"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For a concise visual summary of Missing Values Imputation, check out <a class="af oc" href="https://www.instagram.com/p/C_Kh78qSzoz/" rel="noopener ugc nofollow" target="_blank">the companion Instagram post.</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5043" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Reference</h1><p id="e1d4" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">[1] T. M. Mitchell, <a class="af oc" href="https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html" rel="noopener ugc nofollow" target="_blank">Machine Learning</a> (1997), McGraw-Hill Science/Engineering/Math, pp. 59</p><p id="cb55" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘿𝙖𝙩𝙖 𝙋𝙧𝙚𝙥𝙧𝙤𝙘𝙚𝙨𝙨𝙞𝙣𝙜 𝙢𝙚𝙩𝙝𝙤𝙙𝙨 𝙝𝙚𝙧𝙚:</p><div class="rm rn ro rp rq"><div role="button" tabindex="0" class="ab bx cp kj it rr rs bp rt lw ao"><div class="ru l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rv rw cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rv rw em n ay uh"/></div><div class="rx l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----93e0726284eb--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sa hp l"><h2 class="bf ga xd ic it xe iv iw xf iy ja fz bk">Data Preprocessing</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xg wf wg wh wi lj wj wk us ii wl wm wn uw ux uy ep bm uz oe" href="https://medium.com/@samybaladram/list/data-preprocessing-17a2c49b44e4?source=post_page-----93e0726284eb--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xh l il"><span class="bf b dy z dx">6 stories</span></div></div></div><div class="sj dz sk it ab sl il ed"><div class="ed sd bx se sf"><div class="dz l"><img alt="" class="dz" src="../Images/f7ead0fb9a8dc2823d7a43d67a1c6932.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*T1bcJ8sv5Rc1lsOyGS1nig.png"/></div></div><div class="ed sd bx kk sg sh"><div class="dz l"><img alt="Cartoon illustration of two figures embracing, with letters ‘A’, ‘B’, ‘C’ and numbers ‘1’, ‘2’, ‘3’ floating around them. A pink heart hovers above, symbolizing affection. The background is a pixelated pattern of blue and green squares, representing data or encoding. This image metaphorically depicts the concept of encoding categorical data, where categories (ABC) are transformed into numerical representations (123)." class="dz" src="../Images/72bb3a287a9ca4c5e7a3871e234bcc4b.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*2_cXKHvfaBTVpDrmz5r5vQ.png"/></div></div><div class="ed bx hx si sh"><div class="dz l"><img alt="A cartoon illustration representing data scaling in machine learning. A tall woman (representing a numerical feature with a large range) is shown shrinking into a child (representing the same feature after scaling to a smaller range). A red arrow indicates the shrinking process, and yellow sparkles around the child signify the positive impact of scaling." class="dz" src="../Images/d261b2c52a3cafe266d1962d4dbabdbd.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*MkX5TTTS1oZhY2eW6AdEkg.png"/></div></div></div></div></div><p id="595f" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:</p><div class="rm rn ro rp rq"><div role="button" tabindex="0" class="ab bx cp kj it rr rs bp rt lw ao"><div class="ru l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rv rw cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rv rw em n ay uh"/></div><div class="rx l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----93e0726284eb--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sa hp l"><h2 class="bf ga xd ic it xe iv iw xf iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xg wf wg wh wi lj wj wk us ii wl wm wn uw ux uy ep bm uz oe" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----93e0726284eb--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xh l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="sj dz sk it ab sl il ed"><div class="ed sd bx se sf"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed sd bx kk sg sh"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx si sh"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div><div class="rm rn ro rp rq"><div role="button" tabindex="0" class="ab bx cp kj it rr rs bp rt lw ao"><div class="ru l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rv rw cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rv rw em n ay uh"/></div><div class="rx l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----93e0726284eb--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sa hp l"><h2 class="bf ga xd ic it xe iv iw xf iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xg wf wg wh wi lj wj wk us ii wl wm wn uw ux uy ep bm uz oe" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----93e0726284eb--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xh l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="sj dz sk it ab sl il ed"><div class="ed sd bx se sf"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This “dummy” doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed sd bx kk sg sh"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx si sh"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>