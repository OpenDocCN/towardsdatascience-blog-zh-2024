<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Revolutionizing Large Dataset Feature Selection with Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Revolutionizing Large Dataset Feature Selection with Reinforcement Learning</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-for-feature-selection-be1e7eeb0acc?source=collection_archive---------2-----------------------#2024-05-19">https://towardsdatascience.com/reinforcement-learning-for-feature-selection-be1e7eeb0acc?source=collection_archive---------2-----------------------#2024-05-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="788e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Leverage the power of reinforcement learning for feature selection when faced with very large datasets</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@baptistelefort?source=post_page---byline--be1e7eeb0acc--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Baptiste Lefort" class="l ep by dd de cx" src="../Images/f57c4077afbff9939521032fa19b7f10.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*1-m7k3ynELLA8Ymu66a7LA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--be1e7eeb0acc--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@baptistelefort?source=post_page---byline--be1e7eeb0acc--------------------------------" rel="noopener follow">Baptiste Lefort</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--be1e7eeb0acc--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">5</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="c4cb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Discover how reinforcement learning transforms feature selection for machine learning models. Learn the process, implementation, and benefits of this innovative approach with practical examples and a dedicated Python library.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/743ea5c0887670d49bff360b95359e4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b68CNWWKPObifME2r22NSw.jpeg"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Photo from Jared Murray on <a class="af ny" href="https://unsplash.com/photos/NSuufgf-BME" rel="noopener ugc nofollow" target="_blank">Unplash</a></figcaption></figure><p id="3ebd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Feature selection is a determining step in the process of building a machine learning model.</strong> Selecting the good features for the model and the task that we want to achieve can definitely improve the performances. Indeed, a feature can add some noise and then disturb the model.</p><p id="82c1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Also, selecting the features is especially more important if we are dealing with high-dimensional data set. It enables the model to learn faster and better. The idea is then to find the optimal number of features and the most meaningful ones.</p><p id="98bb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this article I will tackle this problem and <strong class="ml fr">go beyond by introducing a newly implemented method for feature selection</strong>. Although it exists many different feature selection processes they will not be introduced here since a lot of articles are already dealing with them. I will focus on the feature selection using the reinforcement learning strategy.</p><p id="50ef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First, the reinforcement learning and more especially the Markov Decision Process will be addressed. It is a very new approach in the data science domain and more especially for feature selection purpose. After, I will introduce an implementation of this and how to install and use the python library (<strong class="ml fr">FSRLearning</strong>). Finally, I will prove the efficiency of this implementation. Among the possible feature selection approaches like wrappers or filtering, <strong class="ml fr">the reinforcement learning is the most powerful and efficient</strong>.</p><p id="16f4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The goal of this article is to emphasise on the implementation for concret and real-problem oriented utilisation. The theoretical aspect of this problem will be simplified with examples although some references will be available at the end.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="cc97" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">Reinforcement Learning : The Markov Decision Problem for feature selection</h2><p id="b794" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">It has been demonstrated that reinforcement learning (RL) technics can be very efficient for problems like game solving. The concept of RL is based on Markovian Decision Process (MDP). The point here is not to define deeply the MDP but to get the general idea of how it works and how it can be useful to our problem.</p><p id="225b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The naive idea behind RL is that an agent starts in an unknown environnement. This agent has to take actions to complete a task. In function of the current state of the agent and the action he has selected previously, the agent will be more inclined to choose some actions. At every new state reached and action taken, the agent receives a reward. Here are then the main parameters that we need to define for feature selection purpose:</p><ul class=""><li id="a7fe" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ph pi pj bk">What is a state ?</li><li id="8f67" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk">What is an action ?</li><li id="b6c3" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk">What are the rewards ?</li><li id="2223" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk">How do we choose an action ?</li></ul><p id="27b6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Firstly, the state is merely a subset of features that exist in the data set. For example, if the data set has three features (Age, Gender, Height) plus one label, here will be the possible states:</p><pre class="ni nj nk nl nm pp pq pr ps ay pt bk"><span id="4b38" class="oh oi fq pq b hw pu pv l im pw">[]                                              --&gt; Empty set                           <br/>[Age], [Gender], [Height]                       --&gt; 1-feature set<br/>[Age, Gender], [Gender, Height], [Age, Height]  --&gt; 2-feature set<br/>[Age, Gender, Height]                           --&gt; All-feature set</span></pre><p id="4fa0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In a state, the order of the features does not matter and it will be explained why a little bit later in the article. We have to consider it as a set and not a list of features.</p><p id="aee7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Concerning the actions, from a subset we can go to any other subset with one not-already explored feature more than the current state. In the feature selection problem, an action is then selecting a not-already explored feature in the current state and add it to the next state. Here is a sample of possible actions:</p><pre class="ni nj nk nl nm pp pq pr ps ay pt bk"><span id="6c4d" class="oh oi fq pq b hw pu pv l im pw">[Age] -&gt; [Age, Gender]<br/>[Gender, Height] -&gt; [Age, Gender, Height]</span></pre><p id="c6bb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here is an example of impossible actions:</p><pre class="ni nj nk nl nm pp pq pr ps ay pt bk"><span id="30bd" class="oh oi fq pq b hw pu pv l im pw">[Age] -&gt; [Age, Gender, Height]<br/>[Age, Gender] -&gt; [Age]<br/>[Gender] -&gt; [Gender, Gender]</span></pre><p id="997f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We have defined the states and the actions but not the reward. The reward is a real number that is used for evaluating the quality of a state. For example if a robot is trying to reach the exit of a maze and decides to go to the exit as his next action, then the reward associated to this action will be “good”. If he selects as a next action to go in a trap then the reward will be “not good”. The reward is a value that brought information about the previous action taken.</p><p id="fc02" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the problem of feature selection an interesting reward could be a value of accuracy that is added to the model by adding a new feature. Here is an example of how the reward is computed:</p><pre class="ni nj nk nl nm pp pq pr ps ay pt bk"><span id="c498" class="oh oi fq pq b hw pu pv l im pw">[Age] --&gt; Accuracy = 0.65<br/>[Age, Gender] --&gt; Accuracy = 0.76<br/>Reward(Gender) = 0.76 - 0.65 = 0.11</span></pre><p id="825b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For each state that we visit for the first time a classifier will be trained with the set of features. This value is stored in the state and the training of the classifier, which is very costly, will only happens once even if the state is reached again later. The classifier does not consider the order of the feature. This is why we can see this problem as a graph and not a tree. In this example, the reward of the action of selecting Gender as a new feature for the model is the difference between the accuracy of the current state and the next state.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng px"><img src="../Images/10e93339802fdda3ba8d0913277d4826.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*OFU-FKnhjUgzh7DcqNhajg.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Each state has several possible actions and rewards associated (Image by the author)</figcaption></figure><p id="6aa4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">On the graph above, each feature has been mapped to a number (i.e “Age” is 1, “Gender” is 2 and “Height” is 3). It is totally possible to take other metrics to maximise to find the optimal set. In many business applications the recall is more considered than the accuracy.</p><p id="6f11" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The next important question is how do we select the next state from the current state or how do we explore our environement. We have to find the most optimal way to do it since it can quickly become a very complex problem. Indeed, if we naively explore all the possible set of features in a problem with 10 features, the number of states would be</p><pre class="ni nj nk nl nm pp pq pr ps ay pt bk"><span id="1084" class="oh oi fq pq b hw pu pv l im pw">10! + 2 = 3 628 802 possible states</span></pre><p id="4dd1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The +2 is because we consider an empty state and a state that contains all the possible features. In this problem we would have to train the same model on all the states to get the set of features that maximises the accuracy. In the RL approach we will not have to go in all the states and to train a model every time that we go in an already visited state.</p><p id="0707" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We had to determine some stop conditions for this problem and they will be detailed later. For now the epsilon-greedy state selection has been chosen. The idea is from a current state we select the next action randomly with a probability of epsilon (between 0 and 1 and often around 0.2) and otherwise select the action that maximises a function. For feature selection the function is the average of reward that each feature has brought to the accuracy of the model.</p><p id="547c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The epsilon-greedy algorithm implies two steps:</p><ol class=""><li id="5e2c" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne py pi pj bk">A random phase : with a probability epsilon, we select randomly the next state among the possible neighbours of the current state (we can imagine either a uniform or a softmax selection)</li><li id="1c3d" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne py pi pj bk">A greedy phase : we select the next state such that the feature added to the current state has the maximal contribution of accuracy to the model. To reduce the time complexity, we have initialised a list containing this values for each feature. This list is updated every time that a feature is chosen. The update is very optimal thanks to the following formula:</li></ol><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng pz"><img src="../Images/fe8b18d529027bc03b6f4c32265f2db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xoEmO0GTRsevO_tqAJLcFA.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Update of the average of reward list for each feature (Image by the author)</figcaption></figure><ul class=""><li id="057f" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ph pi pj bk"><em class="qa">AORf</em> : Average of reward brought by the feature “f”</li><li id="2796" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk"><em class="qa">k</em> : number of times that “f” has been selected</li><li id="91c4" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk"><em class="qa">V(F)</em> : state’s value of the set of features F (not detailed in this article for clarity reasons)</li></ul><p id="693c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The global idea is to find which feature has brought the most accuracy to the model. That is why we need to browse different states to evaluate in many different environments the most global accurate value of a feature for the model.</p><p id="d552" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally I will detail the two stop conditions. Since the goal is to minimise the number of state that the algorithm visits we need to be careful about them. The less never visited state we visit, the less amount of models we will have to train with different set of features. Training the model to get the accuracy is the most costly phase in terms of time and computation power.</p><ol class=""><li id="da7a" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne py pi pj bk">The algorithm stops in any case in the final state which is the set containing all the features. We want to avoid reaching this state since it is the most expensive to train a model with.</li><li id="7769" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne py pi pj bk">Also, it stops browsing the graph if a sequence of visited states see their values degrade successively. A threshold has been set such that after square root of the number of total features in the dataset, it stops exploring.</li></ol><p id="64b5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now that the modelling of the problem has been explained, we will detail the implementation in python.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="31ad" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">The python library for Feature Selection with Reinforcement Learning</h2><p id="3294" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">A python library resolving this problem is available. I will explain in this part how it works and prove that it is an efficient strategy. Also, this article stands as a documentation and you will be able to use this library for your projects by the end of the part.</p><h2 id="ac3e" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">1. The data pre-processing</h2><p id="539c" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">Since we need to evaluate the accuracy of a state that is visited, we need to feed a model with the features and the data used for this feature selection task. The data has to been normalised, the categorical variables encoded and have as few rows as possible (the smallest it is, the fastest the algorithm will be). Also, it’s very important to create a mapping between the features and some integers as explained in the previous part. This step is not mandatory but very recommended. The final result of this step is to get a DataFrame with all the features and another with the labels to predict. Below is an example with a dataset used as a benchmark (it can be found here <a class="af ny" href="https://archive.ics.uci.edu/ml/datasets/statlog+(australian+credit+approval)" rel="noopener ugc nofollow" target="_blank">UCI Irvine Machine Learning Repository</a>).</p><figure class="ni nj nk nl nm nn"><div class="qb io l ed"><div class="qc qd l"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Process the data</figcaption></figure><h2 id="0563" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">2. Installation and importation of the FSRLearning library</h2><p id="5ebe" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">The second step is to install the library with pip. Here is the command to install it:</p><pre class="ni nj nk nl nm pp pq pr ps ay pt bk"><span id="7731" class="oh oi fq pq b hw pu pv l im pw">pip install FSRLearning</span></pre><p id="ecbf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To import the library the following code can be used:</p><figure class="ni nj nk nl nm nn"><div class="qb io l ed"><div class="qc qd l"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Import the library</figcaption></figure><p id="bfda" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You will be able to create a feature selector simply by creating an object Feature_Selector_RL. Some parameters need to be filled in.</p><ul class=""><li id="98a5" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ph pi pj bk"><strong class="ml fr"><em class="qa">feature_number</em></strong> (integer) : number of features in the DataFrame X</li><li id="b7c6" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk"><strong class="ml fr"><em class="qa">feature_structure</em></strong> (dictionary) : dictionary for the graph implementation</li><li id="18fe" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk"><strong class="ml fr"><em class="qa">eps</em></strong> (float [0; 1]) : probability of choosing a random next state, 0 is an only greedy algorithm and 1 only random</li><li id="ae35" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk"><strong class="ml fr"><em class="qa">alpha</em></strong> (float [0; 1]): control the rate of updates, 0 is a very not updating state and 1 a very updated</li><li id="ac56" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk"><strong class="ml fr"><em class="qa">gamma</em></strong> (float [0, 1]): factor of moderation of the observation of the next state, 0 is a shortsighted condition and 1 it exhibits farsighted behavior</li><li id="e932" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk"><strong class="ml fr"><em class="qa">nb_iter</em></strong> (int): number of sequences to go through the graph</li><li id="fc0e" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk"><strong class="ml fr"><em class="qa">starting_state</em></strong> (“empty” or “random”) : if “empty”, the algorithm starts from the empty state and if “random”, the algorithm starts from a random state in the graph</li></ul><p id="ae50" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">All the parameters can be tuned but for most of the problem only few iterations can be good (around 100) and the epsilon value around 0.2 is often enough. The starting state is useful to browse the graph more efficiently but it can be very dependent on the dataset and both of the values can be tested.</p><p id="8911" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally we can initialise very simply the selector with the following code:</p><figure class="ni nj nk nl nm nn"><div class="qb io l ed"><div class="qc qd l"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Selector object initialisation</figcaption></figure><p id="a534" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Training the algorithm is very easy on the same basis than most of the ML library:</p><figure class="ni nj nk nl nm nn"><div class="qb io l ed"><div class="qc qd l"/></div></figure><p id="1503" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here is an example of the output:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qe"><img src="../Images/6ca11e2f53ddc0befd561002a7b034fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5sm88Jq-Q6A58q0faZR78Q.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Output of the selector (Image by the author)</figcaption></figure><p id="3c56" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The output is a 5-tuple as follows:</p><ul class=""><li id="0a9b" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ph pi pj bk">Index of the features in the DataFrame X (like a mapping)</li><li id="340e" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk">Number or times that the feature has been observed</li><li id="281c" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk">Average of reward brought by the feature after all the iterations</li><li id="bb3b" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk">Ranking of the features from the least to the most important (here 2 is the least and 7 the most important feature)</li><li id="c85f" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk">Number of states globally visited</li></ul><p id="5f57" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Another important method of this selector is to get a comparison with the RFE selector of Scikit-Learn. It takes as input X, y and the results of the selector.</p><figure class="ni nj nk nl nm nn"><div class="qb io l ed"><div class="qc qd l"/></div></figure><p id="20d5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The output is a print after each step of selection of the global metric of RFE and FSRLearning. It also outputs a visual comparison of the accuracy of the model with on the x-axis the number of features selected and on the y-axis the accuracy. The two horizontal lines are the median of accuracy for each method. Here is an example:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qf"><img src="../Images/b74a30d601f80f8b0a2970d80f8808a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*sq5MIir3iKNpDWXM4MatwA.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Comparison between RL and RFE method (Image by the author)</figcaption></figure><pre class="ni nj nk nl nm pp pq pr ps ay pt bk"><span id="a95c" class="oh oi fq pq b hw pu pv l im pw">Average benchmark accuracy : 0.854251012145749, rl accuracy : 0.8674089068825909 <br/>Median benchmark accuracy : 0.8552631578947368, rl accuracy : 0.868421052631579 <br/>Probability to get a set of variable with a better metric than RFE : 1.0 <br/>Area between the two curves : 0.17105263157894512</span></pre><p id="7094" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this example the RL method has always given a better set of features for the model than RFE. We can then select with certainty among the sorted set of features any subset and it will give a better accuracy to the model. We can run several times the model and the comparator to get a very accurate estimation but the RL method is always better.</p><p id="d90c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Another interesting method is the get_plot_ratio_exploration. It plots a graph comparing the number of already visited nodes and visited nodes in a sequence for a precise iteration.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qg"><img src="../Images/eaa6224563da601d84a3e70911b0e8f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*cYwC3LzrMa_6VVkB4Z8Gcg.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Comparison of visited and not visited state at each iteration (Image by the author)</figcaption></figure><p id="c405" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Also, thanks to the second stop condition the time complexity of the algorithm decreases exponentially. Then even if the number of feature is big the convergence will be found quickly. The plot bellow is the number of times that a set of a certain size has been visited.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qg"><img src="../Images/313c55b8da35476e077b1f932fa3529c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*YZYC1bcU1vOcSlq6DjZD-w.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Number of visited states in function of their size (Image by the author)</figcaption></figure><p id="4a98" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In all iterations the algorithm visited a state containing 6 variables or less. Beyond 6 variables we can see that the number of state reached is decreasing. It’s a good behaviour since it is faster to train a model with small set of features than the big ones.</p><h1 id="37c1" class="qh oi fq bf oj qi qj gq on qk ql gt or qm qn qo qp qq qr qs qt qu qv qw qx qy bk">Conclusion and references</h1><p id="c0bf" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">Overall, we can see that the RL method is very efficient for maximising a metric of a model. It always converges quickly toward an interesting subset of features. Also, this method is very easy and fast to implement in ML projects with the FSRLearning library.</p><p id="972a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The github repository of the project with a complete documentation is available <a class="af ny" href="https://github.com/blefo/FSRLearning" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="ff1e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you wish to contact me, you can do so directly on linkedin <a class="af ny" href="https://www.linkedin.com/in/baptistelefort/" rel="noopener ugc nofollow" target="_blank">here</a></p><p id="8729" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This library has been implemented with the help of these two articles:</p><ul class=""><li id="8d72" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ph pi pj bk">Sali Rasoul, Sodiq Adewole and Alphonse Akakpo, <a class="af ny" href="https://arxiv.org/pdf/2101.09460.pdf" rel="noopener ugc nofollow" target="_blank">FEATURE SELECTION USING REINFORCEMENT LEARNING</a> (2021), ArXiv</li><li id="2a46" class="mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne ph pi pj bk">Seyed Mehdin Hazrati Fard, Ali Hamzeh and Sattar Hashemi, <a class="af ny" href="https://www.sciencedirect.com/science/article/pii/S0898122113004495" rel="noopener ugc nofollow" target="_blank">Using reinforcement learning to find an optimal set of features</a> (2013), ScienceDirect</li></ul></div></div></div></div>    
</body>
</html>