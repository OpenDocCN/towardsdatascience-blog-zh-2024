<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Fine-tune an Instruct model over raw text data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Fine-tune an Instruct model over raw text data</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tune-an-instruct-model-over-raw-text-data-6db654e7e2ed?source=collection_archive---------1-----------------------#2024-03-26">https://towardsdatascience.com/fine-tune-an-instruct-model-over-raw-text-data-6db654e7e2ed?source=collection_archive---------1-----------------------#2024-03-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="42c2" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Fine-tune a modern chatbot with minimal conversational data for under $10</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@jon.flynn2?source=post_page---byline--6db654e7e2ed--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jon Flynn" class="l ep by dd de cx" src="../Images/492cef280f4ea0b002e5d00ad2e083a5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*16AI0ZxosqDanJ22tGkA_Q.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6db654e7e2ed--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@jon.flynn2?source=post_page---byline--6db654e7e2ed--------------------------------" rel="noopener follow">Jon Flynn</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6db654e7e2ed--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/dd30580c003c480f51f17dc52b585116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BkatUZ015F_EhBpHlEZZAA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><h1 id="7200" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk"><strong class="al">Purpose</strong></h1><p id="23b0" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">Getting a modern chatbot to uphold it’s capabilities on your own data remains a complex task. Context window sizes are increasing rapidly with leading products like Gemini 1.5 Pro’s and Claude 3’s big leap to a 1 million token capacity. However, a company like The Guardian, where I currently work, has countless code repositories containing hundreds of millions of tokens worth of data.</p><p id="5de0" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">The <a class="af oy" href="https://twitter.com/cognition_labs/status/1767548763134964000" rel="noopener ugc nofollow" target="_blank">recently announced Devin</a> by Cognition Labs likely uses clever RAG techniques to complete it’s tasks, but relying on injecting all information into the context window can be problematic. The consensus in the community seems to be that GPT-4 128k can retain great performance for up to around 60K tokens, which isn’t a lot. Even then, retaining the great performance requires better and trickier prompting as the amount of tokens grow. Because of these limitations, it seems likely that the most capable models in the near future will use a combination of good prompting, RAG and fine-tuning. For example, for a code assistant tool, the most recent code could be retrieved through a RAG pipeline. A fine-tuned model could then analyse and reason about this code more effectively than a non fine-tuned model, pointing out any edge cases and risks it may have learned from elsewhere. Additionally, the fine-tuned model would adopt the organisation’s coding conventions and best practices, allowing it to provide more insightful guidance to employees.</p><p id="222c" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">I found limited resources online about high-performing chatbots fine-tuned on smaller datasets. Instead, most research introduces models like <a class="af oy" href="https://arxiv.org/abs/2402.10373" rel="noopener ugc nofollow" target="_blank">BioMistral</a>, which achieve success using large 3 billion token datasets, requiring significant budget and expertise.</p><p id="6f6e" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">This experiment seeks to discover a lighter approach that navigates between the constraints of a 128K context window and the complexities of a model fine-tuned on billions of tokens, perhaps more in the realm of tens of millions of tokens. For a smaller-scale test, I’ll fine-tune Mistral’s <a class="af oy" href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2" rel="noopener ugc nofollow" target="_blank">7B Instruct v0.2 model</a> on <a class="af oy" href="https://github.com/guardian/manage-frontend" rel="noopener ugc nofollow" target="_blank">The Guardian’s manage-frontend repository</a> (the dataset being 1.6 million tokens).</p><p id="beaa" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">The goal of this article was to create a reproducible set of instructions for cost-effective model fine-tuning using easily accessible hardware. Emphasis was placed on ease of use, minimizing trial and error, and maximizing the use of raw text data over labeled conversational data. Hopefully any software developer, with zero experience in deep learning engineering, can pick up <a class="af oy" href="https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=eWE0W7YSVTmx" rel="noopener ugc nofollow" target="_blank">the notebook</a> and train their own model with ease.</p><p id="0770" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">I’ll outline the data used, highlight the best hyperparameters and their results, then conclude with a technical explanation for their effectiveness.</p><h1 id="3c1c" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk"><strong class="al">Training</strong></h1><h2 id="8e98" class="oz nc fq bf nd pa pb pc ng pd pe pf nj og pg ph pi ok pj pk pl oo pm pn po pp bk"><strong class="al">A100 40GB</strong></h2><p id="c3de" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">I used a Nvidia A100 40GB from Colab for all training except for one run where I used an H100 80GB.</p><h2 id="68a6" class="oz nc fq bf nd pa pb pc ng pd pe pf nj og pg ph pi ok pj pk pl oo pm pn po pp bk">Unsloth</h2><p id="a13f" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">I used the Unsloth library for faster and more memory efficient training. <a class="af oy" href="https://huggingface.co/blog/unsloth-trl" rel="noopener ugc nofollow" target="_blank">This blog post</a> gives a good summary on how the <a class="af oy" href="https://github.com/unslothai/unsloth" rel="noopener ugc nofollow" target="_blank">Unsloth library</a> works under the hood and shows benchmarks for training speed increases and memory saving.</p><h2 id="388f" class="oz nc fq bf nd pa pb pc ng pd pe pf nj og pg ph pi ok pj pk pl oo pm pn po pp bk">Differences in training approach to start of the art fine-tuned models</h2><p id="1128" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">Modern examples of fine-tuning to teach a model new domain-specific knowledge include <a class="af oy" href="https://arxiv.org/abs/2402.10373" rel="noopener ugc nofollow" target="_blank">BioMistral</a> and <a class="af oy" href="https://www.stochastic.ai/blog/xfinance-vs-bloomberg-gpt" rel="noopener ugc nofollow" target="_blank">xFinance</a>. xFinance continues the pre-training of the Llama 7B base model, i.e.: the non-instruct version. It uses LoRA. The model is first trained on over 216,626 documents, totalling 236 million tokens. It is then further fine-tuned on 25,000 samples of finance-based conversational data. Similar to standard chatbot training, this approach begins with training on raw text data, lacking instruction tokens or structured conversational elements, and then transitions to training over exclusively conversational data. BioMistral takes a similar approach, though interestingly it starts fine-tuning off the Mistral 7B Instruct v0.2 model.</p><p id="b9f6" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">My approach combines both the raw dataset and the annotated dataset in the same training run as this approach produced the best results. Only one training run is done.</p><h2 id="062d" class="oz nc fq bf nd pa pb pc ng pd pe pf nj og pg ph pi ok pj pk pl oo pm pn po pp bk">TRL’s SFTtrainer</h2><p id="01fd" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">I used the <code class="cx pq pr ps pt b"><a class="af oy" href="https://huggingface.co/docs/trl/en/sft_trainer" rel="noopener ugc nofollow" target="_blank">SFTtrainer</a></code> from the <code class="cx pq pr ps pt b"><a class="af oy" href="https://huggingface.co/docs/trl/en/index" rel="noopener ugc nofollow" target="_blank">trl</a></code> library. I saw it was used in <a class="af oy" href="https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing" rel="noopener ugc nofollow" target="_blank">this Unsloth demo notebook</a> with good results. This is a wrapper over the default HuggingFace trainer. I couldn’t find much documentation on how the SFTtrainer extends it, and the code suggests minimal changes. It appears to prepare the dataset for training by setting target labels identical to input_ids (<a class="af oy" href="https://github.com/huggingface/trl/blob/main/trl/trainer/utils.py#L477-L480" rel="noopener ugc nofollow" target="_blank">see these lines of code</a>). It sets the target <code class="cx pq pr ps pt b">labels</code> to be the same as the <code class="cx pq pr ps pt b">input_ids</code>. <a class="af oy" href="https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb" rel="noopener ugc nofollow" target="_blank">Here’s an example of a notebook</a> doing the same thing with the default HuggingFace trainer. This just boils down to next token prediction with cross-entropy loss using the default trainer provided by HuggingFace, nothing fancy. The only difference in training between the “raw text data” and conversational data are the addition of the special instruction tokens “[INST]” and “[/INST]” that Mistral Instruct has been trained to recognise. Refer to the cell outputs in <a class="af oy" href="https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=sDu17ZImsdVK" rel="noopener ugc nofollow" target="_blank">the notebook</a> to see what the dataset looks like.</p><h1 id="7a25" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk">Creating the raw dataset</h1><p id="116f" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">My raw dataset consists of the repo’s wiki, a snapshot of the main branch from December, and the last 100 pull requests including comments and code changes. I chunked it so each sample was max 8192 tokens.</p><h2 id="9a05" class="oz nc fq bf nd pa pb pc ng pd pe pf nj og pg ph pi ok pj pk pl oo pm pn po pp bk"><strong class="al">Scraping the wiki</strong></h2><p id="1941" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">I just copied and pasted each page into a text file for this</p><h2 id="6a59" class="oz nc fq bf nd pa pb pc ng pd pe pf nj og pg ph pi ok pj pk pl oo pm pn po pp bk"><strong class="al">Scraping the codebase</strong></h2><p id="76fa" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">I wrote a Python script that ran locally and wrote all files to a text file in the following format:</p><pre class="ml mm mn mo mp pu pt pv bp pw bb bk"><span id="85e3" class="px nc fq pt b bg py pz l qa qb">- File: productSwitchTypes.ts<br/>  Content:<br/>export type ProductSwitchType =<br/> | 'to-recurring-contribution'<br/> | 'recurring-contribution-to-supporter-plus';<br/><br/>export interface PreviewResponse {<br/> amountPayableToday: number;<br/> supporterPlusPurchaseAmount: number;<br/> contributionRefundAmount: number;<br/> nextPaymentDate: string;<br/> checkChargeAmountBeforeUpdate: boolean;<br/>}<br/><br/><br/>- File: productTypes.ts<br/>  Content:<br/>...<br/>...<br/>...</span></pre><h2 id="2979" class="oz nc fq bf nd pa pb pc ng pd pe pf nj og pg ph pi ok pj pk pl oo pm pn po pp bk"><strong class="al">Scraping PR data</strong></h2><p id="a2e2" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk"><a class="af oy" href="https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=kssax8bg5OWS" rel="noopener ugc nofollow" target="_blank">The corresponding cell in the Colab notebook</a> will produce an output like so for <a class="af oy" href="https://github.com/octocat/Hello-World/pull/2989" rel="noopener ugc nofollow" target="_blank">this PR</a>:</p><pre class="ml mm mn mo mp pu pt pv bp pw bb bk"><span id="382f" class="px nc fq pt b bg py pz l qa qb">PR #2989: Create devcontainer.json<br/>URL: https://github.com/octocat/Hello-World/pull/2989<br/>Description: None<br/>Created at: 2024-02-26T11:39:03Z<br/>Merged at: None<br/>File: .devcontainer/devcontainer.json, Status: added<br/>Changes: @@ -0,0 +1,5 @@<br/>+{<br/>+  "image": "mcr.microsoft.com/devcontainers/universal:2",<br/>+  "features": {<br/>+  }<br/>+}</span></pre><h1 id="b3f3" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk">Generating conversational data</h1><p id="3b24" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">Despite the title of this article, I did use a bit of labeled conversational data, but it is synthetically and easily generated. This doesn’t match the quality of carefully curated datasets, but synthetic data is becoming common (I read somewhere it amounted for around 50% of the datasets on HuggingFace). While it won’t lead to amazing chatbot performance, the intuition is it may help mitigate any catastrophic forgetting and performance dips, and it’s also an easy way of augmenting our dataset. I used 3 methods of generating the synthetic data:</p><ol class=""><li id="a84d" class="nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os qc qd qe bk">For each Wiki page, I used the GPT-4 Turbo API to generate a few QA samples based on the provided text. This resulted in roughly 300 QA pairs.</li><li id="5930" class="nx ny fq nz b go qf ob oc gr qg oe of og qh oi oj ok qi om on oo qj oq or os qc qd qe bk">For each Wiki page, I created a specific instruction or question. For instance, on the ‘<a class="af oy" href="https://github.com/guardian/manage-frontend/wiki/Fastly-&amp;-Caching" rel="noopener ugc nofollow" target="_blank">Fastly &amp; Caching</a>’ page, the instruction might be ‘Walk me through how Fastly is used in `manage-frontend`.’ The response is then simply the contents of that Wiki page.</li><li id="d59b" class="nx ny fq nz b go qf ob oc gr qg oe of og qh oi oj ok qi om on oo qj oq or os qc qd qe bk">Similar to the previous step, for each file in the codebase, I created a question for it. E.g.: “What does the <code class="cx pq pr ps pt b">package.json</code> file look like in the <code class="cx pq pr ps pt b">manage-frontend</code> repo?” I then prefix each code file with the date of the codebase snapshot used for training, i.e.: “As of December 2023, the <code class="cx pq pr ps pt b">package.json</code> file looks like so: &lt;package.json code here&gt;”</li></ol><p id="35a5" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">The QA data was exported to a JSONL file, the following format is recommended as many tokenizers <a class="af oy" href="https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=jSpOjMopIRWk" rel="noopener ugc nofollow" target="_blank">have a function called </a><code class="cx pq pr ps pt b"><a class="af oy" href="https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=jSpOjMopIRWk" rel="noopener ugc nofollow" target="_blank">apply_chat_template</a></code> which takes in the list inside the <code class="cx pq pr ps pt b">messages</code> property in each line. Here is an example format below:</p><pre class="ml mm mn mo mp pu pt pv bp pw bb bk"><span id="dc69" class="px nc fq pt b bg py pz l qa qb">{"messages":[{"role":"user","content":"What is the capital of France?"},{"role":"assistant","content":"The capital of France is Paris."}]}<br/>{"messages":[{"role":"user","content":"What is the capital of England?"},{"role":"assistant","content":"The capital of England is London."}]}</span></pre><p id="e02b" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">I’m using 10% of this conversational data for the validation dataset.</p><h1 id="ff25" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk">Training the model</h1><h2 id="12d0" class="oz nc fq bf nd pa pb pc ng pd pe pf nj og pg ph pi ok pj pk pl oo pm pn po pp bk"><strong class="al">Hyperparameter sweeps</strong></h2><p id="ef08" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">I used a manual search. My intuition was that the LoRA rank, batch size and learning rate would affect model performance the most. I therefore started with a wide range of these hyperparameters and then iteratively narrowed down the search space based on the performance of the initial sweeps. A learning rate of 2e-5 appeared optimal, which seems to be standard for fine-tuning Mistral. <a class="af oy" href="https://arxiv.org/abs/2402.10373" rel="noopener ugc nofollow" target="_blank">BioMistral</a> continued fine-tuning the instruct model v0.2 with 0 warm up, a cosine scheduler and a learning rate of 2e-5. As I upped the rank and lowered the batch size the eval loss improved. However, it’s important to note that just lowering eval batch size can naturally improve validation loss due to less samples being validated at once, so it’s always good to check your model manually after it’s done training!</p><p id="478b" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">The sweeps in the image below all use a rank of either 512 or 768, with varying alphas; either 1x, 1.5x or 2x the rank. The batch sizes are either 1, 2 or 4. You can see the final hyperparameters I used in <a class="af oy" href="https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=IpcbWcAZgaq9" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="15ae" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">Once I found the optimal hyperparameters, I re-ran the training to include all data to make the most of the little data I had, as is common practice. These runs are noted by the <code class="cx pq pr ps pt b">All-Data</code> tag on the end of the sweep name.</p><p id="ab9e" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">Each sweep took under 3 hours, only a few pounds in Colab. All sweeps probably cost me somewhere between £40 and £50.</p><p id="d981" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk"><em class="qk">Note: </em>I accidentally included my Q&amp;A validation data in my raw text data (I forgot I copied and pasted it into one of my text files 🙃). However, re-running a couple sweeps without this confirmed that the selected hyperparameters remain robust and the validation loss was not much higher, with the optimal run having about a 0.12 eval loss. This is still very low, and indicates almost perfect performance, which is not the case. Therefore the eval strategy needs a bit of investigation and bettering.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ql"><img src="../Images/028b7f558b63b4647ec774872d581054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i8EwT_CJWSkTd_XSfjWfmQ.png"/></div></div></figure><h1 id="6bb3" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk">Expectations</h1><p id="7160" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">My expectations of this experiment were low. With limited online resources on projects of a similar scale and setup, I assumed there were obvious technical reasons for this. My assumption was a lot of catastrophic forgetting, random hallucinations, and a significant drop in performance, though I thought maybe it could answer a simple question like “What tech stack does <code class="cx pq pr ps pt b">manage-frontend</code> use?”.</p><h1 id="dd2b" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk"><strong class="al">Results</strong></h1><p id="1236" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk"><a class="af oy" href="https://colab.research.google.com/drive/1_j_-I_URIdiKshfeFrQoBLfIOpxx7HqR#scrollTo=oNUN3gIobgZ7" rel="noopener ugc nofollow" target="_blank">This notebook</a> includes a Gradio app for experimenting with your chatbot.</p><p id="c4fc" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">The results were better than expected:</p><p id="5ef6" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">The following response to a question regarding ‘product switching’ is impressive, given the lack of any natural language references in the Wiki or PR descriptions. The majority of variable names and conditionals are correct here:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qm"><img src="../Images/8ed834c84e888fff580e96d3c2a42be7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PkmGTwlwFrCmrGKBENjNTw.png"/></div></div></figure><p id="d743" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">A question like the following again has no natural language references, and actually requires digging into the code to realise we don’t allow switches to Paypal, only card and DD. It almost got it right.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qn"><img src="../Images/f6eeb3a6bba78a008ed832087d37404a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ApcwrgI4CHYPHZcKN4lk8Q.png"/></div></div></figure><p id="838f" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">It can recall some code perfectly when explicitly asked:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qo"><img src="../Images/898498e072bc190a08d4506db23df827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NXN6l0dDLeZjJ1BW8TGAYg.png"/></div></div></figure><h2 id="4c2f" class="oz nc fq bf nd pa pb pc ng pd pe pf nj og pg ph pi ok pj pk pl oo pm pn po pp bk">What about conflicting information within our dataset?</h2><p id="f9b8" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">Some of the Wiki is outdated (<a class="af oy" href="https://github.com/guardian/manage-frontend/wiki/Client-side-routing" rel="noopener ugc nofollow" target="_blank">example</a>), including references to our old CI platform TeamCity and our old routing solution using Reach Router. Upon asking the chatbot about these it did answer correctly, but it’s important to note that these are more common and the pre-trained model may be more inclined to suggest these:</p></div></div><div class="mq"><div class="ab cb"><div class="ll qp lm qq ln qr cf qs cg qt ci bh"><div class="ml mm mn mo mp ab ke"><figure class="le mq qu qv qw qx qy paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><img src="../Images/b660fb57680057ec05f13ed231328a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*TqzEyXkdo3cOAqqMdIZR4A.png"/></div></figure><figure class="le mq qz qv qw qx qy paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><img src="../Images/f22cb846041fdd0a12eae39776d6b56e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*Xj3UrS38Q78RkTrayVTZwg.png"/></div></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="3ff6" class="oz nc fq bf nd pa pb pc ng pd pe pf nj og pg ph pi ok pj pk pl oo pm pn po pp bk"><strong class="al">Catastrophic forgetting</strong></h2><p id="b753" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">Catastrophic forgetting is milder than expected, but there is still a noticeable difference between the fine-tuned model and the base model:</p><p id="41d5" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">When asking questions involving JavaScript and Typescript, languages that are prevalent in <code class="cx pq pr ps pt b">manage-frontend</code>(e.g.: “write me a Typescript function doing x and y”), the model may add some patterns used in the <code class="cx pq pr ps pt b">manage-frontend</code> codebase into the response. For example:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ra"><img src="../Images/1fbbe0a9001803401e9d00a8185f04cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XwyHmjPgcGkEbxnLgrHQGw.png"/></div></div></figure><p id="f686" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">Given an instruction to write some Python code, we do not get this kind of injection of knowledge from `manage-frontend` into the response:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rb"><img src="../Images/d512fc85f7f2d33d22383a871a6a9788.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q7MAc7382hKyylhNE5WSmw.png"/></div></div></figure><p id="f8de" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">For non code related questions, there are subtle differences and a decrease in performance. Notice the mistake in the response below, “229,792 kilometers per <em class="qk">hour</em>”, not per second. The original model in 16 bit with the same inference setup does not make this mistake.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rc"><img src="../Images/171d86d8c9f86703fb7500dd9288a3b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ufcaE2U6mfXheCQoQ8blg.png"/></div></div></figure><h2 id="feca" class="oz nc fq bf nd pa pb pc ng pd pe pf nj og pg ph pi ok pj pk pl oo pm pn po pp bk"><strong class="al">Text Generation Strategies</strong></h2><p id="0859" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">See the <a class="af oy" href="https://huggingface.co/docs/transformers/generation_strategies" rel="noopener ugc nofollow" target="_blank">text generation strategies docs</a> in HuggingFace.</p><p id="3f6a" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">I have <code class="cx pq pr ps pt b"><a class="af oy" href="https://colab.research.google.com/drive/1_j_-I_URIdiKshfeFrQoBLfIOpxx7HqR#scrollTo=LznZr5T_B01O" rel="noopener ugc nofollow" target="_blank">do_sample</a></code> set to False, so the model generates text using a deterministic approach using a greedy search under the hood. It picks the most likely next word or the most likely sequence of words based on the probabilities predicted by the model. Parameters such as <code class="cx pq pr ps pt b">temperature</code> and <code class="cx pq pr ps pt b">top_p</code> are therefore irrelevant because the model is not sampling from the probability distribution of the next word. Instead, it's directly choosing the token with the highest probability. <a class="af oy" href="https://medium.com/@jessica_lopez/understanding-greedy-search-and-beam-search-98c1e3cd821d" rel="noopener">Here’s </a>a good article for learning more about deterministic approaches in text generation. I found the responses to be slightly better using this approach, using a probabilistic approach and setting <code class="cx pq pr ps pt b">temperature</code> and <code class="cx pq pr ps pt b">top_p</code> to more extreme values lead to significantly worse performance.</p><h2 id="8311" class="oz nc fq bf nd pa pb pc ng pd pe pf nj og pg ph pi ok pj pk pl oo pm pn po pp bk">Why did these hyperparameters perform best?</h2><p id="32ac" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">I don’t know the definitive answer to this, but I’ll give my best educated assumption:</p><p id="fd26" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk"><strong class="nz fr">Batch size:</strong></p><p id="693a" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">Using a lower batch sizes introduces more variability and noise into the gradient estimation. This noise allows the optimiser to see the intricacies of the loss landscape with each update, responding more dynamically to the specific features of individual data points. At a high level, using smaller batch sizes allows the model to focus on and learn from the unique characteristics of each individual data sample. This approach encourages a more detailed and nuanced understanding of the dataset, as the model adjusts and responds to the specific features and intricacies of every single example it encounters during training. This is perhaps exacerbated with a small dataset like the one used in this experiment.</p><p id="5474" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk"><strong class="nz fr">LoRA Rank:</strong></p><p id="dac7" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">As results kept improving as the rank was upped, I also tried a very high rank of 2048 (with an alpha of 2048) on an H100 80GB, the results were not as good. I’ll include instructions down below on a cheap and quick way to get Unsloth set up on an H100 80GB.</p><p id="769e" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">Using a rank of 768 might have struck the right balance between adaptability and maintaining the pre-trained model’s generalisation capabilities. My training runs which used lower ranks not only had worse performance on the new data but also lead to more forgetting. A lower rank means that the matrices introduced for adaptation are more constrained, leading to fewer parameters being updated during the fine-tuning process. This can result in a model that is more focused on the new fine-tuning data, which is perhaps the explanation for the worse forgetting. Furthermore, a higher rank increases the model’s capacity to learn task-specific nuances by giving us more trainable parameters, and hence essentially makes it more “intelligent”. Therefore, too low of a rank was not enough for the model to learn the intricacies of the new data, but a rank of 2048 allowed the model too much freedom to deviate from its valuable pre-trained knowledge. <a class="af oy" href="https://www.reddit.com/r/MachineLearning/comments/13rp5sa/d_does_lora_actually_mitigate_catastrophic/" rel="noopener ugc nofollow" target="_blank">Here’s a good thread</a> for reading more about LoRA’s affect on mitigating forgetting.</p></div></div></div><div class="ab cb rd re rf rg" role="separator"><span class="rh by bm ri rj rk"/><span class="rh by bm ri rj rk"/><span class="rh by bm ri rj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0c9a" class="nb nc fq bf nd ne rl gq ng nh rm gt nj nk rn nm nn no ro nq nr ns rp nu nv nw bk"><strong class="al">Conclusion</strong></h1><p id="0e39" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">These results are encouraging, especially given the limited size and quality of the training data. With better training data, we could see significant improvements. There’s an abundance of high-quality text data readily available inside a company’s messaging tool, ticket and issue management system, and emails. Additionally, developers could invest time in creating high-quality conversational data.</p></div></div></div><div class="ab cb rd re rf rg" role="separator"><span class="rh by bm ri rj rk"/><span class="rh by bm ri rj rk"/><span class="rh by bm ri rj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="8dee" class="nb nc fq bf nd ne rl gq ng nh rm gt nj nk rn nm nn no ro nq nr ns rp nu nv nw bk"><strong class="al">Fine-tuning on an H100 80GB</strong></h1><p id="1dd2" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">If you’d like to experiment with more compute, here are some instructions for getting a model working quickly on the cloud with a graphics card beyond what Colab can provide:</p><ol class=""><li id="f646" class="nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os qc qd qe bk">I used <a class="af oy" href="https://lambdalabs.com/" rel="noopener ugc nofollow" target="_blank">LambdaLabs</a> for this. It’s the cheapest I can find and also gives you a link to a Jupyter Lab instance you can use directly from your browser. It was about $2.79 an hour. Bear in mind, this may seem cheap for what it is but as we know linux and python package management is the most difficult developer task out there so it’s easy to burn through the money debugging a broken setup.</li><li id="f415" class="nx ny fq nz b go qf ob oc gr qg oe of og qh oi oj ok qi om on oo qj oq or os qc qd qe bk">As of March 2024, the disk shipped with each instance comes pre-installed with CUDA 12.2, which seems to be a bit of an odd choice as there is no stable release of PyTorch yet that supports this version of CUDA. Anyways, you’ll need to SSH into the instance and run the following to get Unsloth working:</li><li id="7186" class="nx ny fq nz b go qf ob oc gr qg oe of og qh oi oj ok qi om on oo qj oq or os qc qd qe bk">Install PyTorch 2.2.0. PyTorch actually comes with it’s own CUDA runtime so this means there’s no annoying version matching needed. Run the following command then restart your instance:</li></ol><pre class="ml mm mn mo mp pu pt pv bp pw bb bk"><span id="be48" class="px nc fq pt b bg py pz l qa qb">pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \<br/>  --index-url https://download.pytorch.org/whl/cu121</span></pre><p id="7db7" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">4. Run these commands:</p><pre class="ml mm mn mo mp pu pt pv bp pw bb bk"><span id="b7c2" class="px nc fq pt b bg py pz l qa qb">pip install --upgrade pip setuptools wheel<br/>pip install packaging</span></pre><p id="20af" class="pw-post-body-paragraph nx ny fq nz b go ot ob oc gr ou oe of og ov oi oj ok ow om on oo ox oq or os fj bk">5. Install Unsloth:</p><pre class="ml mm mn mo mp pu pt pv bp pw bb bk"><span id="fab5" class="px nc fq pt b bg py pz l qa qb">pip install "unsloth[cu121-torch220] @ git+https://github.com/unslothai/unsloth.git"</span></pre></div></div></div></div>    
</body>
</html>