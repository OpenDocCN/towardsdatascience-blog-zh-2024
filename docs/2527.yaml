- en: The Accuracy vs Interpretability Trade-off Is a Lie
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-accuracy-vs-interpretability-trade-off-is-a-lie-0bf89c84f5b3?source=collection_archive---------5-----------------------#2024-10-16](https://towardsdatascience.com/the-accuracy-vs-interpretability-trade-off-is-a-lie-0bf89c84f5b3?source=collection_archive---------5-----------------------#2024-10-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why, if we look at the bigger picture, black-box models are not more accurate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://conorosullyds.medium.com/?source=post_page---byline--0bf89c84f5b3--------------------------------)[![Conor
    O''Sullivan](../Images/2dc50a24edb12e843651d01ed48a3c3f.png)](https://conorosullyds.medium.com/?source=post_page---byline--0bf89c84f5b3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0bf89c84f5b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0bf89c84f5b3--------------------------------)
    [Conor O''Sullivan](https://conorosullyds.medium.com/?source=post_page---byline--0bf89c84f5b3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0bf89c84f5b3--------------------------------)
    ·7 min read·Oct 16, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c825d5b59c5d7e78ee67cff1bb29dfd0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nathan Cima](https://unsplash.com/@nathan_cima?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: When I started as a data scientist, I was expecting to use state-of-the-art
    models. XGBoost, Neural Networks. These things are complex and interesting and
    surely they would drive improvements. Little did I know, the models faced a hurdle
    — explaining them to other people.
  prefs: []
  type: TYPE_NORMAL
- en: Who’d have thought you need to understand the decisions your automated systems
    make?
  prefs: []
  type: TYPE_NORMAL
- en: To my joy, I stumbled down the rabbit hole of [model agnostic methods](/what-are-model-agnostic-methods-387b0e8441ef).
    With these, I could have the best of both worlds. I could train black box models
    and then explain them using methods like [SHAP](/introduction-to-shap-with-python-d27edc23c454),
    [LIME](/a-deep-dive-on-lime-for-local-interpretations-872bea23952f), [PDPs](/the-ultimate-guide-to-pdps-and-ice-plots-4182885662aa),
    [ALEs](/deep-dive-on-accumulated-local-effect-plots-ales-with-python-0fc9698ed0ee)
    and [Friedman’s H-stat](/understanding-freidmans-h-statistic-h-stat-for-interactions-43fb5e31a586).
    We no longer need to trade accuracy for interpretability!
  prefs: []
  type: TYPE_NORMAL
- en: Not so fast. That thinking is flawed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our pursuit of best performance, we often miss the point of machine learning:
    that is, to make accurate predictions on new unseen data. Let’s discuss why complex
    models are not always the best way of achieving this. Even if we can explain them
    using other methods.'
  prefs: []
  type: TYPE_NORMAL
- en: What is the accuracy vs interpretability trade-off?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
