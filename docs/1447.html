<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>From Masked Image Modeling to Autoregressive Image Modeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>From Masked Image Modeling to Autoregressive Image Modeling</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-masked-image-modeling-to-autoregressive-image-modeling-d9a3cadf72a1?source=collection_archive---------6-----------------------#2024-06-10">https://towardsdatascience.com/from-masked-image-modeling-to-autoregressive-image-modeling-d9a3cadf72a1?source=collection_archive---------6-----------------------#2024-06-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="32b5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A brief review of the image foundation model pre-training objectives</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://mengliuz.medium.com/?source=post_page---byline--d9a3cadf72a1--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mengliu Zhao" class="l ep by dd de cx" src="../Images/0b950a0785fa065db3319ed5be4a91de.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*siAyGzGqa7K3xsa639R_2w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d9a3cadf72a1--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://mengliuz.medium.com/?source=post_page---byline--d9a3cadf72a1--------------------------------" rel="noopener follow">Mengliu Zhao</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d9a3cadf72a1--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="1c84" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We crave large models, don’t we?</p><p id="586f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The GPT series has proved its ability to revolutionize the NLP world, and everyone is excited to see the same transformation in the computer vision domain. The most popular image foundation models in recent years include <a class="af nf" href="https://ai.meta.com/research/publications/segment-anything/" rel="noopener ugc nofollow" target="_blank">SegmentAnything</a>, <a class="af nf" href="https://dinov2.metademolab.com/" rel="noopener ugc nofollow" target="_blank">DINOv2</a>, and many others. <em class="ng">The natural question is, what are the key differences between the pre-training stage of these foundation models?</em></p><p id="be06" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Instead of answering this question directly, we will gently review the image foundation model pre-training objectives using Masked Image Modeling in this blog article. We will also discuss a paper (to be) published in ICML’24, applying the A<strong class="ml fr">utoregression Modeling</strong> to foundation model pre-training.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni nj"><img src="../Images/ed3dac3e22d71ab2da2225ce80d73cef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fKVYISfkklHB22i5rA5cfw.jpeg"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Image source: <a class="af nf" href="https://pxhere.com/en/photo/1025277" rel="noopener ugc nofollow" target="_blank">https://pxhere.com/en/photo/1025277</a></figcaption></figure></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="ade4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">What is model pre-training in LLM?</strong></p><p id="7a76" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Model pre-training is a terminology used in general large models (LLM, image foundation models) to describe the stage where no label is given to the model, but training the model purely using a self-supervised manner.</p><p id="3445" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Common pre-training techniques mostly originated from LLMs. For example, the BERT model used Masked Language Modeling, which inspired Masked Image Modeling such as BEiT, MAE-ViT, and SimMM. The GPT series used Autoregressive Language Modeling, and a recently accepted ICML publication extended this idea to Autoregressive Image Modeling.</p><p id="d4cb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So, what are Masked Language Modeling and Autoregressive Language Modeling?</p><p id="d059" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">The Masked Language Modeling</strong> was first proposed in the BERT paper in 2018. The approach was described as “simply masking some percentage of the input tokens randomly and then predicting those masked tokens.” It’s a bi-directional representation approach, as the model will try to predict back and forth at the masked token.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div class="nh ni oi"><img src="../Images/efb412706aa74332a895e837f63bf5e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*WjBXQ6OD-Jd0haPOPqZ_cQ.png"/></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Masked LM pre-training. Image source: <a class="af nf" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1810.04805</a></figcaption></figure><p id="5090" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The Autoregressive Language Modeling was famously known from the GPT3 paper. It has a clearer definition in the XLNet paper as follows, and we can see the model is unidirectional. The reason the GPT series uses a unidirectional language model is that the architecture is decoder-based, which only needs self-attention on the prompt and the completion:</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni oj"><img src="../Images/1f066d218758e91005f465051652db05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mwrCHerj-Gy-rZNRNI9BEQ.png"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">AR — Autoregression. Source: <a class="af nf" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1906.08237</a></figcaption></figure></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="37d6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Pre-training in Image Domain</strong></p><p id="d081" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When moving into the image domain, the immediate question is how we form the image “token sequence.” The natural thinking is just to use the ViT architecture, breaking an image into a grid of image patches (visual tokens).</p><p id="7102" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">BEiT. </strong>Published as an arXiv preprint in 2022, the idea of BEiT is straightforward. After tokenizing an image into a sequence of 14*14 visual tokens, 40% of the tokens are randomly masked, replaced by learnable embeddings, and fed into the transformer. The pre-training objective is to maximize the log-likelihood of the correct visual tokens, and no decoder is needed for this stage. The pipeline is shown in the figure below.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni ok"><img src="../Images/0239c16d6d3c0f7182d41b3d43b024bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D8C5QWcJPLr-5BXiM9IXCA.png"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">BEiT pre-training pipeline. Image source: <a class="af nf" href="https://arxiv.org/abs/2106.08254" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2106.08254</a></figcaption></figure><p id="e3d3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the original paper, the authors also provided a theoretical link between the BEiT and the Variational Autoencoder. So the natural question is, can an Autoencoder be used for pre-training purposes?</p><p id="dd4a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">MAE-ViT.</strong> This paper answered the question above by designing a masked autoencoder architecture. Using the same ViT formulation and random masking, the authors proposed to “discard” the masked patches during training and only use unmasked patches in the visual token sequence as input to the encoder. The mask tokens will be used for reconstruction during the decoding stage at the pre-training. The decoder could be flexible, ranging from 1–12 transformer blocks with dimensionality between 128 and 1024. More detailed architectural information could be found in the original paper.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni ol"><img src="../Images/8580b8bdd3dc485d19cded136b935276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*po5c_L3w0BfL9JKqSxBIOw.png"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Masked Autoencoder architecture. Image source: <a class="af nf" href="https://arxiv.org/abs/2111.06377" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2111.06377</a></figcaption></figure><p id="6396" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">SimMIM</strong>. Slightly different from BEiT and MAE-ViT, the paper proposes using a flexible backbone such as Swin Transformer for encoding purposes. The proposed prediction head is extremely lightweight—a single linear layer of a 2-layer MLP to regress the masked pixels.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni om"><img src="../Images/9448a4de2f7470c0ab555a5b915789ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*StrSZHV79VcQfzHqk7MkeQ.png"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">SimMIM pipeline. Image source: <a class="af nf" href="https://arxiv.org/abs/2111.09886" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2111.09886</a></figcaption></figure></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="0bbd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">AIM.</strong> A recent paper accepted by ICML’24 proposed using the Autoregressive model (or causal model) for pre-training purposes. Instead of using a masked sequence, the model takes the full sequence to a causal transformer, using prefixed self-attention with causal masks.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div class="nh ni on"><img src="../Images/5e44758b0fbc559cf32ecebdd018f9f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*N0VFQpnzi3hZ9Yir9SyO9w.png"/></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">AIM with Causal Transformer. Image source: <a class="af nf" href="https://arxiv.org/abs/2401.08541" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2401.08541</a></figcaption></figure><p id="48c3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">What is prefixed causal attention? There are detailed tutorials on causal attention masking on <a class="af nf" href="https://www.kaggle.com/code/aisuko/causal-self-attention" rel="noopener ugc nofollow" target="_blank">Kaggle</a>, and <a class="af nf" href="https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention" rel="noopener ugc nofollow" target="_blank">here</a>, it is masking out “future” tokens on self-attention. However, in this paper, the authors claim that the discrepancy between the causal mask and downstream bidirectional self-attention would lead to a performance issue. The solution is to use partial causal masking or prefixed causal attention. In the prefix sequence, bidirectional self-attention is used, and causal attention is applied for the rest of the sequence.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div class="nh ni oo"><img src="../Images/3eb89c34ff50ee6a78b6f565bd0a6c24.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*BPbeDSWTn0uU4-RIsFRwrw.png"/></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Causal attention during pre-training. Image source: <a class="af nf" href="https://arxiv.org/abs/2401.08541" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2401.08541</a></figcaption></figure></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="6fe2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">What is the advantage of Autoregressive Image Masking</strong>? The answer lies in the scaling, of both the model and data sizes. The paper claims that the model scale directly correlates with the pre-training loss and the downstream task performance (the following left subplot). The uncurated pre-training data scale is also directly linked to the downstream task performance (the following right subplot).</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni op"><img src="../Images/bc10a0a3305f5400190f0c76d29b8d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oaorrYzEFDsY5qeNvHpz_A.png"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Scaling effect of the AIM. Image source: <a class="af nf" href="https://arxiv.org/abs/2401.08541" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2401.08541</a></figcaption></figure><p id="a3e3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Compared to a 50% masking ratio, the AIM achieved an astonishing 8% performance increase over Masked Image Modeling.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni oq"><img src="../Images/98963eaadfb259adbee7cbd30c5fa630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*taUMFBO-pA0cAswK_luKvA.png"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Table source: <a class="af nf" href="https://arxiv.org/abs/2401.08541" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2401.08541</a></figcaption></figure></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="1fd6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So, what is the big takeaway here? The AIM paper discussed different trade-offs between the state-of-the-art pre-training methods, and we won’t repeat them here. A shallower but more intuitive lesson is that there is likely still much work left to <strong class="ml fr">improve the vision foundation models using existing experience from the LLM domain, especially on scalability</strong>. Hopefully, we’ll see those improvements in the coming years.</p></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c6e9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">References</strong></p><ul class=""><li id="5f91" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne or os ot bk">El-Nouby et al., Scalable Pre-training of Large Autoregressive Image Models. ICML 2024. Github: <a class="af nf" href="https://github.com/apple/ml-aim" rel="noopener ugc nofollow" target="_blank">https://github.com/apple/ml-aim</a></li><li id="c3fa" class="mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne or os ot bk">Xie et al., SimMIM: a Simple Framework for Masked Image Modeling. CVPR 2022. Github: <a class="af nf" href="https://github.com/microsoft/SimMIM" rel="noopener ugc nofollow" target="_blank">https://github.com/microsoft/SimMIM</a></li><li id="d52a" class="mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne or os ot bk">Bao et al., BEiT: BERT Pre-Training of Image Transformers. <em class="ng">arXiv preprint 2022. </em>Github: <a class="af nf" href="https://github.com/microsoft/unilm/tree/master/beit" rel="noopener ugc nofollow" target="_blank">https://github.com/microsoft/unilm/tree/master/beit</a></li><li id="9526" class="mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne or os ot bk">He et al., Masked autoencoders are scalable vision learners. CVPR 2022. HuggingFace Official: <a class="af nf" href="https://huggingface.co/docs/transformers/en/model_doc/vit_mae" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/docs/transformers/en/model_doc/vit_mae</a></li><li id="a3e1" class="mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne or os ot bk">Caron et al., Emerging Properties in Self-Supervised Vision Transformers. ICCV 2021. Github: <a class="af nf" href="https://github.com/facebookresearch/dino?tab=readme-ov-file" rel="noopener ugc nofollow" target="_blank">https://github.com/facebookresearch/dino?tab=readme-ov-file</a></li><li id="3f91" class="mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne or os ot bk">Liu et al., Swin transformer: Hierarchical vision transformer using shifted windows. ICCV 2021. Github: <a class="af nf" href="https://github.com/microsoft/Swin-Transformer" rel="noopener ugc nofollow" target="_blank">https://github.com/microsoft/Swin-Transformer</a></li><li id="5c59" class="mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne or os ot bk">Brown et al., Language Models are Few-Shot Learners. NeurIPS 2020. Github: <a class="af nf" href="https://github.com/openai/gpt-3" rel="noopener ugc nofollow" target="_blank">https://github.com/openai/gpt-3</a></li><li id="98cd" class="mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne or os ot bk">Yang et al., Xlnet: Generalized autoregressive pretraining for language understanding. NeurIPS 2019. Github: <a class="af nf" href="https://github.com/zihangdai/xlnet" rel="noopener ugc nofollow" target="_blank">https://github.com/zihangdai/xlnet</a></li><li id="03ba" class="mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne or os ot bk">Devlin et al., BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. <em class="ng">arXiv preprint 2018</em>. HuggingFace Official: <a class="af nf" href="https://huggingface.co/docs/transformers/en/model_doc/bert" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/docs/transformers/en/model_doc/bert</a></li></ul></div></div></div></div>    
</body>
</html>