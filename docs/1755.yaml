- en: A Python Engineer’s Introduction to 3D Gaussian Splatting (Part 3)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-python-engineers-introduction-to-3d-gaussian-splatting-part-3-398d36ccdd90?source=collection_archive---------5-----------------------#2024-07-18](https://towardsdatascience.com/a-python-engineers-introduction-to-3d-gaussian-splatting-part-3-398d36ccdd90?source=collection_archive---------5-----------------------#2024-07-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Part 3 of our Gaussian Splatting tutorial, showing how to render splats onto
    a 2D image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dcaustin33?source=post_page---byline--398d36ccdd90--------------------------------)[![Derek
    Austin](../Images/1bcc5955f32cb798988af5713baae212.png)](https://medium.com/@dcaustin33?source=post_page---byline--398d36ccdd90--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--398d36ccdd90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--398d36ccdd90--------------------------------)
    [Derek Austin](https://medium.com/@dcaustin33?source=post_page---byline--398d36ccdd90--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--398d36ccdd90--------------------------------)
    ·8 min read·Jul 18, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we reach the most intriguing phase of the Gaussian splatting process:
    rendering! This step is arguably the most crucial, as it determines the realism
    of our model. Yet, it might also be the simplest. In [part 1](https://medium.com/towards-data-science/a-python-engineers-introduction-to-3d-gaussian-splatting-part-1-e133b0449fc6)
    and [part 2](https://medium.com/towards-data-science/a-python-engineers-introduction-to-3d-gaussian-splatting-part-2-7e45b270c1df)
    of our series we demonstrated how to transform raw splats into a format ready
    for rendering, but now we actually have to do the work and render onto a fixed
    set of pixels. The authors have developed a fast rendering engine using CUDA,
    which can be somewhat challenging to follow. Therefore, I believe it is beneficial
    to first walk through the code in Python, using straightforward for loops for
    clarity. For those eager to dive deeper, all the necessary code is available on
    our G[itHub](https://github.com/dcaustin33/intro_to_gaussian_splatting).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss how to render each individual pixel. From our previous [article](https://medium.com/towards-data-science/a-python-engineers-introduction-to-3d-gaussian-splatting-part-2-7e45b270c1df),
    we have all the necessary components: 2D points, associated colors, covariance,
    sorted depth order, inverse covariance in 2D, minimum and maximum x and y values
    for each splat, and associated opacity. With these components, we can render any
    pixel. Given specific pixel coordinates, we iterate through all splats until we
    reach a saturation threshold, following the splat depth order relative to the
    camera plane (projected to the camera plane and then sorted by depth). For each
    splat, we first check if the pixel coordinate is within the bounds defined by
    the minimum and maximum x and y values. This check determines if we should continue
    rendering or ignore the splat for these coordinates. Next, we compute the Gaussian
    splat strength at the pixel coordinate using the splat mean, splat covariance,
    and pixel coordinates.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We multiply this weight by the splat’s opacity to obtain a parameter called
    alpha. Before adding this new value to the pixel, we need to check if we have
    exceeded our saturation threshold. We do not want a splat behind other splats
    to affect the pixel coloring and use computing resources if the pixel is already
    saturated. Thus, we use a threshold that allows us to stop rendering once it is
    exceeded. In practice, we start our saturation threshold at 1 and then multiply
    it by min(0.99, (1 — alpha)) to get a new value. If this value is less than our
    threshold (0.0001), we stop rendering that pixel and consider it complete. If
    not, we add the colors weighted by the saturation * (1 — alpha) value and update
    the saturation as new_saturation = old_saturation * (1 — alpha). Finally, we loop
    over every pixel (or every 16x16 tile in practice) and render. The complete code
    is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now that we can render a pixel we can render a patch of an image, or what the
    authors refer to as a tile!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: And finally we can use all of those tiles to render an entire image. Note how
    we check to make sure the splat will actually affect the current tile (x_in_tile
    and y_in_tile code).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: At long last now that we have all the necessary components we can render an
    image. We take all the 3D points from the treehill dataset and initialize them
    as gaussian splats. In order to avoid a costly nearest neighbor search we initialize
    all scale variables as .01 (Note that with such a small variance we will need
    a strong concentration of splats in one spot to be visible. Larger variance makes
    the process quite slow.). Then all we have to do is call render_image with the
    image number we are trying to emulate and as you an see we get a sparse set of
    point clouds that resemble our image! (Check out our bonus section at the bottom
    for an equivalent CUDA kernel using pyTorch’s nifty tool that compiles CUDA code!)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e598799a3b44d0da6514a6f8399487a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Actual image, CPU implementation, CUDA implementation. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: While the backwards pass is not part of this tutorial, one note should be made
    that while we start with only these few points, we soon have hundreds of thousands
    of splats for most scenes. This is caused by the breaking up of large splats (as
    defined by larger variance on axes) into smaller splats and removing splats that
    have extremely low opacity. For instance, if we truly initialized the scale to
    the mean of the three closest nearest neighbors we would have a majority of the
    space covered. In order to get fine detail we would need to break these down into
    much smaller splats that are able to capture fine detail. They also need to populate
    areas with very few gaussians. They refer to these two scenarios as over reconstruction
    and under reconstruction and define both scenarios by large gradient values for
    various splats. They then split or clone the splats depending on size (see image
    below) and continue the optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: Although the backward pass is not covered in this tutorial, it’s important to
    note that we start with only a few points but soon have hundreds of thousands
    of splats in most scenes. This increase is due to the splitting of large splats
    (with larger variances on axes) into smaller ones and the removal of splats with
    very low opacity. For instance, if we initially set the scale to the mean of the
    three nearest neighbors, most of the space would be covered. To achieve fine detail,
    we need to break these large splats into much smaller ones. Additionally, areas
    with very few Gaussians need to be populated. These scenarios are referred to
    as over-reconstruction and under-reconstruction, characterized by large gradient
    values for various splats. Depending on their size, splats are split or cloned
    (see image below), and the optimization process continues.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1028ac2dcb239fa565cc73156eef6e93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the Author’s original paper on how gaussians are split or cloned in training.
    Source: [https://arxiv.org/abs/2308.04079](https://arxiv.org/abs/2308.04079)'
  prefs: []
  type: TYPE_NORMAL
- en: And that is an easy introduction to Gaussian Splatting! You should now have
    a good intuition on what exactly is going on in the forward pass of a gaussian
    scene render. While a bit daunting and not exactly neural networks, all it takes
    is a bit of linear algebra and we can render 3D geometry in 2D!
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to leave comments about confusing topics or if I got something wrong
    and you can always connect with me on LinkedIn or twitter!
  prefs: []
  type: TYPE_NORMAL
- en: Bonus — CUDA Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Use PyTorch’s CUDA compiler to write a custom CUDA kernel!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
