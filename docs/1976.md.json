["```py\nfrom tensorflow.keras import layers, models\n```", "```py\ninputs = layers.Input(shape=(None,)) # (N, 96*k)\nx = layers.Reshape((-1, 1))(inputs)  # (N, 96*k, 1)\n\n# Conv1D parameters: filters, kernel_size, strides, padding\nx = layers.Conv1D(40, 5, 3, 'same', activation='relu')(x) # (N, 32*k, 40)\nx = layers.Conv1D(40, 3, 2, 'same', activation='relu')(x) # (N, 16*k, 40)\nx = layers.Conv1D(40, 3, 2, 'same', activation='relu')(x) # (N, 8*k, 40)\nx = layers.Conv1D(40, 3, 2, 'same', activation='relu')(x) # (N, 4*k, 40)\nx = layers.Conv1D(40, 3, 2, 'same', activation='relu')(x) # (N, 2*k, 40)\nx = layers.Conv1D(20, 3, 2, 'same')(x) # (N, k, 20)\n\nz_mean = x[: ,:, :10]   # (N, k, 10)\nz_log_var = x[:, :, 10:] # (N, k, 10)\nz = Sampling()([z_mean, z_log_var]) # custom layer sampling from gaussian\n\nencoder = models.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n```", "```py\n# input shape: (batch_size, time_length/96, latent_features)\ninputs = layers.Input(shape=(None, 10)) # (N, k, 10)\n\n# Conv1DTranspose parameters: filters, kernel_size, strides, padding\nx = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(inputs) # (N, 2*k, 40)\nx = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(x) # (N, 4*k, 40)\nx = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(x) # (N, 8*k, 40)\nx = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(x) # (N, 16*k, 40)\nx = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(x) # (N, 32*k, 40)\nx = layers.Conv1DTranspose(1,  5, 3, 'same')(x) # (N, 96*k, 1)\n\noutputs = layers.Reshape((-1,))(x) # (N, 96*k)\n\ndecoder = models.Model(inputs, outputs, name='decoder')\n```", "```py\n# seasonal inputs shape: (N, k, 6)\ninputs = layers.Input(shape=(None, 2*3)) \n\nx = layers.Dense(20, use_bias=False)(inputs) # (N, k, 20)\nz_mean = x[:, :, :10]  # (N, k, 10)\nz_log_var = x[:, :, 10:] # (N, k, 10)\nz = Sampling()([z_mean, z_log_var]) # (N, k, 10)\n\nprior = models.Model(inputs, [z_mean, z_log_var, z], name='seasonal_prior')\n```", "```py\nclass VAE(models.Model):\n    def __init__(self, encoder, decoder, prior, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.prior = prior\n        self.noise_log_var = self.add_weight(name='var', shape=(1,), initializer='zeros', trainable=True)\n\n    @tf.function\n    def vae_loss(self, data):\n        values, seasonal = data\n        z_mean, z_log_var, z = self.encoder(values)\n        reconstructed = self.decoder(z)\n        reconstruction_loss = -log_lik_normal_sum(values, reconstructed, self.noise_log_var)/INPUT_SIZE\n        seasonal_z_mean, seasonal_z_log_var, _ = self.prior(seasonal)\n        kl_loss_z = kl_divergence_sum(z_mean, z_log_var, seasonal_z_mean, seasonal_z_log_var)/INPUT_SIZE\n        return reconstruction_loss, kl_loss_z\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            reconstruction_loss, kl_loss_z = self.vae_loss(data)\n            total_loss = reconstruction_loss + kl_loss_z\n\n        gradients = tape.gradient(total_loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n\n        return {'loss': total_loss}\n```"]