- en: Run and Serve Faster VLMs Like Pixtral and Phi-3.5 Vision with vLLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/run-and-serve-faster-vlms-like-pixtral-and-phi-3-5-vision-with-vllm-1db7226c2035?source=collection_archive---------4-----------------------#2024-09-23](https://towardsdatascience.com/run-and-serve-faster-vlms-like-pixtral-and-phi-3-5-vision-with-vllm-1db7226c2035?source=collection_archive---------4-----------------------#2024-09-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding how much memory you need to serve a VLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--1db7226c2035--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--1db7226c2035--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1db7226c2035--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1db7226c2035--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--1db7226c2035--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1db7226c2035--------------------------------)
    ·10 min read·Sep 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9383dc47c9e69050884d5582cbb02bb1.png)'
  prefs: []
  type: TYPE_IMG
- en: An image encoded by Pixtral — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: vLLM is currently one of the fastest inference engines for large language models
    (LLMs). It supports a wide range of model architectures and quantization methods.
  prefs: []
  type: TYPE_NORMAL
- en: vLLM also supports vision-language models (VLMs) with multimodal inputs containing
    both images and text prompts. For instance, vLLM can now serve models like Phi-3.5
    Vision and Pixtral, which excel at tasks such as image captioning, optical character
    recognition (OCR), and visual question answering (VQA).
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will show you how to use VLMs with vLLM, focusing on key
    parameters that impact memory consumption. We will see why VLMs consume much more
    memory than standard LLMs. We’ll use Phi-3.5 Vision and Pixtral as case studies
    for a multimodal application that processes prompts containing text and images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for running Phi-3.5 Vision and Pixtral with vLLM is provided in this
    notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Get the notebook (#105)](https://newsletter.kaitchup.com/p/notebooks)'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Memory Consumption of VLMs in vLLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In transformer models, generating text token by token is slow because each prediction
    depends on all previous tokens…
  prefs: []
  type: TYPE_NORMAL
