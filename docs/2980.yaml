- en: 'Understanding DDPG: The Algorithm That Solves Continuous Action Control Challenges'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-ddpg-the-algorithm-that-solves-continuous-action-control-challenges-742c67e0783a?source=collection_archive---------2-----------------------#2024-12-11](https://towardsdatascience.com/understanding-ddpg-the-algorithm-that-solves-continuous-action-control-challenges-742c67e0783a?source=collection_archive---------2-----------------------#2024-12-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Discover how DDPG solves the puzzle of continuous action control, unlocking
    possibilities in AI-driven medical robotics.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sirinebhouri?source=post_page---byline--742c67e0783a--------------------------------)[![Sirine
    Bhouri](../Images/ae904be69cb3ca9bb39185aaa7be4233.png)](https://medium.com/@sirinebhouri?source=post_page---byline--742c67e0783a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--742c67e0783a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--742c67e0783a--------------------------------)
    [Sirine Bhouri](https://medium.com/@sirinebhouri?source=post_page---byline--742c67e0783a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--742c67e0783a--------------------------------)
    ·10 min read·Dec 11, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine you’re controlling a roboticarm in a surgical procedure. Discrete actions
    might be:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Move up,**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Move down**,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grab**, or'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Release**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are clear, direct commands, easy to execute in simple scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what about performing delicate movements, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Move the arm by 0.5 mm to avoid damaging the tissue**,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apply a force of 3N for tissue compression**, or'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rotate the wrist by 15° to adjust the incision angle**?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In these situations, you need more than just choosing an action — you must decide
    *how much* of that action is needed. This is the world of **continuous action
    spaces**, and this is where Deep Deterministic Policy Gradient **(DDPG)** shines!
  prefs: []
  type: TYPE_NORMAL
- en: Traditional methods like Deep Q-Networks (**DQN**) work well with discrete actions
    but struggle with continuous ones. DeterministicPolicyGradient **(DPG)** on the
    other hand, tackled this issue but faced challenges with poor exploration and
    instability. **DDPG** which wasfirst introduced in T P. Lillicrap et al’s [paper](https://arxiv.org/pdf/1509.02971v6)
    **combines** the **strengths** of **DPG** and **DQN** to **improve stability**
    and **performance** in environments with **continuous action spaces.**
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will discuss the theory and architecture behind DDPG, look
    at an implementation of it on Python, evaluate its performance (by testing it
    on MountainCarContinuous game) and briefly discuss how DDPG can be used in the
    bioengineering field.
  prefs: []
  type: TYPE_NORMAL
- en: '**DDPG Architecture**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike **DQN**, which evaluates every possible state-action pair to find the
    best action (impossible in continuous spaces due to infinite combinations), **DPG**
    uses an **Actor-Critic architecture**. The Actor learns a policy that directly
    maps states to actions, avoiding exhaustive searches and focusing on learning
    the best action for each state.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, DPG faces two main challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: It is a **deterministic** algorithm which **limits exploration** of the action
    space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It cannot use neural networks effectively due to **instability** in the learning
    process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**DDPG** improves DPG by introducing **exploration noise** via the Ornstein-Uhlenbeck
    process and stabilising training with **Batch Normalisation** and DQN techniques
    like **Replay Buffer** and **Target Networks**.'
  prefs: []
  type: TYPE_NORMAL
- en: With these enhancements, DDPG is well-suited to train agents in continuous action
    spaces, such as controlling robotic systems in bioengineering applications.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s explore the key components of the DDPG model!
  prefs: []
  type: TYPE_NORMAL
- en: '**Actor-Critic Framework**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Actor (Policy Network)**: Tells the agent which action to take given the
    state it is in. The network’s parameters (i.e. weights) are represented by θμ.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6ae607f0b6394ea2af1bc4e008128a96.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Tip!** Think of the Actor Network as the decision-maker: it maps the current
    state to a single action.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Critic (Q-value Network)**: Evaluates how good the action taken by the actor
    by estimating the Q-value of that state-action pair.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/095e9c51285694906190157f570dd843.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Tip**! Think of the Critic Network as the evaluator, it assigns a quality
    score to each action and helps improve the Actor’s policy to make sure it indeed
    generates the best action to take in each given state.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**! The critic will use the estimated Q-value for two things:'
  prefs: []
  type: TYPE_NORMAL
- en: To improve the Actor’s policy (Actor Policy Update).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Actor’s goal is to adjust its parameters (θμ) so that it outputs actions
    that maximise the critic’s Q-value.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, the Actor needs to understand both how the selected action a affects
    the Critic’s Q-value and how its internal parameters affect its Policy which is
    done through this Policy Gradient equation (it is the mean of all the gradients
    calculated from the mini-batch):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ef06bfb2a896ecd595c0da22b4ff7e1.png)'
  prefs: []
  type: TYPE_IMG
- en: '**2.** To improve its own network (Critic Q-value Network Update) by minimising
    the loss function below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb44da31f91612d3172e85cec236c8bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Where N is the number of experiences sampled in the mini-batch and y_i is the
    target Q-value calculated as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c534b1294b3cfbefd8bf06b9da2bb423.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Replay Buffer**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the agent explores the environment, past experiences (state, action, reward,
    next state) are stored as tuples (s, a, r, s′) in the replay buffer. During training,
    mini-batches consisting of some of these experiences are then randomly sampled
    to train the agent.
  prefs: []
  type: TYPE_NORMAL
- en: '**Question!** How does replay buffer *actually* reduce instability?'
  prefs: []
  type: TYPE_NORMAL
- en: By randomly sampling experiences, the replay buffer breaks the correlation between
    consecutive samples, reducing bias and leading to more stable training.
  prefs: []
  type: TYPE_NORMAL
- en: '**Target Networks**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Target Networks are slowly updated copies of the Actor and Critic. They provide
    stable Q-value targets, preventing rapid changes and ensuring smooth, consistent
    updates.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad2698b89ffe352ffc56ed2573b32f88.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Question!** How do target networks *actually* reduce instability?'
  prefs: []
  type: TYPE_NORMAL
- en: Without the **Critic target network**, the target Q-value is calculated directly
    from the Critic Q-value network, which is updated continuously. This causes the
    target Q-value to shift at each step, creating a “moving target” problem. As a
    result, the Critic ends up chasing a constantly changing target, making training
    unstable.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, since the **Actor** relies on the Critic’s feedback, errors in
    one network can amplify errors in the other, creating an interdependent loop of
    instability.
  prefs: []
  type: TYPE_NORMAL
- en: By introducing **target networks** that are updated gradually with a soft update
    rule, we ensure the target Q-value remains more consistent, reducing abrupt changes
    and improving learning stability.
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch Normalisation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Batch Normalisation standardises the inputs to each layer of the neural network,
    ensuring mean of zero and a unit variance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Question!** How does batch normalisation *actually* reduce instability?'
  prefs: []
  type: TYPE_NORMAL
- en: Samples drawn from the replay buffer may have different distributions than real-time
    data, leading to instability during network updates.
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalisation ensures consistent scaling of inputs to prevent erratic
    updates caused by varying input distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploration Noise**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the Actor’s policy is deterministic, exploration noise is added to actions
    during training to encourage the agent to explore the as much of the action space
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8d1d609db7533482e5a40ac6532ab59.png)'
  prefs: []
  type: TYPE_IMG
- en: On the DDPG publication, the authors used the **Ornstein-Uhlenbeck process**
    to generate temporally correlated noise, in order to mimick real-world system
    dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: 'DDPG Pseudocode: A Step-by-Step Breakdown'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/67ae26120aeeafaba2fbe7b7b9d7ef7d.png)'
  prefs: []
  type: TYPE_IMG
- en: Pseudocode taken from [http://arxiv.org/abs/1509.02971](http://arxiv.org/abs/1509.02971)
    (see reference 1 in ‘References’ section)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5490d989d60df6c77b8e3d32123600df.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram drawn by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Define Actor and Critic Networks**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Define Replay Buffer**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A ReplayBuffer class is implemented to store and sample the transition tuples
    (s, a, r, s’) discussed in the previous section to enable mini-batch off-policy
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Define OU Noise class**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An OUNoise class is added to generate exploration noise, helping the agent explore
    the action space more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Define the DDPG agent**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A DDPG class was defined and it encapsulates the agent’s behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialisation: Creates Actor and Critic networks, along with their target
    counterparts and the replay buffer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Action Selection: The `select_action` method chooses actions based on the
    current policy.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '3\. Training: The `train` method defines how the networks are updated using
    experiences from the replay buffer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note!** Since the paper introduced the use of target networks and batch normalisation
    to improve stability, I designed the `train` method to allow us to toggle these
    methods on or off. This lets us compare the agent’s performance with and without
    them. See code below for exact implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Train the DDPG agent**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bringing all the defined classes and methods together, we can train the DDPG
    agent. My `train_dppg` function follows the pseudocode and DDPG model diagram
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tip:** To make it easier for you to understand, I’ve labeled each code section
    with the corresponding step number from both the pseudocode and diagram. Hope
    that helps! :)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Performance and Results: Evaluating DDPG’s Effectiveness'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DDPG’s effectiveness in a continuous action space was tested in the `MountainCarContinuous-v0`
    environment, where the agent learns to where the agent learns to gain momentum
    to drive the car up a steep hill. The results show that using **Target Networks**
    and **Batch Normalisation** leads to faster convergence, higher rewards, and more
    stable learning than other configurations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3126a241c28e6b5fd6c59ec4cf1380f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph generated by author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5dafde30d56caffaaeb03e189a322450.png)'
  prefs: []
  type: TYPE_IMG
- en: GIF generated by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Note!** You can implement this yourself on any environment of your choice
    by running the code which can be found on my [GitHub](https://github.com/sirine-b/DDPG)
    as is and simply changing the environment’s name as needed!'
  prefs: []
  type: TYPE_NORMAL
- en: 'DDPG in Bioengineering: Unlocking Precision and Adaptability'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Through this blog post, we’ve seen that DDPG is a powerful algorithm for training
    agents in environments with continuous action spaces. By combining techniques
    from both DPG and DQN, DDPG improves exploration, stability, and performance —
    key factors for applications in robotic surgery and bioengineering.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a robotic surgeon, like the **da Vinci system**, using DDPG to control
    fine movements in real-time, ensuring **precise adjustments** without any errors.
    With DDPG, the robot could adjust its arm’s position by millimeters, apply exact
    force when suturing, or even make slight wrist rotations for an optimal incision.
    Such **real-time precision** could transform surgical outcomes, reduce recovery
    time, and minimise human error.
  prefs: []
  type: TYPE_NORMAL
- en: But DDPG’s potential goes beyond surgery. It’s already advancing bioengineering,
    enabling robotic prosthetics and assistive devices to replicate the natural motion
    of human limbs (check out this super interesting [article](https://www.tandfonline.com/doi/abs/10.1080/00207179.2023.2201644)!).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the theory behind DDPG, it’s time for you to explore
    its implementation. Start with simple examples and gradually dive into more complex
    scenarios!
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T, Tassa Y, et al. Continuous
    control with deep reinforcement learning [Internet]. arXiv; 2019\. Available from:
    [http://arxiv.org/abs/1509.02971](http://arxiv.org/abs/1509.02971)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
