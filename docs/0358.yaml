- en: 'Policy Gradients: The Foundation of RLHF'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度：RLHF的基础
- en: 原文：[https://towardsdatascience.com/policy-gradients-the-foundation-of-rlhf-337346beef40?source=collection_archive---------7-----------------------#2024-02-06](https://towardsdatascience.com/policy-gradients-the-foundation-of-rlhf-337346beef40?source=collection_archive---------7-----------------------#2024-02-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/policy-gradients-the-foundation-of-rlhf-337346beef40?source=collection_archive---------7-----------------------#2024-02-06](https://towardsdatascience.com/policy-gradients-the-foundation-of-rlhf-337346beef40?source=collection_archive---------7-----------------------#2024-02-06)
- en: Understanding policy optimization and how it is used in reinforcement learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解策略优化及其在强化学习中的应用
- en: '[](https://wolfecameron.medium.com/?source=post_page---byline--337346beef40--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page---byline--337346beef40--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--337346beef40--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--337346beef40--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page---byline--337346beef40--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page---byline--337346beef40--------------------------------)[![Cameron
    R. Wolfe, 博士](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page---byline--337346beef40--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--337346beef40--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--337346beef40--------------------------------)
    [Cameron R. Wolfe, 博士](https://wolfecameron.medium.com/?source=post_page---byline--337346beef40--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--337346beef40--------------------------------)
    ·15 min read·Feb 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--337346beef40--------------------------------)
    ·15分钟阅读·2024年2月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/aab3d055f178ffac6b75ff4d80f331ad.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aab3d055f178ffac6b75ff4d80f331ad.png)'
- en: (Photo by [WrongTog](https://unsplash.com/@wrongtog?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/blue-and-red-light-streaks-nYh3nHalEMA?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由[WrongTog](https://unsplash.com/@wrongtog?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)拍摄，来源于[Unsplash](https://unsplash.com/photos/blue-and-red-light-streaks-nYh3nHalEMA?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)）
- en: Although useful for a variety of applications, reinforcement learning (RL) is
    a key component of the alignment process for large language models (LLMs) due
    to its use in [reinforcement learning from human feedback (RLHF)](https://aman.ai/primers/ai/RLHF/).
    Unfortunately, RL is less widely understood within the AI community. Namely, many
    practitioners (including myself) are more familiar with supervised learning techniques,
    which creates an implicit bias against using RL despite its massive utility. Within
    this series of overviews, our goal is to mitigate this bias via a comprehensive
    survey of RL that starts with basic ideas and moves towards modern algorithms
    like [proximal policy optimization (PPO)](https://openai.com/research/openai-baselines-ppo)
    [7] that are heavily used for RLHF.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管强化学习（RL）在多种应用中非常有用，但它在大规模语言模型（LLMs）对齐过程中的作用至关重要，特别是在[强化学习与人类反馈（RLHF）](https://aman.ai/primers/ai/RLHF/)中的应用。不幸的是，RL在AI社区中并不广为人知。也就是说，许多从业者（包括我自己）更熟悉监督学习技术，这导致了对使用RL的潜在偏见，尽管它具有巨大的实用性。在这一系列概述中，我们的目标是通过全面回顾RL，从基本思想入手，逐步过渡到现代算法，如[近端策略优化（PPO）](https://openai.com/research/openai-baselines-ppo)
    [7]，这些算法在RLHF中被广泛使用，从而减少这种偏见。
- en: '![](../Images/40b82c39eff8ef3118d07ced407be0ae.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40b82c39eff8ef3118d07ced407be0ae.png)'
- en: Taxonomy of modern RL algorithms (from [5])
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现代强化学习算法的分类（来自[5]）
- en: '**This overview.** As shown above, there are two types of model-free RL algorithms:
    Q-Learning and Policy Optimization. Previously, we learned about Q-Learning, the
    basics of RL, and how these ideas can be generalized to language model finetuning.
    Within this overview, we will overview policy optimization and policy gradients,
    two ideas that are heavily utilized…'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**本概述。** 如上所示，存在两种类型的无模型强化学习（RL）算法：Q学习和策略优化。之前，我们学习了Q学习、强化学习的基础知识，以及这些思想如何可以推广到语言模型微调。在本概述中，我们将概述策略优化和策略梯度这两个在实践中广泛应用的思想……'
