- en: 'Policy Gradients: The Foundation of RLHF'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/policy-gradients-the-foundation-of-rlhf-337346beef40?source=collection_archive---------7-----------------------#2024-02-06](https://towardsdatascience.com/policy-gradients-the-foundation-of-rlhf-337346beef40?source=collection_archive---------7-----------------------#2024-02-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding policy optimization and how it is used in reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page---byline--337346beef40--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page---byline--337346beef40--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--337346beef40--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--337346beef40--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page---byline--337346beef40--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--337346beef40--------------------------------)
    ·15 min read·Feb 6, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aab3d055f178ffac6b75ff4d80f331ad.png)'
  prefs: []
  type: TYPE_IMG
- en: (Photo by [WrongTog](https://unsplash.com/@wrongtog?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/blue-and-red-light-streaks-nYh3nHalEMA?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: Although useful for a variety of applications, reinforcement learning (RL) is
    a key component of the alignment process for large language models (LLMs) due
    to its use in [reinforcement learning from human feedback (RLHF)](https://aman.ai/primers/ai/RLHF/).
    Unfortunately, RL is less widely understood within the AI community. Namely, many
    practitioners (including myself) are more familiar with supervised learning techniques,
    which creates an implicit bias against using RL despite its massive utility. Within
    this series of overviews, our goal is to mitigate this bias via a comprehensive
    survey of RL that starts with basic ideas and moves towards modern algorithms
    like [proximal policy optimization (PPO)](https://openai.com/research/openai-baselines-ppo)
    [7] that are heavily used for RLHF.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40b82c39eff8ef3118d07ced407be0ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Taxonomy of modern RL algorithms (from [5])
  prefs: []
  type: TYPE_NORMAL
- en: '**This overview.** As shown above, there are two types of model-free RL algorithms:
    Q-Learning and Policy Optimization. Previously, we learned about Q-Learning, the
    basics of RL, and how these ideas can be generalized to language model finetuning.
    Within this overview, we will overview policy optimization and policy gradients,
    two ideas that are heavily utilized…'
  prefs: []
  type: TYPE_NORMAL
