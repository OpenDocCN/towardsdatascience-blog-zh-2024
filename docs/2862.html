<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Optimizing Transformer Models for Variable-Length Input Sequences</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Optimizing Transformer Models for Variable-Length Input Sequences</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimizing-transformer-models-for-variable-length-input-sequences-19fb88fddf71?source=collection_archive---------3-----------------------#2024-11-26">https://towardsdatascience.com/optimizing-transformer-models-for-variable-length-input-sequences-19fb88fddf71?source=collection_archive---------3-----------------------#2024-11-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a980" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How PyTorch NestedTensors, FlashAttention2, and xFormers can Boost Performance and Reduce AI Costs</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--19fb88fddf71--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--19fb88fddf71--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--19fb88fddf71--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--19fb88fddf71--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/0a9af14b8f889cbba8fa1fe74fb680aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KTgbpA3zQGTR4ugq"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@tanjazoellner?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tanja Zöllner</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="663a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As generative AI (genAI) models grow in both popularity and scale, so do the computational demands and costs associated with their training and deployment. Optimizing these models is crucial for enhancing their runtime performance and reducing their operational expenses. At the heart of modern genAI systems is the Transformer architecture and its attention mechanism, which is notably compute-intensive.</p><p id="cb1b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In a <a class="af nc" rel="noopener" target="_blank" href="/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6">previous post</a>, we demonstrated how using optimized attention kernels can significantly accelerate the performance of Transformer models. In this post, we continue our exploration by addressing the challenge of variable-length input sequences — an inherent property of real-world data, including documents, code, time-series, and more.</p><h2 id="d2fc" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">The Challenge of Batching Variable-Length Input</h2><p id="c2b5" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In a typical deep learning workload, individual samples are grouped into batches before being copied to the GPU and fed to the AI model. Batching improves computational efficiency and often aids model convergence during training. Usually, batching involves <a class="af nc" href="https://pytorch.org/docs/stable/generated/torch.stack.html" rel="noopener ugc nofollow" target="_blank">stacking</a> all of the sample tensors along a new dimension — the <em class="oz">batch</em> dimension. However, <a class="af nc" href="https://pytorch.org/docs/stable/generated/torch.stack.html" rel="noopener ugc nofollow" target="_blank">torch.stack</a> requires all tensors to have the same shape, which is not the case with variable-length sequences.</p><h2 id="3a25" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Padding and its Inefficiencies</h2><p id="070d" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">The traditional way to address this challenge is to pad the input sequences to a fixed length and then perform <a class="af nc" href="https://pytorch.org/docs/stable/generated/torch.stack.html" rel="noopener ugc nofollow" target="_blank">stacking</a>. This solution requires appropriate masking within the model so that the output is not affected by the irrelevant tensor elements. In the case of attention layers, a padding mask indicates which tokens are padding and should not be attended to (e.g., see <a class="af nc" href="https://github.com/pytorch/pytorch/blob/v2.5.1/torch/nn/modules/activation.py#L1139" rel="noopener ugc nofollow" target="_blank">PyTorch MultiheadAttention</a>). However, padding can waste considerable GPU resources, increasing costs and slowing development. This is especially true for large-scale AI models.</p><h2 id="a983" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Don’t Pad, Concatenate</h2><p id="f9cb" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">One way to avoid padding is to <a class="af nc" href="https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat" rel="noopener ugc nofollow" target="_blank">concatenate</a> sequences along an existing dimension instead of <a class="af nc" href="https://pytorch.org/docs/stable/generated/torch.stack.html" rel="noopener ugc nofollow" target="_blank">stacking</a> them along a new dimension. Contrary to <a class="af nc" href="https://pytorch.org/docs/stable/generated/torch.stack.html" rel="noopener ugc nofollow" target="_blank">torch.stack</a>, <a class="af nc" href="https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat" rel="noopener ugc nofollow" target="_blank">torch.cat</a> allows inputs of different shapes. The output of concatenation is a single sequence whose length equals the sum of the lengths of the individual sequences. For this solution to work, our single sequence would need to be supplemented by an attention mask that would ensure that each token attends only to other tokens in the same original sequence, in a process sometimes referred to as <a class="af nc" href="https://pytorch.org/blog/flexattention/#document-maskingjagged-sequences" rel="noopener ugc nofollow" target="_blank">document masking</a>. Denoting the sum of the lengths of all of the individual sequences by <em class="oz">N </em>and adopting <a class="af nc" href="https://en.wikipedia.org/wiki/Big_O_notation" rel="noopener ugc nofollow" target="_blank">”big O” notation</a>, the size of this mask would need to be <em class="oz">O(N²)</em>, as would the compute complexity of a naive attention layer (which applies the mask only after calculating the attention scores), making this solution highly inefficient.</p><h2 id="29cb" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Attention Layer Optimization</h2><p id="ddf7" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">The solution to this problem comes in the form of specialized attention layers. Contrary to the standard attention layer that performs the full set of <em class="oz">O(N²) attention scores</em> only to mask out the irrelevant ones, these optimized attention kernels are designed to calculate only the <em class="oz">scores</em> that matter. In this post we will explore several solutions, each with their own distinct characteristics. These include:</p><ul class=""><li id="7e4c" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pa pb pc bk"><a class="af nc" href="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#nestedtensor-and-dense-tensor-support" rel="noopener ugc nofollow" target="_blank">PyTorch's SDPA (Scaled Dot Product Attention) with NestedTensors</a>,</li><li id="1949" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><a class="af nc" href="https://github.com/Dao-AILab/flash-attention" rel="noopener ugc nofollow" target="_blank">FlashAttention2</a>, and</li><li id="6682" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><a class="af nc" href="https://facebookresearch.github.io/xformers/components/ops.html" rel="noopener ugc nofollow" target="_blank">xFormers' memory-efficient attention</a>.</li></ul><h2 id="89cd" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Integration into Existing HuggingFace Models</h2><p id="c438" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">For teams working with pre-trained models, transitioning to these optimizations might seem challenging. We will demonstrate how <a class="af nc" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">HuggingFace’s</a> APIs simplify this process, enabling developers to integrate these techniques with minimal code changes and effort.</p><h2 id="89d6" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk"><strong class="al">Disclaimers</strong></h2><ul class=""><li id="f67d" class="nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny pa pb pc bk">Please do not interpret our use of any platforms, libraries, or optimization techniques as an endorsement for their use. The best options for you will depend greatly on the specifics of your own use-case.</li><li id="69bb" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk">Some of the APIs discussed here are in prototype or beta stages and may change in the future.</li><li id="eef1" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk">The code examples provided are for demonstrative purposes only. We make no claims regarding their accuracy, optimality, or robustness.</li></ul><p id="c3bd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Special thanks to <a class="af nc" href="https://www.linkedin.com/in/yitzhak-levi-49a217201/" rel="noopener ugc nofollow" target="_blank">Yitzhak Levi</a> and <a class="af nc" href="https://www.linkedin.com/in/peleg-nahaliel-b304a61a5/?originalSubdomain=il" rel="noopener ugc nofollow" target="_blank">Peleg Nahaliel</a> for their contributions to this post.</p><h1 id="481a" class="pi oa fq bf ob pj pk gq of pl pm gt oj pn po pp pq pr ps pt pu pv pw px py pz bk">Toy LLM Model</h1><p id="65f7" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">To facilitate our discussion we will define a simple generative model (partially inspired by the <a class="af nc" href="https://en.wikipedia.org/wiki/GPT" rel="noopener ugc nofollow" target="_blank">GPT</a> model defined <a class="af nc" href="https://github.com/karpathy/nanoGPT/tree/master" rel="noopener ugc nofollow" target="_blank">here</a>). For a more comprehensive guide on building language models, please see one of the many excellent tutorials available online (e.g., <a class="af nc" href="https://www.youtube.com/watch?v=kCc8FmEb1nY" rel="noopener ugc nofollow" target="_blank">here</a>).</p><h2 id="152a" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Transformer Block</h2><p id="a886" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">We begin by constructing a basic Transformer block, specifically designed to facilitate experimentation with different attention mechanisms and optimizations. While our block performs the same computation as standard Transformer blocks, we make slight modifications to the usual choice of operators in order to support the possibility of PyTorch <a class="af nc" href="https://pytorch.org/docs/stable/nested.html#supported-operations" rel="noopener ugc nofollow" target="_blank">NestedTensor</a> inputs (as described <a class="af nc" href="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#causal-self-attention" rel="noopener ugc nofollow" target="_blank">here</a>).</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="b722" class="qe oa fq qb b bg qf qg l qh qi"># general imports<br/>import time, functools<br/><br/># torch imports<br/>import torch<br/>from torch.utils.data import Dataset, DataLoader<br/>import torch.nn as nn<br/><br/># Define Transformer settings<br/>BATCH_SIZE = 32<br/>NUM_HEADS = 16<br/>HEAD_DIM = 64<br/>DIM = NUM_HEADS * HEAD_DIM<br/>DEPTH = 24<br/>NUM_TOKENS = 1024<br/>MAX_SEQ_LEN = 1024<br/>PAD_ID = 0<br/>DEVICE = 'cuda'<br/><br/>class MyAttentionBlock(nn.Module):<br/>    def __init__(<br/>            self,<br/>            attn_fn,<br/>            dim,<br/>            num_heads,<br/>            format=None,<br/>            **kwargs<br/>    ):<br/>        super().__init__()<br/>        self.attn_fn = attn_fn<br/>        self.num_heads = num_heads<br/>        self.dim = dim<br/>        self.head_dim = dim // num_heads<br/>        self.norm1 = nn.LayerNorm(dim, bias=False)<br/>        self.norm2 = nn.LayerNorm(dim, bias=False)<br/>        self.qkv = nn.Linear(dim, dim * 3)<br/>        self.proj = nn.Linear(dim, dim)<br/><br/>        # mlp layers<br/>        self.fc1 = nn.Linear(dim, dim * 4)<br/>        self.act = nn.GELU()<br/>        self.fc2 = nn.Linear(dim * 4, dim)<br/><br/>        self.permute = functools.partial(torch.transpose, dim0=1, dim1=2)<br/>        if format == 'bshd':<br/>            self.permute = nn.Identity()<br/><br/>    def mlp(self, x):<br/>        x = self.fc1(x)<br/>        x = self.act(x)<br/>        x = self.fc2(x)<br/>        return x<br/><br/>    def reshape_and_permute(self,x, batch_size):<br/>        x = x.view(batch_size, -1, self.num_heads, self.head_dim)<br/>        return self.permute(x)<br/><br/>    def forward(self, x_in, attn_mask=None):<br/>        batch_size = x_in.size(0)<br/>        x = self.norm1(x_in)<br/>        qkv = self.qkv(x)<br/><br/>        # rather than first reformatting and then splitting the input<br/>        # state, we first split and then reformat q, k, v in order to<br/>        # support PyTorch Nested Tensors<br/>        q, k, v = qkv.chunk(3, -1)<br/>        q = self.reshape_and_permute(q, batch_size)<br/>        k = self.reshape_and_permute(k, batch_size)<br/>        v = self.reshape_and_permute(v, batch_size)<br/>        <br/>        # call the attn_fn with the input attn_mask<br/>        x = self.attn_fn(q, k, v, attn_mask=attn_mask)<br/><br/>        # reformat output<br/>        x = self.permute(x).reshape(batch_size, -1, self.dim)<br/>        x = self.proj(x)<br/>        x = x + x_in<br/>        x = x + self.mlp(self.norm2(x))<br/>        return x</span></pre><h2 id="676d" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Transformer Decoder Model</h2><p id="bb36" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Building on our programmable Transformer block, we construct a typical Transformer decoder model.</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="b915" class="qe oa fq qb b bg qf qg l qh qi">class MyDecoder(nn.Module):<br/>    def __init__(<br/>            self,<br/>            block_fn,<br/>            num_tokens,<br/>            dim,<br/>            num_heads,<br/>            num_layers,<br/>            max_seq_len,<br/>            pad_idx=None<br/>    ):<br/>        super().__init__()<br/>        self.num_heads = num_heads<br/>        self.pad_idx = pad_idx<br/>        self.embedding = nn.Embedding(num_tokens, dim, padding_idx=pad_idx)<br/>        self.positional_embedding = nn.Embedding(max_seq_len, dim)<br/>        self.blocks = nn.ModuleList([<br/>            block_fn(<br/>                dim=dim,<br/>                num_heads=num_heads<br/>            )<br/>            for _ in range(num_layers)])<br/>        self.output = nn.Linear(dim, num_tokens)<br/><br/>    def embed_tokens(self, input_ids, position_ids=None):<br/>        x = self.embedding(input_ids)<br/>        if position_ids is None:<br/>            position_ids = torch.arange(input_ids.shape[1],<br/>                                        device=x.device)<br/>        x = x + self.positional_embedding(position_ids)<br/>        return x<br/><br/>    def forward(self, input_ids, position_ids=None, attn_mask=None):<br/>        # Embed tokens and add positional encoding<br/>        x = self.embed_tokens(input_ids, position_ids)<br/>        if self.pad_idx is not None:<br/>            assert attn_mask is None<br/>            # create a padding mask - we assume boolean masking<br/>            attn_mask = (input_ids != self.pad_idx)<br/>            attn_mask = attn_mask.view(BATCH_SIZE, 1, 1, -1) \<br/>                .expand(-1, self.num_heads, -1, -1)<br/><br/>        for b in self.blocks:<br/>            x = b(x, attn_mask)<br/><br/>        logits = self.output(x)<br/>        return logits</span></pre><h2 id="9ef6" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Variable Length Sequence Input</h2><p id="f054" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Next, we create a dataset containing sequences of variable lengths, where each sequence is made up of randomly generated tokens. For simplicity, we (arbitrarily) select a fixed distribution for the sequence lengths. In real-world scenarios, the distribution of sequence lengths typically reflects the nature of the data, such as the length of documents or audio segments. Note, that the distribution of lengths directly affects the computational inefficiencies caused by padding.</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="f167" class="qe oa fq qb b bg qf qg l qh qi"># Use random data<br/>class FakeDataset(Dataset):<br/>    def __len__(self):<br/>        return 1000000<br/><br/>    def __getitem__(self, index):<br/>        length = torch.randint(1, MAX_SEQ_LEN, (1,))<br/>        sequence = torch.randint(1, NUM_TOKENS, (length + 1,))<br/>        inputs = sequence[:-1]<br/>        targets = sequence[1:]<br/>        return inputs, targets<br/><br/>def pad_sequence(sequence, length, pad_val):<br/>    return torch.nn.functional.pad(<br/>        sequence,<br/>        (0, length - sequence.shape[0]),<br/>        value=pad_val<br/>    )<br/><br/>def collate_with_padding(batch):<br/>    padded_inputs = []<br/>    padded_targets = []<br/>    for b in batch:<br/>        padded_inputs.append(pad_sequence(b[0], MAX_SEQ_LEN, PAD_ID))<br/>        padded_targets.append(pad_sequence(b[1], MAX_SEQ_LEN, PAD_ID))<br/>    padded_inputs = torch.stack(padded_inputs, dim=0)<br/>    padded_targets = torch.stack(padded_targets, dim=0)<br/>    return {<br/>        'inputs': padded_inputs,<br/>        'targets': padded_targets<br/>    }<br/><br/>def data_to_device(data, device):<br/>    if isinstance(data, dict):<br/>        return {<br/>            key: data_to_device(val,device)<br/>            for key, val in data.items()<br/>        }<br/>    elif isinstance(data, (list, tuple)):<br/>        return type(data)(<br/>            data_to_device(val, device) for val in data<br/>        )<br/>    elif isinstance(data, torch.Tensor):<br/>        return data.to(device=device, non_blocking=True)<br/>    else:<br/>        return data.to(device=device)</span></pre><h2 id="eaea" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Training/Evaluation Loop</h2><p id="18b1" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Lastly, we implement a <em class="oz">main</em> function that performs training/evaluation on input sequences of varying length.</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="8dc2" class="qe oa fq qb b bg qf qg l qh qi">def main(<br/>    block_fn, <br/>    data_collate_fn=collate_with_padding,<br/>    pad_idx=None,<br/>    train=True,<br/>    compile=False<br/>):<br/>    torch.random.manual_seed(0)<br/>    device = torch.device(DEVICE)<br/>    torch.set_float32_matmul_precision("high")<br/><br/>    # Create dataset and dataloader<br/>    data_set = FakeDataset()<br/>    data_loader = DataLoader(<br/>        data_set,<br/>        batch_size=BATCH_SIZE,<br/>        collate_fn=data_collate_fn,<br/>        num_workers=12,<br/>        pin_memory=True,<br/>        drop_last=True<br/>    )<br/><br/>    model = MyDecoder(<br/>        block_fn=block_fn,<br/>        num_tokens=NUM_TOKENS,<br/>        dim=DIM,<br/>        num_heads=NUM_HEADS,<br/>        num_layers=DEPTH,<br/>        max_seq_len=MAX_SEQ_LEN,<br/>        pad_idx=pad_idx<br/>    ).to(device)<br/><br/>    if compile:<br/>        model = torch.compile(model)<br/><br/>    # Define loss and optimizer<br/>    criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_ID)<br/>    optimizer = torch.optim.SGD(model.parameters())<br/><br/>    def train_step(model, inputs, targets, <br/>                   position_ids=None, attn_mask=None):<br/>        with torch.amp.autocast(DEVICE, dtype=torch.bfloat16):<br/>            outputs = model(inputs, position_ids, attn_mask)<br/>            outputs = outputs.view(-1, NUM_TOKENS)<br/>            targets = targets.flatten()<br/>            loss = criterion(outputs, targets)<br/>        optimizer.zero_grad(set_to_none=True)<br/>        loss.backward()<br/>        optimizer.step()<br/><br/>    @torch.no_grad()<br/>    def eval_step(model, inputs, targets, <br/>                  position_ids=None, attn_mask=None):<br/>        with torch.amp.autocast(DEVICE, dtype=torch.bfloat16):<br/>            outputs = model(inputs, position_ids, attn_mask)<br/>            if outputs.is_nested:<br/>                outputs = outputs.data._values<br/>                targets = targets.data._values<br/>            else:<br/>                outputs = outputs.view(-1, NUM_TOKENS)<br/>                targets = targets.flatten()<br/>            loss = criterion(outputs, targets)<br/>        return loss<br/><br/>    if train:<br/>        model.train()<br/>        step_fn = train_step<br/>    else:<br/>        model.eval()<br/>        step_fn = eval_step<br/><br/>    t0 = time.perf_counter()<br/>    summ = 0<br/>    count = 0<br/><br/>    for step, data in enumerate(data_loader):<br/>        # Copy data to GPU<br/>        data = data_to_device(data, device=device)<br/>        step_fn(model, data['inputs'], data['targets'],<br/>                       position_ids=data.get('indices'),<br/>                       attn_mask=data.get('attn_mask'))<br/><br/>        # Capture step time<br/>        batch_time = time.perf_counter() - t0<br/>        if step &gt; 20:  # Skip first steps<br/>            summ += batch_time<br/>            count += 1<br/>        t0 = time.perf_counter()<br/>        if step &gt;= 100:<br/>            break<br/>    print(f'average step time: {summ / count}')</span></pre><h2 id="1192" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">PyTorch SDPA with Padding</h2><p id="b2d6" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">For our baseline experiments, we configure our Transformer block to utilize PyTorch’s <a class="af nc" href="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html" rel="noopener ugc nofollow" target="_blank">SDPA</a> mechanism. In our experiments, we run both training and evaluation, both with and without <a class="af nc" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="noopener ugc nofollow" target="_blank">torch.compile</a>. These were run on an <a class="af nc" href="https://www.nvidia.com/en-eu/data-center/h100/" rel="noopener ugc nofollow" target="_blank">NVIDIA H100</a> with <a class="af nc" href="https://developer.nvidia.com/cuda-toolkit" rel="noopener ugc nofollow" target="_blank">CUDA 12.4</a> and <a class="af nc" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">PyTorch</a> 2.5.1</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="0853" class="qe oa fq qb b bg qf qg l qh qi">from torch.nn.functional import scaled_dot_product_attention as sdpa<br/>block_fn = functools.partial(MyAttentionBlock, attn_fn=sdpa)<br/>causal_block_fn = functools.partial(<br/>    MyAttentionBlock,<br/>    attn_fn=functools.partial(sdpa, is_causal=True)<br/>)<br/><br/>for mode in ['eval', 'train']:<br/>    for compile in [False, True]:<br/>        block_func = causal_block_fn\<br/>            if mode == 'train' else block_fn<br/>        print(f'{mode} with {collate}, '<br/>              f'{"compiled" if compile else "uncompiled"}')<br/>        main(block_fn=block_func,<br/>             pad_idx=PAD_ID,<br/>             train=mode=='train',<br/>             compile=compile)</span></pre><p id="9a4f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Performance Results:</p><ul class=""><li id="3af2" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pa pb pc bk"><strong class="nf fr">Evaluation</strong>: 132 milliseconds (ms) without torch.compile, 130 ms with torch.compile</li><li id="75ef" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">Training</strong>: 342 ms without torch.compile, 299 ms with torch.compile</li></ul><h1 id="d382" class="pi oa fq bf ob pj pk gq of pl pm gt oj pn po pp pq pr ps pt pu pv pw px py pz bk">Optimizing for Variable Length Input</h1><p id="ad4d" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In this section, we will explore several optimization techniques for handling variable-length input sequences in Transformer models.</p><h2 id="7f5e" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Padding Optimization</h2><p id="f78a" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Our first optimization relates not to the attention kernel but to our padding mechanism. Rather than padding the sequences in each batch to a constant length, we pad to the length of the longest sequence in the batch. The following block of code consists of our revised collation function and updated experiments.</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="f77f" class="qe oa fq qb b bg qf qg l qh qi">def collate_pad_to_longest(batch):<br/>    padded_inputs = []<br/>    padded_targets = []<br/>    max_length = max([b[0].shape[0] for b in batch])<br/>    for b in batch:<br/>        padded_inputs.append(pad_sequence(b[0], max_length, PAD_ID))<br/>        padded_targets.append(pad_sequence(b[1], max_length, PAD_ID))<br/>    padded_inputs = torch.stack(padded_inputs, dim=0)<br/>    padded_targets = torch.stack(padded_targets, dim=0)<br/>    return {<br/>        'inputs': padded_inputs,<br/>        'targets': padded_targets<br/>    }<br/><br/>for mode in ['eval', 'train']:<br/>    for compile in [False, True]:<br/>        block_func = causal_block_fn\<br/>            if mode == 'train' else block_fn<br/>        print(f'{mode} with {collate}, '<br/>              f'{"compiled" if compile else "uncompiled"}')<br/>        main(block_fn=block_func,<br/>             data_collate_fn=collate_pad_to_longest,<br/>             pad_idx=PAD_ID,<br/>             train=mode=='train',<br/>             compile=compile)</span></pre><p id="5fca" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Padding to the longest sequence in each batch results in a slight performance acceleration:</p><ul class=""><li id="6440" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pa pb pc bk"><strong class="nf fr">Evaluation</strong>: 129 ms without torch.compile, 116 ms with torch.compile</li><li id="544c" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">Training</strong>: 337 ms without torch.compile, 294 ms with torch.compile</li></ul><h2 id="cf4f" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">SDPA with PyTorch NestedTensors</h2><p id="6cdd" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Next, we take advantage of the built-in support for <a class="af nc" href="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#nestedtensor-and-dense-tensor-support" rel="noopener ugc nofollow" target="_blank">PyTorch NestedTensors</a> in SDPA in evaluation mode. Currently a prototype feature, <a class="af nc" href="https://pytorch.org/tutorials/prototype/nestedtensor.html" rel="noopener ugc nofollow" target="_blank">PyTorch NestedTensors</a> allows for grouping together tensors of varying length. These are sometimes referred to as <em class="oz">jagged</em> or <em class="oz">ragged</em> tensors. In the code block below, we define a collation function for grouping our sequences into NestedTensors. We also define an <em class="oz">indices </em>entry so that we can properly calculate the <a class="af nc" href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" rel="noopener ugc nofollow" target="_blank">positional embeddings</a>.</p><p id="5aff" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">PyTorch NestedTensors are supported by a <a class="af nc" href="https://pytorch.org/tutorials/prototype/nestedtensor.html#nested-tensor-operations" rel="noopener ugc nofollow" target="_blank">limited number of PyTorch ops</a>. Working around these limitations can require some creativity. For example, addition between NestedTensors is only supported when they share precisely the same “jagged” shape. In the code below we use a workaround to ensure that the <em class="oz">indices </em>entry shares the same shape as the model inputs.</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="8377" class="qe oa fq qb b bg qf qg l qh qi">def nested_tensor_collate(batch):<br/>    inputs = torch.nested.as_nested_tensor([b[0] for b in batch],<br/>                                           layout=torch.jagged)<br/>    targets = torch.nested.as_nested_tensor([b[1] for b in batch],<br/>                                            layout=torch.jagged)<br/>    indices = torch.concat([torch.arange(b[0].shape[0]) for b in batch])<br/><br/>    # workaround for creating a NestedTensor with identical "jagged" shape<br/>    xx = torch.empty_like(inputs)<br/>    xx.data._values[:] = indices<br/><br/>    return {<br/>        'inputs': inputs,<br/>        'targets': targets,<br/>        'indices': xx<br/>    }<br/><br/>for compile in [False, True]:<br/>    print(f'eval with nested tensors, '<br/>          f'{"compiled" if compile else "uncompiled"}')<br/>    main(<br/>        block_fn=block_fn,<br/>        data_collate_fn=nested_tensor_collate,<br/>        train=False,<br/>        compile=compile<br/>    )</span></pre><p id="3270" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Although, with torch.compile, the NestedTensor optimization results in a step time of 131 ms, similar to our baseline result, in compiled mode the step time drops to 42 ms for an impressive ~3x improvement.</p><h2 id="ee84" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">FlashAttention2</h2><p id="8e7a" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In our <a class="af nc" rel="noopener" target="_blank" href="/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6">previous post</a> we demonstrated the use of <a class="af nc" href="https://github.com/Dao-AILab/flash-attention" rel="noopener ugc nofollow" target="_blank">FlashAttention</a> and its impact on the performance of a transformer model. In this post we demonstrate the use of <a class="af nc" href="https://github.com/Dao-AILab/flash-attention/blob/v2.7.0/hopper/flash_attn_interface.py#L429" rel="noopener ugc nofollow" target="_blank">flash_attn_varlen_func</a> from <a class="af nc" href="https://pypi.org/project/flash-attn/" rel="noopener ugc nofollow" target="_blank">flash-attn (2.7.0)</a>, an API designed for use with variable-sized inputs. To use this function, we concatenate all of the sequences in the batch into a single sequence. We also create a <em class="oz">cu_seqlens </em>tensor that points to the indices within the concatenated tensor where each of the individual sequences start. The code block below includes our collation function followed by evaluation and training experiments. Note, that <a class="af nc" href="https://github.com/Dao-AILab/flash-attention/blob/v2.7.0/hopper/flash_attn_interface.py#L429" rel="noopener ugc nofollow" target="_blank">flash_attn_varlen_func</a> does not support torch.compile (at the time of this writing).</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="ea71" class="qe oa fq qb b bg qf qg l qh qi">def collate_concat(batch):<br/>    inputs = torch.concat([b[0] for b in batch]).unsqueeze(0)<br/>    targets = torch.concat([b[1] for b in batch]).unsqueeze(0)<br/>    indices = torch.concat([torch.arange(b[0].shape[0]) for b in batch])<br/>    seqlens = torch.tensor([b[0].shape[0] for b in batch])<br/>    seqlens = torch.cumsum(seqlens, dim=0, dtype=torch.int32)<br/>    cu_seqlens = torch.nn.functional.pad(seqlens, (1, 0))<br/><br/>    return {<br/>        'inputs': inputs,<br/>        'targets': targets,<br/>        'indices': indices,<br/>        'attn_mask': cu_seqlens<br/>    }<br/><br/>from flash_attn import flash_attn_varlen_func<br/>fa_varlen = lambda q, k, v, attn_mask: flash_attn_varlen_func(<br/>    q.squeeze(0),<br/>    k.squeeze(0),<br/>    v.squeeze(0),<br/>    cu_seqlens_q=attn_mask,<br/>    cu_seqlens_k=attn_mask,<br/>    max_seqlen_q=MAX_SEQ_LEN,<br/>    max_seqlen_k=MAX_SEQ_LEN<br/>).unsqueeze(0)<br/><br/>fa_varlen_causal = lambda q, k, v, attn_mask: flash_attn_varlen_func(<br/>    q.squeeze(0),<br/>    k.squeeze(0),<br/>    v.squeeze(0),<br/>    cu_seqlens_q=attn_mask,<br/>    cu_seqlens_k=attn_mask,<br/>    max_seqlen_q=MAX_SEQ_LEN,<br/>    max_seqlen_k=MAX_SEQ_LEN,<br/>    causal=True<br/>).unsqueeze(0)<br/><br/>block_fn = functools.partial(MyAttentionBlock,<br/>                             attn_fn=fa_varlen,<br/>                             format='bshd')<br/><br/>causal_block_fn = functools.partial(MyAttentionBlock,<br/>                                    attn_fn=fa_varlen_causal,<br/>                                    format='bshd')<br/><br/>print('flash-attn eval')<br/>main(<br/>    block_fn=block_fn,<br/>    data_collate_fn=collate_concat,<br/>    train=False<br/>)<br/><br/>print('flash-attn train')<br/>main(<br/>    block_fn=causal_block_fn,<br/>    data_collate_fn=collate_concat,<br/>    train=True,<br/>)</span></pre><p id="c006" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The impact of this optimization is dramatic, 51 ms for evaluation and 160 ms for training, amounting to 2.6x and 2.1x performance boosts compared to our baseline experiment.</p><h2 id="4ab6" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">XFormers Memory Efficient Attention</h2><p id="2538" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In our previous post we demonstrated the use of the <a class="af nc" href="https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention" rel="noopener ugc nofollow" target="_blank">memory_efficient_attention</a> operator from <a class="af nc" href="https://pypi.org/project/xformers/" rel="noopener ugc nofollow" target="_blank">xFormers (0.0.28)</a>. Here we demonstrate the use of <a class="af nc" href="https://facebookresearch.github.io/xformers/_modules/xformers/ops/fmha/attn_bias.html#BlockDiagonalMask" rel="noopener ugc nofollow" target="_blank">BlockDiagonalMask</a>, which is specifically designed for input sequences of arbitrary length. The required collation function appears in the code block below followed by the evaluation and training experiments. Note, that torch.compile failed in training mode.</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="99af" class="qe oa fq qb b bg qf qg l qh qi">from xformers.ops import fmha<br/>from xformers.ops import memory_efficient_attention as mea<br/><br/>def collate_xformer(batch):<br/>    inputs = torch.concat([b[0] for b in batch]).unsqueeze(0)<br/>    targets = torch.concat([b[1] for b in batch]).unsqueeze(0)<br/>    indices = torch.concat([torch.arange(b[0].shape[0]) for b in batch])<br/>    seqlens = [b[0].shape[0] for b in batch]<br/>    batch_sizes = [1 for b in batch]<br/>    block_diag = fmha.BlockDiagonalMask.from_seqlens(seqlens, device='cpu')<br/>    block_diag._batch_sizes = batch_sizes<br/><br/>    return {<br/>        'inputs': inputs,<br/>        'targets': targets,<br/>        'indices': indices,<br/>        'attn_mask': block_diag<br/>    }<br/><br/>mea_eval = lambda q, k, v, attn_mask: mea(<br/>    q,k,v, attn_bias=attn_mask)<br/><br/>mea_train = lambda q, k, v, attn_mask: mea(<br/>    q,k,v, attn_bias=attn_mask.make_causal())<br/><br/>block_fn = functools.partial(MyAttentionBlock,<br/>                             attn_fn=mea_eval,<br/>                             format='bshd')<br/><br/>causal_block_fn = functools.partial(MyAttentionBlock,<br/>                             attn_fn=mea_train,<br/>                             format='bshd')<br/><br/>print(f'xFormer Attention ')<br/>for compile in [False, True]:<br/>    print(f'eval with xFormer Attention, '<br/>          f'{"compiled" if compile else "uncompiled"}')<br/>    main(block_fn=block_fn,<br/>         train=False,<br/>         data_collate_fn=collate_xformer,<br/>         compile=compile)<br/><br/>print(f'train with xFormer Attention')<br/>main(block_fn=causal_block_fn,<br/>     train=True,<br/>     data_collate_fn=collate_xformer)</span></pre><p id="0a8d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The resultant step time were 50 ms and 159 ms for evaluation and training without torch.compile. Evaluation with torch.compile resulted in a step time of 42 ms.</p><h2 id="20b4" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Results</h2><p id="9f88" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">The table below summarizes the results of our optimization methods.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qj"><img src="../Images/2797be369d1201a6cbfde979b8581934.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oNIilOLnAXOGMTW3gZmzYg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Step time results for different optimization methods (lower is better) — by Author</figcaption></figure><p id="8b68" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The best performer for our toy model is <a class="af nc" href="https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention" rel="noopener ugc nofollow" target="_blank">xFormer’s memory_efficient_attention</a> which delivered a ~3x performance for evaluation and ~2x performance for training. We caution against deriving any conclusions from these results as the performance impact of different attention functions can vary significantly depending on the specific model and use case.</p><h1 id="e734" class="pi oa fq bf ob pj pk gq of pl pm gt oj pn po pp pq pr ps pt pu pv pw px py pz bk">Optimizing a HuggingFace Model for Variable-Length Input</h1><p id="cd10" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">The tools and techniques described above are easy to implement when creating a model from scratch. However, these days it is not uncommon for ML developers to adopt existing (pretrained) models and finetune them for their use case. While the optimizations we have described can be integrated without changing the set of model weights and without altering the model behavior, it is not entirely clear what the best way to do this is. In an ideal world, our ML framework would allow us to program the use of an attention mechanism that is optimized for variable-length inputs. In this section, we demonstrate how to optimize HuggingFace models for variable-length inputs.</p><h2 id="57f8" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">A Toy HuggingFace Model - GPT2LMHeadModel</h2><p id="baf0" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">To facilitate the discussion, we create a toy example in which we train a HuggingFace <a class="af nc" href="https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/gpt2#transformers.GPT2LMHeadModel" rel="noopener ugc nofollow" target="_blank">GPT2LMHead</a> model on variable-length sequences. This requires adapting our random dataset and data-padding collation function according to HuggingFace's input specifications.</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="8d3b" class="qe oa fq qb b bg qf qg l qh qi">from transformers import GPT2Config, GPT2LMHeadModel<br/><br/># Use random data<br/>class HuggingFaceFakeDataset(Dataset):<br/>    def __len__(self):<br/>        return 1000000<br/><br/>    def __getitem__(self, index):<br/>        length = torch.randint(1, MAX_SEQ_LEN, (1,))<br/>        input_ids = torch.randint(1, NUM_TOKENS, (length,))<br/>        labels = input_ids.clone()<br/>        labels[0] = PAD_ID # ignore first token<br/>        return {<br/>            'input_ids': input_ids,<br/>            'labels': labels<br/>        }<br/>        return input_ids, labels<br/><br/>def hf_collate_with_padding(batch):<br/>    padded_inputs = []<br/>    padded_labels = []<br/>    for b in batch:<br/>        input_ids = b['input_ids']<br/>        labels = b['labels']<br/>        padded_inputs.append(pad_sequence(input_ids, MAX_SEQ_LEN, PAD_ID))<br/>        padded_labels.append(pad_sequence(labels, MAX_SEQ_LEN, PAD_ID))<br/>    padded_inputs = torch.stack(padded_inputs, dim=0)<br/>    padded_labels = torch.stack(padded_labels, dim=0)<br/>    return {<br/>        'input_ids': padded_inputs,<br/>        'labels': padded_labels,<br/>        'attention_mask': (padded_inputs != PAD_ID)<br/>    }</span></pre><h2 id="98c8" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Training Function</h2><p id="b940" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Our training function instantiates a <a class="af nc" href="https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/gpt2#transformers.GPT2LMHeadModel" rel="noopener ugc nofollow" target="_blank">GPT2LMHeadModel</a> based on the requested <a class="af nc" href="https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/gpt2#transformers.GPT2Config" rel="noopener ugc nofollow" target="_blank">GPT2Config</a> and proceeds to train it on our variable-length sequences.</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="6307" class="qe oa fq qb b bg qf qg l qh qi">def hf_main(<br/>    config,<br/>    collate_fn=hf_collate_with_padding,<br/>    compile=False<br/>):<br/>    torch.random.manual_seed(0)<br/>    device = torch.device(DEVICE)<br/>    torch.set_float32_matmul_precision("high")<br/><br/>    # Create dataset and dataloader<br/>    data_set = HuggingFaceFakeDataset()<br/>    data_loader = DataLoader(<br/>        data_set,<br/>        batch_size=BATCH_SIZE,<br/>        collate_fn=collate_fn,<br/>        num_workers=12 if DEVICE == "CUDA" else 0,<br/>        pin_memory=True,<br/>        drop_last=True<br/>    )<br/><br/>    model = GPT2LMHeadModel(config).to(device)<br/><br/>    if compile:<br/>        model = torch.compile(model)<br/><br/>    # Define loss and optimizer<br/>    criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_ID)<br/>    optimizer = torch.optim.SGD(model.parameters())<br/><br/>    model.train()<br/><br/>    t0 = time.perf_counter()<br/>    summ = 0<br/>    count = 0<br/><br/>    for step, data in enumerate(data_loader):<br/>        # Copy data to GPU<br/>        data = data_to_device(data, device=device)<br/>        input_ids = data['input_ids']<br/>        labels = data['labels']<br/>        position_ids = data.get('position_ids')<br/>        attn_mask = data.get('attention_mask')<br/>        with torch.amp.autocast(DEVICE, dtype=torch.bfloat16):<br/>            outputs = model(input_ids=input_ids,<br/>                            position_ids=position_ids,<br/>                            attention_mask=attn_mask)<br/>            logits = outputs.logits[..., :-1, :].contiguous()<br/>            labels = labels[..., 1:].contiguous()<br/>            loss = criterion(logits.view(-1, NUM_TOKENS), labels.flatten())<br/><br/>        optimizer.zero_grad(set_to_none=True)<br/>        loss.backward()<br/>        optimizer.step()<br/><br/>        # Capture step time<br/>        batch_time = time.perf_counter() - t0<br/>        if step &gt; 20:  # Skip first steps<br/>            summ += batch_time<br/>            count += 1<br/>        t0 = time.perf_counter()<br/>        if step &gt;= 100:<br/>            break<br/>    print(f'average step time: {summ / count}')</span></pre><h2 id="f9be" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">SDPA with Padding</h2><p id="e11d" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In the callback below we call our training function with the default sequence-padding collator.</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="229f" class="qe oa fq qb b bg qf qg l qh qi">config = GPT2Config(<br/>        n_layer=DEPTH,<br/>        n_embd=DIM,<br/>        n_head=NUM_HEADS,<br/>        vocab_size=NUM_TOKENS,<br/>    )<br/><br/>for compile in [False, True]:<br/>    print(f"HF GPT2 train with SDPA, compile={compile}")<br/>    hf_main(config=config, compile=compile)</span></pre><p id="e122" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The resultant step times are 815 ms without torch.compile and 440 ms with torch.compile.</p><h2 id="60ff" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">FlashAttention2</h2><p id="909c" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">We now take advantage of HuggingFace’s <a class="af nc" href="https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/gpt2#using-flash-attention-2" rel="noopener ugc nofollow" target="_blank">built-in support for FlashAttention2</a>, by setting the <em class="oz">attn_implementation </em>parameter to “flash_attention_2”. Behind the scenes, HuggingFace will <a class="af nc" href="https://github.com/huggingface/transformers/blob/v4.46.3/src/transformers/modeling_flash_attention_utils.py#L246" rel="noopener ugc nofollow" target="_blank"><em class="oz">unpad</em></a> the padded data input and then pass them to the optimized <a class="af nc" href="https://github.com/Dao-AILab/flash-attention/blob/v2.7.0/hopper/flash_attn_interface.py#L429" rel="noopener ugc nofollow" target="_blank">flash_attn_varlen_func</a> function we saw above:</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="a676" class="qe oa fq qb b bg qf qg l qh qi">flash_config = GPT2Config(<br/>        n_layer=DEPTH,<br/>        n_embd=DIM,<br/>        n_head=NUM_HEADS,<br/>        vocab_size=NUM_TOKENS,<br/>        attn_implementation='flash_attention_2'<br/>    )<br/><br/>print(f"HF GPT2 train with flash")<br/>hf_main(config=flash_config)</span></pre><p id="d656" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The resultant time step is 620 ms, amounting to a 30% boost (in uncompiled mode) with just a simple flick of a switch.</p><h2 id="0192" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">FlashAttention2 with Unpadded Input</h2><p id="f457" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Of course, padding the sequences in the collation function only to have them unpadded, hardly seems sensible. In a recent <a class="af nc" href="https://huggingface.co/blog/packing-with-FA2" rel="noopener ugc nofollow" target="_blank">update to HuggingFace</a>, support was added for passing in concatenated (unpadded) sequences to a select number of models. Unfortunately, (as of the time of this writing) our GPT2 model did not make the cut. However, adding support requires just five small line additions to <a class="af nc" href="https://github.com/huggingface/transformers/blob/v4.46.3/src/transformers/models/gpt2/modeling_gpt2.py" rel="noopener ugc nofollow" target="_blank">modeling_gpt2.py</a> in order to propagate the sequence <a class="af nc" href="https://github.com/huggingface/transformers/blob/v4.46.3/src/transformers/models/gpt2/modeling_gpt2.py#L985" rel="noopener ugc nofollow" target="_blank"><em class="oz">position_ids</em></a><em class="oz"> </em>to the <a class="af nc" href="https://github.com/huggingface/transformers/blob/v4.46.3/src/transformers/models/gpt2/modeling_gpt2.py#L436" rel="noopener ugc nofollow" target="_blank">flash-attention kernel</a>. The full <em class="oz">patch </em>appears in the block below:</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="2227" class="qe oa fq qb b bg qf qg l qh qi">@@ -370,0 +371 @@<br/>+        position_ids = None<br/>@@ -444,0 +446 @@<br/>+            position_ids=position_ids<br/>@@ -611,0 +614 @@<br/>+        position_ids=None<br/>@@ -621,0 +625 @@<br/>+            position_ids=position_ids<br/>@@ -1140,0 +1145 @@<br/>+                    position_ids=position_ids</span></pre><p id="ac2a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We define a collate function that concatenates our sequences and train our hugging face model on unpadded sequences. (Also see the built-in <a class="af nc" href="https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithFlattening" rel="noopener ugc nofollow" target="_blank">DataCollatorWithFlattening</a> utility.)</p><pre class="mm mn mo mp mq qa qb qc bp qd bb bk"><span id="5ea5" class="qe oa fq qb b bg qf qg l qh qi">def collate_flatten(batch):<br/>    input_ids = torch.concat([b['input_ids'] for b in batch]).unsqueeze(0)<br/>    labels = torch.concat([b['labels'] for b in batch]).unsqueeze(0)<br/>    position_ids = [torch.arange(b['input_ids'].shape[0]) for b in batch]<br/>    position_ids = torch.concat(position_ids)<br/><br/>    return {<br/>        'input_ids': input_ids,<br/>        'labels': labels,<br/>        'position_ids': position_ids<br/>    }<br/><br/>print(f"HF GPT2 train with flash, no padding")<br/>hf_main(config=flash_config, collate_fn=collate_flatten)</span></pre><p id="4561" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The resulting step time is 323 ms, 90% faster than running flash-attention on the padded input.</p><h2 id="25b4" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Results</h2><p id="cfce" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">The results of our HuggingFace experiments are summarized below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qk"><img src="../Images/a52cc54ef95a4e7ba82fb1f706350d64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*ZNq4Hw1nKM4L7QMC5rOVHg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Step time results for different optimization methods (lower is better) — by Author</figcaption></figure><p id="22a3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">With little effort, we were able to boost our runtime performance by 2.5x when compared to the uncompiled baseline experiment, and by 36% when compared to the compiled version.</p><p id="684b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this section, we demonstrated how the HuggingFace APIs allow us to leverage the optimized kernels in FlashAttention2, significantly boosting the training performance of existing models on sequences of varying length.</p><h1 id="d97e" class="pi oa fq bf ob pj pk gq of pl pm gt oj pn po pp pq pr ps pt pu pv pw px py pz bk">Summary</h1><p id="bb55" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">As AI models continue to grow in both popularity and complexity, optimizing their performance has become essential for reducing runtime and costs. This is especially true for compute-intensive components like attention layers. In this post, we have continued our exploration of attention layer optimization, and demonstrated new tools and techniques for enhancing Transformer model performance. For more insights on AI model optimization, be sure to check out the <a class="af nc" rel="noopener" target="_blank" href="/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6">first post</a> in this series as well as our <a class="af nc" href="https://chaimrand.medium.com/" rel="noopener">many other posts</a> on this topic.</p></div></div></div></div>    
</body>
</html>