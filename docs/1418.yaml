- en: Automating Prompt Engineering with DSPy and Haystack
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DSPy和Haystack自动化提示工程
- en: 原文：[https://towardsdatascience.com/automating-prompt-engineering-with-dspy-and-haystack-926a637a3f43?source=collection_archive---------0-----------------------#2024-06-07](https://towardsdatascience.com/automating-prompt-engineering-with-dspy-and-haystack-926a637a3f43?source=collection_archive---------0-----------------------#2024-06-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/automating-prompt-engineering-with-dspy-and-haystack-926a637a3f43?source=collection_archive---------0-----------------------#2024-06-07](https://towardsdatascience.com/automating-prompt-engineering-with-dspy-and-haystack-926a637a3f43?source=collection_archive---------0-----------------------#2024-06-07)
- en: Teach your LLM how to talk through examples
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过示例教会你的LLM如何回答问题
- en: '[](https://medium.com/@dataqa?source=post_page---byline--926a637a3f43--------------------------------)[![Maria
    Mestre](../Images/3e2586487691b731293b1bfa091b651e.png)](https://medium.com/@dataqa?source=post_page---byline--926a637a3f43--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--926a637a3f43--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--926a637a3f43--------------------------------)
    [Maria Mestre](https://medium.com/@dataqa?source=post_page---byline--926a637a3f43--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dataqa?source=post_page---byline--926a637a3f43--------------------------------)[![Maria
    Mestre](../Images/3e2586487691b731293b1bfa091b651e.png)](https://medium.com/@dataqa?source=post_page---byline--926a637a3f43--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--926a637a3f43--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--926a637a3f43--------------------------------)
    [Maria Mestre](https://medium.com/@dataqa?source=post_page---byline--926a637a3f43--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--926a637a3f43--------------------------------)
    ·9 min read·Jun 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--926a637a3f43--------------------------------)
    ·阅读时长9分钟·2024年6月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/da9ea834c6eb33d8ee5a3ba9234bf9f0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da9ea834c6eb33d8ee5a3ba9234bf9f0.png)'
- en: Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 摄影：由[Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=medium&utm_medium=referral)提供，图片来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: One of the most frustrating parts of building gen-AI applications is the manual
    process of optimising prompts. In a [publication](https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product)
    made by LinkedIn earlier this year, they described what they learned after deploying
    an agentic RAG application. One of the main challenges was obtaining consistent
    quality. They spent 4 months tweaking various parts of the application, including
    prompts, to mitigate issues such as hallucination.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 构建生成式AI应用程序时，最令人沮丧的部分之一是手动优化提示的过程。在今年早些时候LinkedIn发布的[文章](https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product)中，他们描述了在部署一个代理式RAG应用程序后学到的经验。其中一个主要挑战是确保一致的质量。他们花了4个月时间调整应用程序的各个部分，包括提示，以缓解诸如幻觉等问题。
- en: DSPy is an open-source library that tries to parameterise prompts so that it
    becomes an optimisation problem. The [original paper](https://arxiv.org/abs/2310.03714)
    calls prompt engineering “brittle and unscalable” and compares it to “hand-tuning
    the weights for a classifier”.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: DSPy是一个开源库，它试图将提示参数化，从而将其转化为一个优化问题。[原始论文](https://arxiv.org/abs/2310.03714)将提示工程称为“脆弱且不可扩展”，并将其与“手动调优分类器的权重”进行了比较。
- en: '[Haystack](https://github.com/deepset-ai/haystack) is an open-source library
    to build LLM applications, including RAG pipelines. It is platform-agnostic and
    offers a large number of integrations with different LLM providers, search databases
    and more. It also has its own [evaluation metrics](https://docs.haystack.deepset.ai/docs/evaluation).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[Haystack](https://github.com/deepset-ai/haystack)是一个开源库，用于构建LLM应用程序，包括RAG管道。它是平台无关的，并且与不同的LLM提供商、搜索数据库等有大量集成。它还拥有自己的[评估指标](https://docs.haystack.deepset.ai/docs/evaluation)。'
- en: In this article, we will briefly go over the internals of DSPy, and show how
    it can be used to teach an LLM to prefer more concise answers when answering questions
    over an academic medical dataset.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将简要介绍DSPy的内部工作原理，并展示如何使用它教会LLM在回答学术医学数据集中的问题时，更倾向于提供简洁的答案。
- en: Quick overview of DSPy
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DSPy 快速概述
- en: This [article](https://medium.com/towards-data-science/prompt-like-a-data-scientist-auto-prompt-optimization-and-testing-with-dspy-ff699f030cb7)
    from TDS provides a great in-depth exploration of DSPy. We will be summarising
    and using some of their examples.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇来自 TDS 的[文章](https://medium.com/towards-data-science/prompt-like-a-data-scientist-auto-prompt-optimization-and-testing-with-dspy-ff699f030cb7)对
    DSPy 进行了深入的探讨。我们将总结并使用其中的一些示例。
- en: 'In order to build a LLM application that can be optimised, DSPy offers two
    main abstractions: **signatures** and **modules**. A signature is a way to define
    the input and output of a system that interacts with LLMs. The signature is translated
    internally into a prompt by DSPy.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个可以优化的 LLM 应用程序，DSPy 提供了两个主要的抽象概念：**签名**和**模块**。签名是定义与 LLM 交互的系统输入和输出的一种方式。签名会在内部由
    DSPy 转化为提示。
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When using the DSPy `Predict` module (more on this later), this signature is
    turned into the following prompt:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DSPy 的 `Predict` 模块时（稍后会详细介绍），这个签名会转化为以下的提示：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, DSPy also has **modules** which define the “predictors” that have parameters
    that can be optimised, such as the selection of few-shot examples. The simplest
    module is `dspy.Predict` which does not modify the signature. Later in this article
    we will use the module `dspy.ChainOfThought` which asks the LLM to provide reasoning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，DSPy 还有 **模块**，这些模块定义了“预测器”，这些预测器拥有可以优化的参数，例如选择少量示例的方式。最简单的模块是 `dspy.Predict`，它不会修改签名。在本文后面，我们将使用
    `dspy.ChainOfThought` 模块，它会要求 LLM 提供推理过程。
- en: 'Things start to get interesting once we try to optimise a module (or as DSPy
    calls it “compiling” a module). When optimising a module, you typically need to
    specify 3 things:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们尝试优化一个模块（或者说，DSPy 称之为“编译”一个模块），事情就开始变得有趣了。优化模块时，通常需要指定三件事：
- en: the module to be optimised,
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要优化的模块，
- en: a training set, which might have labels,
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个训练集，可能包含标签，
- en: and some evaluation metrics.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及一些评估指标。
- en: When using the `dspy.Predict` or the `dspy.ChainOfThought` modules, DSPy searches
    through the training set and selects the best examples to add to the prompt as
    few-shot examples. In the case of RAG, it can also include the context that was
    used to get the final response. It calls these examples “demonstrations”.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `dspy.Predict` 或 `dspy.ChainOfThought` 模块时，DSPy 会在训练集上进行搜索，选择最好的示例，作为少量示例添加到提示中。在
    RAG 的情况下，它还可以包括用于生成最终回答的上下文。它将这些示例称为“示范”。
- en: 'You also need to specify the type of [optimiser](https://dspy-docs.vercel.app/docs/building-blocks/optimizers)
    you want to use to search through the parameter space. In this article, we use
    the `BootstrapFewShot` optimiser. How does this algorithm work internally? It
    is actually very simple and the paper provides some simplified pseudo-code:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要指定一个[优化器](https://dspy-docs.vercel.app/docs/building-blocks/optimizers)类型，用来在参数空间中进行搜索。在这篇文章中，我们使用的是
    `BootstrapFewShot` 优化器。这个算法是如何在内部工作的呢？其实它非常简单，文中提供了一些简化的伪代码：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The search algorithm goes through every training input in the `trainset` , gets
    a prediction and then checks whether it “passes” the metric by looking at `self.metric(example,
    prediction, predicted_traces)`. If the metric passes, then the examples are added
    to the `demonstrations` of the compiled program.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索算法会遍历 `trainset` 中的每一个训练输入，获取预测结果，然后通过查看 `self.metric(example, prediction,
    predicted_traces)` 来检查它是否“通过”评估指标。如果通过，那么这些示例会被添加到已编译程序的 `demonstrations` 中。
- en: Let’s create a custom Haystack pipeline
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们创建一个自定义的 Haystack 流水线
- en: The entire code can be found in this [cookbook with associated colab](https://github.com/deepset-ai/haystack-cookbook/blob/main/notebooks/prompt_optimization_with_dspy.ipynb),
    so we will only go through some of the most important steps here. For the example,
    we use a [dataset](https://huggingface.co/datasets/vblagoje/PubMedQA_instruction/viewer/default/train?row=0)
    derived from the [PubMedQA dataset](https://github.com/pubmedqa/pubmedqa) (both
    under the MIT license). It has questions based on abstracts of medical research
    papers and their associated answers. Some of the answers provided can be quite
    long, so we will be using DSPy to “teach” the LLM to prefer more concise answers,
    while keeping the accuracy of the final answer high.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在这个[食谱及关联的 colab](https://github.com/deepset-ai/haystack-cookbook/blob/main/notebooks/prompt_optimization_with_dspy.ipynb)中找到，因此我们这里只会讲解其中一些最重要的步骤。作为示例，我们使用了一个[数据集](https://huggingface.co/datasets/vblagoje/PubMedQA_instruction/viewer/default/train?row=0)，它来源于[PubMedQA
    数据集](https://github.com/pubmedqa/pubmedqa)（都在 MIT 许可下）。该数据集包含基于医学研究论文摘要的问题及其相关答案。某些提供的答案可能相当长，因此我们将使用
    DSPy 来“教导”LLM 优先生成更简洁的答案，同时保持最终答案的准确性。
- en: 'After adding the first 1000 examples to an in-memory document store (which
    can be replaced by any number of [retrievers](https://docs.haystack.deepset.ai/docs/retrievers)),
    we can now build our RAG pipeline:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在将前1000个示例添加到内存文档存储（可以被任何数量的[检索器](https://docs.haystack.deepset.ai/docs/retrievers)替换）后，我们现在可以构建我们的RAG管道：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let’s try it out!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试吧！
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The answer to the above question:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述问题的答案：
- en: Ketamine inhibits the proliferation of rat neural stem cells in a dose-dependent
    manner at concentrations of 200, 500, 800, and 1000µM. Additionally, ketamine
    decreases intracellular Ca(2+) concentration, suppresses protein kinase C-α (PKCα)
    activation, and phosphorylation of extracellular signal-regulated kinases 1/2
    (ERK1/2) in rat neural stem cells. These effects do not seem to be mediated through
    caspase-3-dependent apoptosis.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 凯他命在200、500、800和1000µM浓度下以剂量依赖的方式抑制大鼠神经干细胞的增殖。此外，凯他命还减少细胞内Ca(2+)浓度，抑制蛋白激酶C-α（PKCα）激活，并抑制大鼠神经干细胞中细胞外信号调节激酶1/2（ERK1/2）的磷酸化。这些效应似乎不是通过半胱天冬酶-3依赖性凋亡介导的。
- en: We can see how the answers tend to be very detailed and long.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，答案通常非常详细且较长。
- en: Use DSPy to get more concise answers
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DSPy获取更简洁的答案
- en: 'We start by creating a DSPy signature of the input and output fields:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从创建输入和输出字段的DSPy签名开始：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As we can see, we already specify in our description that we are expecting a
    short answer.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们在描述中已经指定我们期望一个简短的答案。
- en: 'Then, we create a DSPy module that will be later compiled:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个DSPy模块，稍后将进行编译：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We are using the Haystack retriever previously defined to search the documents
    in the document store `results = retriever.run(query=question)`. The prediction
    step is done with the DSPy module `dspy.ChainOfThought` which teaches the LM to
    think step-by-step before committing to the response.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用之前定义的Haystack检索器来搜索文档存储中的文档 `results = retriever.run(query=question)`。预测步骤使用DSPy模块
    `dspy.ChainOfThought`，该模块教会语言模型在做出回答之前逐步思考。
- en: 'During compilation, the prompt that will be optimised to look like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在编译过程中，优化后的提示将如下所示：
- en: '![](../Images/68ebd2d7ccc8136d323e686b67686a38.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68ebd2d7ccc8136d323e686b67686a38.png)'
- en: All the bold text is replaced by the examples selected by DSPy and the question-context
    for the specific query. Made by author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所有粗体文本都被DSPy选择的示例和特定查询的问答上下文替换。由作者制作。
- en: 'Finally, we have to define the metrics that we would like to optimise. The
    evaluator will have two parts:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要定义我们希望优化的度量标准。评估器将包括两个部分：
- en: '`[SASEvaluator](https://docs.haystack.deepset.ai/docs/sasevaluator)` : The
    semantic answer similarity metric is a score between 0 and 1 that computes the
    similarity between the given output and the actual output.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[SASEvaluator](https://docs.haystack.deepset.ai/docs/sasevaluator)` : 语义答案相似度度量是一个介于0和1之间的分数，用于计算给定输出与实际输出之间的相似度。'
- en: We will apply a penalty for answers that are longer than 20 words that will
    grow proportionally to the number of words up to a maximum of 0.5.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将对超过20个单词的答案进行惩罚，惩罚会根据单词数的增加成比例增长，最大为0.5。
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Our evaluation dataset is composed of 20 training examples and 50 examples in
    the devset.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的评估数据集由20个训练示例和50个开发集示例组成。
- en: If we evaluate the current naive RAG pipeline with the code below, we get an
    average score of 0.49.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用下面的代码评估当前的简单RAG管道，我们得到一个平均得分为0.49。
- en: 'Looking at some examples can give us some intuition on what the score is doing:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 查看一些示例可以帮助我们直观理解得分的作用：
- en: 'Question: Is increased time from neoadjuvant chemoradiation to surgery associated
    with higher pathologic complete response rates in esophageal cancer?'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题：新辅助化疗放疗到手术的时间增加是否与食管癌的病理完全缓解率较高相关？
- en: ''
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Predicted answer: Yes, increased time from neoadjuvant chemoradiation to surgery
    is associated with higher pathologic complete response rates in esophageal cancer.'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 预测答案：是的，新辅助化疗放疗到手术的时间增加与食管癌的病理完全缓解率较高相关。
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Score: 0.78**'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**得分：0.78**'
- en: But
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但是
- en: 'Question: Is epileptic focus localization based on resting state interictal
    MEG recordings feasible irrespective of the presence or absence of spikes?'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题：基于静息状态间歇期MEG记录的癫痫灶定位是否可以不考虑尖波的存在或缺失？
- en: ''
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Predicted answer: Yes.'
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 预测答案：是的。
- en: ''
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Score: 0.089**'
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**得分：0.089**'
- en: As we can see from the examples, if the answer is too short, it gets a low score
    because its similarity with the ground truth answer drops.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些示例中我们可以看到，如果答案太短，它会得到一个较低的得分，因为它与真实答案的相似度降低。
- en: 'We then compile the RAG pipeline with DSPy:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用DSPy编译RAG管道：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After we do this and re-evaluate the compiled pipeline, the score is now 0.69!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们完成这一过程并重新评估编译后的管道后，得分现在是0.69！
- en: Now it’s time to get the final optimised prompt and add it into our Haystack
    pipeline.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候获取最终优化的提示并将其添加到我们的Haystack管道中。
- en: Getting the final prompt-optimised pipeline
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取最终的提示优化管道
- en: 'We can see the few-shot examples selected by DSPy by looking at the `demos`
    field in the `compiled_rag` object:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看`compiled_rag`对象中的`demos`字段来查看DSPy选择的少量示例：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'There are 2 types of examples provided in the final prompt: few-shot examples
    and bootstrapped demos, like in the prompt shown above. The few-shot examples
    are question-answer pairs:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最终提示中提供了两种类型的示例：少量示例和引导演示，类似于上面所展示的提示。少量示例是问答对：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Whereas the bootstrapped demo has the full trace of the LLM, including the
    context and reasoning provided (in the `rationale` field below):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 而引导演示则包含LLM的完整追踪，包括提供的上下文和推理（在下面的`rationale`字段中）：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'All we need to do now is extract these examples found by DSPy and insert them
    in our Haystack pipeline:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们所需要做的就是提取这些由DSPy找到的示例，并将它们插入到我们的Haystack管道中：
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Our new pipeline becomes:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新管道变为：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let’s check the same question we tried before.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下之前尝试过的相同问题。
- en: 'Question:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：
- en: What effects does ketamine have on rat neural stem cells?
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 速氯胺对大鼠神经干细胞有哪些作用？
- en: 'Before:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 之前：
- en: Ketamine inhibits the proliferation of rat neural stem cells in a dose-dependent
    manner at concentrations of 200, 500, 800, and 1000µM. Additionally, ketamine
    decreases intracellular Ca(2+) concentration, suppresses protein kinase C-α (PKCα)
    activation, and phosphorylation of extracellular signal-regulated kinases 1/2
    (ERK1/2) in rat neural stem cells. These effects do not seem to be mediated through
    caspase-3-dependent apoptosis.
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 速氯胺（Ketamine）在200、500、800和1000µM的浓度下以剂量依赖性方式抑制大鼠神经干细胞的增殖。此外，速氯胺还降低了细胞内Ca(2+)浓度，抑制了蛋白激酶C-α（PKCα）激活以及大鼠神经干细胞中细胞外信号调节激酶1/2（ERK1/2）的磷酸化。这些效应似乎并非通过半胱天冬酶-3依赖的凋亡途径介导。
- en: 'After:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 之后：
- en: Ketamine at higher concentrations inhibits the proliferation of rat neural stem
    cells, while not affecting apoptosis. Additionally, it decreases intracellular
    calcium concentration and suppresses PKCα activation and ERK1/2 phosphorylation
    in these cells.
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 速氯胺在较高浓度下抑制大鼠神经干细胞的增殖，而不影响凋亡。此外，它降低了细胞内钙浓度，抑制了PKCα激活和这些细胞中ERK1/2的磷酸化。
- en: Job done!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 工作完成！
- en: A few words of conclusion
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单的结论
- en: In this post, we have used DSPy to optimise the prompt used in a Haystack RAG
    pipeline. We have done so by using a custom metric based on Haystack’s evaluation
    framework that penalised the LLM for long answers while keeping the similarity
    with the correct answer high. With this approach, we have managed to improve our
    performance by almost 40% without having to do any manual prompt engineering.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们使用DSPy优化了在Haystack RAG管道中使用的提示。我们通过使用基于Haystack评估框架的自定义度量，惩罚LLM的长答案，同时保持与正确答案的相似度较高。通过这种方法，我们成功地提高了大约40%的性能，而无需进行任何手动提示工程。
