["```py\nimport re\nimport json\nimport spacy\nimport torch\nimport openai\nimport vertexai\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\nfrom pinecone import Pinecone, ServerlessSpec\nfrom vertexai.language_models import TextEmbeddingModel\nfrom utils_google import authenticate\ncredentials, PROJECT_ID, service_account, pinecone_API_KEY = authenticate() \nfrom utils_openai import authenticate\nOPENAI_API_KEY = authenticate() \n\nopenai_client = openai.OpenAI(api_key=OPENAI_API_KEY)\n\nREGION = \"us-central1\"\nvertexai.init(project = PROJECT_ID,\n              location = REGION,\n              credentials = credentials)\n\npc = Pinecone(api_key=pinecone_API_KEY)\n\n# download spacy model\n#!python -m spacy download en_core_web_sm\n```", "```py\nrecipes = pd.read_json(\"recipes_v2.json\")\nrecipes.head()\n```", "```py\nplt.bar(recipes.recipe_type.unique(), recipes.recipe_type.value_counts(normalize=True).values)\nplt.show()\n```", "```py\nrecipes[\"dense_feature\"] = recipes.title + \"; \" + recipes.tags.apply(lambda x: str(x).strip(\"[]\").replace(\"'\", \"\")) + \"; \" + recipes.introduction\nrecipes[\"dense_feature\"].head()\n```", "```py\n# example output\n{'title': 'Creamy Mashed Potatoes',\n 'ingredients': 'The quantities here are for about four adult portions. If you are planning on eating this as a side dish, it might be more like 6-8 portions. * 1kg potatoes * 200ml milk* * 200ml mayonnaise* * ~100g cheese * Garlic powder * 12-16 strips of bacon * Butter * 3-4 green onions * Black pepper * Salt  *You can play with the proportions depending on how creamy or dry you want the mashed potatoes to be.',\n 'direction': '1\\. Peel and cut the potatoes into medium sized pieces. 2\\. Put the potatoes in a pot with some water so that it covers the potatoes and   boil them for about 20-30 minutes, or until the potatoes are soft. 3\\. About ten minutes before removing the potatoes from the boiling water, cut   the bacon into little pieces and fry it. 4\\. Warm up the milk and mayonnaise. 5\\. Shred the cheese. 6\\. When the potatoes are done, remove all water from the pot, add the warm milk   and mayonnaise mix, add some butter, and mash with a potato masher or a   blender. 7\\. Add some salt, black pepper and garlic powder to taste and continue mashing   the mix. 8\\. Once the mix is somewhat homogeneous and the potatoes are properly mashed,   add the shredded cheese and fried bacon and mix a little. 9\\. Serve and top with chopped green onions.'}\n```", "```py\nrecipes[\"ID\"] = range(len(recipes))\n```", "```py\nmodel_id = \"naver/splade-cocondenser-ensembledistil\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForMaskedLM.from_pretrained(model_id)\n\ndef to_sparse_vector(text, tokenizer, model):\n    tokens = tokenizer(text, return_tensors='pt')\n    output = model(**tokens)\n    vec = torch.max(\n        torch.log(1 + torch.relu(output.logits)) * tokens.attention_mask.unsqueeze(-1), dim=1\n    )[0].squeeze()\n\n    cols = vec.nonzero().squeeze().cpu().tolist()\n    weights = vec[cols].cpu().tolist()\n    sparse_dict = dict(zip(cols, weights))\n    return sparse_dict\n\nsparse_vectors = []\n\nfor i in tqdm(range(len(recipes))):\n    sparse_vectors.append(to_sparse_vector(recipes.iloc[i][\"ingredients\"], tokenizer, model))\n\nrecipes[\"sparse_vectors\"] = sparse_vectors\n```", "```py\n# running this code will create costs !!!\nmodel = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n\ndef to_dense_vector(text, model):\n    dense_vectors = model.get_embeddings([text])\n    return [dense_vector.values for dense_vector in dense_vectors][0]\n\ndense_vectors = []\n\nfor i in tqdm(range(len(recipes))):\n    dense_vectors.append(to_dense_vector(recipes.iloc[i][\"dense_feature\"], model))\n\nrecipes[\"dense_vectors\"] = dense_vectors\n```", "```py\n# running this code will create costs !!!\n\n# Create dense embeddings using OpenAIs text embedding model with 768 dimensions\nmodel = \"text-embedding-3-small\"\n\ndef to_dense_vector_openAI(text, client, model, dimensions):\n    dense_vectors = client.embeddings.create(model=model, dimensions=dimensions, input=[text])\n    return [dense_vector.values for dense_vector in dense_vectors][0]\n\ndense_vectors = []\n\nfor i in tqdm(range(len(recipes))):\n    dense_vectors.append(to_dense_vector_openAI(recipes.iloc[i][\"dense_feature\"], openai_client, model, 768))\n\nrecipes[\"dense_vectors\"] = dense_vectors\n```", "```py\n# load pandas DataFrame with pre-generated embeddings if you\n# didn't generate them in the last step\nrecipes = pd.read_pickle(\"recipes_with_vectors.pkl\")\n\n# if you need to delte an existing index\npc.delete_index(\"index-name\")\n\n# create a new index \npc.create_index(\n    name=\"recipe-project\",\n    dimension=768, # adjust if needed\n    metric=\"dotproduct\",\n    spec=ServerlessSpec(\n        cloud=\"aws\",\n        region=\"us-west-2\"\n    )\n)\n\npc.describe_index(\"recipe-project\")\n```", "```py\n# upsert to pinecone in batches\ndef sparse_to_dict(data):\n    dict_ = {\"indices\": list(data.keys()),\n             \"values\": list(data.values())}\n    return dict_\n\nbatch_size = 100\nindex = pc.Index(\"recipe-project\")\n\nfor i in tqdm(range(0, len(recipes), batch_size)):\n    i_end = min(i + batch_size, len(recipes))\n    meta_batch = recipes.iloc[i: i_end][[\"ID\", \"recipe_type\"]]\n    meta_dict = meta_batch.to_dict(orient=\"records\")\n\n    sparse_batch = recipes.iloc[i: i_end][\"sparse_vectors\"].apply(lambda x: sparse_to_dict(x))\n    dense_batch = recipes.iloc[i: i_end][\"dense_vectors\"]\n\n    upserts = []\n\n    ids = [str(x) for x in range(i, i_end)]\n    for id_, meta, sparse_, dense_ in zip(ids, meta_dict, sparse_batch, dense_batch):\n        upserts.append({\n            \"id\": id_,\n            \"sparse_values\": sparse_,\n            \"values\": dense_,\n            \"metadata\": meta\n        })\n\n    index.upsert(upserts)\n\nindex.describe_index_stats()\n```", "```py\nindex.fetch(ids=[\"50\"])\n```", "```py\nuser_query = \"I want to cook some Italian dish with rice\"\nrecipe_type = \"vegetarian\"\n```", "```py\n# running this code will create costs !!!\n\n# If you used VertexAI and gecko003 to create dense embeddings\nmodel = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n\ndef to_dense_vector(text, model):\n    dense_vectors = model.get_embeddings([text])\n    return [dense_vector.values for dense_vector in dense_vectors][0]\n\ntext_dense_vector = to_dense_vector(user_query, model)\n```", "```py\n# running this code will create costs !!!\n\n# If you used OpenAI to create dense embeddings\nmodel = \"text-embedding-3-small\"\n\ndef to_dense_vector_openAI(text, client, model, dimensions):\n    dense_vectors = client.embeddings.create(model=model, dimensions=dimensions, input=[text])\n    return [dense_vector.values for dense_vector in dense_vectors][0]\n\ntext_dense_vector = to_dense_vector_openAI(user_query, openai_client, model, 768)\n```", "```py\nindex = pc.Index(\"recipe-project\")\n\nretrieved_items = index.query(vector=text_dense_vector,\n                              include_values=False,\n                              include_metadata=True,\n                              top_k=3,\n                              filter={\"recipe_type\": {\"$eq\": recipe_type}})\n\nretrieved_ids = [item.get(\"metadata\").get(\"ID\") for item in retrieved_items.get(\"matches\")]\n\nretrieved_items\n```", "```py\nrecipes[recipes.ID.isin(retrieved_ids)].output.values\n```", "```py\nrecipes[recipes.ID.isin(retrieved_ids)].output.values[0]\n```", "```py\n{'title': 'Pasta Arrabbiata',\n 'ingredients': '- Pasta - Olive oil - Chilli flakes or diced chilli peppers - Crushed garlic cloves - Crushed tomatoes (about 800 gramms for 500 gramms of pasta) - Chopped parsley - Grated Pecorino Romano or Parmigiano Reggiano (optional, but highly recommended)',\n 'direction': '1\\. Start heating up water for the pasta. 2\\. Heat up a few tablespoons of olive oil over low heat. 3\\. Crush several cloves of garlic into the olive oil, add the chilli flakes or chilli peppers and fry them for a short time, while being careful not to burn the garlic. 4\\. Add your crushed tomatoes, together with some salt and pepper, increase the heat to medium and let simmer for 10-15 minutes or until it looks nicely thickened. 5\\. When the water starts boiling, put a handful of salt into it and then your pasta of choice. Ideally leave the pasta slightly undercooked, because it will go in the hot sauce and finish cooking there. 6\\. When the sauce is almost ready, add most of your chopped parsley and stir it around. Save some to top the dish later. 8\\. When the pasta is ready (ideally at the same time as the sauce or slightly later), strain it and add it to the sauce, which should be off the heat. If the sauce looks a bit too thick, add some of the pasta water. Mix well. 9\\. Add some of the grated cheese of your choice and stir it in. 10\\. Serve with some more grated cheese and chopped parsley on top.'}\n```", "```py\nmodel_id = \"naver/splade-cocondenser-ensembledistil\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForMaskedLM.from_pretrained(model_id)\n\ndef to_sparse_vector(text, tokenizer, model):\n    tokens = tokenizer(text, return_tensors='pt')\n    output = model(**tokens)\n    vec = torch.max(\n        torch.log(1 + torch.relu(output.logits)) * tokens.attention_mask.unsqueeze(-1), dim=1\n    )[0].squeeze()\n\n    cols = vec.nonzero().squeeze().cpu().tolist()\n    weights = vec[cols].cpu().tolist()\n    sparse_dict = dict(zip(cols, weights))\n    return sparse_dict\n\ntext_sparse_vector = to_sparse_vector(user_query, tokenizer, model)\n```", "```py\n# running this code will create costs !!!\n\n# If you used VertexAI and gecko003 to create dense embeddings\nmodel = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n\ntext_dense_vector = to_dense_vector(user_query, model)\n```", "```py\ndef hybride_search(sparse_dict, dense_vectors, alpha):\n\n    # check alpha value is in range\n    if alpha < 0 or alpha > 1:\n        raise ValueError(\"Alpha must be between 0 and 1\")\n    # scale sparse and dense vectors to create hybrid search vecs\n    hsparse = {\n        \"indices\": list(sparse_dict.keys()),\n        \"values\": [v * (1 - alpha) for v in list(sparse_dict.values())]\n    }\n    hdense = [v * alpha for v in dense_vectors]\n    return hdense, hsparse\n\nuser_query = \"What can I cook with potatos, mushrooms, and beef?\"\nrecipe_type = [\"regular\", \"vegetarian\", \"vegan\"] # allows for all recipe types\n\ndense_vector, sparse_dict = hybride_search(text_sparse_vector, text_dense_vector, 1.0)\n\nretrieved_items = index.query(vector=dense_vector,\n                              sparse_vector=sparse_dict,\n                              include_values=False,\n                              include_metadata=True,\n                              top_k=1,\n                              filter={\"recipe_type\": {\"$in\": recipe_type}})\n\nretrieved_ids = [item.get(\"metadata\").get(\"ID\") for item in retrieved_items.get(\"matches\")]\n\n[x.get(\"ingredients\") for x in recipes[recipes.ID.isin(retrieved_ids)].output.values]\n```", "```py\n# retrived output with alpha=1.0\n['- 1 beef kidney - 60g butter - 2 onions - 2 shallots - 1 sprig of fresh parsley - 3 bay leaves - 400g croutons or toasted bread in pieces']\n```", "```py\ndense_vector, sparse_dict = hybride_search(text_sparse_vector, text_dense_vector, 0.5)\n\nretrieved_items = index.query(vector=dense_vector,\n                              sparse_vector=sparse_dict,\n                              include_values=False,\n                              include_metadata=True,\n                              top_k=1,\n                              filter={\"recipe_type\": {\"$in\": recipe_type}})\n\nretrieved_ids = [item.get(\"metadata\").get(\"ID\") for item in retrieved_items.get(\"matches\")]\n\n[x.get(\"ingredients\") for x in recipes[recipes.ID.isin(retrieved_ids)].output.values]\n```", "```py\n# retrived output with alpha=0.5\n['* 500g beef * 300-400g potatoes * 1 carrot * 1 medium onion * 12 tablespoons tomato paste * 500ml water * 3-4 garlic cloves * 3-4 bay leaves * Curcuma * Paprika * Oregano * Parsley * Caraway * Basil (optional) * Cilantro (optional) * 2-3 champignon mushrooms (optional)']Using a serverless index has the advantage that you do not need to pay for a server instance that runs 24/7\\. Instead, you are billed by queries or read and write units, as they are called by Pinecone. Sparse and dense vector searches work well with a serverless index. However, please keep in mind the following limitation.\n```"]