- en: Understanding Direct Preference Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-the-implications-of-direct-preference-optimization-a4bbd2d85841?source=collection_archive---------0-----------------------#2024-02-18](https://towardsdatascience.com/understanding-the-implications-of-direct-preference-optimization-a4bbd2d85841?source=collection_archive---------0-----------------------#2024-02-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A look at the “Direct Preference Optimization: Your Language Model is Secretly
    a Reward Model” paper and its findings'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--a4bbd2d85841--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--a4bbd2d85841--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a4bbd2d85841--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a4bbd2d85841--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--a4bbd2d85841--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a4bbd2d85841--------------------------------)
    ·8 min read·Feb 18, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b763aeff4b52b196cfe8e0e6785650d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author via DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: '*This blog post was inspired by a discussion I recently had with some friends
    about the Direct Preference Optimization (DPO) paper. The discussion was lively
    and went over many important topics in LLMs and Machine Learning in general. Below
    is an expansion on some of those ideas and the concepts discussed in the paper.*'
  prefs: []
  type: TYPE_NORMAL
- en: Direct Preference Optimization (DPO) has become the way that new foundation
    models are fine-tuned. Famously Mixtral 8x7B, the Sparse Mixture of Experts model
    created by Mistral, was able to reach LLaMa 70B levels of performance with significantly
    fewer parameters by using DPO. Naturally, this success has led many in the community
    to begin fine-tuning their own models with DPO.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into what exactly DPO is and how we got here.
  prefs: []
  type: TYPE_NORMAL
- en: High Level Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s begin with setting out what fine-tuning should do from a high level. Once
    you have a pre-trained a model to have strong generative capacities, you typically
    want to control its output somehow. Whether that be optimizing it to respond in
    dialogue as a chat-bot or to respond in code rather…
  prefs: []
  type: TYPE_NORMAL
