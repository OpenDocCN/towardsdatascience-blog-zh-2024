- en: Streamlining Object Detection with Metaflow, AWS, and Weights & Biases
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Metaflow、AWS和Weights & Biases优化物体检测
- en: 原文：[https://towardsdatascience.com/streamlining-object-detection-with-metaflow-aws-and-weights-biases-b44a14cb2e11?source=collection_archive---------1-----------------------#2024-07-19](https://towardsdatascience.com/streamlining-object-detection-with-metaflow-aws-and-weights-biases-b44a14cb2e11?source=collection_archive---------1-----------------------#2024-07-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/streamlining-object-detection-with-metaflow-aws-and-weights-biases-b44a14cb2e11?source=collection_archive---------1-----------------------#2024-07-19](https://towardsdatascience.com/streamlining-object-detection-with-metaflow-aws-and-weights-biases-b44a14cb2e11?source=collection_archive---------1-----------------------#2024-07-19)
- en: How to create a production-grade pipeline for object detection
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何为物体检测创建生产级管道
- en: '[](https://medium.com/@ed.izaguirre?source=post_page---byline--b44a14cb2e11--------------------------------)[![Ed
    Izaguirre](../Images/c9eded1f06c47571baa662107428483f.png)](https://medium.com/@ed.izaguirre?source=post_page---byline--b44a14cb2e11--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b44a14cb2e11--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b44a14cb2e11--------------------------------)
    [Ed Izaguirre](https://medium.com/@ed.izaguirre?source=post_page---byline--b44a14cb2e11--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ed.izaguirre?source=post_page---byline--b44a14cb2e11--------------------------------)[![Ed
    Izaguirre](../Images/c9eded1f06c47571baa662107428483f.png)](https://medium.com/@ed.izaguirre?source=post_page---byline--b44a14cb2e11--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b44a14cb2e11--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b44a14cb2e11--------------------------------)
    [Ed Izaguirre](https://medium.com/@ed.izaguirre?source=post_page---byline--b44a14cb2e11--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b44a14cb2e11--------------------------------)
    ·14 min read·Jul 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b44a14cb2e11--------------------------------)
    ·14分钟阅读·2024年7月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c157f2b7af842d1ff59de906324c2968.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c157f2b7af842d1ff59de906324c2968.png)'
- en: Overview of the project flow. Image by author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 项目流程概述。图片来自作者。
- en: '**Table of Contents**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**目录**'
- en: '[Introduction (Or What’s in a Title)](#ed7d)'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[介绍（或标题中的内容）](#ed7d)'
- en: '[The Reality of MLOps without the Ops](#658e)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[没有Ops的MLOps现实](#658e)'
- en: '[Managing Dependencies Effectively](#5cc4)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[有效管理依赖关系](#5cc4)'
- en: '[How to Debug a Production Flow](#04ab)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[如何调试生产流程](#04ab)'
- en: '[Finding the Goldilocks Step Size](#de07)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[找到合适的步长](#de07)'
- en: '[Takeaways](#1c44)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[要点总结](#1c44)'
- en: '[References](#d14a)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[参考文献](#d14a)'
- en: '**Relevant Links**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关链接**'
- en: '[GitHub repo](https://github.com/EdIzaguirre/plant-object-detection)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub 仓库](https://github.com/EdIzaguirre/plant-object-detection)'
- en: '[Link to earlier article discussing a dev version of this project](https://medium.com/towards-data-science/object-detection-using-retinanet-and-kerascv-b07940327b6c)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[链接到讨论该项目开发版本的早期文章](https://medium.com/towards-data-science/object-detection-using-retinanet-and-kerascv-b07940327b6c)'
- en: Introduction (Or What’s In a Title)
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍（或标题中的内容）
- en: 'Navigating the world of data science job titles can be overwhelming. Here are
    just some of the examples I’ve seen recently on LinkedIn:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学职位名称的世界中导航可能令人不知所措。以下是我最近在LinkedIn上看到的一些例子：
- en: Data scientist
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家
- en: Machine learning engineer
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习工程师
- en: ML Ops engineer
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLOps工程师
- en: Data scientist/machine learning engineer
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家/机器学习工程师
- en: Machine learning performance engineer
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习性能工程师
- en: …
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: …
- en: 'and the list goes on and on. Let’s focus on two key roles: **data scientist**
    and **machine learning engineer.** According to Chip Huyen in her book, *Introduction
    to Machine Learning Interviews* [1]:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 话题还可以继续深入。让我们关注两个关键角色：**数据科学家**和**机器学习工程师**。根据Chip Huyen在她的书《*Introduction to
    Machine Learning Interviews*》中的描述[1]：
- en: The goal of data science is to **generate business insights**, whereas the goal
    of ML engineering is to **turn data into products**. This means that data scientists
    tend to be better statisticians, and ML engineers tend to be better engineers.
    ML engineers definitely need to know ML algorithms, whereas many data scientists
    can do their jobs without ever touching ML.
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据科学的目标是**生成商业洞察**，而机器学习工程的目标是**将数据转化为产品**。这意味着数据科学家往往更擅长统计学，而机器学习工程师则通常是更优秀的工程师。机器学习工程师肯定需要了解机器学习算法，而许多数据科学家则可以在不涉及机器学习的情况下完成他们的工作。
- en: Got it. So data scientists must know statistics, while ML engineers must know
    ML algorithms. But if the goal of data science is to generate business insights,
    and in 2024 the most powerful algorithms that generate the best insights tend
    to come from machine learning (deep learning in particular), then the line between
    the two becomes blurred. Perhaps this explains the combined *Data scientist/machine
    learning engineer* title we saw earlier?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 明白了。所以数据科学家必须懂得统计学，而机器学习工程师则必须了解机器学习算法。但如果数据科学的目标是产生商业洞察，并且到2024年，最强大的算法，特别是深度学习，往往能够产生最佳洞察，那么两者之间的界限就变得模糊了。或许这也能解释我们之前看到的*数据科学家/机器学习工程师*这一职位的结合？
- en: 'Huyen goes on to say:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Huyen 接着说：
- en: As a company’s adoption of ML matures, it might want to have a specialized ML
    engineering team. However, with an increasing number of prebuilt and pretrained
    models that can work off-the-shelf, it’s possible that developing ML models will
    require less ML knowledge, and ML engineering and data science will be even more
    unified.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 随着公司对机器学习的采用逐步成熟，可能希望拥有一个专门的机器学习工程团队。然而，随着越来越多的预构建和预训练模型可以即插即用，开发机器学习模型可能不再需要那么多的机器学习知识，机器学习工程与数据科学将更加统一。
- en: This was written in 2020\. By 2024, the line between ML engineering and data
    science has indeed blurred. So, if the ability to implement ML models is not the
    dividing line, then what is?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是2020年写的。到2024年，机器学习工程和数据科学之间的界限确实变得模糊了。那么，如果实现机器学习模型的能力不是分界线，那么究竟是什么呢？
- en: 'The line varies by practitioner of course. Today, the stereotypical data scientist
    and ML engineer differ as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这一界限因实践者而异。如今，典型的数据科学家和机器学习工程师的区别如下：
- en: '**Data scientist:** Works in Jupyter notebooks, has never heard of Airflow,
    Kaggle expert, pipeline consists of manual execution of code cells in just the
    right order, master at hyperparameter tuning, Dockers? Great shoes for the summer!
    Development-focused.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据科学家：** 使用 Jupyter notebook，从未听说过 Airflow，Kaggle 专家，管道由手动按正确顺序执行代码单元组成，擅长超参数调优，Docker？夏天穿的好鞋！专注于开发。'
- en: '**Machine learning engineer:** Writes Python scripts, has heard of Airflow
    but doesn’t like it (go Prefect!), Kaggle middleweight, automated pipelines, leaves
    tuning to the data scientist, Docker aficionado. Production-focused.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习工程师：** 编写 Python 脚本，听说过 Airflow，但不喜欢它（支持 Prefect！），Kaggle 中级选手，自动化管道，模型调优交给数据科学家，Docker
    爱好者。专注于生产环境。'
- en: 'In large companies, data scientists develop machine learning models to solve
    business problems and then hand them off to ML engineers. The engineers productionize
    and deploy these models, ensuring scalability and robustness. In a nutshell: **the
    fundamental difference today between a data scientist and a machine learning engineer
    is not about who uses machine learning, but whether you are focused on development
    or production**.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在大公司中，数据科学家开发机器学习模型来解决业务问题，然后交给机器学习工程师。工程师将这些模型投入生产并进行部署，确保其可扩展性和鲁棒性。简而言之：**今天数据科学家和机器学习工程师之间的根本区别，不在于谁在使用机器学习，而在于你是否专注于开发还是生产环境**。
- en: 'But what if you don’t have a large company, and instead are a startup or a
    company at small scale with only the budget to higher one or a few people for
    the data science team? They would love to hire the *Data scientist/machine learning
    engineer* who is able to do both! With an eye toward becoming this mythical “[full-stack
    data scientist](https://podcasts.apple.com/us/podcast/dataframed/id1336150688?i=1000661883935)”,
    I decided to take an earlier project of mine, *Object Detection using RetinaNet
    and KerasCV,* and productionize it (see link above for related article and code).
    The original project, done using a Jupyter notebook, had a few deficiencies:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你没有一家大公司，而是处于初创公司或小型公司的情况下，预算只能雇佣一位或几位数据科学团队成员呢？他们可能希望招聘能够兼做*数据科学家/机器学习工程师*的人员！为了成为这个神话般的“[全栈数据科学家](https://podcasts.apple.com/us/podcast/dataframed/id1336150688?i=1000661883935)”，我决定将我之前的一个项目，*使用
    RetinaNet 和 KerasCV 进行物体检测*，进行生产化（请参阅上述链接获取相关文章和代码）。原始项目是使用 Jupyter notebook 完成的，但存在一些不足之处：
- en: There was no model versioning, data versioning or even code versioning. If a
    particular run of my Jupyter notebook worked, and a subsequent one did not, there
    was no methodical way of going back to the working script/model (Ctrl + Z? The
    save notebook option in Kaggle?)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以前没有模型版本控制、数据版本控制甚至代码版本控制。如果我的 Jupyter notebook 在某次运行时有效，而在随后的运行中无效，那时没有任何系统化的方法可以回到有效的脚本/模型（Ctrl
    + Z？Kaggle 中的保存 notebook 选项？）
- en: Model evaluation was fairly simple, using Matplotlib and some KerasCV plots.
    There was no storing of evaluations.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估相当简单，使用了 Matplotlib 和一些 KerasCV 图表。没有存储评估结果。
- en: We were compute limited to the free 20 hours of Kaggle GPU. It was not possible
    to use a larger compute instance, or to train multiple models in parallel.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的计算资源受限于 Kaggle 免费的 20 小时 GPU。无法使用更大的计算实例，也不能并行训练多个模型。
- en: The model was never deployed to any endpoint, so it could not yield any predictions
    outside of the Jupyter notebook (no business value).
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型从未部署到任何端点，因此无法在 Jupyter notebook 以外的地方进行预测（没有业务价值）。
- en: 'To accomplish this task, I decided to try out [Metaflow](https://docs.metaflow.org/introduction/why-metaflow).
    Metaflow is an open-source ML platform designed to help data scientists train
    and deploy ML models. Metaflow primarily serves two functions:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个任务，我决定尝试使用 [Metaflow](https://docs.metaflow.org/introduction/why-metaflow)。Metaflow
    是一个开源的机器学习平台，旨在帮助数据科学家训练和部署机器学习模型。Metaflow 主要有两个功能：
- en: a **workflow orchestrator.** Metaflow breaks down a workflow into steps. Turning
    a Python function into a Metaflow step is as simple as adding a `@step` decorator
    above the function. Metaflow doesn’t necessarily have all of the bells and whistles
    that a workflow tool like [Airflow](https://github.com/apache/airflow) can give
    you, but it is simple, Pythonic, and can be setup to use AWS Step Functions as
    an external orchestrator. In addition, there is nothing wrong with using proper
    orchestrators like [Airflow or Prefect in conjunction with Metaflow](https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**工作流编排工具**。Metaflow 将一个工作流分解为多个步骤。将一个 Python 函数转化为 Metaflow 步骤非常简单，只需在函数上方添加
    `@step` 装饰器即可。Metaflow 并不一定具备像 [Airflow](https://github.com/apache/airflow) 这样的工作流工具所提供的所有功能，但它简单、符合
    Python 风格，并且可以设置使用 AWS Step Functions 作为外部编排器。此外，使用像 [Airflow 或 Prefect 与 Metaflow
    配合使用](https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat)也是完全没问题的。
- en: an **infrastructure abstraction tool.** This is where Metaflow really shines.
    Normally a data scientist would have to manually set up the infrastructure required
    to send model training jobs from their laptop to AWS. This would potentially require
    knowledge of infrastructure such as API gateways, virtual private clouds (VPCs),
    Docker/Kubernetes, subnet masks, and much more. This sounds more like the work
    of the machine learning engineer! However, by using a Cloud Formation template
    (infrastructure-as-code file) and the `@batch` Metaflow decorator, the data scientist
    is able to ship compute jobs to the cloud in a simple and reliable way.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**基础设施抽象工具**。这正是 Metaflow 的真正优势所在。通常，数据科学家需要手动设置基础设施，将模型训练任务从他们的笔记本电脑发送到 AWS。这可能需要了解基础设施方面的知识，如
    API 网关、虚拟私有云（VPC）、Docker/Kubernetes、子网掩码等。听起来这更像是机器学习工程师的工作！然而，通过使用 Cloud Formation
    模板（基础设施即代码文件）和 `@batch` Metaflow 装饰器，数据科学家能够以简单可靠的方式将计算任务发送到云端。
- en: 'This article details my journey in productionizing an object detection model
    using Metaflow, AWS, and Weights & Biases. We’ll explore four key lessons learned
    during this process:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本文详细介绍了我使用 Metaflow、AWS 和 Weights & Biases 生产化物体检测模型的历程。我们将在这个过程中探讨四个关键的学习经验：
- en: The reality of “MLOps without the Ops”
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “没有 Ops 的 MLOps”现实
- en: Effective dependency management
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有效的依赖管理
- en: Debugging strategies for production flows
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生产环境工作流的调试策略
- en: Optimizing workflow structure
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化工作流结构
- en: By sharing these insights I hope to guide you, my fellow data practitioner,
    in your transition from development to production-focused work, highlighting both
    the challenges and solutions encountered along the way.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分享这些见解，我希望能指导你们这些数据从业者，从开发转向生产相关的工作，突出在这一过程中遇到的挑战和解决方案。
- en: 'Before we dive into the specifics, let’s take a look at the high-level structure
    of our Metaflow pipeline. This will give you a bird’s-eye view of the workflow
    we’ll be discussing throughout the article:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入具体内容之前，让我们先来看一下我们 Metaflow 管道的高层结构。这将为你提供一个鸟瞰视图，帮助你了解本文中讨论的工作流：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This structure forms the backbone of our production-grade object detection pipeline.
    Metaflow is Pythonic, using decorators to denote functions as steps in a pipeline,
    handle dependency management, and move compute to the cloud. Steps are run sequentially
    via the `self.next()` command. For more on Metaflow, see [the documentation.](https://docs.metaflow.org/introduction/why-metaflow)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结构构成了我们生产级目标检测流水线的骨架。Metaflow 是 Python 风格的，使用装饰器将函数标记为流水线中的步骤，处理依赖关系管理，并将计算任务移到云端。步骤通过
    `self.next()` 命令按顺序执行。更多关于 Metaflow 的内容，请参见 [文档](https://docs.metaflow.org/introduction/why-metaflow)。
- en: The Reality of MLOps without Ops
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 没有运维的 MLOps 现实
- en: 'One of the promises of Metaflow is that a data scientist should be able to
    focus on the things they care about; typically model development and feature engineering
    (think Kaggle), while abstracting away the things that they don’t care about (where
    compute is run, where data is stored, etc.) There is a phrase for this idea: “*MLOps
    without the Ops*”. I took this to mean that I would be able to abstract away the
    work of an MLOps Engineer, without actually learning or doing much of the ops
    myself. I thought I could get away without learning about Docker, CloudFormation
    templating, [EC2 instance types](https://aws.amazon.com/ec2/instance-types/),
    AWS Service Quotas, [Sagemaker endpoints](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html),
    and AWS Batch configurations.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow 的一个承诺是数据科学家应该能够专注于他们关心的事情；通常是模型开发和特征工程（想想 Kaggle），同时将他们不关心的事情（计算任务在哪儿运行，数据存储在哪儿，等等）抽象化。对此有一句话：“*没有运维的
    MLOps*”。我以为这意味着我能够抽象化 MLOps 工程师的工作，而无需自己学习或做太多运维工作。我以为我可以不用了解 Docker、CloudFormation
    模板、[EC2 实例类型](https://aws.amazon.com/ec2/instance-types/)、AWS 服务配额、[Sagemaker
    端点](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html)以及
    AWS 批量配置。
- en: Unfortunately, this was naive. I realized that the [CloudFormation template](https://github.com/Netflix/metaflow-tools/blob/master/aws/cloudformation/metaflow-cfn-template.yml)
    linked on so many Metaflow tutorials provided no way of provisioning GPUs from
    AWS(!). This is a fundamental part of doing data science in the cloud, so the
    lack of documentation was surprising. ([I am not the first to wonder about the
    lack of documentation on this.](https://github.com/Netflix/metaflow/issues/250))
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这是天真了。我意识到许多 Metaflow 教程中链接的 [CloudFormation 模板](https://github.com/Netflix/metaflow-tools/blob/master/aws/cloudformation/metaflow-cfn-template.yml)并没有提供从
    AWS 配置 GPU 的方法（！）。这是在云端做数据科学的一个基本部分，因此缺乏文档令我感到惊讶。（[我不是第一个对缺乏文档感到疑惑的人](https://github.com/Netflix/metaflow/issues/250)）
- en: 'Below is a code snippet demonstrating what sending a job to the cloud looks
    like in Metaflow:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个代码片段，演示了在 Metaflow 中将作业发送到云端的样子：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note the importance of specifying what libraries are required and the necessary
    environment variables. Because the compute job is run on the cloud, it will not
    have access to the virtual environment on your local computer or to the environment
    variables in your `.env` file. Using Metaflow decorators to solve this issue is
    elegant and simple.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意指定所需库和必要环境变量的重要性。因为计算任务是在云端运行的，它将无法访问你本地计算机上的虚拟环境或 `.env` 文件中的环境变量。使用 Metaflow
    装饰器来解决这个问题既优雅又简单。
- en: It is true that you do not have to be an AWS expert to be able to run compute
    jobs on the cloud, but don’t expect to just install Metaflow, use the stock CloudFormation
    template, and have success. *MLOps without the Ops* **is** too good to be true;
    perhaps the phrase should be *MLOps without the Ops; after learning some Ops*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，你不必成为 AWS 专家才能在云端运行计算任务，但不要指望仅仅安装 Metaflow，使用默认的 CloudFormation 模板就能成功。*没有运维的
    MLOps* **实在**太美好，难以置信；也许这个短语应该是 *没有运维的 MLOps；在学习了一些运维之后*。
- en: Managing Dependencies Effectively
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有效管理依赖关系
- en: One of the most important considerations when trying to turn a dev project into
    a production project is how to manage dependencies. Dependencies refer to Python
    packages, such as TensorFlow, PyTorch, Keras, Matplotlib, etc.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个开发项目转变为生产项目时，最重要的考虑因素之一是如何管理依赖关系。依赖关系指的是 Python 包，例如 TensorFlow、PyTorch、Keras、Matplotlib
    等。
- en: Dependency management is comparable to managing ingredients in a recipe to ensure
    consistency. A recipe might say “*Add a tablespoon of salt.*” This is somewhat
    reproducible, but the knowledgable reader may ask “[*Diamond Crystal or Morton*](https://www.simplyrecipes.com/how_to_swap_morton_kosher_salt_for_diamond_crystal_and_vice_versa/)?”
    Specifying the exact brand of salt used maximizes reproducibility of the recipe.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖管理类似于管理食谱中的食材，以确保一致性。一个食谱可能会说“*加入一汤匙盐*”。这在某种程度上是可重复的，但有经验的读者可能会问“[*Diamond
    Crystal 还是 Morton*](https://www.simplyrecipes.com/how_to_swap_morton_kosher_salt_for_diamond_crystal_and_vice_versa/)？”指定使用的盐的确切品牌可以最大程度地提高食谱的可重复性。
- en: 'In a similar way, there are levels to dependency management in machine learning:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在机器学习中，依赖管理有不同的层次：
- en: 'Use a `requirements.txt` file. This simple option lists all Python packages
    with pinned versions. This works fairly well, but has limitations: although you
    may pin these high level dependencies, you may not pin any transitive dependencies
    (dependencies of dependencies). This makes it very difficult to create reproducible
    environments and slows down runtime as packages are downloaded and installed.
    For example:'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`requirements.txt`文件。这种简单的方式列出了所有带有固定版本的Python包。它工作得相当不错，但也有局限性：虽然你可以固定这些高层依赖，但无法固定任何传递依赖（依赖的依赖）。这使得创建可重复的环境变得非常困难，并且因为包被下载和安装，运行时也会变慢。例如：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This works fairly well, but has limitations: although you may pin these high
    level dependencies, you may not pin any transitive dependencies (dependencies
    of dependencies). This makes it very difficult to create reproducible environments
    and slows down runtime as packages are downloaded and installed.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这工作得相当不错，但也有局限性：虽然你可以固定这些高层依赖，但无法固定任何传递依赖（依赖的依赖）。这使得创建可重复的环境变得非常困难，并且因为包被下载和安装，运行时也会变慢。
- en: Use a Docker container. This is the gold standard. This encapsulates the entire
    environment, including the operating system, libraries, dependencies, and configuration
    files, making it very consistent and reproducible. Unfortunately, working with
    Docker containers can be somewhat heavy and difficult, especially if the data
    scientist doesn’t have prior experience with the platform.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Docker容器。这是黄金标准。它封装了整个环境，包括操作系统、库、依赖项和配置文件，使其非常一致且可重复。不幸的是，使用Docker容器可能会比较复杂，尤其是当数据科学家没有平台使用经验时。
- en: '[Metaflow](https://docs.metaflow.org/scaling/dependencies/libraries) `[@pypi/@conda](https://docs.metaflow.org/scaling/dependencies/libraries)`
    [decorators](https://docs.metaflow.org/scaling/dependencies/libraries) cut a middle
    road between these two options, being both lightweight and simple for the data
    scientist to use, while being more robust and reproducible than a `requirements.txt`
    file. These decorators essentially do the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[Metaflow](https://docs.metaflow.org/scaling/dependencies/libraries) `[@pypi/@conda](https://docs.metaflow.org/scaling/dependencies/libraries)`
    [装饰器](https://docs.metaflow.org/scaling/dependencies/libraries)在这两种选项之间找到了一个折中方案，既轻量且简单，便于数据科学家使用，同时比`requirements.txt`文件更具鲁棒性和可重复性。这些装饰器基本上执行以下操作：'
- en: Create isolated virtual environments for every step of your flow.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为流程中的每一步创建独立的虚拟环境。
- en: Pin the Python interpreter versions, which a simple `requirements.txt` file
    won’t do.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 锁定Python解释器版本，而简单的`requirements.txt`文件做不到这一点。
- en: Resolves the full dependency graph for every step and locks it for stability
    and reproducibility. This locked graph is stored as metadata, allowing for easy
    auditing and consistent environment recreation.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每一步解析完整的依赖图，并将其锁定以确保稳定性和可重复性。这个锁定的图被存储为元数据，便于审计和一致的环境重建。
- en: Ships the locally resolved environment for remote execution, even if the remote
    environment has a different OS and CPU architecture than the client.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将本地解析的环境传输到远程执行，即使远程环境的操作系统和CPU架构与客户端不同。
- en: This is much better then simply using a `requirements.txt` file, while requiring
    no additional learning on the part of the data scientist.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这比仅仅使用`requirements.txt`文件要好得多，而且不需要数据科学家额外学习任何内容。
- en: 'Let’s go revisit the train step to see an example:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下训练步骤，看看一个示例：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: All we have to do is specify the library and version, and Metaflow will handle
    the rest.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所要做的就是指定库和版本，Metaflow会处理剩下的部分。
- en: Unfortunately, there is a catch. My personal laptop is a Mac. However, the compute
    instances in AWS Batch have a Linux architecture. This means that we must create
    the isolated virtual environments for Linux machines, not Macs. This requires
    what is known as **cross-compiling**. We are only able to cross-compile when working
    with .whl (binary) packages. We can’t use .tar.gz or other source distributions
    when attempting to cross-compile. This is a feature of `pip` not a Metaflow issue.
    Using the `@conda` decorator works (`conda` appears to resolve what `pip` cannot),
    but then I have to use the `tensorflow-gpu` package from conda if I want to use
    my GPU for compute, which comes with its own host of issues. There are workarounds,
    but they add too much complication for a tutorial that I want to be straightforward.
    As a result, I essentially had to go the `pip install -r requirements.txt` (used
    a custom Python `@pip` decorator to do so.) Not great, but hey, it does work.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，事情并非完全顺利。我的个人笔记本电脑是 Mac，但 AWS Batch 中的计算实例采用的是 Linux 架构。这意味着我们必须为 Linux
    机器创建隔离的虚拟环境，而不是 Mac。这就需要所谓的**交叉编译**。我们只有在处理 `.whl`（二进制）包时才能进行交叉编译。我们不能在尝试交叉编译时使用
    `.tar.gz` 或其他源代码发行版。这是 `pip` 的一个特点，而不是 Metaflow 的问题。使用 `@conda` 装饰器是有效的（`conda`
    似乎能够解决 `pip` 不能解决的问题），但如果我想利用 GPU 进行计算，则必须使用 conda 中的 `tensorflow-gpu` 包，这也带来了自己的问题。虽然有一些解决方法，但它们为我希望保持简洁的教程增加了太多复杂性。因此，我实际上不得不选择了
    `pip install -r requirements.txt`（使用了自定义 Python `@pip` 装饰器来实现）。虽然不太理想，但它确实有效。
- en: How to Debug a Production Flow
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何调试生产环境中的流程
- en: Initially, using Metaflow felt slow. Each time a step failed, I had to add print
    statements and re-run the entire flow — a time-consuming and costly process, especially
    with compute-intensive steps.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，使用 Metaflow 感觉有些慢。每次步骤失败时，我都需要添加打印语句并重新运行整个流程——这是一个耗时且代价高昂的过程，尤其是在计算密集型步骤中。
- en: 'Once I discovered that I could store [flow variables as artifacts](https://docs.metaflow.org/metaflow/client),
    and then access the values for these artifacts afterwards in a Jupyter notebook,
    my iteration speed increased dramatically. For example, when working with the
    output of the `model.predict` call, I stored variables as artifacts for easy debugging.
    Here’s how I did it:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我发现可以将[流程变量作为工件](https://docs.metaflow.org/metaflow/client)存储，并且之后可以在 Jupyter
    notebook 中访问这些工件的值，我的迭代速度大大提升。例如，在处理 `model.predict` 调用的输出时，我将变量作为工件存储，以便于调试。以下是我如何做到的：
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, `model` is my fully-trained object detection model, and `image` is a sample
    image. When I was working on this script, I had trouble working with the output
    of the `model.predict` call. What type was being output? What was the structure
    of the output? Was there an issue with the code to pull the example image?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`model` 是我经过充分训练的目标检测模型，`image` 是一张示例图像。当我在处理这个脚本时，我遇到了处理 `model.predict`
    调用输出的问题。输出是什么类型的？输出的结构是什么样的？拉取示例图像的代码是否有问题？
- en: 'To inspect these variables, I stored them as artifacts using the `self._` notation.
    Any object that can be [pickled](https://docs.python.org/3/library/pickle.html)
    can be stored as a Metaflow artifact. If you follow my tutorial, these artifacts
    will be stored in an Amazon S3 buckets for referencing in the future. To check
    that the example image is correctly being loaded, I can open up a Jupyter notebook
    in my same repository on my local computer, and access the image via the following
    code:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查这些变量，我使用 `self._` 语法将它们作为工件存储。任何可以被[pickle](https://docs.python.org/3/library/pickle.html)序列化的对象都可以作为
    Metaflow 工件存储。如果你跟随我的教程，这些工件将被存储在 Amazon S3 存储桶中，供以后引用。为了检查示例图像是否正确加载，我可以在我的本地计算机的同一代码库中打开
    Jupyter notebook，并通过以下代码访问该图像：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, we get the latest run of our flow and make sure we are getting our flow’s
    information by specifying `main_flow` in the Flow call. The artifacts I stored
    came from the `evaluate_model` step, so I specify this step. I get the image data
    itself by calling `.data.image` . Finally we can plot the image to check and see
    if our test image is valid, or if it got messed up somewhere in the pipeline:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们获取流程的最新运行，并通过在 Flow 调用中指定 `main_flow` 来确保获取到流程的信息。我存储的工件来自 `evaluate_model`
    步骤，因此我指定了这一步骤。我通过调用 `.data.image` 获取图像数据。最后，我们可以绘制图像来检查并查看我们的测试图像是否有效，或者是否在管道的某个环节被破坏了：
- en: '![](../Images/190557d883fea877399a116a9732e6e1.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/190557d883fea877399a116a9732e6e1.png)'
- en: Output image in my Jupyter notebook. Image by author.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的 Jupyter notebook 中输出的图像。图像来源：作者。
- en: 'Great, this matches the original image downloaded from the PlantDoc dataset
    (as strange as the colors appear.) To check out the predictions from our object
    detection model, we can use the following code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒，这和从PlantDoc数据集中下载的原始图像一致（尽管颜色看起来有些奇怪）。为了查看我们物体检测模型的预测结果，我们可以使用以下代码：
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/8d79e9da2d852fa191d977303249ae23.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d79e9da2d852fa191d977303249ae23.png)'
- en: Predictions from the object detection model. Image by author.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 来自物体检测模型的预测。图片由作者提供。
- en: The output appears to suggest that there were no predicted bounding boxes from
    this image. This is interesting to note, and can illuminate why a step is behaving
    oddly or breaking.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果似乎表明这个图像没有预测的边界框。这一点很有意思，可能有助于解释某个步骤为何表现异常或出现错误。
- en: 'All of this is done from a simple Jupyter notebook that all data scientists
    are comfortable with. So when should you store variables as artifacts in Metaflow?
    Here is a heuristic from Ville Tuulos [2]:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都可以通过一个简单的Jupyter笔记本完成，这是所有数据科学家都很熟悉的。那么，何时应将变量作为工件存储在Metaflow中呢？Ville Tuulos给出了一个启发式的方法[2]：
- en: RULE OF THUMB Use instance variables, such as self, to store any data and objects
    that may have value outside the step. Use local variables only for inter- mediary,
    temporary data. When in doubt, use instance variables because they make debugging
    easier.
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一条经验法则：使用实例变量（例如self）来存储任何可能在步骤外部有用的数据和对象。仅将本地变量用于中间的临时数据。如果不确定，使用实例变量，因为它们使调试更加容易。
- en: 'Learn from my lesson if you are using Metaflow: **take full advantage of artifacts
    and Jupyter notebooks to make debugging a breeze in your production-grade project.**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在使用Metaflow，请从我的经验中吸取教训：**充分利用工件和Jupyter笔记本，使调试在生产级项目中变得轻松。**
- en: 'One more note on debugging: if a flow fails in a particular step, and you want
    to re-run the flow from that failed step, use the `resume` command in Metaflow.
    This will load in all relevant output from previous steps without wasting time
    on re-executing them. I didn’t appreciate the simplicity of this until I tried
    out [Prefect](https://docs.prefect.io/latest/), and found out that there was no
    easy way to do the same.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 关于调试的另一个注意事项：如果一个流程在某个特定步骤失败，且你希望从该失败步骤重新运行流程，可以在Metaflow中使用`resume`命令。这样可以加载之前步骤的所有相关输出，而无需重新执行它们，从而节省时间。直到我尝试了[Prefect](https://docs.prefect.io/latest/)，才意识到那里并没有一个简单的方法来做到这一点。
- en: Finding the Goldilocks Step Size
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找合适的步骤大小
- en: What is the [Goldilocks](https://en.wikipedia.org/wiki/Goldilocks_and_the_Three_Bears)
    size of a step? In theory, you can stuff your entire script into one huge `pull_and_augment_data_and_train_model_and_evaluate_model_and_deploy`
    step, but this is not advisable. If a part of this flow fails, you can’t easily
    use the `resume` function to skip re-running the entire flow.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[Goldilocks](https://en.wikipedia.org/wiki/Goldilocks_and_the_Three_Bears)步骤的大小应该是多少？理论上，你可以把整个脚本塞进一个巨大的`pull_and_augment_data_and_train_model_and_evaluate_model_and_deploy`步骤中，但这样并不可取。如果流程中的某个部分失败，你就不能轻松使用`resume`功能跳过重新运行整个流程。'
- en: 'Conversely, it is also possible to chunk a script into a hundred micro-steps,
    but this is also not advisable. Storing artifacts and managing steps creates some
    overhead, and having a hundred steps would dominate the execution time. To find
    the Goldilocks size of a step, Tuulos tells us:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，将脚本拆分为一百个微小步骤也是可能的，但这同样不推荐。存储工件和管理步骤会带来一定的开销，拥有一百个步骤会占据大部分执行时间。为了找到一个合适的步骤大小，Tuulos告诉我们：
- en: RULE OF THUMB Structure your workflow in logical steps that are easily explainable
    and understandable. When in doubt, err on the side of small steps. They tend to
    be more easily understandable and debuggable than large steps.
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一条经验法则：将工作流结构化为逻辑清晰、容易解释和理解的步骤。如果不确定，倾向于选择较小的步骤。小步骤往往比大步骤更容易理解和调试。
- en: 'Initially, I structured my flow with these steps:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我将我的流程结构设计为这些步骤：
- en: Augment data
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强数据
- en: Train model
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Evaluate model
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型
- en: Deploy model
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署模型
- en: 'After augmenting the data, I had to upload the data to an S3 bucket, and then
    download the augmented data in the `train` step for training the model for two
    reasons:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在增强数据后，我需要将数据上传到一个S3存储桶，然后在`train`步骤中下载增强后的数据，用于模型训练，原因有两个：
- en: the `augment` step was to take place on my local laptop while the `train` step
    was going to be sent to a GPU instance on the cloud.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`augment`步骤将在我的本地笔记本上进行，而`train`步骤则会发送到云端的GPU实例。'
- en: Metaflow’s artifacts, normally used for passing data between steps, couldn’t
    handle TensorFlow Dataset objects as they are not pickle-able. I had to convert
    them to `tfrecords` and upload them to S3.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metaflow 的工件通常用于在步骤之间传递数据，但它无法处理 TensorFlow 数据集对象，因为它们不能被 pickle。于是我将它们转换为 `tfrecords`
    格式并上传到 S3。
- en: This upload/download process took a long time. So I combined the data augmentation
    and training steps into one. This reduced the flow’s runtime and complexity. If
    you’re curious, check out the `separate_augement_train` branch in my GitHub repo
    for the version with separated steps.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个上传/下载过程花费了很长时间。因此，我将数据增强和训练步骤合并为一个步骤。这样减少了流程的运行时间和复杂性。如果你感兴趣，可以查看我 GitHub
    仓库中的 `separate_augement_train` 分支，该版本包含了分开的步骤。
- en: Takeaways
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要收获
- en: 'In this article, I discussed some of the highs and lows I experienced when
    productionizing my object detection project. A quick summary:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我讨论了在将我的目标检测项目投入生产时所经历的一些高峰和低谷。简要总结如下：
- en: You will have to learn some ops in order to get to MLOps without the ops. But
    after learning some of the fundamental setup required, you will be able to ship
    compute jobs out to AWS using just a Python decorator. The repo attached to this
    article covers how to provision GPUs in AWS, so study this closely if this is
    one of your goals.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你必须学习一些操作，才能在没有操作的情况下实现 MLOps。但在学习了一些基础的设置后，你将能够仅使用 Python 装饰器将计算任务发送到 AWS。本文附带的代码库介绍了如何在
    AWS 中配置 GPU，因此如果这是你的目标之一，请仔细研究。
- en: Dependency management is a critical step in production. A `requirements.txt`
    file is the bare minimum, Docker is the gold standard, while Metaflow has a middle
    path that is usable for many projects. Just not this one, unfortunately.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖管理是生产中的一个关键步骤。`requirements.txt` 文件是最基本的要求，Docker 是黄金标准，而 Metaflow 提供了一条中间路径，适用于许多项目。只不过这个项目不适用，不幸的是。
- en: Use artifacts and Jupyter notebooks for easy debugging in Metaflow. Use the
    `resume` to avoid re-running time/compute-intensive steps.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Metaflow 中，使用工件和 Jupyter 笔记本可以方便地进行调试。使用 `resume` 功能可以避免重新运行时间或计算密集型的步骤。
- en: When breaking a script into steps for entry into a Metaflow flow, try to break
    up the steps into reasonable size steps, erring on the side of small steps. But
    don’t be afraid to combine steps if the overhead is just too much.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将脚本拆分为适合 Metaflow 流程的步骤时，尽量将步骤拆分成合理大小的小步骤，倾向于使用较小的步骤。但如果开销过大，也不要害怕合并步骤。
- en: There are still aspects of this project that I would like to improve on. One
    would be adding data so that we would be able to detect diseases on more varied
    plant species. Another would be to add a front end to the project and allow users
    to upload images and get object detections on demand. A library like Streamlit
    would work well for this. Finally, I would like the performance of the final model
    to become state-of-the-art. Metaflow has the ability to parallelize training many
    models simultaneously which would help with this goal. Unfortunately this would
    require lots of compute and money, but this is required of any state-of-the-art
    model.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目仍然有一些我希望改进的方面。一个方面是添加更多的数据，这样我们就能在更多种类的植物上检测疾病。另一个方面是为项目添加前端，允许用户上传图片并按需获取物体检测。像
    Streamlit 这样的库非常适合这个功能。最后，我希望最终模型的性能能达到最先进的水平。Metaflow 具备并行训练多个模型的能力，这将有助于实现这一目标。不幸的是，这需要大量的计算资源和资金，但这是任何最先进模型所必需的。
- en: References
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] C. Huyen, [Introduction to Machine Learning Interviews](https://huyenchip.com/ml-interviews-book/)(2021),
    Self-published'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] C. Huyen, [《机器学习面试简介》](https://huyenchip.com/ml-interviews-book/)(2021),
    自出版'
- en: '[2] V. Tuulos, [Effective Data Science Infrastructure](https://www.manning.com/books/effective-data-science-infrastructure)
    (2022), Manning Publications Co.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] V. Tuulos, [《高效的数据科学基础设施》](https://www.manning.com/books/effective-data-science-infrastructure)
    (2022), Manning 出版社'
