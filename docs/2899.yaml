- en: 'Model Validation Techniques, Explained: A Visual Guide with Code Examples'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/model-validation-techniques-explained-a-visual-guide-with-code-examples-eb13bbdc8f88?source=collection_archive---------1-----------------------#2024-11-30](https://towardsdatascience.com/model-validation-techniques-explained-a-visual-guide-with-code-examples-eb13bbdc8f88?source=collection_archive---------1-----------------------#2024-11-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MODEL EVALUATION & OPTIMIZATION
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 12 must-know methods to v**alidate your machine learning**
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--eb13bbdc8f88--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--eb13bbdc8f88--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--eb13bbdc8f88--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--eb13bbdc8f88--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--eb13bbdc8f88--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--eb13bbdc8f88--------------------------------)
    ·26 min read·Nov 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Every day, machines make millions of predictions — from detecting objects in
    photos to helping doctors find diseases. But before trusting these predictions,
    we need to know if they’re any good. After all, no one would want to use a machine
    that’s wrong most of the time!
  prefs: []
  type: TYPE_NORMAL
- en: This is where validation comes in. Validation methods test machine predictions
    to measure their reliability. While this might sound simple, different validation
    approaches exist, each designed to handle specific challenges in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Here, I’ve organized these validation techniques — all 12 of them — in a tree
    structure, showing how they evolved from basic concepts into more specialized
    ones. And of course, we will use clear visuals and a consistent dataset to show
    what each method does differently and why method selection matters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1f5d5ea3c85d86aa30c1a32e4af95d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: What is Model Validation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model validation is the process of testing how well a machine learning model
    works with data it hasn’t seen or used during training. Basically, we use existing
    data to check the model’s performance instead of using new data. This helps us
    identify problems before deploying the model for real use.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several validation methods, and each method has specific strengths
    and addresses different validation challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: Different validation methods can produce different results, so choosing the
    right method matters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some validation techniques work better with specific types of data and models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using incorrect validation methods can give misleading results about the model’s
    true performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is a tree diagram showing how these validation methods relate to each
    other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef40a8b199595fb3a2ea907fc7d8c4e7.png)'
  prefs: []
  type: TYPE_IMG
- en: The tree diagram shows which validation methods are connected to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at each validation method more closely by showing exactly how
    they work. To make everything easier to understand, we’ll walk through clear examples
    that show how these methods work with real data.
  prefs: []
  type: TYPE_NORMAL
- en: 📊 📈 Our Running Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the same example throughout to help you understand each testing
    method. While this dataset may not be appropriate for some validation methods,
    for education purpose, using this one example makes it easier to compare different
    methods and see how each one works.
  prefs: []
  type: TYPE_NORMAL
- en: 📊 The Golf Playing Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll work with this dataset that predicts whether someone will play golf based
    on weather conditions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a76a1336de0cf6952c9aee515376a7ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: ‘Overcast (one-hot-encoded into 3 columns)’, ’Temperature’ (in Fahrenheit),
    ‘Humidity’ (in %), ‘Windy’ (Yes/No) and ‘Play’ (Yes/No, target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 📈 Our Model Choice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use a [decision tree classifier](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    for all our tests. See the following article if you are not familiar with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----eb13bbdc8f88--------------------------------)
    [## Decision Tree Classifier, Explained: A Visual Guide with Code Examples for
    Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: A fresh look on our favorite upside-down tree
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----eb13bbdc8f88--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: We picked this model because we can easily draw the resulting model as a tree
    structure, with each branch showing different decisions. To keep things simple
    and focus on how we test the model, we will use the default `scikit-learn` parameter
    with a fixed `random_state`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s be clear about these two terms we’ll use: The decision tree classifier
    is our **learning algorithm** — it’s the method that finds patterns in our data.
    When we feed data into this algorithm, it creates a **model** (in this case, a
    tree with clear branches showing different decisions). This model is what we’ll
    actually use to make predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05f04d2e03922330e874044c751e77f9.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Each time we split our data differently for validation, we’ll get different
    models with different decision rules. Once our validation shows that our algorithm
    works reliably, we’ll create one final model using all our data. This final model
    is the one we’ll actually use to predict if someone will play golf or not.
  prefs: []
  type: TYPE_NORMAL
- en: With this setup ready, we can now focus on understanding how each validation
    method works and how it helps us make better predictions about golf playing based
    on weather conditions. Let’s examine each validation method one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Hold-out Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hold-out methods are the most basic way to check how well our model works. In
    these methods, we basically save some of our data just for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Train-Test Split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This method is simple: we split our data into two parts. We use one part to
    train our model and the other part to test it. Before we split the data, we mix
    it up randomly so the order of our original data doesn’t affect our results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the training and test dataset size depends on our total dataset size,
    usually denoted by their ratio. To determine their size, you can follow this guideline:'
  prefs: []
  type: TYPE_NORMAL
- en: For small datasets (around 1,000–10,000 samples), use 80:20 ratio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For medium datasets (around 10,000–100,000 samples), use 70:30 ratio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large datasets (over 100,000 samples), use 90:10 ratio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/ea1cda2f5b4ebaf2ac345e82232c49e6.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9b572acdaf081abe2ed17104646ae2ac.png)'
  prefs: []
  type: TYPE_IMG
- en: This method is easy to use, but it has some limitation — the results can change
    a lot depending on how we randomly split the data. This is why we always need
    to try out different `random_state` to make sure that the result is consistent.
    Also, if we don’t have much data to start with, we might not have enough to properly
    train or test our model.
  prefs: []
  type: TYPE_NORMAL
- en: Train-Validation-Test Split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This method split our data into three parts. The middle part, called validation
    data, is being used to tune the parameters of the model and we’re aiming to have
    the least amount of error there.
  prefs: []
  type: TYPE_NORMAL
- en: Since the validation results is considered many times during this tuning process,
    our model might start doing too well on this validation data (which is what we
    want). This is the reason of why we make the separate test set. We are only testing
    it once at the very end — it gives us the truth of how well our model works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are typical ways to split your data:'
  prefs: []
  type: TYPE_NORMAL
- en: For smaller datasets (1,000–10,000 samples), use 60:20:20 ratio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For medium datasets (10,000–100,000 samples), use 70:15:15 ratio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large datasets (> 100,000 samples), use 80:10:10 ratio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/8abee1c3e7b3526152ccf2256108da3f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4b6cfed209a3227dfc62eaf3f28413a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Hold-out methods work differently depending on how much data you have. They
    work really well when you have lots of data (> 100,000). But when you have less
    data (< 1,000) this method is not be the best. With smaller datasets, you might
    need to use more advanced validation methods to get a better understanding of
    how well your model really works.
  prefs: []
  type: TYPE_NORMAL
- en: 📊 Moving to Cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We just learned that hold-out methods might not work very well with small datasets.
    This is exactly the challenge we currently face— we only have 28 days of data.
    Following the hold-out principle, we’ll keep 14 days of data separate for our
    final test. This leaves us with 14 days to work with for trying other validation
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81a27f280c9b79b4950ec9a9f00ae731.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the next part, we’ll see how cross-validation methods can take these 14 days
    and split them up multiple times in different ways. This gives us a better idea
    of how well our model is really working, even with such limited data.
  prefs: []
  type: TYPE_NORMAL
- en: Cross Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-validation changes how we think about testing our models. Instead of testing
    our model just once with one split of data, we test it many times using different
    splits of the same data. This helps us understand much better how well our model
    really works.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea of cross-validation is to test our model multiple times, and each
    time the training and test dataset come from different part of the our data. This
    helps prevent bias by one really good (or really bad) split of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s why this matters: say our model gets 95% accuracy when we test it one
    way, but only 75% when we test it another way using the same data. Which number
    shows how good our model really is? Cross-validation helps us answer this question
    by giving us many test results instead of just one. This gives us a clearer picture
    of how well our model actually performs.'
  prefs: []
  type: TYPE_NORMAL
- en: K-Fold Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***Basic K-Fold Cross-Validation*** *K*-fold cross-validation fixes a big problem
    with basic splitting: relying too much on just one way of splitting the data.
    Instead of splitting the data once, *K*-fold splits the data into *K* equal parts.
    Then it tests the model multiple times, using a different part for testing each
    time while using all other parts for training.'
  prefs: []
  type: TYPE_NORMAL
- en: The number we pick for *K* changes how we test our model. Most people use 5
    or 10 for *K*, but this can change based on how much data we have and what we
    need for our project. Let’s say we use *K* = 3\. This means we split our data
    into three equal parts. We then train and test our model three different times.
    Each time, 2/3 of the data is used for training and 1/3 for testing, but we rotate
    which part is being used for testing. This way, every piece of data gets used
    for both training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06b5298158f9daf8c0fdf2f24ba9d7f9.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`Validation accuracy: 0.433 ± 0.047`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3597efd5b0c424bb9ef27f510ef42907.png)'
  prefs: []
  type: TYPE_IMG
- en: When we’re done with all the rounds, we calculate the average performance from
    all *K* tests. This average gives us a more trustworthy measure of how well our
    model works. We can also learn about how stable our model is by looking at how
    much the results change between different rounds of testing.
  prefs: []
  type: TYPE_NORMAL
- en: '***Stratified K-Fold*** Basic K-fold cross-validation usually works well, but
    it can run into problems when our data is unbalanced — meaning we have a lot more
    of one type than others. For example, if we have 100 data points and 90 of them
    are type A while only 10 are type B, randomly splitting this data might give us
    pieces that don’t have enough type B to test properly.'
  prefs: []
  type: TYPE_NORMAL
- en: Stratified K-fold fixes this by making sure each split has the same mix as our
    original data. If our full dataset has 10% type B, each split will also have about
    10% type B. This makes our testing more reliable, especially when some types of
    data are much rarer than others.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6410d2ca1a0a1801423584f4ee9c30dd.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`Validation accuracy: 0.650 ± 0.071`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a845da93fa64fae2b73609521f534a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Keeping this balance helps in two ways. First, it makes sure each split properly
    represents what our data looks like. Second, it gives us more consistent test
    results . This means that if we test our model multiple times, we’ll most likely
    get similar results each time.
  prefs: []
  type: TYPE_NORMAL
- en: '***Repeated K-Fold*** Sometimes, even when we use K-fold validation, our test
    results can change a lot between different random splits. Repeated K-fold solves
    this by running the entire K-fold process multiple times, using different random
    splits each time.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say we run 5-fold cross-validation three times. This means
    our model goes through training and testing 15 times in total. By testing so many
    times, we can better tell which differences in results come from random chance
    and which ones show how well our model really performs. The downside is that all
    this extra testing takes more time to complete.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f2b033a8e6b90cc1bad5a07059d1457a.png)![](../Images/53d8b614cdb52f63a8289ec002c7dce5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`Validation accuracy: 0.425 ± 0.107`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e94bb4e17347fd027be23da900507dd9.png)'
  prefs: []
  type: TYPE_IMG
- en: When we look at repeated K-fold results, since we have many sets of test results,
    we can do more than just calculate the average — we can also figure out how confident
    we are in our results. This gives us a better understanding of how reliable our
    model really is.
  prefs: []
  type: TYPE_NORMAL
- en: '***Repeated Stratified K-Fold*** This method combines two things we just learned
    about: keeping class balance (stratification) and running multiple rounds of testing
    (repetition). It keeps the right mix of different types of data while testing
    many times. This works especially well when we have a small dataset that’s uneven
    — where we have a lot more of one type of data than others.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9084cb48d717d1a53b287556171438e.png)![](../Images/9736bf44bf4d82ce511033183bcb338a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`Validation accuracy: 0.542 ± 0.167`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61517576ff80c26f2c20ca066afe43fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, there’s a trade-off: this method takes more time for our computer
    to run. Each time we repeat the whole process, it multiplies how long it takes
    to train our model. When deciding whether to use this method, we need to think
    about whether having more reliable results is worth the extra time it takes to
    run all these tests.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Group K-Fold*** Sometimes our data naturally comes in groups that should
    stay together. Think about golf data where we have many measurements from the
    same golf course throughout the year. If we put some measurements from one golf
    course in training data and others in test data, we create a problem: our model
    would indirectly learn about the test data during training because it saw other
    measurements from the same course.'
  prefs: []
  type: TYPE_NORMAL
- en: Group K-fold fixes this by keeping all data from the same group (like all measurements
    from one golf course) together in the same part when we split the data. This prevents
    our model from accidentally seeing information it shouldn’t, which could make
    us think it performs better than it really does.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cde7db87459ae48728b9dd87dd26ac88.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`Validation accuracy: 0.417 ± 0.143`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/616c266d6ae7d923873b81a26b0df5de.png)'
  prefs: []
  type: TYPE_IMG
- en: This method can be important when working with data that naturally comes in
    groups, like multiple weather readings from the same golf course or data that
    was collected over time from the same location.
  prefs: []
  type: TYPE_NORMAL
- en: '***Time Series Split*** When we split data randomly in regular K-fold, we assume
    each piece of data doesn’t affect the others. But this doesn’t work well with
    data that changes over time, where what happened before affects what happens next.
    Time series split changes K-fold to work better with this kind of time-ordered
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of splitting data randomly, time series split uses data in order, from
    past to future. The training data only includes information from times before
    the testing data. This matches how we use models in real life, where we use past
    data to predict what will happen next.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81146fa4c70beaca8801445fd200d3fb.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`Validation accuracy: 0.556 ± 0.157`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d881beb9b6d1811302457256067f38f.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, with *K*=3 and our golf data, we might train using weather data
    from January and February to predict March’s golf playing patterns. Then we’d
    train using January through March to predict April, and so on. By only going forward
    in time, this method gives us a more realistic idea of how well our model will
    work when predicting future golf playing patterns based on weather.
  prefs: []
  type: TYPE_NORMAL
- en: Leave-Out Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***Leave-One-Out Cross-Validation (LOOCV)*** Leave-One-Out Cross-Validation
    (LOOCV) is the most thorough validation method. It uses just *one* sample for
    testing and all other samples for training. The validation is repeated until every
    single piece of data has been used for testing.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we have 100 days of golf weather data. LOOCV would train and test
    the model 100 times. Each time, it uses 99 days for training and 1 day for testing.
    This method removes any randomness in testing — if you run LOOCV on the same data
    multiple times, you’ll always get the same results.
  prefs: []
  type: TYPE_NORMAL
- en: However, LOOCV takes a lot of computing time. If you have *N* pieces of data,
    you need to train your model *N* times. With large datasets or complex models,
    this might take too long to be practical. Some simpler models, like linear ones,
    have shortcuts that make LOOCV faster, but this isn’t true for all models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52e0f4b1c42428101fe15f6f81637446.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`Validation accuracy: 0.429 ± 0.495`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40709bb360f71992f6218297c2d2242e.png)'
  prefs: []
  type: TYPE_IMG
- en: LOOCV works really well when we don’t have much data and need to make the most
    of every piece we have. Since the result depend on every single data, the results
    can change a lot if our data has noise or unusual values in it.
  prefs: []
  type: TYPE_NORMAL
- en: '***Leave-P-Out Cross-Validation*** Leave-P-Out builds on the idea of Leave-One-Out,
    but instead of testing with just one piece of data, it tests with P pieces at
    a time. This creates a balance between Leave-One-Out and K-fold validation. The
    number we choose for P changes how we test the model and how long it takes.'
  prefs: []
  type: TYPE_NORMAL
- en: The main problem with Leave-P-Out is how quickly the number of possible test
    combinations grows. For example, if we have 100 days of golf weather data and
    we want to test with 5 days at a time (P=5), there are millions of different possible
    ways to choose those 5 days. Testing all these combinations takes too much time
    when we have lots of data or when we use a larger number for P.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b08884b049867b549153b90e059cd20b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`Validation accuracy: 0.441 ± 0.254`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b285dc37f93b1968c4e62df28fba57d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Because of these practical limits, Leave-P-Out is mostly used in special cases
    where we need very thorough testing and have a small enough dataset to make it
    work. It’s especially useful in research projects where getting the most accurate
    test results matters more than how long the testing takes.
  prefs: []
  type: TYPE_NORMAL
- en: Random Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***ShuffleSplit Cross-Validation*** ShuffleSplit works differently from other
    validation methods by using completely random splits. Instead of splitting data
    in an organized way like K-fold, or testing every possible combination like Leave-P-Out,
    ShuffleSplit creates random training and testing splits each time.'
  prefs: []
  type: TYPE_NORMAL
- en: What makes ShuffleSplit different from K-fold is that the splits don’t follow
    any pattern. In K-fold, each piece of data gets used exactly once for testing.
    But in ShuffleSplit, a single day of golf weather data might be used for testing
    several times, or might not be used for testing at all. This randomness gives
    us a different way to understand how well our model performs.
  prefs: []
  type: TYPE_NORMAL
- en: ShuffleSplit works especially well with large datasets where K-fold might take
    too long to run. We can choose how many times we want to test, no matter how much
    data we have. We can also control how big each split should be. This lets us find
    a good balance between thorough testing and the time it takes to run.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/923156cc72cc526aae6376b4a83d7b24.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`Validation accuracy: 0.333 ± 0.272`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c1642d4aa7fd19870eddc81316dddfa.png)'
  prefs: []
  type: TYPE_IMG
- en: Since ShuffleSplit can create as many random splits as we want, it’s useful
    when we want to see how our model’s performance changes with different random
    splits, or when we need more tests to be confident about our results.
  prefs: []
  type: TYPE_NORMAL
- en: '***Stratified ShuffleSplit*** Stratified ShuffleSplit combines random splitting
    with keeping the right mix of different types of data. Like Stratified K-fold,
    it makes sure each split has about the same percentage of each type of data as
    the full dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This method gives us the best of both worlds: the freedom of random splitting
    and the fairness of keeping data balanced. For example, if our golf dataset has
    70% “yes” days and 30% “no” days for playing golf, each random split will try
    to keep this same 70–30 mix. This is especially useful when we have uneven data,
    where random splitting might accidentally create test sets that don’t represent
    our data well.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6f0a11a4d547374bf57dafa1869e61c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`Validation accuracy: 0.556 ± 0.157`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96409b67fddbc5e5715ea4f0bed7f347.png)'
  prefs: []
  type: TYPE_IMG
- en: However, trying to keep both the random nature of the splits and the right mix
    of data types can be tricky. The method sometimes has to make small compromises
    between being perfectly random and keeping perfect proportions. In real use, these
    small trade-offs rarely cause problems, and having balanced test sets is usually
    matters more than having perfectly random splits.
  prefs: []
  type: TYPE_NORMAL
- en: 🌟 Validation Techniques Summarized & Code Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To summarize, model validation methods fall into two main categories: hold-out
    methods and cross-validation methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hold-out Methods** · Train-Test Split: The simplest approach, dividing data
    into two parts'
  prefs: []
  type: TYPE_NORMAL
- en: '· Train-Validation-Test Split: A three-way split for more complex model development'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross-validation Methods** Cross-validation methods make better use of available
    data through multiple rounds of validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*K-Fold Methods* Rather than a single split, these methods divide data into
    K parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '· Basic K-Fold: Rotates through different test sets'
  prefs: []
  type: TYPE_NORMAL
- en: '· Stratified K-Fold: Maintains class balance across splits'
  prefs: []
  type: TYPE_NORMAL
- en: '· Group K-Fold: Preserves data grouping'
  prefs: []
  type: TYPE_NORMAL
- en: '· Time Series Split: Respects temporal order'
  prefs: []
  type: TYPE_NORMAL
- en: · Repeated K-Fold
  prefs: []
  type: TYPE_NORMAL
- en: · Repeated Stratified K-Fold
  prefs: []
  type: TYPE_NORMAL
- en: '*Leave-Out Methods* These methods take validation to the extreme:'
  prefs: []
  type: TYPE_NORMAL
- en: '· Leave-P-Out: Tests on P data points at a time'
  prefs: []
  type: TYPE_NORMAL
- en: '· Leave-One-Out: Tests on single data points'
  prefs: []
  type: TYPE_NORMAL
- en: '*Random Methods* These introduce controlled randomness:'
  prefs: []
  type: TYPE_NORMAL
- en: '· ShuffleSplit: Creates random splits repeatedly'
  prefs: []
  type: TYPE_NORMAL
- en: '· Stratified ShuffleSplit: Random splits with balanced classes'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`Validation accuracy: 0.429 ± 0.495`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Test accuracy: 0.714`'
  prefs: []
  type: TYPE_NORMAL
- en: '***Comment on the result above:*** The large gap between validation and test
    accuracy, along with the very high standard deviation in validation scores, suggests
    our model’s performance is unstable. This inconsistency likely comes from using
    LeaveOneOut validation on our small weather dataset — testing on single data points
    causes performance to vary dramatically. A different validation method using larger
    validation sets might give us more reliable results.'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Right Validation Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Choosing how to validate your model isn’t simple — different situations need
    different approaches. Understanding which method to use can mean the difference
    between getting reliable or misleading results. Here are some aspect that you
    should consider when choosing the validation method:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Dataset Size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The size of your dataset strongly influences which validation method works
    best. Let’s look at different sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Large Datasets (More than 100,000 samples)**'
  prefs: []
  type: TYPE_NORMAL
- en: When you have large datasets, the amount of time to test becomes one of the
    main consideration. Simple hold-out validation (splitting data once into training
    and testing) often works well because you have enough data for reliable testing.
    If you need to use cross-validation, using just 3 folds or using ShuffleSplit
    with fewer rounds can give good results without taking too long to run.
  prefs: []
  type: TYPE_NORMAL
- en: '***Medium Datasets (1,000 to 100,000 samples)***'
  prefs: []
  type: TYPE_NORMAL
- en: For medium-sized datasets, regular K-fold cross-validation works best. Using
    5 or 10 folds gives a good balance between reliable results and reasonable computing
    time. This amount of data is usually enough to create representative splits but
    not so much that testing takes too long.
  prefs: []
  type: TYPE_NORMAL
- en: '***Small Datasets (Less than 1,000 samples)***'
  prefs: []
  type: TYPE_NORMAL
- en: Small datasets, like our example of 28 days of golf records, need more careful
    testing. Leave-One-Out Cross-Validation or Repeated K-fold with more folds can
    actually work well in this case. Even though these methods take longer to run,
    they help us get the most reliable results when we don’t have much data to work
    with.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Computational Resource
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When choosing a validation method, we need to think about our computing resources.
    There’s a three-way balance between dataset size, how complex our model is, and
    which validation method we use:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fast Training Models**'
  prefs: []
  type: TYPE_NORMAL
- en: Simple models like decision trees, logistic regression, and linear SVM can use
    more thorough validation methods like Leave-One-Out Cross-Validation or Repeated
    Stratified K-fold because they train quickly. Since each training round takes
    just seconds or minutes, we can afford to run many validation iterations. Even
    running LOOCV with its N training rounds might be practical for these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource-Heavy Models**'
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks, random forests with many trees, or gradient boosting models
    take much longer to train. When using these models, more intensive validation
    methods like Repeated K-fold or Leave-P-Out might not be practical. We might need
    to choose simpler methods like basic K-fold or ShuffleSplit to keep testing time
    reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory Considerations**'
  prefs: []
  type: TYPE_NORMAL
- en: Some methods like K-fold need to track multiple splits of data at once. ShuffleSplit
    can help with memory limitations since it handles one random split at a time.
    For large datasets with complex models (like deep neural networks that need lots
    of memory), simpler hold-out methods might be necessary. If we still need thorough
    validation with limited memory, we could use Time Series Split since it naturally
    processes data in sequence rather than needing all splits in memory at once.
  prefs: []
  type: TYPE_NORMAL
- en: When resources are limited, using a simpler validation method that we can run
    properly (like basic K-fold) is better than trying to run a more complex method
    (like Leave-P-Out) that we can’t complete properly.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Class Distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Class imbalance strongly affects how we should validate our model. With unbalanced
    data, stratified validation methods become essential. Methods like Stratified
    K-fold and Stratified ShuffleSplit make sure each testing split has about the
    same mix of classes as our full dataset. Without using these stratified methods,
    some test sets might end up with no particular class at all, making it impossible
    to properly test how well our model makes prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Time Series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When working with data that changes over time, we need special validation approaches.
    Regular random splitting methods don’t work well because time order matters.With
    time series data, we must use methods like Time Series Split that respect time
    order.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Group Dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many datasets contain natural groups of related data. These connections in our
    data need special handling when we validate our models. When data points are related,
    we need to use methods like Group K-fold to prevent our model from accidentally
    learning things it shouldn’t.
  prefs: []
  type: TYPE_NORMAL
- en: Practical Guidelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This flowchart will help you select the most appropriate validation method for
    your data. The steps below outline a clear process for choosing the best validation
    approach, assuming you have sufficient computing resources.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c111d0da670ace01dcf36fc6effc876.png)'
  prefs: []
  type: TYPE_IMG
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model validation is essential for building reliable machine learning models.
    After exploring many validation methods, from simple train-test splits to complex
    cross-validation approaches, we’ve learned that there is always a suitable validation
    method for whatever data you have.
  prefs: []
  type: TYPE_NORMAL
- en: While machine learning keeps changing with new methods and tools, these basic
    rules of validation stay the same. When you understand these principles well,
    I believe you’ll build models that people can trust and rely on.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of the [validation methods in](https://scikit-learn.org/stable/api/sklearn.model_selection.html)
    `[scikit-learn](https://scikit-learn.org/stable/api/sklearn.model_selection.html)`,
    readers can refer to the official documentation, which provides comprehensive
    information on its usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙈𝙤𝙙𝙚𝙡 𝙀𝙫𝙖𝙡𝙪𝙖𝙩𝙞𝙤𝙣 & 𝙊𝙥𝙩𝙞𝙢𝙞𝙯𝙖𝙩𝙞𝙤𝙣 𝙢𝙚𝙩𝙝𝙤𝙙𝙨 𝙝𝙚𝙧𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----eb13bbdc8f88--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Model Evaluation & Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/model-evaluation-optimization-331287896864?source=post_page-----eb13bbdc8f88--------------------------------)3
    stories![](../Images/18fa82b1435fa7d5571ee54ae93a6c62.png)![](../Images/c95e89d05d1de700c631c342cd008de0.png)![](../Images/30e20e1a8ba3ced1e77644b706acd18d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----eb13bbdc8f88--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----eb13bbdc8f88--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----eb13bbdc8f88--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----eb13bbdc8f88--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  prefs: []
  type: TYPE_NORMAL
