- en: How to Use Elastic Net Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-use-elastic-net-regression-85a6a393222b?source=collection_archive---------9-----------------------#2024-03-14](https://towardsdatascience.com/how-to-use-elastic-net-regression-85a6a393222b?source=collection_archive---------9-----------------------#2024-03-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Cast a flexible net that only retains big fish
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@cjtayl2?source=post_page---byline--85a6a393222b--------------------------------)[![Chris
    Taylor](../Images/a5a0b096777cc262cc5adc3350fadab4.png)](https://medium.com/@cjtayl2?source=post_page---byline--85a6a393222b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--85a6a393222b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--85a6a393222b--------------------------------)
    [Chris Taylor](https://medium.com/@cjtayl2?source=post_page---byline--85a6a393222b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--85a6a393222b--------------------------------)
    ·9 min read·Mar 14, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: The code used in this article utilizes three custom scripts,* `data_cleaning`*,*
    `data_review`*, and ,* `eda`*, that can be accessed through a public* [*GitHub
    repository*](https://github.com/CJTAYL/elastic_net_medium)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22442aa74d6abaf90e723802601a8a45.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Eric BARBEAU](https://unsplash.com/@ericbarbeau?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: It is like a stretchable fishing net that retains ‘all the big fish’ Zou & Hastie
    (2005) p. 302
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Background**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression is a commonly used teaching tool in data science and, under
    the appropriate conditions (e.g., linear relationship between the independent
    and dependent variables, absence of multicollinearity), it can be an effective
    method for predicting a response. However, in some scenarios (e.g., when the model’s
    structure becomes complex), its use can be problematic.
  prefs: []
  type: TYPE_NORMAL
- en: To address some of the algorithm’s limitations, penalization or regularization
    techniques have been suggested [1]. Two popular methods of regularization are
    ridge and lasso regression, but choosing between these methods can be difficult
    for those new to the field of data science.
  prefs: []
  type: TYPE_NORMAL
- en: One approach to choosing between ridge and lasso regression is to examine the
    relevancy of the features to the response variable [2]. When the majority of features
    in the model are relevant (i.e., contribute to the predictive power of the model),
    the ridge regression penalty (or L2 penalty) should be added to linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the ridge regression penalty is added, the cost function of the model
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b298d8d69cbf1957e25f9cda2caa8eb6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: θ = the vector of parameters or coefficients of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: α = the overall strength of the regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*m* = the number of training examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n* = the number of features in the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the majority of features are irrelevant (i.e., do not contribute to the
    predictive power of the model), the lasso regression penalty (or L1 penalty) should
    be added to linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the lasso regression penalty is added, the cost function of the model
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57e2c8bd0be6960ce028e9aff73d468d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Relevancy can be determined through manual review or cross validation; however,
    when working with several features, the process becomes time consuming and computationally
    expensive.
  prefs: []
  type: TYPE_NORMAL
- en: An efficient and flexible solution to this issue is using elastic net regression,
    which combines the ridge and lasso penalties.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost function for elastic net regression is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4a2c4e3f8965f8a2fd3e1895e4d90293.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: r = the mixing ratio between ridge and lasso regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When r is 1, only the lasso penalty is used and when r is 0 , only the ridge
    penalty is used. When r is a value between 0 and 1, a mixture of the penalties
    is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to being well-suited for datasets with several features, elastic
    net regression has other attributes that make it an appealing tool for data scientists
    [1]:'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic selection of relevant features, which results in parsimonious models
    that are easy to interpret
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous shrinkage, which gradually reduces the coefficients of less relevant
    features towards zero (opposed to an immediate reduction to zero)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to select groups of correlated features, instead of selecting one feature
    from the group arbitrarily
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to its utility and flexibility, Zou and Hastie (2005) compared the model
    to a “…stretchable fishing net that retains all the big fish.” (p. 302), where
    big fish are analogous to relevant features.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have some background, we can move forward to implementing elastic
    net regression on a real dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementation**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A great resource for data is the University of California at Irvine’s Machine
    Learning Repository (UCI ML Repo). For the tutorial, we’ll use the Wine Quality
    Dataset [3], which is licensed under a [Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/legalcode)
    license.
  prefs: []
  type: TYPE_NORMAL
- en: The function displayed below can be used to obtain datasets and variable information
    from the UCI ML Repo by entering the identification number as the parameter of
    the function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A pandas dataframe has been assigned to the variable “df” and information about
    the dataset has been printed.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory Data Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Based on the variable information, we can see that there are 11 “features”,
    1 “target”, and 1 “other” variables in the dataset. This is interesting information
    — if we had extracted the data without the variable information, we may not have
    known that there were data available on the family (or color) of wine. At this
    time, we won’t be incorporating the “color” variable into the model, but it’s
    nice to know it’s there for future iterations of the project.
  prefs: []
  type: TYPE_NORMAL
- en: The “description” column in the variable information suggests that the “quality”
    variable is categorical. The data are likely ordinal, meaning they have a hierarchical
    structure but the intervals between the data are not guaranteed to be equal or
    known. In practical terms, it means a wine rated as 4 is not twice as good as
    a wine rated as 2\. To address this issue, we’ll convert the data to the proper
    data-type.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To gain a better understanding of the data, we can use the `countplot()` method
    from the `seaborn` package to visualize the distribution of the “quality” variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b6af5dd87b5f361d902a404c7bc06f25.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'When conducting an exploratory data analysis, creating histograms for numeric
    features is beneficial. Additionally, grouping the variables by a categorical
    variable can provide new insights. The best option for grouping the data is “quality”.
    However, given there are 7 groups of quality, the plots could become difficult
    to read. To simplify grouping, we can create a new feature, “rating”, that organizes
    the data on “quality” into three categories: low, medium, and high.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To determine how many wines are each group, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Based on the output of the code, we can see that the majority of wines are categorized
    as “medium”.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can plot histograms of the numeric features groups by “rating”. To plot
    the histogram we’ll need to use the `gen_histograms_by_category()` method from
    the `eda` script in the GitHub repository shared at the beginning of the article.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2e1491e6afb60c5a265de19bd1f2b853.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Above is one of the plots generated by the method. A review of the plot indicates
    there is some skew in the data. To gain a more precise measure of skew, along
    with other statistics, we can use the `get_statistics()` method from the `data_review`
    script.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Consistent with the histogram, the feature labeled “fixed_acidity” has a skewness
    of 1.72 indicating significant right-skewness.
  prefs: []
  type: TYPE_NORMAL
- en: To determine if there are correlations between the variables, we can use another
    function from the `eda` script.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/271e84969b393ccdde17bfde8c8860f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Although there a few moderate and strong relationships between features, elastic
    net regression performs well with correlated variables, therefore, no action is
    required [2].
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Cleaning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the elastic net regression algorithm to run correctly, the numeric data
    must be scaled and the categorical variables must be encoded.
  prefs: []
  type: TYPE_NORMAL
- en: 'To clean the data, we’ll take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Scale the data using the the `scale_data()` method from the the `data_cleaning`
    script
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encode the “quality” and “rating” variables using the the `get_dummies()` method
    from `pandas`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Separate the features (i.e., X) and response variable (i.e., y) using the `separate_data()`
    method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into train and test sets using `train_test_split()`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**Model Building and Evaluation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train the model, we’ll use `ElasticNetCV()` which has two parameters, `alpha`
    and `l1_ratio`, and built-in cross validation. The `alpha` parameter determines
    the strength of the regularization applied to the model and `l1_ratio` determines
    the mix of the lasso and ridge penalty (it is equivalent to the variable *r* that
    was reviewed in the *Background* section).
  prefs: []
  type: TYPE_NORMAL
- en: When `l1_ratio` is set to a value of 0, the ridge regression penalty is used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When `l1_ratio` is set to a value of 1, the lasso regression penalty is used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When `l1_ratio` is set to a value between 0 and 1, a mixture of both penalties
    are used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing values for `alpha` and `l1_ratio` can be challenging; however, the
    task is made easier through the use of cross validation, which is built into `ElasticNetCV()`.
    To make the process easier, you don’t have to provide a list of values from `alpha`
    and `l1_ratio` — you can let the method do the heavy lifting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Based on the printout, we can see the best values for `alpha` and `l1_ratio`
    are 0.001 and 0.5, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: To determine how well the model performed, we can calculate the Mean Squared
    Error and the R-squared score of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**Conclusion**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the evaluation metrics, the model performs moderately well. However,
    its performance could be enhanced through some additional steps, like detecting
    and removing outliers, additional feature engineering, and providing a specific
    set of values for `alpha` and `l1_ratio` in `ElasticNetCV()`. Unfortunately, those
    steps are beyond the scope of this simple tutorial; however, they may provide
    some ideas for how this project could be improved by others.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for taking the time to read this article. If you have any questions
    or feedback, please leave a comment.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] H. Zou & T. Hastie, Regularization and Variable Selection Via the Elastic
    Net, Journal of the Royal Statistical Society Series B: Statistical Methodology,
    Volume 67, Issue 2, April 2005, Pages 301–320, [https://doi.org/10.1111/j.1467-9868.2005.00503.x](https://doi.org/10.1111/j.1467-9868.2005.00503.x)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] A. Géron, Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow:
    Concepts, Tools, and Techniques to Build Intelligent Systems (2021), O’Reilly.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] P. Cortez, A. Cerdeira, F. Almeida, T. Matos, & Reis,J.. (2009). Wine Quality.
    UCI Machine Learning Repository. [https://doi.org/10.24432/C56S3T](https://doi.org/10.24432/C56S3T).'
  prefs: []
  type: TYPE_NORMAL
