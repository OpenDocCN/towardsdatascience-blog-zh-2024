- en: Python Water Quality — Baseline Classification Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/python-water-quality-baseline-classification-model-bb5584226a82?source=collection_archive---------10-----------------------#2024-01-08](https://towardsdatascience.com/python-water-quality-baseline-classification-model-bb5584226a82?source=collection_archive---------10-----------------------#2024-01-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Assess feature importance when estimating water quality using a reference baseline
    model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jamesmcneill06.medium.com/?source=post_page---byline--bb5584226a82--------------------------------)[![James
    McNeill](../Images/5f71dfac8d1d37a1232d88c4ac04bf84.png)](https://jamesmcneill06.medium.com/?source=post_page---byline--bb5584226a82--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bb5584226a82--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bb5584226a82--------------------------------)
    [James McNeill](https://jamesmcneill06.medium.com/?source=post_page---byline--bb5584226a82--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bb5584226a82--------------------------------)
    ·11 min read·Jan 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14dc02e38c52e177eb13286d0101fc97.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Unseen Studio](https://unsplash.com/@craftedbygc?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Understanding what can be used to classify the water quality can be a challenge.
    Having expert knowledge of different regions can provide local insights into what
    helps to see how water flows best. Without the time to fully review these details,
    it reduces the possibility of learning from mistakes to benefit others. A quantifiable
    approach can be taken by collecting datasets of the features that impact water.
    With quantification it allows the user to apply computer science techniques to
    gain data-driven insights. For this article, we are aiming to apply a baseline
    Machine Learning classification model to help highlight key features. Model predictions
    will be produced with unseen data, commonly referred to as test data to validate
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: For prior details on the initial exploratory data analysis performed on the
    input dataset the article “Python water quality EDA and Potability analysis” is
    shared at the end.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this piece of analysis, the Water Quality dataset has been taken from Kaggle¹.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.kaggle.com/datasets/adityakadiwal/water-potability?source=post_page-----bb5584226a82--------------------------------)
    [## Water Quality'
  prefs: []
  type: TYPE_NORMAL
- en: Drinking water potability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.kaggle.com](https://www.kaggle.com/datasets/adityakadiwal/water-potability?source=post_page-----bb5584226a82--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: A jupyter notebook instance written with Python code was used for processing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: After running the script above an output would show that version 3.7.10 of Python
    was used. To be able to replicate the results that follow, users should ensure
    that their working environment has Python 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To begin the process steps several Python libraries are needed. Each library
    shown above contains a range of methods, functions, and outputs that have been
    developed to aid data analysis. For user knowledge both sets of libraries have
    been built on top of each other i.e., one is taken as the base and used to produce
    additional outputs. Pandas is built on top of NumPy to produce data analysis.
    With Seaborn built on top of matplotlib to aid data visualization.
  prefs: []
  type: TYPE_NORMAL
- en: A common initial step to begin working with Pandas for data analysis is to import
    a CSV file. The code shown below references the folder containing the file for
    review.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Pre-processing data for modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before beginning to produce classification models we need to understand and
    pre-process the data. As mentioned earlier the EDA (Exploratory Data Analysis)
    was assessed in a previous article. Using the knowledge gained a pre-processing
    data pipeline can be produced. Creating a data pipeline has a range of benefits.
    Firstly, the steps that have been tested on sample data are used to automate processing
    for future iterations. Secondly, it allows other users to quickly begin working
    with a cleaner version of the dataset and avoid having to review the same initial
    steps. Lastly, the data pipeline can be copied or forked by other users and new
    additions can be added without impacting the initial pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: With ML modelling the main aim of analysis is to have a robust data pipeline
    that contains data pre-processing to allow other users to test different ML model
    algorithms. A great book called Effective Pandas by Matt Harrison shows how a
    chaining method can aid code legibility. We will show two methods that aim to
    process similar tasks. Readers are welcome to follow either approach.
  prefs: []
  type: TYPE_NORMAL
- en: A common data pre-processing step is to review missing data values. How these
    ultimately impact the model is unknown until the testing phase. It is advised
    to retain the raw data variable and develop a new variable to allow for later
    comparison. For the steps below we will leave this step out and aim to update
    the raw data variable.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly we need to review the volume of missing values associated with each
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/60e295c97e554da4fcd4b57fb16f2ded.png)'
  prefs: []
  type: TYPE_IMG
- en: Output 1.1 Missing value per column within DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: The output highlights three variables with missing values. However, there is
    a wide variation in the total missing. Sulfate shows the highest proportion with
    lower values for the other variables. When larger proportions of missing values
    are present, we must be careful when applying adjustments. By applying methods
    that remove the underlying characteristics of the missing values, final results
    could provide estimates that do not align with expectations. Having expert knowledge
    within the dataset domain can help to understand different options.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method #1'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Application of a mean value from all non-missing values provides a good first
    approximation. Multiple variables required updating and Key Word (KW) parameters
    within the method fillna cater for in-line updates. Including the KW inplace will
    apply the method to the input dataframe df without requiring a copy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method #2'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The second method seeks to use the chaining method to perform variable updates.
    A mean value adjustment is still being applied. Using the assign method with a
    single-line lambda function allows for greater readability. Another important
    aspect is that previous lines can be (un)commented out. If a review of pre- and
    post-processing was required it would be a simple step to uncomment and comment
    lines within the code. The output below highlights that all missing values have
    been updated.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c579c1925c8494ee6bb3e0e5ff5207a.png)'
  prefs: []
  type: TYPE_IMG
- en: Output 1.2 Post-processing update made to resolve missing values shows zero
    null values
  prefs: []
  type: TYPE_NORMAL
- en: With pre-processing complete we are now able to split the dataframe into dependent
    (target or y) and independent (X) variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Python allows for multiple variables to be produced on the left-hand side of
    the formula within the same line of code. Adding a comma between the variables
    on each side of the formula, Python interprets that two new variables are being
    created.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d74211e4240dcfc9b1ca8f31ea454698.png)'
  prefs: []
  type: TYPE_IMG
- en: Output 1.3 Top 5 rows from the DataFrame show only independent variables
  prefs: []
  type: TYPE_NORMAL
- en: The top 5 rows have been shown with the head method. A numpy array variable
    contains the y binary values.
  prefs: []
  type: TYPE_NORMAL
- en: Classification model — Baseline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common Python library used to develop ML models is scikit-learn. Within the
    library repository a wide range of techniques aid with model development. Many
    years of development have resulted in a mature library that continues to progress.
  prefs: []
  type: TYPE_NORMAL
- en: When building a classification model many users dive straight into development
    with the latest ML techniques. However, a better approach is to first develop
    a baseline model. It can act as a point of reference with any model estimates
    below this baseline showing less effective techniques. A good first approximation
    can be produced before attempting to make adjustments to the model hyper-parameters.
    With hyper-parameters being KW variables that can be adjusted to improve the ML
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit learn contains a dummy classifier algorithm that can provide a baseline
    model. With the model output, it can be compared against more complex classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The model steps above create a model classifier that can then be fit to the
    input data. A prediction of the target (y) is produced using the predict method.
    Finally, scoring will show the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: As the dummy classifier applied the most frequent value we are effectively predicting
    that the target value is 0 for each. It should be noted that applying this method
    can provide good context for future predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To validate the output of the predicted value of 0.60989 we can perform a value_counts
    of the target variable. The output below displays that the same percentage is
    shown as the scored prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67f000f732fd5950b934d2272f8b03ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Output 1.4 Display the percentage portion of the binary target variable
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, should any future classification model result in a lower estimated
    score we should discount this model as not producing better results.
  prefs: []
  type: TYPE_NORMAL
- en: Classification model — Complex approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let's attempt to produce a more complex model to understand the ML challenge.
    It is with Gradient Boosted Models (GBM) that we will look for improved performance.
    A GBM is a tree-based model that allows for the development of multiple trees.
    With each tree, the input data is assessed to understand how model features predict
    the target variable. For this exercise, we will use a light GBM classifier. Alternatives
    such as XGBoost, which stands for Extreme Gradient Boosting, can be used in future
    developments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For the ML model above common library imports are shown. Each section shows
    the relevant steps used during model build and testing. Pre-processing aims to
    ensure that a pipeline of steps can be constructed to aid future developments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Firstly, we need to split the input data into training and testing samples.
    Having this split reduces the chance of overfitting the data. The goal is to create
    a model with the best performance on unseen data i.e., how models are used in
    the wild on real-life data. Using the test data to review how the trained model
    performs with unseen values aims to showcase areas for improvement. Including
    the keyword parameter stratify ensures that the target variable distribution is
    aligned across train and test data. Applying this step aims to ensure that the
    characteristics of the underlying variable distribution are not lost. Model predictions
    should align with what is observed within the data.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the classifier to the variable lgbm allows the user to work with all
    of the methods (functions) and attributes (data) of the Python object. Standard
    steps to train the model and score on the test data are followed.
  prefs: []
  type: TYPE_NORMAL
- en: The results displayed below highlight how the model performance has increased
    relative to the baseline model. Applying to the test data provides comfort that
    predictions are working well. Accuracy has been produced with the scoring method.
    It displays the total number of correct predictions over all possible outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Using the classification report highlights the classification metrics of most
    interest. Precision shows how well the True Positive values have been predicted
    relative to all positive predictions (True Positive and False Positive). Having
    too many False Positive values results in a Type 1 error i.e., misclassifying
    an instance as positive, such as medical screening produces misdiagnosis. Recall
    assesses the True Positive values relative to all positive actuals (True Positive
    and False Negative). With too many False Negative values it produces a Type II
    error i.e., misclassifying an instance as negative, such as Fraud detection can
    result in financial loss.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f378eb1978d792b5de19e3fec6e8bb7c.png)'
  prefs: []
  type: TYPE_IMG
- en: Output 1.5 Provides details on accuracy and classification metrics
  prefs: []
  type: TYPE_NORMAL
- en: When working with ML algorithms a set of default key parameter values are included
    to produce baseline results. It is the optimization of these initial parameters
    that will produce better model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Using the method get_params for the lgbm variable will display the output shown
    below. For further details on what each variable means users can review the documentation
    online.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ae64e42c2e7bd519b725628f6e9439e.png)'
  prefs: []
  type: TYPE_IMG
- en: Output 1.6 Default keyword parameter values
  prefs: []
  type: TYPE_NORMAL
- en: The next important step in ML model development is to review the hyperparameter
    space of potential values. By performing hyperparameter tuning across a relevant
    space of options it is possible to efficiently produce improved predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Introducing a pipeline that scales numeric variables to align to similar scales
    will reduce the potential for variables with larger numeric ranges to dominate.
    Two steps have been included within the pipeline to produce scaled independent
    variables and then apply the LGBM classifier. By producing code in this format
    it aids other users understanding of pre-processing steps. A pipeline can include
    many more steps depending on the complexity of the steps required.
  prefs: []
  type: TYPE_NORMAL
- en: A parameter dictionary has been produced to allow for a mixture of hyperparameter
    inputs to be tested. Including the variable referencing the lgbm model with a
    double underscore will let Python recognize that the lgbm variable is to be adjusted.
  prefs: []
  type: TYPE_NORMAL
- en: The gridsearchCV method will review each of the input parameters in combination
    to produce models for all combinations. By including a CV (cross-validation) parameter,
    it will perform three cross-validation procedures. Each validation run will select
    a different sample to train the model. The aim is to ensure that a model does
    not overfit a unique aspect shown within only one sample of the input independent
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Once processing has been completed, the best score and hyperparameters from
    the lgbm can be reviewed. As we have only reviewed a small number of potential
    hyperparameters, users could have identified this best model using a more brute-force
    manual approach. However, the real benefit of the gridsearch would be the inclusion
    of a much larger hyperparameter input space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a86efd27e2fed31fc100b636a7a6163e.png)'
  prefs: []
  type: TYPE_IMG
- en: Output 1.7 Results from the hyperparameter tuning of the LGBM classifier
  prefs: []
  type: TYPE_NORMAL
- en: After selecting the best parameters for the lgbm we can see an improvement in
    the model accuracy. It is this parameter selection that could be applied when
    new data is available to make future predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Further steps to improve model performance could include a review of the independent
    variables relationships via correlation analysis. Also assessing if variables
    require more refined pre-processing of missing values could be assessed.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Within this article, we have reviewed how the inclusion of a baseline ML model
    can help when assessing how well models are making predictions. Using a model
    accuracy metric will determine if alternative approaches have provided improvements.
    Instead of making a blind assessment of the model performance, there is a data-driven
    approach in place. We also reviewed how pipeline steps and hyperparameter tuning
    can aid with ML model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks very much for reading! If you have any comments I would appreciate these
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**You can reach out to me on** [**LinkedIn**](https://www.linkedin.com/in/james-mc-neill-180a9057/)
    **for a friendly chat about all things data. Other stories that I have shared:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/python-water-quality-eda-and-potability-analysis-ebc1cf553081?source=post_page-----bb5584226a82--------------------------------)
    [## Python water quality EDA and Potability analysis'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data analysis, visualization techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/python-water-quality-eda-and-potability-analysis-ebc1cf553081?source=post_page-----bb5584226a82--------------------------------)
    [](/getting-started-with-nlp-in-python-6a14d0bf4cfe?source=post_page-----bb5584226a82--------------------------------)
    [## Getting started with NLP in Python
  prefs: []
  type: TYPE_NORMAL
- en: Beginning a journey into the Natural Language Processing space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/getting-started-with-nlp-in-python-6a14d0bf4cfe?source=post_page-----bb5584226a82--------------------------------)
    [](/deep-dive-into-sql-window-functions-bdcb29b05853?source=post_page-----bb5584226a82--------------------------------)
    [## Deep dive into SQL window functions
  prefs: []
  type: TYPE_NORMAL
- en: The SQL window function performs calculations across a set of table rows to
    streamline data analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/deep-dive-into-sql-window-functions-bdcb29b05853?source=post_page-----bb5584226a82--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[1] : Kaggle dataset water quality from [https://www.kaggle.com/datasets/adityakadiwal/water-potability](https://www.kaggle.com/datasets/adityakadiwal/water-potability),
    with a license agreement of [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)'
  prefs: []
  type: TYPE_NORMAL
