<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Solving Reasoning Problems with LLMs in 2023</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Solving Reasoning Problems with LLMs in 2023</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d?source=collection_archive---------1-----------------------#2024-01-06">https://towardsdatascience.com/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d?source=collection_archive---------1-----------------------#2024-01-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Zhaocheng Zhu" class="l ep by dd de cx" src="../Images/80d09cfe902ca99c97fd6cfd6e387c2f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*NHNjOEUlcvbEk643vvvRtw.jpeg"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------" rel="noopener follow">Zhaocheng Zhu</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">17 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx kq kr ab q ee ks kt" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="kp"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ko kp">2</span></p></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="ku k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al kv an ao ap hr kw kx ky" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep kz cn"><div class="l ae"><div class="ab cb"><div class="la lb lc ld le lf ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="5713" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">It’s the beginning of 2024 and ChatGPT just celebrated its one-year birthday. One year is a super long time for the community of large language models, where a myriad of interesting works have taken place. Let’s revisit the progress and discuss topics for the coming year.</p></div></div><div class="mv"><div class="ab cb"><div class="la mw lb mx lc my cf mz cg na ci bh"><figure class="ne nf ng nh ni mv nj nk paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc nd"><img src="../Images/d47fb72b435abfef88d39b07d7a27d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/0*BXRAIi7ZzaOfkJgt.png"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">The School of Agents: LLMs are retrieving knowledge from textbooks and performing reasoning. Image by authors &amp; DALL·E 3.</figcaption></figure></div></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f95f" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><em class="od">This post was co-authored with </em><a class="af oe" href="https://twitter.com/michael_galkin" rel="noopener ugc nofollow" target="_blank"><em class="od">Michael Galkin</em></a><em class="od"> (Intel AI Lab), </em><a class="af oe" href="https://asaparov.org/" rel="noopener ugc nofollow" target="_blank"><em class="od">Abulhair Saparov</em></a><em class="od"> (New York University), </em><a class="af oe" href="https://twitter.com/Ber18791531" rel="noopener ugc nofollow" target="_blank"><em class="od">Shibo Hao</em></a><em class="od"> (UC San Diego) and </em><a class="af oe" href="https://twitter.com/yihong_thu" rel="noopener ugc nofollow" target="_blank"><em class="od">Yihong Chen</em></a><em class="od"> (University College London &amp; Meta AI Research). Many insights in this post were formed during the fruitful discussions with </em><a class="af oe" href="https://www.linkedin.com/in/yuan-emily-xue-3483012/" rel="noopener ugc nofollow" target="_blank"><em class="od">Emily Xue</em></a><em class="od"> (Google), </em><a class="af oe" href="https://twitter.com/hanjundai" rel="noopener ugc nofollow" target="_blank"><em class="od">Hanjun Dai</em></a><em class="od"> (Google DeepMind) and </em><a class="af oe" href="https://twitter.com/brunofmr" rel="noopener ugc nofollow" target="_blank"><em class="od">Bruno Ribeiro</em></a><em class="od"> (Purdue University).</em></p></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="1319" class="of og fq bf oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa pb pc bk">Table of Contents</h1><ol class=""><li id="603b" class="lx ly fq lz b ma pd mc md me pe mg mh mi pf mk ml mm pg mo mp mq ph ms mt mu pi pj pk bk"><a class="af oe" href="#2669" rel="noopener ugc nofollow">Introduction</a></li><li id="f0b7" class="lx ly fq lz b ma pl mc md me pm mg mh mi pn mk ml mm po mo mp mq pp ms mt mu pi pj pk bk"><a class="af oe" href="#0b5d" rel="noopener ugc nofollow">Tool Use</a><br/>1. <a class="af oe" href="#1acd" rel="noopener ugc nofollow">In-context learning enables using more tools</a><br/>2. <a class="af oe" href="#89ee" rel="noopener ugc nofollow">Most used tools: code interpreters and retrievers</a><br/>3. <a class="af oe" href="#9108" rel="noopener ugc nofollow">Let LLMs create their own tools</a></li><li id="fa0c" class="lx ly fq lz b ma pl mc md me pm mg mh mi pn mk ml mm po mo mp mq pp ms mt mu pi pj pk bk"><a class="af oe" href="#d8e6" rel="noopener ugc nofollow">Reasoning</a><br/>1. <a class="af oe" href="#d2e7" rel="noopener ugc nofollow">Planning</a><br/>2. <a class="af oe" href="#3582" rel="noopener ugc nofollow">Self series</a><br/>3. <a class="af oe" href="#0f22" rel="noopener ugc nofollow">Evaluations and observations</a></li><li id="de5e" class="lx ly fq lz b ma pl mc md me pm mg mh mi pn mk ml mm po mo mp mq pp ms mt mu pi pj pk bk"><a class="af oe" href="#7e91" rel="noopener ugc nofollow">What needs to be solved in 2024?</a></li></ol></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2669" class="of og fq bf oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa pb pc bk">Introduction</h1><p id="d48a" class="pw-post-body-paragraph lx ly fq lz b ma pd mc md me pe mg mh mi pf mk ml mm pg mo mp mq ph ms mt mu fj bk">🔥 Large language models (LLMs) must be the hottest topic in 2023. At the NeurIPS conference last month, the recurring topics in social events were: 1) what research are we doing with/for LLMs? 2) how can my research be integrated with LLMs? 3) what is the best strategy to shift from XXX to LLMs? 4) what research can we do as a <a class="af oe" href="https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini" rel="noopener ugc nofollow" target="_blank">GPU-poor</a> group? The reason is that everyone was informed about the groundbreaking news of LLMs on X, Discord, Slack, and everywhere else.</p><p id="ef30" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">If you take a look at the language model papers on arXiv, there is a leap from 2,837 to 11,033 in 2023, which breaks the linear trend from 2019 to 2022. Papers in the past year can be roughly clustered into 3 major categories: 1️⃣ pretraining and alignment; 2️⃣ tool use and reasoning; 3️⃣ systems and serving. As the title indicates, <strong class="lz fr">this post will focus on the progress of LLM research on tool use and reasoning.</strong> We picked ~20 👀 mind-blowing 👀 papers and summarized their insights and implications. By no means can this post be a comprehensive summary of all the achievements made by the community. Feel free to comment on any topics we missed.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc pq"><img src="../Images/2b76e17e1ac1e676a1b1776c4af794e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RKcOi94i9TUZ52Jl.png"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">Plot made by authors &amp; ChatGPT.</figcaption></figure><p id="849e" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">This post is composed of two topics: <strong class="lz fr">tool use</strong> and <strong class="lz fr">reasoning</strong>.</p><ul class=""><li id="d788" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu pr pj pk bk">Tool use is more about how to solve reasoning problems by equipping LLMs with <strong class="lz fr">external</strong> tools, such as retrievers, search engines, and code interpreters. While tool use is not essential for building strong AI (see Yann’s classification below), tool use provides a practical solution to many applications when domain-specific tools are easily accessible.</li><li id="316e" class="lx ly fq lz b ma pl mc md me pm mg mh mi pn mk ml mm po mo mp mq pp ms mt mu pr pj pk bk">By contrast, reasoning focuses on solving complex problems with the <strong class="lz fr">internal</strong> reasoning capacities of LLMs. Research on reasoning tries to figure out the limit of the capabilities that LLMs possess and approaches to push that limit.</li></ul><p id="133d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">There isn’t a strict dichotomy between the two topics, as we will see in the rest of this post.</p><figure class="ne nf ng nh ni mv"><div class="ps ic l ed"><div class="pt pu l"/></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">Yann LeCun’s classification of retrieval and reasoning.</figcaption></figure></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0b5d" class="of og fq bf oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa pb pc bk">Tool Use</h1><h2 id="1acd" class="pv og fq bf oh pw px py ol pz qa qb op mi qc qd qe mm qf qg qh mq qi qj qk ql bk">In-context learning enables using more tools</h2><p id="27ad" class="pw-post-body-paragraph lx ly fq lz b ma pd mc md me pe mg mh mi pf mk ml mm pg mo mp mq ph ms mt mu fj bk"><strong class="lz fr">➡️</strong> One limitation of LLM tool usage is the necessity of sufficient human annotations. Whenever we want to teach an LLM to use a tool, we need enough annotated tool calls to finetune the LLM. In the <a class="af oe" href="https://arxiv.org/abs/2302.04761" rel="noopener ugc nofollow" target="_blank">Toolformer</a> paper by Meta, the authors use in-context learning to create a model that annotates tool calls for the input query. This model is then used to generate tool calls on an unllabeled dataset. While the generations may be far from perfect, incorrect calls can be filtered by executing the tools and filtering the outputs based on the ground truth answer. The correct calls are collected and used to finetune the model. In this way, we can teach Transformers to use any tool based on a conventional dataset and merely 5 additional annotations — easy work for any engineer.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/2413879f08a9fefbd39a955d6044af62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cwBjyuXrgkKxDZv-"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">Automatic annotation of tool calls. Source: <a class="af oe" href="https://arxiv.org/abs/2302.04761" rel="noopener ugc nofollow" target="_blank">Schick et al.</a></figcaption></figure></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="42f6" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ <a class="af oe" href="https://arxiv.org/abs/2304.09842" rel="noopener ugc nofollow" target="_blank">Lu et al.</a> proposed <strong class="lz fr">Chameleon </strong>🦎 to compose tools for multi-step reasoning. The core idea is to use an LLM to decompose the question into a sequence of tools, and then generate the arguments for each tool call. Both steps are implemented by few-shot prompts. Such an idea is reminiscent of <a class="af oe" href="https://arxiv.org/abs/1511.02799" rel="noopener ugc nofollow" target="_blank">Neural Module Networks (NMNs)</a> from 2016, which decompose a question into sub tasks and learn a module for each sub task. The main obstacle of NMNs is that they are hard to train without annotations of the decompositions (see <a class="af oe" href="https://arxiv.org/abs/1811.12889" rel="noopener ugc nofollow" target="_blank">this study</a>). Fortunately, this is not a problem in pretrained LLMs. With in-context learning, Chameleon can generate different compositions of tool calls to solve a problem. <a class="af oe" href="https://arxiv.org/abs/2211.11559" rel="noopener ugc nofollow" target="_blank">A similar idea on visual reasoning</a> got the best paper award in CVPR this year.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/be14f7112ad6ccee215eb810ecf2b688.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Taxw0D34_Qj3QzLk"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">Chameleon for multi-step tool use. Source: <a class="af oe" href="https://arxiv.org/abs/2304.09842" rel="noopener ugc nofollow" target="_blank">Lu et al.</a></figcaption></figure><p id="984d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ While in-context learning is highly efficient compared to traditional methods, it faces certain limitations, such as difficulty in managing a large array of tools. Addressing this, <a class="af oe" href="https://arxiv.org/abs/2305.11554" rel="noopener ugc nofollow" target="_blank">Hao et al.</a> introduced <strong class="lz fr">ToolkenGPT</strong>, which augments a frozen LLM with new token embeddings specifically for tools, termed “toolkens”. This technique was originally used in <a class="af oe" href="https://arxiv.org/abs/1910.11856" rel="noopener ugc nofollow" target="_blank">multilingual language models</a> to accomodate a new language. ToolkenGPT allows tool calling during inference in the same way as next word token prediction. It demonstrates the capacity to handle over 200 tools while being cost-efficient, establishing a new effectiveness-efficiency trade-off compared to LoRA finetuning. Similar ideas are also integrated into multi-modal LLMs for <a class="af oe" href="https://arxiv.org/abs/2307.15818" rel="noopener ugc nofollow" target="_blank">robotic actions</a> and <a class="af oe" href="https://arxiv.org/abs/2309.11499" rel="noopener ugc nofollow" target="_blank">image generation</a>.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/52ecf8ab7d5d6708483fb644e9601483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4YxGGE9tOp59HHEj"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">ToolkenGPT for massive tool use. Source: <a class="af oe" href="https://arxiv.org/abs/2305.11554" rel="noopener ugc nofollow" target="_blank">Hao et al.</a></figcaption></figure><h2 id="89ee" class="pv og fq bf oh pw px py ol pz qa qb op mi qc qd qe mm qf qg qh mq qi qj qk ql bk">Most used tools: code interpreters and retrievers</h2><p id="7557" class="pw-post-body-paragraph lx ly fq lz b ma pd mc md me pe mg mh mi pf mk ml mm pg mo mp mq ph ms mt mu fj bk">If you ask us which tools are most generally applicable to reasoning tasks, we would say they are <strong class="lz fr">code interpreters</strong> and <strong class="lz fr">retrievers</strong>. Code interpreters are probably the most expressive environment that humans have invented for logic and computation. Retrievers are a good complement to the parametric knowledge of LLMs when a question or assumed knowledge is out of their training distribution. Let’s see how these tools can be used by LLMs.</p><p id="80fb" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ One common failure of <a class="af oe" href="https://arxiv.org/abs/2201.11903" rel="noopener ugc nofollow" target="_blank">chain-of-thought (CoT)</a> is that LLMs fail to perform arithmetic operations. In <a class="af oe" href="https://arxiv.org/abs/2211.10435" rel="noopener ugc nofollow" target="_blank">program-aided language modeling (PAL)</a> and <a class="af oe" href="https://arxiv.org/abs/2211.12588" rel="noopener ugc nofollow" target="_blank">program-of-thoughts (PoT)</a> prompting, the authors prompt a code language model with programs to solve math problems. One may insert standard chain-of-thought texts as comments in the programs. The final answer is then generated by executing a Python interpreter. The insight behind these methods is that the code interpreter provides a perfect tool for all kinds of calculation, reducing failure cases to only incorrect reasoning. Code-style prompts are also commonly used in <a class="af oe" href="https://arxiv.org/abs/2305.16653" rel="noopener ugc nofollow" target="_blank">planning tasks</a>.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/dd4f4150c2e2a642854d8791b55c4a09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*c6SERFQWvrPVNc9_"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">Comparison between CoT and PAL. Source: <a class="af oe" href="https://arxiv.org/abs/2211.10435" rel="noopener ugc nofollow" target="_blank">Gao and Madaan et al.</a></figcaption></figure><p id="2dc7" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ Retrievers are commonly used as preprocessing tools for LLMs to augment the question with relevant documents, often referred to as <a class="af oe" href="https://arxiv.org/abs/2005.11401" rel="noopener ugc nofollow" target="_blank">retrieval-augmented generation (RAG)</a>. However, when it comes to multi-step question answering, it is challenging to select the correct documents based on the question alone. In the <strong class="lz fr">IRCoT</strong> proposed by <a class="af oe" href="https://arxiv.org/abs/2212.10509" rel="noopener ugc nofollow" target="_blank">Trivedi et al.</a>, the authors interleave thought generation and knowledge retrieval. Whenever the LLM generates a thought sentence, IRCoT uses the sentence to retrieve documents from the corpus. The documents are prepended to the prompt to augment later generations. Even with a weak retriever like <a class="af oe" href="https://en.wikipedia.org/wiki/Okapi_BM25" rel="noopener ugc nofollow" target="_blank">BM25</a>, IRCoT outperforms one-step RAG on several open-domain question answering benchmarks.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/0e047cefb88b0636fb931ccbe6ca4277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CtGKRSReemUCV6yr"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">IRCoT that interleaves CoT and knowledge retrieval. Source: <a class="af oe" href="https://arxiv.org/abs/2212.10509" rel="noopener ugc nofollow" target="_blank">Trivedi et al.</a></figcaption></figure><p id="8ca5" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ <a class="af oe" href="https://arxiv.org/abs/2306.15626" rel="noopener ugc nofollow" target="_blank">Yang et al.</a> presented a novel usage of RAG for theorem proving. They built a gym-like environment <strong class="lz fr">LeanDojo </strong>🏯 based on the proof assistant Lean. <a class="af oe" href="https://en.wikipedia.org/wiki/Lean_(proof_assistant)" rel="noopener ugc nofollow" target="_blank">Lean</a> is an interactive programming environment where its compiler can verify whether the written proof proved the goal. It also includes numerous proved theorems in its standard libraries, similar to STL in C++. The cool thing is that because proofs are constructed by decomposing theorems into known premises, theorem proving can benefit from RAG. Given a theorem, we retrieve the relevant premises from the standard libraries and then ask an LLM to generate a proof step. The authors show that RAG requires much less training resources and generalizes better to novel premises.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/c56af9f2fb5c435c163c807a4fa4ffd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tSw-prYgSQFC9v3X"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">Proof of a simple logical theorem in Lean. Source: <a class="af oe" href="https://www.youtube.com/watch?v=POHVMMG7pqE" rel="noopener ugc nofollow" target="_blank">Xena Project</a></figcaption></figure><p id="bed3" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ Finally, <a class="af oe" href="https://github.com/stanfordnlp/dspy" rel="noopener ugc nofollow" target="_blank">DSPy</a> by <a class="af oe" href="https://arxiv.org/abs/2310.03714" rel="noopener ugc nofollow" target="_blank">Khattab et al.</a> presents a novel approach towards programming LLMs where the framework can actually improve the prompts over time and combine prompting techniques (CoT, PoT) with retrieval automatically. Further, DSPy introduces <em class="od">teleprompters</em> for optimizing the prompts and bootstrapping new ones. It’s hard to fit a description of DSPy in one paragraph — it’s not your average RAG technique, but rather an evolution of it.</p><h2 id="9108" class="pv og fq bf oh pw px py ol pz qa qb op mi qc qd qe mm qf qg qh mq qi qj qk ql bk">Let LLMs create their own tools</h2><p id="5bdd" class="pw-post-body-paragraph lx ly fq lz b ma pd mc md me pe mg mh mi pf mk ml mm pg mo mp mq ph ms mt mu fj bk">Tool use has an inherent limitation: it relies on the presence of tools for a specific task. In nature, tool use is not an exclusive skill of humans, as many other animals can also use tools. However, what distinguishes humans from other animals is the ability to <strong class="lz fr">create</strong> tools. In 2023, we’ve seen a few preliminary works exploring the ability of tool making in LLMs.</p><p id="fbf3" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ In LLMs as tool makers (<strong class="lz fr">LATM</strong>) proposed by <a class="af oe" href="https://arxiv.org/abs/2305.17126" rel="noopener ugc nofollow" target="_blank">Cai et al.</a>, the authors prompt an LLM to craft tools in the form of Python functions for a given task. The tools are then verified on a few samples, similar to how engineers solve problems on LeetCode. Once some tools pass the verification test, they are wrapped with documentation strings generated by an LLM to describe their usage. At test time, an LLM is prompted to dispatch the question to one of the tools at hand, and execute the tool based on the usage. LATM significantly outperforms CoT on a wide range of reasoning tasks in BigBench.</p><p id="ff7a" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ <a class="af oe" href="https://arxiv.org/abs/2305.16291" rel="noopener ugc nofollow" target="_blank">Voyager</a> brought the idea of tool making to the world of Minecraft, and came up with incredible results. The core idea of Voyager is to use an LLM to propose tasks based on existing skills and world state. Then the LLM is prompted to synthesize code (i.e. skills) to solve the tasks. The skills are refined based on environment feedback and mastered skills are committed to an external memory. Because new skills are built on top of existing skills, this significantly reduced the difficulty of learning a complex skill (e.g. building a diamond tool in Minecraft). While the idea of learning a library of skills can be traced back to <a class="af oe" href="https://arxiv.org/abs/2006.08381" rel="noopener ugc nofollow" target="_blank">DreamCoder</a>, Voyager demonstrates the superiority of GPT-4 in searching over skills in a challenging open-world game. Take a look at <a class="af oe" href="https://voyager.minedojo.org/" rel="noopener ugc nofollow" target="_blank">the fancy demos</a> from the paper!</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/3dc3a1462663cecf5c056fab3a88b4d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dC0K4Qh7zUfnxWkt"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">Minecraft items and skills discovered over time. Source: <a class="af oe" href="https://arxiv.org/abs/2305.16291" rel="noopener ugc nofollow" target="_blank">Wang et al.</a></figcaption></figure><p id="9b60" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ Both of the above works craft tools as code. In fact, tools can be in natural language, too. (shameless self-promotion) In the hypothesis-to-theories (<strong class="lz fr">HtT</strong>) work from <a class="af oe" href="https://arxiv.org/abs/2310.07064" rel="noopener ugc nofollow" target="_blank">Zhu et al.</a>, the authors show that we can use LLMs to induce a library of textual rules from a standard multi-step reasoning training set. The insight is that among all the rules that LLMs produce for different samples, rules that occur and lead to correct answers more often are likely to be correct. We then collect the rules and prepend them to a standard CoT prompt to perform deduction and get the answer. One interesting aspect about HtT is that it can be viewed as a novel way of learning: instead of learning model parameters, we learn a library of rules, which works well with black-box LLMs.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/e30b407b2550861646906f5e0ad43916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yAXSJVamv5pTT7WG"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">HtT that learns textual rules for multi-hop reasoning. Source: <a class="af oe" href="https://arxiv.org/abs/2310.07064" rel="noopener ugc nofollow" target="_blank">Zhu et al.</a></figcaption></figure></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="d8e6" class="of og fq bf oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa pb pc bk">Reasoning</h1><h2 id="d2e7" class="pv og fq bf oh pw px py ol pz qa qb op mi qc qd qe mm qf qg qh mq qi qj qk ql bk">Planning</h2><p id="c252" class="pw-post-body-paragraph lx ly fq lz b ma pd mc md me pe mg mh mi pf mk ml mm pg mo mp mq ph ms mt mu fj bk">One drawback for CoT-style reasoning is that LLMs have to greedily decode a path towards an answer. This is problematic for complex problems like math questions or games, since it is hard to predict a path without trial-and-error. In 2023, the community made some progress on this issue with new frameworks that enable planning with LLMs.</p><p id="0422" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ If we conceptualize CoT as “system 1” reasoning — characterized by its automatic, unconscious nature — then a question arises: Is it feasible to replicate the more conscious “system 2” reasoning of humans using LLMs? This query finds relevance in two methodologies: <a class="af oe" href="https://arxiv.org/abs/2305.14992" rel="noopener ugc nofollow" target="_blank">reasoning-via-planning (RAP)</a> and <a class="af oe" href="https://arxiv.org/abs/2305.10601" rel="noopener ugc nofollow" target="_blank">tree-of-thoughts (ToT)</a>. Both empower LLMs to navigate through possible reasoning steps, and to search for the optimal reasoning chain based on specific evaluations. RAP additionally prompts an LLM as a “world model”, which predicts the next states following actions. This enables the LLM to operate within a self-simulated world, as opposed to interacting with an external environment. Both algorithms are available in the <a class="af oe" href="https://github.com/Ber666/llm-reasoners/" rel="noopener ugc nofollow" target="_blank">LLM Reasoners</a> library now!</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/76d337122e2af90fdd867f1c7ab0e5a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Oi7S43E2LGdru1q5"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">RAP that repurposes LLMs as an agent and a world model. Source: <a class="af oe" href="https://arxiv.org/abs/2305.14992" rel="noopener ugc nofollow" target="_blank">Hao et al.</a></figcaption></figure><h2 id="3582" class="pv og fq bf oh pw px py ol pz qa qb op mi qc qd qe mm qf qg qh mq qi qj qk ql bk">Self series</h2><p id="af03" class="pw-post-body-paragraph lx ly fq lz b ma pd mc md me pe mg mh mi pf mk ml mm pg mo mp mq ph ms mt mu fj bk">Self series are a family of techniques that replace human efforts with LLM predictions in the loop of LLM development. The year of 2023 has witnessed quite a few papers on this track. Let’s take a closer look at some representative works.</p><p id="5122" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ Many people have the experience that ChatGPT doesn’t provide the desired output on the first trial, and this sometimes can be fixed by pointing out its mistake. <a class="af oe" href="https://arxiv.org/abs/2304.05128" rel="noopener ugc nofollow" target="_blank">Self-debugging</a> and <a class="af oe" href="https://arxiv.org/abs/2303.17651" rel="noopener ugc nofollow" target="_blank">self-refinement</a> automate this procedure by replacing human feedback with machine feedback. The feedback either comes from a program executor or an LLM that compares the generation with the explanation of the problem. One key observation is that the performance of self-refine depends on the quality of the feedback, where stronger base models that provide better feedback benefit more. Such iterative refinement methods have also been shown to be super effective in <a class="af oe" href="https://arxiv.org/abs/1507.06550" rel="noopener ugc nofollow" target="_blank">pose estimation</a> and <a class="af oe" href="https://www.nature.com/articles/s41586-021-03819-2" rel="noopener ugc nofollow" target="_blank">protein structure prediction</a>, where it is difficult to predict the structure in a single run.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/2b3c4d3580bf9a786395a972b481534c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xR4epS27JO9JmUo2"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">Illustration of Self-Debugging. Source: <a class="af oe" href="https://arxiv.org/abs/2304.05128" rel="noopener ugc nofollow" target="_blank">Chen et al.</a></figcaption></figure><p id="93fa" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ In the memory-of-thought (<strong class="lz fr">MoT</strong>) framework from <a class="af oe" href="https://arxiv.org/abs/2305.05181" rel="noopener ugc nofollow" target="_blank">Li and Qiu</a>, the authors ask an LLM to generate CoT rationales on an unlabelled dataset and use them for RAG. You may ask how this can be useful given that the generated rationales often contain errors. The key trick is to filter the rationales based on majority vote or entropy minimization (a similar idea is used in <a class="af oe" href="https://arxiv.org/abs/2305.14106" rel="noopener ugc nofollow" target="_blank">Wan et al.</a> to filter rationales). Once we have good rationales on the unlabelled dataset, we dynamically retrieve few-shot examples based on the test question, which is shown to be much better than fixed few-shot examples. MoT can be interpreted as converting a parametric model to a non-parametric model without additional supervision.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/07dcece1803feb415c68d2727bc2c037.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Y2rCEaB4mXWLUO5-"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">MoT that generates and recalls memory. Source: <a class="af oe" href="https://arxiv.org/abs/2305.05181" rel="noopener ugc nofollow" target="_blank">Li and Qiu</a>.</figcaption></figure><p id="6c8c" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ Going beyond MoT, <a class="af oe" href="https://arxiv.org/abs/2310.01714" rel="noopener ugc nofollow" target="_blank">Yasunaga et al.</a> proposed <strong class="lz fr">analogical prompting</strong> that eliminates the need of dumping rationales on an unlabeled dataset. Analogical prompting asks an LLM to recall relevant exemplars based on the question, and thereby generates dynamic few-shot exemplars from scratch. In fact, the authors found that analogical prompting is an emergent ability in large language models, similar to previous works on <a class="af oe" href="https://arxiv.org/abs/2209.10063" rel="noopener ugc nofollow" target="_blank">open-domain question answering</a>. Larger-scale LLMs can self-generate better exemplars compared to standard RAG solutions. Besides, this work provides a cool trick to fuse multi-step generations into a single prompt with markdown grammar — a godsend for prompt engineers with a tight budget! 💡</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/cff6c1216d04791bb9d00343530b5700.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aHLEcmTr578RRM8s"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">Analogical prompting. Source: <a class="af oe" href="https://arxiv.org/abs/2310.01714" rel="noopener ugc nofollow" target="_blank">Yasunaga et al.</a></figcaption></figure><p id="c168" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ Are self-refine and self-generate the limit of LLM reasoning? <a class="af oe" href="https://arxiv.org/abs/2309.03409" rel="noopener ugc nofollow" target="_blank">Yang et al.</a> show a more advanced usage of the reasoning abilities of LLMs — to optimize a prompt based on the history of generated prompts. This is a cool reinvention of the famous meta-learning paper “<a class="af oe" href="https://arxiv.org/abs/1606.04474" rel="noopener ugc nofollow" target="_blank">Learning to learn by gradient descent by gradient descent</a>”, but all the steps here are performed by LLMs on text. At each step, an LLM is prompted with previous solutions and corresponding performance metrics and tries to predict a new solution. Notably, even without telling the LLM how to perform optimization, the LLM can gradually find better solutions that maximize the metric. Maybe this work brings prompt engineers one step closer to unemployment?</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/2c7ff40222823804440ab08816082b7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TWNnOzJubWvDb5ja"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">Performance of prompts optimized by LLM. Source: <a class="af oe" href="https://arxiv.org/abs/2309.03409" rel="noopener ugc nofollow" target="_blank">Yang et al.</a></figcaption></figure><p id="b813" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">🔁 Probably the most eye-opening 👀 work in self series is the self-taught optimizer (<strong class="lz fr">STOP</strong>) by <a class="af oe" href="https://arxiv.org/abs/2310.02304" rel="noopener ugc nofollow" target="_blank">Zelikman et al.</a> We know LLMs are guided by textual prompts, take texts as input and output texts. While these these texts are usually separate variables, what will happen if we model them as a single variable? In STOP, the authors draw inspiration from <a class="af oe" href="https://en.wikipedia.org/wiki/Self-modifying_code" rel="noopener ugc nofollow" target="_blank">self-modifying code</a> and use a self-improvement prompt to improve itself.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/3da91a00d4241277814ce2cfc36e8dda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nfn9ObNRopSJ_xxY"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">The seed improver that improves itself in STOP. Source: <a class="af oe" href="https://arxiv.org/abs/2310.02304" rel="noopener ugc nofollow" target="_blank">Zelikman et al.</a></figcaption></figure><p id="20ec" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">While the seed prompt isn’t more complicated than a random search algorithm, with a strong LLM, one can discover many advanced meta-heuristic algorithms. Interestingly, GPT-4 discovers many prompting strategies that are published after the training cutoff for GPT-4, including <a class="af oe" href="https://arxiv.org/abs/2305.10601" rel="noopener ugc nofollow" target="_blank">ToT</a> and <a class="af oe" href="https://arxiv.org/abs/2212.10561" rel="noopener ugc nofollow" target="_blank">Parsel</a>. It seems that the day when LLMs conduct research for themselves is approaching. One step in this direction is a recent work by <a class="af oe" href="https://arxiv.org/abs/2310.03302" rel="noopener ugc nofollow" target="_blank">Huang et al.</a> showing that LLMs are capable of designing ML models for common benchmarks and even Kaggle challenges.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/e030d2aa8360e5a443b744b6852437af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*e_nM_6cwHRZJSod6"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">Algorithms found by STOP. Source: <a class="af oe" href="https://arxiv.org/abs/2310.02304" rel="noopener ugc nofollow" target="_blank">Zelikman et al.</a></figcaption></figure><h2 id="0f22" class="pv og fq bf oh pw px py ol pz qa qb op mi qc qd qe mm qf qg qh mq qi qj qk ql bk">Evaluations and observations</h2><p id="9d45" class="pw-post-body-paragraph lx ly fq lz b ma pd mc md me pe mg mh mi pf mk ml mm pg mo mp mq ph ms mt mu fj bk">➡️ <a class="af oe" href="https://arxiv.org/abs/2211.08411" rel="noopener ugc nofollow" target="_blank">Kandpal et al.</a> conducted a systematic study on the memorization ability of LLMs. They asked an LLM about factual questions from Wikipedia and found that the accuracy is highly correlated with the frequency of questioned entities in the pretraining documents, regardless of the scale of the model. By extrapolating the trend, the authors estimate that a model with 10¹⁸ is needed to match human performance on long-tail entities — which is way bigger than today’s LLMs. Hence an important takeaway is to use LLM reasoning for tasks related to frequent knowledge, and consider RAG or other tools for tasks related to long-tail knowledge.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/d7500b22b9573287416b537dcd301dcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*M2qvNKUdqy54N-Wc"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">LLMs can hardly memorize long-tail knowledge. Source: <a class="af oe" href="https://arxiv.org/abs/2211.08411" rel="noopener ugc nofollow" target="_blank">Kandpal et al.</a></figcaption></figure><p id="8e93" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ As the community tries to build bigger mixtures for training LLMs, one concern is that LLMs may not learn to actually reason but simply to memorize the solutions from the training distribution, just like humans in <a class="af oe" href="https://en.wikipedia.org/wiki/Teaching_to_the_test" rel="noopener ugc nofollow" target="_blank">teaching to the test</a>. <a class="af oe" href="https://arxiv.org/abs/2307.02477" rel="noopener ugc nofollow" target="_blank">Wu et al.</a> answers this concern by comparing the performance of GPT-4 with zero-shot CoT on 11 different tasks, each with a default setting and a counterfactual setting. They observe that despite LLMs performing better than random in the counterfactual settings, their performance is consistently behind that in the default settings. It remains an open question how we can train models to focus more on reasoning rather than memorization.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div role="button" tabindex="0" class="nl nm ed nn bh no"><div class="nb nc qm"><img src="../Images/03f7be28616d30d4236ececb57c2aff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eeza0EVtLuTHd9ce"/></div></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">GPT-4 underperforms on counterfactual variants. Source: <a class="af oe" href="https://arxiv.org/abs/2307.02477" rel="noopener ugc nofollow" target="_blank">Wu et al.</a></figcaption></figure><p id="8d25" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ <a class="af oe" href="https://arxiv.org/abs/2305.15269" rel="noopener ugc nofollow" target="_blank">Saparov et al.</a> extended a synthetic dataset <a class="af oe" href="https://arxiv.org/abs/2210.01240" rel="noopener ugc nofollow" target="_blank">PrOntoQA</a> to OOD setting to test generalization ability of LLMs on deductive reasoning with controlled depth, width, compositional structure, etc. The authors found that CoT can generalize to compositional and longer proofs. This is in contrast with previous conclusions on <a class="af oe" href="https://arxiv.org/abs/2205.12253" rel="noopener ugc nofollow" target="_blank">compositional semantic parsing</a>, possibly because deductive reasoning only requires composing deduction steps, while semantic parsing additionally deals with growing outputs. While LLMs are able to use most deduction rules, they require explicit demonstrations of <a class="af oe" href="https://en.wikipedia.org/wiki/Disjunction_elimination" rel="noopener ugc nofollow" target="_blank"><em class="od">proof by cases</em></a> and <a class="af oe" href="https://en.wikipedia.org/wiki/Proof_by_contradiction" rel="noopener ugc nofollow" target="_blank"><em class="od">proof by contradiction</em></a>. There are also counterintuitive qualitative differences between in-context learning and supervised learning.</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div class="ab cn cb qn"><img src="../Images/37163127677f52ac47a0d2999eb0ebd6.png" data-original-src="https://miro.medium.com/v2/format:webp/0*LhskkUizJ8uQpNY-"/></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">OOD generalization over deductive reasoning. Source: <a class="af oe" href="https://arxiv.org/abs/2305.15269" rel="noopener ugc nofollow" target="_blank">Saparov et al.</a></figcaption></figure><p id="4e54" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">➡️ Regarding the parametric knowledge in LLMs, <a class="af oe" href="https://arxiv.org/abs/2309.12288" rel="noopener ugc nofollow" target="_blank">Berglund et al.</a> found a phenomenon they called the <em class="od">reversal curse</em>. That is, LLMs trained to memorize “A is B” do not know that “B is A” in closed-book question answering, despite the fact that they can be prompted to perform deductive reasoning. This indicates that LLMs lack certain kinds of symmetry in its parametric knowledge, and it is crucial to endow them with such symmetry to enable better generalization. Actually, the knowledge graph community has been a leader in this area, with works like <a class="af oe" href="https://arxiv.org/abs/2302.01313" rel="noopener ugc nofollow" target="_blank">double permutation equivariance</a> and <a class="af oe" href="https://arxiv.org/abs/1902.10197" rel="noopener ugc nofollow" target="_blank">relational rotation</a>. It would be interesting to see how these ideas are adapted to LLMs.</p></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="7e91" class="of og fq bf oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa pb pc bk">What needs to be solved in 2024?</h1><p id="823d" class="pw-post-body-paragraph lx ly fq lz b ma pd mc md me pe mg mh mi pf mk ml mm pg mo mp mq ph ms mt mu fj bk">2023 has been an exciting year for tool use and reasoning, and we expect the new year to be more exciting. Let’s wrap up this post with predictions from the authors.</p><blockquote class="qo qp qq"><p id="db0f" class="lx ly od lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Zhaocheng Zhu:</p></blockquote><p id="19e6" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">1️⃣ Reasoning with LLMs still requires ad-hoc engineering efforts for each specific task. By contrast, once humans acquire the skills for a task, they can quickly adapt the skills to similar tasks with very few or even no samples (e.g. from chess to poker). If we can create LLM solutions that generalize across tasks, it will save a lot of engineering efforts and boost the performance in low-resource domains.</p><p id="9408" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">2️⃣ Solving reasoning problems usually involves a lot of commonsense knowledge, ranging from math, physics to strategies like enumeration and proof by contradiction, if any. While LLMs may have obtained such knowledge from their training data, we lack precise control over the parameteric knowledge in LLMs. We would like to see new studies on the knowledge representations of LLMs, and techniques that verbalize, inject or delete knowledge in LLMs.</p><blockquote class="qo qp qq"><p id="05ec" class="lx ly od lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Michael Galkin:</p></blockquote><p id="1a2e" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">1️⃣ In 2023, we saw an increasing effort to understand the basic principles of <em class="od">what can be learned</em> by Transformer-based LLMs — can we actually expect LLMs to be able to solve any arbitrary reasoning tasks? A few famous papers like <a class="af oe" href="https://arxiv.org/abs/2305.18654" rel="noopener ugc nofollow" target="_blank">faith and fate</a> and <a class="af oe" href="https://arxiv.org/abs/2310.16028" rel="noopener ugc nofollow" target="_blank">on length generalization</a> suggest that the autoregressive nature of LLMs might not be the optimal way to approach complex reasoning. In 2024, I’d expect more efforts on understanding the algorithmic alignment with LLMs.</p><p id="7deb" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">2️⃣ It is likely that in 2024, most open and closed foundation models will be multi-modal supporting vision, text, audio, and other inputs. Incorporating other modalities into reasoning is the natural next step.</p><blockquote class="qo qp qq"><p id="3435" class="lx ly od lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Abulhair Saparov:</p></blockquote><p id="7406" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">1️⃣ I anticipate there will be more efforts to find a more mechanistic understanding of reasoning in LLMs. What algorithms do they use when performing reasoning tasks? More precisely to what extent do they exploit shortcuts or heuristics that hurt robustness/generalization?</p><p id="d633" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">2️⃣ Relatedly, I would expect researchers to make progress in answering whether increasing the scale of LLMs and/or their training will resolve their limitations in reasoning, or whether these limitations are fundamental, e.g. inherent to the architecture.</p><blockquote class="qo qp qq"><p id="e1a1" class="lx ly od lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Shibo Hao:</p></blockquote><p id="ee8a" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">1️⃣ Over the past year, the primary focus of LLM reasoning research has been on prompting and supervised fine-tuning, with some approaches, like <a class="af oe" href="https://arxiv.org/abs/2203.14465" rel="noopener ugc nofollow" target="_blank">STaR</a>, <a class="af oe" href="https://arxiv.org/abs/2303.11366" rel="noopener ugc nofollow" target="_blank">Reflexion</a>, and <a class="af oe" href="https://arxiv.org/abs/2305.14992" rel="noopener ugc nofollow" target="_blank">RAP</a>, already drawing inspiration from RL. However, we have yet to witness a breakthrough method that effectively employs RL to enhance an LLM’s reasoning capabilities, especially when compared to the advancements in RLHF for alignment.</p><p id="676b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">2️⃣ On the flip side, in the future, language could become the primary medium of expression in RL systems. The key advantage lies in the rich information of language compared with the traditional scalar rewards / values. The prospect of an LLM agent that can autonomously improve its reasoning abilities with RL (no need for supervision data or prompting engineering), is not only exciting but may also indicate a significant leap towards AGI.</p><blockquote class="qo qp qq"><p id="1299" class="lx ly od lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Yihong Chen:</p></blockquote><p id="9769" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">1️⃣ Structured &amp; unstructured. I assume LLMs will be gradually eating the pies of traditional products, which are mostly based on large databases, rules and piles of small classifiers. In this case, what we are referring as “LLM reasoning” is probably the hope that we will have a method “X” that can bridge the structured world, which most product data is currently living in, and the unstructured world, which most LLMs are living in. Knowledge graphs kind of champion the structured world and there have been fruity research on how to reason well on a knowledge graph, while LLMs are championing the unstructured world though we are still unclear about how they do the reasoning. They have different advantages and limitations. I’d expect that a nice bridge between the two would lead us to more pragmatic solutions for products.</p><p id="6df2" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">2️⃣ Sample efficiency. As Zhaocheng mentioned, current LLMs reasoning is hard at generalising across a large number of tasks. Instead of ad-hoc efforts, which are usually customised for specific problems, I would be interested in if we can simply pretrain a LLM that generalize with less data, similar to what’s done for <a class="af oe" href="https://arxiv.org/abs/2307.01163" rel="noopener ugc nofollow" target="_blank">generalizing across multiple languages</a>.</p><p id="17b0" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">3️⃣ Reasoning inside LLMs. As Abulhair and Michael mentioned, the community do not have a crystal understanding about how LLMs perform reasoning, if they indeed are reasoning. I’d expect more efforts on reverse-engineer LLMs’ reasoning process, either in a <a class="af oe" href="https://www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide" rel="noopener ugc nofollow" target="_blank">mechanistic way</a> or other interpretability approaches.</p></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9d47" class="of og fq bf oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa pb pc bk">Meme Time</h1><p id="0cc9" class="pw-post-body-paragraph lx ly fq lz b ma pd mc md me pe mg mh mi pf mk ml mm pg mo mp mq ph ms mt mu fj bk">Following the tradition of <a class="af oe" href="https://mgalkin.medium.com/" rel="noopener">Michael Galkin</a>, no blog post is truely complete without a meme. DALL·E 3 is almost a meme wizard if it can spell words correctly. Guess what prompts are used for each panel?</p><figure class="ne nf ng nh ni mv nb nc paragraph-image"><div class="ab cn cb qn"><img src="../Images/1c0de490cd0427c4127e390af9d93917.png" data-original-src="https://miro.medium.com/v2/format:webp/0*UgF51S3_pKZHY7TN"/></div><figcaption class="nq nr ns nb nc nt nu bf b bg z dx">What an LLM learned and what it can reason about. Image by authors &amp; DALL·E 3.</figcaption></figure></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="db17" class="of og fq bf oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa pb pc bk">Read More</h1><p id="c7f1" class="pw-post-body-paragraph lx ly fq lz b ma pd mc md me pe mg mh mi pf mk ml mm pg mo mp mq ph ms mt mu fj bk">If this blog left you wanting to learn more about LLM reasoning, take a look at the following awesome blog posts.</p><ul class=""><li id="2040" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu pr pj pk bk"><a class="af oe" href="https://www.notion.so/c2b4a51355b44764975f88e6a42d4e75?pvs=21" rel="noopener ugc nofollow" target="_blank">Towards Complex Reasoning: the Polaris of Large Language Models</a> by Yao Fu.</li><li id="4e07" class="lx ly fq lz b ma pl mc md me pm mg mh mi pn mk ml mm po mo mp mq pp ms mt mu pr pj pk bk"><a class="af oe" href="https://lilianweng.github.io/posts/2023-06-23-agent/" rel="noopener ugc nofollow" target="_blank">LLM Powered Autonomous Agents</a> by Lilian Weng.</li></ul></div></div></div></div>    
</body>
</html>