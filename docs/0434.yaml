- en: 'Proximal Policy Optimization (PPO): The Key to LLM Alignment'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/proximal-policy-optimization-ppo-the-key-to-llm-alignment-923aa13143d4?source=collection_archive---------7-----------------------#2024-02-15](https://towardsdatascience.com/proximal-policy-optimization-ppo-the-key-to-llm-alignment-923aa13143d4?source=collection_archive---------7-----------------------#2024-02-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Modern policy gradient algorithms and their application to language models…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page---byline--923aa13143d4--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page---byline--923aa13143d4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--923aa13143d4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--923aa13143d4--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page---byline--923aa13143d4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--923aa13143d4--------------------------------)
    ·18 min read·Feb 15, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd5ca84ae9538a8e1d8a9539273ae459.png)'
  prefs: []
  type: TYPE_IMG
- en: (Photo by [Daniel Olah](https://unsplash.com/@danesduet?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/white-lines-on-track-field-2cHW5TKr9Vs?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: Recent AI research has revealed that reinforcement learning (RL) — *reinforcement
    learning from human feedback (RLHF) in particular* — is a key component of training
    large language models (LLMs). However, many AI practitioners (admittedly) avoid
    the use of RL due to several factors, including a lack of familiarity with RL
    or preference for supervised learning techniques. There are valid arguments against
    the use of RL; e.g., the curation of human preference data is expensive and RL
    can be data inefficient. However, *we should not avoid using RL simply due to
    a lack of understanding or familiarity*! These techniques are not difficult to
    grasp and, as shown by a variety of recent papers, can massively benefit LLM performance.
  prefs: []
  type: TYPE_NORMAL
- en: This overview is part three in a series that aims to demystify RL and how it
    is used to train LLMs. Although we have mostly covered fundamental ideas related
    to RL up until this point, we will now dive into the algorithm that lays the foundation
    for language model alignment — *Proximal Policy Optimization (PPO)* [2]. As we
    will see, PPO works well and is incredibly easy to understand and use, making
    it a desirable algorithm from a practical perspective. For these reasons, PPO…
  prefs: []
  type: TYPE_NORMAL
