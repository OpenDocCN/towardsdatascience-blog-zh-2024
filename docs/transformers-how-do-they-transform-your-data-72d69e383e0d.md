# Transformerï¼šå®ƒä»¬å¦‚ä½•è½¬åŒ–ä½ çš„æ•°æ®ï¼Ÿ

> åŸæ–‡ï¼š[https://towardsdatascience.com/transformers-how-do-they-transform-your-data-72d69e383e0d?source=collection_archive---------0-----------------------#2024-03-28](https://towardsdatascience.com/transformers-how-do-they-transform-your-data-72d69e383e0d?source=collection_archive---------0-----------------------#2024-03-28)

## æ·±å…¥æ¢ç´¢Transformeræ¶æ„åŠå…¶åœ¨è¯­è¨€ä»»åŠ¡ä¸­æ— æ•Œçš„åŸå› 

[](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)[![Maxime Wolf](../Images/259b3659d0e6dd1d0f0eec4ae92d02e9.png)](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------) [Maxime Wolf](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------) Â·é˜…è¯»æ—¶é—´11åˆ†é’ŸÂ·2024å¹´3æœˆ28æ—¥

--

![](../Images/4dd0f4cd464558d94b73f8274580ac94.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

åœ¨äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ é£é€Ÿå‘å±•çš„ä»Šå¤©ï¼Œæœ‰ä¸€ç§åˆ›æ–°è„±é¢–è€Œå‡ºï¼Œå¯¹æˆ‘ä»¬å¤„ç†ã€ç†è§£å’Œç”Ÿæˆæ•°æ®çš„æ–¹å¼äº§ç”Ÿäº†æ·±è¿œå½±å“ï¼š**Transformer**ã€‚Transformerå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åŠå…¶ä»–é¢†åŸŸï¼Œä¸ºä»Šå¤©ä¸€äº›æœ€å…ˆè¿›çš„AIåº”ç”¨æä¾›äº†åŠ¨åŠ›ã€‚ä½†ç©¶ç«Ÿä»€ä¹ˆæ˜¯Transformerï¼Œå®ƒä»¬åˆæ˜¯å¦‚ä½•ä»¥å¦‚æ­¤çªç ´æ€§çš„æ–¹å¼è½¬æ¢æ•°æ®çš„å‘¢ï¼Ÿæœ¬æ–‡å°†æ­ç§˜Transformeræ¨¡å‹çš„å†…éƒ¨å·¥ä½œåŸç†ï¼Œé‡ç‚¹è®²è§£**ç¼–ç å™¨æ¶æ„**ã€‚æˆ‘ä»¬å°†ä»Pythonä¸­Transformerç¼–ç å™¨çš„å®ç°å…¥æ‰‹ï¼Œé€æ­¥è§£æå…¶ä¸»è¦ç»„ä»¶ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å¯è§†åŒ–Transformerå¦‚ä½•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¤„ç†å¹¶é€‚åº”è¾“å…¥æ•°æ®ã€‚

è™½ç„¶æœ¬åšå®¢å¹¶æœªæ¶µç›–æ‰€æœ‰æ¶æ„ç»†èŠ‚ï¼Œä½†å®ƒæä¾›äº†ä¸€ä¸ªå®ç°å¹¶å¸®åŠ©ä½ å…¨é¢ç†è§£Transformerçš„å˜é©æ€§åŠ›é‡ã€‚æƒ³æ·±å…¥äº†è§£Transformerçš„å·¥ä½œåŸç†ï¼Œæˆ‘å»ºè®®ä½ å‚è€ƒæ–¯å¦ç¦å¤§å­¦çš„ä¼˜ç§€CS224-nè¯¾ç¨‹ã€‚

æˆ‘è¿˜å»ºè®®å…³æ³¨ä¸æœ¬æ–‡ç›¸å…³çš„[GitHub ä»“åº“](https://github.com/maxime7770/Transformers-Insights)ä»¥è·å–æ›´å¤šç»†èŠ‚ã€‚ğŸ˜Š

# ä»€ä¹ˆæ˜¯Transformerç¼–ç å™¨æ¶æ„ï¼Ÿ

![](../Images/df39c5bc0e96c04388b637bb391a7fed.png)

æ¥è‡ª[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)çš„Transformeræ¨¡å‹

è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†åŸå§‹Transformeræ¶æ„ï¼Œå°†ç¼–ç å™¨å’Œè§£ç å™¨ç»“åˆç”¨äºåºåˆ—åˆ°åºåˆ—çš„è¯­è¨€ä»»åŠ¡ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹ä»‹ç»ç¼–ç å™¨æ¶æ„ï¼ˆå›¾ä¸­çš„çº¢è‰²å—ï¼‰ã€‚è¿™æ­£æ˜¯æµè¡Œçš„BERTæ¨¡å‹åœ¨åå°ä½¿ç”¨çš„æ¶æ„ï¼šå…¶ä¸»è¦å…³æ³¨çš„æ˜¯**ç†è§£å’Œè¡¨ç¤ºæ•°æ®**ï¼Œè€Œä¸æ˜¯ç”Ÿæˆåºåˆ—ã€‚å®ƒå¯ä»¥ç”¨äºå¤šç§åº”ç”¨ï¼šæ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€æŠ½å–å¼é—®ç­”ç­‰ã€‚

é‚£ä¹ˆï¼Œè¿™äº›æ•°æ®ç©¶ç«Ÿæ˜¯å¦‚ä½•é€šè¿‡è¯¥æ¶æ„è¿›è¡Œè½¬æ¢çš„å‘¢ï¼Ÿæˆ‘ä»¬å°†è¯¦ç»†è§£é‡Šæ¯ä¸ªç»„ä»¶ï¼Œä½†è¿™é‡Œæ˜¯è¿‡ç¨‹çš„æ¦‚è¿°ã€‚

+   è¾“å…¥æ–‡æœ¬è¢«**æ ‡è®°åŒ–**ï¼šPythonå­—ç¬¦ä¸²è¢«è½¬æ¢æˆæ ‡è®°ï¼ˆæ•°å­—ï¼‰åˆ—è¡¨ã€‚

+   æ¯ä¸ªæ ‡è®°éƒ½é€šè¿‡ä¸€ä¸ª**åµŒå…¥å±‚**ï¼Œè¯¥å±‚è¾“å‡ºæ¯ä¸ªæ ‡è®°çš„å‘é‡è¡¨ç¤ºã€‚

+   ç„¶åï¼Œè¿™äº›åµŒå…¥ä¼šé€šè¿‡**ä½ç½®ç¼–ç å±‚**è¿›ä¸€æ­¥ç¼–ç ï¼Œæ·»åŠ æ¯ä¸ªæ ‡è®°åœ¨åºåˆ—ä¸­çš„ä½ç½®ä¿¡æ¯ã€‚

+   è¿™äº›æ–°çš„åµŒå…¥é€šè¿‡ä¸€ç³»åˆ—**ç¼–ç å™¨å±‚**è¿›è¡Œè½¬æ¢ï¼Œä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚

+   å¯ä»¥æ·»åŠ ä¸€ä¸ª**ä»»åŠ¡ç‰¹å®šçš„å¤´**ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬ç¨åå°†ä½¿ç”¨ä¸€ä¸ªåˆ†ç±»å¤´ï¼Œå°†ç”µå½±è¯„è®ºåˆ†ç±»ä¸ºæ­£é¢æˆ–è´Ÿé¢ã€‚

é‡è¦çš„æ˜¯è¦ç†è§£ï¼ŒTransformeræ¶æ„é€šè¿‡å°†åµŒå…¥å‘é‡ä»é«˜ç»´ç©ºé—´ä¸­çš„ä¸€ä¸ªè¡¨ç¤ºæ˜ å°„åˆ°åŒä¸€ç©ºé—´ä¸­çš„å¦ä¸€ä¸ªè¡¨ç¤ºï¼Œåº”ç”¨ä¸€ç³»åˆ—å¤æ‚çš„å˜æ¢æ¥è½¬æ¢è¿™äº›åµŒå…¥ã€‚

# åœ¨Pythonä¸­å®ç°ç¼–ç å™¨æ¶æ„

## ä½ç½®ç¼–ç å™¨å±‚

ä¸RNNæ¨¡å‹ä¸åŒï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸åˆ©ç”¨è¾“å…¥åºåˆ—çš„é¡ºåºã€‚PositionalEncoderç±»é€šè¿‡ä½¿ç”¨ä¸¤ç§æ•°å­¦å‡½æ•°ï¼šä½™å¼¦å’Œæ­£å¼¦ï¼Œå‘è¾“å…¥åµŒå…¥æ·»åŠ ä½ç½®ç¼–ç ã€‚

![](../Images/578ef5784eef3551c213f00d3bfdbdd4.png)

ä½ç½®ç¼–ç çŸ©é˜µå®šä¹‰æ¥è‡ª[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

æ³¨æ„ï¼Œä½ç½®ç¼–ç ä¸åŒ…å«å¯è®­ç»ƒçš„å‚æ•°ï¼šå®ƒä»¬æ˜¯ç¡®å®šæ€§è®¡ç®—çš„ç»“æœï¼Œè¿™ä½¿å¾—è¯¥æ–¹æ³•éå¸¸å¯å¤„ç†ã€‚æ­¤å¤–ï¼Œæ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„å€¼ä»‹äº-1å’Œ1ä¹‹é—´ï¼Œå¹¶å…·æœ‰æœ‰åŠ©äºæ¨¡å‹å­¦ä¹ **å•è¯ç›¸å¯¹ä½ç½®**çš„æœ‰ç”¨å‘¨æœŸæ€§ç‰¹æ€§ã€‚

```py
class PositionalEncoder(nn.Module):
    def __init__(self, d_model, max_length):
        super(PositionalEncoder, self).__init__()
        self.d_model = d_model
        self.max_length = max_length

        # Initialize the positional encoding matrix
        pe = torch.zeros(max_length, d_model)

        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))

        # Calculate and assign position encodings to the matrix
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.pe = pe.unsqueeze(0)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)] # update embeddings
        return x
```

## å¤šå¤´è‡ªæ³¨æ„åŠ›

è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯ç¼–ç å™¨æ¶æ„çš„å…³é”®ç»„ä»¶ã€‚æˆ‘ä»¬æš‚æ—¶å¿½ç•¥â€œå¤šå¤´â€éƒ¨åˆ†ã€‚æ³¨æ„åŠ›æ˜¯ä¸€ç§æ–¹æ³•ï¼Œç”¨æ¥ç¡®å®šæ¯ä¸ªæ ‡è®°ï¼ˆå³æ¯ä¸ªåµŒå…¥ï¼‰ä¸**æ‰€æœ‰å…¶ä»–åµŒå…¥ä¸è¯¥æ ‡è®°çš„ç›¸å…³æ€§**ï¼Œä»è€Œè·å¾—æ›´ç²¾ç»†å’Œä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„ç¼–ç ã€‚

![](../Images/cd6d84be3aae29b2a4108ac244d7dd6b.png)

â€œå®ƒâ€æ˜¯å¦‚ä½•å…³æ³¨åºåˆ—ä¸­å…¶ä»–å•è¯çš„ï¼Ÿ([The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/))

è‡ªæ³¨æ„åŠ›æœºåˆ¶æœ‰ä¸‰ä¸ªæ­¥éª¤ã€‚

+   ä½¿ç”¨çŸ©é˜µQã€Kå’ŒVåˆ†åˆ«è½¬æ¢è¾“å…¥çš„â€œ**æŸ¥è¯¢**â€ã€â€œ**é”®**â€å’Œâ€œ**å€¼**â€ã€‚è¯·æ³¨æ„ï¼Œå¯¹äºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ŒæŸ¥è¯¢ã€é”®å’Œå€¼éƒ½æ˜¯ç­‰äºæˆ‘ä»¬çš„è¾“å…¥åµŒå…¥ã€‚

+   é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼‰è®¡ç®—**æŸ¥è¯¢**å’Œ**é”®**ä¹‹é—´çš„æ³¨æ„åŠ›å¾—åˆ†ã€‚å¾—åˆ†ä¼šé€šè¿‡åµŒå…¥ç»´åº¦çš„å¹³æ–¹æ ¹è¿›è¡Œç¼©æ”¾ï¼Œä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦ã€‚

+   ä½¿ç”¨ softmax å±‚å°†è¿™äº›å¾—åˆ†è½¬åŒ–ä¸º**æ¦‚ç‡**ã€‚

+   è¾“å‡ºæ˜¯**å€¼**çš„åŠ æƒå¹³å‡ï¼Œä½¿ç”¨æ³¨æ„åŠ›å¾—åˆ†ä½œä¸ºæƒé‡ã€‚

ä»æ•°å­¦è§’åº¦æ¥çœ‹ï¼Œè¿™å¯¹åº”äºä»¥ä¸‹å…¬å¼ã€‚

![](../Images/c5773d753ad9db7bab34c7e7739de18c.png)

[æ³¨æ„åŠ›æœºåˆ¶æ¥è‡ªã€ŠAttention Is All You Needã€‹](https://arxiv.org/pdf/1706.03762.pdf)

â€œå¤šå¤´â€æ˜¯ä»€ä¹ˆæ„æ€ï¼ŸåŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å¹¶è¡Œå¤šæ¬¡åº”ç”¨ä¸Šè¿°è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿‡ç¨‹ï¼Œå¹¶å°†è¾“å‡ºè¿›è¡Œæ‹¼æ¥å’ŒæŠ•å½±ã€‚è¿™ä½¿å¾—æ¯ä¸ªå¤´å¯ä»¥**ä¸“æ³¨äºå¥å­çš„ä¸åŒè¯­ä¹‰æ–¹é¢**ã€‚

æˆ‘ä»¬é¦–å…ˆå®šä¹‰å¤´çš„æ•°é‡ã€åµŒå…¥çš„ç»´åº¦ï¼ˆd_modelï¼‰ä»¥åŠæ¯ä¸ªå¤´çš„ç»´åº¦ï¼ˆhead_dimï¼‰ã€‚æˆ‘ä»¬è¿˜åˆå§‹åŒ–äº† Qã€K å’Œ V çŸ©é˜µï¼ˆçº¿æ€§å±‚ï¼‰ï¼Œä»¥åŠæœ€ç»ˆçš„æŠ•å½±å±‚ã€‚

```py
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        self.head_dim = d_model // num_heads

        self.query_linear = nn.Linear(d_model, d_model)
        self.key_linear = nn.Linear(d_model, d_model)
        self.value_linear = nn.Linear(d_model, d_model)      
        self.output_linear = nn.Linear(d_model, d_model)
```

ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›æ—¶ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªæ³¨æ„åŠ›å¤´åº”ç”¨ä¸€ä¸ªå‡å°‘ç»´åº¦çš„å¤„ç†ï¼ˆä½¿ç”¨ head_dim è€Œé d_modelï¼‰ï¼Œå°±åƒåŸè®ºæ–‡ä¸­æ‰€è¿°ï¼Œè¿™ä½¿å¾—æ€»çš„è®¡ç®—æˆæœ¬ç±»ä¼¼äºä¸€ä¸ªå…¨ç»´åº¦çš„å•å¤´æ³¨æ„åŠ›å±‚ã€‚è¯·æ³¨æ„ï¼Œè¿™åªæ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„æ‹†åˆ†ã€‚å¤šå¤´æ³¨æ„åŠ›ä¹‹æ‰€ä»¥å¼ºå¤§ï¼Œæ˜¯å› ä¸ºå®ƒä»ç„¶å¯ä»¥é€šè¿‡å•ä¸€çš„çŸ©é˜µæ“ä½œæ¥è¡¨ç¤ºï¼Œä»è€Œä½¿å¾— GPU ä¸Šçš„è®¡ç®—éå¸¸é«˜æ•ˆã€‚

```py
def split_heads(self, x, batch_size):
        # Split the sequence embeddings in x across the attention heads
        x = x.view(batch_size, -1, self.num_heads, self.head_dim)
        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)
```

æˆ‘ä»¬è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼Œå¹¶ä½¿ç”¨æ©ç é¿å…åœ¨å¡«å……çš„æ ‡è®°ä¸Šä½¿ç”¨æ³¨æ„åŠ›ã€‚æˆ‘ä»¬åº”ç”¨ softmax æ¿€æ´»å‡½æ•°å°†è¿™äº›å¾—åˆ†è½¬åŒ–ä¸ºæ¦‚ç‡ã€‚

```py
def compute_attention(self, query, key, mask=None):
      # Compute dot-product attention scores
      # dimensions of query and key are (batch_size * num_heads, seq_length, head_dim)
      scores = query @ key.transpose(-2, -1) / math.sqrt(self.head_dim)
      # Now, dimensions of scores is (batch_size * num_heads, seq_length, seq_length)
      if mask is not None:
          scores = scores.view(-1, scores.shape[0] // self.num_heads, mask.shape[1], mask.shape[2]) # for compatibility
          scores = scores.masked_fill(mask == 0, float('-1e20')) # mask to avoid attention on padding tokens
          scores = scores.view(-1, mask.shape[1], mask.shape[2]) # reshape back to original shape
      # Normalize attention scores into attention weights
      attention_weights = F.softmax(scores, dim=-1)

      return attention_weights
```

`forward` å±æ€§æ‰§è¡Œå¤šå¤´é€»è¾‘æ‹†åˆ†å¹¶è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å°†è¿™äº›æƒé‡ä¸å€¼ç›¸ä¹˜æ¥è·å¾—è¾“å‡ºã€‚æœ€åï¼Œæˆ‘ä»¬é‡æ–°è°ƒæ•´è¾“å‡ºçš„å½¢çŠ¶å¹¶é€šè¿‡çº¿æ€§å±‚è¿›è¡ŒæŠ•å½±ã€‚

```py
def forward(self, query, key, value, mask=None):
      batch_size = query.size(0)

      query = self.split_heads(self.query_linear(query), batch_size)
      key = self.split_heads(self.key_linear(key), batch_size)
      value = self.split_heads(self.value_linear(value), batch_size)

      attention_weights = self.compute_attention(query, key, mask)

      # Multiply attention weights by values, concatenate and linearly project outputs
      output = torch.matmul(attention_weights, value)
      output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)
      return self.output_linear(output)
```

## ç¼–ç å™¨å±‚

è¿™æ˜¯è¯¥æ¶æ„çš„ä¸»è¦ç»„ä»¶ï¼Œå®ƒåˆ©ç”¨äº†å¤šå¤´è‡ªæ³¨æ„åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆå®ç°ä¸€ä¸ªç®€å•çš„ç±»ï¼Œé€šè¿‡ 2 ä¸ªå…¨è¿æ¥å±‚æ‰§è¡Œå‰å‘æ“ä½œã€‚

```py
class FeedForwardSubLayer(nn.Module):
    def __init__(self, d_model, d_ff):
        super(FeedForwardSubLayer, self).__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()

    def forward(self, x):
        return self.fc2(self.relu(self.fc1(x)))
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥ç¼–å†™ç¼–ç å™¨å±‚çš„é€»è¾‘ã€‚æˆ‘ä»¬é¦–å…ˆå¯¹è¾“å…¥åº”ç”¨è‡ªæ³¨æ„åŠ›ï¼Œå¾—åˆ°ä¸€ä¸ªç›¸åŒç»´åº¦çš„å‘é‡ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨å¸¦æœ‰å±‚å½’ä¸€åŒ–å±‚çš„å°å‹å‰é¦ˆç½‘ç»œã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨åº”ç”¨å½’ä¸€åŒ–ä¹‹å‰è¿˜ä½¿ç”¨äº†è·³è·ƒè¿æ¥ã€‚

```py
class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        attn_output = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output)) # skip connection and normalization
        ff_output = self.feed_forward(x)
        return self.norm2(x + self.dropout(ff_output)) # skip connection and normalization
```

## å°†ä¸€åˆ‡æ•´åˆåœ¨ä¸€èµ·

ç°åœ¨æ˜¯æ—¶å€™åˆ›å»ºæˆ‘ä»¬çš„æœ€ç»ˆæ¨¡å‹äº†ã€‚æˆ‘ä»¬é€šè¿‡åµŒå…¥å±‚å°†æ•°æ®ä¼ é€’è¿›å»ã€‚è¿™å°†åŸå§‹æ ‡è®°ï¼ˆæ•´æ•°ï¼‰è½¬æ¢ä¸ºæ•°å€¼å‘é‡ã€‚ç„¶åæˆ‘ä»¬åº”ç”¨ä½ç½®ç¼–ç å™¨å’Œè‹¥å¹²ï¼ˆnum_layersï¼‰ç¼–ç å™¨å±‚ã€‚

```py
class TransformerEncoder(nn.Module):
    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):
        super(TransformerEncoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)
        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])

    def forward(self, x, mask):
        x = self.embedding(x)
        x = self.positional_encoding(x)
        for layer in self.layers:
            x = layer(x, mask)
        return x
```

æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ª `ClassifierHead` ç±»ï¼Œç”¨äºå°†æœ€ç»ˆçš„åµŒå…¥è½¬æ¢ä¸ºåˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«æ¦‚ç‡ã€‚

```py
class ClassifierHead(nn.Module):
    def __init__(self, d_model, num_classes):
        super(ClassifierHead, self).__init__()
        self.fc = nn.Linear(d_model, num_classes)

    def forward(self, x):
        logits = self.fc(x[:, 0, :]) # first token corresponds to the classification token
        return F.softmax(logits, dim=-1)
```

æ³¨æ„ï¼Œç¨ å¯†å±‚å’Œ softmax å±‚åªåº”ç”¨äºç¬¬ä¸€ä¸ªåµŒå…¥ï¼ˆå¯¹åº”äºè¾“å…¥åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ï¼‰ã€‚è¿™æ˜¯å› ä¸ºåœ¨åˆ†è¯æ–‡æœ¬æ—¶ï¼Œç¬¬ä¸€ä¸ªæ ‡è®°æ˜¯[CLS]æ ‡è®°ï¼Œä»£è¡¨â€œåˆ†ç±»â€ã€‚[CLS]æ ‡è®°çš„è®¾è®¡ç›®çš„æ˜¯å°†æ•´ä¸ªåºåˆ—çš„ä¿¡æ¯èšåˆæˆä¸€ä¸ªå•ä¸€çš„åµŒå…¥å‘é‡ï¼Œä½œä¸ºå¯ä»¥ç”¨äºåˆ†ç±»ä»»åŠ¡çš„æ‘˜è¦è¡¨ç¤ºã€‚

æ³¨æ„ï¼šåŒ…å«[CLS]æ ‡è®°çš„æ¦‚å¿µæºè‡ª BERTï¼ŒBERT æœ€åˆæ˜¯åœ¨ç±»ä¼¼ä¸‹ä¸€å¥é¢„æµ‹ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚[CLS]æ ‡è®°è¢«æ’å…¥ä»¥é¢„æµ‹å¥å­ B æ˜¯å¦è·Ÿéšå¥å­ Aï¼Œè€Œ[SEP]æ ‡è®°åˆ™åˆ†éš”è¿™ä¸¤ä¸ªå¥å­ã€‚å¯¹äºæˆ‘ä»¬çš„æ¨¡å‹ï¼Œ[SEP]æ ‡è®°åªæ˜¯æ ‡è®°è¾“å…¥å¥å­çš„ç»“æŸï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

![](../Images/d3feb19e839643803e928bfc05828ad2.png)

BERT æ¶æ„ä¸­çš„[CLS]æ ‡è®° ([All About AI](https://seunghan96.github.io/dl/nlp/28.-nlp-BERT-%EC%9D%B4%EB%A1%A0/))

å½“ä½ ä»”ç»†æƒ³æƒ³ï¼ŒçœŸæ˜¯ä»¤äººéœ‡æƒŠï¼Œè¿™ä¸ªå•ä¸€çš„[CLS]åµŒå…¥èƒ½å¤Ÿæ•è·å¦‚æ­¤å¤šå…³äºæ•´ä¸ªåºåˆ—çš„ä¿¡æ¯ï¼Œè¿™è¦å½’åŠŸäºè‡ªæ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿæƒè¡¡å’Œç»¼åˆæ¯ä¸ªæ–‡æœ¬ç‰‡æ®µä¹‹é—´ç›¸äº’å…³ç³»çš„é‡è¦æ€§ã€‚

# è®­ç»ƒä¸å¯è§†åŒ–

å¸Œæœ›ä¸Šä¸€èŠ‚èƒ½å¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£æˆ‘ä»¬çš„ Transformer æ¨¡å‹æ˜¯å¦‚ä½•è½¬æ¢è¾“å…¥æ•°æ®çš„ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ç¼–å†™æˆ‘ä»¬çš„è®­ç»ƒæµç¨‹ï¼Œç”¨äºå¤„ç† IMDB æ•°æ®é›†ï¼ˆç”µå½±è¯„è®ºï¼‰çš„äºŒåˆ†ç±»ä»»åŠ¡ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­[CLS]æ ‡è®°çš„åµŒå…¥ï¼Œçœ‹çœ‹æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦‚ä½•è½¬æ¢å®ƒçš„ã€‚

æˆ‘ä»¬é¦–å…ˆå®šä¹‰è¶…å‚æ•°ï¼Œå¹¶ä¸”ä½¿ç”¨ BERT åˆ†è¯å™¨ã€‚åœ¨ GitHub ä»“åº“ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°æˆ‘è¿˜ç¼–å†™äº†ä¸€ä¸ªå‡½æ•°ï¼Œæ¥é€‰æ‹©æ•°æ®é›†çš„ä¸€ä¸ªå­é›†ï¼Œå…¶ä¸­åŒ…å« 1200 ä¸ªè®­ç»ƒæ ·æœ¬å’Œ 200 ä¸ªæµ‹è¯•æ ·æœ¬ã€‚

```py
num_classes = 2 # binary classification
d_model = 256 # dimension of the embedding vectors
num_heads = 4 # number of heads for self-attention
num_layers = 4 # number of encoder layers
d_ff = 512\. # dimension of the dense layers in the encoder layers
sequence_length = 256 # maximum sequence length 
dropout = 0.4 # dropout to avoid overfitting
num_epochs = 20
batch_size = 32

loss_function = torch.nn.CrossEntropyLoss()

dataset = load_dataset("imdb")
dataset = balance_and_create_dataset(dataset, 1200, 200) # check GitHub repo

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', model_max_length=sequence_length)
```

ä½ å¯ä»¥å°è¯•åœ¨å…¶ä¸­ä¸€ä¸ªå¥å­ä¸Šä½¿ç”¨ BERT åˆ†è¯å™¨ï¼š

```py
print(tokenized_datasets['train']['input_ids'][0])
```

æ¯ä¸ªåºåˆ—åº”ä»¥ 101 å·æ ‡è®°å¼€å§‹ï¼Œå¯¹åº”[CLS]ï¼Œæ¥ç€æ˜¯ä¸€äº›éé›¶æ•´æ•°ï¼Œå¦‚æœåºåˆ—é•¿åº¦å°äº 256ï¼Œåˆ™ç”¨é›¶å¡«å……ã€‚æ³¨æ„ï¼Œè¿™äº›é›¶åœ¨ä½¿ç”¨æˆ‘ä»¬çš„â€œæ©ç â€è¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—æ—¶ä¼šè¢«å¿½ç•¥ã€‚

```py
tokenized_datasets = dataset.map(encode_examples, batched=True)
tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=batch_size, shuffle=True)

vocab_size = tokenizer.vocab_size

encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)
classifier = ClassifierHead(d_model, num_classes)

optimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=1e-4)
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥ç¼–å†™è®­ç»ƒå‡½æ•°ï¼š

```py
def train(dataloader, encoder, classifier, optimizer, loss_function, num_epochs):
    for epoch in range(num_epochs):        
        # Collect and store embeddings before each epoch starts for visualization purposes (check repo)
        all_embeddings, all_labels = collect_embeddings(encoder, dataloader)
        reduced_embeddings = visualize_embeddings(all_embeddings, all_labels, epoch, show=False)
        dic_embeddings[epoch] = [reduced_embeddings, all_labels]

        encoder.train()
        classifier.train()
        correct_predictions = 0
        total_predictions = 0
        for batch in tqdm(dataloader, desc="Training"):
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask'] # indicate where padded tokens are
            # These 2 lines make the attention_mask a matrix instead of a vector
            attention_mask = attention_mask.unsqueeze(-1)
            attention_mask = attention_mask & attention_mask.transpose(1, 2) 
            labels = batch['label']
            optimizer.zero_grad()
            output = encoder(input_ids, attention_mask)
            classification = classifier(output)
            loss = loss_function(classification, labels)
            loss.backward()
            optimizer.step()
            preds = torch.argmax(classification, dim=1)
            correct_predictions += torch.sum(preds == labels).item()
            total_predictions += labels.size(0)

        epoch_accuracy = correct_predictions / total_predictions
        print(f'Epoch {epoch} Training Accuracy: {epoch_accuracy:.4f}')
```

ä½ å¯ä»¥åœ¨ GitHub ä»“åº“ä¸­æ‰¾åˆ° collect_embeddings å’Œ visualize_embeddings å‡½æ•°ã€‚å®ƒä»¬å­˜å‚¨äº†è®­ç»ƒé›†æ¯ä¸ªå¥å­çš„[CLS]æ ‡è®°åµŒå…¥ï¼Œåº”ç”¨ä¸€ç§å«åš t-SNE çš„é™ç»´æŠ€æœ¯å°†å…¶è½¬åŒ–ä¸ºäºŒç»´å‘é‡ï¼ˆè€Œä¸æ˜¯ 256 ç»´å‘é‡ï¼‰ï¼Œå¹¶ä¿å­˜åŠ¨ç”»å›¾è¡¨ã€‚

è®©æˆ‘ä»¬æ¥å¯è§†åŒ–ç»“æœã€‚

![](../Images/23987d3afe279c76504d4c31a47e8066.png)

æ¯ä¸ªè®­ç»ƒç‚¹çš„æŠ•å½±[CLS]åµŒå…¥ï¼ˆè“è‰²ä»£è¡¨æ­£å‘å¥å­ï¼Œçº¢è‰²ä»£è¡¨è´Ÿå‘å¥å­ï¼‰

é€šè¿‡è§‚å¯Ÿæ¯ä¸ªè®­ç»ƒç‚¹çš„[CLS]åµŒå…¥çš„æŠ•å½±å›¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç»è¿‡è‹¥å¹²è½®è®­ç»ƒåï¼Œæ­£å‘ï¼ˆè“è‰²ï¼‰å’Œè´Ÿå‘ï¼ˆçº¢è‰²ï¼‰å¥å­ä¹‹é—´çš„æ˜æ˜¾åŒºåˆ«ã€‚è¿™ä¸€å¯è§†åŒ–å±•ç¤ºäº†Transformeræ¶æ„éšç€æ—¶é—´æ¨ç§»è°ƒæ•´åµŒå…¥çš„æ˜¾è‘—èƒ½åŠ›ï¼Œçªå‡ºäº†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å¼ºå¤§åŠŸèƒ½ã€‚æ•°æ®ä»¥ä¸€ç§æ–¹å¼è¿›è¡Œè½¬åŒ–ï¼Œä½¿å¾—æ¯ä¸ªç±»åˆ«çš„åµŒå…¥å¾—åˆ°äº†è‰¯å¥½çš„åˆ†ç¦»ï¼Œä»è€Œå¤§å¤§ç®€åŒ–äº†åˆ†ç±»å™¨å¤´çš„ä»»åŠ¡ã€‚

# ç»“è®º

éšç€æˆ‘ä»¬å¯¹Transformeræ¶æ„çš„æ¢ç´¢çš„ç»“æŸï¼Œæ˜¾ç„¶è¿™äº›æ¨¡å‹æ“…é•¿å°†æ•°æ®å®šåˆ¶åŒ–ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚é€šè¿‡ä½¿ç”¨ä½ç½®ç¼–ç å’Œå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ŒTransformerä¸ä»…ä»…æ˜¯å¤„ç†æ•°æ®ï¼šå®ƒä»¬ä»¥ä¸€ç§å‰æ‰€æœªè§çš„å¤æ‚ç¨‹åº¦æ¥è§£é‡Šå’Œç†è§£ä¿¡æ¯ã€‚èƒ½å¤ŸåŠ¨æ€åœ°æƒè¡¡è¾“å…¥æ•°æ®ä¸åŒéƒ¨åˆ†çš„ç›¸å…³æ€§ï¼Œä½¿å¾—å¯¹è¾“å…¥æ–‡æœ¬çš„ç†è§£å’Œè¡¨ç¤ºæ›´åŠ ç»†è‡´ã€‚è¿™æå‡äº†åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚

ç°åœ¨ä½ å·²ç»æ›´å¥½åœ°ç†è§£äº†ç¼–ç å™¨æ¶æ„ï¼Œä½ å¯ä»¥æ·±å…¥æ¢è®¨è§£ç å™¨å’Œç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ä¸æˆ‘ä»¬åˆšåˆšæ¢è®¨çš„éå¸¸ç›¸ä¼¼ã€‚è§£ç å™¨åœ¨ç”Ÿæˆä»»åŠ¡ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œæ˜¯æµè¡Œçš„GPTæ¨¡å‹çš„æ ¸å¿ƒéƒ¨åˆ†ã€‚

+   éšæ—¶åœ¨[LinkedIn](https://www.linkedin.com/in/maxime-wolf/)ä¸Šä¸æˆ‘è”ç³»

+   è¯·åœ¨[GitHub](https://github.com/maxime7770)ä¸Šå…³æ³¨æˆ‘ï¼Œè·å–æ›´å¤šå†…å®¹

+   è®¿é—®æˆ‘çš„ç½‘ç«™: [maximewolf.com](http://maximewolf.com)

**å‚è€ƒæ–‡çŒ®**

[1] Vaswani, Ashish, ç­‰äºº. â€œAttention Is All You Need.â€ *ç¬¬31å±Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿä¼šè®®ï¼ˆNIPS 2017ï¼‰*, ç¾å›½åŠ åˆ©ç¦å°¼äºšå·é•¿æ»©.

[2] â€œThe Illustrated Transformer.â€ *Jay Alammarçš„åšå®¢*, 2018å¹´6æœˆ, [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)

[3] Transformeræ¶æ„çš„å®˜æ–¹PyTorchå®ç°. *GitHubä»£ç åº“*, PyTorch, [https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py)

[4] Manning, Christopher, ç­‰äºº. â€œCS224n: ä½¿ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†.â€ *æ–¯å¦ç¦å¤§å­¦*, æ–¯å¦ç¦CS224N NLPè¯¾ç¨‹, [http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)
