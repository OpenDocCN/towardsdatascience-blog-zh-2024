<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Simple Example Using PCA for Outlier Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Simple Example Using PCA for Outlier Detection</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-simple-example-using-pca-for-outlier-detection-ab2773b98e4a?source=collection_archive---------0-----------------------#2024-11-02">https://towardsdatascience.com/a-simple-example-using-pca-for-outlier-detection-ab2773b98e4a?source=collection_archive---------0-----------------------#2024-11-02</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="3aa0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Improve accuracy, speed, and memory usage by performing PCA transformation before outlier detection</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@wkennedy934?source=post_page---byline--ab2773b98e4a--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="W Brett Kennedy" class="l ep by dd de cx" src="../Images/b3ce55ffd028167326c117d47c64c467.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*v8pf2r3SPMLuHoSmF4IwlA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ab2773b98e4a--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@wkennedy934?source=post_page---byline--ab2773b98e4a--------------------------------" rel="noopener follow">W Brett Kennedy</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ab2773b98e4a--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">19 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 2, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ld le">1</span></p></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="lj k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lk an ao ap ig ll lm ln" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lo cn"><div class="l ae"><div class="ab cb"><div class="lp lq lr ls lt lu ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="62db" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This article continues a series related to applications of PCA (principal component analysis) for outlier detection, following <a class="af ni" href="https://medium.com/towards-data-science/using-pca-for-outlier-detection-afecab4d2b78" rel="noopener">An Introduction to PCA for Outlier Detection</a>. That article described PCA itself, and introduced the two main ways we can use PCA for outlier detection: evaluating the reconstruction error, and running standard outlier detectors on the PCA-transformed space. It also gave an example of the first approach, using reconstruction error, which is straightforward to do using the PCA and KPCA detectors provided by <a class="af ni" href="https://github.com/yzhao062/pyod" rel="noopener ugc nofollow" target="_blank">PyOD</a>.</p><p id="5937" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This article covers the second approach, where we first transform the data space using PCA and then run standard outlier detection on this. As covered in the previous article, this can in some cases lower interpretability, but it does have some surprising benefits in terms of accuracy, execution time, and memory usage.</p><p id="8223" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This article is also part of a larger series on outlier detection, so far covering <a class="af ni" rel="noopener" target="_blank" href="/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a">FPOF</a>, <a class="af ni" rel="noopener" target="_blank" href="/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a">Counts Outlier Detector</a>, <a class="af ni" rel="noopener" target="_blank" href="/distance-metric-learning-for-outlier-detection-5b4840d01246">Distance Metric Learning</a>, <a class="af ni" rel="noopener" target="_blank" href="/shared-nearest-neighbors-a-more-robust-distance-metric-064d7f99ffb7">Shared Nearest Neighbors</a>, and <a class="af ni" rel="noopener" target="_blank" href="/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4">Doping</a>. This article also includes an excerpt from my book <a class="af ni" href="https://www.manning.com/books/outlier-detection-in-python" rel="noopener ugc nofollow" target="_blank">Outlier Detection in Python</a>.</p><p id="b6a5" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">If you’re reasonably familiar with PCA itself (as it’s used for dimensionality reduction or visualization), you can probably skip the previous article if you wish, and dive straight into this one. I will, though, very quickly review the main idea.</p><p id="ff48" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">PCA is a means to transform data (viewing data records as points in high-dimensional space) from one set of coordinates to another. If we start with a dataset (as shown below in the left pane), with 100 records and two features, then we can view the data as 100 points in 2-dimensional space. With more realistic data, we would have many more records and many more dimensions, but the same idea holds. Using PCA, we move the data to a new set of coordinates, so effectively create a new set of features describing each record. As described in the previous article, this is done by identifying orthogonal lines through the data (shown in the left pane as the blue and orange lines) that fit the data well.</p><p id="5827" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">So, if we start with a dataset, such as is shown in the left pane below, we can apply PCA transformation to transform the data into something like is shown in the right pane. In the right pane, we show the two PCA components the data was mapped to. The components are simply named 0 and 1.</p><figure class="nj nk nl nm nn no np nq paragraph-image"><div class="ab cn cb nr"><img src="../Images/69421afe0afc9c2b38d05f22f8393014.png" data-original-src="https://miro.medium.com/v2/format:webp/1*sc4quH1apgtS1gWzBaerJA.png"/></div><figcaption class="nt nu nv np nq nw nx bf b bg z dx">Left pane: 100 data points in a dataset with two features. The blue and orange lines show orthogonal lines that may be drawn through the data to capture the location of the points well. These are used to determine the PCA transformation. Right pane: the same data after PCA transformation. We have the same 100 data points, but two new coordinates, called Component 0 and Component 1.</figcaption></figure><p id="1cb6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">One thing to note about PCA components is that they are completely uncorrelated. This is a result of how they are constructed; they are based on lines, planes, or hyperplanes through the original data that are all strictly orthogonal to each other. We can see in the right pane, there is no relationship between component 0 and component 1.</p><p id="de92" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This has strong implications for outlier detection; in particular it means that outliers tend to be transformed into extreme values in one or more of the components, and so are easier to detect. It also means that more sophisticated outlier tests (that test for unusual associations among the features) are not necessary, and simpler tests can be used.</p><h1 id="d632" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Univariate and Multivariate outer detectors</h1><p id="900e" class="pw-post-body-paragraph mm mn fq mo b go ou mq mr gr ov mt mu mv ow mx my mz ox nb nc nd oy nf ng nh fj bk">Before looking closer at the benefits of PCA for for outlier detection, I’ll quickly go over two types of outlier detectors. There are many ways to classify outlier detection algorithms, but one useful way is to distinguish between what are called <em class="oz">univariate </em>from <em class="oz">multivariate tests</em>.</p><h2 id="57e3" class="pa nz fq bf oa pb pc pd od pe pf pg og mv ph pi pj mz pk pl pm nd pn po pp pq bk">Univariate Tests</h2><p id="c3a0" class="pw-post-body-paragraph mm mn fq mo b go ou mq mr gr ov mt mu mv ow mx my mz ox nb nc nd oy nf ng nh fj bk">The term <em class="oz">univariate</em> refers to tests that just check one feature — tests that identify the rare or extreme values in that one feature. Examples are tests based on z-score, interquartile range (IQR), inter-decile range (IDR), median absolute deviation (MAD), histogram tests, KDE tests, and so on.</p><p id="3adc" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">One histogram-based test provided by <a class="af ni" href="https://github.com/yzhao062/pyod" rel="noopener ugc nofollow" target="_blank">PyOD </a>(PyOD is probably the most complete and useful tool for outlier detection on tabular data available in Python today) is HBOS (Histogram-based Outlier Score — described in my Medium article on <a class="af ni" href="https://medium.com/towards-data-science/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a" rel="noopener">Counts Outlier Detector</a>, and in detail in <a class="af ni" href="https://www.manning.com/books/outlier-detection-in-python" rel="noopener ugc nofollow" target="_blank">Outlier Detection in Python</a>).</p><p id="6bfa" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As covered in <a class="af ni" href="https://medium.com/towards-data-science/using-pca-for-outlier-detection-afecab4d2b78" rel="noopener">Introducing PCA for Outlier Detection</a>, another univariate test provided by PyOD is <a class="af ni" href="https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ecod" rel="noopener ugc nofollow" target="_blank">ECOD</a>.</p><p id="faac" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To describe univariate tests, we look at an example of outlier detection for a specific real-world dataset. The following table is a subset of the <a class="af ni" href="https://www.openml.org/search?type=data&amp;sort=version&amp;status=any&amp;order=asc&amp;exact_name=baseball&amp;id=185" rel="noopener ugc nofollow" target="_blank">baseball </a>dataset from OpenML (available with a public license), here showing just three rows and five columns (there are several more features in the full dataset). Each row represents one player, with statistics for each, including the number of seasons they played, number of games, and so on.</p><figure class="nj nk nl nm nn no np nq paragraph-image"><div class="np nq pr"><img src="../Images/e916e7a6f4ce367cdd789279488994c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*YobBzeQiY8JNFf6f2D9B9Q.png"/></div><figcaption class="nt nu nv np nq nw nx bf b bg z dx">Subset of the baseball dataset</figcaption></figure><p id="014c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To identify unusual players, we can look for those records with unusual single values (for example, players that played in unusually many seasons, had unusually many At bats, and so on). These would be found with univariate tests.</p><p id="2987" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, using z-score tests to find unusual records, we would actually perform a z-score test on each column, one at a time. We’d first check the Number seasons column (assessing how unusual each value in the column is relative to that column), then the Games played column and so on.</p><p id="c7f8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">When checking, for example, the Number seasons column, using a z-score test, we would first determine the mean and standard deviation of the column. (Other tests may determine the median and interquartile range for the column, histogram bin counts, etc.).</p><p id="6a11" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We would then determine the absolute z-score for each value in the Number seasons column: the number of standard deviations each value is from the mean. The larger the z-score, the more unusual the value. Any values with an absolute z-score over about 4.0 or 5.0 can likely be considered anomalous, though this depends on the size of the data and the distribution.</p><p id="afff" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We’d then repeat this for each other column. Once this is done, we have, for each row, a score for how unusual each value in the row is relative to their columns. So, each row would have a set of scores: one score for each value in that row.</p><p id="8249" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We then need to determine an overall outlier score for each record. There are different ways to do this, and some nuances associated with each, but two simple methods are to take the average z-score of the values per row, or to take the maximum z-score per row.</p><h2 id="480f" class="pa nz fq bf oa pb pc pd od pe pf pg og mv ph pi pj mz pk pl pm nd pn po pp pq bk">Multivariate Tests</h2><p id="8fb2" class="pw-post-body-paragraph mm mn fq mo b go ou mq mr gr ov mt mu mv ow mx my mz ox nb nc nd oy nf ng nh fj bk"><em class="oz">Multivariate </em>tests consider multiple features at once. In fact, almost all multivariate outlier detectors consider all features at once.</p><p id="792f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The majority of outlier detectors (including Isolation Forest, Local Outlier Factor (LOF), KNN, and so on) are based on multivariate tests.</p><p id="41ab" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The advantage of these detectors is, we can look for records with unusual combinations of values. For example, some players may have a typical number of Runs and a typical number of At bats, but may have unusually many (or possibly unusually few) Runs given their number of At bats. These would be found with multivariate tests.</p><p id="8943" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In the scatter plot above (considering the original data in the left pane), Point A is extreme in both dimensions, so could be detected by a univariate test. In fact, a univariate test on Feature A would likely flag Point A, and a univariate test on Feature B would likely as well, and so Point A, being anomalous in both features, would be scored highly using univariate tests.</p><p id="4b14" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Point B, though, is typical in both dimensions. Only the combination of values is unusual, and to detect this as an anomaly, we would require a multivariate test.</p><p id="c140" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Normally, when performing outlier detection on tabular data, we’re looking for unusual rows, as opposed to unusual single values. And, unusual rows will include both those rows with unusual single values, as well as unusual combinations of values. So, both univariate and multivariate tests are typically useful. However, multivariate tests will catch both univariate and multivariate outliers (in the scatter plot, a multivariate test such as Isolation Forest, LOF, or KNN would generally catch both Point A and Point B), and so in practice, multivariate tests tend to be used more often.</p><p id="8a33" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Nevertheless, in outlier detection do we quite often limit analysis to univariate tests. Univariate tests are faster — often much faster (which can be very important in real-time environments, or environments where there are very large volumes of data to assess). Univariate tests also tend to be more interpretable.</p><p id="725d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">And they don’t suffer from the curse of dimensionality. This is covered in <a class="af ni" rel="noopener" target="_blank" href="/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a">Counts Outlier Detector</a>, <a class="af ni" rel="noopener" target="_blank" href="/shared-nearest-neighbors-a-more-robust-distance-metric-064d7f99ffb7">Shared Nearest Neighbors</a>, and <a class="af ni" href="https://www.manning.com/books/outlier-detection-in-python" rel="noopener ugc nofollow" target="_blank">Outlier Detection in Python</a>, but the general idea is that multivariate tests can break down when working with too many features. This is for a number of reasons, but an important one is that distance calculations (which many outlier detectors, including LOF and KNN, rely on) can become meaningless given enough dimensions. Often working with just 20 or more features, and very often with about 50 or more, outlier scores can become unreliable.</p><p id="ecba" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Univariate tests scale to higher dimensions much better than multivariate tests, as they do not rely on distance calculations between the rows.</p><p id="8c52" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">And so, there are some major advantages to using univariate tests. But, also some major disadvantages: these miss outliers that relate to unusual combinations of values, and so can detect only a portion of the relevant outliers.</p><h1 id="91ee" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Univariate tests on PCA components</h1><p id="c3ae" class="pw-post-body-paragraph mm mn fq mo b go ou mq mr gr ov mt mu mv ow mx my mz ox nb nc nd oy nf ng nh fj bk">So, in most contexts, it’s useful (and more common) to run multivariate tests. But, they are slower, less interpretable, and more susceptible to the curse of dimensionality.</p><p id="1c5e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">An interesting effect of PCA transformation is that univariate tests become much more practical. Once PCA transformation is done, there are no associations between the features, and so there is no concept of unusual combinations of values.</p><p id="58fb" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In the scatter plot above (right pane — after the PCA transformation), we can see that Points A and B can both be identified simply as extreme values. Point A is extreme in Component 0; Point B is extreme in Component 1.</p><p id="e1cd" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Which means, we can perform outlier detection effectively using simple statistical tests, such as z-score, IQR, IDR or MAD tests, or using simple tools such as HBOS and ECOD.</p><p id="9511" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Having said that, it’s also possible, after transforming the dataspace using PCA, to still use standard multivariate tests such as Isolation Forest, LOF, or any other standard tools. If these are the tools we most commonly use, there is a convenience to continuing to use them, and to simply first transform the data using PCA as a pre-processing step.</p><p id="4a47" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">One advantage they provide over statistical methods (such as z-score, etc.) is that they automatically provide a single outlier score for each record. If we use z-score tests on each record, and the data has, say, 20 features and we convert this to 10 components (it’s possible to not use all components, as described below), then each record will have 10 outlier scores — one related to how unusual it is in each of the 10 components used. It’s then necessary to combine these scores into a single outlier score. As indicated above, there are simple ways to do this (including taking the mean, median, or maximum z-score for each value per row), but there are some complications doing this (as covered in <a class="af ni" href="https://www.manning.com/books/outlier-detection-in-python" rel="noopener ugc nofollow" target="_blank">Outlier Detection in Python</a>). This is quite manageable, but having a detector provide a single score is convenient as well.</p><h1 id="f507" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Example of outlier detection with PCA</h1><p id="74d6" class="pw-post-body-paragraph mm mn fq mo b go ou mq mr gr ov mt mu mv ow mx my mz ox nb nc nd oy nf ng nh fj bk">We’ll now look at an example using PCA to help better identify outliers in a dataset. To make it easier to see how outlier detection works with PCA, for this example we’ll create two quite straightforward synthetic datasets. We’ll create both with 100,000 rows and 10 features. And we add some known outliers, somewhat similar to Points A and B in the scatter plot above.</p><p id="0058" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We limit the datasets to ten features for simplicity, but as suggested above and in the previous article, there can be strong benefits to using PCA in high-dimensional space, and so (though it’s not covered in this example), more of an advantage to using PCA with, say, hundreds of features, than ten. The datasets used here, though, are reasonably easy to work with and to understand.</p><p id="c10d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The first dataset, data_corr, is created to have strong associations (correlations) between the features. We update the last row to contain some large (but not exceptionally large) values. The main thing is that this row deviates from the normal patterns between the features.</p><p id="0014" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We create another test dataset called data_extreme, which has no associations between the features. The last row of this is modified to contain extreme values in some features.</p><p id="9470" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This allows us to test with two well-understood data distributions as well as well-understood outlier types (we have one outlier in data_corr that ignores the normal correlations between the features; and we have one outlier in data_extreme that has extreme values in some features).</p><p id="72e1" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This example uses several PyOD detectors, which requires first executing:</p><pre class="nj nk nl nm nn ps pt pu bp pv bb bk"><span id="4443" class="pw nz fq pt b bg px py l pz qa">pip install pyod</span></pre><p id="2f12" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The code then starts with creating the first test dataset:</p><pre class="nj nk nl nm nn ps pt pu bp pv bb bk"><span id="4988" class="pw nz fq pt b bg px py l pz qa">import numpy as np<br/>import pandas as pd<br/>from sklearn.decomposition import PCA<br/>from pyod.models.ecod import ECOD<br/>from pyod.models.iforest import IForest<br/>from pyod.models.lof import LOF<br/>from pyod.models.hbos import HBOS<br/>from pyod.models.gmm import GMM<br/>from pyod.models.abod import ABOD<br/>import time<br/><br/>np.random.seed(0)<br/><br/>num_rows = 100_000<br/>num_cols = 10<br/>data_corr = pd.DataFrame({0: np.random.random(num_rows)}) <br/><br/>for i in range(1, num_cols):<br/>  data_corr[i] = data_corr[i-1] + (np.random.random(num_rows) / 10.0)<br/><br/>copy_row = data_corr[0].argmax()<br/>data_corr.loc[num_rows-1, 2] = data_corr.loc[copy_row, 2]<br/>data_corr.loc[num_rows-1, 4] = data_corr.loc[copy_row, 4]<br/>data_corr.loc[num_rows-1, 6] = data_corr.loc[copy_row, 6]<br/>data_corr.loc[num_rows-1, 8] = data_corr.loc[copy_row, 8]<br/><br/>start_time = time.process_time() <br/>pca = PCA(n_components=num_cols)<br/>pca.fit(data_corr)<br/>data_corr_pca = pd.DataFrame(pca.transform(data_corr), <br/> columns=[x for x in range(num_cols)])<br/>print("Time for PCA tranformation:", (time.process_time() - start_time))</span></pre><p id="5be6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We now have the first test dataset, data_corr. When creating this, we set each feature to be the sum of the previous features plus some randomness, so all features are well-correlated. The last row is deliberately set as an outlier. The values are large, though not outside of the existing data. The values in the known outlier, though, do not follow the normal patterns between the features.</p><p id="66f4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We then calculate the PCA transformation of this.</p><p id="079b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We next do this for the other test dataset:</p><pre class="nj nk nl nm nn ps pt pu bp pv bb bk"><span id="5611" class="pw nz fq pt b bg px py l pz qa">np.random.seed(0)<br/> <br/>data_extreme = pd.DataFrame()<br/>for i in range(num_cols):<br/>    data_extreme[i] = np.random.random(num_rows)<br/><br/>copy_row = data_extreme[0].argmax()<br/>data_extreme.loc[num_rows-1, 2] = data_extreme[2].max() * 1.5<br/>data_extreme.loc[num_rows-1, 4] = data_extreme[4].max() * 1.5<br/>data_extreme.loc[num_rows-1, 6] = data_extreme[6].max() * 1.5<br/>data_extreme.loc[num_rows-1, 8] = data_extreme[8].max() * 1.5<br/><br/>start_time = time.process_time() <br/>pca = PCA(n_components=num_cols)<br/>pca.fit(data_corr)<br/>data_extreme_pca = pd.DataFrame(pca.transform(data_corr), <br/>    columns=[x for x in range(num_cols)])<br/><br/>print("Time for PCA tranformation:", (time.process_time() - start_time))</span></pre><p id="7976" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Here each feature is created independently, so there are no associations between the features. Each feature simply follows a uniform distribution. The last row is set as an outlier, having extreme values in features 2, 4, 6, and 8, so in four of the ten features.</p><p id="b46e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We now have both test datasets. We next define a function that, given a dataset and a detector, will train the detector on the full dataset as well as predict on the same data (so will identify the outliers in a single dataset), timing both operations. For the ECOD (empirical cumulative distribution) detector, we add special handling to create a new instance so as not to maintain a memory from previous executions (this is not necessary with the other detectors):</p><pre class="nj nk nl nm nn ps pt pu bp pv bb bk"><span id="586e" class="pw nz fq pt b bg px py l pz qa">def evaluate_detector(df, clf, model_type):<br/>    """<br/>    params:<br/>    df: data to be assessed, in a pandas dataframe<br/>    clf: outlier detector<br/>    model_type: string indicating the type of the outlier detector<br/>    """<br/><br/>    global scores_df<br/> <br/>    if "ECOD" in model_type:<br/>       clf = ECOD()<br/>    start_time = time.process_time()<br/>    clf.fit(df)<br/>    time_for_fit = (time.process_time() - start_time)<br/><br/>    start_time = time.process_time()<br/>    pred = clf.decision_function(df)<br/>    time_for_predict = (time.process_time() - start_time)<br/> <br/>    scores_df[f'{model_type} Scores'] = pred<br/>    scores_df[f'{model_type} Rank'] =\<br/>    scores_df[f'{model_type} Scores'].rank(ascending=False)<br/>  <br/>    print(f"{model_type:&lt;20} Fit Time: {time_for_fit:.2f}")<br/>    print(f"{model_type:&lt;20} Predict Time: {time_for_predict:.2f}")</span></pre><p id="cb87" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The next function defined executes for each dataset, calling the previous method for each. Here we test four cases: using the original data, using the PCA-transformed data, using the first 3 components of the PCA-transformed data, and using the last 3 components. This will tell us how these four cases compare in terms of time and accuracy.</p><pre class="nj nk nl nm nn ps pt pu bp pv bb bk"><span id="c519" class="pw nz fq pt b bg px py l pz qa">def evaluate_dataset_variations(df, df_pca, clf, model_name): <br/>    evaluate_detector(df, clf, model_name)<br/>    evaluate_detector(df_pca, clf, f'{model_name} (PCA)')<br/>    evaluate_detector(df_pca[[0, 1, 2]], clf, f'{model_name} (PCA - 1st 3)')<br/>    evaluate_detector(df_pca[[7, 8, 9]], clf, f'{model_name} (PCA - last 3)')</span></pre><p id="69b1" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As described below, using just the last three components works well here in terms of accuracy, but in other cases, using the early components (or the middle components) can work well. This is included here as an example, but the remainder of the article will focus just on the option of using the last three components.</p><p id="9e05" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The final function defined is called for each dataset. It executes the previous function for each detector tested here. For this example, we use six detectors, each from PyOD (Isolation Forest, LOF, ECOD, HBOS, Gaussian Mixture Models (GMM), and Angle-based Outlier Detector (ABOD)):</p><pre class="nj nk nl nm nn ps pt pu bp pv bb bk"><span id="9c97" class="pw nz fq pt b bg px py l pz qa">def evaluate_dataset(df, df_pca): <br/>  clf = IForest()<br/>  evaluate_dataset_variations(df, df_pca, clf, 'IF')<br/>  <br/>  clf = LOF(novelty=True)<br/>  evaluate_dataset_variations(df, df_pca, clf, 'LOF')<br/><br/>  clf = ECOD()<br/>  evaluate_dataset_variations(df, df_pca, clf, 'ECOD')<br/><br/>  clf = HBOS()<br/>  evaluate_dataset_variations(df, df_pca, clf, 'HBOS')<br/><br/>  clf = GMM()<br/>  evaluate_dataset_variations(df, df_pca, clf, 'GMM')<br/><br/>  clf = ABOD()<br/>  evaluate_dataset_variations(df, df_pca, clf, 'ABOD')</span></pre><p id="9c67" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We finally call the evaluate_dataset() method for both test datasets and print out the top outliers (the known outliers are known to be in the last rows of the two test datasets):</p><pre class="nj nk nl nm nn ps pt pu bp pv bb bk"><span id="4b5b" class="pw nz fq pt b bg px py l pz qa"># Test the first dataset<br/># scores_df stores the outlier scores given to each record by each detector<br/>scores_df = data_corr.copy()<br/>evaluate_dataset(data_corr, data_corr_pca)<br/>rank_columns = [x for x in scores_df.columns if type(x) == str and 'Rank' in x]<br/>print(scores_df[rank_columns].tail())<br/><br/># Test the second dataset<br/>scores_df = data_extreme.copy()<br/>evaluate_dataset(data_extreme, data_extreme_pca)<br/>rank_columns = [x for x in scores_df.columns if type(x) == str and 'Rank' in x]<br/>print(scores_df[rank_columns].tail())</span></pre><p id="589e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">There are several interesting results. We look first at the fit times for the data_corr dataset, shown in table below (the fit and predict times for the other test set were similar, so not shown here). The tests were conducted on Google colab, with the times shown in seconds. We see that different detectors have quite different times. ABOD is significantly slower than the others, and HBOS considerably faster. The other univariate detector included here, ECOD, is also very fast.</p><p id="b987" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The times to fit the PCA-transformed data are about the same as the original data, which makes sense given this data is the same size: we converted the 10 features to 10 components, which are equivalent, in terms of time, to process.</p><p id="ed13" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We also test using only the last three PCA components (components 7, 8, and 9), and the fit times are drastically reduced in some cases, particularly for local outlier factor (LOF). Compared to using all 10 original features (19.4s), or using all 10 PCA components (16.9s), using 3 components required only 1.4s. In all cases as well, other than Isolation Forest, there is a notable drop in fit time.</p><figure class="nj nk nl nm nn no np nq paragraph-image"><div class="np nq qb"><img src="../Images/f791b1b6c8550f23b69b04cba6b1425e.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*K1jP-b08W8ycCZk68UXY9A.png"/></div><figcaption class="nt nu nv np nq nw nx bf b bg z dx">Fit times for 6 PyOD detectors on the first test dataset, data_corr.</figcaption></figure><p id="c98a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In the next table, we see the predict times for the data_corr dataset (the times for the other test set were similar here as well). Again, we see a very sizable drop in prediction times using just three components, especially for LOF. We also see again that the two univariate detectors, HBOS and ECOD were among the fastest, though GMM is as fast or faster in the case of prediction (though slightly slower in terms of fit time).</p><p id="e96b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">With Isolation Forest (IF), as we train the same number of trees regardless of the number of features, and pass all records to be evaluated through the same set of trees, the times are unaffected by the number of features. For all other detectors shown here, however, the number of features is very relevant: all others show a significant drop in predict time when using 3 components compared to all 10 original features or all 10 components.</p><figure class="nj nk nl nm nn no np nq paragraph-image"><div class="np nq qc"><img src="../Images/71090f8118234923ebff0b322089a10f.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*2CJDPyGgV2HvhU32zn27GA.png"/></div><figcaption class="nt nu nv np nq nw nx bf b bg z dx">Predict times for 6 PyOD detectors on the first dataset, data_corr</figcaption></figure><p id="3750" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In terms of accuracy, all five detectors performed well on the two datasets most of the time, in terms of assigning the highest outlier score to the last row, which, for both test datasets, is the one known outlier. The results are shown in the next table. There are two rows, one for each dataset. For each, we show the rank assigned by each detector to the one known outlier. Ideally, all detectors would assign this rank 1 (the highest outlier score).</p><p id="381f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In most cases, the last row was, in fact, given the highest or nearly highest rank, with the exception of IF, ECOD, and HBOS on the first dataset. This is a good example where even strong detectors such as IF can occasionally do poorly even for clear outliers.</p><figure class="nj nk nl nm nn no np nq paragraph-image"><div class="np nq qd"><img src="../Images/23d2232bbf72b0e91ad348856dc06b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*4kxH7kqEhJ6ad5pfyTPt_g.png"/></div><figcaption class="nt nu nv np nq nw nx bf b bg z dx">Rank assigned to the one known outlier in both test datasets using 6 PyOD detectors when executed on the original data.</figcaption></figure><p id="1a9c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For the first dataset, ECOD and HBOS completely miss the outlier, but this is as expected, as it is an outlier based on a combination of values (it ignores the normal linear relationship among the features), which univariate tests are unable to detect. The second dataset’s outlier is based on extreme values, which both univariate and multivariate tests are typically able to detect reliably, and can do so here.</p><p id="41a4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We see a drastic improvement in accuracy when using PCA for these datasets and these detectors, shown in the next table. This is not always the case, but it does hold true here. When the detectors execute on the PCA-transformed data, all 6 detectors rank the known outlier the highest on both datasets. When data is PCA-transformed, the components are all unassociated with each other; the outliers are the extreme values, which are much easier to identify.</p><figure class="nj nk nl nm nn no np nq paragraph-image"><div class="np nq qd"><img src="../Images/f96745007f69c81ad2309ad048438f51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*-1ZiGzZxUD8T0GkCoTZEww.png"/></div><figcaption class="nt nu nv np nq nw nx bf b bg z dx">Rank assigned to the one known outlier in both test datasets using 6 PyOD detectors when executed on the PCA-transformed data, using all 10 components.</figcaption></figure><p id="77ae" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Also interesting is that only the last three components are necessary to rank the known outliers as the top outliers, shown in the table here.</p><figure class="nj nk nl nm nn no np nq paragraph-image"><div class="np nq qd"><img src="../Images/f96745007f69c81ad2309ad048438f51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*-1ZiGzZxUD8T0GkCoTZEww.png"/></div><figcaption class="nt nu nv np nq nw nx bf b bg z dx">Similar to the previous table, but using only 3 PCA components.</figcaption></figure><p id="2e1a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">And, as we saw above, fit and predict times are substantially shorter in these cases. This is where we can achieve significant performance improvements using PCA: it’s often necessary to use only a small number of the components.</p><p id="f634" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Using only a small set of components will also reduce memory requirements. This is not always an issue, but often when working with large datasets, this can be an important consideration.</p><p id="3072" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This experiment covered two of the main types of outliers we can have with data: extreme values and values that deviate from a linear pattern, both of which are identifiable in the later components. In these cases, using the last three components worked well.</p><p id="03e8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It can vary how many components to use, and which components are best to use, and some experimentation will be needed (likely best discovered using <a class="af ni" href="https://medium.com/towards-data-science/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4" rel="noopener">doped data</a>). In some cases, it may be preferable (in terms of execution time, detecting the relevant outliers reliably, and reducing noise) to use the earlier components, in some cases the middle, and in some cases the later. As we can see in the scatter plot at the beginning of this article, different components can tend to highlight different types of outlier.</p><h1 id="805c" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Improving the outlier detection system over time</h1><p id="d6a0" class="pw-post-body-paragraph mm mn fq mo b go ou mq mr gr ov mt mu mv ow mx my mz ox nb nc nd oy nf ng nh fj bk">Another useful benefit of working with PCA components is that it can make it easier to tune the outlier detection system over time. Often with outlier detection, the system is run not just once on a single dataset, but on an ongoing basis, so constantly assessing new data as it arrives (for example, new financial transactions, sensor readings, web site logs, network logs, etc.), and over time we gain a better sense of what outliers are most relevant to us, and which are being under- and over-reported.</p><p id="a33d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As the outliers reported when working with PCA-transformed data all relate to a single component, we can see how many relevant and irrelevant outliers being reported are associated with each component. This can be particularly easy when using simple univariate tests on each component, like z-score, IQR, IDR, MAD-based tests, and similar tests.</p><p id="51a6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Over time, we can learn to weight outliers associated with some components more highly and other components lower (depending on our tolerance for false positive and false negatives).</p><h1 id="b86e" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Visualization</h1><p id="5761" class="pw-post-body-paragraph mm mn fq mo b go ou mq mr gr ov mt mu mv ow mx my mz ox nb nc nd oy nf ng nh fj bk">Dimensionality reduction also has some advantages in that it can help visualize the outliers, particularly where we reduce the data to two or three dimensions. Though, as with the original features, even where there are more than three dimensions, we can view the PCA components one at a time in the form of histograms, or two at a time in scatter plots.</p><p id="22ba" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, inspecting the last two components of the first test dataset, data_corr (which contained unusual combinations of values) we can see the known outlier clearly, as shown below. However, it’s somewhat questionable how informative this is, as the components themselves are difficult to understand.</p><figure class="nj nk nl nm nn no np nq paragraph-image"><div class="np nq qe"><img src="../Images/a7e1089037dfa55b4e0f7ee2e89532ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*i04ELoVuFa-6dX9txx4E7g.png"/></div><figcaption class="nt nu nv np nq nw nx bf b bg z dx">Scatterplot of the last two components (components 8 and 9) of the PCA transformation of the first dataset, which contained an unusual combination of values. Here we see a single point in the top-right of the space. It is clear that the point is a strong outlier, though it is not as clear what components 8 and 9 represent.</figcaption></figure><h1 id="ede8" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Conclusions</h1><p id="a795" class="pw-post-body-paragraph mm mn fq mo b go ou mq mr gr ov mt mu mv ow mx my mz ox nb nc nd oy nf ng nh fj bk">This article covered PCA, but there are other dimensionality reduction tools that can be similarly used, including t-SNE (as with PCA, this is provided in scikit-learn), <a class="af ni" href="https://github.com/lmcinnes/umap" rel="noopener ugc nofollow" target="_blank">UMAP</a>, and auto-encoders (also covered in <a class="af ni" href="https://www.manning.com/books/outlier-detection-in-python" rel="noopener ugc nofollow" target="_blank">Outlier Detection in Python</a>).</p><p id="6abf" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As well, using PCA, methods based on reconstruction error (measuring how well the values of a record can be approximated using only a subset of the components) can be very effective and is often worth investigating, as covered in the <a class="af ni" href="https://medium.com/towards-data-science/using-pca-for-outlier-detection-afecab4d2b78" rel="noopener">previous article</a> in this series.</p><p id="24a5" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This article covered using standard outlier detectors (though, as demonstrated, this can more readily include simple univariate outlier detectors than is normally possible) for outlier detection, showing the benefits of first transforming the data using PCA.</p><p id="f5c5" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">How well this process will work depends on the data (for example, PCA relies on there being strong linear relationships between the features, and can breakdown if the data is heavily clustered) and the types of outliers you’re interested in finding. It’s usually necessary to use <a class="af ni" href="https://medium.com/towards-data-science/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4" rel="noopener">doping</a> or other forms of testing to determine how well this works, and to tune the process — particularly determining which components are used. Where there are no constraints related to execution time or memory limits though, it can be a good starting point to simply use all components and weight them equally.</p><p id="74f4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As well, in outlier detection, usually no single outlier detection process will reliably identify all the types of outliers you’re interested in (especially where you’re interested in finding all records that can be reasonably considered statistically unusual in one way or another), and so multiple outlier detection methods generally need to be used. Combining PCA-based outlier detection with other methods can cover a wider range of outliers than can be detected using just PCA-based methods, or just methods without PCA transformations.</p><p id="e57d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">But, where PCA-based methods work well, they can often provide more accurate detection, as the outliers are often better separated and easier to detect.</p><p id="713e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">PCA-based methods can also execute more quickly (particularly where they’re sufficient and do not need to be combined with other methods), because: 1) simpler (and faster) detectors such as z-score, IQR, HBOS and ECOD can be used; and 2) fewer components may be used. The PCA transformations themselves are generally extremely fast, with times almost negligible compared to fitting or executing outlier detection.</p><p id="8d2a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Using PCA, at least where only a subset of the components are necessary, can also reduce memory requirements, which can be an issue when working with particularly large datasets.</p><p id="6415" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">All images by author</p></div></div></div></div>    
</body>
</html>