<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Feature Engineering with Microsoft Fabric and PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Feature Engineering with Microsoft Fabric and PySpark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-engineering-with-microsoft-fabric-and-pyspark-16d458018744?source=collection_archive---------9-----------------------#2024-04-08">https://towardsdatascience.com/feature-engineering-with-microsoft-fabric-and-pyspark-16d458018744?source=collection_archive---------9-----------------------#2024-04-08</a></blockquote><div><div class="em ff fg fh fi fj"/><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><div/><div><h2 id="93f8" class="pw-subtitle-paragraph go fq fr bf b gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cq dx">Fabric Madness part 2</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@roger_noble?source=post_page---byline--16d458018744--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Roger Noble" class="l ep by dd de cx" src="../Images/869b5b0f237f24b119ca6c41c2e31162.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*DSDhBVvFKAUKXJfpbO5beg.jpeg"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--16d458018744--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@roger_noble?source=post_page---byline--16d458018744--------------------------------" rel="noopener follow">Roger Noble</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--16d458018744--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk lb lc ab q ee ld le" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lf"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap ie li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/e76cf994c16355962435cec1d006594b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xBVdP3mItqXf4DOFdlnkJQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author and ChatGPT. “Design an illustration, focusing on a basketball player in action, this time the theme is on using pyspark to generate features for machine leaning models in a graphic novel style” prompt. ChatGPT, 4, OpenAI, 4 April. 2024. <a class="af nc" href="https://chat.openai.com./" rel="noopener ugc nofollow" target="_blank">https://chat.openai.com.</a></figcaption></figure><p id="cd47" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk"><em class="nz">A Huge thanks to </em><a class="af nc" href="https://medium.com/@mgrc99" rel="noopener"><em class="nz">Martim Chaves</em></a><em class="nz"> who co-authored this post and developed the example scripts.</em></p><p id="539c" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In our <a class="af nc" href="https://medium.com/towards-data-science/fabric-madness-96b84dc5f241" rel="noopener">previous post</a> we took a high level view of how to train a machine learning model in <a class="af nc" href="https://www.microsoft.com/en-us/microsoft-fabric" rel="noopener ugc nofollow" target="_blank">Microsoft Fabric</a>. In this post we wanted to dive deeper into the process of feature engineering.</p><p id="58a6" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Feature engineering is a crucial part of the development lifecycle for any Machine Learning (ML) systems. It is a step in the development cycle where raw data is processed to better represent its underlying structure and provide additional information that enhance our ML models. Feature engineering is both an art and a science. Even though there are specific steps that we can take to create good features, sometimes, it is only through experimentation that good results are achieved. Good features are crucial in guaranteeing a good system performance.</p><p id="82a1" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">As datasets grow exponentially, traditional feature engineering may struggle with the size of very large datasets. This is where PySpark can help — as it is a scalable and efficient processing platform for massive datasets. A great thing about Fabric is that it makes using PySpark easy!</p><p id="0c73" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In this post, we’ll be going over:</p><ul class=""><li id="192f" class="nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny oa ob oc bk">How does PySpark Work?</li><li id="d6c8" class="nd ne fr nf b gp od nh ni gs oe nk nl nm of no np nq og ns nt nu oh nw nx ny oa ob oc bk">Basics of PySpark</li><li id="aed0" class="nd ne fr nf b gp od nh ni gs oe nk nl nm of no np nq og ns nt nu oh nw nx ny oa ob oc bk">Feature Engineering in Action</li></ul><p id="5c87" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">By the end of this post, hopefully you’ll feel comfortable carrying out feature engineering with PySpark in Fabric. Let’s get started!</p><h1 id="57de" class="oi oj fr bf ok ol om gr on oo op gu oq or os ot ou ov ow ox oy oz pa pb pc pd bk">How does PySpark work?</h1><p id="b0c4" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">Spark is a distributed computing system that allows for the processing of large datasets with speed and efficiency across a cluster of machines. It is built around the concept of a Resilient Distributed Dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. RDDs are the fundamental data structure of Spark, and they allow for the distribution of data across a cluster of machines.</p><p id="56db" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">PySpark is the Python API for Spark. It allows for the creation of Spark DataFrames, which are similar to Pandas DataFrames, but with the added benefit of being distributed across a cluster of machines. PySpark DataFrames are the core data structure in PySpark, and they allow for the manipulation of large datasets in a distributed manner.</p><p id="84b2" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">At the core of PySpark is the <code class="cx pj pk pl pm b">SparkSession</code> object, which is what fundamentally interacts with Spark. This <code class="cx pj pk pl pm b">SparkSession</code> is what allows for the creation of DataFrames, and other functionalities. Note that, when running a Notebook in Fabric, a <code class="cx pj pk pl pm b">SparkSession</code> is automatically created for you, so you don't have to worry about that.</p><p id="51ca" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Having a rough idea of how PySpark works, let’s get to the basics.</p><h1 id="61f8" class="oi oj fr bf ok ol om gr on oo op gu oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Basics of PySpark</h1><p id="b158" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">Although Spark DataFrames may remind us of Pandas DataFrames due to their similarities, the syntax when using PySpark can be a bit different. In this section, we’ll go over some of the basics of PySpark, such as reading data, combining DataFrames, selecting columns, grouping data, joining DataFrames, and using functions.</p><h2 id="2bbc" class="pn oj fr bf ok po pp pq on pr ps pt oq nm pu pv pw nq px py pz nu qa qb qc qd bk">The Data</h2><p id="0e39" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">The data we are looking at is from the 2024 US college basketball tournaments, which was obtained from the on-going <em class="nz">March Machine Learning Mania 2024</em> Kaggle competition, the details of which can be found <a class="af nc" href="https://www.kaggle.com/competitions/march-machine-learning-mania-2024/overview" rel="noopener ugc nofollow" target="_blank">here</a>, and is licensed under CC BY 4.0 [1]</p><h2 id="9d8d" class="pn oj fr bf ok po pp pq on pr ps pt oq nm pu pv pw nq px py pz nu qa qb qc qd bk">Reading data</h2><p id="396b" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">As mentioned in the <a class="af nc" href="https://medium.com/towards-data-science/fabric-madness-96b84dc5f241" rel="noopener">previous post</a> of this series, the first step is usually to create a Lakehouse and upload some data. Then, when creating a Notebook, we can attach it to the created Lakehouse, and we’ll have access to the data stored there.</p><p id="bb20" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">PySpark Dataframes can read various data formats, such as CSV, JSON, Parquet, and others. Our data is stored in CSV format, so we’ll be using that, like in the following code snippet:</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="28bd" class="qh oj fr pm b bg qi qj l qk ql"># Read women's data<br/>w_data = (<br/>    spark.read.option("header", True)<br/>    .option("inferSchema", True)<br/>    .csv(f"Files/WNCAATourneyDetailedResults.csv")<br/>    .cache()<br/>)</span></pre><p id="5c2a" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In this code snippet, we’re reading the detailed results data set of the final women’s basketball college tournament matches. Note that the <code class="cx pj pk pl pm b">"header"</code> option being true means that the names of the columns will be derived from the first row of the CSV file. The <code class="cx pj pk pl pm b">inferSchema</code> option tells Spark to guess the data types of the columns - otherwise they would all be read as strings. <code class="cx pj pk pl pm b">.cache()</code> is used to keep the DataFrame in memory.</p><p id="036d" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">If you’re coming from Pandas, you may be wondering what the equivalent of <code class="cx pj pk pl pm b">df.head()</code> is for PySpark - it's <code class="cx pj pk pl pm b">df.show(5)</code>. The default for <code class="cx pj pk pl pm b">.show()</code> is the top 20 rows, hence the need to specifically select 5.</p><h2 id="65c2" class="pn oj fr bf ok po pp pq on pr ps pt oq nm pu pv pw nq px py pz nu qa qb qc qd bk">Combining DataFrames</h2><p id="29a2" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">Combining DataFrames can be done in multiple ways. The first we will look at is a union, where the columns are the same for both DataFrames:</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="b4b8" class="qh oj fr pm b bg qi qj l qk ql"># Read women's data<br/>...<br/><br/># Read men's data<br/>m_data = (<br/>    spark.read.option("header", True)<br/>    .option("inferSchema", True)<br/>    .csv(f"Files/MNCAATourneyDetailedResults.csv")<br/>    .cache()<br/>)<br/><br/># Combine (union) the DataFrames<br/>combined_results = m_data.unionByName(w_data)</span></pre><p id="0f8d" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Here, <code class="cx pj pk pl pm b">unionByName</code> joins the two DataFrames by matching the names of the columns. Since both the women's and the men's <em class="nz">detailed match results</em> have the same columns, this is a good approach. Alternatively, there's also <code class="cx pj pk pl pm b">union</code>, which combines two DataFrames, matching column positions.</p><h2 id="e061" class="pn oj fr bf ok po pp pq on pr ps pt oq nm pu pv pw nq px py pz nu qa qb qc qd bk">Selecting Columns</h2><p id="6adc" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">Selecting columns from a DataFrame in PySpark can be done using the <code class="cx pj pk pl pm b">.select()</code> method. We just have to indicate the name or names of the columns that are relevant as a parameter.</p><p id="0c3b" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Here’s the output for <code class="cx pj pk pl pm b">w_scores.show(5)</code>:</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="4af1" class="qh oj fr pm b bg qi qj l qk ql"># Selecting a single column<br/>w_scores = w_data.select("WScore")<br/><br/># Selecting multiple columns<br/>teamid_w_scores = w_data.select("WTeamID", "WScore")<br/>```<br/><br/>Here's the output for `w_scores.show(5)`:<br/>```<br/>+------+<br/>|Season|<br/>+------+<br/>|  2010|<br/>|  2010|<br/>|  2010|<br/>|  2010|<br/>|  2010|<br/>+------+<br/>only showing top 5 rows</span></pre><p id="a400" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The columns can also be renamed when being selected using the <code class="cx pj pk pl pm b">.alias()</code> method:</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="e8a2" class="qh oj fr pm b bg qi qj l qk ql">winners = w_data.select(<br/>    w_data.WTeamID.alias("TeamID"),<br/>    w_data.WScore.alias("Score")<br/>)</span></pre><h2 id="38fb" class="pn oj fr bf ok po pp pq on pr ps pt oq nm pu pv pw nq px py pz nu qa qb qc qd bk">Grouping Data</h2><p id="1d32" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">Grouping allows us to carry out certain operations for the groups that exist within the data and is usually combined with a aggregation functions. We can use <code class="cx pj pk pl pm b">.groupBy()</code> for this:</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="8917" class="qh oj fr pm b bg qi qj l qk ql"># Grouping and aggregating<br/>winners_average_scores = winners.groupBy("TeamID").avg("Score")</span></pre><p id="8973" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In this example, we are grouping by <code class="cx pj pk pl pm b">"TeamID"</code>, meaning we're considering the groups of rows that have a distinct value for <code class="cx pj pk pl pm b">"TeamID"</code>. For each of those groups, we're calculating the average of the <code class="cx pj pk pl pm b">"Score"</code>. This way, we get the average score for each team.</p><p id="8094" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Here’s the output of <code class="cx pj pk pl pm b">winners_average_scores.show(5)</code>, showing the average score of each team:</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="97a5" class="qh oj fr pm b bg qi qj l qk ql">+------+-----------------+<br/>|TeamID|       avg(Score)|<br/>+------+-----------------+<br/>|  3125|             68.5|<br/>|  3345|             74.2|<br/>|  3346|79.66666666666667|<br/>|  3376|73.58333333333333|<br/>|  3107|             61.0|<br/>+------+-----------------+</span></pre><h2 id="bc2b" class="pn oj fr bf ok po pp pq on pr ps pt oq nm pu pv pw nq px py pz nu qa qb qc qd bk">Joining Data</h2><p id="3790" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">Joining two DataFrames can be done using the <code class="cx pj pk pl pm b">.join()</code> method. Joining is essentially extending the DataFrame by adding the columns of one DataFrame to another.</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="bce0" class="qh oj fr pm b bg qi qj l qk ql"># Joining on Season and TeamID<br/>final_df = matches_df.join(stats_df, on=['Season', 'TeamID'], how='left')</span></pre><p id="e082" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In this example, both <code class="cx pj pk pl pm b">stats_df</code> and <code class="cx pj pk pl pm b">matches_df</code> were using <code class="cx pj pk pl pm b">Season</code> and <code class="cx pj pk pl pm b">TeamID</code> as unique identifiers for each row. Besides <code class="cx pj pk pl pm b">Season</code> and <code class="cx pj pk pl pm b">TeamID</code>, <code class="cx pj pk pl pm b">stats_df</code> has other columns, such as statistics for each team during each season, whereas <code class="cx pj pk pl pm b">matches_df</code> has information about the matches, such as date and location. This operation allows us to add those interesting statistics to the matches information!</p><h2 id="8edf" class="pn oj fr bf ok po pp pq on pr ps pt oq nm pu pv pw nq px py pz nu qa qb qc qd bk">Functions</h2><p id="9154" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">There are several functions that PySpark provides that help us transform DataFrames. You can find the full list <a class="af nc" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="24ad" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Here’s an example of a simple function:</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="a5bf" class="qh oj fr pm b bg qi qj l qk ql">from pyspark.sql import functions as F<br/><br/>w_data = w_data.withColumn("HighScore", F.when(F.col("Score") &gt; 80, "Yes").otherwise("No"))</span></pre><p id="0b3a" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In the code snippet above, a <code class="cx pj pk pl pm b">"HighScore"</code> column is created when the score is higher than 80. For each row in the <code class="cx pj pk pl pm b">"Score"</code> column (indicated by the <code class="cx pj pk pl pm b">.col()</code> function), the value <code class="cx pj pk pl pm b">"Yes"</code> is chosen for the <code class="cx pj pk pl pm b">"HighScore"</code> column if the <code class="cx pj pk pl pm b">"Score"</code> value is larger than 80, determined by the <code class="cx pj pk pl pm b">.when()</code> function. <code class="cx pj pk pl pm b">.otherwise()</code>, the value chosen is <code class="cx pj pk pl pm b">"No"</code>.</p><h1 id="4a9b" class="oi oj fr bf ok ol om gr on oo op gu oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Feature Engineering in Action</h1><p id="e0ae" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">Now that we have a basic understanding of PySpark and how it can be used, let’s go over how the regular season statistics features were created. These features were then used as inputs into our machine learning model to try to predict the outcome of the final tournament games.</p><p id="e9c1" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The starting point was a DataFrame, <code class="cx pj pk pl pm b">regular_data</code>, that contained match by match statistics for the <em class="nz">regular seasons</em>, which is the United States College Basketball Season that happens from November to March each year.</p><p id="2010" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Each row in this DataFrame contained the season, the day the match was held, the ID of team 1, the ID of team 2, and other information such as the location of the match. Importantly, it also contained statistics for <em class="nz">each team</em> for that <em class="nz">specific match</em>, such as <code class="cx pj pk pl pm b">"T1_FGM"</code>, meaning the Field Goals Made (FGM) for team 1, or <code class="cx pj pk pl pm b">"T2_OR"</code>, meaning the Offensive Rebounds (OR) of team 2.</p><p id="a351" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The first step was selecting which columns would be used. These were columns that strictly contained in-game statistics.</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="81ba" class="qh oj fr pm b bg qi qj l qk ql"># Columns that we'll want to get statistics from<br/>boxscore_cols = [<br/>    'T1_FGM', 'T1_FGA', 'T1_FGM3', 'T1_FGA3', 'T1_OR', 'T1_DR', 'T1_Ast', 'T1_Stl', 'T1_PF', <br/>    'T2_FGM', 'T2_FGA', 'T2_FGM3', 'T2_FGA3', 'T2_OR', 'T2_DR', 'T2_Ast', 'T2_Stl', 'T2_PF'<br/>]</span></pre><p id="8fc9" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">If you’re interested, here’s what each statistic’s code means:</p><ul class=""><li id="581d" class="nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny oa ob oc bk">FGM: Field Goals Made</li><li id="bfa1" class="nd ne fr nf b gp od nh ni gs oe nk nl nm of no np nq og ns nt nu oh nw nx ny oa ob oc bk">FGA: Field Goals Attempted</li><li id="c050" class="nd ne fr nf b gp od nh ni gs oe nk nl nm of no np nq og ns nt nu oh nw nx ny oa ob oc bk">FGM3: Field Goals Made from the 3-point-line</li><li id="e492" class="nd ne fr nf b gp od nh ni gs oe nk nl nm of no np nq og ns nt nu oh nw nx ny oa ob oc bk">FGA3: Field Goals Attempted for 3-point-line goals</li><li id="0a85" class="nd ne fr nf b gp od nh ni gs oe nk nl nm of no np nq og ns nt nu oh nw nx ny oa ob oc bk">OR: Offensive Rebounds. A rebounds is when the ball rebounds from the board when a goal is attempted, not getting in the net. If the team that <em class="nz">attempted</em> the goal gets possession of the ball, it’s called an “Offensive” rebound. Otherwise, it’s called a “Defensive” Rebound.</li><li id="2e34" class="nd ne fr nf b gp od nh ni gs oe nk nl nm of no np nq og ns nt nu oh nw nx ny oa ob oc bk">DR: Defensive Rebounds</li><li id="33bf" class="nd ne fr nf b gp od nh ni gs oe nk nl nm of no np nq og ns nt nu oh nw nx ny oa ob oc bk">Ast: Assist, a pass that led directly to a goal</li><li id="29e8" class="nd ne fr nf b gp od nh ni gs oe nk nl nm of no np nq og ns nt nu oh nw nx ny oa ob oc bk">Stl: Steal, when the possession of the ball is stolen</li><li id="8a29" class="nd ne fr nf b gp od nh ni gs oe nk nl nm of no np nq og ns nt nu oh nw nx ny oa ob oc bk">PF: Personal Foul, when a player makes a foul</li></ul><p id="b2f1" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">From there, a dictionary of <em class="nz">aggregation expressions</em> was created. Basically, for each column name in the previous list of columns, a function was stored that would calculate the mean of the column, and rename it, by adding a suffix, <code class="cx pj pk pl pm b">"mean"</code>.</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="0773" class="qh oj fr pm b bg qi qj l qk ql">from pyspark.sql import functions as F<br/>from pyspark.sql.functions import col  # select a column<br/><br/>agg_exprs = {col: F.mean(col).alias(col + 'mean') for col in boxscore_cols}</span></pre><p id="04f6" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Then, the data was grouped by <code class="cx pj pk pl pm b">"Season"</code> and <code class="cx pj pk pl pm b">"T1_TeamID"</code>, and the aggregation functions of the previously created dictionary were used as the argument for <code class="cx pj pk pl pm b">.agg()</code>.</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="abfa" class="qh oj fr pm b bg qi qj l qk ql">season_statistics = regular_data.groupBy(["Season", "T1_TeamID"]).agg(*agg_exprs.values())</span></pre><p id="c1e3" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Note that the grouping was done by season and the <strong class="nf fs">ID of team 1</strong> — this means that <code class="cx pj pk pl pm b">"T2_FGAmean"</code>, for example, will actually be the mean of the Field Goals Attempted made by the <strong class="nf fs">opponents</strong> of T1, not necessarily of a specific team. So, we actually need to rename the columns that are something like <code class="cx pj pk pl pm b">"T2_FGAmean"</code> to something like <code class="cx pj pk pl pm b">"T1_opponent_FGAmean"</code>.</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="933c" class="qh oj fr pm b bg qi qj l qk ql"># Rename columns for T1<br/>for col in boxscore_cols:<br/>    season_statistics = season_statistics.withColumnRenamed(col + 'mean', 'T1_' + col[3:] + 'mean') if 'T1_' in col \<br/>        else season_statistics.withColumnRenamed(col + 'mean', 'T1_opponent_' + col[3:] + 'mean')</span></pre><p id="9cc2" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">At this point, it’s important to mention that the <code class="cx pj pk pl pm b">regular_data</code> DataFrame actually has <strong class="nf fs">two</strong> rows per each match that occurred. This is so that both teams can be "T1" and "T2", for each match. This little "trick" is what makes these statistics useful.</p><p id="8be6" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Note that we “only” have the statistics for “T1”. We “need” the statistics for “T2” as well — “need” in quotations because there are no new statistics being calculated. We just need the same data, but with the columns having different names, so that for a match with “T1” and “T2”, we have statistics for both T1 and T2. So, we created a mirror DataFrame, where, instead of “T1&amp;mldr;mean” and “T1_opponent_&amp;mldr;mean”, we have “T2&amp;mldr;mean” and “T2_opponent_&amp;mldr;mean”. This is important because, later on, when we’re joining these regular season statistics to tournament matches, we’ll be able to have statistics for both team 1 <strong class="nf fs">and</strong> team 2.</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="13e0" class="qh oj fr pm b bg qi qj l qk ql">season_statistics_T2 = season_statistics.select(<br/>    *[F.col(col).alias(col.replace('T1_opponent_', 'T2_opponent_').replace('T1_', 'T2_')) if col not in ['Season'] else F.col(col) for col in season_statistics.columns]<br/>)</span></pre><p id="2a40" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Now, there are two DataFrames, with season statistics for “both” T1 and T2. Since the final DataFrame will contain the “Season”, the “T1TeamID” and the “T2TeamID”, we can join these newly created features with a join!</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="c5a7" class="qh oj fr pm b bg qi qj l qk ql">tourney_df = tourney_df.join(season_statistics, on=['Season', 'T1_TeamID'], how='left')<br/>tourney_df = tourney_df.join(season_statistics_T2, on=['Season', 'T2_TeamID'], how='left')</span></pre><h2 id="b48d" class="pn oj fr bf ok po pp pq on pr ps pt oq nm pu pv pw nq px py pz nu qa qb qc qd bk">Elo Ratings</h2><p id="df20" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">First created by <a class="af nc" href="https://en.wikipedia.org/wiki/Elo_rating_system" rel="noopener ugc nofollow" target="_blank">Arpad Elo</a>, Elo is a rating system for zero-sum games (games where one player wins and the other loses), like basketball. With the Elo rating system, each team has an Elo rating, a value that generally conveys the team’s quality. At first, every team has the same Elo, and whenever they win, their Elo increases, and when they lose, their Elo decreases. A key characteristic of this system is that this value increases more with a win against a strong opponent than with a win against a weak opponent. Thus, it can be a very useful feature to have!</p><p id="806b" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">We wanted to capture the Elo rating of a team at the end of the regular season, and use that as feature for the tournament. To do this, we calculated the Elo for each team on a per match basis. To calculate Elo for this feature, we found it more straightforward to use Pandas.</p><p id="6e62" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Central to Elo is calculating the expected score for each team. It can be described in code like so:</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="34fa" class="qh oj fr pm b bg qi qj l qk ql"># Function to calculate expected score<br/>def expected_score(ra, rb):<br/>    # ra = rating (Elo) team A<br/>    # rb = rating (Elo) team B<br/>    # Elo function<br/>    return 1 / (1 + 10 ** ((rb - ra) / 400))</span></pre><p id="6d17" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Considering a team A and a team B, this function computes the expected score of team A against team B.</p><p id="bd8f" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">For each match, we would update the teams’ Elos. Note that the location of the match also played a part — winning at home was considered less impressive than winning away.</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="c99c" class="qh oj fr pm b bg qi qj l qk ql"># Function to update Elo ratings, keeping T1 and T2 terminology<br/>def update_elo(t1_elo, t2_elo, location, T1_Score, T2_Score):<br/>    expected_t1 = expected_score(t1_elo, t2_elo)<br/>    expected_t2 = expected_score(t2_elo, t1_elo)<br/>    <br/>    actual_t1 = 1 if T1_Score &gt; T2_Score else 0<br/>    actual_t2 = 1 - actual_t1<br/><br/>    # Determine K based on game location<br/>    # The larger the K, the bigger the impact<br/>    # team1 winning at home (location=1) less impressive than winning away (location = -1)<br/>    if actual_t1 == 1:  # team1 won<br/>        if location == 1:<br/>            k = 20<br/>        elif location == 0:<br/>            k = 30<br/>        else:  # location = -1<br/>            k = 40<br/>    else:  # team2 won<br/>        if location == 1:<br/>            k = 40<br/>        elif location == 0:<br/>            k = 30<br/>        else:  # location = -1<br/>            k = 20<br/>    <br/>    new_t1_elo = t1_elo + k * (actual_t1 - expected_t1)<br/>    new_t2_elo = t2_elo + k * (actual_t2 - expected_t2)<br/>    <br/>    return new_t1_elo, new_t2_elo</span></pre><p id="41aa" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">To apply the Elo rating system, we iterated through each season’s matches, initializing teams with a base rating and updating their ratings match by match. The final Elo available for each team in each season will, hopefully, be a good descriptor of the team’s quality.</p><pre class="mm mn mo mp mq qe pm qf bp qg bb bk"><span id="a243" class="qh oj fr pm b bg qi qj l qk ql">def calculate_elo_through_seasons(regular_data):<br/><br/>    # For this feature, using Pandas<br/>    regular_data = regular_data.toPandas()<br/>    <br/>    # Set value of initial elo<br/>    initial_elo = 1500<br/><br/>    # DataFrame to collect final Elo ratings<br/>    final_elo_list = []<br/><br/>    for season in sorted(regular_data['Season'].unique()):<br/>        print(f"Season: {season}")<br/>        # Initialize elo ratings dictionary<br/>        elo_ratings = {}<br/><br/>        print(f"Processing Season: {season}")<br/>        # Get the teams that played in the season<br/>        season_teams = set(regular_data[regular_data['Season'] == season]['T1_TeamID']).union(set(regular_data[regular_data['Season'] == season]['T2_TeamID']))<br/>        <br/>        # Initialize season teams' Elo ratings<br/>        for team in season_teams:<br/>            if (season, team) not in elo_ratings:<br/>                elo_ratings[(season, team)] = initial_elo<br/><br/>        # Update Elo ratings per game<br/>        season_games = regular_data[regular_data['Season'] == season]<br/>        for _, row in season_games.iterrows():<br/>            t1_elo = elo_ratings[(season, row['T1_TeamID'])]<br/>            t2_elo = elo_ratings[(season, row['T2_TeamID'])]<br/><br/>            new_t1_elo, new_t2_elo = update_elo(t1_elo, t2_elo, row['location'], row['T1_Score'], row['T2_Score'])<br/>            <br/>            # Only keep the last season rating<br/>            elo_ratings[(season, row['T1_TeamID'])] = new_t1_elo<br/>            elo_ratings[(season, row['T2_TeamID'])] = new_t2_elo<br/><br/>        # Collect final Elo ratings for the season<br/>        for team in season_teams:<br/>            final_elo_list.append({'Season': season, 'TeamID': team, 'Elo': elo_ratings[(season, team)]})<br/><br/>    # Convert list to DataFrame<br/>    final_elo_df = pd.DataFrame(final_elo_list)<br/><br/>    # Separate DataFrames for T1 and T2<br/>    final_elo_t1_df = final_elo_df.copy().rename(columns={'TeamID': 'T1_TeamID', 'Elo': 'T1_Elo'})<br/>    final_elo_t2_df = final_elo_df.copy().rename(columns={'TeamID': 'T2_TeamID', 'Elo': 'T2_Elo'})<br/><br/>    # Convert the pandas DataFrames back to Spark DataFrames<br/>    final_elo_t1_df = spark.createDataFrame(final_elo_t1_df)<br/>    final_elo_t2_df = spark.createDataFrame(final_elo_t2_df)<br/><br/>    return final_elo_t1_df, final_elo_t2_df</span></pre><p id="9ac1" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Ideally, we wouldn’t calculate Elo changes on a match-by-match basis to determine each team’s final Elo for the season. However, we couldn’t come up with a better approach. Do you have any ideas? If so, let us know!</p><h2 id="71ab" class="pn oj fr bf ok po pp pq on pr ps pt oq nm pu pv pw nq px py pz nu qa qb qc qd bk">Value Added</h2><p id="36b2" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">The feature engineering steps demonstrated show how we can transform raw data — regular season statistics — into valuable information with predictive power. It is reasonable to assume that a team’s performance during the regular season is indicative of its potential performance in the final tournaments. By calculating the mean of observed match-by-match statistics for both the teams and their opponents, along with each team’s Elo rating in their final match, we were able to create a dataset suitable for modelling. Then, models were trained to predict the outcome of tournament matches using these features, among others developed in a similar way. With these models, we only need the two team IDs to look up the mean of their regular season statistics and their Elos to feed into the model and predict a score!</p><h1 id="e4b2" class="oi oj fr bf ok ol om gr on oo op gu oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Conclusion</h1><p id="9558" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">In this post, we looked at some of the theory behind Spark and PySpark, how that can be applied, and a concrete practical example. We explored how feature engineering can be done in the case of sports data, creating regular season statistics to use as features for final tournament games. Hopefully you’ve found this interesting and helpful — happy feature engineering!</p><p id="96f1" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk"><strong class="nf fs">The full source code for this post and others in the series can be found </strong><a class="af nc" href="https://dev.azure.com/nobledynamic/_git/FabricMadness" rel="noopener ugc nofollow" target="_blank"><strong class="nf fs">here</strong></a><strong class="nf fs">.</strong></p></div></div></div><div class="ab cb qm qn qo qp" role="separator"><span class="qq by bm qr qs qt"/><span class="qq by bm qr qs qt"/><span class="qq by bm qr qs"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><p id="751c" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk"><em class="nz">Originally published at </em><a class="af nc" href="https://nobledynamic.com/posts/fabric-madness-2/" rel="noopener ugc nofollow" target="_blank"><em class="nz">https://nobledynamic.com</em></a><em class="nz"> on April 8, 2024.</em></p><h2 id="5a26" class="pn oj fr bf ok po pp pq on pr ps pt oq nm pu pv pw nq px py pz nu qa qb qc qd bk">References</h2><p id="d2b2" class="pw-post-body-paragraph nd ne fr nf b gp pe nh ni gs pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fk bk">[1] Jeff Sonas, Ryan Holbrook, Addison Howard, Anju Kandru. (2024). March Machine Learning Mania 2024. Kaggle. <a class="af nc" href="https://kaggle.com/competitions/march-machine-learning-mania-2024" rel="noopener ugc nofollow" target="_blank">https://kaggle.com/competitions/march-machine-learning-mania-2024</a></p></div></div></div></div>    
</body>
</html>