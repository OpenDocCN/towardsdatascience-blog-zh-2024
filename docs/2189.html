<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Quantizing Neural Network Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Quantizing Neural Network Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quantizing-neural-network-models-8ce49332f1d3?source=collection_archive---------7-----------------------#2024-09-07">https://towardsdatascience.com/quantizing-neural-network-models-8ce49332f1d3?source=collection_archive---------7-----------------------#2024-09-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="5aba" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Understanding post-training quantization, quantization-aware training, and the straight through estimator</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@arunnanda?source=post_page---byline--8ce49332f1d3--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Arun Nanda" class="l ep by dd de cx" src="../Images/48836e7e13dbe0821bed6902209f2d25.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*kp3EETBZ43AvPhT2YETFQw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--8ce49332f1d3--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@arunnanda?source=post_page---byline--8ce49332f1d3--------------------------------" rel="noopener follow">Arun Nanda</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--8ce49332f1d3--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/6db4148a239d387bc1be88110a5f676c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z4DJ1SfLrNZdKQUKJcPGFA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image created by author</figcaption></figure><p id="356f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Large AI models are resource-intensive. This makes them expensive to use and very expensive to train. A current area of active research, therefore, is about reducing the size of these models while retaining their accuracy. Quantization has emerged as one of the most promising approaches to achieve this goal.</p><p id="0b59" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The previous article, <a class="af ny" rel="noopener" target="_blank" href="/quantizing-the-weights-of-ai-models-39f489455194"><em class="nz">Quantizing the Weights of AI Models</em></a>, illustrated the arithmetics of quantization with numerical examples. It also discussed different types and levels of quantization. This article discusses the next logical step — how to get a quantized model starting from a standard model.</p><p id="efe4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Broadly, there are two approaches to quantizing models:</p><ul class=""><li id="6a0b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oa ob oc bk">Train the model with higher-precision weights and quantize the weights of the trained model. This is post-training quantization (PTQ).</li><li id="c03d" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk">Start with a quantized model and train it while taking the quantization into account. This is called Quantization Aware Training (QAT).</li></ul><p id="0ecd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since quantization involves replacing high-precision 32-bit floating point weights with 8-bit, 4-bit, or even binary weights, it inevitably results in a loss of model accuracy. The challenge, therefore, is how to quantize models, while minimizing the drop in accuracy.</p><p id="18aa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Because it is an evolving field, researchers and developers often adopt new and innovative approaches. In this article, we discuss two broad techniques:</p><ul class=""><li id="efa6" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oa ob oc bk">Quantizing a Trained Model — Post-Training Quantization (PTQ)</li><li id="cbaa" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk">Training a Quantized Model — Quantization Aware Training (QAT)</li></ul><h1 id="c8ba" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Quantizing a Trained Model — Post-Training Quantization (PTQ)</h1><p id="986d" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Conventionally, AI models have been trained using 32-bit floating point weights. There is already a large library of pre-trained models. These trained models can be quantized to lower precision. After quantizing the trained model, one can choose to further fine-tune it using additional data, calibrate the model’s parameters using a small dataset, or just use the quantized model as-is. This is called Post-Training Quantization (PTQ).</p><p id="2a61" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are two broad categories of PTQ:</p><ul class=""><li id="07a4" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oa ob oc bk">Quantizing only the weights</li><li id="22b9" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk">Quantizing both weights and activations.</li></ul><h2 id="92f4" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py pz bk">Weights-only quantization</h2><p id="9aad" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">In this approach, the activations remain in high precision. Only the weights of the trained model are quantized. Weights can be quantized at different granularity levels (per layer, per tensor, etc.). The article <a class="af ny" href="https://medium.com/@arunnanda/different-approaches-to-quantization-e3fac905bd5a" rel="noopener"><em class="nz">Different Approaches to Quantization</em></a> explains granularity levels.</p><p id="0f91" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After quantizing the weights, it is also common to have additional steps like cross-layer equalization. In neural networks, often the weights of different layers and channels can have very different ranges (W_max and W_min). This can cause a loss of information when these weights are quantized using the same quantization parameters. To counter this, it is helpful to modify the weights such that different layers have similar weight ranges. The modification is done in such a way that the output of the activation layers (which the weights feed into) is not affected. This technique is called Cross Layer Equalization. It exploits the scale-equivariance property of the activation function. Nagel et al., in their paper <a class="af ny" href="https://arxiv.org/pdf/1906.04721" rel="noopener ugc nofollow" target="_blank"><em class="nz">Data-Free Quantization Through Weight Equalization and Bias Correction</em></a>, discuss cross-layer equalization (Section 4) in detail.</p><h2 id="e028" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py pz bk">Weights and Activation quantization</h2><p id="5eba" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">In addition to quantizing the weights as before, for higher accuracy, some methods also quantize the activations. Activations are less sensitive to quantization than weights are. It is empirically observed that activations can be quantized down to 8 bits while retaining almost the same accuracy as 32 bits. However, when the activations are quantized, it is necessary to use additional training data to calibrate the quantization range of the activations.</p><h2 id="cc0f" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py pz bk">Advantages and disadvantages of PTQ</h2><p id="be3b" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The advantage is that the training process remains the same and the model doesn’t need to be re-trained. It is thus faster to have a quantized model. There are also many trained 32-bit models to choose from. You start with a trained model and quantize the weights (of the trained model) to any precision — such as 16-bit, 8-bit, or even 1-bit.</p><p id="92f6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The disadvantage is loss of accuracy. The training process optimized the model’s performance based on high-precision weights. So when the weights are quantized to a lower precision, the model is no longer optimized for the new set of quantized weights. Thus, its inference performance takes a hit. Despite the application of various quantization and optimization techniques, the quantized model doesn’t perform as well as the high-precision model. It is also often observed that the PTQ model shows acceptable performance on the training dataset but fails to on new previously unseen data.</p><p id="b5bb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To tackle the disadvantages of PTQ, many developers prefer to train the quantized model, sometimes from scratch.</p><h1 id="0b89" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Training a Quantized Model — Quantization Aware Training (QAT)</h1><p id="a020" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The alternative to PTQ is to train the quantized model. To train a model with low-precision weights, it is necessary to modify the training process to account for the fact that most of the model is now quantized. This is called quantization-aware training (QAT). There are two approaches to doing this:</p><ul class=""><li id="5fc6" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oa ob oc bk">Quantize the untrained model and train it from scratch</li><li id="d29a" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk">Quantize a trained model and then re-train the quantized model. This is often considered a hybrid approach.</li></ul><p id="7094" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In many cases, the starting point for QAT is not an untrained model with random weights but rather a pre-trained model. Such approaches are often adopted in extreme quantization situations. The BinaryBERT model discussed later in this series in the article <a class="af ny" href="https://medium.com/@arunnanda/extreme-quantization-1-bit-ai-models-07169ee29d96" rel="noopener"><em class="nz">Extreme Quantization: 1-bit AI Models</em></a> applies a similar approach.</p><h2 id="56e1" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py pz bk">Advantages and disadvantages of QAT</h2><p id="fa29" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The advantage of QAT is the model performs better because the inference process uses weights of the same precision as was used during the forward pass of the training. The model is trained to perform well on the quantized weights.</p><p id="a5a2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The disadvantage is that most models are currently trained using higher precision weights and need to be retrained. This is resource-intensive. It remains to be established if they can match the performance of older higher-precision models in real-world usage. It also remains to be validated if quantized models can be successfully scaled.</p><h2 id="031e" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py pz bk">Historical background of QAT</h2><p id="57be" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">QAT, as a practice, has been around for at least a few years. Courbariaux et al, in their <a class="af ny" href="https://arxiv.org/pdf/1511.00363" rel="noopener ugc nofollow" target="_blank">2015 paper titled <em class="nz">BinaryConnect: Training Deep Neural Networks with binary weights during propagations</em></a>, discuss their approach to quantizing Computer Vision neural networks to use binary weights. They quantized weights during the forward pass and unquantized weights during the backpropagation (section 2.3). Jacob et al, then working at Google explain the idea of QAT, in their 2017 paper titled <a class="af ny" href="https://arxiv.org/pdf/1712.05877" rel="noopener ugc nofollow" target="_blank">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a> (section 3). They do not explicitly use the phrase Quantization Aware Training but call it simulated quantization instead.</p><h2 id="1069" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py pz bk">Overview of the QAT process</h2><p id="ec12" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The steps below present the important parts of the QAT process, based on the papers referenced earlier. Note that other researchers and developers have adopted variations of these steps, but the overall principle remains the same.</p><ul class=""><li id="ec58" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oa ob oc bk">Maintain an unquantized copy of the weights throughout the process. This copy is sometimes called the latent weights or shadow weights.</li><li id="f7b5" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk">Run the forward pass (inference) based on a quantized version of the latest shadow weights. This simulates the working of the quantized model. The steps in the forward pass are:<br/>- Quantize the weights and the inputs before matrix-multiplying them.<br/>- Dequantize the output of the convolution (matrix multiplication).<br/>- Add (accumulate) the biases (unquantized) to the output of the convolution.<br/>- Pass the result of the accumulation through the activation function to get the output.<br/>- Compare the model’s output with the expected output and compute the loss of the model.</li><li id="2932" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk">Backpropagation happens in full precision. This allows for small changes to the model parameters. To perform the backpropagation: <br/>- Compute the gradients in full precision<br/>- Update via gradient descent the full-precision copy of all weights and biases</li><li id="1709" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk">After training the model, the final quantized version of the weights is exported to use for inference.</li></ul><p id="69c9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">QAT is sometimes referred to as “fake quantization” — it just means that the model training happens using the unquantized weights and the quantized weights are used only for the forward pass. The (latest version of the) unquantized weights are quantized during the forward pass.</p><p id="be76" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The flowchart below gives an overview of the QAT process. The dotted green arrow represents the backpropagation path for updating the model weights while training.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/f034c7467ede7b6428127806b59a68ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*16SGl4DvQvtIXZk4UNDPDg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image created by author</figcaption></figure><p id="e8a5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The next section explains some of the finer points involved in backpropagating quantized weights.</p><h1 id="5b9f" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">BackPropagation in Quantization Aware Training</h1><p id="2b3b" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">It is important to understand how the gradient computation works when using quantized weights. When the forward pass is modified to include the quantizer function, the backward pass must also be modified to include the gradient of this quantizer function. To refresh neural networks and backprop concepts, refer to <a class="af ny" href="https://medium.com/@simon.palma/understanding-weight-update-in-neural-networks-a9f6e23ce984" rel="noopener"><em class="nz">Understanding Weight Update in Neural Networks</em></a> by Simon Palma.</p><p id="cf2c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In a regular neural network, given inputs X, weights W, and bias B, the result of the convolution accumulation operation is:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qb"><img src="../Images/cf65bdd802af4f64c54e0009d3d37ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*dxvCyccJ3FTDaC4mV9XARg.png"/></div></figure><p id="9288" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Applying the sigmoid activation function on the convolution gives the model’s output. This is expressed as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qb"><img src="../Images/4ee3a91289babcfff84958a560a08e78.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*T2qBcImBTG3pCOw6gAR5fQ.png"/></div></figure><p id="32fe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The Cost, C, is a function of the difference between the expected and the actual output. The standard backpropagation process estimates the partial derivative of the cost function, C, with respect to the weights, using the chain rule:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qc"><img src="../Images/3fbb7db6212854269a1c3501cfa90837.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*Z33oG8BI6ANAhEzld1uhXg.png"/></div></figure><p id="b96a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When quantization is involved, the above equation changes to reflect the quantized weight:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/03916c9a0da0cedc48bf32f3c29f3d34.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*dFGgdLsKJ71yNdd0J8K_zA.png"/></div></figure><p id="b429" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Notice that there is an additional term — which is the partial derivative of the quantized weights with respect to the unquantized weights. Look closely at this (last) partial derivative.</p><h2 id="9e9d" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py pz bk">Partial derivative of the quantized weights</h2><p id="ee4b" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The quantizer function can simplistically be represented as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/928225e1329720c988f81d467a0ed432.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*Ul3QZx7w94UhEfoZcN0vGQ.png"/></div></figure><p id="9c1e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the expression above, w is the original (unquantized, full-precision) weight, and s is the scaling factor. Recall from <a class="af ny" href="https://medium.com/@arunnanda/quantizing-the-weights-of-ai-models-39f489455194" rel="noopener">Quantizing the Weights of AI Models</a> (or from basic maths) that the graph of the function mapping the floating point weights to the binary weights is a step function, as shown below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/d4e43419f9c1b053ac389cfee3cf2ee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yw1yjS1duiRxnxErV40Krg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure><p id="2fc9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is the function for which we need the partial derivative. The derivative of the step function is either 0 or undefined — it is undefined at the boundaries between the intervals and 0 everywhere else. To work around this, it is common to use a “Straight-Through Estimator(STE)” for the backprop.</p><h2 id="4ab2" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py pz bk">The Straight Through Estimator (STE)</h2><p id="03e5" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Bengio et al, in their 2013 paper <a class="af ny" href="https://arxiv.org/abs/1308.3432" rel="noopener ugc nofollow" target="_blank"><em class="nz">Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation</em></a>, propose the concept of the STE. Huh et al, in their 2023 paper <a class="af ny" href="https://arxiv.org/abs/2305.08842" rel="noopener ugc nofollow" target="_blank">Straightening Out the Straight-Through Estimator: Overcoming Optimization Challenges in Vector Quantized Networks</a>, explain the application of the STE to the derivative of the loss function using the chain rule (Section 2, Equation 7).</p><p id="14fe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The STE assumes that the gradient with respect to the unquantized weight is essentially equal to the gradient with respect to the quantized weight. In other words, it assumes that within the intervals of the Clip function,</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qe"><img src="../Images/2170673a04eedad7d62f9c832d4ec110.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*q6uTCO0-RB_83MQAWfg8EA.png"/></div></figure><p id="6e42" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Hence, the derivative of the cost function, C, with respect to the unquantized weights is assumed to be equal to the derivative based on the quantized weights.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qb"><img src="../Images/6a86087c7afa96cc7912722aa320f00d.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*54A3HtVLWWqiMhrXowCz0g.png"/></div></figure><p id="6ca5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Thus, the gradient of the Cost is expressed as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qf"><img src="../Images/b72df6b5f039aa908f36b128b581e139.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*vODRVDPqW_GW0pOaS11GJw.png"/></div></figure><p id="c026" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is how the Straight Through Estimator enables the gradient computation in the backward pass using quantized weights. After estimating the gradients. The weights for the next iteration are updated as usual (alpha in the expression below refers to the learning rate):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/0461f0f9f7b8110f309b35d30dfa6a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*xghuZk7KdD0iELmoGbkxeQ.png"/></div></figure><p id="beb3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The clip function above is to ensure that the updated (unquantized) weights remain within the boundaries, W_min, and W_max.</p><h1 id="2472" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Conclusion</h1><p id="388b" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Quantizing neural network models makes them accessible enough to run on smaller servers and possibly even edge devices. There are two broad approaches to quantizing models, each with its advantages and disadvantages:</p><ul class=""><li id="f39d" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oa ob oc bk">Post-Training Quantization (PTQ): Starting with a high-precision trained model and quantizing it (post-training quantization) to lower-precision.</li><li id="0a6f" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk">Quantization Aware Training (QAT): Applying the quantization during the forward pass of training a model so that the optimization accounts for quantized inference</li></ul><p id="2ef7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This article discusses both these approaches but focuses on QAT, which is more effective, especially for modern 1-bit quantized LLMs like <a class="af ny" href="https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3" rel="noopener">BitNet</a> and <a class="af ny" href="https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a" rel="noopener">BitNet b1.58</a>. Since 2021, <a class="af ny" href="https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/" rel="noopener ugc nofollow" target="_blank">NVIDIA’s TensorRT has included a Quantization Toolkit to perform both QAT and quantized inference with 8-bit model weights</a>. For a more in-depth discussion of the principles of quantizing neural networks, refer to the 2018 whitepaper <a class="af ny" href="https://arxiv.org/pdf/1806.08342" rel="noopener ugc nofollow" target="_blank"><em class="nz">Quantizing deep convolutional networks for efficient inference</em></a>, by Krishnamoorthi.</p><p id="df14" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Quantization encompasses a broad range of techniques that can be applied at different levels of precision, different granularities within a network, and in different ways during the training process. The next article,<em class="nz"> </em><a class="af ny" href="https://medium.com/@arunnanda/different-approaches-to-quantization-e3fac905bd5a" rel="noopener"><em class="nz">Different Approaches to Quantization</em></a>, discusses these varied approaches, which are applied in modern implementations like <a class="af ny" href="https://medium.com/@arunnanda/extreme-quantization-1-bit-ai-models-07169ee29d96" rel="noopener">BinaryBERT</a>, <a class="af ny" href="https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3" rel="noopener">BitNet</a>, and <a class="af ny" href="https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a" rel="noopener">BitNet b1.58</a>.</p></div></div></div></div>    
</body>
</html>