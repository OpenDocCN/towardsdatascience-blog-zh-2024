<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Random Forest, Explained: A Visual Guide with Code Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Random Forest, Explained: A Visual Guide with Code Examples</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=collection_archive---------0-----------------------#2024-11-07">https://towardsdatascience.com/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=collection_archive---------0-----------------------#2024-11-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="09e8" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">ENSEMBLE LEARNING</h2><div/><div><h2 id="dd1a" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Making tree-mendous predictions with random trees</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--9f736a6e1b3c--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--9f736a6e1b3c--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--9f736a6e1b3c--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--9f736a6e1b3c--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">5</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><div class="mo mp mq mr ms mt"><a rel="noopener follow" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----9f736a6e1b3c--------------------------------"><div class="mu ab il"><div class="mv ab co cb mw mx"><h2 class="bf ga ib z it my iv iw mz iy ja fz bk">Decision Tree Classifier, Explained: A Visual Guide with Code Examples for Beginners</h2><div class="na l"><h3 class="bf b ib z it my iv iw mz iy ja dx">A fresh look on our favorite upside-down tree</h3></div><div class="gq l"><p class="bf b dy z it my iv iw mz iy ja dx">towardsdatascience.com</p></div></div><div class="nb l"><div class="nc l nd ne nf nb ng lw mt"/></div></div></a></div><p id="32dc" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><a class="af od" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">Decision trees</a> are a great starting point in machine learning — they’re clear and make sense. But there’s a catch: they often don’t work well when dealing with new data. The predictions can be inconsistent and unreliable, which is a real problem when you’re trying to build something useful.</p><p id="1436" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">This is where Random Forest comes in. It takes what’s good about decision trees and makes them work better by combining multiple trees together. It’s become a favorite tool for many data scientists because it’s both effective and practical.</p><p id="ac77" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Let’s see how Random Forest works and why it might be exactly what you need for your next project. It’s time to stop getting lost in the trees and see the forest for what it really is — your next reliable tool in machine learning.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/a46fad3053f42f475b5f070c27b60998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="349d" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Definition</h1><p id="db06" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">A Random Forest is an ensemble machine learning model that combines multiple decision trees. Each tree in the forest is trained on a random sample of the data (bootstrap sampling) and considers only a random subset of features when making splits (feature randomization).</p><p id="9bc1" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">For classification tasks, the forest predicts by majority voting among trees, while for regression tasks, it averages the predictions. The model’s strength comes from its “wisdom of crowds” approach — while individual trees might make errors, the collective decision-making process <strong class="nj ga">tends to average out these mistakes</strong> and arrive at more reliable predictions.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/a7c8ea0333ddfcdeb2779b785e896929.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XZJQXwJtYey5s-EauRA8dQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Random Forest is a part of bagging (bootstrap aggregating) algorithm because it builds each tree using different random part of data and combines their answers together.</figcaption></figure><h1 id="1adf" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Dataset Used</h1><p id="bd4c" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">Throughout this article, we’ll focus on the classic golf dataset as an example for classification. While Random Forests can handle both classification and regression tasks equally well, we’ll concentrate on the classification part — predicting whether someone will play golf based on weather conditions. The concepts we’ll explore can be easily adapted to regression problems (like predicting number of player) using the same principles.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/05586d1dcea17f8a18206b58019181ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O0_DqZWXc5OM--Zxp3_uuw.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Columns: ‘Overcast (one-hot-encoded into 3 columns)’, ’Temperature’ (in Fahrenheit), ‘Humidity’ (in %), ‘Windy’ (Yes/No) and ‘Play’ (Yes/No, target feature)</figcaption></figure><pre class="oh oi oj ok ol pz qa qb bp qc bb bk"><span id="3b41" class="qd oy fq qa b bg qe qf l qg qh">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/><br/># Create and prepare dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', <br/>                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy',<br/>                'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast',<br/>                'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,<br/>                   72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,<br/>                   88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,<br/>                 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,<br/>                 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True,<br/>             True, False, True, True, False, False, True, False, True, True, False,<br/>             True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes',<br/>             'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes',<br/>             'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/><br/># Prepare data<br/>df = pd.DataFrame(dataset_dict)<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Rearrange columns<br/>column_order = ['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind', 'Play']<br/>df = df[column_order]<br/><br/># Prepare features and target<br/>X,y = df.drop('Play', axis=1), df['Play']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)</span></pre><h1 id="0030" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Main Mechanism</h1><p id="0108" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">Here’s how Random Forest works:</p><ol class=""><li id="0777" class="nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qi qj qk bk"><strong class="nj ga">Bootstrap Sampling:</strong> Each tree gets its own unique training set, created by randomly sampling from the original data with replacement. This means some data points may appear multiple times while others aren’t used.</li><li id="f833" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Random Feature Selection:</strong> When making a split, each tree only considers a random subset of features (typically square root of total features).</li><li id="0cb6" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Growing Trees:</strong> Each tree grows using only its bootstrap sample and selected features, making splits until it reaches a stopping point (like pure groups or minimum sample size).</li><li id="9df1" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Final Prediction:</strong> All trees vote together for the final prediction. For classification, take the majority vote of class predictions; for regression, average the predicted values from all trees.</li></ol><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/3e815426c28ac98519c4884d12f43fac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FisK9hkTkWP2D92ANxAp9w.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">A Random Forest Classifier makes predictions by combining results from 100 different decision trees, each analyzing features like temperature and outlook conditions. The final prediction comes from the most common answer among all trees.</figcaption></figure><h1 id="9bed" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Training Steps</h1><p id="4fdd" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">The Random Forest algorithm constructs multiple decision trees and combines them. Here’s how it works:</p><p id="24b6" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><strong class="nj ga">Step 1: Bootstrap Sample Creation</strong> <br/>1.0. Set the number of trees (default = 100)<br/>1.1. For each tree in the forest: <br/>a. Create new training set by random sampling original data with replacement until reaching original dataset size. This is called <strong class="nj ga">bootstrap sampling</strong>.<br/>b. Mark and set aside non-selected samples as out-of-bag (OOB) samples for later error estimation</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/1671fe794bccb45ec5a642740a8d0d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZfXxPDuDm9s2a1OZvG-yw.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Random Forest creates different training sets for each tree by randomly picking data points from the original training set, with some numbers appearing multiple times. The unused data points become test sets for checking each tree’s performance.</figcaption></figure><pre class="oh oi oj ok ol pz qa qb bp qc bb bk"><span id="b3ef" class="qd oy fq qa b bg qe qf l qg qh"># Generate 100 bootstrap samples<br/>n_samples = len(X_train)<br/>n_bootstraps = 100<br/>all_bootstrap_indices = []<br/>all_oob_indices = []<br/><br/>np.random.seed(42)  # For reproducibility<br/>for i in range(n_bootstraps):<br/>    # Generate bootstrap sample indices<br/>    bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)<br/>    <br/>    # Find OOB indices<br/>    oob_indices = list(set(range(n_samples)) - set(bootstrap_indices))<br/>    <br/>    all_bootstrap_indices.append(bootstrap_indices)<br/>    all_oob_indices.append(oob_indices)<br/><br/># Print details for samples 1, 2, and 100<br/>samples_to_show = [0, 1, 99]<br/><br/>for i in samples_to_show:<br/>    print(f"\nBootstrap Sample {i+1}:")<br/>    print(f"Chosen indices: {sorted(all_bootstrap_indices[i])}")<br/>    print(f"Number of unique chosen indices: {len(set(all_bootstrap_indices[i]))}")<br/>    print(f"OOB indices: {sorted(all_oob_indices[i])}")<br/>    print(f"Number of OOB samples: {len(all_oob_indices[i])}")<br/>    print(f"Percentage of OOB: {len(all_oob_indices[i])/n_samples*100:.1f}%")</span></pre><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qq"><img src="../Images/fd0d49cc5d9614e8b4d96dedd10a6ca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_smMUwVcgHdpla5vGIDVrw.png"/></div></div></figure><p id="0e5f" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Notice how similar the percentages of OOB above? When doing bootstrap sampling of <em class="qr">n</em> samples, each individual sample has about a 37% chance of never being picked. This comes from the probability calculation (1–1/<em class="qr">n</em>)<em class="qr">ⁿ</em>, which approaches 1/e ≈ 0.368 as <em class="qr">n</em> gets larger. That’s why each tree ends up using roughly 63% of the data for training, with the remaining 37% becoming OOB samples.</p><p id="7dc5" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><strong class="nj ga">Step 2: Tree Construction</strong> <br/>2.1. Start at root node with complete bootstrap sample</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/58dd94d352eb90afbfe2312662b55270.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v4X7vEWV5j1oOfEX4R5lRw.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">When building each decision tree, Random Forest considers a subset of data points and creates splits based on questions about their values — sending smaller values to the left and larger values to the right to make predictions.</figcaption></figure><p id="c9f5" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">a. Calculate initial node impurity using all samples in node<br/>· Classification: Gini or entropy <br/>· Regression: MSE</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/49ffa6509e2a0b7efc710bf7c8891e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zNarI6dKgX4FiJjFyD3n2Q.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Random Forest starts by calculating the Gini Impurity of the entire dataset (before any splits) using the ratio of YES and NO labels — a measure of how mixed the labels are in the current data.</figcaption></figure><p id="4889" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">b. Select random subset of features from total available features: <br/>· Classification: √n_features <br/>· Regression: n_features/3</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/71393bd15609a6d43354efe5d9f06ac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yeu2LzcuzUiWBUkmNY_lFQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">For each split in a tree, Random Forest randomly picks a subset of weather features (here 2 out of 6) to consider, making each tree focus on different aspects of the data.</figcaption></figure><p id="2176" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">c. For each selected feature:<br/>· Sort data points by feature values<br/>· Identify potential split points (midpoints between consecutive unique feature values)</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/31b2530c2f2eab495683e971a5cc88da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2e-uAc0hSvvdidQkFAZFmQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">For each chosen feature, Random Forest looks at all possible split points in the sorted data (like temperature values 66.0, 69.0, 71.0, etc.) to find the best way to separate the data into two groups.</figcaption></figure><p id="5c43" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">d. For each potential split point: <br/>· Divide samples into left and right groups <br/>· Calculate left child impurity using its samples <br/>· Calculate right child impurity using its samples <br/>· Calculate impurity reduction: <br/>parent_impurity — (left_weight × left_impurity + right_weight × right_impurity)</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/3a76a1bc476cd60a143540debd4b04b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mhNv0BglEfXOxkyBJ6c2bg.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">To find the best split point, Random Forest calculates Gini Impurity for each possible split, takes a weighted average based on group sizes, and picks the split that gives the biggest reduction in impurity from the parent node.</figcaption></figure><p id="1f9d" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">e. Split the current node data using the feature and split point that gives the highest impurity reduction. Then pass data points to the respective child nodes.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/e71e8da4672712136d3478ad9d1cd59b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FXkpHfB6Qi_UyXF9PiboXg.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">After comparing all possible splits, Random Forest picks the temperature threshold of 73.5°F as it gives the largest impurity reduction (0.041), creating two groups: one mixed group with temperatures below 73.5°F and one pure group.</figcaption></figure><p id="3729" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">f. For each child node, repeat the process (step b-e) until: <br/>- Pure node or minimum impurity decrease <br/>- Minimum samples threshold<br/>- Maximum depth <br/>- Maximum leaf nodes</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/8e9f1677218a52ede01934cdc731077e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QPkjzqkKWFGdwFI3VVZzGw.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">This process continues for each new group (node): randomly select features, find the best split point, and divide the data further until each group is pure (all YES or all NO) or can’t be split anymore.</figcaption></figure><p id="ce71" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><strong class="nj ga">Step 3: Tree Construction<br/></strong>Repeat the whole Step 2 for other bootstrap samples.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/5a6197d5a37f101b794221d1c2c3383d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u9omyxuFVrVnM8HbmTCt3A.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Each decision tree in the Random Forest splits data in different ways using different features and thresholds. This variety helps the forest make better predictions than any single tree.</figcaption></figure><pre class="oh oi oj ok ol pz qa qb bp qc bb bk"><span id="7f62" class="qd oy fq qa b bg qe qf l qg qh">import matplotlib.pyplot as plt<br/>from sklearn.tree import plot_tree<br/>from sklearn.ensemble import RandomForestClassifier<br/><br/># Train Random Forest<br/>np.random.seed(42)  # For reproducibility<br/>rf = RandomForestClassifier(n_estimators=100, random_state=42)<br/>rf.fit(X_train, y_train)<br/><br/># Create visualizations for trees 1, 2, and 100<br/>trees_to_show = [0, 1, 99]  # Python uses 0-based indexing<br/>feature_names = X_train.columns.tolist()<br/>class_names = ['No', 'Yes']<br/><br/># Set up the plot<br/>fig, axes = plt.subplots(1, 3, figsize=(20, 6), dpi=300)  # Reduced height, increased DPI<br/>fig.suptitle('Decision Trees from Random Forest', fontsize=16)<br/><br/># Plot each tree<br/>for idx, tree_idx in enumerate(trees_to_show):<br/>    plot_tree(rf.estimators_[tree_idx], <br/>              feature_names=feature_names,<br/>              class_names=class_names,<br/>              filled=True,<br/>              rounded=True,<br/>              ax=axes[idx],<br/>              fontsize=10)  # Increased font size<br/>    axes[idx].set_title(f'Tree {tree_idx + 1}', fontsize=12)<br/><br/>plt.tight_layout(rect=[0, 0.03, 1, 0.95])</span></pre><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qs"><img src="../Images/b46220bdf49910aa49e1d13e353fe1fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*boY9Qp94g4ISXJig9TjWEQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Accessing the internal bootstrap indices directly isn’t possible in the current scikit-learn implementation so this gives different trees than the one calculated in our previous example.</figcaption></figure><h1 id="4dc2" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Testing Step</h1><p id="0d4d" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">For prediction, route new samples through all trees and aggregate: <br/>- Classification: majority vote <br/>- Regression: mean prediction</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/e8bd423acc3d8361e5ce82ec3fd542e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PoslIYcQKd1MN89jWt14JQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">When new data comes in, each tree in the Random Forest uses its own decision path to make a prediction. The forest combines all these predictions (74 YES vs 26 NO) and the majority vote becomes the final answer (YES in this case).</figcaption></figure><h2 id="5267" class="qt oy fq bf oz qu qv qw pc qx qy qz pf nq ra rb rc nu rd re rf ny rg rh ri fw bk">Out-of-Bag (OOB) Evaluation</h2><p id="b116" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">Remember those samples that didn’t get used for training each tree — that leftover 1/3? Those are your OOB samples. Instead of just ignoring them, Random Forest uses them as a convenient validation set for each tree.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/afccd043882db9aba10d7e5fb93558e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iX_BcJIgo4xt7_pwhOp2uA.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Each tree gets tested on its own out-of-bag samples (data not used in its training). By averaging these individual OOB accuracy scores (50%, 66.6%, 60%), Random Forest provides a built-in way to measure performance without needing a separate test set.</figcaption></figure><h1 id="ab74" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Evaluation Step</h1><p id="01c9" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">After building all the trees, we can evaluate the test set.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/55f4ae7dd4d6a6513400959cda5c3c12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i9HhZ7Qb-PNAj8Dz1BS__g.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">By combining multiple diverse decision trees and using majority voting, Random Forest achieves a high accuracy of 85.7% — typically better than single decision trees or simpler models!</figcaption></figure><h1 id="5328" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Key Parameters</h1><p id="af10" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">The key Random Forest parameters (especially in <code class="cx rj rk rl qa b">scikit-learn</code>) include all Decision Tree parameters, plus some unique ones.</p><h2 id="328d" class="qt oy fq bf oz qu qv qw pc qx qy qz pf nq ra rb rc nu rd re rf ny rg rh ri fw bk">Random Forest-specific parameters</h2><ul class=""><li id="066e" class="nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc rm qj qk bk"><code class="cx rj rk rl qa b">oob_score<br/></code>This uses leftover data (out-of-bag samples) to check how well the model works. This gives you a way to test your model without setting aside separate test data. It’s especially helpful with small datasets.</li><li id="8161" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc rm qj qk bk"><code class="cx rj rk rl qa b">n_estimators<br/></code>This parameter controls how many trees to build (default is 100). <strong class="nj ga"><br/></strong>To find the optimal number of trees, <strong class="nj ga">track the OOB error rate</strong> as you add more trees to the forest. The error typically drops quickly at first, then levels off. <strong class="nj ga">The point where it stabilizes suggests the optimal number</strong> — adding more trees after this gives minimal improvement while increasing computation time.</li></ul><pre class="oh oi oj ok ol pz qa qb bp qc bb bk"><span id="25dc" class="qd oy fq qa b bg qe qf l qg qh"># Calculate OOB error for different numbers of trees<br/>n_trees_range = range(10, 201)<br/>oob_errors = [<br/>    1 - RandomForestClassifier(n_estimators=n, oob_score=True, random_state=42).fit(X_train, y_train).oob_score_<br/>    for n in n_trees_range<br/>]<br/><br/># Create a plot<br/>plt.figure(figsize=(7, 5), dpi=300)<br/>plt.plot(n_trees_range, oob_errors, 'b-', linewidth=2)<br/>plt.xlabel('Number of Trees')<br/>plt.ylabel('Out-of-Bag Error Rate')<br/>plt.title('Random Forest OOB Error vs Number of Trees')<br/>plt.grid(True, alpha=0.2)<br/>plt.tight_layout()<br/><br/># Print results at key intervals<br/>print("OOB Error by Number of Trees:")<br/>for i, error in enumerate(oob_errors, 1):<br/>    if i % 10 == 0:<br/>        print(f"Trees: {i:3d}, OOB Error: {error:.4f}")</span></pre><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of rn"><img src="../Images/97234d73657a128e85e5409216f55df5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QZGLjKBkq2Sv4GcJKALRgA.png"/></div></div></figure><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of ro"><img src="../Images/3d6b30f69790966d7d1dc81b1bed0c83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L2s739Wdeb89lNjtMLKznQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">In our results, while around 27 trees showed the best score (0.2857), this early performance can be unreliable. Between 40–100 trees, the error rates settle around 0.5000, showing more consistent results. Using more than 100 trees doesn’t help and sometimes makes things worse. This suggests that using about 50–60 trees is a good choice — it’s stable, efficient, and reliable.</figcaption></figure><ul class=""><li id="ff19" class="nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc rm qj qk bk"><code class="cx rj rk rl qa b">bootstrap<br/></code>This decides whether each tree learns from a random sample of data (<code class="cx rj rk rl qa b">True</code>) or uses all data ( <code class="cx rj rk rl qa b">False</code>). The default (<code class="cx rj rk rl qa b">True</code>) helps create different kinds of trees, which is key to how Random Forests work. Only consider <strong class="nj ga">setting it to </strong><code class="cx rj rk rl qa b"><strong class="nj ga">False</strong></code><strong class="nj ga"> when you have very little data</strong> and can’t afford to skip any samples.</li><li id="0a59" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc rm qj qk bk"><code class="cx rj rk rl qa b">n_jobs<br/></code>This controls how many processor cores to use during training. Setting it to <code class="cx rj rk rl qa b">-1</code> uses all available cores, making training faster but using more memory. With big datasets, you might need to use fewer cores to avoid running out of memory.</li></ul><h2 id="aa7b" class="qt oy fq bf oz qu qv qw pc qx qy qz pf nq ra rb rc nu rd re rf ny rg rh ri fw bk">Shared parameters with Decision Trees</h2><p id="b55d" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">The following parameters works the <a class="af od" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">same way as in Decision Tree</a>.</p><ul class=""><li id="e218" class="nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc rm qj qk bk"><code class="cx rj rk rl qa b">max_depth</code>: Maximum tree depth</li><li id="7b0c" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc rm qj qk bk"><code class="cx rj rk rl qa b">min_samples_split</code>: Minimum samples needed to split a node</li><li id="66ee" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc rm qj qk bk"><code class="cx rj rk rl qa b">min_samples_leaf</code>: Minimum samples required at leaf node</li></ul><p id="8991" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Compared to Decision Tree, here are key differences in parameter importance:</p><ol class=""><li id="2c1d" class="nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qi qj qk bk"><code class="cx rj rk rl qa b">max_depth<br/></code>This matters less in Random Forests because combining many trees helps prevent overfitting, even with deeper trees. You can usually let trees grow deeper to catch complex patterns in your data.</li><li id="67fe" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><code class="cx rj rk rl qa b">min_samples_split</code> and <code class="cx rj rk rl qa b">min_samples_leaf<br/></code>These are less important in Random Forests because using many trees naturally helps avoid overfitting. You can usually set these to smaller numbers than you would with a single decision tree.</li></ol><h1 id="65c9" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Pros &amp; Cons</h1><h2 id="619a" class="qt oy fq bf oz qu qv qw pc qx qy qz pf nq ra rb rc nu rd re rf ny rg rh ri fw bk">Pros:</h2><ol class=""><li id="11f4" class="nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc qi qj qk bk"><strong class="nj ga">Strong and Reliable:</strong> Random Forests give accurate results and are less likely to overfit than single decision trees. By using random sampling and mixing up which features each tree considers at each node, they work well across many problems without needing much adjustment.</li><li id="8dfa" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Feature Importance:</strong> The model can tell you which features matter most in making predictions by measuring how much each feature helps across all trees. This helps you understand what drives your predictions.</li><li id="001e" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Minimal Preprocessing: </strong>Random Forests handle both numerical and categorical variables well without much preparation. They work well with missing values and outliers, and can find complex relationships in your data automatically.</li></ol><h1 id="83df" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Cons:</h1><ol class=""><li id="8229" class="nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc qi qj qk bk"><strong class="nj ga">Computational Cost:</strong> Training and using the model takes more time as you add more trees or make them deeper. While you can speed up training by using multiple processors, it still needs substantial computing power for big datasets.</li><li id="09ed" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Limited Interpretability:</strong> While you can see which features are important overall, it’s harder to understand exactly why the model made a specific prediction, unlike with single decision trees. This can be a problem when you need to explain each decision.</li><li id="417d" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Prediction Speed:</strong> To make a prediction, data must go through all trees and then combine their answers. This makes Random Forests slower than simpler models, which might be an issue for real-time applications.</li></ol><h1 id="570a" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Final Remarks</h1><p id="a592" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">I’ve grown to really like Random Forests after seeing how well they work in practice. By combining multiple trees and letting each one learn from different parts of the data, they consistently make better predictions — of course, more than using just one tree alone.</p><p id="7d9e" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">While you do need to adjust some settings like the number of trees, they usually perform well even without much fine-tuning. They do need more computing power (and sometimes struggle with rare cases in the data) but their reliable performance and ease of use make them my go-to choice for many projects. It’s clear why so many data scientists feel the same way!</p><h1 id="e2db" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">🌟 Random Forest Classifier Code Summarized</h1><pre class="oh oi oj ok ol pz qa qb bp qc bb bk"><span id="0555" class="qd oy fq qa b bg qe qf l qg qh">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.ensemble import RandomForestClassifier<br/><br/># Create dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', <br/>                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy',<br/>                'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast',<br/>                'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,<br/>                   72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,<br/>                   88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,<br/>                 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,<br/>                 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True,<br/>             True, False, True, True, False, False, True, False, True, True, False,<br/>             True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes',<br/>             'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes',<br/>             'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/><br/># Prepare data<br/>df = pd.DataFrame(dataset_dict)<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Rearrange columns<br/>column_order = ['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind', 'Play']<br/>df = df[column_order]<br/><br/># Split features and target<br/>X, y = df.drop('Play', axis=1), df['Play']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Train Random Forest<br/>rf = RandomForestClassifier(n_estimators=100, max_features='sqrt', random_state=42)<br/>rf.fit(X_train, y_train)<br/><br/># Predict and evaluate<br/>y_pred = rf.predict(X_test)<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre><h1 id="2f9e" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">🌟 Random Forest Regressor Code Summarized</h1><pre class="oh oi oj ok ol pz qa qb bp qc bb bk"><span id="d4b6" class="qd oy fq qa b bg qe qf l qg qh">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import root_mean_squared_error<br/>from sklearn.ensemble import RandomForestRegressor<br/><br/># Create dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', <br/>                'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain',<br/>                'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast',<br/>                'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temp.': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,<br/>              72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,<br/>              88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humid.': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,<br/>               90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,<br/>               65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True,<br/>             True, False, True, True, False, False, True, False, True, True, False,<br/>             True, False, False, True, False, False],<br/>    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29,<br/>                    25, 51, 41, 14, 34, 29, 49, 36, 57, 21, 23, 41]<br/>}<br/><br/># Prepare data<br/>df = pd.DataFrame(dataset_dict)<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='')<br/>df['Wind'] = df['Wind'].astype(int)<br/><br/># Split features and target<br/>X, y = df.drop('Num_Players', axis=1), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Train Random Forest<br/>rf = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=42)<br/>rf.fit(X_train, y_train)<br/><br/># Predict and evaluate<br/>y_pred = rf.predict(X_test)<br/>rmse = root_mean_squared_error(y_test, y_pred)<br/><br/>print(f"Root Mean Squared Error: {rmse:.2f}")<br/></span></pre></div></div></div><div class="ab cb rp rq rr rs" role="separator"><span class="rt by bm ru rv rw"/><span class="rt by bm ru rv rw"/><span class="rt by bm ru rv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="b432" class="qt oy fq bf oz qu qv qw pc qx qy qz pf nq ra rb rc nu rd re rf ny rg rh ri fw bk">Further Reading</h2><p id="9495" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">For a detailed explanation of the <a class="af od" href="https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">RandomForestClassifier</a> and <a class="af od" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" rel="noopener ugc nofollow" target="_blank">RandomForestRegressor</a> and its implementation in scikit-learn, readers can refer to the official documentation, which provides comprehensive information on its usage and parameters.</p><h2 id="6253" class="qt oy fq bf oz qu qv qw pc qx qy qz pf nq ra rb rc nu rd re rf ny rg rh ri fw bk">Technical Environment</h2><p id="5264" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions.</p><h2 id="dcdb" class="qt oy fq bf oz qu qv qw pc qx qy qz pf nq ra rb rc nu rd re rf ny rg rh ri fw bk">About the Illustrations</h2><p id="d4cc" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><p id="3ec8" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙀𝙣𝙨𝙚𝙢𝙗𝙡𝙚 𝙇𝙚𝙖𝙧𝙣𝙞𝙣𝙜 𝙝𝙚𝙧𝙚:</p><div class="mo mp mq mr ms"><div role="button" tabindex="0" class="ab bx cp kj it rx ry bp rz lw ao"><div class="sa l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by sb sc cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l sb sc em n ay um"/></div><div class="sd l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----9f736a6e1b3c--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sg hp l"><h2 class="bf ga xj ic it xk iv iw mz iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xl wl wm wn wo lj wp wq ux ii wr ws wt vb vc vd ep bm ve ot" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----9f736a6e1b3c--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xm l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="sp dz sq it ab sr il ed"><div class="ed sj bx sk sl"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed sj bx kk sm sn"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx so sn"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div><p id="ae35" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:</p><div class="mo mp mq mr ms"><div role="button" tabindex="0" class="ab bx cp kj it rx ry bp rz lw ao"><div class="sa l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by sb sc cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l sb sc em n ay um"/></div><div class="sd l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----9f736a6e1b3c--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sg hp l"><h2 class="bf ga xj ic it xk iv iw mz iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xl wl wm wn wo lj wp wq ux ii wr ws wt vb vc vd ep bm ve ot" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----9f736a6e1b3c--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xm l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="sp dz sq it ab sr il ed"><div class="ed sj bx sk sl"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed sj bx kk sm sn"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx so sn"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>