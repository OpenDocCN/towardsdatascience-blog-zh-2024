- en: An Introduction to Prompting for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-introduction-to-prompting-for-llms-61d36aec2048?source=collection_archive---------3-----------------------#2024-02-22](https://towardsdatascience.com/an-introduction-to-prompting-for-llms-61d36aec2048?source=collection_archive---------3-----------------------#2024-02-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How do we communicate effectively with LLMs?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@anand.subu10?source=post_page---byline--61d36aec2048--------------------------------)[![Anand
    Subramanian](../Images/096dc5504d6ada2493e0ac26959e60f0.png)](https://medium.com/@anand.subu10?source=post_page---byline--61d36aec2048--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--61d36aec2048--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--61d36aec2048--------------------------------)
    [Anand Subramanian](https://medium.com/@anand.subu10?source=post_page---byline--61d36aec2048--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--61d36aec2048--------------------------------)
    ·30 min read·Feb 22, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Unless you’ve been completely disconnected from the buzz on social media and
    in the news, it’s unlikely that you’d have missed the excitement around Large
    Language Models (LLMs).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb60be0bd7ee67fd4f1a4f16239215c8.png)'
  prefs: []
  type: TYPE_IMG
- en: The evolution of LLMs. Image borrowed from the paper [1] ([Source](https://github.com/Mooler0410/LLMsPracticalGuide/blob/main/imgs/tree.png)).
    Even as I add this image, the pace of current LLM development makes this picture
    obsolete.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have become ubiquitous, with new models being released almost daily. They’ve
    also been made more accessible to the general public, thanks to a thriving open-source
    community that has played a crucial role in reducing memory requirements and developing
    efficient fine-tuning methods for LLMs, even with limited compute resources.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most exciting use cases for LLMs is their remarkable ability to excel
    at tasks they were not explicitly trained for, using just a task description and,
    optionally, a few examples. You can now get a capable LLM to generate a story
    in the style of your favorite author, summarize long emails into concise ones,
    and develop innovative marketing campaigns by describing your task to the model
    without needing to fine-tune it. But how do you best communicate your requirements
    to the LLM? This is where prompting comes in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table of Contents:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[What is Prompting?](#0f7f)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Why is Prompting Important?](#a04b)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exploring different prompting strategies](#f3d4)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[How can we implement these techniques?](#3a00) 4.1\. [Prompting Llama 2 7B-Chat
    with a Zero-Shot Prompt](#5921)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4.2\. [Prompting Llama 2 7B-Chat with a Few-Shot Prompt](#0ac4)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.3\. [What happens if we don’t adhere to the chat template?](#a04e)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.4\. [Prompting Llama 2 7B-Chat with CoT Prompting](#bfe1)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.5\. [Failure Modes in CoT for Llama 2](#0ac0)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.6\. [Prompting GPT-3.5 with a Zero-Shot Prompt](#c938)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.7\. [Prompting GPT-3.5 with a Few-Shot Prompt](#a575)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.8\. [Prompting GPT-3.5 with CoT Prompting](#18e9)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Conclusion and Takeaways](#913f)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Reproducibility](#f8e9)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[References](#42e4)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**What is Prompting?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompting, or Prompt Engineering, is a technique used to design inputs or prompts
    that guide artificial intelligence models — particularly those in natural language
    processing and image generation — to produce specific, desired outputs. Prompting
    involves structuring your requirements into an input format that effectively communicates
    the desired outcomes to the model, thereby obtaining the intended output.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) demonstrate an ability for **in-context learning**
    [2] [3]. This means these models can understand and execute various tasks based
    solely on task descriptions and examples provided to the model through a prompt
    without requiring specialized fine-tuning for each new task. Prompting is significant
    in this context as it is the primary interface between the user and the model
    to harness this ability. A well-defined prompt helps define the nature and expectations
    of the task to the LLM, along with how to provide the output in a utilizable manner
    to the user.
  prefs: []
  type: TYPE_NORMAL
- en: You might be inclined to think that prompting an LLM shouldn’t be that hard;
    after all, it’s just about describing your requirements to the model in natural
    language, right? In practice, it isn’t as straightforward. You will discover that
    different LLMs have varying strengths. Some might better adhere to your desired
    output format, while others may necessitate more detailed instructions. The task
    you wish the LLM to perform could be complex, requiring elaborate and precise
    instructions. Therefore, devising a suitable prompt often entails a lot of experimentation
    and benchmarking.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why is Prompting important?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In practice, LLMs are sensitive to how the input is structured and provided
    to them. We can analyze this along various axes to better understand the situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adhering to Prompt Formats**: LLMs often utilize varying prompt formats to
    accept user input. This is typically done when models are instruction-tuned or
    optimized for chat use cases [4] [5]. At a high level, most prompt formats include
    the ***instruction*** and the ***input***. The instruction describes the task
    to be performed by the model, while the input contains the text on which the task
    needs to be executed. Let’s take the Alpaca Instruction format, for example (taken
    from [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Given the models are instruction-tuned using a template like this, the model
    is expected to perform optimally when a user prompts it using the same format.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. **Describing output formats for parseability:** Having provided a prompt
    to the model, you’d want to extract what you need from the model’s output. Ideally,
    these outputs should be in a format you can effortlessly parse through programming
    methods. Depending on the task, such as text classification, this might involve
    leveraging regular expressions (regex) to sift through the LLM’s output. In contrast,
    you might prefer a format like JSON for your output for tasks requiring more fine-grained
    data like Named Entity Recognition (NER).
  prefs: []
  type: TYPE_NORMAL
- en: However, the more you work with LLMs, the faster you learn that obtaining parseable
    outputs can be challenging. LLMs often struggle to deliver outputs precisely in
    the format requested by the user. While strategies like few-shot prompting can
    significantly mitigate this issue, achieving consistent, programmatically parsable
    outputs from LLMs demands careful experimentation and adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. **Prompting for optimal performance:** LLMs are quite sensitive to how the
    task is described. A prompt that is not well-crafted or leaves too much room for
    interpretation can lead to subpar performance. Imagine explaining a task to someone
    — the clearer and more detailed your explanation, the better the understanding
    on the other end. However, there is no magic formula for arriving at the ideal
    prompt. This requires careful experimentation and evaluation of different prompts
    to select the best-performing prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploring different prompting strategies**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully, you’re convinced you need to take prompting seriously by this point.
    If prompting is a toolkit, what are the tools we can leverage?
  prefs: []
  type: TYPE_NORMAL
- en: '**Zero-shot Prompting:** Zero-shot prompting [2] [3] involves instructing an
    LLM to perform a task described solely in the prompt without providing examples.
    The term “zero-shot” signifies that the model must rely entirely on the task description
    in the prompt, as it receives no specific demonstrations related to the task.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf12c12c151c864333b4f2fb77809d97.png)'
  prefs: []
  type: TYPE_IMG
- en: An overview of Zero-Shot Prompting. (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'In many cases, zero-shot prompting can suffice for instructing an LLM to perform
    your desired task. However, zero-shot prompting may have limitations if your task
    is too ambiguous, open-ended, or vague. Suppose you want an LLM to rank an answer
    on a scale from 1 to 5\. Although the model could perform this task with a zero-shot
    prompt, two possible problems can arise here:'
  prefs: []
  type: TYPE_NORMAL
- en: The LLM might not have an objective understanding of what each number on the
    scoring scale signifies. It may struggle to decide when to assign a score of 3
    or 4 to an answer if the task description lacks nuance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM could have its own concept of scoring from 1 to 5, which might contradict
    your personal scoring rubrics. You might prioritize the factuality of an answer
    when scoring it, but the model could evaluate the answer based on how well it
    is written.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To ground the model in your scoring expectations, you can provide a few examples
    of answers and how you might score them. Now, the model has more context and reference
    on how to score documents, thereby narrowing the ambiguity in the task. This brings
    us to few-shot prompting.
  prefs: []
  type: TYPE_NORMAL
- en: '**Few-shot Prompting:** Few-shot prompting enriches the task description with
    a small number of example inputs and their corresponding outputs [3]. This technique
    enhances the model’s understanding by including several example pairs illustrating
    the task.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58cc2240a055d9a5cce738de853f7c5b.png)'
  prefs: []
  type: TYPE_IMG
- en: An overview of Few-Shot Prompting. (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: For instance, to guide an LLM in sentiment classification of movie reviews,
    you would present a few reviews along with their sentiment ratings. The primary
    benefit of few-shot over zero-shot prompting is the ability to demonstrate examples
    of how to perform the task instead of expecting the LLM to perform the task with
    just a description.
  prefs: []
  type: TYPE_NORMAL
- en: '**Chain of Thought:** Chain of Thought (CoT) prompting [6] is a technique that
    enables LLMs to solve complex problems by breaking them down into simpler, intermediate
    steps. This approach encourages the model to “think aloud,” making its reasoning
    process transparent and allowing the LLM to solve reasoning problems more effectively.
    As mentioned by the authors of the work [6], CoT mimics how humans try to solve
    reasoning problems by decomposing the problem into simpler steps and solving them
    one at a time rather than jumping directly to the answer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79baf1845e581cfe3140508309256a9c.png)'
  prefs: []
  type: TYPE_IMG
- en: An overview of Chain-of-Thought Prompting. (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: CoT prompting is typically implemented as a few-shot prompt, where the model
    receives a task description and examples of input-output pairs. These examples
    include reasoning steps that systematically lead to the correct answer, demonstrating
    how to process the information. Thus, to perform CoT prompting effectively, users
    need high-quality demonstration examples. However, this can be challenging for
    tasks requiring specialized domain expertise. For instance, using an LLM for medical
    diagnosis based on a patient’s history would necessitate the assistance of domain
    experts, such as doctors or physicians, to articulate the correct reasoning steps.
    Moreover, CoT is particularly effective in models with a sufficiently large parameter
    scale. According to the paper [6], CoT is most effective for the 137B parameter
    LaMBDA [7], the 175B parameter GPT-3 [3], and the 540B parameter PaLM [8] models.
    This limitation can restrict its applicability for smaller-scale models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e9847b192ff473c6e794fd82530c80e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure taken from [6] ([Source](https://arxiv.org/abs/2201.11903)) shows that
    the performance improvement provided by CoT prompting improves substantially with
    the scale of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect of CoT prompting that sets it apart from standard prompting is
    that the model needs to generate significantly more tokens before arriving at
    the final answer. While not necessarily a drawback, this is a factor to consider
    if you are compute-bound at inference time.
  prefs: []
  type: TYPE_NORMAL
- en: If you want a deeper overview, I recommend OpenAI’s prompting resources, available
    at [https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions](https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions).
  prefs: []
  type: TYPE_NORMAL
- en: '**How can we implement these techniques?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All code and resources related to this article are made available at [this Github
    repository](https://github.com/anand-subu/blog_resources/tree/main), under the
    introduction_to_prompting folder. Feel free to pull the repository and run the
    notebooks directly to run these experiments. Please let me know if you have any
    feedback or observations or if you notice any mistakes!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can explore these techniques on a sample dataset to make understanding easier.
    To this end, we will work with the MedQA dataset [9], which contains questions
    testing medical and clinical knowledge. We will specifically utilize the USMLE
    questions from this dataset. This task is ideal for analyzing various prompting
    techniques, as answering the questions requires knowledge and reasoning. We will
    test the capabilities of Llama-2 7B [10] and GPT-3.5 [11] on this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first download the dataset. The MedQA dataset can be downloaded from [this
    link](https://drive.google.com/file/d/1ImYUSLk9JbgHXOemfvyiDiirluZHPeQw/view).
    After downloading the dataset, we can parse and begin processing the questions.
    The test set contains a total of 1,273 questions. We randomly sample 300 questions
    from the test set to evaluate the models and select 3 random examples from the
    training set as our few-shot demonstrations for the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Prompting Llama 2 7B-Chat with a Zero-Shot Prompt**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Llama series of models were released by Meta. They are a decoder-only family
    of LLMs spanning parameter counts from 7B to 70B. The Llama-2 series of models
    comes in two variants: the base version and the chat/instruction-tuned variant.
    For this exercise, we’ll work with the chat-version of the Llama 2-7B model.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s see how well we can prompt the Llama model to answer these medical questions.
    We load the model into memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you’re working with Nvidia Ampere GPUs, you can load the model using torch.bfloat16\.
    It offers speedups to inference and utilizes lesser GPU memory than normal FP16/FP32.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'First, let’s now craft a basic prompt for our task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Our prompt is straightforward. It includes information about the nature of the
    task and provides instructions on the format for the output. We’ll see how effectively
    this prompt works in practice.
  prefs: []
  type: TYPE_NORMAL
- en: The Llama-2 chat models have a particular chat template to be followed for prompting
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The task description should be provided between the <<SYS>> tokens, followed
    by the actual question the model needs to answer. The prompt is concluded with
    a [/INST] token to indicate the end of the input text.
  prefs: []
  type: TYPE_NORMAL
- en: The role can be one of “**user**”, “**system**”, or “**assistant**”. The “system”
    role provides the model with the task description, and the “user” role contains
    the input to which the model needs to respond. This is the same convention we
    will utilize later on when interacting with GPT-3.5\. It is equivalent to creating
    a fictional multi-turn conversation history provided to Llama-2, where each turn
    corresponds to an example demonstration and an ideal output from the model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sounds complicated? Thankfully, the Huggingface Transformers library supports
    converting prompts to the chat template. We will utilize this functionality to
    make our lives easier. Let’s start with helper functionalities to process the
    dataset and create prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This function constructs the query to provide to the LLM. The MedQA dataset
    stores each question as a JSON element, with the questions and options provided
    as keys. We parse the JSON and construct the question along with the choices.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start obtaining outputs from the model. The current task involves answering
    the provided medical question by selecting the correct answer from various options.
    Unlike creative tasks such as content writing or summarization, which may require
    the model to be imaginative and creative in its output, this is a knowledge-based
    task designed to test the model’s ability to answer questions based on knowledge
    encoded in its parameters. Therefore, we will use greedy decoding while generating
    the answer. Let’s define a helper function for parsing the model responses and
    calculating accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We get a performance of 3̶6̶%̶ 35% in the zero-shot setting. Not a bad start,
    but let’s see if we can push this performance further.
  prefs: []
  type: TYPE_NORMAL
- en: '**Edit on 09/03/2024** — I noticed a minor “bug” in the way the prompt was
    formatted and tokenized. Specifically, I used the tokenizer.apply_chat_template(zero_shot_prompt_messages,
    tokenize = False) and then called the tokenizer to get the input_ids earlier.
    This method adds an extra <s> token at the start to the prompt. The apply_chat_template
    already adds an <s> token to the sequence, but calling the tokenizer after creating
    the chat template adds the specical token <s> again at the start. I fixed this
    by changing tokenize=True in apply_chat_template. The new score I got was 35%,
    a small dip of 1% compared to the old setting where the score was 36%. It’s a
    bit amusing that fixing this “bug” leads to a minor score dip, but I’m making
    the correction here and in the code to avoid any confusion in using chat templates.
    None of the findings or takeaways in this article are otherwise affected by this
    fix.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Prompting Llama 2 7B-Chat with a Few-Shot Prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now provide task demonstrations to the model. We use the three randomly
    sampled questions from the training set and append them to the model as task demonstrations.
    Fortunately, we can continue using the chat-template support provided by the Transformers
    library and the tokenizer to append our few-shot examples with minimal code changes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Let’s visualize what our few-shot prompt looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The prompt is quite long, given that we append three demonstrations. Let’s
    now run Llama-2 with the few-shot prompt and get the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We now get an overall accuracy of 4̵1̵.̵6̵7̵%̵ 40.33%. Not bad, nearly 6̵%̵
    5% improvement over zero-shot prompting with the same model!
  prefs: []
  type: TYPE_NORMAL
- en: '**Edit on 09/03/2024** — Similar to the zero-shot setting, I fixed the way
    the prompt is sent to the model for the few-shot setting. The new score I got
    was 40.33%, a small dip of ~1.3% compared to the old setting where the score was
    41.67%. It’s again a bit amusing that fixing this “bug” leads to a minor score
    dip, but I’m making the correction here and in the code to avoid any confusion
    in using chat templates. None of the findings or takeaways in this article are
    otherwise affected by this fix.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What happens if we don’t adhere to the chat template?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier, I observed that it is advisable to structure our prompt according to
    the prompt template that was used to fine-tune an LLM originally. Let’s verify
    if not adhering to the chat template hurts our performance. We create a function
    that builds a few-shot prompt using the same examples without adhering to the
    chat format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Our prompts now look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now evaluate Llama 2 with these prompts and observe how it performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We achieve an accuracy of 36%. This is 6̶%̶ 4.3% lower than our earlier few-shot
    score. This reinforces our previous argument that it is crucial to structure our
    prompts according to the template used to fine-tune the LLM we intend to work
    with. Prompt templates matter!
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompting Llama 2 7B-Chat with CoT Prompting**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s conclude by evaluating CoT prompting. Remember, our dataset includes questions
    designed to test medical knowledge through the USMLE exam. Such questions often
    require both factual recall and conceptual reasoning to answer. This makes it
    a perfect task for testing how well CoT works.
  prefs: []
  type: TYPE_NORMAL
- en: First, we must provide an example CoT prompt to the model to demonstrate how
    to reason about a question. For this purpose, we will use one of the prompts from
    Google’s MedPALM paper [12].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e947fa6ba06d18bae65fe3354e5778aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Five-shot CoT prompt used for evaluating the MedPALM model on the MedQA dataset.
    Prompt borrowed from Table A.18, Page 41 of [12] ([Source](https://arxiv.org/abs/2212.13138)).
  prefs: []
  type: TYPE_NORMAL
- en: We use this five-shot prompt for evaluating the models. Since this prompt style
    differs slightly from our earlier prompts, let’s create some helper functions
    again to process them and obtain the outputs. While utilizing CoT prompting, we
    generate the output with a larger output token count to enable the model to “think”
    and “reason” before answering the question.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Our performance dips to 2̶0̶%̶ 21.33% using CoT prompting for Llama 2–7B. This
    is generally in line with the findings of the CoT paper [6], where the authors
    mention that CoT is an emergent property for LLMs that improves with the scale
    of the model. That being said, let’s analyze why the performance dipped drastically.
  prefs: []
  type: TYPE_NORMAL
- en: '**Edit on 09/03/2024** — Similar to the zero-shot setting, I fixed the way
    the prompt was sent to the model for the CoT setting. The new score I got was
    21.33%, a small increase of ~1.33% compared to the old setting where the score
    was 20%. I’m making the correction here and in the code to avoid any confusion
    about using chat templates. None of the findings or takeaways in this article
    are otherwise affected by this fix.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Failure Modes in CoT for Llama 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We sample a few of the responses provided by Llama 2 on some of the test set
    questions to analyze error cases:'
  prefs: []
  type: TYPE_NORMAL
- en: The analysis of these CoT samples and the figures are not affected by the fixing
    of the minor “bug”. I’ve verified that the predictions used in these figures are
    identical in both the old and new setting.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/bafde2d1cb6d802d5b8c1c826acfc01c.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample Prediction 1 — The model arrives at an answer but does not adhere to
    the format, making parsing the result hard. (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9192f54c14ae9038111571c6319de6cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample Prediction 2 — The model fails to adhere to the prompt format and arrive
    at a conclusive answer. (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: While CoT prompting allows the model to “think” before arriving at the final
    answer, in most cases, the model either does not arrive at a conclusive answer
    or mentions the answer in a format inconsistent with our example demonstrations.
    A failure mode I haven’t analyzed here, but potentially worth exploring, is to
    check cases in the test set where the model “reasons” incorrectly and, therefore,
    arrives at the wrong answer. This is beyond the scope of the current article and
    my medical knowledge, but it is certainly something I intend to revisit later.
  prefs: []
  type: TYPE_NORMAL
- en: Prompting GPT-3.5 with a Zero-Shot Prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin defining some helper functions that help us process these inputs
    for utilizing the GPT API. You would need to generate an API key to use the GPT-3.5
    API. You can set the API key in Windows using:'
  prefs: []
  type: TYPE_NORMAL
- en: '`setx OPENAI_API_KEY "your-api-key-here"`'
  prefs: []
  type: TYPE_NORMAL
- en: 'or in Linux using:'
  prefs: []
  type: TYPE_NORMAL
- en: '`export OPENAI_API_KEY "your-api-key-here"`'
  prefs: []
  type: TYPE_NORMAL
- en: in the current session you are using.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This function now constructs the prompt in the format for the GPT-3.5 API. We
    can interact with the GPT-3.5 model through the chat-completions API provided
    by the library. The API requires messages to be structured as a list of dictionaries
    for sending to the API. Each message must specify the role and the content. The
    conventions followed regarding the ***“system”,*** “***user***”, and “***assistant***”
    roles are the same as those described earlier for the Llama-7B Chat Model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now use the GPT-3.5 API to process the test set and obtain the responses.
    After receiving all the responses, we extract the options from the model’s responses
    and calculate the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Our performance now stands at 63%. This is a significant improvement from the
    performance of Llama 2–7B. This isn’t surprising, given that GPT-3.5 is likely
    much larger and trained on more data than Llama 2–7B, along with other proprietary
    optimizations that OpenAI may have included to the model. Let’s see how well few-shot
    prompting works now.
  prefs: []
  type: TYPE_NORMAL
- en: Prompting GPT-3.5 with a Few-Shot Prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To provide few-shot examples to the LLM, we reuse the three examples we sampled
    from the training set and append them to the prompt. For GPT-3.5, we create a
    list of messages with examples, similar to our earlier processing for Llama 2\.
    The inputs are appended using the “user” role, and the corresponding option is
    presented in the “assistant” role. We reuse the earlier function for building
    few-shot prompts.
  prefs: []
  type: TYPE_NORMAL
- en: This is again equivalent to creating a fictional multi-turn conversation history
    provided to GPT-3.5, where each turn corresponds to an example demonstration.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s now obtain the outputs using GPT-3.5.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We’ve managed to push the performance from 63% to 67% using few-shot prompting!
    This is a significant improvement, highlighting the value of providing task demonstrations
    to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Prompting GPT-3.5 with CoT Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now evaluate GPT-3.5 with CoT prompting. We re-use the same CoT prompt
    and get the outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Using CoT prompting with GPT-3.5 results in an accuracy of 71%! This represents
    a further 4% improvement over few-shot prompting. It appears that enabling the
    model to “think” out loud before answering the question is beneficial for this
    task. This is also consistent with the findings of the paper [6] that CoT unlocked
    performance improvements for larger parameter models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conclusion and Takeaways:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompting is a crucial skill for working with Large Language Models (LLMs),
    and understanding that there are various tools in the prompting toolkit that can
    help extract better performance from LLMs for your tasks depending on the context.
    I hope this article serves as a broad and (hopefully!) accessible introduction
    to this subject. However, it does not aim to provide a comprehensive overview
    of all prompting strategies. Prompting remains a highly active field of research,
    with numerous methods being introduced such as ReAct [13], Tree-of-Thought prompting
    [14] etc. I recommend exploring these techniques to better understand them and
    enhance your prompting toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I’ve aimed to make all experiments as deterministic and reproducible
    as possible. We use greedy decoding to obtain our outputs for zero-shot, few-shot,
    and CoT prompting with Llama-2\. While these scores should technically be reproducible,
    in rare cases, Cuda/GPU-related or library issues could lead to slightly different
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, when obtaining responses from the GPT-3.5 API, we use a temperature
    of 0 to get results and choose only the next most likely token without sampling
    for all prompt settings. This makes the results [“mostly deterministic”](https://platform.openai.com/docs/guides/text-generation/reproducible-outputs),
    so it is possible that sending the same prompts to GPT-3.5 again may result in
    slightly different results.
  prefs: []
  type: TYPE_NORMAL
- en: I have provided the outputs of the models under all prompt settings, along with
    the sub-sampled test set, few-shot prompt examples, and CoT prompt (from the MedPALM
    paper) for reproducing the scores reported in this article.
  prefs: []
  type: TYPE_NORMAL
- en: '**References:**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All papers referred to in this blog post are listed here. Please let me know
    if I might have missed out any references, and I will add them!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1] Yang, J., Jin, H., Tang, R., Han, X., Feng, Q., Jiang, H., … & Hu, X. (2023).
    Harnessing the power of llms in practice: A survey on chatgpt and beyond. *arXiv
    preprint arXiv:2304.13712*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).
    Language models are unsupervised multitask learners. *OpenAI blog*, *1*(8), 9.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P.,
    … & Amodei, D. (2020). Language models are few-shot learners. *Advances in neural
    information processing systems*, *33*, 1877–1901.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., … & Le,
    Q. V. (2021). Finetuned language models are zero-shot learners. *arXiv preprint
    arXiv:2109.01652*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P.,
    … & Lowe, R. (2022). Training language models to follow instructions with human
    feedback. *Advances in Neural Information Processing Systems*, *35*, 27730–27744.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., … & Zhou,
    D. (2022). Chain-of-thought prompting elicits reasoning in large language models.
    *Advances in Neural Information Processing Systems*, *35*, 24824–24837.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A.,
    Cheng, H. T., … & Le, Q. (2022). Lamda: Language models for dialog applications.
    *arXiv preprint arXiv:2201.08239*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts,
    A., … & Fiedel, N. (2023). Palm: Scaling language modeling with pathways. *Journal
    of Machine Learning Research*, *24*(240), 1–113.'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Jin, D., Pan, E., Oufattole, N., Weng, W. H., Fang, H., & Szolovits, P.
    (2021). What disease does this patient have? a large-scale open domain question
    answering dataset from medical exams. *Applied Sciences*, *11*(14), 6421.'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., … & Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W.,
    … & Natarajan, V. (2023). Large language models encode clinical knowledge. *Nature*,
    *620*(7972), 172–180.'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., & Cao,
    Y. (2022, September). ReAct: Synergizing Reasoning and Acting in Language Models.
    In *The Eleventh International Conference on Learning Representations*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., & Narasimhan,
    K. (2024). Tree of thoughts: Deliberate problem solving with large language models.
    *Advances in Neural Information Processing Systems*, *36*.'
  prefs: []
  type: TYPE_NORMAL
