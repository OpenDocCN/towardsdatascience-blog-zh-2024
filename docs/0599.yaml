- en: 'Real-Time Hand Tracking and Gesture Recognition with MediaPipe: Rerun Showcase'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/real-time-hand-tracking-and-gesture-recognition-with-mediapipe-rerun-showcase-9ec57cb0c831?source=collection_archive---------4-----------------------#2024-03-05](https://towardsdatascience.com/real-time-hand-tracking-and-gesture-recognition-with-mediapipe-rerun-showcase-9ec57cb0c831?source=collection_archive---------4-----------------------#2024-03-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to visualise MediaPipe’s Hand Tracking and Gesture Recognition with Rerun
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://andreasnaoum.medium.com/?source=post_page---byline--9ec57cb0c831--------------------------------)[![Andreas
    Naoum](../Images/e14d545f270170877e0af31572275e17.png)](https://andreasnaoum.medium.com/?source=post_page---byline--9ec57cb0c831--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9ec57cb0c831--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9ec57cb0c831--------------------------------)
    [Andreas Naoum](https://andreasnaoum.medium.com/?source=post_page---byline--9ec57cb0c831--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9ec57cb0c831--------------------------------)
    ·8 min read·Mar 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ec748453f3b800b051414e9d6c7ca7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Hand Tracking and Gesture Recognition | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In this post, I’m presenting an example of **Hand Tracking and Gesture Recognition
    using** [**MediaPipe**](https://mediapipe-studio.webapps.google.com/home) **Python**
    **and** [**Rerun SDK**](https://www.rerun.io).
  prefs: []
  type: TYPE_NORMAL
- en: If you’re interested in delving deeper and expanding your understanding, I will
    guide you on **how to install** [**MediaPipe**](https://mediapipe-studio.webapps.google.com/home)
    **Python** **and** [**Rerun SDK**](https://www.rerun.io)to track a hand, recognise
    different gestures and visualise the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, you’ll learn:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How to install MediaPipe Python and Rerun
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use [MediaPipe Gesture Recognition](https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer#models)
    for Hand Tracking and Gesture Recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to visualise the results of the hand-tracking and gesture recognition in
    the [Rerun Viewer](https://www.rerun.io/docs/getting-started/viewer-walkthrough)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you’re just eager to give the example a try, simply use the provided code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Hand Tracking and Gesture Recognition Technology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we proceed, let’s give credit to the technology that makes this possible.
    The hand tracking and gesture recognition technology aims to give the ability
    of the devices to interpret hand movements and gestures as commands or inputs.
    At the core of this technology, a pre-trained machine-learning model analyses
    the visual input and identifies hand landmarks and hand gestures. The real applications
    of such technology vary, as hand movements and gestures can be used to control
    smart devices. Human-Computer Interaction, Robotics, Gaming, and Augmented Reality
    are a few of the fields where the potential applications of this technology appear
    most promising.
  prefs: []
  type: TYPE_NORMAL
- en: However, we should always be conscious of how we use such a technology. It’s
    really challenging to use it in sensitive and critical systems because the model
    can misinterpret gestures and the potential for false positives or negatives is
    not minimal. Ethical and legal challenges arise from utilising this, as users
    may not want their gestures to be recorded especially in public spaces. If you
    intend to implement this technology in real-world scenarios, it’s important to
    take into account any ethical and legal considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites & Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, you need to install the necessary libraries, including OpenCV, MediaPipe
    and Rerun. [**MediaPipe Python**](https://mediapipe-studio.webapps.google.com/home)
    is a handy tool for developers looking to integrate on-device ML solutions for
    computer vision and machine learning, and [**Rerun**](https://www.rerun.io) is
    an SDK for visualizing multimodal data that changes over time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you have to download the predefined model from here: [HandGestureClassifier](https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/latest/gesture_recognizer.task)'
  prefs: []
  type: TYPE_NORMAL
- en: Hand Tracking and Gesture Recognition using MediaPipe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/47ff9b3771ef5e9e98811c796da311bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Image via [Gesture Recognition Task Guide](https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer)
    by [Google](https://about.google/brand-resource-center/)
  prefs: []
  type: TYPE_NORMAL
- en: “The MediaPipe Gesture Recognizer task lets you recognize hand gestures in real
    time, and provides the recognized hand gesture results along with the landmarks
    of the detected hands. You can use this task to recognize specific hand gestures
    from a user, and invoke application features that correspond to those gestures.”
    from [Gesture Recognition Task Guide](https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now, let’s try to use the MediaPipe pre-trained model for gesture recognition
    for a sample image. Overall, the below code sets the foundation for initialising
    and configuring a MediaPipe Gesture Recognition solution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `detect` function within the `GestureDetectorLogger` class accepts an image
    as its argument and prints the model results, highlighting the top recognized
    gesture and the detected hand landmarks. For additional details regarding the
    model, refer to its [model card](https://storage.googleapis.com/mediapipe-assets/gesture_recognizer/model_card_hand_gesture_classification_with_faireness_2022.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af87cc0b5c9dacee7900b4dab9433aee.png)'
  prefs: []
  type: TYPE_IMG
- en: Image via [Gesture Recognition Task Guide](https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer)
    by [Google](https://about.google/brand-resource-center/)
  prefs: []
  type: TYPE_NORMAL
- en: 'You can try it by yourself using the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Verify, Debug and Demo using Rerun
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This step allows you to ensure the reliability and effectiveness of your solution.
    With the model now prepared, visualise the results to verify the accuracy, debug
    any potential issues, and demonstrate its capabilities. Visualising the results
    could be simple and fast using Rerun SDK.
  prefs: []
  type: TYPE_NORMAL
- en: How do we use Rerun?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/dd9474d4f31555cc7fc2c9f655597e18.png)'
  prefs: []
  type: TYPE_IMG
- en: Image via [Rerun Docs](https://www.rerun.io/docs) by [Rerun](https://www.rerun.io)
  prefs: []
  type: TYPE_NORMAL
- en: Stream multimodal data from your code by logging it with the Rerun SDK
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualise and interact with **live or recorded streams**, whether local or remote
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interactively build layouts and customize visualisations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extend Rerun when you need to
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before getting into the code, you should visit the page [installing the Rerun
    Viewer](https://www.rerun.io/docs/getting-started/installing-viewer) to install
    the Viewer. Then, I highly suggested getting familiar with Rerun SDK by reading
    these guides [Python Quick Start](https://www.rerun.io/docs/getting-started/python)
    and [Logging Data in Python](https://www.rerun.io/docs/getting-started/logging-python).
    These initial steps will ensure a smooth setup and help you get started with the
    upcoming code implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Run from Video or Real-Time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For video streaming, OpenCV is employed. You can select either a file path for
    a specific video or access your own camera by providing an argument of 0 or 1
    (use 0 for the default camera; on Mac, you may use 1).
  prefs: []
  type: TYPE_NORMAL
- en: It’s noteworthy to emphasise the introduction of [timelines](https://www.rerun.io/docs/concepts/timelines).
    Rerun timelines’ functions enable the association of data with one or more timelines.
    Consequently, each frame of the video is associated with its corresponding timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Logging Data for Visualisation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/669757c874516e02624cf43b52a69a60.png)'
  prefs: []
  type: TYPE_IMG
- en: Logging 2D data using Rerun SDK | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: To visualise the data in the Rerun Viewer, it’s essential to log the data using
    the Rerun SDK. The guides mentioned earlier provide insights into this process.
    In this context, we extract hand landmark points as normalized values, and then
    utilise the image’s width and height for conversion into image coordinates. These
    coordinates are then logged as [2D points](https://www.rerun.io/docs/reference/types/archetypes/points2d)
    to the Rerun SDK. Additionally, we identify connections between the landmarks
    and log them as [2D linestrips](https://www.rerun.io/docs/reference/types/archetypes/line_strips2d).
  prefs: []
  type: TYPE_NORMAL
- en: For gesture recognition, the results are printed to the console. However, within
    the source code, you can explore a method to present these results to the viewer
    using [TextDocument](https://www.rerun.io/docs/reference/types/archetypes/text_document)
    and emojis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 3D Points
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we examine how we can present the hand landmarks as 3D points. We first
    define the connections between the points using keypoints from [Annotation Context](https://www.rerun.io/docs/concepts/annotation-context)
    in the init function, and then we log them as [3D points](https://www.rerun.io/docs/reference/types/archetypes/points3d).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/140447578f2915dba04718f2d11f1588.png)'
  prefs: []
  type: TYPE_IMG
- en: Logging 3D data using Rerun SDK | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You’re ready! Let the magic begin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The full source code for this example is available on [GitHub](https://github.com/rerun-io/rerun/tree/main/examples/python/gesture_detection).
    Feel free to explore, modify, and understand the inner workings of the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond Hand-Tracking and Gesture Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e3e98cf7eca2c494781c91cf7e150a09.png)'
  prefs: []
  type: TYPE_IMG
- en: Rerun Examples | Image by [Rerun](https://www.rerun.io)
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if you have a keen interest in visualising streams of multimodal data
    across a diverse range of applications, I encourage you to explore the [Rerun
    Examples](https://www.rerun.io/examples/real-data). These examples highlight potential
    real-world cases and provide valuable insights into the practical applications
    of such visualisation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you found this article useful and insightful, there’s more coming! I regularly
    share in-depth content on robotics and computer vision visualisation posts that
    you won’t want to miss. For future updates and exciting projects, follow me!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, you can find me on* [***LinkedIn***](http://www.linkedin.com/in/andreas-naoum)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: '*Similar articles:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://ai.gopubby.com/real-time-face-and-face-landmark-detection-with-mediapipe-rerun-showcase-40481baa1763?source=post_page-----9ec57cb0c831--------------------------------)
    [## Real-Time Face and Face Landmark Detection with MediaPipe: Rerun Showcase'
  prefs: []
  type: TYPE_NORMAL
- en: How to easily visualise MediaPipe’s face and face landmark detection in 2D and
    3D with Rerun
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ai.gopubby.com](https://ai.gopubby.com/real-time-face-and-face-landmark-detection-with-mediapipe-rerun-showcase-40481baa1763?source=post_page-----9ec57cb0c831--------------------------------)
    [](/human-pose-tracking-with-mediapipe-rerun-showcase-125053cfe64f?source=post_page-----9ec57cb0c831--------------------------------)
    [## Human Pose Tracking with MediaPipe in 2D and 3D: Rerun Showcase'
  prefs: []
  type: TYPE_NORMAL
- en: How to easily visualise MediaPipe’s human pose tracking with Rerun
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/human-pose-tracking-with-mediapipe-rerun-showcase-125053cfe64f?source=post_page-----9ec57cb0c831--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Portions of this page are reproduced from work created and [shared by Google](https://developers.google.com/readme/policies)
    and used according to terms described in the [Creative Commons 4.0 Attribution
    License](https://creativecommons.org/licenses/by/4.0/).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
