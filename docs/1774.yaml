- en: Let’s reproduce NanoGPT with JAX!(Part 1)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们用 JAX 重建 NanoGPT！（第1部分）
- en: 原文：[https://towardsdatascience.com/lets-reproduce-nanogpt-with-jax-part-1-95bec4630eb4?source=collection_archive---------2-----------------------#2024-07-21](https://towardsdatascience.com/lets-reproduce-nanogpt-with-jax-part-1-95bec4630eb4?source=collection_archive---------2-----------------------#2024-07-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/lets-reproduce-nanogpt-with-jax-part-1-95bec4630eb4?source=collection_archive---------2-----------------------#2024-07-21](https://towardsdatascience.com/lets-reproduce-nanogpt-with-jax-part-1-95bec4630eb4?source=collection_archive---------2-----------------------#2024-07-21)
- en: '[Part 1: Build 124M GPT2 with JAX.](https://medium.com/@lou1swang/lets-reproduce-nanogpt-with-jax-part-1-95bec4630eb4)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[第1部分：使用 JAX 构建 124M GPT2。](https://medium.com/@lou1swang/lets-reproduce-nanogpt-with-jax-part-1-95bec4630eb4)'
- en: '[Part 2: Optimize the training speed in Single GPU.](https://medium.com/@lou1swang/lets-reproduce-nanogpt-with-jax-part-2-175k-1350k-tokens-sec-in-single-gpu-ff2664ef18d3)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[第2部分：在单GPU中优化训练速度。](https://medium.com/@lou1swang/lets-reproduce-nanogpt-with-jax-part-2-175k-1350k-tokens-sec-in-single-gpu-ff2664ef18d3)'
- en: 'Part 3: Multi-GPU Training in Jax.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 第3部分：在 JAX 中进行多GPU训练。
- en: '[](https://lou1swang.medium.com/?source=post_page---byline--95bec4630eb4--------------------------------)[![Louis
    Wang](../Images/259879ae7cd20b7843936d1f4cdbec52.png)](https://lou1swang.medium.com/?source=post_page---byline--95bec4630eb4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--95bec4630eb4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--95bec4630eb4--------------------------------)
    [Louis Wang](https://lou1swang.medium.com/?source=post_page---byline--95bec4630eb4--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://lou1swang.medium.com/?source=post_page---byline--95bec4630eb4--------------------------------)[![Louis
    Wang](../Images/259879ae7cd20b7843936d1f4cdbec52.png)](https://lou1swang.medium.com/?source=post_page---byline--95bec4630eb4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--95bec4630eb4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--95bec4630eb4--------------------------------)
    [Louis Wang](https://lou1swang.medium.com/?source=post_page---byline--95bec4630eb4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--95bec4630eb4--------------------------------)
    ·8 min read·Jul 21, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--95bec4630eb4--------------------------------)
    ·阅读时间：8分钟·2024年7月21日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Inspired by Andrej Karpathy’s recent youtube video on [Let’s reproduce GPT-2
    (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU&t=1646s), I’d like to rebuild
    it with most of the training optimizations in Jax. Jax is built for highly efficient
    computation speed, and it is quite interesting to compare Pytorch with its recent
    training optimization, and Jax with its related libraries like Flax (Layers API
    for neural network training for Jax)and Optax (a gradient processing and optimization
    library for JAX). We will quickly learn what is Jax, and rebuild the GPT with
    Jax. In the end, we will compare the token/sec with multiGPU training between
    Pytorch and Jax!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 Andrej Karpathy 最近的 YouTube 视频[让我们重建 GPT-2（124M）](https://www.youtube.com/watch?v=l8pRSuU81PU&t=1646s)的启发，我想用
    JAX 重建它，并进行大多数训练优化。JAX 专为高效计算速度而构建，非常有趣的是，可以将 Pytorch 与其最近的训练优化以及 JAX 与其相关库（如
    Flax：JAX 的神经网络训练层 API 和 Optax：JAX 的梯度处理和优化库）进行对比。我们将迅速了解 JAX，并用 JAX 重建 GPT。最后，我们将比较
    Pytorch 和 JAX 在多GPU训练中的 token/sec。
- en: '![](../Images/327bdd2b1dfc0479960467df61f2d5da.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/327bdd2b1dfc0479960467df61f2d5da.png)'
- en: AI generated GPT
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: AI 生成的 GPT
- en: What is Jax?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 Jax？
- en: Based on its [readthedoc](https://jax.readthedocs.io/en/latest/index.html),
    JAX is a Python library for accelerator-oriented array computation and program
    transformation, designed for high-performance numerical computing and large-scale
    machine learning. I would like to introduce JAX with its name. While someone calls
    it Just Another [XLA](https://github.com/openxla/xla) (Accelerated Linear Algibra),
    I prefer to call it J(it) A(utograd) X(LA) to demonstrate its capability of high
    efficiency.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其[readthedoc](https://jax.readthedocs.io/en/latest/index.html)，JAX 是一个面向加速器的数组计算和程序转换的
    Python 库，旨在实现高性能的数值计算和大规模机器学习。我想用它的名字来介绍 JAX。虽然有人称它为 Just Another [XLA](https://github.com/openxla/xla)（加速线性代数），我更愿意称其为
    J(it) A(utograd) X(LA)，以展示它的高效能力。
- en: J — Just-in-time (JIT) Compilation. When you run your python function, Jax converts
    it into a primitive set of operation called Jaxpr. Then the Jaxpr expression will
    be converted into an input for XLA, which compiles the lower-level scripts to
    produce an optimized exutable for target device (CPU, GPU or TPU).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: J — Just-in-time (JIT) 编译。当你运行Python函数时，Jax将其转换为一组基本操作，称为Jaxpr。然后，Jaxpr表达式会被转换为XLA的输入，XLA将其编译成底层脚本，从而为目标设备（CPU、GPU或TPU）生成优化后的可执行文件。
- en: A — Autograd. Computing gradients is a critical part of modern machine learning
    methods, and you can just call `jax.grad()` to get gradients which enables you
    to optimize the models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: A — Autograd。计算梯度是现代机器学习方法中的一个关键部分，你只需要调用`jax.grad()`来获取梯度，从而优化模型。
- en: X — XLA. This is a open-source machine learning compiler for CPU, GPU and ML
    accelerators. In general, XLA performs several built-in optimization and analysis
    passes on the [StableHLO](https://github.com/openxla/stablehlo) graph, then sends
    the HLO computation to a backend for further HLO-level optimizations. The backend
    then performs target-specific code generation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: X — XLA。这是一个开源的机器学习编译器，支持CPU、GPU和ML加速器。通常，XLA会对[StableHLO](https://github.com/openxla/stablehlo)图进行几个内建的优化和分析传递，然后将HLO计算发送到后端进行进一步的HLO级别优化。后端再进行特定目标的代码生成。
- en: Those are just some key features of JAX, but it also has many user friendly
    numpy-like APIs in `jax.numpy` , and automatic vectorization with `jax.vmap` ,
    and parallize your codes into multiple devices via `jax.pmap` . We will cover
    more Jax concepts nd applications in the futher blogs, but now let’s reproduct
    the NanoGPT with Jax!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是JAX的一些关键特性，但它还有许多类似于numpy的用户友好API，如`jax.numpy`，以及通过`jax.vmap`进行的自动向量化，和通过`jax.pmap`将代码并行化到多个设备上。我们将在以后的博客中介绍更多Jax的概念和应用，但现在让我们用Jax复现NanoGPT！
- en: From Attention to Transformer
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从注意力机制到变换器（Transformer）
- en: GPT is a decoder-only transformer model, and the key building block is Attention
    module. We can first define a model config dataclass to save the model hyperparameters
    of the model, so that the model module can consume it efficiently to initialize
    the model architecture. Similar to the 124M GPT model, here we initialize a 12-layer
    transformer decoder with 12 heads and vocab size as 50257 tokens, each of which
    has 768 embedding dimension. The block size for the attention calculation is 1024.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 是一种仅解码的变换器模型，关键构建模块是注意力模块。我们可以首先定义一个模型配置数据类来保存模型的超参数，这样模型模块就能高效地使用它来初始化模型架构。类似于124M
    GPT模型，在这里我们初始化一个12层的变换器解码器，具有12个头和50257个词汇表大小，每个词汇表项有768维嵌入向量。注意力计算的块大小为1024。
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next comes to the key building block of the transformer model — Attention.
    The idea is to process the inputs into three weight matrics: Key, Query, and Value.
    Here we rely on the `flax` , a the Jax Layer and training API library to initialize
    the 3 weight matrix, by just call the `[flax.linen.Dense](https://flax.readthedocs.io/en/v0.5.3/_autosummary/flax.linen.Dense.html)`
    . As mentioned, Jax has many numpy-like APIs, so we reshape the outputs after
    the weight matrix with `[jax.numpy.reshape](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.reshape.html)`
    from [batch_size, sequence_length, embedding_dim] to [batch_size, sequence_length,
    num_head, embedding_dim / num_head]. Since we need to do matrix multiplication
    on the key and value matrics, jax also has `[jax.numpy.matmul](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.matmul.html)`
    API and `[jax.numpy.transpose](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.transpose.html)`
    (transpose the key matrix for multiplication).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是变换器模型的关键构建模块——注意力机制（Attention）。其思想是将输入处理成三个权重矩阵：Key、Query和Value。在这里，我们依赖于`flax`，这是一个Jax层和训练API库，用来初始化这三个权重矩阵，只需要调用`[flax.linen.Dense](https://flax.readthedocs.io/en/v0.5.3/_autosummary/flax.linen.Dense.html)`。如前所述，Jax有许多类似numpy的API，因此我们使用`[jax.numpy.reshape](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.reshape.html)`将权重矩阵后的输出从[batch_size,
    sequence_length, embedding_dim]重塑为[batch_size, sequence_length, num_head, embedding_dim
    / num_head]。由于我们需要对Key和Value矩阵执行矩阵乘法，jax还提供了`[jax.numpy.matmul](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.matmul.html)`和`[jax.numpy.transpose](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.transpose.html)`
    API（用于转置Key矩阵以进行乘法运算）。
- en: '![](../Images/8f2916bfb42338ef17e1526a677e4f85.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f2916bfb42338ef17e1526a677e4f85.png)'
- en: Multihead Attention
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力（Multihead Attention）
- en: Note that we need to put a mask on the attention matrix to avoid information
    leakage (prevent the previous tokens to have access to the later tokens), `[jax.numpy.tril](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tril.html)`
    helps build a lower triangle array, and `[jax.numpy.where](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html)`
    can fill the infinite number for us to get 0 after softmax `[jax.nn.softmax](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.softmax.html)`
    . The full codes of multihead attention can be found below.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们需要在注意力矩阵上加上一个掩码，以避免信息泄漏（防止之前的 tokens 访问到后面的 tokens），`[jax.numpy.tril](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tril.html)`
    帮助构建一个下三角数组，而 `[jax.numpy.where](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html)`
    可以为我们填充无限大的数值，以便在 softmax `[jax.nn.softmax](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.softmax.html)`
    后得到 0。多头注意力的完整代码如下所示。
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You may notice that there is no `__init__` or `forward` methods as you can see
    in the pytorch. This is the special thing for jax, where you can explicitly define
    the layers with `setup` methods, or implicitly define them withn the forward pass
    by adding `nn.compact` on top of `__call__` method. [[ref](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/setup_or_nncompact.html)]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，在 Pytorch 中常见的 `__init__` 或 `forward` 方法在这里并不存在。这是 jax 的特点，在 jax 中你可以显式地通过
    `setup` 方法定义层，或者通过在 `__call__` 方法上添加 `nn.compact` 来隐式定义它们。[[参考](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/setup_or_nncompact.html)]
- en: Next let’s build the MLP and Block layer, which includes Dense layer, Gelu activation
    function, LayerNorm and Dropout. Again flax.linen has the layer APIs to help us
    build the module. Note that we will pass a `deterministic` boolean variable to
    control different behaviors during training or evaluation for some layers like
    Dropout.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们构建 MLP 和 Block 层，包括 Dense 层、Gelu 激活函数、LayerNorm 和 Dropout。再次，flax.linen
    提供了层的 API，帮助我们构建模块。请注意，我们会传递一个 `deterministic` 布尔变量来控制某些层（如 Dropout）在训练或评估期间的不同行为。
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now Let’s use the above blocks to build the NanoGPT:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用上述模块来构建 NanoGPT：
- en: Given the inputs of a sequence token ids, we use the `[flax.linen.Embed](https://flax.readthedocs.io/en/v0.5.3/_autosummary/flax.linen.Embed.html)`
    layer to get position embeddings and token embeddings. Them we pass them into
    the Block module N times, where N is number of the layers defined in the Model
    Config. In the end, we map the outputs from the last Block into the probabilities
    for each token in the vocab to predict the next token. Besides the forward `__call__`
    method, let’s also create a `init` methods to get the dummy inputs to get the
    model’s parameters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个序列的 token ids 输入，我们使用 `[flax.linen.Embed](https://flax.readthedocs.io/en/v0.5.3/_autosummary/flax.linen.Embed.html)`
    层来获取位置嵌入和 token 嵌入。然后，我们将它们传入 Block 模块 N 次，其中 N 是模型配置中定义的层数。最后，我们将来自最后一个 Block
    的输出映射到每个词汇表 token 的概率，以预测下一个 token。除了前向 `__call__` 方法之外，我们还需要创建一个 `init` 方法来获取虚拟输入并获得模型的参数。
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now let’s varify the number of parameters: We first initialize the model config
    dataclass and the random key, then create a dummy inputs and feed in into the
    GPT model. Then we utilize the `jax.util.treemap` API to create a count parameter
    function. We got **124439808** (124M) parameters, same amount as Huggingface’s
    GPT2, BOOM!'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们验证一下参数的数量：我们首先初始化模型配置的数据类和随机密钥，然后创建一个虚拟输入并将其输入到 GPT 模型中。接着，我们利用 `jax.util.treemap`
    API 创建一个计数参数函数。我们得到了 **124439808**（124M）个参数，与 Huggingface 的 GPT2 相同，哇！
- en: '![](../Images/513bfdf90096dcb03e3cd4a76910d2d7.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/513bfdf90096dcb03e3cd4a76910d2d7.png)'
- en: 'Colab Result: number of parameters'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Colab 结果：参数数量
- en: '![](../Images/576fc03b5cb1912c0ba770b510de7b73.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/576fc03b5cb1912c0ba770b510de7b73.png)'
- en: Verify number of params in huggingface’s GPT2
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 验证 Huggingface 的 GPT2 参数数量
- en: '**DataLoader and Training Loop**'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数据加载器和训练循环**'
- en: Let’s now overfit a small dataset. To make it comparable inAndrej’s video on
    Pytorch NanoGPT, let’s use the toy [dataset](https://github.com/karpathy/build-nanogpt/blob/master/input.txt)
    that he shared in his video. We use the GPT2' tokenizer from `tiktoken` library
    to tokenize all the texts from the input file, and convert the tokens into `jax.numpy.array`
    for Jax’s model training.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在一个小数据集上进行过拟合。为了与 Andrej 的 Pytorch NanoGPT 视频中进行对比，我们使用他在视频中分享的玩具 [dataset](https://github.com/karpathy/build-nanogpt/blob/master/input.txt)。我们使用
    `tiktoken` 库的 GPT2 分词器对输入文件中的所有文本进行分词，并将这些 token 转换为 `jax.numpy.array` 以便 Jax
    的模型训练。
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/da59795faf894d9f4bd6f4d1ed782ece.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da59795faf894d9f4bd6f4d1ed782ece.png)'
- en: 'Colab Result: Simple dataloader with 4 batch size and 128 sequence length'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Colab 结果：简单的数据加载器，批量大小为 4，序列长度为 128
- en: 'Next, let’s forget distributed training and optimization first, and just create
    a naive training loop for a sanity check. The first thing after intialize the
    model is to create a [TrainState](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.train_state.TrainState),
    a model state where we can update the parameters and gradients. The TrainState
    takes three important inputs: apply_fn (model forward function), params (model
    parameters from the init method), and tx (an Optax gradient transformation).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们暂时忽略分布式训练和优化，先创建一个简单的训练循环进行基本检查。初始化模型后的第一件事是创建一个[TrainState](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.train_state.TrainState)，这是一个可以更新参数和梯度的模型状态。TrainState
    接受三个重要输入：apply_fn（模型前向函数）、params（来自初始化方法的模型参数）和 tx（一个 Optax 梯度变换）。
- en: Then we use the train_step function to update the model state (gradients and
    parameters) to proceed the model training. `Optax` provide the softmax cross entropy
    as the loss function for the next token prediction task, and `jax.value_and_grad`
    calculates the gradients and the loss value for the loss function. Finally, we
    update the model’s state with the new parameters using the `apply_gradients` API.
    [[ref](https://flax.readthedocs.io/en/latest/_modules/flax/training/train_state.html)]
    Don’t forget to jit the train_step function to reduce the computation overhead!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用 train_step 函数来更新模型状态（梯度和参数），以继续模型训练。`Optax` 提供了用于下一个令牌预测任务的 softmax 交叉熵作为损失函数，`jax.value_and_grad`
    用于计算损失函数的梯度和损失值。最后，我们使用 `apply_gradients` API 更新模型的状态和新参数。[[ref](https://flax.readthedocs.io/en/latest/_modules/flax/training/train_state.html)]
    别忘了对 train_step 函数进行 JIT 编译，以减少计算开销！
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now everything is ready for the poorman’s training loop.. Let’s check the loss
    value. The model’s prediction should be better than the random guess, so the loss
    should be lower than -ln(1/50257)≈10.825\. What we expect from the overfitting
    a single batch is that: in the beginning the loss is close to 10.825, then it
    goes down to close to 0\. Let’s take a batch of (x, y) and run the training loop
    for 50 times. I also add similar log to calculate the training speed.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切准备就绪，可以开始进行简单的训练循环了……让我们检查损失值。模型的预测应该优于随机猜测，因此损失值应该低于 -ln(1/50257)≈10.825。我们对单批次过拟合的预期是：一开始损失接近
    10.825，然后下降到接近 0。让我们取一批（x，y）并运行训练循环 50 次。我还添加了类似的日志来计算训练速度。
- en: '![](../Images/add7d951685406ff5cfce20cccb41414.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/add7d951685406ff5cfce20cccb41414.png)'
- en: As we can see, the loss value is exactly what we expect, and the training throughput
    is around 400–500 k token/sec. Which is already 40x faster than Pytorch’s initial
    version without any optimization in Andrej’s video. Note that we run the Jax scripts
    in 1 A100 GPU which should remove the hardware difference for the speed comparison.
    There is no `.to(device)` stuff to move your model or data from host CPU to device
    GPU, which is one of the benefits from Jax!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，损失值正是我们预期的，训练吞吐量大约是 400–500 k token/sec。这已经比 Andrej 视频中没有任何优化的 Pytorch
    初始版本快了 40 倍。请注意，我们是在 1 个 A100 GPU 上运行 Jax 脚本，这应该消除了硬件差异对速度比较的影响。这里没有 `.to(device)`
    的操作来将模型或数据从主机 CPU 移动到设备 GPU，这正是 Jax 的一个优势！
- en: So that’s it and we made it. We will make the training 10x more faster in Part
    2 with more optimizations…
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们做到了。我们将在第二部分通过更多优化将训练速度提升至原来的 10 倍…
- en: '[Part 2](https://lou1swang.medium.com/lets-reproduce-nanogpt-with-jax-part-2-175k-1350k-tokens-sec-in-single-gpu-ff2664ef18d3):
    The journey of training optimization to 1350k tokens/sec in a single GPU!'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[第二部分](https://lou1swang.medium.com/lets-reproduce-nanogpt-with-jax-part-2-175k-1350k-tokens-sec-in-single-gpu-ff2664ef18d3)：训练优化之旅，如何在单个
    GPU 上达到 1350k tokens/sec！'
- en: “Unless otherwise noted, all images are by the author”
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: “除非另有说明，所有图片均为作者所提供”
