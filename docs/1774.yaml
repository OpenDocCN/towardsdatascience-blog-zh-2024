- en: Let’s reproduce NanoGPT with JAX!(Part 1)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/lets-reproduce-nanogpt-with-jax-part-1-95bec4630eb4?source=collection_archive---------2-----------------------#2024-07-21](https://towardsdatascience.com/lets-reproduce-nanogpt-with-jax-part-1-95bec4630eb4?source=collection_archive---------2-----------------------#2024-07-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Part 1: Build 124M GPT2 with JAX.](https://medium.com/@lou1swang/lets-reproduce-nanogpt-with-jax-part-1-95bec4630eb4)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Part 2: Optimize the training speed in Single GPU.](https://medium.com/@lou1swang/lets-reproduce-nanogpt-with-jax-part-2-175k-1350k-tokens-sec-in-single-gpu-ff2664ef18d3)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Multi-GPU Training in Jax.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://lou1swang.medium.com/?source=post_page---byline--95bec4630eb4--------------------------------)[![Louis
    Wang](../Images/259879ae7cd20b7843936d1f4cdbec52.png)](https://lou1swang.medium.com/?source=post_page---byline--95bec4630eb4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--95bec4630eb4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--95bec4630eb4--------------------------------)
    [Louis Wang](https://lou1swang.medium.com/?source=post_page---byline--95bec4630eb4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--95bec4630eb4--------------------------------)
    ·8 min read·Jul 21, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by Andrej Karpathy’s recent youtube video on [Let’s reproduce GPT-2
    (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU&t=1646s), I’d like to rebuild
    it with most of the training optimizations in Jax. Jax is built for highly efficient
    computation speed, and it is quite interesting to compare Pytorch with its recent
    training optimization, and Jax with its related libraries like Flax (Layers API
    for neural network training for Jax)and Optax (a gradient processing and optimization
    library for JAX). We will quickly learn what is Jax, and rebuild the GPT with
    Jax. In the end, we will compare the token/sec with multiGPU training between
    Pytorch and Jax!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/327bdd2b1dfc0479960467df61f2d5da.png)'
  prefs: []
  type: TYPE_IMG
- en: AI generated GPT
  prefs: []
  type: TYPE_NORMAL
- en: What is Jax?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on its [readthedoc](https://jax.readthedocs.io/en/latest/index.html),
    JAX is a Python library for accelerator-oriented array computation and program
    transformation, designed for high-performance numerical computing and large-scale
    machine learning. I would like to introduce JAX with its name. While someone calls
    it Just Another [XLA](https://github.com/openxla/xla) (Accelerated Linear Algibra),
    I prefer to call it J(it) A(utograd) X(LA) to demonstrate its capability of high
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: J — Just-in-time (JIT) Compilation. When you run your python function, Jax converts
    it into a primitive set of operation called Jaxpr. Then the Jaxpr expression will
    be converted into an input for XLA, which compiles the lower-level scripts to
    produce an optimized exutable for target device (CPU, GPU or TPU).
  prefs: []
  type: TYPE_NORMAL
- en: A — Autograd. Computing gradients is a critical part of modern machine learning
    methods, and you can just call `jax.grad()` to get gradients which enables you
    to optimize the models.
  prefs: []
  type: TYPE_NORMAL
- en: X — XLA. This is a open-source machine learning compiler for CPU, GPU and ML
    accelerators. In general, XLA performs several built-in optimization and analysis
    passes on the [StableHLO](https://github.com/openxla/stablehlo) graph, then sends
    the HLO computation to a backend for further HLO-level optimizations. The backend
    then performs target-specific code generation.
  prefs: []
  type: TYPE_NORMAL
- en: Those are just some key features of JAX, but it also has many user friendly
    numpy-like APIs in `jax.numpy` , and automatic vectorization with `jax.vmap` ,
    and parallize your codes into multiple devices via `jax.pmap` . We will cover
    more Jax concepts nd applications in the futher blogs, but now let’s reproduct
    the NanoGPT with Jax!
  prefs: []
  type: TYPE_NORMAL
- en: From Attention to Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT is a decoder-only transformer model, and the key building block is Attention
    module. We can first define a model config dataclass to save the model hyperparameters
    of the model, so that the model module can consume it efficiently to initialize
    the model architecture. Similar to the 124M GPT model, here we initialize a 12-layer
    transformer decoder with 12 heads and vocab size as 50257 tokens, each of which
    has 768 embedding dimension. The block size for the attention calculation is 1024.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next comes to the key building block of the transformer model — Attention.
    The idea is to process the inputs into three weight matrics: Key, Query, and Value.
    Here we rely on the `flax` , a the Jax Layer and training API library to initialize
    the 3 weight matrix, by just call the `[flax.linen.Dense](https://flax.readthedocs.io/en/v0.5.3/_autosummary/flax.linen.Dense.html)`
    . As mentioned, Jax has many numpy-like APIs, so we reshape the outputs after
    the weight matrix with `[jax.numpy.reshape](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.reshape.html)`
    from [batch_size, sequence_length, embedding_dim] to [batch_size, sequence_length,
    num_head, embedding_dim / num_head]. Since we need to do matrix multiplication
    on the key and value matrics, jax also has `[jax.numpy.matmul](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.matmul.html)`
    API and `[jax.numpy.transpose](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.transpose.html)`
    (transpose the key matrix for multiplication).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f2916bfb42338ef17e1526a677e4f85.png)'
  prefs: []
  type: TYPE_IMG
- en: Multihead Attention
  prefs: []
  type: TYPE_NORMAL
- en: Note that we need to put a mask on the attention matrix to avoid information
    leakage (prevent the previous tokens to have access to the later tokens), `[jax.numpy.tril](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tril.html)`
    helps build a lower triangle array, and `[jax.numpy.where](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html)`
    can fill the infinite number for us to get 0 after softmax `[jax.nn.softmax](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.softmax.html)`
    . The full codes of multihead attention can be found below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You may notice that there is no `__init__` or `forward` methods as you can see
    in the pytorch. This is the special thing for jax, where you can explicitly define
    the layers with `setup` methods, or implicitly define them withn the forward pass
    by adding `nn.compact` on top of `__call__` method. [[ref](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/setup_or_nncompact.html)]
  prefs: []
  type: TYPE_NORMAL
- en: Next let’s build the MLP and Block layer, which includes Dense layer, Gelu activation
    function, LayerNorm and Dropout. Again flax.linen has the layer APIs to help us
    build the module. Note that we will pass a `deterministic` boolean variable to
    control different behaviors during training or evaluation for some layers like
    Dropout.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now Let’s use the above blocks to build the NanoGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: Given the inputs of a sequence token ids, we use the `[flax.linen.Embed](https://flax.readthedocs.io/en/v0.5.3/_autosummary/flax.linen.Embed.html)`
    layer to get position embeddings and token embeddings. Them we pass them into
    the Block module N times, where N is number of the layers defined in the Model
    Config. In the end, we map the outputs from the last Block into the probabilities
    for each token in the vocab to predict the next token. Besides the forward `__call__`
    method, let’s also create a `init` methods to get the dummy inputs to get the
    model’s parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s varify the number of parameters: We first initialize the model config
    dataclass and the random key, then create a dummy inputs and feed in into the
    GPT model. Then we utilize the `jax.util.treemap` API to create a count parameter
    function. We got **124439808** (124M) parameters, same amount as Huggingface’s
    GPT2, BOOM!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/513bfdf90096dcb03e3cd4a76910d2d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Colab Result: number of parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/576fc03b5cb1912c0ba770b510de7b73.png)'
  prefs: []
  type: TYPE_IMG
- en: Verify number of params in huggingface’s GPT2
  prefs: []
  type: TYPE_NORMAL
- en: '**DataLoader and Training Loop**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now overfit a small dataset. To make it comparable inAndrej’s video on
    Pytorch NanoGPT, let’s use the toy [dataset](https://github.com/karpathy/build-nanogpt/blob/master/input.txt)
    that he shared in his video. We use the GPT2' tokenizer from `tiktoken` library
    to tokenize all the texts from the input file, and convert the tokens into `jax.numpy.array`
    for Jax’s model training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/da59795faf894d9f4bd6f4d1ed782ece.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Colab Result: Simple dataloader with 4 batch size and 128 sequence length'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s forget distributed training and optimization first, and just create
    a naive training loop for a sanity check. The first thing after intialize the
    model is to create a [TrainState](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.train_state.TrainState),
    a model state where we can update the parameters and gradients. The TrainState
    takes three important inputs: apply_fn (model forward function), params (model
    parameters from the init method), and tx (an Optax gradient transformation).'
  prefs: []
  type: TYPE_NORMAL
- en: Then we use the train_step function to update the model state (gradients and
    parameters) to proceed the model training. `Optax` provide the softmax cross entropy
    as the loss function for the next token prediction task, and `jax.value_and_grad`
    calculates the gradients and the loss value for the loss function. Finally, we
    update the model’s state with the new parameters using the `apply_gradients` API.
    [[ref](https://flax.readthedocs.io/en/latest/_modules/flax/training/train_state.html)]
    Don’t forget to jit the train_step function to reduce the computation overhead!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now everything is ready for the poorman’s training loop.. Let’s check the loss
    value. The model’s prediction should be better than the random guess, so the loss
    should be lower than -ln(1/50257)≈10.825\. What we expect from the overfitting
    a single batch is that: in the beginning the loss is close to 10.825, then it
    goes down to close to 0\. Let’s take a batch of (x, y) and run the training loop
    for 50 times. I also add similar log to calculate the training speed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/add7d951685406ff5cfce20cccb41414.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the loss value is exactly what we expect, and the training throughput
    is around 400–500 k token/sec. Which is already 40x faster than Pytorch’s initial
    version without any optimization in Andrej’s video. Note that we run the Jax scripts
    in 1 A100 GPU which should remove the hardware difference for the speed comparison.
    There is no `.to(device)` stuff to move your model or data from host CPU to device
    GPU, which is one of the benefits from Jax!
  prefs: []
  type: TYPE_NORMAL
- en: So that’s it and we made it. We will make the training 10x more faster in Part
    2 with more optimizations…
  prefs: []
  type: TYPE_NORMAL
- en: '[Part 2](https://lou1swang.medium.com/lets-reproduce-nanogpt-with-jax-part-2-175k-1350k-tokens-sec-in-single-gpu-ff2664ef18d3):
    The journey of training optimization to 1350k tokens/sec in a single GPU!'
  prefs: []
  type: TYPE_NORMAL
- en: “Unless otherwise noted, all images are by the author”
  prefs: []
  type: TYPE_NORMAL
