- en: 'Prompt Like a Data Scientist: Auto Prompt Optimization and Testing with DSPy'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/prompt-like-a-data-scientist-auto-prompt-optimization-and-testing-with-dspy-ff699f030cb7?source=collection_archive---------0-----------------------#2024-05-05](https://towardsdatascience.com/prompt-like-a-data-scientist-auto-prompt-optimization-and-testing-with-dspy-ff699f030cb7?source=collection_archive---------0-----------------------#2024-05-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Applying machine learning methodology to prompt building
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jyipkl?source=post_page---byline--ff699f030cb7--------------------------------)[![Julian
    Yip](../Images/2afc0ac6c4dcccaa57ffe70b2f5a14d0.png)](https://medium.com/@jyipkl?source=post_page---byline--ff699f030cb7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ff699f030cb7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ff699f030cb7--------------------------------)
    [Julian Yip](https://medium.com/@jyipkl?source=post_page---byline--ff699f030cb7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ff699f030cb7--------------------------------)
    ·40 min read·May 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/266f734fa2fbe701359eb41fa9d06e9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Drawn by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs are grounded in data science, but our approach to prompt engineering might
    strike us as unscientific:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Manual prompt engineering which does not generalize well**: LLMs are highly
    sensitive to how they are prompted for each task, so we need to handcraft long
    strings of instructions and demonstrations. This requires not only time-consuming
    prompt writing process, but the given string prompt might not generalize to different
    pipelines or across different LMs, data domains, or even inputs. To deal with
    a new problem we often need to handcraft a new prompt.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Lack framework to conduct testing**: Instead of the usual train-test regime
    in typical data science applications to pick the model which maximizes a certain
    metric like AUC, with LLMs we arrive at the best prompt via trial and error, often
    without an objective metric to say how well our model is performing. Thus no matter
    how we try to improve the prompt, we can’t confidently say how reliable our application
    is.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To address these issues, Stanford NLP has published a [paper](https://arxiv.org/abs/2310.03714)
    introducing a new approach with prompt writing: instead of manipulating free-form
    strings, we generate prompts via modularized programming. The associated library,
    called DSPy, can be found [here](https://github.com/stanfordnlp/dspy).'
  prefs: []
  type: TYPE_NORMAL
- en: This article aims to show how this “prompt programming” is done, to go deeper
    in explaining what’s happening behind the optimization process. The code can also
    be found [here](https://github.com/yip-kl/llm_dspy_tutorial).
  prefs: []
  type: TYPE_NORMAL
- en: '*(Speaking of which, you might also find coaxing LLMs to output properly formatted
    JSON very unscientific too, I have also written an article about how to address
    this with Function Calling. Check it out !)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/build-autonomous-ai-agents-with-function-calling-0bb483753975?source=post_page-----ff699f030cb7--------------------------------)
    [## Build Autonomous AI Agents with Function Calling'
  prefs: []
  type: TYPE_NORMAL
- en: Transform your chatbot into an agent that can interact with external APIs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/build-autonomous-ai-agents-with-function-calling-0bb483753975?source=post_page-----ff699f030cb7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'We will spend some time to go over the environment preparation. Afterwards,
    this article is divided into 3 sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Basic concept of DSPy: Signature and Module***'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Basic building blocks in DSPy for describing your task, and the prompt technique
    used
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '***Optimizer: Train our prompt as with machine learning***'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How DSPy optimizes your prompt with bootstrapping
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '***Full fledged example: Prompt comparison with LLM***'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applying the rigour of traditional machine learning for prompt testing and selection
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are now ready to start!
  prefs: []
  type: TYPE_NORMAL
- en: Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Head over to [Github](https://github.com/yip-kl/llm_dspy_tutorial) to clone
    my code. The contents in my article can be found in the `dspy_tutorial` Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please also create and activate a virtual environment, then `pip install -r
    requirements.txt` to install the required packages. If you are on Windows, please
    also install Windows C++ build tools which are required for the `phoneix` library
    with which we will observe how DSPy works
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: My code uses OpenRouter, which allow us to access OpenAI API in blocked regions.
    Please set up your `OPENROUTER_API_KEY` as environment variable, and execute the
    code under the “Preparation” block. Alternatively, you can use `dspy.OpenAI` class
    directly and define Open AI API key if it works for you
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Basic concept of DSPy: Signature and Module'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: They are the building blocks of prompt programming in DSPy. Let’s dive in to
    see what they are about!
  prefs: []
  type: TYPE_NORMAL
- en: '**Signatures: Specification of input/output**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A signature is the most fundamental building block in DSPy’s prompt programming,
    which is a declarative specification of input/output behavior of a DSPy module.
    Signatures allow you to tell the LM **what** it needs to do, rather than specify
    how we should ask the LM to do it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we want to obtain the sentiment of a sentence, traditionally we might write
    such prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: But in DSPy, we can achieve the same by defining a `signature` as below. At
    its most basic form, a signature is as simple as a single string separating the
    inputs and output with a `->`
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: Code in this section contains those referred from DSPy’s documentation
    of* [*Signatures*](https://dspy-docs.vercel.app/docs/building-blocks/signatures)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The prediction is not a good one, but for instructional purpose let’s inspect
    what was the issued prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can see the above prompt is assembled from the `sentence -> sentiment` signature.
    But how did DSPy came up with the `Given the fields…` in the prompt?
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the `dspy.Predict()` class, we see when we pass to it our signature,
    the signature will be parsed as the `signature` attribute of the class, and subsequently
    assembled as a prompt. The `instructions` is a default one hardcoded in the DSPy
    library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: What if we want to provide a more detailed description of our objective to the
    LLM, beyond the basic `sentence -> sentiment` signature? To do so we need to provide
    a more verbose signature in form of **Class-based DSPy Signatures**.
  prefs: []
  type: TYPE_NORMAL
- en: Notice we provide no explicit instruction as to how the LLM should obtain the
    sentiment. We are just describing the task at hand, and also the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: It is now outputting a much better prediction! Again we see the descriptions
    we made when defining the class-based DSPy signatures are assembled into a prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This might do for simple tasks, but advanced applications might require sophisticated
    prompting techniques like Chain of Thought or ReAct. In DSPy these are implemented
    as **Modules**
  prefs: []
  type: TYPE_NORMAL
- en: '**Modules: Abstracting prompting techniques**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We may be used to apply “prompting techniques” by hardcoding phrases like `let’s
    think step by step` in our prompt . In DSPy these prompting techniques are abstracted
    as **Modules**. Let’s see below for an example of applying our class-based signature
    to the `dspy.ChainOfThought` module
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how the “Reasoning: Let’s think step by step…” phrase is added to our
    prompt, and the quality of our prediction is even better now.'
  prefs: []
  type: TYPE_NORMAL
- en: According to DSPy’s [documentation](https://dspy-docs.vercel.app/docs/building-blocks/modules),
    as of time of writing DSPy provides the following prompting techniques in form
    of Modules. Notice the `dspy.Predict` we used in the initial example is also a
    Module, representing no prompting technique!
  prefs: []
  type: TYPE_NORMAL
- en: '`dspy.Predict`: Basic predictor. Does not modify the signature. Handles the
    key forms of learning (i.e., storing the instructions and demonstrations and updates
    to the LM).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`dspy.ChainOfThought`: Teaches the LM to think step-by-step before committing
    to the signature’s response.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`dspy.ProgramOfThought`: Teaches the LM to output code, whose execution results
    will dictate the response.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`dspy.ReAct`: An agent that can use tools to implement the given signature.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`dspy.MultiChainComparison`: Can compare multiple outputs from ChainOfThought
    to produce a final prediction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It also have some function-style modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '6\. `dspy.majority`: Can do basic voting to return the most popular response
    from a set of predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: You can check out further examples in [each module’s respective guide](https://dspy-docs.vercel.app/api/category/modules).
  prefs: []
  type: TYPE_NORMAL
- en: Chaining the modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the other hand, what about RAG? We can chain the modules together to deal
    with bigger problems!
  prefs: []
  type: TYPE_NORMAL
- en: First we define a retriever, for our example we use a ColBERT retriever getting
    information from Wikipedia Abstracts 2017
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we define the `RAG` class inherited from `dspy.Module`. It needs two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `__init__` method will simply declare the sub-modules it needs: `dspy.Retrieve`
    and `dspy.ChainOfThought`. The latter is defined to implement our `context, question
    -> answer` signature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `forward` method will describe the control flow of answering the question
    using the modules we have.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Note: Code in this section is borrowed from* [*DSPy’s introduction notebook*](https://github.com/stanfordnlp/dspy/blob/main/intro.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Then we make use of the class to perform a RAG
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Inspecting the prompt, we see that 3 passages retrieved from Wikipedia Abstracts
    2017 is interpersed as context for Chain of Thought generation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The above examples might not seem much. At its most basic application the DSPy
    seemed only doing nothing that can’t be done with f-string, but it actually present
    a paradigm shift for prompt writing, as this brings **modularity** to prompt composition!
  prefs: []
  type: TYPE_NORMAL
- en: First we describe our objective with `Signature`, then we apply different prompting
    techniques with `Modules`. To test different prompt techniques for a given problem,
    we can simply switch the modules used and compare their results, rather than hardcoding
    the “let’s think step by step…” (for Chain of Thought) or “you will interleave
    Thought, Action, and Observation steps” (for ReAct) phrases. The benefit of modularity
    will be demonstrated later in this article with a full-fledged example.
  prefs: []
  type: TYPE_NORMAL
- en: The power of DSPy is not only limited to modularity, it can also optimize our
    prompt based on training samples, and test it systematically. We will be exploring
    this in the next section!
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizer: Train our prompt as with machine learning**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we try to optimize our prompt for a RAG application with DSPy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking Chain of Thought as an example, beyond just adding the “let’s think
    step by step” phrase, we can boost its performance with a few tweaks:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding suitable examples (aka **few-shot learning**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Furthermore, we can **bootstrap demonstrations of reasoning** to teach the LMs
    to apply proper reasoning to deal with the task at hand.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Doing this manually would be highly time-consuming and can’t generalize to different
    problems, but with DSPy this can be done automatically. Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**#1: Loading test data**: Like machine learning, to train our prompt we need
    to prepare our training and test datasets. Initially this cell will take around
    20 minutes to run.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Inspecting our dataset, which is basically a set of question-and-answer pairs
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**#2 Set up Phoenix for observability**: To facilitate understanding of the
    optimization process, we launch **Phoenix** to observe our DSPy application, which
    is a great tool for LLM observability in general! I will skip pasting the code
    here, but you can execute it in the notebook.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: If you are on Windows, please also install Windows C++ Build Tools [here](https://visualstudio.microsoft.com/visual-cpp-build-tools/),
    which is necessary for Phoenix'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Then we are ready to see what this opimitzation is about! To “train” our prompt,
    we need 3 things:'
  prefs: []
  type: TYPE_NORMAL
- en: A training set. We’ll just use our 20 question–answer examples from `trainset`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A metric for validation. Here we use the native `dspy.evaluate.answer_exact_match`
    which checks if the predicted answer exactly matches the right answer (questionable
    but suffice for demonstration). For real-life applications you can define your
    own evaluation criteria
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A specific **Optimizer** (formerly teleprompter). The DSPy library includes
    a number of optimization strategies and you can check them out [here](https://dspy-docs.vercel.app/docs/building-blocks/optimizers).
    For our example we use `BootstrapFewShot`. Instead of describing it here with
    lengthy description, I will demonstrate it with code subsequently
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we train our prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Before using the `compiled_rag` to answer a question, let’s see what went behind
    the scene during the training process (aka compile). We launch the Phoenix console
    by visiting `http://localhost:6006/` in browser
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d94f1dbe53ce5b4aef6a0ea2796a406a.png)'
  prefs: []
  type: TYPE_IMG
- en: 14 calls during “compile”
  prefs: []
  type: TYPE_NORMAL
- en: In my run I have made 14 calls using the `RAG` class, in each of those calls
    we post a question to LM to obtain a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the result summary table in my notebook, 4 correct answers are made
    from these 14 samples, thus reaching our `max_bootstrapped_demos` parameter and
    stopping the calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what are the prompts DSPy issued to obtain the bootstrapped demos? Here’s
    the prompt for question #14\. We can see as DSPy tries to generate one bootstrapped
    demo, it would randomly add samples from our `trainset` for few-short learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Time to put the `compiled_rag` to test! Here we raise a question which was answered
    wrongly in our summary table, and see if we can get the right answer this time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We now get the right answer!
  prefs: []
  type: TYPE_NORMAL
- en: Again let’s inspect the prompt issued. Notice how the compiled prompt is different
    from the ones that were used during bootstrapping. Apart from the few-shot examples,
    **bootstrapped Context-Question-Reasoning-Answer demonstrations from correct predictions
    are added to the prompt**, improving the LM’s capability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'So the below is basically went behind the scene with BootstrapFewShot during
    compilation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/843ab35d315078ee9a3d523c13b6f5a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Bootstrapping demonstrations to enhance the prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'The above example still falls short of what we typically do with machine learning:
    Even boostrapping maybe useful, we are not yet proving it to improve the quality
    of the responses.'
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, like in traditional machine learning we should define a couple of candidate
    models, see how they perform against the test set, and select the one achieving
    the highest performance score. This is what we will do next!
  prefs: []
  type: TYPE_NORMAL
- en: '**Full fledged example: Prompt comparison with LLM**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The aim of this example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we want to evaluate what is the “best prompt” **(expressed
    in terms of module and optimizer combination)** to perform a RAG against the [HotpotQA
    dataset](https://hotpotqa.github.io/) (distributed under a [CC BY-SA 4.0 License](http://creativecommons.org/licenses/by-sa/4.0/)),
    given the LM we use (GPT 3.5 Turbo).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Modules under evaluation are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vanilla**: Single-hop RAG to answer a question based on the retrieved context,
    without key phrases like “let’s think step by step”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**COT**: Single-hop RAG with Chain of Thought'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ReAct**: Single-hop RAG with ReAct prompting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BasicMultiHop**: 2-hop RAG with Chain of Thought'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And the Optimizer candidates are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**None**: No additional instructions apart from the signature'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Labeled few-shot**: Simply constructs few-shot examples from provided labeled
    Q/A pairs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bootstrap few-shot**: As we demonstrated, self-generate complete demonstrations
    for every stage of our module. Will simply use the generated demonstrations (if
    they pass the metric) without any further optimization. For `Vanilla` it is just
    equal to “Labeled few-shot”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As for evaluation metric, we again use exact match as criteria (`dspy.evaluate.metrics.answer_exact_match`)
    against the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s begin! First, we define our modules
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Then define permutations for our model candidates
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Then I defined a helper class to facilitate the evaluation. The code is a tad
    bit long so I am not pasting it here, but it could be found in my notebook. What
    it does is to apply each the optimizers against the modules, compile the prompt,
    then perform evaluation against the test set.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to start the evaluation, it would take around 20 minutes to
    complete
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Here’s the evaluation result. We can see the `COT` module with `BootstrapFewShot`
    optimizer has the best performance. The scores represent the percentage of correct
    answers (judged by exact match) made for the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d270eb34c7f0f594923555c9e6499f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But before we conclude the exercise, it might be useful to inspect the result
    more deeply: **Multihop with BootstrapFewShot**, which supposedly equips with
    more relevant context than **COT with BootstrapFewShot**, has a worse performance.
    It is strange!'
  prefs: []
  type: TYPE_NORMAL
- en: Debug and fine-tune our prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now head to the Phoenix Console to see what’s going on. We pick a random question
    `William Hughes Miller was born in a city with how many inhabitants ?`, and inspect
    how did COT, ReAct, BasicMultiHop with BoostrapFewShot optimizer came up with
    their answer. You can type this in the search bar for filter: `"""William Hughes
    Miller was born in a city with how many inhabitants ?""" in input.value`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/588fb6cbfd28970cd79b6918e2a6f614.png)'
  prefs: []
  type: TYPE_IMG
- en: The calls follow sequential order, so for each of the module we can pick the
    BootstrapFewShot variant by picking the 3rd call
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the answers provided by the 3 models during my run:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multihop with BootstrapFewShot**: `The answer will vary based on the specific
    city of William Hughes Miller’s birthplace.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ReAct with BootstrapFewShot**: `Kosciusko, Mississippi`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**COT with BootstrapFewShot**: `The city of Kosciusko, Mississippi, has a population
    of approximately 7,402 inhabitants.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The correct answer is `7,402 at the 2010 census`. Both **ReAct with BootstrapFewShot**
    and **COT with BootstrapFewShot** provided relevant answers, but **Multihop with
    BootstrapFewShot** simply failed to provide one.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the execution trace in Phoenix for Multihop with BootstrapFewShot,
    looks like the LM fails to understand what is expected for the `search_query`
    specified in the **signature**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e1e7217ff54735470d988cac17fea8f.png)'
  prefs: []
  type: TYPE_IMG
- en: The LM can’t come up with the search_query during the 1st hop
  prefs: []
  type: TYPE_NORMAL
- en: So we revise the signature, and re-run the evaluation with the code below
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/09f52de40780a66ef1cad4de15b306b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance improved after updating the signatures
  prefs: []
  type: TYPE_NORMAL
- en: We now see the score improved across all models, and **Multihop with LabeledFewShot**
    and **Multihop with no examples** now have the best performance! This indicates
    despite DSPy tries to optimize the prompt, **there is still some prompt engineering
    involved by articulating your objective in signature**.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: Even the signature itself can be optimized with DSPy’s* [*COPRO*](https://dspy-docs.vercel.app/docs/deep-dive/teleprompter/signature-optimizer)*!
    But this article will not deep dive into COPRO, as all could be too much to digest.*'
  prefs: []
  type: TYPE_NORMAL
- en: The best model now produce an exact match for our question!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Since the best prompt is Multihop with LabeledFewShot, the prompt does not contain
    bootstrapped Context-Question-Reasoning-Answer demonstrations. So bootstrapping
    may not surely lead to better performance, **we need to prove which one is the
    best prompt scientifically.**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: It does not mean Multihop with BootstrapFewShot has a worse performance **in
    general** however. Only that for our task, if we use GPT 3.5 Turbo to bootstrap
    demonstration (which might be of questionable quality) and output prediction,
    then we might better do without the bootstrapping, and keep only the few-shot
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'This lead to the question: Is it possible to use a more powerful LM, say GPT
    4 Turbo (aka `teacher`) to generate demonstrations, while keeping cheaper models
    like GPT 3.5 Turbo (aka `student`) for prediction?'
  prefs: []
  type: TYPE_NORMAL
- en: '**“Teacher” to power-up bootstrapping capability**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The answer is **YES** as the following cell demonstrates, we will use GPT 4
    Turbo as teacher.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4d747518e8a217acfc20102d3ffef23c.png)'
  prefs: []
  type: TYPE_IMG
- en: Result using GPT-4 as teacher
  prefs: []
  type: TYPE_NORMAL
- en: Using GPT-4 Turbo as `teacher` does not significantly boost our models’ performance
    however. Still it is worthwhile to see its effect to our prompt. Below is the
    prompt generated just using GPT 3.5
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: And here’s the prompt generated using GPT-4 Turbo as `teacher`. Notice how the
    “Reasoning” is much better articulated here!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Currently we often rely on manual prompt engineering at best abstracted as f-string.
    Also, for LM comparison we often raise underspecified questions like “how do different
    LMs compare on a certain problem”, borrowed from the [Stanford NLP paper](https://arxiv.org/abs/2310.03714)’s
    saying.
  prefs: []
  type: TYPE_NORMAL
- en: But as the above examples demonstrate, with DSPy’s modular, composable programs
    and optimizers, we are now equipped to answer toward **“how they compare on a
    certain problem with Module X when compiled with Optimizer Y”**, which is a well-defined
    and reproducible run, thus reducing the role of artful prompt construction in
    modern AI.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it! Hope you enjoy this article.
  prefs: []
  type: TYPE_NORMAL
- en: '**Unless otherwise noted, all images are by the author*'
  prefs: []
  type: TYPE_NORMAL
