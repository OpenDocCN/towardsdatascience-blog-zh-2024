<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Audio Diffusion: Generative Music’s Secret Sauce</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Audio Diffusion: Generative Music’s Secret Sauce</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/audio-diffusion-generative-musics-secret-sauce-f625d0aca800?source=collection_archive---------0-----------------------#2024-01-22">https://towardsdatascience.com/audio-diffusion-generative-musics-secret-sauce-f625d0aca800?source=collection_archive---------0-----------------------#2024-01-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="fo"><div class="ab cb"><div class="fp fq fr fs ft fu cf fv cg fw ci bh"><figure class="ga gb gc gd ge fo gf gg paragraph-image"><div role="button" tabindex="0" class="gh gi ed gj bh gk"><div class="fx fy fz"><img src="../Images/1bd508ba6fc0a161a09a10ef07843c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*N2njSffV9qhHwnQqx_xurA.png"/></div></div><figcaption class="gn go gp fx fy gq gr bf b bg z dx">Image generated with DALL·E</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8bea" class="pw-subtitle-paragraph hr gt gu bf b hs ht hu hv hw hx hy hz ia ib ic id ie if ig cq dx">Exploring the principles behind diffusion technology and how it is being used to create groundbreaking AI tools for artists and producers.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="ih ii ij ik il ab"><div><div class="ab im"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@crlandschoot?source=post_page---byline--f625d0aca800--------------------------------" rel="noopener follow"><div class="l in io by ip iq"><div class="l ed"><img alt="Christopher Landschoot" class="l ep by dd de cx" src="../Images/99a2569f5a6a3a99fd1f72553aa3d634.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*q0SJqZRuuuY4mliJwhiHKg@2x.jpeg"/><div class="ir by l dd de em n is eo"/></div></div></a></div></div><div class="it ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--f625d0aca800--------------------------------" rel="noopener follow"><div class="l iu iv by ip iw"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ix cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ir by l br ix em n is eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="iy ab q"><div class="ab q iz"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ja jb bk"><a class="af ag ah ai aj ak al am an ao ap aq ar jc" data-testid="authorName" href="https://medium.com/@crlandschoot?source=post_page---byline--f625d0aca800--------------------------------" rel="noopener follow">Christopher Landschoot</a></p></div></div></div><span class="jd je" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ja jb dx"><button class="jf jg ah ai aj ak al am an ao ap aq ar jh ji jj" disabled="">Follow</button></p></div></div></span></div></div><div class="l jk"><span class="bf b bg z dx"><div class="ab cn jl jm jn"><div class="jo jp ab"><div class="bf b bg z dx ab jq"><span class="jr l jk">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar jc ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--f625d0aca800--------------------------------" rel="noopener follow"><p class="bf b bg z js jt ju jv jw jx jy jz bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="jd je" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="ka kb l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr"><div class="h k w ea eb q"><div class="lh l"><div class="ab q li lj"><div class="pw-multi-vote-icon ed jr lk ll lm"><div class=""><div class="ln lo lp lq lr ls lt am lu lv lw lm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l lx ly lz ma mb mc md"><p class="bf b dy z dx"><span class="lo">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ln mg mh ab q ee mi mj" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="mf"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count me mf">2</span></p></button></div></div></div><div class="ab q ks kt ku kv kw kx ky kz la lb lc ld le lf lg"><div class="mk k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ml an ao ap jh mm mn mo" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mp cn"><div class="l ae"><div class="ab cb"><div class="fp fr ft mq mr gl ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ml an ao ap jh ms mt mj mu mv mw mx my s mz na nb nc nd ne nf u ng nh ni"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ml an ao ap jh ms mt mj mu mv mw mx my s mz na nb nc nd ne nf u ng nh ni"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ml an ao ap jh ms mt mj mu mv mw mx my s mz na nb nc nd ne nf u ng nh ni"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="da32" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Much has been made of the hype of recent generative music AI algorithms. Some see it as the future of creativity and others see it as the death of music. While I tend to lean towards the former camp, as an engineer and researcher, I generally attempt to view these advancements through a more objective lens. With this in mind, I wanted to offer an introduction to one of the core technologies fueling the world of generative audio and music: <a class="af of" href="https://en.wikipedia.org/wiki/Diffusion_model" rel="noopener ugc nofollow" target="_blank"><em class="og">Diffusion</em></a>.</p><p id="4c19" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">My goal is not to sell or diminish the hype, but rather shed some light on what is happening under the hood so that musicians, producers, hobbyists, and creators, can better understand these new seemingly magic music-making black boxes. I will answer what it means by the claim that these AI algorithms are “creating something completely new” and how that differs from human originality. I hope that a clearer picture will lower the collective temperature and provide insight into how these powerful technologies can be leveraged to the benefit of the creator.</p><p id="3f7e" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This piece will touch on technical topics, but you do not need an engineering background to follow along. Let’s begin with some context and definitions.</p><h2 id="d1a3" class="oh oi gu bf oj ok ol om on oo op oq or ns os ot ou nw ov ow ox oa oy oz pa pb bk">Background</h2><p id="25b9" class="pw-post-body-paragraph nj nk gu nl b hs pc nn no hv pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">The term “AI-generated” has become pervasive across the music industry, but what qualifies as “AI-generated” is actually quite clouded. Eager to jump on the hype of the buzzword, this claim is casually tossed around whether AI is used to emulate an effect, automatically mix or master, separate <a class="af of" href="https://en.wikipedia.org/wiki/Stem_(audio)" rel="noopener ugc nofollow" target="_blank">stems</a>, or augment timbre. As long as the final audio has been touched in some way by AI, the term gets slapped on the entire piece. However, the vast majority of music presently released continues to be primarily generated through human production (yes, even ghostwriter’s “<a class="af of" href="https://youtu.be/7HZ2ie2ErFI?si=Ie3954lyyj7l-tEG" rel="noopener ugc nofollow" target="_blank">Heart On My Sleeve</a>” 👻).</p><p id="089b" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Even though this “AI-generated” term is becoming hackneyed for clicks, an appropriate use is when new sounds truly are being created by a computer, i.e. <em class="og">Generative Audio</em>.</p><p id="4554" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Audio generation can encompass the creation of sound effects samples, melodies, vocals, and even full songs. The two main ways that this is achieved are via <em class="og">MIDI Generation</em> and A<em class="og">udio Waveform Generation</em>. <a class="af of" href="https://blog.landr.com/what-is-midi/" rel="noopener ugc nofollow" target="_blank">MIDI (Musical Instrument Digital Interface)</a> generation has a much lower computational cost and can provide high-quality outputs, as the generated MIDI data is then run through an existing virtual instrument to produce sounds. This is the same concept as a producer programming MIDI on a <a class="af of" href="https://blog.landr.com/piano-roll/" rel="noopener ugc nofollow" target="_blank">piano roll</a> and playing it through a <a class="af of" href="https://en.wikipedia.org/wiki/Virtual_Studio_Technology" rel="noopener ugc nofollow" target="_blank">VST</a> plugin such as <a class="af of" href="https://xferrecords.com/products/serum/" rel="noopener ugc nofollow" target="_blank">Serum</a>.</p><figure class="pi pj pk pl pm fo fx fy paragraph-image"><div role="button" tabindex="0" class="gh gi ed gj bh gk"><div class="fx fy ph"><img src="../Images/2825dd04aa8dd49f0cb8d0cd21b1550c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Knftczs8icqbEMed_Jb1w.png"/></div></div><figcaption class="gn go gp fx fy gq gr bf b bg z dx">MIDI Piano Roll in Pro Tools</figcaption></figure><p id="5e20" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">While this is compelling, it is only partially generative, as no audio is actually produced by AI, just as humans cannot synthesize instrument sounds from thin air. The creative capabilities are further limited by whatever virtual instruments the algorithm has access to. Even with these limitations, products implementing this technique, such as <a class="af of" href="https://www.aiva.ai/" rel="noopener ugc nofollow" target="_blank">AIVA</a> and <a class="af of" href="https://www.lemonaide.ai/" rel="noopener ugc nofollow" target="_blank">Seeds by Lemonaide</a>, can generate quite compelling outputs.</p><p id="0c7b" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Audio waveform generation is a much more complicated task, as it is an end-to-end system that does not rely on any external technology. In other words, it produces sounds from scratch. This process most precisely aligns with the true definition of “AI-generated” audio.</p><p id="f193" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Audio waveform generation can be accomplished using various approaches and yield different outcomes. It can produce single samples, such as <a class="af of" href="https://audialab.com/features/" rel="noopener ugc nofollow" target="_blank">Audialab’s ED 2</a> and <a class="af of" href="https://audialab.com/humanize/" rel="noopener ugc nofollow" target="_blank">Humanize</a> or my previous work with <a class="af of" rel="noopener" target="_blank" href="/tiny-audio-diffusion-ddc19e90af9b">Tiny Audio Diffusion</a>, all the way up to full songs, with models such as <a class="af of" href="https://google-research.github.io/seanet/audiolm/examples/" rel="noopener ugc nofollow" target="_blank">AudioLM</a>, <a class="af of" href="https://flavioschneider.notion.site/flavioschneider/Audio-Generation-with-Diffusion-c4f29f39048d4f03a23da13078a44cdb" rel="noopener ugc nofollow" target="_blank">Moûsai</a>, <a class="af of" href="https://www.riffusion.com/" rel="noopener ugc nofollow" target="_blank">Riffusion</a>, <a class="af of" href="https://ai.honu.io/papers/musicgen/" rel="noopener ugc nofollow" target="_blank">MusicGen</a>, and <a class="af of" href="https://www.stableaudio.com/" rel="noopener ugc nofollow" target="_blank">Stable Audio</a>. Among these state-of-the-art models, many leverage some form of <em class="og">Diffusion</em> to generate sounds. You have likely at least tangentially heard of diffusion from <a class="af of" href="https://stability.ai/stable-diffusion" rel="noopener ugc nofollow" target="_blank">Stable Diffusion</a> or any of the other top-performing image generation models that took the world by storm. This generative method can be applied to audio as well. But what does any of it actually mean?</p><h1 id="b4e5" class="pn oi gu bf oj po pp hu on pq pr hx or ps pt pu pv pw px py pz qa qb qc qd qe bk">What is Diffusion?</h1><h2 id="e03a" class="oh oi gu bf oj ok ol om on oo op oq or ns os ot ou nw ov ow ox oa oy oz pa pb bk">The Basics</h2><p id="b493" class="pw-post-body-paragraph nj nk gu nl b hs pc nn no hv pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">In the context of AI, diffusion simply refers to the process of adding or removing noise from a signal (like static from an old TV). <em class="og">Forward Diffusion</em> adds noise to a signal (<em class="og">Noising</em>) and <em class="og">Reverse Diffusion</em> removes noise (<em class="og">Denoising</em>). At a conceptual level, diffusion models take white noise and step through the denoising processes until the audio resembles something recognizable, such as a sample or a song. This process of denoising a signal is the secret sauce in the creativity of many generative audio models.</p><figure class="pi pj pk pl pm fo fx fy paragraph-image"><div class="fx fy qf"><img src="../Images/a50d1ee92ac55dd644ea459cf0b23f44.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/0*7OtpaNR2hvOicvAL"/></div><figcaption class="gn go gp fx fy gq gr bf b bg z dx">Audio Waveform Diffusion (Source: <a class="af of" href="https://github.com/simonrouard/CRASH" rel="noopener ugc nofollow" target="_blank">CRASH: Raw Audio Score-based Generative Modeling for Controllable High-resolution Drum Sound Synthesis (Rouard, Hadjeres)</a>)</figcaption></figure><p id="1ced" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This process was originally developed for images. Looking at how the noise resolves into an image (for instance, a puppy sitting next to a tennis ball) provides an even clearer example of how these models work.</p></div></div><div class="fo"><div class="ab cb"><div class="fp fq fr fs ft fu cf fv cg fw ci bh"><figure class="pi pj pk pl pm fo gf gg paragraph-image"><div role="button" tabindex="0" class="gh gi ed gj bh gk"><div class="fx fy qg"><img src="../Images/35afb6da06976e35eca7951128b52ec0.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/0*ekKyxqQ68uTRH_Vp"/></div></div><figcaption class="gn go gp fx fy gq gr bf b bg z dx">Image Diffusion (Image generated with Stable Diffusion)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="025b" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With a conceptual understanding, let’s take a look under the hood at the key components of the architecture of an audio diffusion model. While this will veer into the technical side of things, stick with me, as a deeper understanding of how these algorithms work will better illustrate why and how they produce the results that they do (if not, you can always just ask <a class="af of" href="https://chat.openai.com/" rel="noopener ugc nofollow" target="_blank">ChatGPT</a> for the Cliff Notes).</p><h2 id="338f" class="oh oi gu bf oj ok ol om on oo op oq or ns os ot ou nw ov ow ox oa oy oz pa pb bk">The U-Net Model Architecture, Compression, and Reconstruction</h2><p id="3da8" class="pw-post-body-paragraph nj nk gu nl b hs pc nn no hv pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">At the core of an audio diffusion model is the <a class="af of" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"><em class="og">U-Net</em></a>. Originally developed for medical image segmentation and aptly named for its resemblance to a U, the U-Net has been adapted to generative audio due to its powerful ability to capture both local and global features in data. The original U-Net was a 2-dimensional <a class="af of" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank"><em class="og">convolutional neural network</em></a> (CNN) used for images, but can be adapted to 1-dimensional convolution to work with audio waveform data. See a visual representation of the original U-Net architecture (for images) below.</p><figure class="pi pj pk pl pm fo fx fy paragraph-image"><div role="button" tabindex="0" class="gh gi ed gj bh gk"><div class="fx fy qh"><img src="../Images/7bbc0b12b588c6e781f128256fdcbb55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OAHceZXifK2Yi3W-px5NmQ.png"/></div></div><figcaption class="gn go gp fx fy gq gr bf b bg z dx">U-Net (Source: <a class="af of" href="https://arxiv.org/abs/1505.04597v1" rel="noopener ugc nofollow" target="_blank">U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger, et. al)</a>)</figcaption></figure><p id="3c4b" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Similar to a <a class="af of" rel="noopener" target="_blank" href="/understanding-variational-autoencoders-vaes-f70510919f73">Variational Autoencoder (VAE)</a>, A U-Net consists of an <em class="og">encoder</em> (the left side of the U) and a <em class="og">decoder</em> (the right side of the U), connected by a <em class="og">bottleneck</em> (the bottom layer of the U). Unlike a VAE, however, a U-Net hosts <em class="og">skip connections</em> (shown by the horizontal gray arrows) that link the encoder to the decoder which is the key piece to producing high-resolution outputs. The encoder is responsible for capturing the <a class="af of" href="https://en.wikipedia.org/wiki/Feature_(machine_learning)" rel="noopener ugc nofollow" target="_blank"><em class="og">features</em></a>, or characteristics, of the input audio signal while the decoder is responsible for the reconstruction of the signal.</p><p id="c70c" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">To aid the visualization, picture the audio data entering the top left side of the U, following the red and blue arrows down the encoder to the bottleneck at the bottom, and then back up the decoder following the blue and green arrows to the top right of the U. Each blue rectangle represents a model <em class="og">layer</em>. At each level of the encoder, the input audio signal is compressed further and further until it reaches a highly condensed representation of the sound at the base of the U (the bottleneck). The decoder then takes this compressed signal and effectively reverses the process to reconstruct the signal. Each layer (blue rectangle) the data passes through has a series of adjustable weights associated with it that can be thought of as millions of tiny knobs that can be turned to tweak this compression/reconstruction process. Having layers at different levels of compression allows the model to learn a range of features from the data, from large-scale features (e.g. melody and rhythm) to fine-grained details (e.g. high-frequency timbral characteristics).</p><p id="713d" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Using an analogy, you can think of this full system as the process required to create an <a class="af of" href="https://en.wikipedia.org/wiki/MP3" rel="noopener ugc nofollow" target="_blank">MP3</a> audio file and then listen to that MP3 on a playback device. At its core, an MP3 is a compressed version of an audio signal. Imagine that the encoder’s job is to create a new type of compressed audio format, just like an MP3, in order to consistently condense an audio signal as much as possible without losing fidelity. Then the decoder’s job is to act like your iPhone (or any playback device) and reconstruct the MP3 into a high-fidelity audio representation that can be played through your headphones. The bottleneck can be thought of as this newly created MP3-type format, itself. <strong class="nl gv">The U-Net represents the process of compression and reconstruction, not the audio data</strong>. The model can then be trained with the goal of being able to accurately compress and reconstruct a wide range of audio signals.</p><p id="a22b" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This is all well and good, but we haven’t generated anything yet. We’ve only constructed a way to compress and reconstruct an audio signal. However, this is the base process required to begin generating new audio, and it only requires a slight adjustment to do so.</p><h2 id="509d" class="oh oi gu bf oj ok ol om on oo op oq or ns os ot ou nw ov ow ox oa oy oz pa pb bk">Noising and Denoising</h2><p id="bfdd" class="pw-post-body-paragraph nj nk gu nl b hs pc nn no hv pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Let's revisit the idea of <em class="og">noising</em> and <em class="og">denoising</em> that we touched on earlier. Theoretically, we had some magic model that could be taught to take some white noise and “denoise” it into recognizable audio, perhaps a beautiful concerto. A critical requirement of this magic model is that it must be able to reconstruct the input audio signal at high fidelity. Luckily, the U-Net architecture is designed to do exactly that. So the next piece of the puzzle is to modify the U-Net to perform this denoising process.</p><p id="a9b1" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Counter-intuitively, to teach a model to denoise an audio signal, it is first taught how to add noise to a signal. Once it has learned this process, it inherently knows how to perform the inverse in order to denoise a signal.</p><p id="a25c" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Recall in the previous section that we detailed how the U-Net can learn to compress and reconstruct an audio signal. The noising process follows nearly the same formula, but instead of reconstructing the exact input audio signal, the U-Net is directed to reconstruct the input audio signal with a small amount of noise added to it. This can be visualized as reversing the steps taken in the earlier series of images of the puppy.</p></div></div><div class="fo"><div class="ab cb"><div class="fp fq fr fs ft fu cf fv cg fw ci bh"><figure class="pi pj pk pl pm fo gf gg paragraph-image"><div role="button" tabindex="0" class="gh gi ed gj bh gk"><div class="fx fy qi"><img src="../Images/9e223a528f9d9406258a26a766f85b89.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*dny2lUi1ACTwrIFTR5oeBg.png"/></div></div><figcaption class="gn go gp fx fy gq gr bf b bg z dx">Diffusion Noising Steps (Image generated with Stable Diffusion)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e3ef" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The process of adding noise to a signal must be probabilistic (i.e. predictable). The model is shown an audio signal and then instructed to predict the same signal with a small amount of <a class="af of" href="https://en.wikipedia.org/wiki/Gaussian_noise#:~:text=In%20signal%20processing%20theory%2C%20Gaussian,can%20take%20are%20Gaussian%2Ddistributed." rel="noopener ugc nofollow" target="_blank">Gaussian noise</a> added to it. Because of its properties, Gaussian noise is most commonly used but it is not required. The noise must be defined by probabilistic distribution, meaning that it follows a specific pattern that is consistently predictable. This process of instructing the model to add small amounts of predictable noise to the audio signal is repeated for a number of <em class="og">steps</em> until the signal has effectively become just noise.</p><figure class="pi pj pk pl pm fo fx fy paragraph-image"><div class="fx fy qf"><img src="../Images/c04fe7144cc50b5a0c8ba8149e95d410.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*ZCdCK_FcI_9ZgicKhBiPbg.gif"/></div><figcaption class="gn go gp fx fy gq gr bf b bg z dx">Noising a Snare Sample (Source: <a class="af of" href="https://github.com/simonrouard/CRASH" rel="noopener ugc nofollow" target="_blank">CRASH: Raw Audio Score-based Generative Modeling for Controllable High-resolution Drum Sound Synthesis (Rouard, Hadjeres)</a>)</figcaption></figure><p id="4a51" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">For example, let’s take a one-shot sample of a snare drum. The U-Net is provided this snare sample and it is asked to reconstruct that snare sound, but with a little noise added making it sound a little less clean. Then this slightly noisy snare sample is provided to the model, and it is again instructed to reconstruct this snare sample with even more noise. This cycle is repeated until it sounds as if the snare sample no longer exists, rather only white noise remains. The model is then taught how to do this for a wide range of sounds. Once it becomes an expert at predicting how to add noise to an input audio signal, because the process is probabilistic, it can simply be reversed so that at each step a little noise is removed. This is how the model can generate a snare sample when provided with white noise.</p><p id="1ee9" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl gv">Because of the probabilistic nature of this process, some incredible capabilities arise, specifically the ability to simulate creativity.</strong></p><p id="ff59" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s continue with our snare example. Imagine the model was trained on thousands of one-shot snare samples. You would imagine that it could take some white noise and then turn it into any one of these snare samples. However, that is not exactly how the model learns. Because it is shown such a wide range of sounds, it instead learns to create sounds that are generally similar to any of the snares that it has been trained on, but not exactly. This is how brand new sounds are created and these models appear to exhibit a spark of creativity.</p><p id="003e" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">To illustrate this, let’s use the following sketch.</p><figure class="pi pj pk pl pm fo fx fy paragraph-image"><div role="button" tabindex="0" class="gh gi ed gj bh gk"><div class="fx fy qj"><img src="../Images/b4ad00a6c897be31a523510cd5e747c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L8naco7ZFOvkdVJ51WyEHg.png"/></div></div></figure><p id="ba36" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Pretend that all possible sounds, from guitar strums to dog barks to white noise, can be plotted on a 2-dimensional plane represented by the black rectangle in the image above. Within this space, there is a region where snare hits exist. They are somewhat grouped together because of their similar timbral and transient characteristics. This is shown by the blue blob and each blue dot is representative of a single snare sample that we trained our model on. The red dots represent the fully noised versions of the snares the model was trained on and correspond to their un-noised blue dot counterparts.</p><p id="0fd9" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In essence, our model learned to take dots from the “not snare” region and bring them into the “snare” region. So if we take a new green dot in the “not snare” region (e.g. random noise) that does not correspond to any blue dot, and ask our model to bring it into the “snare” region, it will bring it to a new location within that “snare” region. This is the model generating a “new” snare sample that contains some similarities to all other snares it was trained on in the snare region, but also some new unknown characteristics.</p><p id="142b" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This concept can be applied to any type of sound, including full songs. This is an incredible innovation that can lead to numerous new ways to create. It is important to understand that these models will not create something outside of the bounds of how they are trained, however. As shown in the previous illustration, while our conceptual model can take in any type of sound, it can only produce snare samples similar to those it was trained on. This holds true for any of these audio diffusion models. Because of this, it is critical to train models on extensive datasets so the known regions (like the snare region) are sufficiently diverse and large enough to not simply copy the training data.</p><p id="96f7" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl gv">All of this means that no model can replicate human creativity, just simulate variations of it.</strong></p><h2 id="b5ad" class="oh oi gu bf oj ok ol om on oo op oq or ns os ot ou nw ov ow ox oa oy oz pa pb bk">Applications of Diffusion Models</h2><p id="8ae1" class="pw-post-body-paragraph nj nk gu nl b hs pc nn no hv pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">These models will not magically generate new genres or explore unknown sonic landscapes as humans do. With this understanding, these generative models should not be viewed as a replacement for human creativity, but rather as tools that can enhance creativity. Below are just a few ways that this technology can be leveraged for creative means:</p><ul class=""><li id="f91d" class="nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe qk ql qm bk"><strong class="nl gv">Creativity Through Curation</strong>: Searching through sample packs to find a desired sound is a common practice in production. These models can effectively be used as a version of an “unlimited sample pack”, enhancing an artist’s creativity through the curation of sounds.</li><li id="1834" class="nj nk gu nl b hs qn nn no hv qo nq nr ns qp nu nv nw qq ny nz oa qr oc od oe qk ql qm bk"><strong class="nl gv">Voice Transfer</strong>: Just like how diffusion models can take random noise and change it into recognizable audio, they can also be fed other sounds and “transfer” them to another type of sound. If we take our previous snare model, for example, and feed it a kick drum sample instead of white noise, it will take the kick sample and begin to morph it into a snare sound. This allows for very unique creations, being able to combine the characteristics of multiple different sounds.</li><li id="a386" class="nj nk gu nl b hs qn nn no hv qo nq nr ns qp nu nv nw qq ny nz oa qr oc od oe qk ql qm bk"><strong class="nl gv">Sound Variability (Humanization)</strong>: When humans play a live instrument, such as a hi-hat on a drum set, there is always inherent variability in each hit. Various virtual instruments have attempted to simulate this via a number of different methods, but can still sound artificial and lack character. Audio diffusion allows for the unlimited variation of a single sound, which can add a human element to an audio sample. For example, if you program a drum kit, audio diffusion can be leveraged so that each hit is slightly different in timbre, velocity, attack, etc. to humanize what might sound like a stale performance.</li><li id="d77f" class="nj nk gu nl b hs qn nn no hv qo nq nr ns qp nu nv nw qq ny nz oa qr oc od oe qk ql qm bk"><strong class="nl gv">Sound Design Adjustments</strong>: Similar to the human variability potential, this concept can also be applied to sound design to create slight changes to a sound. Perhaps you mostly like the sound of a door slam sample, but you wish that it had more body or crunch. A diffusion model can take this sample and slightly change it to maintain most of its characteristics while taking on a few new ones. This can add, remove, or change the spectral content of a sound at a more fundamental level than applying an EQ or filter.</li><li id="c945" class="nj nk gu nl b hs qn nn no hv qo nq nr ns qp nu nv nw qq ny nz oa qr oc od oe qk ql qm bk"><strong class="nl gv">Melody Generation:</strong> Similar to surfing through sample packs, audio diffusion models can generate melodies that can spark ideas to build on.</li><li id="f28e" class="nj nk gu nl b hs qn nn no hv qo nq nr ns qp nu nv nw qq ny nz oa qr oc od oe qk ql qm bk"><strong class="nl gv">Stereo Effect</strong>: There are several different mixing tricks to add stereo width to a single-channel (mono) sound. However, they can often add undesired coloration, delay, or phase shifts. Audio diffusion can be leveraged to generate a sound nearly identical to the mono sound, but different enough in its content to expand the stereo width while avoiding many of the unwanted phenomena.</li><li id="00c6" class="nj nk gu nl b hs qn nn no hv qo nq nr ns qp nu nv nw qq ny nz oa qr oc od oe qk ql qm bk"><strong class="nl gv">Super Resolution:</strong> Audio diffusion models can enhance the resolution and quality of audio recordings, making them clearer and more detailed. This can be particularly useful in audio restoration or when working with low-quality recordings.</li><li id="ce0b" class="nj nk gu nl b hs qn nn no hv qo nq nr ns qp nu nv nw qq ny nz oa qr oc od oe qk ql qm bk"><strong class="nl gv">Inpainting:</strong> Diffusion models can be leveraged to fill in missing or corrupted parts of audio signals, restoring them to their original or improved state. This is valuable for repairing damaged audio recordings, completing sections of audio that may be missing, or adding transitions between audio clips.</li></ul><h1 id="19d8" class="pn oi gu bf oj po pp hu on pq pr hx or ps pt pu pv pw px py pz qa qb qc qd qe bk">Conclusion</h1><p id="5a6c" class="pw-post-body-paragraph nj nk gu nl b hs pc nn no hv pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">There is no doubt that these new generative AI models are incredible technological advancements, independent of whether they are viewed in a positive or negative light. There are many more aspects to diffusion models that can optimize their performance regarding speed, diversity, and quality, but we have discussed the base principles that govern the functionality of these models. This knowledge provides a deeper context into what it really means when these models are generating “new sounds”.</p><p id="a5c8" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">On a broader level, it is not only the music, itself, that people care about — it is the human element in the creation of that music. Ask yourself, if you were to hear a recording of a virtuosic lightning-fast guitar solo, would you be impressed? It all depends. If it was artificially generated by a virtual MIDI instrument programmed by a producer, you will likely be unphased and may not even like how it sounds. However, if you know an actual guitarist played the solo on a real guitar, or even saw him or her do it, you will be completely enamored by their expertise and precision. We are drawn to the deftness in a performance, the thoughts and emotions behind lyrics, and the considerations that go into each decision when crafting a song.</p><p id="febb" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">While these incredible advancements have led to some existential dread for artists and producers, AI can never take that human element away from the sounds and music that we create. So we should approach these new advancements with the intent that they are tools for enhancing artists’ creativity rather than replacing it.</p></div></div></div><div class="ab cb qs qt qu qv" role="separator"><span class="qw by bm qx qy qz"/><span class="qw by bm qx qy qz"/><span class="qw by bm qx qy"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4ac8" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><em class="og">All images, unless otherwise noted, are by the author.</em></p><p id="f0b6" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">I am an audio machine learning engineer and researcher as well as a lifelong musician. If you are interested in more audio AI applications, see my previously published articles on <a class="af of" href="https://medium.com/towards-data-science/tiny-audio-diffusion-ddc19e90af9b" rel="noopener">Tiny Audio Diffusion</a> and <a class="af of" href="https://medium.com/rock-nheavy/the-music-demixing-ai-revolution-9d1528c6ef7c" rel="noopener">Music Demixing</a>.</p><p id="0b7d" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Find me on <a class="af of" href="https://www.linkedin.com/in/christopher-landschoot/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a> &amp; <a class="af of" href="https://github.com/crlandsc" rel="noopener ugc nofollow" target="_blank">GitHub</a> and keep up to date with my current work and research here: <a class="af of" href="https://www.chrislandschoot.com/" rel="noopener ugc nofollow" target="_blank">www.chrislandschoot.com</a></p><p id="c283" class="pw-post-body-paragraph nj nk gu nl b hs nm nn no hv np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Find my music on <a class="af of" href="https://open.spotify.com/artist/2i6noWJnJQPXPsudoiJuMS?si=fvLOxUPqTAWKB894WXMz5Q" rel="noopener ugc nofollow" target="_blank">Spotify</a>, <a class="af of" href="https://music.apple.com/us/artist/after-august/259370281" rel="noopener ugc nofollow" target="_blank">Apple Music</a>, <a class="af of" href="https://www.youtube.com/AfterAugust" rel="noopener ugc nofollow" target="_blank">YouTube</a>, <a class="af of" href="https://soundcloud.com/after-august" rel="noopener ugc nofollow" target="_blank">SoundCloud</a>, and other streaming platforms as <a class="af of" href="https://www.instagram.com/the_after_august/" rel="noopener ugc nofollow" target="_blank">After August</a>.</p></div></div></div></div>    
</body>
</html>