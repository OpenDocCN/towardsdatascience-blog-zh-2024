<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Python Engineer’s Introduction to 3D Gaussian Splatting (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Python Engineer’s Introduction to 3D Gaussian Splatting (Part 1)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-python-engineers-introduction-to-3d-gaussian-splatting-part-1-e133b0449fc6?source=collection_archive---------2-----------------------#2024-06-11">https://towardsdatascience.com/a-python-engineers-introduction-to-3d-gaussian-splatting-part-1-e133b0449fc6?source=collection_archive---------2-----------------------#2024-06-11</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7d9b" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Understanding and coding Gaussian Splatting from a Python Engineer’s perspective</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@dcaustin33?source=post_page---byline--e133b0449fc6--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Derek Austin" class="l ep by dd de cx" src="../Images/1bcc5955f32cb798988af5713baae212.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*jO7ooF0USlA22GWVFwKkEw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e133b0449fc6--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@dcaustin33?source=post_page---byline--e133b0449fc6--------------------------------" rel="noopener follow">Derek Austin</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e133b0449fc6--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 11, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/813c95a2e396af5470c94772b24a2b06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0aNkM8EHNVuinS6rz3XFTg.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@sigmund?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">rivage</a> on <a class="af nc" href="https://unsplash.com/photos/white-and-blue-box-on-white-table-KznImGeQGWE?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="61c4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In early 2023, authors from Université Côte d’Azur and Max-Planck-Institut für Informatik published a paper titled “3D Gaussian Splatting for Real-Time Field Rendering.”¹ The paper presented a significant advancement in real-time neural rendering, surpassing the utility of previous methods like NeRF’s.² Gaussian splatting not only reduced latency but also matched or exceeded the rendering quality of NeRF’s, taking the world of neural rendering by storm.</p><p id="b84e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Gaussian splatting, while effective, can be challenging to understand for those unfamiliar with camera matrices and graphics rendering. Moreover, I found that resources for implementing gaussian splatting in Python are scarce, as even the author’s source code is written in CUDA! This tutorial aims to bridge that gap, providing a Python-based introduction to gaussian splatting for engineers versed in python and machine learning but less experienced with graphics rendering. The accompanying code on <a class="af nc" href="https://github.com/dcaustin33/intro_to_gaussian_splatting" rel="noopener ugc nofollow" target="_blank">GitHub</a> demonstrates how to initialize and render points from a COLMAP scan into a final image that resembles the forward pass in splatting applications(and some bonus CUDA code for those interested). This tutorial also has a companion jupyter notebook (<a class="af nc" href="https://github.com/dcaustin33/intro_to_gaussian_splatting/blob/main/part_1.ipynb" rel="noopener ugc nofollow" target="_blank">part_1.ipynb</a> in the GitHub) that has all the code needed to follow along. While we will not build a full gaussian splat scene, if followed, this tutorial should equip readers with the foundational knowledge to delve deeper into splatting technique.</p><p id="1bcd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To begin, we use COLMAP, a software that extracts points consistently seen across multiple images using Structure from Motion (SfM).³ SfM essentially identifies points (e.g., the top right edge of a doorway) found in more than 1 picture. By matching these points across different images, we can estimate the depth of each point in 3D space. This closely emulates how human stereo vision works, where depth is perceived by comparing slightly different views from each eye. Thus, SfM generates a set of 3D points, each with x, y, and z coordinates, from the common points found in multiple images giving us the “structure” of the scene.</p><p id="2760" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this tutorial we will use a prebuilt COLMAP scan that is available for <a class="af nc" href="https://storage.googleapis.com/gresearch/refraw360/360_extra_scenes.zip" rel="noopener ugc nofollow" target="_blank">download here</a> (Apache 2.0 license). Specifically we will be using the Treehill folder within the downloaded dataset.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk nz"><img src="../Images/b16ab5f5a417adfd630e37b233a4855e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eZtLBj2jHK7A41nwyPv4nA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The image along with all the points extracted from all images fed to COLMAP. See sample code below or in part_1.ipynb to understand the process. Apache 2.0 license.</figcaption></figure><p id="1583" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The folder consists of three files corresponding to the camera parameters, the image parameters and the actual 3D points. We will start with the 3D points.</p><p id="0f94" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The points file consists of thousands of points in 3D along with associated colors. The points are centered around what is called the world origin, essentially their x, y, or z coordinates are based upon where they were observed in reference to this world origin. The exact location of the world origin isn’t crucial for our purposes, so we won’t focus on it as it can be any arbitrary point in space. Instead, its only essential to know where you are in the world in relation to this origin. That is where the image file becomes useful!</p><p id="85ca" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Broadly speaking the image file tells us where the image was taken and the orientation of the camera, both in relation to the world origin. Therefore, the key parameters we care about are the quaternion vector and the translation vector. The quaternion vector describes the rotation of the camera in space using 4 distinct float values that can be used to form a rotation matrix (3Blue1Brown has a great video explaining exactly what quaternions are <a class="af nc" href="https://www.youtube.com/watch?v=d4EgbgTm0Bg" rel="noopener ugc nofollow" target="_blank">here</a>). The translation vector then tells us the camera’s position relative to the origin. Together, these parameters form the extrinsic matrix, with the quaternion values used to compute a 3x3 rotation matrix (<a class="af nc" href="https://automaticaddison.com/how-to-convert-a-quaternion-to-a-rotation-matrix/" rel="noopener ugc nofollow" target="_blank">formula</a>) and the translation vector appended to this matrix.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk oa"><img src="../Images/09cc599d85fe8a84a64ec9deea9f4c11.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*DjLWdw_bBdSGcBmIgC8WKQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A typical “extrinsic” matrix. By combining a 3x3 rotation matrix and a 3x1 translation vector we are able to translate coordinates from the world coordinate system to our camera coordinate system. Image by author.</figcaption></figure><p id="8aef" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The extrinsic matrix translates points from world space (the coordinates in the points file) to camera space, making the camera the new center of the world. For example, if the camera is moved 2 units up in the y direction without any rotation, we would simply subtract 2 units from the y-coordinates of all points in order to obtain the points in the new coordinate system.</p><p id="574a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">When we convert coordinates from world space to camera space, we still have a 3D vector, where the z coordinate represents the depth in the camera’s view. This depth information is crucial for determining the order of splats, which we need for rendering later on.</p><p id="90a8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We end our COLMAP examination by explaining the camera parameters file. The camera file provides parameters such as height, width, focal length (x and y), and offsets (x and y). Using these parameters, we can compose the intrinsic matrix, which represents the focal lengths in the x and y directions and the principal point coordinates.</p><p id="4acd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If you are completely unfamiliar with camera matrices I would point you to the First Principles of <a class="af nc" href="https://fpcv.cs.columbia.edu/" rel="noopener ugc nofollow" target="_blank">Computer Vision lectures</a> given by Shree Nayar. Specially the Pinhole and Prospective Projection <a class="af nc" href="https://youtu.be/_EhY31MSbNM" rel="noopener ugc nofollow" target="_blank">lecture</a> followed by the Intrinsic and Extrinsic matrix <a class="af nc" href="https://www.youtube.com/watch?v=2XM2Rb2pfyQ&amp;t=66s" rel="noopener ugc nofollow" target="_blank">lecture</a>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ob"><img src="../Images/b73bd7d9026d6ff1f86e406c282d2570.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*fHVt2hdbmAQi_F8Nd5f1ng.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The typical intrinsic matrix. Representing the focal length in the x and y direction, along with the principal point coordinates. Image by author.</figcaption></figure><p id="c6ab" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The intrinsic matrix is used to transform points from camera coordinates (obtained using the extrinsic matrix) to a 2D image plane, ie. what you see as your “image.” Points in camera coordinates alone do not indicate their appearance in an image as depth must be reflected in order to assess exactly what a camera will see.</p><p id="c164" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To convert COLMAP points to a 2D image, we first project them to camera coordinates using the extrinsic matrix, and then project them to 2D using the intrinsic matrix. However, an important detail is that we use homogeneous coordinates for this process. The extrinsic matrix is 4x4, while our input points are 3x1, so we stack a 1 to the input points to make them 4x1.</p><p id="deb8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here’s the process step-by-step:</p><ol class=""><li id="275a" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny oc od oe bk">Transform the points to camera coordinates: multiply the 4x4 extrinsic matrix by the 4x1 point vector.</li><li id="4eaa" class="nd ne fq nf b go of nh ni gr og nk nl nm oh no np nq oi ns nt nu oj nw nx ny oc od oe bk">Transform to image coordinates: multiply the 3x4 intrinsic matrix by the resulting 4x1 vector.</li></ol><p id="eb67" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This results in a 3x1 matrix. To get the final 2D coordinates, we divide by the third coordinate of this 3x1 matrix and obtain an x and y coordinate in the image! You can see exactly how this should look about for image number 100 and the code to replicate the results is shown below.</p><pre class="mm mn mo mp mq ok ol om bp on bb bk"><span id="385d" class="oo op fq ol b bg oq or l os ot">def get_intrinsic_matrix(<br/>    f_x: float, f_y: float, c_x: float, c_y: float<br/>) -&gt; torch.Tensor:<br/>    """<br/>    Get the homogenous intrinsic matrix for the camera<br/>    """<br/>    return torch.Tensor(<br/>        [<br/>            [f_x, 0, c_x, 0],<br/>            [0, f_y, c_y, 0],<br/>            [0, 0, 1, 0],<br/>        ]<br/>    )<br/><br/><br/>def get_extrinsic_matrix(R: torch.Tensor, t: torch.Tensor) -&gt; torch.Tensor:<br/>    """<br/>    Get the homogenous extrinsic matrix for the camera<br/>    """<br/>    Rt = torch.zeros((4, 4))<br/>    Rt[:3, :3] = R<br/>    Rt[:3, 3] = t<br/>    Rt[3, 3] = 1.0<br/>    return Rt<br/><br/>def project_points(<br/>    points: torch.Tensor, intrinsic_matrix: torch.Tensor, extrinsic_matrix: torch.Tensor<br/>) -&gt; torch.Tensor:<br/>    """<br/>    Project the points to the image plane<br/><br/>    Args:<br/>        points: Nx3 tensor<br/>        intrinsic_matrix: 3x4 tensor<br/>        extrinsic_matrix: 4x4 tensor<br/>    """<br/>    homogeneous = torch.ones((4, points.shape[0]), device=points.device)<br/>    homogeneous[:3, :] = points.T<br/>    projected_to_camera_perspective = extrinsic_matrix @ homogeneous<br/>    projected_to_image_plane = (intrinsic_matrix @ projected_to_camera_perspective).T # Nx4<br/>    <br/>    x = projected_to_image_plane[:, 0] / projected_to_image_plane[:, 2]<br/>    y = projected_to_image_plane[:, 1] / projected_to_image_plane[:, 2]<br/>    return x, y<br/><br/><br/><br/>colmap_path = "treehill/sparse/0"<br/>reconstruction = pycolmap.Reconstruction(colmap_path)<br/><br/>points3d = reconstruction.points3D<br/>images = read_images_binary(f"{colmap_path}/images.bin")<br/>cameras = reconstruction.cameras<br/><br/>all_points3d = []<br/>all_point_colors = []<br/><br/>for idx, point in enumerate(points3d.values()):<br/>    if point.track.length() &gt;= 2:<br/>        all_points3d.append(point.xyz)<br/>        all_point_colors.append(point.color)<br/><br/>gaussians = Gaussians(<br/>    torch.Tensor(all_points3d), <br/>    torch.Tensor(all_point_colors),<br/>    model_path="point_clouds"<br/>)<br/><br/># we will examine the 100th image<br/>image_num = 100    <br/>image_dict = read_image_file(colmap_path)<br/>camera_dict = read_camera_file(colmap_path)<br/><br/># convert quaternion to rotation matrix<br/>rotation_matrix = build_rotation(torch.Tensor(image_dict[image_num].qvec).unsqueeze(0))<br/>translation = torch.Tensor(image_dict[image_num].tvec).unsqueeze(0)<br/>extrinsic_matrix = get_extrinsic_matrix(<br/>    rotation_matrix, translation<br/>)<br/>focal_x, focal_y = camera_dict[image_dict[image_num].camera_id].params[:2]<br/>c_x, c_y = camera_dict[image_dict[image_num].camera_id].params[2:4]<br/>intrinsic_matrix = get_intrinsic_matrix(focal_x, focal_y, c_x, c_y)<br/><br/>points = project_points(gaussians.points, intrinsic_matrix, extrinsic_matrix)</span></pre><p id="6515" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To review, we can now take any set of 3D points and project where they would appear on a 2D image plane as long as we have the various location and camera parameters we need! With that in hand we can move forward with understanding the “gaussian” part of gaussian splatting in <a class="af nc" href="https://medium.com/towards-data-science/a-python-engineers-introduction-to-3d-gaussian-splatting-part-2-7e45b270c1df" rel="noopener">part 2</a>.</p></div></div></div><div class="ab cb ou ov ow ox" role="separator"><span class="oy by bm oz pa pb"/><span class="oy by bm oz pa pb"/><span class="oy by bm oz pa"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><ol class=""><li id="9024" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny oc od oe bk">Kerbl, Bernhard, et al. “3d gaussian splatting for real-time radiance field rendering.” <em class="pc">ACM Transactions on Graphics</em> 42.4 (2023): 1–14.</li><li id="b2d1" class="nd ne fq nf b go of nh ni gr og nk nl nm oh no np nq oi ns nt nu oj nw nx ny oc od oe bk">Mildenhall, Ben, et al. “Nerf: Representing scenes as neural radiance fields for view synthesis.” <em class="pc">Communications of the ACM</em> 65.1 (2021): 99–106.</li><li id="3191" class="nd ne fq nf b go of nh ni gr og nk nl nm oh no np nq oi ns nt nu oj nw nx ny oc od oe bk">Snavely, Noah, Steven M. Seitz, and Richard Szeliski. “Photo tourism: exploring photo collections in 3D.” <em class="pc">ACM siggraph 2006 papers</em>. 2006. 835–846.</li><li id="2bde" class="nd ne fq nf b go of nh ni gr og nk nl nm oh no np nq oi ns nt nu oj nw nx ny oc od oe bk">Barron, Jonathan T., et al. “Mip-nerf 360: Unbounded anti-aliased neural radiance fields.” <em class="pc">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2022.</li></ol></div></div></div></div>    
</body>
</html>