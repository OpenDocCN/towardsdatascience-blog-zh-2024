<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How Neural Networks Learn: A Probabilistic Viewpoint</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How Neural Networks Learn: A Probabilistic Viewpoint</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-neural-networks-learn-a-probabilistic-viewpoint-0f6a78dc58e2?source=collection_archive---------1-----------------------#2024-12-26">https://towardsdatascience.com/how-neural-networks-learn-a-probabilistic-viewpoint-0f6a78dc58e2?source=collection_archive---------1-----------------------#2024-12-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="9759" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Understanding loss functions for training neural networks</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@bilalhsp?source=post_page---byline--0f6a78dc58e2--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Bilal Ahmed" class="l ep by dd de cx" src="../Images/840f8aedf34c06a410f35290443769e2.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*LHCf2WuU_F7bgsJsZ_honQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--0f6a78dc58e2--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@bilalhsp?source=post_page---byline--0f6a78dc58e2--------------------------------" rel="noopener follow">Bilal Ahmed</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--0f6a78dc58e2--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="2c41" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Machine learning is very hands-on, and everyone charts their own path. There isn’t a standard set of courses to follow, as was traditionally the case. There’s no ‘Machine Learning 101,’ so to speak. However, this sometimes leaves gaps in understanding. If you’re like me, these gaps can feel uncomfortable. For instance, I used to be bothered by things we do casually, like the choice of a loss function. I admit that some practices are learned through heuristics and experience, but most concepts are rooted in solid mathematical foundations. Of course, not everyone has the time or motivation to dive deeply into those foundations — unless you’re a researcher.</p><p id="2402" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I have attempted to present some basic ideas on how to approach a machine learning problem. Understanding this background will help practitioners feel more confident in their design choices. The concepts I covered include:</p><ul class=""><li id="eb07" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk">Quantifying the difference in probability distributions using cross-entropy.</li><li id="c24a" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk">A probabilistic view of neural network models.</li><li id="c9ca" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk">Deriving and understanding the loss functions for different applications.</li></ul><h1 id="9094" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Entropy</h1><p id="3dda" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">In information theory, entropy is a measure of the uncertainty associated with the values of a random variable. In other words, it is used to quantify the spread of distribution. The narrower the distribution the lower the entropy and vice versa. Mathematically, entropy of distribution <strong class="ml fr"><em class="oo">p(x)</em></strong> is defined as;</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq or"><img src="../Images/33dd85ca764eeece3a21440ee7905d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-PFEvxfWbxlxJMTl0q11Q.png"/></div></div></figure><p id="3bb7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It is common to use log with the base 2 and in that case entropy is measured in bits. The figure below compares two distributions: the blue one with high entropy and the orange one with low entropy.</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div class="op oq pd"><img src="../Images/964baafb5e977e35c85107b5f4900b32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*DA0iEWJkLdAgtTGlfPB2aA.png"/></div><figcaption class="pe pf pg op oq ph pi bf b bg z dx">Visualization examples of distributions having high and low entropy — created by the author using Python.</figcaption></figure><p id="3448" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can also measure entropy between two distributions. For example, consider the case where we have observed some data having the distribution <strong class="ml fr"><em class="oo">p(x)</em></strong> and a distribution <strong class="ml fr"><em class="oo">q(x)</em></strong> that could potentially serve as a model for the observed data. In that case we can compute cross-entropy <strong class="ml fr"><em class="oo">Hpq​(X)</em></strong> between data distribution <strong class="ml fr"><em class="oo">p(x)</em></strong> and the model distribution <strong class="ml fr"><em class="oo">q(x)</em></strong>. Mathematically cross-entropy is written as follows:</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pj"><img src="../Images/4fa58dee8edcc1c39b14f48a872a761a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-F7o-cUKOs06JjLdoc6IBA.png"/></div></div></figure><p id="b10c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Using cross entropy we can compare different models and the one with lowest cross entropy is better fit to the data. This is depicted in the contrived example in the following figure. We have two candidate models and we want to decide which one is better model for the observed data. As we can see the model whose distribution exactly matches that of the data has lower cross entropy than the model that is slightly off.</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pk"><img src="../Images/0969895a26721b15b064252d92294c46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VlUMj6R5fvLPGngx19Mrhg.png"/></div></div><figcaption class="pe pf pg op oq ph pi bf b bg z dx">Comparison of cross entropy of data distribution p(x) with two candidate models. (a) candidate model exactly matches data distribution and has low cross entropy. (b) candidate model does not match the data distribution hence it has high cross entropy — created by the author using Python.</figcaption></figure><p id="53e5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There is another way to state the same thing. As the model distribution deviates from the data distribution cross entropy increases. While trying to fit a model to the data i.e. training a machine learning model, we are interested in minimizing this deviation. This increase in cross entropy due to deviation from the data distribution is defined as relative entropy commonly known as <strong class="ml fr">Kullback-Leibler Divergence</strong> of simply <strong class="ml fr">KL-Divergence.</strong></p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pl"><img src="../Images/2b6f13e6fb4bf7a5a1edcc559307df54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tG7x9Hopkn9IJPIcnYlNww.png"/></div></div></figure><p id="3a43" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Hence, we can quantify the divergence between two probability distributions using cross-entropy or KL-Divergence. To train a model we can adjust the parameters of the model such that they minimize the cross-entropy or KL-Divergence. Note that minimizing cross-entropy or KL-Divergence achieves the same solution. KL-Divergence has a better interpretation as its minimum is zero, that will be the case when the model exactly matches the data.</p><p id="ed6d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Another important consideration is how do we pick the model distribution? This is dictated by two things: the problem we are trying to solve and our preferred approach to solving the problem. Let’s take the example of a classification problem where we have <strong class="ml fr"><em class="oo">(X, Y)</em></strong> pairs of data, with <strong class="ml fr"><em class="oo">X</em></strong> representing the input features and <strong class="ml fr"><em class="oo">Y</em></strong> representing the true class labels. We want to train a model to correctly classify the inputs. There are two ways we can approach this problem.</p><h1 id="0f8a" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Discriminative vs Generative</h1><p id="4219" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">The generative approach refers to modeling the joint distribution <strong class="ml fr"><em class="oo">p(X,Y)</em></strong> such that it learns the data-generating process, hence the name ‘generative’. In the example under discussion, the model learns the prior distribution of class labels <strong class="ml fr"><em class="oo">p(Y)</em></strong> and for given class label <strong class="ml fr"><em class="oo">Y</em></strong>, it learns to generate features <strong class="ml fr"><em class="oo">X</em></strong> using <strong class="ml fr"><em class="oo">p(X|Y)</em></strong>.</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pm"><img src="../Images/5eb90240b9184995d34273875592b62f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TdX5wvjj_bbJImN8uu97CA.png"/></div></div></figure><p id="faa5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It should be clear that the learned model is capable of generating new data <strong class="ml fr"><em class="oo">(X,Y)</em></strong>. However, what might be less obvious is that it can also be used to classify the given features <strong class="ml fr"><em class="oo">X</em></strong> using Bayes’ Rule, though this may not always be feasible depending on the model’s complexity. Suffice it to say that using this for a task like classification might not be a good idea, so we should instead take the direct approach.</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pn"><img src="../Images/a8c29ddac205624e27902f56d5dcc526.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4OX-X97JbRoqqCRO30DnTg.png"/></div></div><figcaption class="pe pf pg op oq ph pi bf b bg z dx">Discriminative vs generative approach of modelling — created by the author using Python.</figcaption></figure><p id="e15a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Discriminative approach refers to modelling the relationship between input features <strong class="ml fr"><em class="oo">X</em></strong> and output labels <strong class="ml fr"><em class="oo">Y</em></strong> directly i.e. modelling the conditional distribution <strong class="ml fr"><em class="oo">p(Y|X)</em></strong>. The model thus learnt need not capture the details of features <strong class="ml fr"><em class="oo">X</em></strong> but only the class discriminatory aspects of it. As we saw earlier, it is possible to learn the parameters of the model by minimizing the cross-entropy between observed data and model distribution. The cross-entropy for a discriminative model can be written as:</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq po"><img src="../Images/16acd7e725501a5072a2743d1d0c700d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_uMcEEKL5M8jDtQWQsN8PQ.png"/></div></div></figure><p id="c206" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Where the right most sum is the sample average and it approximates the expectation w.r.t data distribution. Since our learning rule is to minimize the cross-entropy, we can call it our general loss function.</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pp"><img src="../Images/a033e75b7e65062de1656ecb8e087c7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CQLRP6puQTQR9N80VeHHWg.png"/></div></div></figure><p id="0bfe" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Goal of learning (training the model) is to minimize this loss function. Mathematically, we can write the same statement as follows:</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pq"><img src="../Images/e3beee4f01aff22571a56f8d67802dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*liWvXE1Qgy-5YYOXNKb3hw.png"/></div></div></figure><p id="bfd5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s now consider specific examples of discriminative models and apply the general loss function to each example.</p><h1 id="8feb" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Binary Classification</h1><p id="fee5" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">As the name suggests, the class label <strong class="ml fr"><em class="oo">Y</em></strong> for this kind of problem is either <strong class="ml fr"><em class="oo">0</em></strong> or <strong class="ml fr"><em class="oo">1</em></strong>. That could be the case for a face detector, or a cat vs dog classifier or a model that predicts the presence or absence of a disease. How do we model a binary random variable? That’s right — it’s a Bernoulli random variable. The probability distribution for a Bernoulli variable can be written as follows:</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pr"><img src="../Images/ea45a2b2ec777baffd917d5767c3efd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_5sGkYROdxFMQRz3FCvD4Q.png"/></div></div></figure><p id="5ce4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">where <strong class="ml fr"><em class="oo">π</em></strong> is the probability of getting <strong class="ml fr"><em class="oo">1</em></strong> i.e. <strong class="ml fr"><em class="oo">p(Y=1) = π</em></strong>.</p><p id="3b9b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Since we want to model <strong class="ml fr"><em class="oo">p(Y|X)</em></strong>, let’s make <strong class="ml fr"><em class="oo">π</em></strong> a function of <strong class="ml fr"><em class="oo">X</em></strong> i.e. output of our model <strong class="ml fr"><em class="oo">π(X) </em></strong>depends on input features <strong class="ml fr"><em class="oo">X</em></strong>. In other words, our model takes in features <strong class="ml fr"><em class="oo">X</em></strong> and predicts the probability of <strong class="ml fr"><em class="oo">Y</em></strong>=1. Please note that in order to get a valid probability at the output of the model, it has to be constrained to be a number between <strong class="ml fr"><em class="oo">0</em></strong> and <strong class="ml fr"><em class="oo">1</em></strong>. This is achieved by applying a sigmoid non-linearity at the output.</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq ps"><img src="../Images/2f67b498697edf028bc2c24b22c827b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uFY-AXCtCtMWzexW9lYEgg.png"/></div></div></figure><p id="1a1c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To simplify, let’s rewrite this explicitly in terms of true label and predicted label as follows:</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pt"><img src="../Images/fa4841469e25d666a92da564ccebeb56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pECYIPnobLfMrUZ8r0_LNQ.png"/></div></div></figure><p id="ce70" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can write the general loss function for this specific conditional distribution as follows:</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pp"><img src="../Images/5216ae575375531b227f467ceb2049be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PsdQCo2Fkgmi76Jsn_3Q2w.png"/></div></div></figure><p id="920d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is the commonly referred to as binary cross entropy (BCE) loss.</p><h1 id="55ea" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Multi-class Classification</h1><p id="04c3" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">For a multi-class problem, the goal is to predict a category from <strong class="ml fr"><em class="oo">C</em></strong> classes for each input feature <strong class="ml fr"><em class="oo">X</em></strong>.<strong class="ml fr"> </strong>In this case we can model the output <strong class="ml fr"><em class="oo">Y</em></strong> as a categorical random variable, a random variable that takes on a state c out of all possible <strong class="ml fr"><em class="oo">C</em></strong> states. As an example of categorical random variable, think of a six-faced die that can take on one of six possible states with each roll.</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pp"><img src="../Images/37005c1eb91a4b7bd720e337b07a66d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKWhZBtYECC38iw0XmHnyw.png"/></div></div></figure><p id="aee6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can see the above expression as easy extension of the case of binary random variable to a random variable having multiple categories. We can model the conditional distribution <strong class="ml fr"><em class="oo">p(Y|X)</em></strong> by making <strong class="ml fr"><em class="oo">λ</em></strong>’s as function of input features <strong class="ml fr"><em class="oo">X</em></strong>. Based on this, let’s we write the conditional categorical distribution of <strong class="ml fr"><em class="oo">Y</em></strong> in terms of predicted probabilities as follows:</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pp"><img src="../Images/6e29155a0355083486acf8a0c0e05db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U2nB4L073Gqo_aDYntowWQ.png"/></div></div></figure><p id="7e26" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Using this conditional model distribution we can write the loss function using the general loss function derived earlier in terms of cross-entropy as follows:</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pu"><img src="../Images/6ad3ae465bf243324e79a6d733c009de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6hh5gPAocZUY0nlswNGJSg.png"/></div></div></figure><p id="3af2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is referred to as Cross-Entropy loss in PyTorch. The thing to note here is that I have written this in terms of predicted probability of each class. In order to have a valid probability distribution over all <strong class="ml fr"><em class="oo">C </em></strong>classes, a softmax non-linearity is applied at the output of the model. Softmax function is written as follows:</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pp"><img src="../Images/4ac6a4e9220c2e17d64eb467e6b72096.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KYzACij6q0eZjEFdQPRNmQ.png"/></div></div></figure><h1 id="0593" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Regression</h1><p id="1aeb" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">Consider the case of data <strong class="ml fr"><em class="oo">(X, Y)</em></strong> where <strong class="ml fr"><em class="oo">X</em></strong> represents the input features and <strong class="ml fr"><em class="oo">Y</em></strong> represents output that can take on any real number value. Since <strong class="ml fr"><em class="oo">Y</em></strong> is real valued, we can model the its distribution using a Gaussian distribution.</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pu"><img src="../Images/01010a2acfb87ce81341cc905424a55d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lyPl2MaqiBNKuvRT3hpqwA.png"/></div></div></figure><p id="f2a9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Again, since we are interested in modelling the conditional distribution <strong class="ml fr"><em class="oo">p(Y|X).</em></strong> We can capture the dependence on <strong class="ml fr"><em class="oo">X</em></strong> by making the conditional mean of <strong class="ml fr"><em class="oo">Y</em></strong> a function of <strong class="ml fr"><em class="oo">X</em></strong>. For simplicity, we set variance equal to 1. The conditional distribution can be written as follows:</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pv"><img src="../Images/0d630333a23272a40e4824a00d0c5841.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4wWmieiF7KjfkqID3ENNEg.png"/></div></div></figure><p id="2b29" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can now write our general loss function for this conditional model distribution as follows:</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pw"><img src="../Images/003a6bd679d934b65064b8b13cefe42d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WOQISJP6WizaiuRZEJHepA.png"/></div></div></figure><p id="b159" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is the famous MSE loss for training the regression model. Note that the constant factor is irrelevant here as we are only interest in finding the location of minima and can be dropped.</p><h1 id="e293" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Summary</h1><p id="95fb" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">In this short article, I introduced the concepts of entropy, cross-entropy, and KL-Divergence. These concepts are essential for computing similarities (or divergences) between distributions. By using these ideas, along with a probabilistic interpretation of the model, we can define the general loss function, also referred to as the objective function. Training the model, or ‘learning,’ then boils down to minimizing the loss with respect to the model’s parameters. This optimization is typically carried out using gradient descent, which is mostly handled by deep learning frameworks like PyTorch. Hope this helps — happy learning!</p></div></div></div></div>    
</body>
</html>