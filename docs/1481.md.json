["```py\nimport pandas as pd\nimport numpy as np\n\ndata = pd.read_excel(\"mock_data.xlsx\")\nairport_data = pd.read_csv(\"airport_volume_airport_locations.csv\")\n```", "```py\n# Below code is taken from geeksforgeeks\nfrom math import radians, cos, sin, asin, sqrt\n\ndef distance_to_airport(lat, airport_lat, lon, airport_lon):\n\n    #  Convert latitude and longitude values from decimal degrees to radians\n    lon = radians(lon)\n    airport_lon = radians(airport_lon)\n    lat = radians(lat)\n    airport_lat = radians(airport_lat)\n\n    # Haversine formula\n    dlon = airport_lon - lon\n    dlat = airport_lat - lat\n    a = sin(dlat / 2)**2 + cos(lat) * cos(airport_lat) * sin(dlon / 2)**2\n\n    c = 2 * asin(sqrt(a))\n\n    # Radius of earth in kilometers.\n    r = 6371\n\n    # return distance in KM\n    return(c * r)\n\n#Apply the distance_to_airport functions to each hotel\ndata[\"distance_to_airport\"] = data.apply(lambda row: distance_to_airport(row[\"Latitude\"],row[\"Airport1Latitude\"],row[\"Longitude\"],row[\"Airport1Longitude\"]),axis=1)\ndata.head()\n```", "```py\n# Drop Columns that we dont need\n# For the purpose of benchmarking we will keep the hotel feautures, and distance to airport\ncol_to_drop = [\"Latitude\",\"Longitude\",\"Airport Code\",\"Orig\",\"Name\",\"TotalSeats\",\"Country Name\",\"Airport1Latitude\",\"Airport1Longitude\"]\n\ndata_clean = data.drop(col_to_drop,axis=1)\ndata_clean.head()\n```", "```py\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create a LabelEncoder object for each object column\nbrand_encoder = LabelEncoder()\nmarket_encoder = LabelEncoder()\ncountry_encoder = LabelEncoder()\nmarket_tier_encoder = LabelEncoder()\nhclass_encoder = LabelEncoder()\n\n# Fit each LabelEncoder on the unique values of the corresponding column\ndata_clean['BRAND'] = brand_encoder.fit_transform(data_clean['BRAND'])\ndata_clean['Market'] = market_encoder.fit_transform(data_clean['Market'])\ndata_clean['Country'] = country_encoder.fit_transform(data_clean['Country'])\ndata_clean['Market Tier'] = market_tier_encoder.fit_transform(data_clean['Market Tier'])\ndata_clean['HCLASS']= hclass_encoder.fit_transform(data_clean['HCLASS'])\n\n# create a dictionnary with all the encoders for reverse encoding\nencoders ={\"BRAND\" : brand_encoder,\n           \"Market\": market_encoder,\n           \"Country\": country_encoder,\n           \"Market Tier\": market_tier_encoder,\n           \"HCLASS\": hclass_encoder\n}\n\ndata_clean.head()\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data_clean)\ndata_scaled\n```", "```py\nfrom sklearn.neighbors import NearestNeighbors\n\nnns = NearestNeighbors()\nnns.fit(data_scaled)\nnns_results_model_0 = nns.kneighbors(data_scaled)[1]\n\nnns_results_model_0\n```", "```py\ndef clean_results(nns_results: np.ndarray,\n                  encoders: dict,\n                  data: pd.DataFrame):\n  \"\"\"\n  Returns a dataframe with a list of labels for each Nearest Neighobor group\n  \"\"\"\n  result = pd.DataFrame()\n\n  # 1\\. Get a list of Nearest Hotels based on our model\n  for i in range(len(nns_results)):\n\n    results = {} #empty dictionary to append each rows values\n\n    # Each row in nns_results contains the indexs of the selected nearest neighbors\n    # We use those index to get the Hotel names in our main data set\n    results[\"Hotels\"] = list(data.iloc[nns_results[i]].index)\n\n    # 2\\. Get the values for each features for all Nearest Neighbors groups\n    for item in  data_clean.columns:\n      results[item] = list(data.iloc[nns_results[i]][item])\n\n    # 3\\. Create a row for each Nearest Neighbor group and append to main DataFrame\n    df = pd.DataFrame([results])\n    result = pd.concat([result,df],axis=0)\n\n  # 4\\. Decode the labels to the encoded columns\n  for key, val in encoders.items():\n    result[key] = result[key].apply(lambda x : list(val.inverse_transform(x)))\n\n  result.reset_index(drop=True,inplace=True) # Reset the index for clarity\n  return result\n\nresults_model_0 = clean_results(nns_results=nns_results_model_0,\n                                encoders=encoders,\n                                data=data_clean)\nresults_model_0.head()\n```", "```py\nfrom scipy.stats import entropy\nfrom sklearn.preprocessing import MinMaxScaler\nfrom collections import Counter\n\ndef categorical_variance(data):\n    \"\"\"\n    Calculate entropy for a categorical variable from a list.\n    A higher entropy value indicates datas with diverse classes.\n    A lower entropy value indicates a more homogeneous subset of data.\n    \"\"\"\n    # Count frequency of each unique value\n    value_counts = Counter(data)\n    total_count = sum(value_counts.values())\n    probabilities = [count / total_count for count in value_counts.values()]\n    return entropy(probabilities)\n\n#set scoring weights giving higher weights to the most important features\nscoring_weights = {\"BRAND\": 0.3,\n           \"Room_count\": 0.025,\n           \"Market\": 0.25,\n           \"Country\": 0.15,\n           \"Market Tier\": 0.15,\n           \"HCLASS\": 0.05,\n           \"Demand\": 0.025,\n           \"Price range\": 0.025,\n           \"distance_to_airport\": 0.025}\n\ndef calculate_weighted_variance(df, weights):\n    \"\"\"\n    Calculate the weighted variance score for clusters in the dataset\n    \"\"\"\n    # Initialize a DataFrame to store the variances\n    variance_df = pd.DataFrame()\n\n    # 1\\. Calculate variances for numerical features\n    numerical_features = ['Room_count', 'Demand', 'Price range', 'distance_to_airport']\n    for feature in numerical_features:\n        variance_df[f'{feature}'] = df[feature].apply(np.var)\n\n    # 2\\. Calculate entropy for categorical features\n    categorical_features = ['BRAND', 'Market','Country','Market Tier','HCLASS']\n    for feature in categorical_features:\n        variance_df[f'{feature}'] = df[feature].apply(categorical_variance)\n\n    # 3\\. Normalize the variance and entropy values\n    scaler = MinMaxScaler()\n    normalized_variances = pd.DataFrame(scaler.fit_transform(variance_df),\n                                        columns=variance_df.columns,\n                                        index=variance_df.index)\n\n    # 4\\. Compute weighted average\n\n    cat_weights = {f'{feature}': weights[f'{feature}'] for feature in categorical_features}\n    num_weights = {f'{feature}': weights[f'{feature}'] for feature in numerical_features}\n\n    cat_weighted_scores = normalized_variances[categorical_features].mul(cat_weights)\n    df['cat_weighted_variance_score'] = cat_weighted_scores.sum(axis=1)\n\n    num_weighted_scores = normalized_variances[numerical_features].mul(num_weights)\n    df['num_weighted_variance_score'] = num_weighted_scores.sum(axis=1)\n\n    return df['cat_weighted_variance_score'].mean(), df['num_weighted_variance_score'].mean()\n```", "```py\n# define a function to store the results of our experiments\ndef model_score(data: pd.DataFrame,\n                weights: dict = scoring_weights,\n                model_name: str =\"model_0\"):\n  cat_score,num_score = calculate_weighted_variance(data,weights)\n  results ={\"Model\": model_name,\n            \"Primary features score\": cat_score,\n            \"Secondary features score\": num_score}\n  return results\n\nmodel_0_score= model_score(results_model_0,scoring_weights)\nmodel_0_score\n```", "```py\nnns = NearestNeighbors()\nnns.fit(data_scaled)\nnns_results_model_0 = nns.kneighbors(data_scaled)[1]\n```", "```py\n# the below is taken directly from scikit learn source\n\nfrom sklearn.neighbors._base import KNeighborsMixin, NeighborsBase, RadiusNeighborsMixin\n\nclass NearestNeighbors_(KNeighborsMixin, RadiusNeighborsMixin, NeighborsBase):\n    \"\"\"Unsupervised learner for implementing neighbor searches.\n    Parameters\n    ----------\n    n_neighbors : int, default=5\n        Number of neighbors to use by default for :meth:`kneighbors` queries.\n\n    radius : float, default=1.0\n        Range of parameter space to use by default for :meth:`radius_neighbors`\n        queries.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Default is \"minkowski\", which\n        results in the standard Euclidean distance when p = 2\\. See the\n        documentation of `scipy.spatial.distance\n        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n        the metrics listed in\n        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n        values.\n\n    p : float (positive), default=2\n        Parameter for the Minkowski metric from\n        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2\\. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n  \"\"\"\n\n    def __init__(\n        self,\n        *,\n        n_neighbors=5,\n        radius=1.0,\n        algorithm=\"auto\",\n        leaf_size=30,\n        metric=\"minkowski\",\n        p=2,\n        metric_params=None,\n        n_jobs=None,\n    ):\n        super().__init__(\n            n_neighbors=n_neighbors,\n            radius=radius,\n            algorithm=algorithm,\n            leaf_size=leaf_size,\n            metric=metric,\n            p=p,\n            metric_params=metric_params,\n            n_jobs=n_jobs,\n        )\n```", "```py\nnns_1= NearestNeighbors(n_neighbors=4)\nnns_1.fit(data_scaled)\nnns_1_results_model_1 = nns_1.kneighbors(data_scaled)[1]\nresults_model_1 = clean_results(nns_results=nns_1_results_model_1,\n                                encoders=encoders,\n                                data=data_clean)\nmodel_1_score= model_score(results_model_1,scoring_weights,model_name=\"baseline_k_4\")\nmodel_1_score\n```", "```py\n# set up weights for distance calculation\nweights_dict =  {\"BRAND\": 5,\n           \"Room_count\": 2,\n           \"Market\": 4,\n           \"Country\": 3,\n           \"Market Tier\": 3,\n           \"HCLASS\": 1.5,\n           \"Demand\": 1,\n           \"Price range\": 1,\n           \"distance_to_airport\": 1}\n# Transform the wieghts dictionnary into a list by keeping the scaled data column order\nweights = [ weights_dict[idx] for idx in list(scaler.get_feature_names_out())]\n\nnns_2= NearestNeighbors(n_neighbors=4,metric_params={ 'w': weights})\nnns_2.fit(data_scaled)\nnns_2_results_model_2 = nns_2.kneighbors(data_scaled)[1]\nresults_model_2 = clean_results(nns_results=nns_2_results_model_2,\n                                encoders=encoders,\n                                data=data_clean)\nmodel_2_score= model_score(results_model_2,scoring_weights,model_name=\"baseline_with_weights\")\nmodel_2_score\n```", "```py\nnns_3= NearestNeighbors(n_neighbors=4,p=1,metric_params={ 'w': weights})\nnns_3.fit(data_scaled)\nnns_3_results_model_3 = nns_3.kneighbors(data_scaled)[1]\nresults_model_3 = clean_results(nns_results=nns_3_results_model_3,\n                                encoders=encoders,\n                                data=data_clean)\nmodel_3_score= model_score(results_model_3,scoring_weights,model_name=\"Manhattan_with_weights\")\nmodel_3_score\n```", "```py\n#  Define the custom weighted Chebyshev distance function\ndef weighted_chebyshev(u, v, w):\n    \"\"\"Calculate the weighted Chebyshev distance between two points.\"\"\"\n    return np.max(w * np.abs(u - v))\n\nnns_4 = NearestNeighbors(n_neighbors=4,metric=weighted_chebyshev,metric_params={ 'w': weights})\nnns_4.fit(data_scaled)\nnns_4_results_model_4 = nns_4.kneighbors(data_scaled)[1]\nresults_model_4 = clean_results(nns_results=nns_4_results_model_4,\n                                encoders=encoders,\n                                data=data_clean)\nmodel_4_score= model_score(results_model_4,scoring_weights,model_name=\"Chebyshev_with_weights\")\nmodel_4_score\n```", "```py\nresults_df = pd.DataFrame([model_0_score,model_1_score,model_2_score,model_3_score,model_4_score]).set_index(\"Model\")\nresults_df.plot(kind='barh')\n```", "```py\n# Histogram of Primary features score\nresults_model_3[\"cat_weighted_variance_score\"].plot(kind=\"hist\")\n```", "```py\nexceptions = results_model_3[results_model_3[\"cat_weighted_variance_score\"]>=0.4]\n\nprint(f\" There are {exceptions.shape[0]} benchmark sets with significant variance across the primary features\")\n```"]