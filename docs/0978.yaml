- en: 'Pushing RL Boundaries: Integrating Foundational Models, e.g. LLMs and VLMs,
    into Reinforcement Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推动强化学习的边界：将基础模型（如LLMs和VLMs）整合到强化学习中
- en: 原文：[https://towardsdatascience.com/pushing-boundaries-integrating-foundational-models-e-g-556cfb6d0632?source=collection_archive---------5-----------------------#2024-04-17](https://towardsdatascience.com/pushing-boundaries-integrating-foundational-models-e-g-556cfb6d0632?source=collection_archive---------5-----------------------#2024-04-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/pushing-boundaries-integrating-foundational-models-e-g-556cfb6d0632?source=collection_archive---------5-----------------------#2024-04-17](https://towardsdatascience.com/pushing-boundaries-integrating-foundational-models-e-g-556cfb6d0632?source=collection_archive---------5-----------------------#2024-04-17)
- en: In-Depth Exploration of Integrating Foundational Models such as LLMs and VLMs
    into RL Training Loop
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨将基础模型（如LLMs和VLMs）整合到强化学习训练循环中的方法
- en: '[](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--556cfb6d0632--------------------------------)[![Elahe
    Aghapour](../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png)](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--556cfb6d0632--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--556cfb6d0632--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--556cfb6d0632--------------------------------)
    [Elahe Aghapour](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--556cfb6d0632--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--556cfb6d0632--------------------------------)[![Elahe
    Aghapour](../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png)](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--556cfb6d0632--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--556cfb6d0632--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--556cfb6d0632--------------------------------)
    [Elahe Aghapour](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--556cfb6d0632--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--556cfb6d0632--------------------------------)
    ·15 min read·Apr 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--556cfb6d0632--------------------------------)
    ·15分钟阅读·2024年4月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Authors:** [Elahe Aghapour](https://medium.com/u/75214fb27311?source=post_page---user_mention--556cfb6d0632--------------------------------),
    [Salar Rahili](https://medium.com/u/6dff1eb2cc9f?source=post_page---user_mention--556cfb6d0632--------------------------------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：** [Elahe Aghapour](https://medium.com/u/75214fb27311?source=post_page---user_mention--556cfb6d0632--------------------------------)，[Salar
    Rahili](https://medium.com/u/6dff1eb2cc9f?source=post_page---user_mention--556cfb6d0632--------------------------------)'
- en: 'Overview:'
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述：
- en: With the rise of the transformer architecture and high-throughput compute, training
    foundational models has turned into a hot topic recently. This has led to promising
    efforts to either integrate or train foundational models to enhance the capabilities
    of reinforcement learning (RL) algorithms, signaling an exciting direction for
    the field. Here, we’re discussing how foundational models can give reinforcement
    learning a major boost.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着变压器架构的兴起和高吞吐量计算的应用，训练基础模型最近成为了一个热门话题。这引发了将基础模型整合或训练基础模型以增强强化学习（RL）算法能力的有前景的努力，标志着该领域一个激动人心的方向。在这里，我们讨论的是基础模型如何为强化学习提供重大提升。
- en: 'Before diving into the latest research on how foundational models can give
    reinforcement learning a major boost, let’s engage in a brainstorming session.
    Our goal is to pinpoint areas where pre-trained foundational models, particularly
    Large Language Models (LLMs) or Vision-Language Models (VLMs), could assist us,
    or how we might train a foundational model from scratch. A useful approach is
    to examine each element of the reinforcement learning training loop individually,
    to identify where there might be room for improvement:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究基础模型如何为强化学习提供重大提升之前，让我们进行一次头脑风暴。我们的目标是明确预训练的基础模型，特别是大型语言模型（LLMs）或视觉语言模型（VLMs），在哪些领域能够为我们提供帮助，或者我们如何从零开始训练一个基础模型。一种有用的方法是逐个检查强化学习训练循环中的每个要素，以识别可能存在的改进空间：
- en: '![](../Images/d6e65070c5e31ca5151feccf4c58c6b2.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6e65070c5e31ca5151feccf4c58c6b2.png)'
- en: 'Fig 1: Overview of foundation models in RL (Image by author)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：强化学习中的基础模型概览（图像来源：作者）
- en: '**1-** Environment: Given that pre-trained foundational models understand the
    causal relationships between events, they can be utilized to forecast environmental
    changes resulting from current actions. Although this concept is intriguing, we’re
    not yet aware of any specific studies that focus on it. There are two primary
    reasons holding us back from exploring this idea further for now.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**1-** 环境：鉴于预训练的基础模型理解事件之间的因果关系，它们可以被用来预测当前动作引发的环境变化。尽管这一概念很有趣，但我们目前尚未了解有任何专门聚焦于此的研究。现在有两个主要原因使得我们暂时无法进一步探索这一想法。'
- en: While the reinforcement learning training process demands highly accurate predictions
    for the next step observations, pre-trained LLMs/VLMs haven’t been directly trained
    on datasets that enable such precise forecasting and thus fall short in this aspect.
    It’s important to note, as we highlighted in [our previous post](/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66),
    that a high-level planner, particularly one used in lifelong learning scenarios,
    could effectively incorporate a foundational model.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的训练过程需要对下一步观察做出高度准确的预测，但预训练的LLM/VLM并没有在能够进行如此精确预测的数据集上进行直接训练，因此在这一方面存在不足。需要指出的是，正如我们在[上一篇文章](/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66)中强调的那样，一个高层次的规划者，特别是在终身学习场景中使用的规划者，可以有效地结合基础模型。
- en: Latency in environment steps is a critical factor that can constrain the RL
    algorithm, especially when working within a fixed budget for training steps. The
    presence of a very large model that introduces significant latency can be quite
    restrictive. Note that while it might be challenging, distillation into a smaller
    network can be a solution here.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境步骤中的延迟是一个关键因素，它可能会限制强化学习（RL）算法，尤其是在训练步骤预算固定的情况下。一个引入显著延迟的非常大模型的存在可能会非常具有限制性。需要注意的是，虽然这可能具有挑战性，但将其蒸馏为一个更小的网络可能是一个解决方案。
- en: '**2-** State (LLM/VLM Based State Generator): While experts often use the terms
    observation and state interchangeably, there are distinctions between them. A
    state is a comprehensive representation of the environment, while an observation
    may only provide partial information. In the standard RL framework, we don’t often
    discuss the specific transformations that extract and merge useful features from
    observations, past actions, and any internal knowledge of the environment to produce
    “state”, the policy input. Such a transformation could be significantly enhanced
    by employing LLMs/VLMs, which allow us to infuse the “state” with broader knowledge
    of the world, physics, and history (refer to Fig. 1, highlighted in pink).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**2-** 状态（基于LLM/VLM的状态生成器）：虽然专家们经常将观察和状态这两个术语互换使用，但它们之间是有区别的。状态是环境的综合表示，而观察可能仅提供部分信息。在标准的RL框架中，我们通常不会讨论从观察、过去的动作以及任何环境的内部知识中提取和合并有用特征以生成“状态”，即策略输入的具体转化过程。通过使用LLM/VLM，这样的转化可以显著增强，它们使我们能够将更广泛的世界、物理和历史知识融入到“状态”中（参见图1，粉红色高亮部分）。'
- en: '**3-** Policy (Foundational Policy Model): Integrating foundational models
    into the policy, the central decision-making component in RL, can be highly beneficial.
    Although employing such models to generate high-level plans has proven successful,
    transforming the state into low-level actions has challenges we’ll delve into
    later. Fortunately, there has been some promising research in this area recently.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**3-** 策略（基础策略模型）：将基础模型融入RL中的核心决策组件——策略，可能会带来很大的益处。尽管采用此类模型生成高层次计划已经证明是成功的，但将状态转化为低层次动作仍然面临挑战，我们将在后面深入探讨。幸运的是，近期在这一领域已有一些
    promising的研究成果。'
- en: '**4-** Reward (LLM/VLM Based Reward Generator): Leveraging foundational models
    to more accurately assess chosen actions within a trajectory has been a primary
    focus among researchers. This comes as no surprise, given that rewards have traditionally
    served as the communication channel between humans and agents, setting goals and
    guiding the agent towards what is desired.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**4-** 奖励（基于LLM/VLM的奖励生成器）：利用基础模型更准确地评估轨迹中选择的动作，已成为研究者们的主要关注点。考虑到奖励传统上作为人类与智能体之间的沟通渠道，设定目标并引导智能体朝着期望的方向发展，这一点并不令人惊讶。'
- en: Pre-trained foundational models come with a deep knowledge of the world, and
    injecting this kind of understanding into our decision-making processes can make
    those decisions more in tune with human desires and more likely to succeed. Moreover,
    using foundational models to evaluate the agent’s actions can quickly trim down
    the search space and equip the agent with a head start in understanding, as opposed
    to starting from scratch.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练的基础模型具备深厚的世界知识，将这种理解注入我们的决策过程中，可以使这些决策更加符合人类的愿望，并更有可能成功。此外，使用基础模型评估代理的动作可以迅速缩小搜索空间，并为代理提供理解的起点，而不是从零开始。
- en: Pre-trained foundational models have been trained on internet-scale data generated
    mostly by humans, which has enabled them to understand worlds similarly to humans.
    This makes it possible to use foundational models as cost-effective annotators.
    They can generate labels or assess trajectories or rollouts on a large scale.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练的基础模型已经在大规模的互联网数据上进行了训练，这些数据主要由人类生成，这使得它们能够像人类一样理解世界。这使得将基础模型作为成本效益高的标注器变得可能。它们可以在大规模上生成标签或评估轨迹或展开。
- en: '**1- Foundational models in reward**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**1- 奖励中的基础模型**'
- en: It is challenging to use foundational models to generate low level control actions
    as low level actions are highly dependent on the setting of the agent and are
    underrepresented in foundational models’ training dataset. Hence, the foundation
    model application is generally focused on high level plans rather than low level
    actions. Reward bridges the gap between high-level planner and low level actions
    where foundation models can be used. Researchers have adopted various methodologies
    integrating foundation models for reward assignment. However, the core principle
    revolves around employing a VLM/LLM to effectively track the progress towards
    a subgoal or task.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基础模型生成低级控制动作是具有挑战性的，因为低级动作高度依赖于代理的设置，并且在基础模型的训练数据集中出现较少。因此，基础模型的应用通常集中在高层次的计划上，而不是低级动作。奖励弥补了高层规划者和低级动作之间的差距，这也是基础模型可以应用的地方。研究人员采用了多种方法将基础模型集成到奖励分配中。然而，核心原则是使用VLM/LLM来有效追踪朝向子目标或任务的进展。
- en: '**1.a Assigning reward values based on similarity**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.a 根据相似性分配奖励值**'
- en: 'Consider the reward value as a signal that indicates whether the agent’s previous
    action was beneficial in moving towards the goal. A sensible method involves evaluating
    how closely the previous action aligns with the current objective. To put this
    approach into practice, as can be seen in Fig. 2, it’s essential to:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 将奖励值视为一种信号，表示代理的先前动作是否有助于朝着目标前进。一种合理的方法是评估先前的动作与当前目标的对齐程度。为了将这种方法付诸实践，如图2所示，至关重要的是：
- en: '- Generate meaningful embeddings of these actions, which can be done through
    images, videos, or text descriptions of the most recent observation.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '- 生成这些动作的有意义的嵌入，可以通过图像、视频或对最新观察结果的文本描述来完成。'
- en: '- Generate meaningful representations of the current objective.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '- 生成当前目标的有意义表示。'
- en: '- Assess the similarity between these representations.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '- 评估这些表示之间的相似性。'
- en: '![](../Images/22b45a375b7df3fefaee9eb893077dc4.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22b45a375b7df3fefaee9eb893077dc4.png)'
- en: Fig 2\. Reward values based on similarity (Image by author).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. 基于相似性的奖励值（图源自作者）。
- en: Let’s explore the specific mechanics behind the leading research in this area.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一下这一领域领先研究背后的具体机制。
- en: Dense and well-shaped reward functions enhance the stability and training speed
    of the RL agent. Intrinsic rewards address this challenge by rewarding the agent
    for novel states’ exploration. However, in large environments where most of the
    unseen states are irrelevant to the downstream task, this approach becomes less
    effective. [ELLM](https://arxiv.org/pdf/2302.06692.pdf) uses background knowledge
    of LLM to shape the exploration. It queries LLM to generate a list of possible
    goals/subgoals given a list of the agent’s available actions and a text description
    of the agent current observation, generated by a state captioner. Then, at each
    time step, the reward is computed by the semantic similarity, cosine similarity,
    between the LLM generated goal and the description of the agent’s transition.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 稠密且良好形状的奖励函数能够提高RL代理的稳定性和训练速度。内在奖励通过奖励代理探索新状态来应对这一挑战。然而，在大规模环境中，大多数未见过的状态与下游任务无关，这种方法变得不太有效。[ELLM](https://arxiv.org/pdf/2302.06692.pdf)利用LLM的背景知识来塑造探索。它查询LLM生成一份可能的目标/子目标列表，给定代理可用的动作列表和由状态说明生成的代理当前观察的文本描述。然后，在每个时间步，奖励通过语义相似度，即LLM生成的目标与代理转移描述之间的余弦相似度来计算。
- en: '[LiFT](https://arxiv.org/pdf/2312.08958.pdf) has a similar framework but also
    leverages [CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf)-style VLMs for reward
    assignment. [CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf) is pre-trained to
    align videos and corresponding language descriptions through contrastive learning.
    In [LiFT](https://arxiv.org/pdf/2312.08958.pdf), the agent is rewarded based on
    the alignment score, cosine similarity, between the task instructions and videos
    of the agent’s corresponding behavior, both encoded by [CLIP4CLIP](https://arxiv.org/pdf/2104.08860.pdf).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[LiFT](https://arxiv.org/pdf/2312.08958.pdf)有一个类似的框架，但还利用了[CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf)风格的VLM进行奖励分配。[CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf)通过对比学习预训练以对齐视频和相应的语言描述。在[LiFT](https://arxiv.org/pdf/2312.08958.pdf)中，代理根据任务指令和代理相应行为视频之间的对齐得分（余弦相似度）来获得奖励，二者都通过[CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf)进行编码。'
- en: '[UAFM](https://arxiv.org/pdf/2307.09668.pdf) has a similar framework where
    the main focus is on robotic manipulation tasks, e.g., stacking a set of objects.
    For reward assignment, they measure the similarity between the agent state image
    and the task description, both embedded by [CLIP](https://arxiv.org/pdf/2103.00020.pdf).
    They finetune [CLIP](https://arxiv.org/pdf/2103.00020.pdf) on a small amount of
    data from the simulated stacking domain to be more aligned in this use case.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[UAFM](https://arxiv.org/pdf/2307.09668.pdf)有一个类似的框架，主要关注机器人操作任务，例如堆叠一组物体。在奖励分配方面，它们通过测量代理状态图像与任务描述之间的相似度来分配奖励，二者都通过[CLIP](https://arxiv.org/pdf/2103.00020.pdf)进行嵌入。它们在模拟堆叠领域的少量数据上对[CLIP](https://arxiv.org/pdf/2103.00020.pdf)进行微调，以便在此用例中获得更好的对齐。'
- en: '**1.b Assigning rewards through reasoning on auxiliary tasks:**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.b 通过推理辅助任务来分配奖励：**'
- en: In scenarios where the foundational model has the proper understanding of the
    environment, it becomes feasible to directly pass the observations within a trajectory
    to the model, LLM/VLM. This evaluation can be done either through straightforward
    QA sessions based on the observations or by verifying the model’s capability in
    predicting the goal only by looking at the observation trajectory.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在基础模型正确理解环境的情况下，直接将轨迹中的观察结果传递给模型LLM/VLM成为可行。这种评估可以通过基于观察结果的简单问答会话进行，或者通过验证模型仅通过观察轨迹预测目标的能力来进行。
- en: '![](../Images/43e8a34aa903d5be475522d623219faf.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43e8a34aa903d5be475522d623219faf.png)'
- en: Fig 3\. Assigning reward through reasoning (Image by author).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. 通过推理分配奖励（图片来源：作者）。
- en: '[Read and Reward](https://arxiv.org/pdf/2302.04449.pdf) integrates the environment’s
    instruction manual into reward generation through two key components, as can be
    seen in Fig. 3:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[阅读与奖励](https://arxiv.org/pdf/2302.04449.pdf)通过两个关键组成部分将环境的说明手册融入奖励生成中，如图3所示：'
- en: 'QA extraction module: it creates a summary of game objectives and features.
    This LLM-based module, [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf)-large,
    takes in the game manual and a question, and extracts the corresponding answer
    from the text. Questions are focused on the game objective, and agent-object interaction,
    identified by their significance using TF-IDF. For each critical object, a question
    as: “What happens when the player hits a <object>?” is added to the question set.
    A summary is then formed with the concatenation of all non-empty question-answer
    pairs.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: QA提取模块：它创建游戏目标和特征的摘要。这个基于LLM的模块，[RoBERTa](https://arxiv.org/pdf/1907.11692.pdf)-large，接受游戏手册和一个问题，并从文本中提取相应的答案。问题集中在游戏目标和代理-对象交互上，通过TF-IDF来识别其重要性。对于每个关键对象，都会增加一个类似于：“当玩家击中<对象>时，会发生什么？”的问题到问题集。然后，通过将所有非空的问答对连接起来，形成一个摘要。
- en: 'Reasoning module: During gameplay, a rule-based algorithm detects “hit” events.
    Following each “hit” event, the LLM based reasoning module is queried with the
    summary of the environment and a question: “Should you hit a <object of interaction>
    if you want to win?” where the possible answer is limited to {yes, no}. A “yes”
    response adds a positive reward, while “no” leads to a negative reward.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推理模块：在游戏过程中，基于规则的算法检测“击中”事件。在每个“击中”事件后，基于LLM的推理模块会查询环境的摘要和一个问题：“如果你想获胜，是否应该击中一个<互动对象>？”其中可能的回答仅限于{是，否}。回答“是”会增加正奖励，而回答“否”则会导致负奖励。
- en: '[EAGER](https://arxiv.org/pdf/2206.09674.pdf) introduces a unique method for
    creating intrinsic rewards through a specially designed auxiliary task. This approach
    presents a novel concept where the auxiliary task involves predicting the goal
    based on the current observation. If the model predicts accurately, this indicates
    a strong alignment with the intended goal, and thus, a larger intrinsic reward
    is given based on the prediction confidence level. To achieve this goal, To accomplish
    this, two modules are employed:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[EAGER](https://arxiv.org/pdf/2206.09674.pdf)引入了一种通过专门设计的辅助任务创建内在奖励的独特方法。这种方法提出了一个新概念，其中辅助任务涉及基于当前观察预测目标。如果模型预测准确，这表明与预期目标的高度一致，因此，根据预测的置信度水平，会给予更大的内在奖励。为实现这一目标，采用了两个模块：'
- en: 'Question Generation (QG): This component works by masking all nouns and adjectives
    in the detailed objective provided by the user.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题生成（QG）：这个组件通过屏蔽用户提供的详细目标中的所有名词和形容词来工作。
- en: 'Question Answering (QA): This is a model trained in a supervised manner, which
    takes the observation, question masks, and actions, and predicts the masked tokens.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答（QA）：这是一个通过监督方式训练的模型，它接受观察、问题掩码和动作，并预测被掩盖的标记。
- en: (P.S. Although this work does not utilize a foundational model, we’ve included
    it here due to its intriguing approach, which can be easily adapted to any pre-trained
    LLM)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: （P.S. 尽管这项工作没有使用基础模型，但由于它的有趣方法，我们将其包括在内，因为该方法可以很容易地适应任何预训练的LLM。）
- en: '**1.c Generating reward function code**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.c 生成奖励函数代码**'
- en: Up to this point, we’ve discussed generating reward values directly for the
    reinforcement learning algorithms. However, running a large model at every step
    of the RL loop can significantly slow down the speed of both training and inference.
    To bypass this bottleneck, one strategy involves utilizing our foundational model
    to generate the code for the reward function. This allows for the direct generation
    of reward values at each step, streamlining the process.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了直接为强化学习算法生成奖励值。然而，在每个RL循环步骤中运行一个大型模型可能会显著降低训练和推理的速度。为了绕过这一瓶颈，一种策略是利用我们的基础模型生成奖励函数的代码。这样可以在每一步直接生成奖励值，从而简化过程。
- en: 'For the code generation schema to work effectively, two key components are
    required:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使代码生成方案有效工作，需要两个关键组件：
- en: 1- A code generator, LLM, which receives a detailed prompt containing all the
    necessary information to craft the code.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 一个代码生成器LLM，它接收一个包含所有必要信息的详细提示，以便生成代码。
- en: 2- A refinement process that evaluates and enhances the code in collaboration
    with the code generator.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 2- 一个与代码生成器协作的细化过程，用于评估和增强代码。
- en: 'Let’s look at the key contributions for generating reward code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看生成奖励代码的关键贡献：
- en: '[R2R2S](https://arxiv.org/pdf/2306.08647.pdf) generates reward function code
    through two main components:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[R2R2S](https://arxiv.org/pdf/2306.08647.pdf)通过两个主要组件生成奖励函数代码：'
- en: 'LLM based motion descriptor: This module uses a pre-defined template to describe
    robot movements, and leverages Large Language Models (LLMs) to understand the
    motion. The Motion Descriptor fills in the template, replacing placeholders e.g.
    “Destination Point Coordinate” with specific details, to describe the desired
    robot motion within a pre-defined template.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于LLM的动作描述符：该模块使用预定义模板来描述机器人动作，并利用大语言模型（LLMs）来理解这些动作。动作描述符填写模板，替换占位符，例如将“目标点坐标”替换为具体细节，以在预定义模板内描述所需的机器人动作。
- en: 'LLM based reward coder: this component generates the reward function by processing
    a prompt containing: a motion description, a list of functions with their description
    that LLM can use to generate the reward function code, an example code of how
    the response should look like, and constraints and rules the reward function must
    follow.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于LLM的奖励编码器：该组件通过处理包含以下内容的提示来生成奖励函数：动作描述、LLM可以用来生成奖励函数代码的函数列表及其描述、响应应如何呈现的示例代码，以及奖励函数必须遵循的约束和规则。
- en: '[Text2Reward](https://arxiv.org/pdf/2309.11489.pdf) develops a method to generate
    dense reward functions as an executable code within iterative refinement. Given
    the subgoal of the task, it has two key components:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[Text2Reward](https://arxiv.org/pdf/2309.11489.pdf)开发了一种方法，用于在迭代细化过程中生成可执行代码的密集奖励函数。根据任务的子目标，它有两个关键组件：'
- en: 'LLM-based reward coder: generates reward function code. Its prompt consists
    of: an abstract of observation and available actions, a compact pythonic style
    environment to represent the configuration of the objects, robot, and callable
    functions; a background knowledge for reward function design (e.g. “reward function
    for task X typically includes a term for the distance between object x and y”),
    and a few-shot examples. They assume access to a pool of instruction, and reward
    function pairs that top k similar instructions are retrieved as few-shot examples.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于LLM的奖励编码器：生成奖励函数代码。它的提示包括：观察和可用动作的摘要，表示物体、机器人和可调用函数配置的简洁Python风格环境；奖励函数设计的背景知识（例如，“任务X的奖励函数通常包括物体x和y之间的距离项”），以及少量示例。他们假设可以访问一个指令和奖励函数对的池，并检索最相似的前k个指令作为少量示例。
- en: 'LLM-Based Refinement: once the reward code is generated, the code is executed
    to identify the syntax errors and runtime errors. These feedbacks are integrated
    into subsequent prompts to generate more refined reward functions. Additionally,
    human feedback is requested based on a task execution video by the current policy.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于LLM的细化：一旦生成了奖励代码，代码会被执行以识别语法错误和运行时错误。这些反馈会被集成到后续的提示中，以生成更精细的奖励函数。此外，还会根据当前策略请求人类反馈，反馈内容来源于任务执行视频。
- en: '[Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf) has a similar algorithm
    to [Text2Reward](https://arxiv.org/pdf/2309.11489.pdf), to generate the reward
    function code, see Fig. 4\. The main difference is in the refinement stage where
    it has two modules, both LLMs:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf)与[Text2Reward](https://arxiv.org/pdf/2309.11489.pdf)具有相似的算法，用于生成奖励函数代码，见图4。主要区别在于细化阶段，它有两个模块，都是LLM：'
- en: 'LLM-Based Reward Critic: It evaluates the code and provides feedback on whether
    the code is self-consistent and free of syntax and semantic errors.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于LLM的奖励评论员：它评估代码并提供反馈，判断代码是否自洽、是否没有语法和语义错误。
- en: 'LLM-Based Trajectory Analyser: It reviews the historical information of the
    interaction between the trained agent and the environment and uses it to guide
    the modifications of the reward function.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于LLM的轨迹分析器：它回顾训练过的智能体与环境之间的历史互动信息，并利用这些信息来指导奖励函数的修改。
- en: '![](../Images/9f84ed9097ad74b7b0d223a4572abf62.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f84ed9097ad74b7b0d223a4572abf62.png)'
- en: Fig 4\. Overview of [Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf) (paper
    taken from [Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf) paper)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. [Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf)概览（图来自[Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf)论文）
- en: '[EUREKA](https://arxiv.org/pdf/2310.12931.pdf) generates reward code without
    the need for task-specific prompting, predefined reward templates, or predefined
    few-shot examples. To achieve this goal, it has two stages:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[EUREKA](https://arxiv.org/pdf/2310.12931.pdf)生成奖励代码，无需特定任务提示、预定义奖励模板或预定义的少量示例。为实现这一目标，它有两个阶段：'
- en: 'LLM-based code generation: The raw environment code, the task, generic reward
    design and formatting tips are fed to the LLM as context and LLM returns the executable
    reward code with a list of its components.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于LLM的代码生成：原始环境代码、任务、通用奖励设计和格式化提示作为上下文输入到LLM，LLM返回可执行的奖励代码以及其组件列表。
- en: 'Evolutionary search and refinement: At each iteration, [EUREKA](https://arxiv.org/pdf/2310.12931.pdf)
    queries the LLM to generate several i.i.d reward functions. Training an agent
    with executable reward functions provides feedback on how well the agent is performing.
    For a detailed and focused analysis of the rewards, the feedback also includes
    scalar values for each component of the reward function. The LLM takes top-performing
    reward code along with this detailed feedback to mutate the reward code in-context.
    In each subsequent iteration, the LLM uses the top reward code as a reference
    to generate K more i.i.d reward codes. This iterative optimization continues until
    a specified number of iterations has been reached.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进化搜索和优化：在每次迭代中，[EUREKA](https://arxiv.org/pdf/2310.12931.pdf) 查询LLM生成多个独立同分布（i.i.d）奖励函数。使用可执行奖励函数训练代理可以提供关于代理表现的反馈。为了对奖励进行详细和专注的分析，反馈还包括奖励函数每个组件的标量值。LLM根据表现最好的奖励代码以及这些详细反馈，进行奖励代码的上下文突变。在每次后续迭代中，LLM使用最优奖励代码作为参考，生成K个新的i.i.d奖励代码。这个迭代优化过程会持续进行，直到达到指定的迭代次数。
- en: Within these two steps, [EUREKA](https://arxiv.org/pdf/2310.12931.pdf) is able
    to generate reward functions that outperform expert human-engineered rewards without
    any task specific templates.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个步骤中，[EUREKA](https://arxiv.org/pdf/2310.12931.pdf)能够生成超越专家人工设计的奖励函数，而无需任何特定任务模板。
- en: '**1.d. Train a reward model based on preferences (RLAIF)**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.d. 基于偏好的奖励模型训练（RLAIF）**'
- en: An alternative method is to use a foundational model to generate data for training
    a reward function model. The significant successes of Reinforcement Learning with
    Human Feedback ([RLHF](https://arxiv.org/pdf/2203.02155.pdf)) have recently drawn
    increased attention towards employing trained reward functions on a larger scale.
    The heart of such algorithms is the use of a preference dataset to train a reward
    model which can subsequently be integrated into reinforcement learning algorithms.
    Given the high cost associated with generating preference data (e.g., action A
    is preferable to action B) through human feedback, there’s growing interest in
    constructing this dataset by obtaining feedback from an AI agent, i.e. VLM/LLM.
    Training a reward function, using AI-generated data and integrating it within
    a reinforcement learning algorithm, is known as Reinforcement Learning with AI
    Feedback (RLAIF).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用基础模型生成数据来训练奖励函数模型。最近，强化学习与人类反馈（[RLHF](https://arxiv.org/pdf/2203.02155.pdf)）的显著成功吸引了越来越多的关注，特别是如何在更大规模上使用训练好的奖励函数。这类算法的核心在于利用偏好数据集训练奖励模型，之后将其整合到强化学习算法中。由于通过人类反馈生成偏好数据（例如，行为A优于行为B）成本高昂，越来越多的研究开始关注通过AI代理（如VLM/LLM）获取反馈来构建该数据集。使用AI生成的数据训练奖励函数，并将其整合到强化学习算法中，这一过程被称为“AI反馈强化学习”（RLAIF）。
- en: '[MOTIF](https://arxiv.org/pdf/2310.00166.pdf) requires access to a passive
    dataset of observations with sufficient coverage. Initially, LLM is queried with
    a summary of desired behaviors within the environment and a text description of
    two randomly sampled observations. It then generates the preference, selecting
    between 1, 2, or 0 (indicating no preference), as seen in Fig. 5\. This process
    constructs a dataset of preferences between observation pairs. Subsequently, this
    dataset is used to train a reward model employing [preference based RL techniques](https://jmlr.org/papers/volume18/16-634/16-634.pdf).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[MOTIF](https://arxiv.org/pdf/2310.00166.pdf)需要访问一个具有充分覆盖的被动观察数据集。最初，LLM通过提供环境中期望行为的总结和两个随机采样的观察描述来进行查询。然后，它生成偏好，在1、2或0之间进行选择（表示无偏好），如图5所示。这个过程构建了一个关于观察对之间偏好的数据集。随后，该数据集被用来训练一个奖励模型，采用[基于偏好的RL技术](https://jmlr.org/papers/volume18/16-634/16-634.pdf)。'
- en: '![](../Images/80cc3f30ff6414b84db9a4b52ba2d4f2.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80cc3f30ff6414b84db9a4b52ba2d4f2.png)'
- en: Fig 5\. A schematic representation of the three phases of [MOTIF](https://arxiv.org/pdf/2310.00166.pdf)
    (image taken from [MOTIF](https://arxiv.org/pdf/2310.00166.pdf) paper)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5. [MOTIF](https://arxiv.org/pdf/2310.00166.pdf)的三个阶段的示意图（图片来源于[MOTIF](https://arxiv.org/pdf/2310.00166.pdf)论文）
- en: '**2- Foundation models as Policy**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**2- 基础模型作为策略**'
- en: Achieving the capability to train a foundational policy that not only excels
    in tasks previously encountered but also possesses the ability to reason about
    and adapt to new tasks using past learning, is an ambition within the RL community.
    Such a policy would ideally generalize from past experiences to tackle novel situations
    and, through environmental feedback, achieve goals previously unseen with human-like
    adaptability.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实现训练一个基础策略的能力，不仅能够在以前遇到的任务中表现出色，而且能够利用过去的学习对新任务进行推理和适应，这是RL社区的一个目标。这样的策略理想情况下能够从过去的经验中推广到新情况，并通过环境反馈，实现以前未见过的目标，展现出类人适应性。
- en: 'However, several challenges stand in the way of training such agents. Among
    these challenges are:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，训练这种代理面临着一些挑战。其中一些挑战包括：
- en: The necessity of managing a very large model, which introduces significant latency
    into the decision-making process for low-level control actions.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理一个非常大的模型的必要性，这会给低级控制操作的决策过程带来显著的延迟。
- en: The requirement to collect a vast amount of interaction data across a wide array
    of tasks to enable effective learning.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集大量跨多个任务的交互数据，以便进行有效学习的需求。
- en: Additionally, the process of training a very large network from scratch using
    RL introduces extra complexities. This is because backpropagation efficiency inherently
    is weaker in RL compared to supervised training methods .
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，从头开始使用RL训练一个非常大的网络还引入了额外的复杂性。这是因为与监督训练方法相比，反向传播效率在RL中本身就较弱。
- en: Up to now, it’s mostly been teams with substantial resources and top-notch setups
    who’ve really pushed the envelope in this domain.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，主要是那些拥有大量资源和一流设备的团队在这一领域真正推动了技术的边界。
- en: '[AdA](https://arxiv.org/pdf/2301.07608.pdf) paved the way for training an RL
    foundation model within the X.Land 2.0 3D environment. This model achieves human
    time-scale adaptation on held-out test tasks without any further training. The
    model’s success is founded on three ingredients:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[AdA](https://arxiv.org/pdf/2301.07608.pdf)为在X.Land 2.0 3D环境中训练RL基础模型铺平了道路。该模型能够在没有进一步训练的情况下，在保留的测试任务上实现人类时间尺度的适应。该模型的成功建立在三个关键因素之上：'
- en: The core of the [AdA](https://arxiv.org/pdf/2301.07608.pdf)’s learning mechanism
    is a Transformer-XL architecture from 23 to 265 million parameters, employed alongside
    the [Muesli](https://arxiv.org/pdf/2104.06159.pdf) RL algorithm. Transformer-XL
    takes in a trajectory of observations, actions, and rewards from time t to T and
    outputs a sequence of hidden states for each time step. The hidden state is utilized
    to predict reward, value, and action distribution π. The combination of both long-term
    and short-term memory is critical for fast adaptation. Long-term memory is achieved
    through slow gradient updates, whereas short-term memory can be captured within
    the context length of the transformer. This unique combination allows the model
    to preserve knowledge across multiple task attempts by retaining memory across
    trials, even though the environment resets between trials.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[AdA](https://arxiv.org/pdf/2301.07608.pdf)学习机制的核心是一个23到2.65亿参数的Transformer-XL架构，与[
    Muesli](https://arxiv.org/pdf/2104.06159.pdf) RL算法一起使用。Transformer-XL接受从时间t到T的一段观察、行动和奖励轨迹，并为每个时间步输出一系列隐藏状态。隐藏状态用于预测奖励、价值和行动分布π。长期记忆和短期记忆的结合对快速适应至关重要。长期记忆通过缓慢的梯度更新实现，而短期记忆则可以在变压器的上下文长度内捕获。这种独特的结合使得模型能够通过在试验之间保留记忆，从多个任务尝试中保留知识，即使环境在试验之间重置。'
- en: The model benefits from meta-RL training across 1⁰⁴⁰ different partially observable
    Markov decision processes (POMDPs) tasks. Since [transformers are meta-learners](https://arxiv.org/pdf/2206.06614.pdf),
    no additional meta step is required.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型从在1⁰⁴⁰个不同的部分可观察马尔可夫决策过程（POMDP）任务上进行元强化学习训练中受益。由于[变压器是元学习者](https://arxiv.org/pdf/2206.06614.pdf)，因此不需要额外的元步骤。
- en: Given the size and diversity of the task pool, many tasks will either be too
    easy or too hard to generate a good training signal. To tackle this, they used
    an automated curriculum to prioritize tasks that are within its capability frontier.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴于任务池的规模和多样性，许多任务将过于简单或过于困难，无法生成良好的训练信号。为了解决这个问题，他们使用了一个自动化课程来优先处理那些在其能力范围内的任务。
- en: '[RT-2](https://arxiv.org/pdf/2307.15818.pdf) introduces a method to co-finetune
    a VLM on both robotic trajectory data and vision-language tasks, resulting in
    a policy model called [RT-2](https://arxiv.org/pdf/2307.15818.pdf). To enable
    vision-language models to generate low-level actions, actions are discretized
    into 256 bins and represented as language tokens.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[RT-2](https://arxiv.org/pdf/2307.15818.pdf)提出了一种方法，通过在机器人轨迹数据和视觉语言任务上联合微调VLM，生成一种名为[RT-2](https://arxiv.org/pdf/2307.15818.pdf)的策略模型。为了使视觉语言模型能够生成低级动作，动作被离散化为256个桶，并表示为语言符号。'
- en: By representing actions as language tokens, [RT-2](https://arxiv.org/pdf/2307.15818.pdf)
    can directly utilize pre-existing VLM architectures without requiring substantial
    modifications. Hence, VLM input includes robot camera image and textual task description
    formatted similarly to Vision Question Answering tasks and the output is a series
    of language tokens that represent the robot’s low-level actions; see Fig. 6.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将动作表示为语言符号，[RT-2](https://arxiv.org/pdf/2307.15818.pdf)可以直接利用现有的视觉语言模型（VLM）架构，而无需进行大量修改。因此，VLM的输入包括机器人摄像头图像和格式化类似于视觉问答任务的文本任务描述，输出则是一系列语言符号，代表机器人低级动作；见图6。
- en: '![](../Images/94312d770f343bd4684840e4a373dbc6.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94312d770f343bd4684840e4a373dbc6.png)'
- en: Fig 6\. [RT-2](https://arxiv.org/pdf/2307.15818.pdf) overview (image taken from
    [RT-2](https://arxiv.org/pdf/2307.15818.pdf) paper)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图6. [RT-2](https://arxiv.org/pdf/2307.15818.pdf) 概述（图像来自 [RT-2](https://arxiv.org/pdf/2307.15818.pdf)
    论文）
- en: They noticed that co-finetuning on both types of data with the original web
    data leads to more generalizable policies. The co-finetuning process equips [RT-2](https://arxiv.org/pdf/2307.15818.pdf)
    with the ability to understand and execute commands that were not explicitly present
    in its training data, showcasing remarkable adaptability. This approach enabled
    them to leverage internet-scale pretraining of VLM to generalize to novel tasks
    through semantic reasoning.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 他们注意到，使用原始网页数据与这两种类型数据进行联合微调，能够得到更具普适性的策略。联合微调过程使得[RT-2](https://arxiv.org/pdf/2307.15818.pdf)具备了理解和执行训练数据中未明确出现的指令的能力，展现出卓越的适应性。这种方法使得他们能够利用互联网规模的视觉语言模型预训练，通过语义推理泛化到新的任务。
- en: '**3- Foundation Models as State Representation**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**3- 基础模型作为状态表示**'
- en: In RL, a policy’s understanding of the environment at any given moment comes
    from its “state” which is essentially how it perceives its surroundings. Looking
    at the RL block diagram, a reasonable module to inject world knowledge into is
    the state. If we can enrich observations with general knowledge useful for completing
    tasks, the policy can pick up new tasks much faster compared to RL agents that
    begin learning from scratch.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，策略对环境的理解来自于它的“状态”，本质上就是它如何感知其周围的环境。通过观察强化学习的框架图，一个合理的模块来注入世界知识的是状态。如果我们能够通过有助于完成任务的一般知识来丰富观察数据，那么与从零开始学习的强化学习智能体相比，策略将能更快地掌握新任务。
- en: '[PR2L](https://arxiv.org/pdf/2402.02651.pdf) introduces a novel approach to
    inject the background knowledge of VLMs from internet scale data into RL.[PR2L](https://arxiv.org/pdf/2402.02651.pdf)
    employs generative VLMs which generate language in response to an image and a
    text input. As VLMs are proficient in understanding and responding to visual and
    textual inputs, they can provide a rich source of semantic features from observations
    to be linked to actions.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[PR2L](https://arxiv.org/pdf/2402.02651.pdf)提出了一种新颖的方法，通过来自互联网规模数据的视觉语言模型（VLM）背景知识注入到强化学习（RL）中。[PR2L](https://arxiv.org/pdf/2402.02651.pdf)采用了生成型VLM，这些模型根据图像和文本输入生成语言。由于VLM擅长理解和响应视觉与文本输入，它们可以提供丰富的语义特征来源，这些特征可以与动作关联。'
- en: '[PR2L](https://arxiv.org/pdf/2402.02651.pdf), queries a VLM with a task-relevant
    prompt for each visual observation received by the agent, and receives both the
    generated textual response and the model’s intermediate representations. They
    discard the text and use some or all of the models intermediate representation
    generated for both the visual and text input and the VLM’s generated textual response
    as “promptable representations”. Due to the variable size of these representations,
    [PR2L](https://arxiv.org/pdf/2402.02651.pdf) incorporates an encoder-decoder Transformer
    layer to embed all the information embedded in promptable representations into
    a fixed size embedding. This embedding, combined with any available non-visual
    observation data, is then provided to the policy network, representing the state
    of the agent. This innovative integration allows the RL agent to leverage the
    rich semantic understanding and background knowledge of VLMs, facilitating more
    rapid and informed learning of tasks.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[PR2L](https://arxiv.org/pdf/2402.02651.pdf)查询VLM（视觉语言模型）并为每个收到的视觉观测提供与任务相关的提示，接收生成的文本响应和模型的中间表示。他们丢弃文本，并使用生成的视觉和文本输入的部分或全部模型中间表示以及VLM生成的文本响应作为“可提示表示”。由于这些表示的大小可变，[PR2L](https://arxiv.org/pdf/2402.02651.pdf)采用了一个编码-解码器Transformer层，将所有信息嵌入到一个固定大小的嵌入向量中。这个嵌入向量结合任何可用的非视觉观测数据，然后提供给策略网络，表示代理的状态。这种创新的整合方法使得RL（强化学习）代理能够利用VLM的丰富语义理解和背景知识，从而更快速和更有信息地学习任务。'
- en: '**Also Read Our Previous Post:** [Towards AGI: LLMs and Foundational Models’
    Roles in the Lifelong Learning Revolution](/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66)'
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**另请阅读我们的上一篇文章：** [走向通用人工智能：大语言模型和基础模型在终身学习革命中的作用](/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66)'
- en: '**References:**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[1] [ELLM](https://arxiv.org/pdf/2302.06692.pdf): Du, Yuqing, et al. “Guiding
    pretraining in reinforcement learning with large language models.” 2023.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [ELLM](https://arxiv.org/pdf/2302.06692.pdf): Du, Yuqing 等人。“通过大语言模型引导强化学习的预训练。”2023年。'
- en: '[2] [Text2Reward](https://arxiv.org/pdf/2309.11489.pdf): Xie, Tianbao, et al.
    “Text2reward: Automated dense reward function generation for reinforcement learning.”
    2023.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [Text2Reward](https://arxiv.org/pdf/2309.11489.pdf): Xie, Tianbao 等人。“Text2reward：强化学习的自动化稠密奖励函数生成。”2023年。'
- en: '[3] [R2R2S](https://arxiv.org/pdf/2306.08647.pdf): Yu, Wenhao, et al. “Language
    to rewards for robotic skill synthesis.” 2023.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [R2R2S](https://arxiv.org/pdf/2306.08647.pdf): Yu, Wenhao 等人。“从语言到奖励：机器人技能合成。”2023年。'
- en: '[4] [EUREKA](https://arxiv.org/pdf/2310.12931.pdf): Ma, Yecheng Jason, et al.
    “Eureka: Human-level reward design via coding large language models.” 2023.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [EUREKA](https://arxiv.org/pdf/2310.12931.pdf): Ma, Yecheng Jason 等人。“Eureka：通过编码大语言模型进行人类水平的奖励设计。”2023年。'
- en: '[5] [MOTIF](https://arxiv.org/pdf/2310.00166.pdf): Klissarov, Martin, et al.
    “Motif: Intrinsic motivation from artificial intelligence feedback.” 2023.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [MOTIF](https://arxiv.org/pdf/2310.00166.pdf): Klissarov, Martin 等人。“Motif：来自人工智能反馈的内在动机。”2023年。'
- en: '[6] [Read and Reward](https://arxiv.org/pdf/2302.04449.pdf): Wu, Yue, et al.
    “Read and reap the rewards: Learning to play atari with the help of instruction
    manuals.” 2024.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [Read and Reward](https://arxiv.org/pdf/2302.04449.pdf): Wu, Yue 等人。“阅读并获得奖励：通过使用说明书学习玩Atari。”2024年。'
- en: '[7] [Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf): Li, Hao, et al.
    “Auto MC-reward: Automated dense reward design with large language models for
    minecraft.” 2023.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf): Li, Hao 等人。“Auto
    MC-reward：利用大语言模型为Minecraft自动设计稠密奖励。”2023年。'
- en: '[8] [EAGER](https://arxiv.org/pdf/2206.09674.pdf): Carta, Thomas, et al. “Eager:
    Asking and answering questions for automatic reward shaping in language-guided
    RL.” 2022.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [EAGER](https://arxiv.org/pdf/2206.09674.pdf): Carta, Thomas 等人。“Eager：提出和回答问题以进行语言引导的RL中的自动奖励塑造。”2022年。'
- en: '[9] [LiFT](https://arxiv.org/pdf/2312.08958.pdf): Nam, Taewook, et al. “LiFT:
    Unsupervised Reinforcement Learning with Foundation Models as Teachers.” 2023.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [LiFT](https://arxiv.org/pdf/2312.08958.pdf): Nam, Taewook 等人。“LiFT：使用基础模型作为教师的无监督强化学习。”2023年。'
- en: '[10] [UAFM](https://arxiv.org/pdf/2307.09668.pdf): Di Palo, Norman, et al.
    “Towards a unified agent with foundation models.” 2023.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] [UAFM](https://arxiv.org/pdf/2307.09668.pdf): Di Palo, Norman 等人。“走向一个统一的代理与基础模型。”2023年。'
- en: '[11] [RT-2](https://arxiv.org/pdf/2307.15818.pdf): Brohan, Anthony, et al.
    “Rt-2: Vision-language-action models transfer web knowledge to robotic control.”
    2023.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] [RT-2](https://arxiv.org/pdf/2307.15818.pdf): Brohan, Anthony 等人。“Rt-2：视觉-语言-动作模型将网络知识转移到机器人控制。”2023年。'
- en: '[12] [AdA](https://arxiv.org/pdf/2301.07608.pdf): Team, Adaptive Agent, et
    al. “Human-timescale adaptation in an open-ended task space.” 2023.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] [AdA](https://arxiv.org/pdf/2301.07608.pdf)：自适应代理团队等人。“在开放式任务空间中进行人类时间尺度的适应。”2023年。'
- en: '[13] [PR2L](https://arxiv.org/pdf/2402.02651.pdf): Chen, William, et al. “Vision-Language
    Models Provide Promptable Representations for Reinforcement Learning.” 2024.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] [PR2L](https://arxiv.org/pdf/2402.02651.pdf)：Chen, William 等人。“视觉-语言模型为强化学习提供可提示的表征。”2024年。'
- en: '[14] [Clip4Clip](https://arxiv.org/pdf/2104.08860.pdf): Luo, Huaishao, et al.
    “Clip4clip: An empirical study of clip for end to end video clip retrieval and
    captioning.” 2022.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] [Clip4Clip](https://arxiv.org/pdf/2104.08860.pdf)：Luo, Huaishao 等人。“Clip4clip：关于Clip在端到端视频片段检索与字幕生成中的实证研究。”2022年。'
- en: '[15] [Clip](https://arxiv.org/pdf/2103.00020.pdf): Radford, Alec, et al. “Learning
    transferable visual models from natural language supervision.” 2021.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] [Clip](https://arxiv.org/pdf/2103.00020.pdf)：Radford, Alec 等人。“从自然语言监督中学习可迁移的视觉模型。”2021年。'
- en: '[16] [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf): Liu, Yinhan, et al. “Roberta:
    A robustly optimized bert pretraining approach.” 2019.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf)：Liu, Yinhan 等人。“Roberta：一种强健优化的BERT预训练方法。”2019年。'
- en: '[17] [Preference based RL](https://jmlr.org/papers/volume18/16-634/16-634.pdf):
    SWirth, Christian, et al. “A survey of preference-based reinforcement learning
    methods.” 2017.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] [基于偏好的强化学习](https://jmlr.org/papers/volume18/16-634/16-634.pdf)：SWirth,
    Christian 等人。“基于偏好的强化学习方法综述。”2017年。'
- en: '[18] [Muesli](https://arxiv.org/pdf/2104.06159.pdf): Hessel, Matteo, et al.
    “Muesli: Combining improvements in policy optimization.” 2021.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] [Muesli](https://arxiv.org/pdf/2104.06159.pdf)：Hessel, Matteo 等人。“Muesli：结合政策优化的改进。”2021年。'
- en: '[19] Melo, Luckeciano C. “[Transformers are meta-reinforcement learners](https://arxiv.org/pdf/2206.06614.pdf).”
    2022.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Melo, Luckeciano C. “[变压器是元强化学习者](https://arxiv.org/pdf/2206.06614.pdf)。”2022年。'
- en: '[20] [RLHF](https://arxiv.org/pdf/2203.02155.pdf): Ouyang, Long, et al. “Training
    language models to follow instructions with human feedback, 2022.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] [RLHF](https://arxiv.org/pdf/2203.02155.pdf)：Ouyang, Long 等人。“通过人类反馈训练语言模型遵循指令。”2022年。'
