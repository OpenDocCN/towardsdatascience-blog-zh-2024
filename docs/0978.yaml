- en: 'Pushing RL Boundaries: Integrating Foundational Models, e.g. LLMs and VLMs,
    into Reinforcement Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pushing-boundaries-integrating-foundational-models-e-g-556cfb6d0632?source=collection_archive---------5-----------------------#2024-04-17](https://towardsdatascience.com/pushing-boundaries-integrating-foundational-models-e-g-556cfb6d0632?source=collection_archive---------5-----------------------#2024-04-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In-Depth Exploration of Integrating Foundational Models such as LLMs and VLMs
    into RL Training Loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--556cfb6d0632--------------------------------)[![Elahe
    Aghapour](../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png)](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--556cfb6d0632--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--556cfb6d0632--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--556cfb6d0632--------------------------------)
    [Elahe Aghapour](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--556cfb6d0632--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--556cfb6d0632--------------------------------)
    ·15 min read·Apr 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '**Authors:** [Elahe Aghapour](https://medium.com/u/75214fb27311?source=post_page---user_mention--556cfb6d0632--------------------------------),
    [Salar Rahili](https://medium.com/u/6dff1eb2cc9f?source=post_page---user_mention--556cfb6d0632--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overview:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the rise of the transformer architecture and high-throughput compute, training
    foundational models has turned into a hot topic recently. This has led to promising
    efforts to either integrate or train foundational models to enhance the capabilities
    of reinforcement learning (RL) algorithms, signaling an exciting direction for
    the field. Here, we’re discussing how foundational models can give reinforcement
    learning a major boost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before diving into the latest research on how foundational models can give
    reinforcement learning a major boost, let’s engage in a brainstorming session.
    Our goal is to pinpoint areas where pre-trained foundational models, particularly
    Large Language Models (LLMs) or Vision-Language Models (VLMs), could assist us,
    or how we might train a foundational model from scratch. A useful approach is
    to examine each element of the reinforcement learning training loop individually,
    to identify where there might be room for improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6e65070c5e31ca5151feccf4c58c6b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 1: Overview of foundation models in RL (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: '**1-** Environment: Given that pre-trained foundational models understand the
    causal relationships between events, they can be utilized to forecast environmental
    changes resulting from current actions. Although this concept is intriguing, we’re
    not yet aware of any specific studies that focus on it. There are two primary
    reasons holding us back from exploring this idea further for now.'
  prefs: []
  type: TYPE_NORMAL
- en: While the reinforcement learning training process demands highly accurate predictions
    for the next step observations, pre-trained LLMs/VLMs haven’t been directly trained
    on datasets that enable such precise forecasting and thus fall short in this aspect.
    It’s important to note, as we highlighted in [our previous post](/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66),
    that a high-level planner, particularly one used in lifelong learning scenarios,
    could effectively incorporate a foundational model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latency in environment steps is a critical factor that can constrain the RL
    algorithm, especially when working within a fixed budget for training steps. The
    presence of a very large model that introduces significant latency can be quite
    restrictive. Note that while it might be challenging, distillation into a smaller
    network can be a solution here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2-** State (LLM/VLM Based State Generator): While experts often use the terms
    observation and state interchangeably, there are distinctions between them. A
    state is a comprehensive representation of the environment, while an observation
    may only provide partial information. In the standard RL framework, we don’t often
    discuss the specific transformations that extract and merge useful features from
    observations, past actions, and any internal knowledge of the environment to produce
    “state”, the policy input. Such a transformation could be significantly enhanced
    by employing LLMs/VLMs, which allow us to infuse the “state” with broader knowledge
    of the world, physics, and history (refer to Fig. 1, highlighted in pink).'
  prefs: []
  type: TYPE_NORMAL
- en: '**3-** Policy (Foundational Policy Model): Integrating foundational models
    into the policy, the central decision-making component in RL, can be highly beneficial.
    Although employing such models to generate high-level plans has proven successful,
    transforming the state into low-level actions has challenges we’ll delve into
    later. Fortunately, there has been some promising research in this area recently.'
  prefs: []
  type: TYPE_NORMAL
- en: '**4-** Reward (LLM/VLM Based Reward Generator): Leveraging foundational models
    to more accurately assess chosen actions within a trajectory has been a primary
    focus among researchers. This comes as no surprise, given that rewards have traditionally
    served as the communication channel between humans and agents, setting goals and
    guiding the agent towards what is desired.'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-trained foundational models come with a deep knowledge of the world, and
    injecting this kind of understanding into our decision-making processes can make
    those decisions more in tune with human desires and more likely to succeed. Moreover,
    using foundational models to evaluate the agent’s actions can quickly trim down
    the search space and equip the agent with a head start in understanding, as opposed
    to starting from scratch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-trained foundational models have been trained on internet-scale data generated
    mostly by humans, which has enabled them to understand worlds similarly to humans.
    This makes it possible to use foundational models as cost-effective annotators.
    They can generate labels or assess trajectories or rollouts on a large scale.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1- Foundational models in reward**'
  prefs: []
  type: TYPE_NORMAL
- en: It is challenging to use foundational models to generate low level control actions
    as low level actions are highly dependent on the setting of the agent and are
    underrepresented in foundational models’ training dataset. Hence, the foundation
    model application is generally focused on high level plans rather than low level
    actions. Reward bridges the gap between high-level planner and low level actions
    where foundation models can be used. Researchers have adopted various methodologies
    integrating foundation models for reward assignment. However, the core principle
    revolves around employing a VLM/LLM to effectively track the progress towards
    a subgoal or task.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.a Assigning reward values based on similarity**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the reward value as a signal that indicates whether the agent’s previous
    action was beneficial in moving towards the goal. A sensible method involves evaluating
    how closely the previous action aligns with the current objective. To put this
    approach into practice, as can be seen in Fig. 2, it’s essential to:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Generate meaningful embeddings of these actions, which can be done through
    images, videos, or text descriptions of the most recent observation.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Generate meaningful representations of the current objective.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Assess the similarity between these representations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22b45a375b7df3fefaee9eb893077dc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 2\. Reward values based on similarity (Image by author).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore the specific mechanics behind the leading research in this area.
  prefs: []
  type: TYPE_NORMAL
- en: Dense and well-shaped reward functions enhance the stability and training speed
    of the RL agent. Intrinsic rewards address this challenge by rewarding the agent
    for novel states’ exploration. However, in large environments where most of the
    unseen states are irrelevant to the downstream task, this approach becomes less
    effective. [ELLM](https://arxiv.org/pdf/2302.06692.pdf) uses background knowledge
    of LLM to shape the exploration. It queries LLM to generate a list of possible
    goals/subgoals given a list of the agent’s available actions and a text description
    of the agent current observation, generated by a state captioner. Then, at each
    time step, the reward is computed by the semantic similarity, cosine similarity,
    between the LLM generated goal and the description of the agent’s transition.
  prefs: []
  type: TYPE_NORMAL
- en: '[LiFT](https://arxiv.org/pdf/2312.08958.pdf) has a similar framework but also
    leverages [CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf)-style VLMs for reward
    assignment. [CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf) is pre-trained to
    align videos and corresponding language descriptions through contrastive learning.
    In [LiFT](https://arxiv.org/pdf/2312.08958.pdf), the agent is rewarded based on
    the alignment score, cosine similarity, between the task instructions and videos
    of the agent’s corresponding behavior, both encoded by [CLIP4CLIP](https://arxiv.org/pdf/2104.08860.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: '[UAFM](https://arxiv.org/pdf/2307.09668.pdf) has a similar framework where
    the main focus is on robotic manipulation tasks, e.g., stacking a set of objects.
    For reward assignment, they measure the similarity between the agent state image
    and the task description, both embedded by [CLIP](https://arxiv.org/pdf/2103.00020.pdf).
    They finetune [CLIP](https://arxiv.org/pdf/2103.00020.pdf) on a small amount of
    data from the simulated stacking domain to be more aligned in this use case.'
  prefs: []
  type: TYPE_NORMAL
- en: '**1.b Assigning rewards through reasoning on auxiliary tasks:**'
  prefs: []
  type: TYPE_NORMAL
- en: In scenarios where the foundational model has the proper understanding of the
    environment, it becomes feasible to directly pass the observations within a trajectory
    to the model, LLM/VLM. This evaluation can be done either through straightforward
    QA sessions based on the observations or by verifying the model’s capability in
    predicting the goal only by looking at the observation trajectory.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43e8a34aa903d5be475522d623219faf.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 3\. Assigning reward through reasoning (Image by author).
  prefs: []
  type: TYPE_NORMAL
- en: '[Read and Reward](https://arxiv.org/pdf/2302.04449.pdf) integrates the environment’s
    instruction manual into reward generation through two key components, as can be
    seen in Fig. 3:'
  prefs: []
  type: TYPE_NORMAL
- en: 'QA extraction module: it creates a summary of game objectives and features.
    This LLM-based module, [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf)-large,
    takes in the game manual and a question, and extracts the corresponding answer
    from the text. Questions are focused on the game objective, and agent-object interaction,
    identified by their significance using TF-IDF. For each critical object, a question
    as: “What happens when the player hits a <object>?” is added to the question set.
    A summary is then formed with the concatenation of all non-empty question-answer
    pairs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reasoning module: During gameplay, a rule-based algorithm detects “hit” events.
    Following each “hit” event, the LLM based reasoning module is queried with the
    summary of the environment and a question: “Should you hit a <object of interaction>
    if you want to win?” where the possible answer is limited to {yes, no}. A “yes”
    response adds a positive reward, while “no” leads to a negative reward.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[EAGER](https://arxiv.org/pdf/2206.09674.pdf) introduces a unique method for
    creating intrinsic rewards through a specially designed auxiliary task. This approach
    presents a novel concept where the auxiliary task involves predicting the goal
    based on the current observation. If the model predicts accurately, this indicates
    a strong alignment with the intended goal, and thus, a larger intrinsic reward
    is given based on the prediction confidence level. To achieve this goal, To accomplish
    this, two modules are employed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question Generation (QG): This component works by masking all nouns and adjectives
    in the detailed objective provided by the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Question Answering (QA): This is a model trained in a supervised manner, which
    takes the observation, question masks, and actions, and predicts the masked tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (P.S. Although this work does not utilize a foundational model, we’ve included
    it here due to its intriguing approach, which can be easily adapted to any pre-trained
    LLM)
  prefs: []
  type: TYPE_NORMAL
- en: '**1.c Generating reward function code**'
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, we’ve discussed generating reward values directly for the
    reinforcement learning algorithms. However, running a large model at every step
    of the RL loop can significantly slow down the speed of both training and inference.
    To bypass this bottleneck, one strategy involves utilizing our foundational model
    to generate the code for the reward function. This allows for the direct generation
    of reward values at each step, streamlining the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the code generation schema to work effectively, two key components are
    required:'
  prefs: []
  type: TYPE_NORMAL
- en: 1- A code generator, LLM, which receives a detailed prompt containing all the
    necessary information to craft the code.
  prefs: []
  type: TYPE_NORMAL
- en: 2- A refinement process that evaluates and enhances the code in collaboration
    with the code generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the key contributions for generating reward code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[R2R2S](https://arxiv.org/pdf/2306.08647.pdf) generates reward function code
    through two main components:'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM based motion descriptor: This module uses a pre-defined template to describe
    robot movements, and leverages Large Language Models (LLMs) to understand the
    motion. The Motion Descriptor fills in the template, replacing placeholders e.g.
    “Destination Point Coordinate” with specific details, to describe the desired
    robot motion within a pre-defined template.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LLM based reward coder: this component generates the reward function by processing
    a prompt containing: a motion description, a list of functions with their description
    that LLM can use to generate the reward function code, an example code of how
    the response should look like, and constraints and rules the reward function must
    follow.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Text2Reward](https://arxiv.org/pdf/2309.11489.pdf) develops a method to generate
    dense reward functions as an executable code within iterative refinement. Given
    the subgoal of the task, it has two key components:'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM-based reward coder: generates reward function code. Its prompt consists
    of: an abstract of observation and available actions, a compact pythonic style
    environment to represent the configuration of the objects, robot, and callable
    functions; a background knowledge for reward function design (e.g. “reward function
    for task X typically includes a term for the distance between object x and y”),
    and a few-shot examples. They assume access to a pool of instruction, and reward
    function pairs that top k similar instructions are retrieved as few-shot examples.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LLM-Based Refinement: once the reward code is generated, the code is executed
    to identify the syntax errors and runtime errors. These feedbacks are integrated
    into subsequent prompts to generate more refined reward functions. Additionally,
    human feedback is requested based on a task execution video by the current policy.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf) has a similar algorithm
    to [Text2Reward](https://arxiv.org/pdf/2309.11489.pdf), to generate the reward
    function code, see Fig. 4\. The main difference is in the refinement stage where
    it has two modules, both LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM-Based Reward Critic: It evaluates the code and provides feedback on whether
    the code is self-consistent and free of syntax and semantic errors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LLM-Based Trajectory Analyser: It reviews the historical information of the
    interaction between the trained agent and the environment and uses it to guide
    the modifications of the reward function.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/9f84ed9097ad74b7b0d223a4572abf62.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 4\. Overview of [Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf) (paper
    taken from [Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf) paper)
  prefs: []
  type: TYPE_NORMAL
- en: '[EUREKA](https://arxiv.org/pdf/2310.12931.pdf) generates reward code without
    the need for task-specific prompting, predefined reward templates, or predefined
    few-shot examples. To achieve this goal, it has two stages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM-based code generation: The raw environment code, the task, generic reward
    design and formatting tips are fed to the LLM as context and LLM returns the executable
    reward code with a list of its components.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Evolutionary search and refinement: At each iteration, [EUREKA](https://arxiv.org/pdf/2310.12931.pdf)
    queries the LLM to generate several i.i.d reward functions. Training an agent
    with executable reward functions provides feedback on how well the agent is performing.
    For a detailed and focused analysis of the rewards, the feedback also includes
    scalar values for each component of the reward function. The LLM takes top-performing
    reward code along with this detailed feedback to mutate the reward code in-context.
    In each subsequent iteration, the LLM uses the top reward code as a reference
    to generate K more i.i.d reward codes. This iterative optimization continues until
    a specified number of iterations has been reached.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within these two steps, [EUREKA](https://arxiv.org/pdf/2310.12931.pdf) is able
    to generate reward functions that outperform expert human-engineered rewards without
    any task specific templates.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.d. Train a reward model based on preferences (RLAIF)**'
  prefs: []
  type: TYPE_NORMAL
- en: An alternative method is to use a foundational model to generate data for training
    a reward function model. The significant successes of Reinforcement Learning with
    Human Feedback ([RLHF](https://arxiv.org/pdf/2203.02155.pdf)) have recently drawn
    increased attention towards employing trained reward functions on a larger scale.
    The heart of such algorithms is the use of a preference dataset to train a reward
    model which can subsequently be integrated into reinforcement learning algorithms.
    Given the high cost associated with generating preference data (e.g., action A
    is preferable to action B) through human feedback, there’s growing interest in
    constructing this dataset by obtaining feedback from an AI agent, i.e. VLM/LLM.
    Training a reward function, using AI-generated data and integrating it within
    a reinforcement learning algorithm, is known as Reinforcement Learning with AI
    Feedback (RLAIF).
  prefs: []
  type: TYPE_NORMAL
- en: '[MOTIF](https://arxiv.org/pdf/2310.00166.pdf) requires access to a passive
    dataset of observations with sufficient coverage. Initially, LLM is queried with
    a summary of desired behaviors within the environment and a text description of
    two randomly sampled observations. It then generates the preference, selecting
    between 1, 2, or 0 (indicating no preference), as seen in Fig. 5\. This process
    constructs a dataset of preferences between observation pairs. Subsequently, this
    dataset is used to train a reward model employing [preference based RL techniques](https://jmlr.org/papers/volume18/16-634/16-634.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80cc3f30ff6414b84db9a4b52ba2d4f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 5\. A schematic representation of the three phases of [MOTIF](https://arxiv.org/pdf/2310.00166.pdf)
    (image taken from [MOTIF](https://arxiv.org/pdf/2310.00166.pdf) paper)
  prefs: []
  type: TYPE_NORMAL
- en: '**2- Foundation models as Policy**'
  prefs: []
  type: TYPE_NORMAL
- en: Achieving the capability to train a foundational policy that not only excels
    in tasks previously encountered but also possesses the ability to reason about
    and adapt to new tasks using past learning, is an ambition within the RL community.
    Such a policy would ideally generalize from past experiences to tackle novel situations
    and, through environmental feedback, achieve goals previously unseen with human-like
    adaptability.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, several challenges stand in the way of training such agents. Among
    these challenges are:'
  prefs: []
  type: TYPE_NORMAL
- en: The necessity of managing a very large model, which introduces significant latency
    into the decision-making process for low-level control actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The requirement to collect a vast amount of interaction data across a wide array
    of tasks to enable effective learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, the process of training a very large network from scratch using
    RL introduces extra complexities. This is because backpropagation efficiency inherently
    is weaker in RL compared to supervised training methods .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up to now, it’s mostly been teams with substantial resources and top-notch setups
    who’ve really pushed the envelope in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: '[AdA](https://arxiv.org/pdf/2301.07608.pdf) paved the way for training an RL
    foundation model within the X.Land 2.0 3D environment. This model achieves human
    time-scale adaptation on held-out test tasks without any further training. The
    model’s success is founded on three ingredients:'
  prefs: []
  type: TYPE_NORMAL
- en: The core of the [AdA](https://arxiv.org/pdf/2301.07608.pdf)’s learning mechanism
    is a Transformer-XL architecture from 23 to 265 million parameters, employed alongside
    the [Muesli](https://arxiv.org/pdf/2104.06159.pdf) RL algorithm. Transformer-XL
    takes in a trajectory of observations, actions, and rewards from time t to T and
    outputs a sequence of hidden states for each time step. The hidden state is utilized
    to predict reward, value, and action distribution π. The combination of both long-term
    and short-term memory is critical for fast adaptation. Long-term memory is achieved
    through slow gradient updates, whereas short-term memory can be captured within
    the context length of the transformer. This unique combination allows the model
    to preserve knowledge across multiple task attempts by retaining memory across
    trials, even though the environment resets between trials.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model benefits from meta-RL training across 1⁰⁴⁰ different partially observable
    Markov decision processes (POMDPs) tasks. Since [transformers are meta-learners](https://arxiv.org/pdf/2206.06614.pdf),
    no additional meta step is required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given the size and diversity of the task pool, many tasks will either be too
    easy or too hard to generate a good training signal. To tackle this, they used
    an automated curriculum to prioritize tasks that are within its capability frontier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[RT-2](https://arxiv.org/pdf/2307.15818.pdf) introduces a method to co-finetune
    a VLM on both robotic trajectory data and vision-language tasks, resulting in
    a policy model called [RT-2](https://arxiv.org/pdf/2307.15818.pdf). To enable
    vision-language models to generate low-level actions, actions are discretized
    into 256 bins and represented as language tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: By representing actions as language tokens, [RT-2](https://arxiv.org/pdf/2307.15818.pdf)
    can directly utilize pre-existing VLM architectures without requiring substantial
    modifications. Hence, VLM input includes robot camera image and textual task description
    formatted similarly to Vision Question Answering tasks and the output is a series
    of language tokens that represent the robot’s low-level actions; see Fig. 6.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94312d770f343bd4684840e4a373dbc6.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 6\. [RT-2](https://arxiv.org/pdf/2307.15818.pdf) overview (image taken from
    [RT-2](https://arxiv.org/pdf/2307.15818.pdf) paper)
  prefs: []
  type: TYPE_NORMAL
- en: They noticed that co-finetuning on both types of data with the original web
    data leads to more generalizable policies. The co-finetuning process equips [RT-2](https://arxiv.org/pdf/2307.15818.pdf)
    with the ability to understand and execute commands that were not explicitly present
    in its training data, showcasing remarkable adaptability. This approach enabled
    them to leverage internet-scale pretraining of VLM to generalize to novel tasks
    through semantic reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: '**3- Foundation Models as State Representation**'
  prefs: []
  type: TYPE_NORMAL
- en: In RL, a policy’s understanding of the environment at any given moment comes
    from its “state” which is essentially how it perceives its surroundings. Looking
    at the RL block diagram, a reasonable module to inject world knowledge into is
    the state. If we can enrich observations with general knowledge useful for completing
    tasks, the policy can pick up new tasks much faster compared to RL agents that
    begin learning from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PR2L](https://arxiv.org/pdf/2402.02651.pdf) introduces a novel approach to
    inject the background knowledge of VLMs from internet scale data into RL.[PR2L](https://arxiv.org/pdf/2402.02651.pdf)
    employs generative VLMs which generate language in response to an image and a
    text input. As VLMs are proficient in understanding and responding to visual and
    textual inputs, they can provide a rich source of semantic features from observations
    to be linked to actions.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PR2L](https://arxiv.org/pdf/2402.02651.pdf), queries a VLM with a task-relevant
    prompt for each visual observation received by the agent, and receives both the
    generated textual response and the model’s intermediate representations. They
    discard the text and use some or all of the models intermediate representation
    generated for both the visual and text input and the VLM’s generated textual response
    as “promptable representations”. Due to the variable size of these representations,
    [PR2L](https://arxiv.org/pdf/2402.02651.pdf) incorporates an encoder-decoder Transformer
    layer to embed all the information embedded in promptable representations into
    a fixed size embedding. This embedding, combined with any available non-visual
    observation data, is then provided to the policy network, representing the state
    of the agent. This innovative integration allows the RL agent to leverage the
    rich semantic understanding and background knowledge of VLMs, facilitating more
    rapid and informed learning of tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Also Read Our Previous Post:** [Towards AGI: LLMs and Foundational Models’
    Roles in the Lifelong Learning Revolution](/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**References:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] [ELLM](https://arxiv.org/pdf/2302.06692.pdf): Du, Yuqing, et al. “Guiding
    pretraining in reinforcement learning with large language models.” 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Text2Reward](https://arxiv.org/pdf/2309.11489.pdf): Xie, Tianbao, et al.
    “Text2reward: Automated dense reward function generation for reinforcement learning.”
    2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [R2R2S](https://arxiv.org/pdf/2306.08647.pdf): Yu, Wenhao, et al. “Language
    to rewards for robotic skill synthesis.” 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [EUREKA](https://arxiv.org/pdf/2310.12931.pdf): Ma, Yecheng Jason, et al.
    “Eureka: Human-level reward design via coding large language models.” 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [MOTIF](https://arxiv.org/pdf/2310.00166.pdf): Klissarov, Martin, et al.
    “Motif: Intrinsic motivation from artificial intelligence feedback.” 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] [Read and Reward](https://arxiv.org/pdf/2302.04449.pdf): Wu, Yue, et al.
    “Read and reap the rewards: Learning to play atari with the help of instruction
    manuals.” 2024.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] [Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf): Li, Hao, et al.
    “Auto MC-reward: Automated dense reward design with large language models for
    minecraft.” 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] [EAGER](https://arxiv.org/pdf/2206.09674.pdf): Carta, Thomas, et al. “Eager:
    Asking and answering questions for automatic reward shaping in language-guided
    RL.” 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] [LiFT](https://arxiv.org/pdf/2312.08958.pdf): Nam, Taewook, et al. “LiFT:
    Unsupervised Reinforcement Learning with Foundation Models as Teachers.” 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] [UAFM](https://arxiv.org/pdf/2307.09668.pdf): Di Palo, Norman, et al.
    “Towards a unified agent with foundation models.” 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] [RT-2](https://arxiv.org/pdf/2307.15818.pdf): Brohan, Anthony, et al.
    “Rt-2: Vision-language-action models transfer web knowledge to robotic control.”
    2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] [AdA](https://arxiv.org/pdf/2301.07608.pdf): Team, Adaptive Agent, et
    al. “Human-timescale adaptation in an open-ended task space.” 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] [PR2L](https://arxiv.org/pdf/2402.02651.pdf): Chen, William, et al. “Vision-Language
    Models Provide Promptable Representations for Reinforcement Learning.” 2024.'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] [Clip4Clip](https://arxiv.org/pdf/2104.08860.pdf): Luo, Huaishao, et al.
    “Clip4clip: An empirical study of clip for end to end video clip retrieval and
    captioning.” 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: '[15] [Clip](https://arxiv.org/pdf/2103.00020.pdf): Radford, Alec, et al. “Learning
    transferable visual models from natural language supervision.” 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '[16] [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf): Liu, Yinhan, et al. “Roberta:
    A robustly optimized bert pretraining approach.” 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: '[17] [Preference based RL](https://jmlr.org/papers/volume18/16-634/16-634.pdf):
    SWirth, Christian, et al. “A survey of preference-based reinforcement learning
    methods.” 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[18] [Muesli](https://arxiv.org/pdf/2104.06159.pdf): Hessel, Matteo, et al.
    “Muesli: Combining improvements in policy optimization.” 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '[19] Melo, Luckeciano C. “[Transformers are meta-reinforcement learners](https://arxiv.org/pdf/2206.06614.pdf).”
    2022.'
  prefs: []
  type: TYPE_NORMAL
- en: '[20] [RLHF](https://arxiv.org/pdf/2203.02155.pdf): Ouyang, Long, et al. “Training
    language models to follow instructions with human feedback, 2022.'
  prefs: []
  type: TYPE_NORMAL
