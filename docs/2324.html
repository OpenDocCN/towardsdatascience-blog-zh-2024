<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Evaluate RAG If You Don’t Have Ground Truth Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Evaluate RAG If You Don’t Have Ground Truth Data</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-evaluate-rag-if-you-dont-have-ground-truth-data-590697061d89?source=collection_archive---------0-----------------------#2024-09-24">https://towardsdatascience.com/how-to-evaluate-rag-if-you-dont-have-ground-truth-data-590697061d89?source=collection_archive---------0-----------------------#2024-09-24</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="519a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Vector similarity search threshold, synthetic data generation, LLM-as-a-judge, and frameworks</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@jenn-j-dev?source=post_page---byline--590697061d89--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jenn J." class="l ep by dd de cx" src="../Images/d2ef3b8f454d4f7a974edd5a965a80e8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*WlHipDuero5XpQkoxWyZYw.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--590697061d89--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@jenn-j-dev?source=post_page---byline--590697061d89--------------------------------" rel="noopener follow">Jenn J.</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--590697061d89--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 24, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="05fe" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Evaluating a Retrieval-Augmented Generation (RAG) model is much easier when you have ground truth data against which to compare. But what if you don’t? That’s where things get a bit trickier. However, even in the absence of ground truth, there are still ways to assess how well your RAG system is performing. Below, we’ll walk through three effective strategies, ways to create a ground truth dataset from scratch, metrics you can use to evaluate when you do have a dataset, and existing frameworks that can help you with this process.</p><h2 id="4fbd" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Two types of RAG evaluations: Retrieval evaluation and Generation evaluation</h2><p id="bfce" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">Each strategy below will be tagged as either retrieval evaluation, generation evaluation, or both.</p><h1 id="18ce" class="of ng fq bf nh og oh gq nl oi oj gt np ok ol om on oo op oq or os ot ou ov ow bk">How to evaluate RAG if you don’t have ground truth data?</h1><h2 id="2d31" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Vector Similarity Search Threshold</h2><p id="f72d" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">Type: Retrieval Evaluation</p><p id="c53f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you’re working with a vector database like Pinecone, you’re probably familiar with the idea of vector similarity. Essentially, the database retrieves information based on how close the vectors of your query are to the vectors of potential results. Even without a “correct” answer to measure against, you can still lean on metrics like cosine similarity to gauge the quality of the retrieved documents.</p><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy oz"><img src="../Images/1bdcbb4fb6eeffebe1bf091142c9c3f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5P502x7UmnvJcCwJbiBoGA.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Cosine Distance. [Image provided by the author]</figcaption></figure><p id="9230" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For example, Pinecone will return cosine similarity values that show how close each result is to your query.</p><pre class="pa pb pc pd pe pq pr ps bp pt bb bk"><span id="149a" class="pu ng fq pr b bg pv pw l px py"># Create a pinecone (vector database) index that uses cosine similarity<br/>pc.create_index(<br/>    name=index_name,<br/>    dimension=2,<br/>    metric="cosine",<br/>    spec=ServerlessSpec(<br/>        cloud='aws', <br/>        region='us-east-1'<br/>    ) <br/>) <br/><br/># Retrieving top 3 closest vectors<br/>query_results = index.query(<br/>    namespace="example-namespace1",<br/>    vector=[1.0, 1.5],<br/>    top_k=3,<br/>    include_values=True<br/>)<br/><br/># query_results<br/># The "score" here is the cosine similarity value<br/># {'matches': [{'id': 'vec1', 'score': 1.0, 'values': [1.0, 1.5]},<br/># {'id': 'vec2', 'score': 0.868243158, 'values': [2.0, 1.0]},<br/># {'id': 'vec3', 'score': 0.850068152, 'values': [0.1, 3.0]}],</span></pre><p id="641e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">By exposing the similarity score, you can set a passing or failing grade on retrieved documents. A higher threshold (like 0.8 or above) means having a stricter requirement, while a lower threshold will bring in more data, which could be helpful or just noisy.</p><p id="39c6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This process isn’t about finding a perfect number right away — it’s about trial and error. We’ll know if we’ve hit the sweet spot when the results consistently feel useful for our specific application.</p><h2 id="1c3e" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Using Multiple LLMs to Judge Responses</h2><p id="0b2a" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">Type: Retrieval + Generation Evaluation</p><p id="3d37" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Another creative way to evaluate your RAG system is by <a class="af pz" href="https://arxiv.org/html/2402.14860v2" rel="noopener ugc nofollow" target="_blank">leveraging multiple LLMs to judge responses</a>. Even though LLMs can’t provide a perfect answer when ground truth data is missing, you can still use their feedback to compare the quality of the responses.</p><p id="2ed7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">By comparing responses across different LLMs and seeing how they rank them, you can gauge the overall quality of the retrievals and generations. It’s not perfect, but it’s a creative way to get multiple perspectives on the quality of your system’s output.</p><h2 id="a121" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Human-in-the-Loop Feedback: Involving the Experts</h2><p id="25b5" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">Type: Retrieval + Generation Evaluation</p><p id="55bb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Sometimes, the best way to evaluate a system is the old-fashioned way — by asking humans for their judgment. Getting feedback from domain experts can provide insights that even the best models can’t match.</p><p id="43b4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Setting Up Rating Criteria</strong></p><p id="a96f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To make human feedback more reliable, it helps to establish clear and consistent rating criteria. You might ask your reviewers to rate things like:</p><ul class=""><li id="2ca5" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qa qb qc bk"><strong class="ml fr">Relevance:</strong> Does the retrieved information address the query? (Retrieval evaluation)</li><li id="28bc" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><strong class="ml fr">Correctness:</strong> Is the content factually accurate? (Retrieval evaluation)</li><li id="2905" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><strong class="ml fr">Fluency:</strong> Does it read well, or does it feel awkward or forced? (Generation evaluation)</li><li id="4a9d" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><strong class="ml fr">Completeness:</strong> Does it cover the question fully or leave gaps (Retrieval + Generation evaluation)</li></ul><p id="8047" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">With these criteria in place, you can get a more structured sense of how well your system is performing.</p><p id="db6f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Getting a Baseline</strong></p><p id="6ebb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One smart way to evaluate the quality of your human feedback is to check how well different reviewers agree with each other. You can use metrics like <a class="af pz" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">Pearson correlation</a> to see how closely their judgements align. If your reviewers disagree a lot, it might mean your criteria aren’t clear enough. It could also be a sign that the task is more subjective than you anticipated.</p><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qi"><img src="../Images/569d951f471766a129e8f86bdaf0b084.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mIvSqr_pIpZBo-F2_zzK2g.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Pearson correlation coefficient. [Image provided by author]</figcaption></figure><p id="87dc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Reducing Noise</strong></p><p id="774b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Human feedback can be noisy, especially if the criteria are unclear or the task is subjective. Here are a couple of ways to deal with that:</p><ul class=""><li id="9e44" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qa qb qc bk"><strong class="ml fr">Averaging the Scores</strong>: By averaging the ratings of multiple human reviewers, you can smooth out any individual biases or inconsistencies.</li><li id="a64c" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><strong class="ml fr">Focus on Agreement:</strong> Another approach is to only consider cases where your reviewers agree. This will give you a cleaner set of evaluations and help ensure the quality of your feedback.</li></ul></div></div></div><div class="ab cb qj qk ql qm" role="separator"><span class="qn by bm qo qp qq"/><span class="qn by bm qo qp qq"/><span class="qn by bm qo qp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="4f12" class="of ng fq bf nh og qr gq nl oi qs gt np ok qt om on oo qu oq or os qv ou ov ow bk">Creating a Ground Truth Dataset from Scratch</h1><p id="af28" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">When it comes to evaluating a RAG system without ground truth data, another approach is to create your own dataset. It sounds daunting, but there are several strategies to make this process easier, from finding similar datasets to leveraging human feedback and even synthetically generating data. Let’s break down how you can do it.</p><h2 id="fe02" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Finding Similar Datasets Online</h2><p id="90c1" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">This might seem obvious, and most people who have come to the conclusion that they don’t a have ground truth dataset have already exhausted this option. But it’s still worth mentioning that there might be datasets out there that are similar to what you need. Perhaps it’s in a different business domain from your use case but it’s in the question-answer format that you’re working with. Sites like Kaggle have a huge variety of public datasets, and you might be surprised at how many align with your problem space.</p><p id="f34d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Example:</p><ul class=""><li id="132f" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qa qb qc bk"><a class="af pz" href="https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset" rel="noopener ugc nofollow" target="_blank">Stanford Question Answering Dataset</a></li><li id="16e3" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><a class="af pz" href="https://www.kaggle.com/datasets/praneshmukhopadhyay/amazon-questionanswer-dataset" rel="noopener ugc nofollow" target="_blank">Amazon Question/Answer Dataset</a></li></ul><h2 id="2299" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Manually Creating Ground Truth Data</h2><p id="fd3f" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">If you can’t find exactly what you need online, you can always create ground truth data manually. This is where human-in-the-loop feedback comes in handy. Remember the domain expert feedback we talked about earlier? You can use that feedback to build your own mini-dataset.</p><p id="8dad" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">By curating a collection of human-reviewed examples — where the relevance, correctness, and completeness of the results have been validated — you create a foundation for expanding your dataset for evaluation.</p><p id="74bd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There is also a great article from Katherine Munro on an <a class="af pz" rel="noopener" target="_blank" href="/lessons-from-agile-experimental-chatbot-development-73ea515ba762">experimental approach to agile chatbot development</a>.</p><h2 id="a4a6" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Training an LLM as a Judge</h2><p id="c739" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">Once you have your minimal ground truth dataset, you can take things a step further by training an LLM to act as a judge and evaluate your model’s outputs.</p><p id="cd4c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But before relying on an LLM to act as a judge, we first need to ensure that it’s rating our model outputs accurately, or at least reliable. Here’s how you can approach that:</p><ol class=""><li id="a0c3" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qw qb qc bk"><strong class="ml fr">Build human-reviewed examples:</strong> Depending on your use case, 20 to 30 examples should be good enough to get a good sense of how reliable the LLM is in comparison. Refer to the previous section on best criteria to rate and how to measure conflicting ratings.</li><li id="6912" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qw qb qc bk"><strong class="ml fr">Create Your LLM Judge:</strong> Prompt an LLM to give ratings based on the same criteria that you handed to your domain experts. Take the rating and compare how the LLM’s ratings align with the human ratings. Again, you can use metrics like Pearson metrics to help evaluate. A high correlation score will indicate that the LLM is performing as well as a judge.</li><li id="40f0" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qw qb qc bk"><strong class="ml fr">Apply </strong><a class="af pz" href="https://www.bighummingbird.com/blogs/prompt-engineering-best-practices" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr">prompt engineering best practices</strong></a>: Prompt engineering can make or break this process. Techniques like pre-warming the LLM with context or providing a few examples (few-shot learning) can dramatically improve the models’ accuracy when judging.</li></ol><h1 id="ab17" class="of ng fq bf nh og oh gq nl oi oj gt np ok ol om on oo op oq or os ot ou ov ow bk">Creating Specific Datasets</h1><p id="eb8d" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">Another way to boost the quality and quantity of your ground truth datasets is by segmenting your documents into topics or semantic groupings. Instead of looking at entire documents as a whole, break them down into smaller, more focused segments.</p><p id="871e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For example, let’s say you have a document (documentId: 123) that mentions:</p><p id="7978" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="qx">“After launching product ABC, company XYZ saw a 10% increase in revenue for 2024 Q1.”</em></p><p id="d242" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This one sentence contains two distinct pieces of information:</p><ol class=""><li id="c367" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qw qb qc bk"><em class="qx">Launching product ABC</em></li><li id="5498" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qw qb qc bk"><em class="qx">A 10% increase in revenue for 2024 Q1</em></li></ol><p id="7502" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, you can augment each topic into its own query and context. For example:</p><ul class=""><li id="e2ca" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qa qb qc bk"><strong class="ml fr">Query 1:</strong> <em class="qx">“What product did company XYZ launch?”</em></li><li id="cd30" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><strong class="ml fr">Context 1:</strong> <em class="qx">“Launching product ABC”</em></li><li id="8d5c" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><strong class="ml fr">Query 2:</strong> <em class="qx">“What was the change in revenue for Q1 2024?”</em></li><li id="05ea" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><strong class="ml fr">Context 2:</strong> <em class="qx">“Company XYZ saw a 10% increase in revenue for Q1 2024”</em></li></ul><p id="ee7a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">By breaking the data into specific topics like this, you not only create more data points for training but also make your dataset more precise and focused. Plus, if you want to trace each query back to the original document for reliability, you can easily add metadata to each context segment. For instance:</p><ul class=""><li id="f9c1" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qa qb qc bk"><strong class="ml fr">Query 1:</strong> <em class="qx">“What product did company XYZ launch?”</em></li><li id="70d6" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><strong class="ml fr">Context 1:</strong> <em class="qx">“Launching product ABC (documentId: 123)”</em></li><li id="9bc1" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><strong class="ml fr">Query 2:</strong> “What was the change in revenue for Q1 2024?”</li><li id="88d9" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><strong class="ml fr">Context 2:</strong> “Company XYZ saw a 10% increase in revenue for Q1 2024 (documentId: 123)”</li></ul><p id="c3cd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This way, each segment is tied back to its source, making your dataset even more useful for evaluation and training.</p><h1 id="c59c" class="of ng fq bf nh og oh gq nl oi oj gt np ok ol om on oo op oq or os ot ou ov ow bk">Synthetically Creating a Dataset</h1><p id="310b" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">If all else fails, or if you need more data than you can gather manually, synthetic data generation can be a game-changer. Using techniques like data augmentation or even GPT models, you can create new data points based on your existing examples. For instance, you can take a base set of queries and contexts and tweak them slightly to create variations.</p><p id="f92d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For example, starting with the query:</p><ul class=""><li id="adf6" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qa qb qc bk"><em class="qx">“What product did company XYZ launch?”</em></li></ul><p id="feaa" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You could synthetically generate variations like:</p><ul class=""><li id="b469" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qa qb qc bk"><em class="qx">“Which product was introduced by company XYZ?”</em></li><li id="1ff3" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><em class="qx">“What was the name of the product launched by company XYZ?”</em></li></ul><p id="ba19" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This can help you build a much larger dataset without the manual overhead of writing new examples from scratch.</p><p id="03b5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are also frameworks that can automate the process of generating synthetic data for you that we’ll explore in the last section.</p><h1 id="bd7f" class="of ng fq bf nh og oh gq nl oi oj gt np ok ol om on oo op oq or os ot ou ov ow bk">Once You Have a Dataset: Time to Evaluate</h1><p id="4dcc" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">Now that you’ve gathered or created your dataset, it’s time to dive into the evaluation phase. RAG model involves two key areas: retrieval and generation. Both are important and understanding how to assess each will help you fine-tune your model to better meet your needs.</p><h2 id="f405" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Evaluating Retrieval: How Relevant is the Retrieved Data?</h2><p id="10fa" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">The retrieval step in RAG is crucial — if your model can’t pull the right information, it’s going to struggle with generating accurate responses. Here are two key metrics you’ll want to focus on:</p><ul class=""><li id="3b05" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qa qb qc bk"><strong class="ml fr">Context Relevancy:</strong> This measures how well the retrieved context aligns with the query. Essentially, you’re asking: <em class="qx">Is this information actually relevant to the question being asked?</em> You can use your dataset to calculate relevance scores, either by human judgment or by comparing similarity metrics (like cosine similarity) between the query and the retrieved document.</li><li id="7e34" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><strong class="ml fr">Context Recall:</strong> Context recall focuses on how much relevant information was retrieved. It’s possible that the right document was pulled, but only part of the necessary information was included. To evaluate recall, you need to check whether the context your model pulled contains all the key pieces of information to fully answer the query. Ideally, you want high recall: your retrieval should grab the information you need and nothing critical should be left behind.</li></ul><h2 id="1f13" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Evaluating Generation: Is the Response Both Accurate and Useful?</h2><p id="c239" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">Once the right information is retrieved, the next step is generating a response that not only answers the query but does so faithfully and clearly. Here are two critical aspects to evaluate:</p><ul class=""><li id="0219" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qa qb qc bk"><strong class="ml fr">Faithfulness:</strong> This measures whether the generated response accurately reflects the retrieved context. Essentially, you want to avoid hallucinations — where the model makes up information that wasn’t in the retrieved data. Faithfulness is about ensuring that the answer is grounded in the facts presented by the documents your model retrieved.</li><li id="417f" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><strong class="ml fr">Answer Relevancy:</strong> This refers to how well the generated answer matches the query. Even if the information is faithful to the retrieved context, it still needs to be relevant to the question being asked. You don’t want your model to pull out correct information that doesn’t quite answer the user’s question.</li></ul><h2 id="7af8" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Doing a Weighted Evaluation</h2><p id="50f4" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">Once you’ve assessed both retrieval and generation, you can go a step further by combining these evaluations in a weighted way. Maybe you care more about relevancy than recall, or perhaps faithfulness is your top priority. You can assign different weights to each metric depending on your specific use case.</p><p id="4fdf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For example:</p><ul class=""><li id="9462" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qa qb qc bk"><strong class="ml fr">Retrieval: </strong>60% context relevancy + 40% context recall</li><li id="cd1d" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><strong class="ml fr">Generation: </strong>70% faithfulness + 30% answer relevancy</li></ul><p id="a8ca" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This kind of weighted evaluation gives you flexibility in prioritizing what matters most for your application. If your model needs to be 100% factually accurate (like in legal or medical contexts), you may put more weight on faithfulness. On the other hand, if completeness is more important, you might focus on recall.</p><h1 id="b3ea" class="of ng fq bf nh og oh gq nl oi oj gt np ok ol om on oo op oq or os ot ou ov ow bk">Existing Frameworks to Simplify Your Evaluation Process</h1><p id="fe8d" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">If creating your own evaluation system feels overwhelming, don’t worry — there are some great existing frameworks that have already done a lot of the heavy lifting for you. These frameworks come with built-in metrics designed specifically to evaluate RAG systems, making it easier to assess retrieval and generation performance. Let’s look at a few of the most helpful ones.</p><h2 id="173d" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk"><a class="af pz" href="https://docs.ragas.io/en/stable/" rel="noopener ugc nofollow" target="_blank">RAGAS (Retrieval-Augmented Generation Assessment)</a></h2><p id="3b8f" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">RAGAS is a purpose-built framework designed to assess the performance of RAG models. It includes metrics that evaluate both retrieval and generation, offering a comprehensive way to measure how well your system is doing at each step. It also offers synthetic test data generation by employing an evolutionary generation paradigm.</p><blockquote class="qy qz ra"><p id="375d" class="mj mk qx ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">Inspired by works like </em><a class="af pz" href="https://arxiv.org/abs/2304.12244" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr"><em class="fq">Evol-Instruct</em></strong></a><em class="fq">, Ragas achieves this by employing an evolutionary generation paradigm, where questions with different characteristics such as reasoning, conditioning, multi-context, and more are systematically crafted from the provided set of documents. — RAGAS documentation</em></p></blockquote><h2 id="19bc" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk"><a class="af pz" href="https://github.com/stanford-futuredata/ARES" rel="noopener ugc nofollow" target="_blank">ARES: Open-Source Framework Using Synthetic Data and LLM Judge</a></h2><p id="18f9" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk"><strong class="ml fr">ARES</strong> is another powerful tool that combines synthetic data generation with LLM-based evaluation. ARES uses synthetic data — data generated by AI models rather than collected from real-world interactions — to build a dataset that can be used to test and refine your RAG system.</p><p id="87dd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The framework also includes an LLM judge, which, as we discussed earlier, can help evaluate model outputs by comparing them to human annotations or other reference data.</p><h1 id="c56e" class="of ng fq bf nh og oh gq nl oi oj gt np ok ol om on oo op oq or os ot ou ov ow bk">Conclusion</h1><p id="897f" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">Even without ground truth data, these strategies can help you effectively evaluate a RAG system. Whether you’re using vector similarity thresholds, multiple LLMs, LLM-as-a-judge, retrieval metrics, or frameworks, each approach gives you a way to measure performance and improve your model’s results. The key is finding what works best for your specific needs — and not being afraid to tweak things along the way. 🙂</p></div></div></div><div class="ab cb qj qk ql qm" role="separator"><span class="qn by bm qo qp qq"/><span class="qn by bm qo qp qq"/><span class="qn by bm qo qp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e665" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="qx">Join the conversation! </em><a class="af pz" href="https://medium.com/@jenn-j-dev/subscribe" rel="noopener"><em class="qx">Subscribe</em></a><em class="qx"> for practical AI tips, real-world use cases, and behind-the-scenes insights as I build in public.</em></p><p id="cf7d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="qx">Curious to learn more about LLMs? Check out our </em><a class="af pz" href="https://www.bighummingbird.com/blogs" rel="noopener ugc nofollow" target="_blank"><em class="qx">AI-in-Action blog</em></a><em class="qx"> on AI agents, prompt engineering, and LLMOps.</em></p></div></div></div></div>    
</body>
</html>