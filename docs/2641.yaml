- en: Running Large Language Models Privately
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 私人运行大型语言模型
- en: 原文：[https://towardsdatascience.com/running-large-language-models-privately-a-comparison-of-frameworks-models-and-costs-ac33cfe3a462?source=collection_archive---------0-----------------------#2024-10-30](https://towardsdatascience.com/running-large-language-models-privately-a-comparison-of-frameworks-models-and-costs-ac33cfe3a462?source=collection_archive---------0-----------------------#2024-10-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/running-large-language-models-privately-a-comparison-of-frameworks-models-and-costs-ac33cfe3a462?source=collection_archive---------0-----------------------#2024-10-30](https://towardsdatascience.com/running-large-language-models-privately-a-comparison-of-frameworks-models-and-costs-ac33cfe3a462?source=collection_archive---------0-----------------------#2024-10-30)
- en: A comparison of frameworks, models, and costs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 框架、模型与成本比较
- en: '[](https://medium.com/@robert.corwin?source=post_page---byline--ac33cfe3a462--------------------------------)[![Robert
    Corwin](../Images/ea7f47832ae17969ba4682c5b59129aa.png)](https://medium.com/@robert.corwin?source=post_page---byline--ac33cfe3a462--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ac33cfe3a462--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ac33cfe3a462--------------------------------)
    [Robert Corwin](https://medium.com/@robert.corwin?source=post_page---byline--ac33cfe3a462--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@robert.corwin?source=post_page---byline--ac33cfe3a462--------------------------------)[![Robert
    Corwin](../Images/ea7f47832ae17969ba4682c5b59129aa.png)](https://medium.com/@robert.corwin?source=post_page---byline--ac33cfe3a462--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ac33cfe3a462--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ac33cfe3a462--------------------------------)
    [Robert Corwin](https://medium.com/@robert.corwin?source=post_page---byline--ac33cfe3a462--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ac33cfe3a462--------------------------------)
    ·15 min read·Oct 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ac33cfe3a462--------------------------------)
    ·15分钟阅读·2024年10月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Robert Corwin, CEO, Austin Artificial Intelligence
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Robert Corwin，首席执行官，奥斯汀人工智能公司
- en: David Davalos, ML Engineer, Austin Artificial Intelligence
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: David Davalos，机器学习工程师，奥斯汀人工智能公司
- en: Oct 24, 2024
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年10月24日
- en: '*Large Language Models (LLMs) have rapidly transformed the technology landscape,
    but security concerns persist, especially with regard to sending private data
    to external third parties. In this blog entry, we dive into the options for deploying
    Llama models locally and privately, that is, on one’s own computer. We get Llama
    3.1 running locally and investigate key aspects such as speed, power consumption,
    and overall performance across different versions and frameworks. Whether you’re
    a technical expert or simply curious about what’s involved, you’ll find insights
    into local LLM deployment. For a quick overview, non-technical readers can skip
    to our summary tables, while those with a technical background may appreciate
    the deeper look into specific tools and their performance.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*大型语言模型（LLM）迅速改变了技术格局，但安全问题依然存在，尤其是将私人数据发送给外部第三方。在这篇博客文章中，我们深入探讨了将Llama模型本地和私人部署的选项，也就是说，在个人计算机上运行。我们成功在本地运行了Llama
    3.1，并调查了不同版本和框架下的速度、功耗以及整体性能等关键方面。无论你是技术专家，还是仅仅对相关内容感到好奇，你都会从本地LLM部署中获得一些见解。对于快速概览，不懂技术的读者可以跳过详细内容，查看总结表格，而具备技术背景的读者可能会更喜欢深入了解具体工具及其性能。*'
- en: '*All images by authors unless otherwise noted. The authors and Austin Artificial
    Intelligence, their employer, have no affiliations with any of the tools used
    or mentioned in this article.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图片均由作者提供。作者和奥斯汀人工智能公司（其雇主）与本文提到的任何工具或使用的工具没有任何关系。*'
- en: Key Points
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键点
- en: '**Running LLMs:** LLM models can be downloaded and run locally on private servers
    using tools and frameworks widely available in the community. While running the
    most powerful models require rather expensive hardware, smaller models can be
    run on a laptop or desktop computer.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**运行LLM：** LLM模型可以通过社区中广泛使用的工具和框架下载并在私人服务器上运行。虽然运行最强大的模型需要相当昂贵的硬件，但较小的模型可以在笔记本电脑或台式计算机上运行。'
- en: '**Privacy and Customizability:** Running LLMs on private servers provides enhanced
    privacy and greater control over model settings and usage policies.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐私与可定制性：** 在私人服务器上运行LLM可以提供更高的隐私保护，并对模型设置和使用政策拥有更大的控制权。'
- en: '**Model Sizes:** Open-source Llama models come in various sizes. For example,
    Llama 3.1 comes in 8 billion, 70 billion, and 405 billion parameter versions.
    A “parameter” is roughly defined as the weight on one node of the network. More
    parameters increase model performance at the expense of size in memory and disk.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型大小：** 开源Llama模型有多种尺寸。例如，Llama 3.1提供8亿、70亿和405亿参数版本。 “参数”大致定义为网络中某个节点上的权重。更多的参数会增加模型性能，但也会增加内存和磁盘的占用。'
- en: '**Quantization:** Quantization saves memory and disk space by essentially “rounding”
    weights to fewer significant digits — at the expense of accuracy. Given the vast
    number of parameters in LLMs, quantization is very valuable for reducing memory
    usage and speeding up execution.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**量化：** 量化通过将权重“舍入”到更少的有效数字，从而节省内存和磁盘空间——这以牺牲精度为代价。鉴于LLM中参数的庞大数量，量化对于减少内存使用和加速执行非常有价值。'
- en: '**Costs:** Local implementations, referencing GPU energy consumption, demonstrate
    cost-effectiveness compared to cloud-based solutions.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**成本：** 本地实现，通过参考GPU能耗，展示了与基于云的解决方案相比的成本效益。'
- en: Privacy and Reliability as Motivations
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐私和可靠性作为动机
- en: In one of our [previous entries](https://www.austinai.io/blog/the-revolutions-behind-chatgpt-and-llms)
    we explored the key concepts behind LLMs and how they can be used to create customized
    chatbots or tools with frameworks such as **Langchain** (see **Fig. 1**). In such
    schemes, while data can be protected by using synthetic data or obfuscation, we
    still must send data externally a third party and have no control over any changes
    in the model, its policies, or even its availability. A solution is simply to
    run an LLM on a private server (see **Fig. 2**). This approach ensures full privacy
    and mitigates the dependency on external service providers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的[文章](https://www.austinai.io/blog/the-revolutions-behind-chatgpt-and-llms)中，我们探讨了LLM背后的关键概念，以及如何利用**Langchain**等框架创建定制化的聊天机器人或工具（见**图1**）。在这样的方案中，虽然可以通过使用合成数据或混淆来保护数据，但我们仍然需要将数据发送给第三方，而且无法控制模型的任何变化、政策或甚至可用性。一个解决方案是直接在私有服务器上运行LLM（见**图2**）。这种方式可以确保完全的隐私，并减少对外部服务提供商的依赖。
- en: Concerns about implementing LLMs privately include costs, power consumption,
    and speed. In this exercise, we get LLama 3.1 running while varying the 1\. framework
    (tools) and 2\. degrees of **quantization** and compare the ease of use of the
    frameworks, the resultant performance in terms of speed, and power consumption.
    Understanding these trade-offs is essential for anyone looking to harness the
    full potential of AI while retaining control over their data and resources.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 实现私有化LLM时的关注点包括成本、电力消耗和速度。在本次实验中，我们通过改变1.框架（工具）和2.量化程度，运行LLama 3.1，并比较框架的易用性、运行时的速度表现以及电力消耗。理解这些权衡对任何希望在保持数据和资源控制的同时，充分发挥AI潜力的人来说都至关重要。
- en: '![](../Images/4086a3837bbbe117393bc836e2bf9bb6.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4086a3837bbbe117393bc836e2bf9bb6.png)'
- en: '**Fig. 1** Diagram illustrating a typical backend setup for chatbots or tools,
    with ChatGPT (or similar models) functioning as the natural language processing
    engine. This setup relies on prompt engineering to customize responses.”'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1** 展示一个典型的后端设置示意图，用于聊天机器人或工具，其中ChatGPT（或类似模型）作为自然语言处理引擎运行。此设置依赖于提示工程来自定义响应。'
- en: '![](../Images/b7f1e437550e19bfc9ca68a0e1549779.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7f1e437550e19bfc9ca68a0e1549779.png)'
- en: '**Fig. 2** Diagram of a fully private backend configuration where all components,
    including the large language model, are hosted on a secure server, ensuring complete
    control and privacy.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2** 完全私有的后端配置示意图，所有组件，包括大型语言模型，均托管在安全服务器上，从而确保完全的控制和隐私。'
- en: Quantization and GGUF Files
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化与GGUF文件
- en: Before diving into our impressions of the tools we explored, let’s first discuss
    quantization and the *GGUF* format.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨我们所探索的工具之前，首先让我们讨论一下量化和*GGUF*格式。
- en: Quantization is a technique used to reduce the size of a model by converting
    weights and biases from high-precision floating-point values to lower-precision
    representations. LLMs benefit greatly from this approach, given their vast number
    of parameters. For example, the largest version of Llama 3.1 contains a staggering
    405 billion parameters. Quantization can significantly reduce both memory usage
    and execution time, making these models more efficient to run across a variety
    of devices. For an in-depth explanation and nomenclature of quantization types,
    check out this [great introduction](https://huggingface.co/docs/hub/en/gguf).
    A conceptual overview can also be found [here](https://www.maartengrootendorst.com/blog/quantization/).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种通过将权重和偏差从高精度浮动点值转换为低精度表示来减少模型大小的技术。考虑到LLM拥有庞大的参数量，这种方法对它们特别有利。例如，Llama
    3.1的最大版本包含惊人的4050亿个参数。量化可以显著减少内存使用和执行时间，使这些模型在各种设备上运行时更高效。有关量化类型的深入解释和命名法，请查看这个[很好的介绍](https://huggingface.co/docs/hub/en/gguf)。概念性概述也可以在[这里](https://www.maartengrootendorst.com/blog/quantization/)找到。
- en: The [*GGUF* format](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
    is used to store LLM models and has recently gained popularity for distributing
    and running quantized models. It is optimized for fast loading, reading, and saving.
    Unlike tensor-only formats, GGUF also stores model metadata in a standardized
    manner, making it easier for frameworks to support this format or even adopt it
    as the norm.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[*GGUF*格式](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)用于存储LLM模型，并且最近因分发和运行量化模型而受到广泛欢迎。它经过优化，能够快速加载、读取和保存。与仅存储张量的格式不同，GGUF还以标准化方式存储模型元数据，使得框架更容易支持此格式，甚至将其作为标准格式采用。'
- en: Tools and Models Analyzed
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析的工具和模型
- en: 'We explored four tools to run Llama models locally:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了四个工具来本地运行Llama模型：
- en: '[HuggingFace’s transformers library](https://huggingface.co/docs/transformers/en/index)
    and [Hub](https://huggingface.co/docs/hub/en/index)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingFace的transformers库](https://huggingface.co/docs/transformers/en/index)和[Hub](https://huggingface.co/docs/hub/en/index)'
- en: '[vLLM](https://docs.vllm.ai/en/latest/)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[vLLM](https://docs.vllm.ai/en/latest/)'
- en: '[llama.cpp](https://github.com/ggerganov/llama.cpp)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[llama.cpp](https://github.com/ggerganov/llama.cpp)'
- en: '[Ollama](https://ollama.com/)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ollama](https://ollama.com/)'
- en: Our primary focus was on llama.cpp and Ollama, as these tools allowed us to
    deploy models quickly and efficiently right out of the box. Specifically, we explored
    their speed, energy cost, and overall performance. For the models, we primarily
    analyzed the quantized 8B and 70B Llama 3.1 versions, as they ran within a reasonable
    time frame.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要关注点是llama.cpp和Ollama，因为这些工具允许我们直接、迅速且高效地部署模型。具体来说，我们探索了它们的速度、能源成本和整体性能。对于模型，我们主要分析了量化后的8B和70B版本的Llama
    3.1，因为它们在合理的时间范围内运行。
- en: First Impressions and Installation
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初步印象与安装
- en: HuggingFace
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HuggingFace
- en: HuggingFace’s transformers library and Hub are well-known and widely used in
    the community. They offer a wide range of models and tools, making them a popular
    choice for many developers. Its installation generally does not cause major problems
    once a proper environment is set up with Python. At the end of the day, the biggest
    benefit of Huggingface was its online hub, which allows for easy access to quantized
    models from many different providers. On the other hand, using the transformers
    library directly to load models, especially quantized ones, was rather tricky.
    Out of the box, the library seemingly directly dequantizes models, taking a great
    amount of ram and making it unfeasible to run in a local server.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace的transformers库和Hub在社区中广为人知并被广泛使用。它们提供了多种模型和工具，使其成为许多开发者的热门选择。一旦环境配置好并安装了Python，它的安装通常不会导致大问题。最终，HuggingFace的最大优势在于它的在线Hub，允许轻松访问来自许多不同提供者的量化模型。另一方面，直接使用transformers库加载模型，尤其是量化模型，还是相当棘手的。默认情况下，该库似乎会直接对模型进行反量化，占用了大量的RAM，导致在本地服务器上运行时变得不可行。
- en: Although Hugging Face [supports 4- and 8-bit](https://huggingface.co/docs/transformers/en/main_classes/quantization)
    quantization and dequantization with [*bitsandbytes*](https://github.com/bitsandbytes-foundation/bitsandbytes),
    our initial impression is that further optimization is needed. Efficient inference
    may simply not be its primary focus. Nonetheless, Hugging Face offers excellent
    documentation, a large community, and a robust framework for model training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Hugging Face [支持 4 位和 8 位](https://huggingface.co/docs/transformers/en/main_classes/quantization)量化与去量化，并且使用
    [*bitsandbytes*](https://github.com/bitsandbytes-foundation/bitsandbytes)，我们的初步印象是，仍然需要进一步优化。高效推理可能并不是它的主要重点。尽管如此，Hugging
    Face 提供了出色的文档、庞大的社区以及强大的模型训练框架。
- en: vLLM
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: vLLM
- en: Similar to Hugging Face, vLLM is easy to install with a properly configured
    Python environment. However, support for GGUF files is still highly experimental.
    While we were able to quickly set it up to run 8B models, scaling beyond that
    proved challenging, despite the excellent documentation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Hugging Face，vLLM 可以在正确配置的 Python 环境中轻松安装。然而，对 GGUF 文件的支持仍然处于高度实验阶段。尽管我们能够快速设置运行
    8B 模型，但超出这个规模的扩展却证明是具有挑战性的，尽管有着出色的文档支持。
- en: Overall, we believe vLLM has great potential. However, we ultimately opted for
    the llama.cpp and Ollama frameworks for their more immediate compatibility and
    efficiency. To be fair, a more thorough investigation could have been conducted
    here, but given the immediate success we found with other libraries, we chose
    to focus on those.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们认为 vLLM 具有很大的潜力。然而，我们最终选择了 llama.cpp 和 Ollama 框架，因为它们在兼容性和效率上更为直接。公平地说，这里本可以进行更深入的调查，但考虑到我们在其他库中取得的即时成功，我们选择专注于它们。
- en: Ollama
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ollama
- en: We found Ollama to be fantastic. Our initial impression is that it is a user-ready
    tool for inferring Llama models locally, with an ease-of-use that works right
    out of the box. [Installing it](https://ollama.com/download) for Mac and Linux
    users is straightforward, and a Windows version is currently in preview. Ollama
    automatically detects your hardware and manages model offloading between CPU and
    GPU seamlessly. It features its own model library, automatically downloading models
    and supporting GGUF files. Although its speed is slightly slower than llama.cpp,
    it performs well even on CPU-only setups and laptops.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为 Ollama 非常棒。我们的初步印象是，它是一个适合用户本地推理 Llama 模型的工具，具有即插即用的易用性。[为 Mac 和 Linux
    用户安装它](https://ollama.com/download)非常简单，Windows 版本目前处于预览阶段。Ollama 会自动检测硬件，并在 CPU
    和 GPU 之间无缝管理模型卸载。它还具备自己的模型库，自动下载模型并支持 GGUF 文件。尽管其速度略逊于 llama.cpp，但即便在仅配有 CPU 的系统和笔记本电脑上也能良好运行。
- en: For a quick start, once installed, running `ollama run llama3.1:latest` will
    load the latest 8B model in conversation mode directly from the command line.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 快速入门：安装后，运行 `ollama run llama3.1:latest` 将直接从命令行加载最新的 8B 模型进行对话模式。
- en: One downside is that customizing models can be somewhat impractical, especially
    for advanced development. For instance, even adjusting the temperature requires
    creating a new chatbot instance, which in turn loads an installed model. While
    this is a minor inconvenience, it does facilitate the setup of customized chatbots
    — including other parameters and roles — within a single file. Overall, we believe
    Ollama serves as an effective local tool that mimics some of the key features
    of cloud services.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一个缺点是，定制模型在某些情况下可能有些不切实际，尤其是在高级开发中。例如，即使是调整温度，也需要创建一个新的聊天机器人实例，而这个实例又需要加载一个已安装的模型。虽然这只是一个小小的不便，但它确实有助于在一个文件中设置定制化的聊天机器人
    —— 包括其他参数和角色。总体而言，我们认为 Ollama 是一个有效的本地工具，模仿了云服务的一些关键功能。
- en: It is worth noting that Ollama runs as a service, at least on Linux machines,
    and offers handy, simple commands for monitoring which models are running and
    where they’re offloaded, with the ability to stop them instantly if needed. One
    challenge the community has faced is configuring certain aspects, such as where
    models are stored, which requires technical knowledge of Linux systems. While
    this may not pose a problem for end-users, it perhaps slightly hurts the tool’s
    practicality for advanced development purposes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，Ollama 作为服务运行，至少在 Linux 系统上，它提供了方便且简单的命令来监控当前运行的模型以及它们被卸载到何处，并且可以在需要时立即停止这些模型。社区面临的一个挑战是配置某些方面，例如模型存储位置，这需要具备一定的
    Linux 系统技术知识。虽然这对最终用户可能不会构成问题，但它可能会稍微影响该工具在高级开发中的实用性。
- en: llama.cpp
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: llama.cpp
- en: llama.cpp emerged as our favorite tool during this analysis. As stated in its
    [repository](https://github.com/ggerganov/llama.cpp/blob/master/README.md), it
    is designed for running inference on large language models with minimal setup
    and cutting-edge performance. Like Ollama, it supports offloading models between
    CPU and GPU, though this is not available straight out of the box. To enable GPU
    support, you must compile the tool with the appropriate flags — specifically,
    `GGML_CUDA=on`. We recommend using the latest version of the CUDA toolkit, as
    older versions may not be compatible.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次分析中，llama.cpp 成为了我们最喜爱的工具。正如它的 [仓库](https://github.com/ggerganov/llama.cpp/blob/master/README.md)
    中所述，它旨在通过最小的配置运行大规模语言模型，并提供领先的性能。像 Ollama 一样，它支持在 CPU 和 GPU 之间卸载模型，尽管这不是开箱即用的功能。要启用
    GPU 支持，您必须使用适当的标志进行编译——具体来说是 `GGML_CUDA=on`。我们建议使用最新版本的 CUDA 工具包，因为旧版本可能不兼容。
- en: The tool can be installed as a standalone by pulling from the repository and
    compiling, which provides a convenient command-line client for running models.
    For instance, you can execute `llama-cli -p 'you are a useful assistant' -m Meta-Llama-3-8B-Instruct.Q8_0.gguf
    -cnv`. Here, the final flag enables conversation mode directly from the command
    line. llama-cli offers various customization options, such as adjusting the context
    size, repetition penalty, and temperature, and it also supports GPU offloading
    options.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 该工具可以通过从仓库拉取并编译作为独立工具安装，这为运行模型提供了一个方便的命令行客户端。例如，您可以执行 `llama-cli -p 'you are
    a useful assistant' -m Meta-Llama-3-8B-Instruct.Q8_0.gguf -cnv`。这里，最后一个标志可以直接从命令行启用对话模式。llama-cli
    提供了各种定制选项，例如调整上下文大小、重复惩罚和温度，它还支持 GPU 卸载选项。
- en: Similar to Ollama, llama.cpp has a Python binding which can be installed via
    `pip install llama-cpp-python`. This Python library allows for significant customization,
    making it easy for developers to tailor models to specific client needs. However,
    just as with the standalone version, the Python binding requires compilation with
    the appropriate flags to enable GPU support.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Ollama，llama.cpp 也提供了 Python 绑定，可以通过 `pip install llama-cpp-python` 安装。这个
    Python 库允许进行显著的定制，使得开发者可以轻松地根据特定客户需求调整模型。然而，正如独立版本一样，Python 绑定也需要使用适当的标志进行编译，以启用
    GPU 支持。
- en: One minor downside is that the tool doesn’t yet support automatic CPU-GPU offloading.
    Instead, users need to manually specify how many layers to offload onto the GPU,
    with the remainder going to the CPU. While this requires some fine-tuning, it
    is a straightforward, manageable step.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小缺点是该工具尚不支持自动的 CPU-GPU 卸载。相反，用户需要手动指定将多少层卸载到 GPU 上，剩余的部分由 CPU 处理。虽然这需要一些微调，但这一步骤直接且易于管理。
- en: 'For environments with multiple GPUs, like ours, llama.cpp provides two split
    modes: *row mode* and *layer mode*. In row mode, one GPU handles small tensors
    and intermediate results, while in layer mode, layers are divided across GPUs.
    In our tests, both modes delivered comparable performance (see analysis below).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于拥有多个 GPU 的环境（如我们所使用的），llama.cpp 提供了两种分割模式：*行模式*和*层模式*。在行模式下，一个 GPU 处理小型张量和中间结果，而在层模式下，层被划分到多个
    GPU 上。在我们的测试中，这两种模式提供了相似的性能（请参见下面的分析）。
- en: Our Analysis
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的分析
- en: ► *From now on, results concern only llama.cpp and Ollama.*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ► *从现在起，结果仅涉及 llama.cpp 和 Ollama。*
- en: We conducted an analysis of the speed and power consumption of the 70B and 8B
    Llama 3.1 models using Ollama and llama.cpp. Specifically, we examined the speed
    and power consumption per token for each model across various quantizations available
    in [Quant Factory](https://huggingface.co/collections/QuantFactory/llama-31-66a36c08cd3aa07108e10dfe).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Ollama 和 llama.cpp 对 70B 和 8B Llama 3.1 模型的速度和功耗进行了分析。具体来说，我们研究了在 [Quant
    Factory](https://huggingface.co/collections/QuantFactory/llama-31-66a36c08cd3aa07108e10dfe)
    中提供的各种量化方式下，每个模型每个 token 的速度和功耗。
- en: To carry out this analysis, we developed a small application to evaluate the
    models once the tool was selected. During inference, we recorded metrics such
    as speed (tokens per second), total tokens generated, temperature, number of layers
    loaded on GPUs, and the quality rating of the response. Additionally, we measured
    the power consumption of the GPU during model execution. A script was used to
    monitor GPU power usage (via `nvidia-smi`) immediately after each token was generated.
    Once inference concluded, we computed the average power consumption based on these
    readings. Since we focused on models that could fully fit into GPU memory, we
    only measured GPU power consumption.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行此分析，我们开发了一个小型应用程序，在选择工具后评估模型。在推理过程中，我们记录了诸如速度（每秒令牌数）、生成的总令牌数、温度、GPU上加载的层数以及响应的质量评分等指标。此外，我们还测量了模型执行期间GPU的功耗。使用了一个脚本，通过`nvidia-smi`在每生成一个令牌后立即监测GPU的功耗。推理结束后，我们根据这些读数计算了平均功耗。由于我们专注于能够完全适应GPU内存的模型，因此只测量了GPU的功耗。
- en: Additionally, the experiments were conducted with a variety of prompts to ensure
    different output sizes, thus, the data encompass a wide range of scenarios.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了确保不同输出大小的情况，实验使用了多种提示语，因此数据涵盖了各种场景。
- en: Hardware and Software Setup
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬件和软件配置
- en: 'We used a pretty decent server with the following features:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一台相当不错的服务器，具备以下特点：
- en: 'CPU: AMD Ryzen Threadripper PRO 7965WX 24-Cores @ 48x 5.362GHz.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CPU: AMD Ryzen Threadripper PRO 7965WX 24核心 @ 48x 5.362GHz。'
- en: 'GPU: **2x** NVIDIA GeForce RTX 4090.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GPU: **2x** NVIDIA GeForce RTX 4090。'
- en: 'RAM: 515276MiB-'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'RAM: 515276MiB-'
- en: 'OS: Pop 22.04 jammy.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统：Pop 22.04 jammy。
- en: 'Kernel: x86_64 Linux 6.9.3–76060903-generic.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核：x86_64 Linux 6.9.3–76060903-generic。
- en: The retail cost of this setup was somewhere around $15,000 USD. We chose such
    a setup because it is a decent server that, while nowhere near as powerful as
    dedicated, high-end AI servers with 8 or more GPUs, is still quite functional
    and representative of what many of our clients might choose. We have found many
    clients hesitant to invest in high-end servers out of the gate, and this setup
    is a good compromise between cost and performance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这套配置的零售成本大约为15,000美元。我们选择了这样的配置，因为它是一台不错的服务器，虽然无法与配备8个以上GPU的专用高端AI服务器相比，但它仍然非常实用，并且能够代表我们许多客户可能选择的配置。我们发现许多客户在一开始不愿意投资高端服务器，而这套配置在成本和性能之间达到了良好的折衷。
- en: Speed
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 速度
- en: Let us first focus on speed. Below, we present several [box-whisker plots](https://en.wikipedia.org/wiki/Box_plot)
    depicting speed data for several quantizations. The name of each model starts
    with its quantization level; so for example “Q4” means a 4-bit quantization. Again,
    a LOWER quantization level rounds more, reducing size and quality but increasing
    speed.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们关注速度。下面，我们展示了几个[箱形图](https://en.wikipedia.org/wiki/Box_plot)，这些箱形图展示了不同量化方式下的速度数据。每个模型的名称以其量化级别开头；例如，“Q4”表示4位量化。再次强调，较低的量化级别会更多地进行舍入，减小模型的大小和质量，但提高速度。
- en: '► *Technical Issue 1 (A Reminder of Box-Whisker Plots): Box-whisker plots display
    the median, the first and third quartiles, as well as the minimum and maximum
    data points. The whiskers extend to the most extreme points not classified as
    outliers, while outliers are plotted individually. Outliers are defined as data
    points that fall outside the range of Q1 − 1.5 × IQR and Q3 + 1.5 × IQR, where
    Q1 and Q3 represent the first and third quartiles, respectively. The interquartile
    range (IQR) is calculated as IQR = Q3 − Q1.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ► *技术问题1（箱形图的提醒）：箱形图展示了中位数、第一个和第三个四分位数，以及最小值和最大值。图中的胡须延伸到不被视为离群值的最极端数据点，而离群值会单独绘制。离群值被定义为落在Q1
    − 1.5 × IQR和Q3 + 1.5 × IQR之外的数据点，其中Q1和Q3分别表示第一个和第三个四分位数。四分位距（IQR）计算公式为IQR = Q3
    − Q1。*
- en: llama.cpp
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: llama.cpp
- en: Below are the plots for llama.cpp. **Fig. 3** shows the results for all Llama
    3.1 models with 70B parameters available in [QuantFactory](https://huggingface.co/QuantFactory/Meta-Llama-3-70B-Instruct-GGUF-v2),
    while **Fig. 4** depicts some of the models with 8B parameters available [here](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2).
    70B models can offload up to 81 layers onto the GPU while 8B models up to 33\.
    For 70B, offloading all layers is not feasible for Q5 quantization and finer.
    Each quantization type includes the number of layers offloaded onto the GPU in
    parentheses. As expected, coarser quantization yields the best speed performance.
    Since row split mode performs similarly, we focus on layer split mode here.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是针对llama.cpp的图表。**图3**显示了在[QuantFactory](https://huggingface.co/QuantFactory/Meta-Llama-3-70B-Instruct-GGUF-v2)中可用的所有70B参数的Llama
    3.1模型的结果，而**图4**则展示了一些8B参数的模型，您可以在[这里](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2)找到这些模型。70B模型可以将最多81层卸载到GPU上，而8B模型最多可卸载33层。对于70B，Q5量化及更细的量化方式无法完全卸载所有层。每种量化类型后面括号内包含了卸载到GPU上的层数。如预期，粗量化类型提供了最佳的速度性能。由于行分割模式的表现相似，这里我们专注于层分割模式。
- en: '![](../Images/dbb2c0ec9178da55140f4e44e00d4d4f.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbb2c0ec9178da55140f4e44e00d4d4f.png)'
- en: '**Fig. 3** Llama 3.1 models with 70B parameters running under llama.cpp with
    split mode *layer*. As expected, coarser quantization provides the best speed.
    The number of layers offloaded onto the GPU is shown in parentheses next to each
    quantization type. Models with Q5 and finer quantizations do not fully fit into
    VRAM.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**图3** 在llama.cpp中使用层分割模式运行的Llama 3.1模型，参数为70B。如预期，粗量化提供了最佳的速度。卸载到GPU上的层数在每种量化类型旁边的括号中显示。使用Q5及更细量化的模型无法完全适配VRAM。'
- en: '![](../Images/ec621be986a8433692e2383f2b307f51.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec621be986a8433692e2383f2b307f51.png)'
- en: '**Fig. 4** Llama 3.1 models with 8B parameters running under llama.cpp using
    split mode *layer*. In this case, the model fits within the GPU memory for all
    quantization types, with coarser quantization resulting in the fastest speeds.
    Note that high speeds are outliers, while the overall trend hovers around 20 tokens
    per second for Q2_K.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4** 在llama.cpp中使用层分割模式运行的Llama 3.1模型，参数为8B。在这种情况下，所有量化类型下模型均能适应GPU内存，粗量化类型提供了最快的速度。请注意，高速事件为离群值，而总体趋势则在Q2_K的情况下约为每秒20个token。'
- en: Key Observations
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键观察
- en: During inference we observed some high speed events (especially in 8B Q2_K),
    this is where gathering data and understanding its distribution is crucial, as
    it turns out that those events are quite rare.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理过程中，我们观察到了一些高速事件（尤其是在8B Q2_K情况下），这时收集数据并理解其分布非常关键，因为这些事件实际上是相当罕见的。
- en: As expected, coarser quantization types yield the best speed performance. This
    is because the model size is reduced, allowing for faster execution.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如预期，粗量化类型提供了最佳的速度性能。这是因为模型大小被缩减，从而实现了更快的执行速度。
- en: The results concerning 70B models that do not fully fit into VRAM must be taken
    with caution, as using the CPU too could cause a bottleneck. Thus, the reported
    speed may not be the best representation of the model’s performance in those cases.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于70B模型未能完全适配VRAM的结果必须谨慎解读，因为使用CPU可能会造成瓶颈。因此，在这些情况下报告的速度可能不是模型性能的最佳表现。
- en: Ollama
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ollama
- en: We executed the same analysis for Ollama. **Fig. 5** shows the results for the
    default Llama 3.1 and 3.2 models that Ollama automatically downloads. All of them
    fit in the GPU memory except for the 405B model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对Ollama进行了相同的分析。**图5**显示了Ollama自动下载的默认Llama 3.1和3.2模型的结果。除了405B模型外，所有模型都可以适配GPU内存。
- en: '![](../Images/5cf70861d011baf06d6986701b776bdd.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5cf70861d011baf06d6986701b776bdd.png)'
- en: '**Fig. 5** Llama 3.1 and 3.2 models running under Ollama. These are the default
    models when using Ollama. All 3.1 models — specifically 405B, 70B, and 8B (labeled
    as “latest”) — use Q4_0 quantization, while the 3.2 models use Q8_0 (1B) and Q4_K_M
    (3B).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5** 在Ollama下运行的Llama 3.1和3.2模型。这些是使用Ollama时的默认模型。所有3.1模型——特别是405B、70B和8B（标记为“latest”）——使用Q4_0量化，而3.2模型使用Q8_0（1B）和Q4_K_M（3B）量化。'
- en: Key Observations
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键观察
- en: We can compare the 70B Q4_0 model across Ollama and llama.cpp, with Ollama exhibiting
    a slightly slower speed.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以比较Ollama和llama.cpp中的70B Q4_0模型，Ollama的速度略慢。
- en: Similarly, the 8B Q4_0 model is slower under Ollama compared to its llama.cpp
    counterpart, with a more pronounced difference — llama.cpp processes about five
    more tokens per second on average.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，8B Q4_0模型在Ollama下的运行速度较其在llama.cpp中的对应模型慢，且差异更加明显——llama.cpp平均每秒处理多出约五个token。
- en: Summary of Analyzed Frameworks
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 已分析框架总结
- en: ► Before discussing power consumption and rentability, let’s summarize the frameworks
    we analyzed up to this point.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ► 在讨论功耗和租用性之前，让我们总结一下迄今为止分析的框架。
- en: '![](../Images/4523dc3a8ecb6b331bbae7feb7ad5a9e.png)![](../Images/8cf1f9a4e23b4ddf59621e747fe2c680.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4523dc3a8ecb6b331bbae7feb7ad5a9e.png)![](../Images/8cf1f9a4e23b4ddf59621e747fe2c680.png)'
- en: Power Consumption and Rentability
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 功耗与租用性
- en: This analysis is particularly relevant to models that fit all layers into GPU
    memory, as we only measured the power consumption of two RTX 4090 cards. Nonetheless,
    it is worth noting that the CPU used in these tests has a [TDP of 350 W](https://www.techpowerup.com/cpu-specs/ryzen-threadripper-pro-7995wx.c3301),
    which provides an estimate of its power draw at maximum load. If the entire model
    is loaded onto the GPU, the CPU likely maintains a power consumption close to
    idle levels.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 该分析对于将所有层加载到GPU内存中的模型尤为相关，因为我们只测量了两块RTX 4090显卡的功耗。然而，需要注意的是，测试中使用的CPU具有[TDP
    350 W](https://www.techpowerup.com/cpu-specs/ryzen-threadripper-pro-7995wx.c3301)，这提供了其在最大负载下的功耗估计。如果整个模型被加载到GPU上，CPU的功耗可能接近空闲状态。
- en: 'To estimate energy consumption per token, we use the following parameters:
    *tokens per second* (NT) and *power drawn by both GPUs* (P) measured in watts.
    By calculating P/NT, we obtain the energy consumption per token in watt-seconds.
    Dividing this by 3600 gives the energy usage per token in Wh, which is more commonly
    referenced.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估算每个token的能耗，我们使用以下参数：*每秒token数*（NT）和*两块GPU的功率消耗*（P），单位为瓦特。通过计算P/NT，我们得到每个token的能耗，单位为瓦秒。将其除以3600，得到每个token的能耗，单位为Wh，这通常是更常用的参考单位。
- en: llama.cpp
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: llama.cpp
- en: Below are the results for llama.cpp. **Fig. 6** illustrates the energy consumption
    for 70B models, while **Fig. 7** focuses on 8B models. These figures present energy
    consumption data for each quantization type, with average values shown in the
    legend.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是llama.cpp的结果。**图 6**展示了70B模型的能耗，而**图 7**则侧重于8B模型。这些图表展示了每种量化方式的能耗数据，图例中显示了平均值。
- en: '![](../Images/47545ef2848a549f6011535a1d58f92d.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47545ef2848a549f6011535a1d58f92d.png)'
- en: '**Fig. 6** Energy per token for various quantizations of Llama 3.1 models with
    70B parameters under llama.cpp. Both row and layer split modes are shown. Results
    are relevant only for models that fit all 81 layers in GPU memory.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 6** 在llama.cpp下，针对70B参数的Llama 3.1模型的各种量化方式，每个token的能耗。展示了行分割和层分割模式。结果仅适用于将所有81层加载到GPU内存中的模型。'
- en: '![](../Images/efa6856ace45a7e11318c546c90b8f7f.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efa6856ace45a7e11318c546c90b8f7f.png)'
- en: '**Fig. 7** Energy per token for various quantizations of Llama 3.1 models with
    8B parameters under llama.cpp. Both row and layer split modes are shown. All models
    exhibit similar average consumption.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7** 在llama.cpp下，针对8B参数的Llama 3.1模型的各种量化方式，每个token的能耗。展示了行分割和层分割模式。所有模型的平均能耗相似。'
- en: Ollama
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ollama
- en: We also analyzed the energy consumption for Ollama. **Fig. 8** displays results
    for Llama 3.1 8B (Q4_0 quantization) and Llama 3.2 1B and 3B (Q8_0 and Q4_K_M
    quantizations, respectively). **Fig. 9** shows separate energy consumption for
    the 70B and 405B models, both with Q4_0 quantization.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还分析了Ollama的能耗。**图 8**展示了Llama 3.1 8B（Q4_0量化）和Llama 3.2 1B与3B（分别为Q8_0和Q4_K_M量化）的结果。**图
    9**展示了70B和405B模型的单独能耗，二者均采用Q4_0量化。
- en: '![](../Images/672d7536032b24286d90785d4affa630.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/672d7536032b24286d90785d4affa630.png)'
- en: '**Fig. 8** Energy per token for Llama 3.1 8B (Q4_0 quantization) and Llama
    3.2 1B and 3B models (Q8_0 and Q4_K_M quantizations, respectively) under Ollama.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8** 在Ollama下，Llama 3.1 8B（Q4_0量化）和Llama 3.2 1B与3B模型（分别为Q8_0和Q4_K_M量化）的每个token能耗。'
- en: '![](../Images/056b3f9d748ce01c0acfc7af26168536.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/056b3f9d748ce01c0acfc7af26168536.png)'
- en: '**Fig. 9** Energy per token for Llama 3.1 70B (left) and Llama 3.1 405B (right),
    both using Q4_0 quantization under Ollama.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 9** Llama 3.1 70B（左）和Llama 3.1 405B（右）每个token的能耗，二者均在Ollama下使用Q4_0量化。'
- en: Summary of Costs
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本总结
- en: Instead of discussing each model individually, we will focus on those models
    that are comparable across llama.cpp and Ollama, as well of models with Q2_K quantization
    under llama.cpp, since it is the coarsest quantization explored here. To give
    a good idea of the costs, we show in the table below estimations of the energy
    consumption per one million generated tokens (1M) and the cost in USD. The cost
    is calculated based on the average electricity price in Texas, which is $0.14
    per kWh according to this [source](https://bkvenergy.com/blog/average-electricity-bill-texas/).
    For a reference, the current [pricing of GPT-4o](https://openai.com/api/pricing/)
    is at least of $5 USD per 1M tokens and $0.3 USD per 1M tokens for GPT-o mini.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会逐个讨论每个模型，而是将重点放在llama.cpp和Ollama之间可比的模型，以及llama.cpp下使用Q2_K量化的模型，因为它是这里探索的最粗糙量化方式。为了更好地了解成本，我们在下表中展示了每百万生成token（1M）的能耗估算和美元成本。该成本是根据德克萨斯州的平均电价计算的，电价为每千瓦时$0.14，具体参考此[来源](https://bkvenergy.com/blog/average-electricity-bill-texas/)。作为参考，当前[GPT-4o的定价](https://openai.com/api/pricing/)至少为每百万token
    $5 USD，而GPT-o mini则为每百万token $0.3 USD。
- en: llama.cpp
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: llama.cpp
- en: '![](../Images/dfbcafd4499fb5354d8e5832413108af.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfbcafd4499fb5354d8e5832413108af.png)'
- en: Ollama
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ollama
- en: '![](../Images/6d75da0086d99efe5cb4edddfb50abba.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d75da0086d99efe5cb4edddfb50abba.png)'
- en: Key Observations
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键观察
- en: Using Llama 3.1 70B models with Q4_0, there is not much difference in the energy
    consumption between llama.cpp and Ollama.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Llama 3.1 70B模型与Q4_0时，llama.cpp与Ollama之间的能耗差异不大。
- en: For the 8B model llama.cpp spends more energy than Ollama.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于8B模型，llama.cpp的能耗高于Ollama。
- en: Consider that the costs depicted here could be seen as a lower bound of the
    “bare costs” of running the models. Other costs, such as operation, maintenance,
    equipment costs and profit, are not included in this analysis.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，这里所展示的成本可以看作是运行模型的“基本成本”的下限。其他成本，如操作、维护、设备成本和利润，并未包含在此分析中。
- en: The estimations suggest that operating LLMs on private servers can be cost-effective
    compared to cloud services. In particular, comparing Llama 8B with GPT-45o mini
    and Llama 70B with GPT-4o models seem to be a potential good deal under the right
    circumstances.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估算结果表明，相较于云服务，在私人服务器上运行LLM（大型语言模型）可能更具成本效益。特别是，Llama 8B与GPT-45o mini以及Llama
    70B与GPT-4o模型的比较，在合适的情况下似乎是一个潜在的好选择。
- en: ► *Technical Issue 2 (Cost Estimation):* For most models, the estimation of
    energy consumption per 1M tokens (and its variability) is given by the “median
    ± IQR” prescription, where IQR stands for interquartile range. Only for the Llama
    3.1 8B Q4_0 model do we use the “mean ± STD” approach, with STD representing standard
    deviation. These choices are not arbitrary; all models except for Llama 3.1 8B
    Q4_0 exhibit outliers, making the median and IQR more robust estimators in those
    cases. Additionally, these choices help prevent negative values for costs. In
    most instances, when both approaches yield the same central tendency, they provide
    very similar results.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ► *技术问题 2（成本估算）：* 对于大多数模型，每百万个token的能耗估算（及其变异性）通过“中位数 ± 四分位距（IQR）”的方式给出，其中IQR代表四分位距。只有对于Llama
    3.1 8B Q4_0模型，我们采用“均值 ± 标准差（STD）”的方式，其中STD代表标准差。这些选择并非随意作出的；除了Llama 3.1 8B Q4_0模型外，所有模型都存在异常值，使得中位数和IQR在这些情况下是更为稳健的估算器。此外，这些选择还有助于防止成本出现负值。在大多数情况下，当两种方法得出相同的中心趋势时，它们提供的结果非常相似。
- en: Final Word
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终结论
- en: '![](../Images/55c35bbe1cca9419622b9aa1539389d8.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55c35bbe1cca9419622b9aa1539389d8.png)'
- en: Image by Meta AI
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 Meta AI
- en: The analysis of speed and power consumption across different models and tools
    is only part of the broader picture. We observed that lightweight or heavily quantized
    models often struggled with reliability; hallucinations became more frequent as
    chat histories grew or tasks turned repetitive. This isn’t unexpected — smaller
    models don’t capture the extensive complexity of larger models. To counter these
    limitations, settings like repetition penalties and temperature adjustments can
    improve outputs. On the other hand, larger models like the 70B consistently showed
    strong performance with minimal hallucinations. However, since even the biggest
    models aren’t entirely free from inaccuracies, responsible and trustworthy use
    often involves integrating these models with additional tools, such as LangChain
    and vector databases. Although we didn’t explore specific task performance here,
    these integrations are key for minimizing hallucinations and enhancing model reliability.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对不同模型和工具的速度与功耗分析仅仅是更广泛图景的一部分。我们观察到，轻量级或高度量化的模型通常在可靠性上表现不佳；随着聊天历史的增加或任务变得重复，幻觉现象变得更为频繁。这并不意外——较小的模型无法捕捉到较大模型的复杂性。为了解决这些限制，像重复惩罚和温度调整等设置可以改善输出。另一方面，像70B这样的大型模型始终保持强劲的性能，并且幻觉现象极少。然而，由于即便是最大的模型也不完全没有不准确性，负责任和可信赖的使用往往需要将这些模型与额外的工具（如LangChain和向量数据库）结合使用。尽管我们在这里没有探索特定任务的表现，但这些整合对于减少幻觉现象和增强模型可靠性至关重要。
- en: In conclusion, running LLMs on private servers can provide a competitive alternative
    to LLMs as a service, with cost advantages and opportunities for customization.
    Both private and service-based options have their merits, and at *Austin Ai*,
    **we specialize in implementing solutions that suit your needs**, whether that
    means leveraging private servers, cloud services, or a hybrid approach.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，将大型语言模型（LLMs）部署在私有服务器上，可以为大型语言模型作为服务（LLMs as a service）提供一个具有成本优势和定制化机会的竞争性替代方案。私有服务器和基于服务的选项各有其优点，在*Austin
    Ai*，**我们专注于实现符合您需求的解决方案**，无论是利用私有服务器、云服务还是混合方案。
