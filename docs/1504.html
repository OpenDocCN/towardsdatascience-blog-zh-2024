<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Multi-Framework AI/ML Development with Keras 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Multi-Framework AI/ML Development with Keras 3</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-framework-ai-ml-development-with-keras-3-cf7be29eb23d?source=collection_archive---------3-----------------------#2024-06-16">https://towardsdatascience.com/multi-framework-ai-ml-development-with-keras-3-cf7be29eb23d?source=collection_archive---------3-----------------------#2024-06-16</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="be31" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">All hail the return of Keras</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--cf7be29eb23d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--cf7be29eb23d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--cf7be29eb23d--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--cf7be29eb23d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 16, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/2f76631d92e90d65b370eb16d967b59c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LY-57_Wnsq1bsmyN"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@notartistic?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jose Rueda</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj nc"><img src="../Images/60dcf4db50ea7db99db7c65748a3b9d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*YK1rE8tZPRfK6YdZTNhpTw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">By Author</figcaption></figure><p id="a83c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Keras is Back!! First released in 2015 as a high-level Python library for training ML models, Keras grew in popularity due to its clean and simple APIs. Contrary to the ML frameworks of the time, with their awkward and clunky APIs, Keras lowered the entry bar for many incumbent ML developers (the author included). But somewhere along the way the use of Keras became virtually synonymous with TensorFlow development. Consequently, when developers began to turn to alternative frameworks, the relative popularity of Keras began to decline. But now, following a “complete rewrite”, Keras has returned. And with its shiny new engine and its renewed commitment to multi-backend support, it vies to return to its former glory.</p><p id="93b6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this post we will take a new look at Keras and assess its value offering in the current era of AI/ML development. We will demonstrate through example its ease of use and make note of its shortcomings. Importantly, this post is not intended to be an endorsement for or against the adoption of Keras (or any other framework, library, service, etc.). As usual, the best decision for your project development will depend on a great many details, many of which are beyond the scope of this post.</p><p id="722e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The <a class="af nb" href="https://blog.google/technology/developers/gemma-open-models/" rel="noopener ugc nofollow" target="_blank">recent release</a> of Google’s family of open sourced NLP models known as Gemma, and the inclusion of Keras 3 as a core component of the API, offers us an opportunity to evaluate Keras’s goodness and could serve as a great opportunity for its resurgence.</p><h1 id="df17" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Why Use Keras 3?</h1><p id="4a35" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">In our view, the most valuable feature offered by Keras 3 is its multi-framework support. This may surprise some readers who may recall Keras’s distinctiveness to be its user experience. Keras 3 advertises itself, as “simple”, “flexible”, and being “designed for human beings, not machines”. And indeed, it owes its early successes and meteoric rise in popularity to its user experience. But it is now 2024 and there are many high-level deep learning APIs offering “reduced cognitive load”. In our view, the user experience, as good as it may be, is no longer a sufficient motivator to consider Keras over its alternatives. Its multi-framework support is.</p><h2 id="6259" class="pa oa fq bf ob pb pc pd oe pe pf pg oh nm ph pi pj nq pk pl pm nu pn po pp pq bk">The Merits of Multi-Framework Support</h2><p id="60d0" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Keras 3 supports multiple backends for training and running its models. At the time of this writing, these include <a class="af nb" href="https://jax.readthedocs.io/" rel="noopener ugc nofollow" target="_blank">JAX</a>, <a class="af nb" href="https://github.com/tensorflow/tensorflow" rel="noopener ugc nofollow" target="_blank">TensorFlow</a>, and <a class="af nb" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">PyTorch</a>. The <a class="af nb" href="https://keras.io/keras_3/" rel="noopener ugc nofollow" target="_blank">Keras 3 announcement</a> does a pretty good job of explaining the advantages of this feature. We will expand on the documented benefits and add some of our own flavor.</p><p id="d851" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Avoid the difficulty of choosing an AI/ML framework:<br/></strong>Choosing an AI/ML framework is probably one of the most important decisions you will need to make as an ML developer. It is also one of the hardest. There are many considerations that need to factor into this decision. These include user experience, API coverage, programmability, debuggability, the formats and types of input data that are supported, conformance with other components on the development pipeline (e.g., restrictions that may be imposed by the model deployment phase), and, perhaps most importantly, runtime performance. As we have discussed in many of our previous posts (e.g., <a class="af nb" href="https://medium.com/p/6e407a7d2dc8#de85-799b58b79241" rel="noopener">here</a>), AI/ML model development can be extremely expensive and the overall impact on cost of even the smallest speed-up due to the choice of framework can be dramatic. In fact, in many cases it may warrant the overhead of porting your model and code to a different framework and/or even maintaining support for multiple frameworks.</p><p id="29fc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The problem is that it is extremely difficult, if not impossible, to know which framework will be most optimal for your model before you start your development. Moreover, even once you have committed to one framework, you will want to stay on top of the evolution and development of all frameworks and to continuously assess potential opportunities to improve your model and/or reduce the cost of development. The landscape of AI/ML development is extremely dynamic with optimizations and enhancements being designed and developed on a consistent basis. You will not want to fall behind.</p><p id="b91b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Keras 3 solves the framework selection problem by enabling you to develop your model without committing to an underlying backend. The option to toggle between multiple framework-backends allows you to focus on the model definition and, once complete, choose the backend that best suits your needs. And even as the properties of the ML project change or the supported frameworks evolve, Keras 3 enables you to easily assess the impact of changing the backend.</p><p id="1195" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Putting it colloquially, you could say that Keras 3 helps humans avoid one of the things they hate doing most — making decisions and committing to them. But humor aside, AI/ML model development using Keras 3 can certainly prevent you from choosing and being stuck with a suboptimal framework.</p><p id="3a90" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Enjoy the best of all worlds:<br/></strong>PyTorch, TensorFlow, and JAX, each have their own unique advantages and differentiating properties. JAX, for example, supports just-in-time (JIT) compilation in which the model operators are converted into an intermediate computation graph and then compiled together into machine code specifically targeted for the underlying hardware. For many models this results in a considerable boost in runtime performance. On the other hand, PyTorch, which is typically used in a manner in which the operators are executed immediately (a.k.a. “eagerly”) is often considered to: have the most Pythonic interface, be the easiest to debug, and offer the best overall user experience. By using Keras 3 you can enjoy the best of both worlds. You can set the backend to PyTorch during your initial model development and for debugging and switch to JAX for optimal performance when training in production mode.</p><p id="8e9f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Compatibility with the maximum number of AI accelerators and runtime environments:<br/></strong>As we have discussed in the past (e.g., <a class="af nb" rel="noopener" target="_blank" href="/instance-selection-for-deep-learning-7463d774cff0">here</a>) our goal is to be compatible with as many AI accelerators and runtime environments as possible. This is especially important in an era of constrained capacity of AI machines in which the ability to switch between different machine types is a huge advantage. When you develop with Keras 3 and its multi-backend support, you automatically increase the number of platforms that you can potentially train and run your model on. For example, while you may be most accustomed to running in PyTorch on GPUs, by simply changing the backend to JAX you can configure your model to run on <a class="af nb" href="https://cloud.google.com/tpu?hl=en" rel="noopener ugc nofollow" target="_blank">Google Cloud TPUs</a>, as well ( — though this may depend on the details of the model).</p><p id="3563" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Increase model adoption:<br/></strong>If you are targeting your model for use by other AI/ML teams, you will increase your potential audience by supporting multiple frameworks. For all sorts of reasons, some teams may be limited to a specific ML framework. By delivering your model in Keras you remove barriers for adoption. A great example of this is the recent release of Google’s Gemma models which we will discuss in greater detail below.</p><p id="746e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Decouple the data input pipeline from the model execution:<br/></strong>Some frameworks encourage the use of certain data storage formats and/or data loading practices. A classic example of this is TensorFlow’s <a class="af nb" href="https://www.tensorflow.org/tutorials/load_data/tfrecord" rel="noopener ugc nofollow" target="_blank">TFRecord</a> data format for storing a sequence of binary records that are typically stored in <code class="cx pr ps pt pu b">.tfrecord</code> files. While TensorFlow includes native support for parsing and processing data stored TFRecord files, you might find feeding them into a PyTorch training loop to be a bit more difficult. A preferable format for PyTorch training could be <a class="af nb" href="https://pytorch.org/data/main/generated/torchdata.datapipes.iter.WebDataset.html" rel="noopener ugc nofollow" target="_blank">WebDataset</a>. But the creation of training data can be a long process and maintaining it in more than one format could be prohibitively expensive. Thus, the manner in which your training data is stored and maintained might discourage teams from considering alternative frameworks.</p><p id="ad9b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Keras 3 helps teams overcome this obstacle by completely decoupling the data input pipeline from the training loop. You can define your input data pipelines in PyTorch, TensorFlow, Numpy, Keras, and other libraries without any consideration for the backend that will be used in your training loop. With Keras 3, having your training data stored in TFRecord files is no longer a barrier to adopting PyTorch as a backend.</p><h2 id="790f" class="pa oa fq bf ob pb pc pd oe pe pf pg oh nm ph pi pj nq pk pl pm nu pn po pp pq bk">The Disadvantages of Multi-Framework Support</h2><p id="e0cf" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">As with any other new SW solution on the market, it is important to be aware of the potential downsides of Keras 3. A general rule of thumb in SW development is that the higher up the SW stack you go, the less control you have over the behavior and performance of your application. In AI/ML, where the degree of success is often determined by precise tuning of model hyperparameters, initialization settings, appropriate environment configuration, etc., such control could be critical. Here are just a few potential drawbacks to consider:</p><p id="e54c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Potential drop in runtime performance:<br/></strong>Working the high level Keras APIs rather than directly with the framework APIs, may pose limitations on optimizing runtime performance. In our series of posts on the topic of <a class="af nb" rel="noopener" target="_blank" href="/pytorch-model-performance-analysis-and-optimization-10c3c5822869">analyzing and optimizing the performance of PyTorch models</a>, we demonstrated a wide range of tools and techniques for increasing the speed of training. Sometimes these require the direct, unmediated, use of PyTorch’s APIs. For example, Keras’s APIs currently include very limited support for <a class="af nb" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="noopener ugc nofollow" target="_blank">PyTorch’s JIT compilation</a> option (via the <a class="af nb" href="https://keras.io/api/models/model_training_apis/" rel="noopener ugc nofollow" target="_blank"><em class="pv">jit_compile</em></a><em class="pv"> </em>setting). Another example is PyTorch’s built-in support for <a class="af nb" href="https://pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="noopener ugc nofollow" target="_blank">scaled dot product attention</a> which is not supported at the Keras level (as of the time of this writing).</p><p id="b995" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Limitations of cross-framework support:<br/></strong>Although Keras’s cross-framework support is extensive, you may find that it is not all-encompassing. For example, one gap in coverage (as of the time of this writing) is distributed training. Although, Keras introduces <a class="af nb" href="https://keras.io/guides/distribution/" rel="noopener ugc nofollow" target="_blank">the Keras distribution API</a> to support data and model parallelism across all backends, it is currently implemented for the JAX backend only. To run distributed training when using other backends, you will need to fall back to the standard distribution APIs of the relevant framework (e.g., PyTorch’s <a class="af nb" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" rel="noopener ugc nofollow" target="_blank">distributed data parallel API</a>).</p><p id="fac1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Overhead of maintaining cross-framework compatibility:<br/></strong>Keras 3 supports a wide variety of pre-built models that you can reuse (e.g., <a class="af nb" href="https://keras.io/api/keras_nlp/models/" rel="noopener ugc nofollow" target="_blank">here</a>). However, inevitably, you may want to introduce your own customizations. While Keras 3 supports customization of the model layers, metrics, training loop and more, you will need to take care not to break your cross-framework compatibility. For example, if you create a custom layer using Keras’s backend-agnostic APIs (<code class="cx pr ps pt pu b">keras.ops</code>), you can rest assured that multi-backend support is retained. However, sometimes you may choose to rely on framework-specific operations. In such cases maintaining cross-framework compatibility will require a dedicated implementation for each framework and appropriate conditional programming based on the backend in use. The current methods for customizing a <a class="af nb" href="https://keras.io/guides/custom_train_step_in_jax/" rel="noopener ugc nofollow" target="_blank">training step</a> and a <a class="af nb" href="https://keras.io/guides/writing_a_custom_training_loop_in_jax/" rel="noopener ugc nofollow" target="_blank">training loop</a> are framework-specific, meaning that they too would require dedicated implementations for each backend to retain cross-framework compatibility. Thus, as your model grows in complexity, so might the overhead required to maintain this unique capability.</p><p id="f6df" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We have noted just a few potential disadvantages to Keras 3 and its multi-backend support. You may very well likely come across others. While the multi-framework offering is certainly compelling, its adoption is not necessarily free of cost. Borrowing the name of a well-known <a class="af nb" href="https://en.wikipedia.org/wiki/No_free_lunch_theorem" rel="noopener ugc nofollow" target="_blank">theorem</a> in the field of statistical inference, one could say that when it comes to choosing an AI/ML development methodology, there are “no free lunches”.</p><h1 id="c396" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Keras 3 in Practice — A Toy Example</h1><p id="5d1f" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">As in many of our recent posts, the toy model we will define will be a <a class="af nb" href="https://huggingface.co/docs/transformers/en/model_doc/vit" rel="noopener ugc nofollow" target="_blank">Vision Transformer</a> (ViT) backed classification model. We will rely on the reference implementation located in this Keras <a class="af nb" href="https://keras.io/examples/vision/image_classification_with_vision_transformer/" rel="noopener ugc nofollow" target="_blank">tutorial</a>. We have configured our model according to the <a class="af nb" href="https://deci.ai/model-zoo/vit/" rel="noopener ugc nofollow" target="_blank">ViT-Base</a> architecture (~86 million parameters), set the <a class="af nb" href="https://keras.io/api/mixed_precision/" rel="noopener ugc nofollow" target="_blank">mixed_precision</a> policy to use <em class="pv">bfloat16</em>, and defined a <a class="af nb" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html" rel="noopener ugc nofollow" target="_blank">PyTorch<em class="pv"> </em>dataloader</a> with random input data.</p><p id="5abe" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The following block includes the configuration settings followed by definitions of the core ViT model components:</p><pre class="ml mm mn mo mp pw pu px bp py bb bk"><span id="7006" class="pz oa fq pu b bg qa qb l qc qd">import os<br/><br/># choose backend<br/>backend = 'jax' # 'torch'<br/>os.environ["KERAS_BACKEND"] = backend<br/><br/>import keras<br/>from keras import layers<br/>from keras import ops<br/><br/># set mixed precision policy<br/>keras.mixed_precision.set_global_policy('mixed_bfloat16')<br/><br/># use ViT Base settings<br/>num_classes = 1000<br/>image_size = 224<br/>input_shape = (image_size, image_size, 3)<br/>patch_size = 16  # Size of the patches to be extract from the input images<br/>num_patches = (image_size // patch_size) ** 2<br/>projection_dim = 768<br/>num_heads = 12<br/>transformer_units = [<br/>    projection_dim * 4,<br/>    projection_dim,<br/>]  # Size of the transformer layers<br/>transformer_layers = 12<br/><br/># set training hyperparams<br/>batch_size = 128<br/>multi_worker = False # toggle to use multiple data loader workers<br/>preproc_workers = 0 if 'jax' else 16<br/><br/># ViT model components:<br/># ---------------------<br/><br/>def mlp(x, hidden_units, dropout_rate):<br/>    for units in hidden_units:<br/>        x = layers.Dense(units, activation=keras.activations.gelu)(x)<br/>        x = layers.Dropout(dropout_rate)(x)<br/>    return x<br/><br/>class Patches(layers.Layer):<br/>    def __init__(self, patch_size):<br/>        super().__init__()<br/>        self.patch_size = patch_size<br/><br/>    def call(self, images):<br/>        input_shape = ops.shape(images)<br/>        batch_size = input_shape[0]<br/>        height = input_shape[1]<br/>        width = input_shape[2]<br/>        channels = input_shape[3]<br/>        num_patches_h = height // self.patch_size<br/>        num_patches_w = width // self.patch_size<br/>        patches = keras.ops.image.extract_patches(images, size=self.patch_size)<br/>        patches = ops.reshape(<br/>            patches,<br/>            (<br/>                batch_size,<br/>                num_patches_h * num_patches_w,<br/>                self.patch_size * self.patch_size * channels,<br/>            ),<br/>        )<br/>        return patches<br/><br/>class PatchEncoder(layers.Layer):<br/>    def __init__(self, num_patches, projection_dim):<br/>        super().__init__()<br/>        self.num_patches = num_patches<br/>        self.projection = layers.Dense(units=projection_dim)<br/>        self.position_embedding = layers.Embedding(<br/>            input_dim=num_patches, output_dim=projection_dim<br/>        )<br/><br/>    def call(self, patch):<br/>        positions = ops.expand_dims(<br/>            ops.arange(start=0, stop=self.num_patches, step=1), axis=0<br/>        )<br/>        projected_patches = self.projection(patch)<br/>        encoded = projected_patches + self.position_embedding(positions)<br/>        return encoded</span></pre><p id="568d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Using the core components, we define a ViT-backed Keras model:</p><pre class="ml mm mn mo mp pw pu px bp py bb bk"><span id="2371" class="pz oa fq pu b bg qa qb l qc qd"># the attention layer we will use in our ViT classifier<br/>attention_layer = layers.MultiHeadAttention<br/><br/>def create_vit_classifier():<br/>    inputs = keras.Input(shape=input_shape)<br/>    # Create patches.<br/>    patches = Patches(patch_size)(inputs)<br/>    # Encode patches.<br/>    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)<br/><br/>    # Create multiple layers of the Transformer block.<br/>    for _ in range(transformer_layers):<br/>        # Layer normalization 1.<br/>        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)<br/>        # Create a multi-head attention layer.<br/>        attention_output = attention_layer(<br/>            num_heads=num_heads, key_dim=projection_dim//num_heads, dropout=0.1<br/>        )(x1, x1)<br/>        # Skip connection 1.<br/>        x2 = layers.Add()([attention_output, encoded_patches])<br/>        # Layer normalization 2.<br/>        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)<br/>        # MLP.<br/>        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)<br/>        # Skip connection 2.<br/>        encoded_patches = layers.Add()([x3, x2])<br/><br/>    # Create a [batch_size, projection_dim] tensor.<br/>    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)<br/>    representation = layers.GlobalAveragePooling1D()(representation)<br/>    representation = layers.Dropout(0.5)(representation)<br/><br/>    # Classify outputs.<br/>    logits = layers.Dense(num_classes)(representation)<br/><br/>    # Create the Keras model.<br/>    model = keras.Model(inputs=inputs, outputs=logits)<br/>    return model<br/><br/># create the ViT model<br/>model = create_vit_classifier()<br/>model.summary()</span></pre><p id="2663" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the next block we define the optimizer, loss, and dataset.</p><pre class="ml mm mn mo mp pw pu px bp py bb bk"><span id="9a7b" class="pz oa fq pu b bg qa qb l qc qd">model.compile(<br/>    optimizer=keras.optimizers.SGD(),<br/>    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),<br/>    )<br/><br/>def get_data_loader(batch_size):<br/>    import torch<br/>    from torch.utils.data import Dataset, DataLoader<br/><br/>    # create dataset of random image and label data<br/>    class FakeDataset(Dataset):<br/>        def __len__(self):<br/>            return 1000000<br/><br/>        def __getitem__(self, index):<br/>            rand_image = torch.randn([224, 224, 3], dtype=torch.float32)<br/>            label = torch.tensor(data=[index % 1000], dtype=torch.int64)<br/>            return rand_image, label<br/><br/>    ds = FakeDataset()<br/>    dl = DataLoader(<br/>        ds,<br/>        batch_size=batch_size,<br/>        num_workers=preproc_workers if multi_worker else 0,<br/>        pin_memory=True<br/>    )<br/>    return dl<br/><br/>dl = get_data_loader(batch_size)</span></pre><p id="f342" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Finally, we start the training using Keras’s <a class="af nb" href="https://keras.io/api/models/model_training_apis/" rel="noopener ugc nofollow" target="_blank">Model.fit()</a> function:</p><pre class="ml mm mn mo mp pw pu px bp py bb bk"><span id="d1da" class="pz oa fq pu b bg qa qb l qc qd">model.fit(<br/>    dl,<br/>    batch_size=batch_size,<br/>    epochs=1<br/>)</span></pre><p id="1afa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We ran the script above on a Google Cloud Platform (GCP) <a class="af nb" href="https://cloud.google.com/compute/docs/gpus#l4-gpus" rel="noopener ugc nofollow" target="_blank">g2-standard-16</a> VM (with a single NVIDIA L4 GPU) with a dedicated <a class="af nb" href="https://cloud.google.com/deep-learning-vm/docs/release-notes" rel="noopener ugc nofollow" target="_blank">deep learning VM image</a> (common-cu121-v20240514-ubuntu-2204-py310) and installations of PyTorch (2.3.0), JAX (0.4.28), Keras (3.3.3), and <a class="af nb" href="https://keras.io/keras_cv/" rel="noopener ugc nofollow" target="_blank">KerasCV</a> (0.9.0). Please see the <a class="af nb" href="https://github.com/keras-team/keras/blob/master/README.md#installation" rel="noopener ugc nofollow" target="_blank">official Keras documentation</a> for full installation instructions. Note that we manually modified the format of step time reported by the <a class="af nb" href="https://github.com/keras-team/keras/blob/v3.3.3/keras/src/utils/progbar.py#L228" rel="noopener ugc nofollow" target="_blank">Keras progress bar</a>:</p><pre class="ml mm mn mo mp pw pu px bp py bb bk"><span id="ea94" class="pz oa fq pu b bg qa qb l qc qd"> formatted += f" {time_per_unit:.3f}s/{unit_name}"</span></pre><p id="4d46" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Using the <em class="pv">backend </em>flag we were able to easily toggle between the backends supported by Keras and compare the runtime performance of each. For example, when configuring <a class="af nb" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html" rel="noopener ugc nofollow" target="_blank">PyTorch<em class="pv"> </em>dataloader</a> with 0 workers, we found that JAX backend to outperform PyTorch by ~24%. When setting the number of workers to 16 this drops to ~12%.</p><h2 id="9456" class="pa oa fq bf ob pb pc pd oe pe pf pg oh nm ph pi pj nq pk pl pm nu pn po pp pq bk">Custom Attention Layer</h2><p id="e9f0" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">We now define a custom attention layer that replaces <a class="af nb" href="https://keras.io/api/layers/attention_layers/multi_head_attention/" rel="noopener ugc nofollow" target="_blank">Keras’s default attention</a> computation with PyTorch’s <a class="af nb" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="noopener ugc nofollow" target="_blank">flash attention</a> implementation. Note that this will only work when the backend is set to <em class="pv">torch.</em></p><pre class="ml mm mn mo mp pw pu px bp py bb bk"><span id="a04f" class="pz oa fq pu b bg qa qb l qc qd">class MyAttention(layers.MultiHeadAttention):<br/>    def _compute_attention(<br/>            self, query, key, value, attention_mask=None, training=None<br/>    ):<br/>        from torch.nn.functional import scaled_dot_product_attention<br/>        query = ops.multiply(<br/>            query, ops.cast(self._inverse_sqrt_key_dim, query.dtype))<br/>        return scaled_dot_product_attention(<br/>            query.transpose(1,2),<br/>            key.transpose(1,2),<br/>            value.transpose(1,2),<br/>            dropout_p=self._dropout if training else 0.<br/>            ).transpose(1,2), None<br/><br/>attention_layer = MyAttention</span></pre><p id="568e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The results of our experiments are summarized in the table below. Keep in mind that the relative performance results are likely to vary greatly based on the details of the model and the runtime environment.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qe"><img src="../Images/bcef0995a3e1fe23cd9cca46c39e9294.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*GuPgN7EjXJoieqjD_mKdew.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">ViT runtime (by Author)</figcaption></figure><p id="a953" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">When using our custom attention layer, the gap between the JAX and PyTorch backends virtually disappears. This highlights how the use of a multi-backend solution could come at the expense of optimizations uniquely supported by any of the individual frameworks (in our example, PyTorch SDPA).</p><h1 id="93be" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Keras 3 in Gemma</h1><p id="43a2" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk"><a class="af nb" href="https://ai.google.dev/gemma/?utm_source=keyword&amp;utm_medium=referral&amp;utm_campaign=gemma_cta&amp;utm_content=" rel="noopener ugc nofollow" target="_blank">Gemma</a> is a family of lightweight, <a class="af nb" href="https://opensource.googleblog.com/2024/02/building-open-models-responsibly-gemini-era.html" rel="noopener ugc nofollow" target="_blank">open source models</a> recently <a class="af nb" href="https://blog.google/technology/developers/gemma-open-models/" rel="noopener ugc nofollow" target="_blank">released</a> by Google. Keras 3 plays a prominent role in the Gemma release (e.g., see <a class="af nb" href="https://ai.google.dev/gemma/docs/get_started" rel="noopener ugc nofollow" target="_blank">here</a>) and its multi-framework support makes Gemma automatically accessible to AI/ML developers of all persuasions — PyTorch, TensorFlow, and Jax. Please see the official <a class="af nb" href="https://keras.io/api/keras_nlp/models/gemma/" rel="noopener ugc nofollow" target="_blank">documentation</a> in <a class="af nb" href="https://keras.io/api/keras_nlp/" rel="noopener ugc nofollow" target="_blank">KerasNLP</a> for more details on the Gemma API offering.</p><p id="e7de" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The following code is loosely based on the official <a class="af nb" href="https://ai.google.dev/gemma/docs/lora_tuning" rel="noopener ugc nofollow" target="_blank">Gemma fine-tuning tutorial</a>. In order to run the script, please follow the <a class="af nb" href="https://ai.google.dev/gemma/docs/lora_tuning#setup" rel="noopener ugc nofollow" target="_blank">necessary setup instructions</a>.</p><pre class="ml mm mn mo mp pw pu px bp py bb bk"><span id="d7e2" class="pz oa fq pu b bg qa qb l qc qd">import os<br/>backend = 'jax' #'torch'<br/>os.environ["KERAS_BACKEND"] = backend<br/><br/>num_batches = 1000<br/>batch_size = 4 if backend == 'jax' else 2<br/><br/># Avoid memory fragmentation on JAX backend.<br/>os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"]="1.00"<br/>os.environ["KAGGLE_USERNAME"]="chaimrand"<br/>os.environ["KAGGLE_KEY"]="29abebb28f899a81ca48bec1fb97faf1"<br/>import keras<br/>import keras_nlp<br/>keras.mixed_precision.set_global_policy('mixed_bfloat16')<br/><br/>import json<br/>data = []<br/>with open("databricks-dolly-15k.jsonl") as file:<br/>    for line in file:<br/>        features = json.loads(line)<br/>        # Filter out examples with context, to keep it simple.<br/>        if features["context"]:<br/>            continue<br/>        # Format the entire example as a single string.<br/>        template = "Instruction:\n{instruction}\n\nResponse:\n{response}"<br/>        data.append(template.format(**features))<br/><br/># Only use 1000 training batches, to keep it fast.<br/>data = data[:num_batches*batch_size]<br/>gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("gemma_2b_en")<br/># Enable LoRA for the model and set the LoRA rank to 4.<br/>gemma_lm.backbone.enable_lora(rank=4)<br/><br/>gemma_lm.summary()<br/># Limit the input sequence length to 512 (to control memory usage).<br/>gemma_lm.preprocessor.sequence_length = 512<br/><br/>gemma_lm.compile(<br/>   loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),<br/>   optimizer=keras.optimizers.SGD(learning_rate=5e-5),<br/>   weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],<br/>)<br/>gemma_lm.fit(data, epochs=1, batch_size=batch_size)</span></pre><p id="1426" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">When running the script in the same GCP environment described above, we see a significant (and surprising) discrepancy between the runtime performance when using the JAX backend (6.87 samples per second) and the runtime performance when using the PyTorch backend (3.01 samples per second). This is due, in part, to the fact that the JAX backend allows for doubling the training batch size. A deep dive into the causes of this discrepancy is beyond the scope of this post.</p><p id="ba63" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As in our previous example, we demonstrate one way of optimizing the PyTorch runtime by prepending the following configuration of the <a class="af nb" href="https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html" rel="noopener ugc nofollow" target="_blank">matrix multiplication operations</a> to the top of our script:</p><pre class="ml mm mn mo mp pw pu px bp py bb bk"><span id="0c56" class="pz oa fq pu b bg qa qb l qc qd">import torch<br/>torch.set_float32_matmul_precision('high')</span></pre><p id="4088" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This simple change results in a 29% performance boost when running with the PyTorch backend. Once again, we can see the impact of applying framework-specific optimizations. The experiment results are summarized in the table below.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qf"><img src="../Images/5aad8a9425e1f20875979fd728686855.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v1eX0QDxYvwrZxuLd8lHWg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Gemma fine-tuning runtime (by Author)</figcaption></figure><h1 id="a418" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion</h1><p id="3298" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Our demonstrations have indicated that sticking with the backend agnostic Keras code could imply a meaningful runtime performance penalty. In each example, we have seen how a simple, framework-specific optimization had a significant impact on the relative performance of our chosen backends. At the same time, the arguments we have discussed for multi-framework AI/ML development are rather compelling.</p><p id="12dc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If you do choose to adopt Keras as a development framework, you may want to consider designing your code in a manner that includes mechanisms for applying and assessing framework-specific optimizations. You might also consider designing your development process in a way that utilizes Keras during the early stages of the project and, as the project matures, optimizes for the one backend that is revealed to be the most appropriate.</p><h1 id="f2fb" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Summary</h1><p id="e1cb" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">In this post we have explored the new and revised Keras 3 release. No longer an appendage to TensorFlow, Keras 3 offers the ability of framework-agnostic AI/ML model development. As we discussed, this capability has several significant advantages. However, as is often the case in the field of AI development, “there are no free lunches” — the added level of abstraction could mean a reduced level of control over the inner workings of our code which could imply slower training speed and higher costs. The best solution might be one that combines the use of Keras and its multi-framework support with dedicated mechanisms for incorporating framework-specific modifications.</p><p id="1044" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Importantly, the applicability of Keras 3 to your project and the cost-best analysis of the investment required, will depend greatly on a wide variety of factors including: the target audience, the model deployment process, project timelines, and more. Please view this post as a mere introduction into your detailed exploration.</p></div></div></div></div>    
</body>
</html>