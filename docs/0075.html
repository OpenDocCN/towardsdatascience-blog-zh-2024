<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Can LLMs Replace Data Analysts? Learning to Collaborate</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Can LLMs Replace Data Analysts? Learning to Collaborate</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/can-llms-replace-data-analysts-learning-to-collaborate-9d42488dc327?source=collection_archive---------2-----------------------#2024-01-09">https://towardsdatascience.com/can-llms-replace-data-analysts-learning-to-collaborate-9d42488dc327?source=collection_archive---------2-----------------------#2024-01-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="9bde" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Part 3: Teaching the LLM agent to pose and address clarifying questions</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://miptgirl.medium.com/?source=post_page---byline--9d42488dc327--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mariya Mansurova" class="l ep by dd de cx" src="../Images/b1dd377b0a1887db900cc5108bca8ea8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*7fFHr8XBAuR_SgJknIyODA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--9d42488dc327--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://miptgirl.medium.com/?source=post_page---byline--9d42488dc327--------------------------------" rel="noopener follow">Mariya Mansurova</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--9d42488dc327--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">20 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">6</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/fa7743b2c33135033008e395f36e9ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*wUDorQvoHBbMLqPFvvVorw.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by DALL-E 3</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4b60" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">Collaboration</strong> is a core aspect of analysts’ day-to-day jobs. Frequently, we encounter high-level requests such as, “What will be the impact of the new feature?” or “What is going on with retention?". Before jumping to writing queries and pulling data, we usually need to define tasks more clearly: talk to stakeholders, understand their needs thoroughly, and determine how we can provide the best assistance.</p><p id="3293" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">So, for an LLM-powered analyst, mastering the art of posing and addressing follow-up questions is essential since I can’t imagine an analyst working in isolation.</p><p id="6d13" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In this article, we will teach our LLM analyst to ask clarifying questions and follow long conversations. We will talk in detail about different memory implementations in LangChain.</p><p id="4b39" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We’ve already discussed many aspects of LLM agents in the previous articles. So, let me quickly summarise them. Also, since our last implementation, LangChain has been updated, and it’s time to catch up.</p><h1 id="7dee" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">LLM agents recap</h1><p id="4615" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">Let’s quickly recap what we’ve already learned about LLM agents.</p><ul class=""><li id="922f" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pg ph pi bk">We’ve <a class="af pj" rel="noopener" target="_blank" href="/can-llms-replace-data-analysts-building-an-llm-powered-analyst-851578fa10ce">discussed</a> how to empower LLMs with external tools. It helps them overcome limitations (i.e., poor performance on maths tasks) and get access to the world (i.e., your database or internet).</li><li id="9101" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe pg ph pi bk">The core idea of the LLM agents is to use LLM as a reasoning engine to define the set of actions to take and leverage tools. So, in this approach, you don’t need to hardcode the logic and just let LLM make decisions on the following steps to achieve the final goal.</li><li id="0acc" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe pg ph pi bk">We’ve <a class="af pj" rel="noopener" target="_blank" href="/can-llms-replace-data-analysts-getting-answers-using-sql-8cf7da132259">implemented</a> an LLM-powered agent that can work with SQL databases and answer user requests.</li></ul><p id="359c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Since our last iteration, LangChain has been <a class="af pj" href="https://github.com/langchain-ai/langchain/releases" rel="noopener ugc nofollow" target="_blank">updated</a> from 0.0.350 to 0.1.0 version. The documentation and best practices for LLM agents have changed. This domain is developing quickly, so it’s no surprise the tools are evolving, too. Let’s quickly recap.</p><p id="9a63" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">First, LangChain has significantly improved <a class="af pj" href="https://python.langchain.com/docs/modules/agents/agent_types/" rel="noopener ugc nofollow" target="_blank">the documentation</a>, and now you can find a clear, structured view of the supported agent types and the differences between them.</p><p id="642f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It’s easier for models to work with tools with just one input parameter, so some agents have such limitations. However, in most real-life cases, tools have several arguments. So, let’s focus on the agents capable of working with multiple inputs. It leaves us just three possible options.</p><ol class=""><li id="7acb" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pp ph pi bk"><a class="af pj" href="https://python.langchain.com/docs/modules/agents/agent_types/openai_tools" rel="noopener ugc nofollow" target="_blank"><strong class="nl fr">OpenAI tools</strong></a></li></ol><ul class=""><li id="3afe" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pg ph pi bk">It’s the most cutting-edge type of agent since it supports chat history, tools with multiple inputs and even parallel function calling.</li><li id="534e" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe pg ph pi bk">You can use it with the recent OpenAI models (after <code class="cx pq pr ps pt b">1106</code>) since they were fine-tuned for tool calling.</li></ul><p id="d3e3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">2. <a class="af pj" href="https://python.langchain.com/docs/modules/agents/agent_types/openai_functions_agent" rel="noopener ugc nofollow" target="_blank"><strong class="nl fr">OpenAI functions</strong></a></p><ul class=""><li id="c40f" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pg ph pi bk">OpenAI functions agents are close to OpenAI tools but are slightly different under the hood.</li><li id="ee9d" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe pg ph pi bk">Such agents don’t support parallel function calling.</li><li id="04e1" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe pg ph pi bk">You can use recent OpenAI models that were fine-tuned to work with functions (the complete list is <a class="af pj" href="https://platform.openai.com/docs/guides/function-calling/supported-models" rel="noopener ugc nofollow" target="_blank">here</a>) or compatible open-source LLMs.</li></ul><p id="64a9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">3.<strong class="nl fr"> </strong><a class="af pj" href="https://python.langchain.com/docs/modules/agents/agent_types/structured_chat" rel="noopener ugc nofollow" target="_blank"><strong class="nl fr">Structured chat</strong></a></p><ul class=""><li id="2298" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pg ph pi bk">This approach is similar to ReAct. It instructs an agent to follow the Thought -&gt; Action -&gt; Observation framework.</li><li id="b559" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe pg ph pi bk">It doesn’t support parallel function calling, just as OpenAI functions approach.</li><li id="1384" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe pg ph pi bk">You can use it with any model.</li></ul><blockquote class="pu pv pw"><p id="a229" class="nj nk px nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Also, you can notice that the experimental agent types we tried in <a class="af pj" rel="noopener" target="_blank" href="/can-llms-replace-data-analysts-getting-answers-using-sql-8cf7da132259">the previous article</a>, such as BabyAGI, Plan-and-execute and AutoGPT, are still not part of the suggested options. They might be included later (I hope), but for now I wouldn’t recommend using them in production.</p></blockquote><p id="a253" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">After reading the new documentation, I’ve finally realised the difference between OpenAI tools and OpenAI functions agents. With the OpenAI tools approach, an agent can call multiple tools at the same iterations, while other agent types don’t support such functionality. Let’s see how it works and why it matters.</p><p id="526d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s create two agents — OpenAI tools and OpenAI functions. We will empower them with two tools:</p><ul class=""><li id="3afb" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pg ph pi bk"><code class="cx pq pr ps pt b">get_monthly_active_users</code> returns the number of active customers for city and month. To simplify debugging, we will be using a dummy function for it. In practice, we would go to our database to retrieve this data.</li><li id="6f28" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe pg ph pi bk"><code class="cx pq pr ps pt b">percentage_difference</code> calculates the difference between two metrics.</li></ul><p id="cb11" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s create tools from Python functions and specify schemas using Pydantic. If you want to recap this topic, you can find a detailed explanation in <a class="af pj" rel="noopener" target="_blank" href="/can-llms-replace-data-analysts-building-an-llm-powered-analyst-851578fa10ce">the first article</a> of this series.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="e2ba" class="qb og fq pt b bg qc qd l qe qf">from pydantic import BaseModel, Field<br/>from typing import Optional<br/>from langchain.agents import tool<br/><br/># define tools<br/><br/>class Filters(BaseModel):<br/>    month: str = Field(description="Month of the customer's activity in the format %Y-%m-%d")<br/>    city: Optional[str] = Field(description="The city of residence for customers (by default no filter)", <br/>                    enum = ["London", "Berlin", "Amsterdam", "Paris"])<br/><br/>@tool(args_schema=Filters)<br/>def get_monthly_active_users(month: str, city: str = None) -&gt; int:<br/>    """Returns the number of active customers for the specified month. <br/>    Pass month in format %Y-%m-01.<br/>    """<br/><br/>    coefs = {<br/>        'London': 2,<br/>        'Berlin': 1,<br/>        'Amsterdam': 0.5,<br/>        'Paris': 0.25<br/>    }<br/>    <br/>    dt = datetime.datetime.strptime(month, '%Y-%m-%d')<br/>    total = dt.year + 10*dt.month<br/>    <br/>    if city is None:<br/>        return total<br/>    else:<br/>        return int(round(coefs[city]*total))<br/>        <br/>class Metrics(BaseModel):<br/>    metric1: float = Field(description="Base metric value to calculate the difference")<br/>    metric2: float = Field(description="New metric value that we compare with the baseline")<br/><br/>@tool(args_schema=Metrics)<br/>def percentage_difference(metric1: float, metric2: float) -&gt; float:<br/>    """Calculates the percentage difference between metrics"""<br/>    return (metric2 - metric1)/metric1*100<br/><br/># save them into a list for future use<br/><br/>tools = [get_monthly_active_users, percentage_difference]</span></pre><p id="e2de" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">To test a tool, you can execute it using the following commands.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="a9c9" class="qb og fq pt b bg qc qd l qe qf">get_monthly_active_users.run({"month": "2023-12-01", "city": "London"})<br/># 4286<br/><br/>get_monthly_active_users.run({"month": "2023-12-01", "city": "Berlin"})<br/># 2183</span></pre><p id="ad5c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s create a prompt template that we will be using for the agents. It will consist of a system message, a user request and a placeholder for tools’ observations. Our prompt has two variables — <code class="cx pq pr ps pt b">input</code> and <code class="cx pq pr ps pt b">agent_scratchpad</code>.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="3ba0" class="qb og fq pt b bg qc qd l qe qf">from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder<br/><br/># defining prompt<br/><br/>system_message = '''<br/>You are working as a product analyst for a e-commerce company. <br/>Your work is very important, since your product team makes decisions based on the data you provide. So, you are extremely accurate with the numbers you provided. <br/>If you're not sure about the details of the request, you don't provide the answer and ask follow-up questions to have a clear understanding.<br/>You are very helpful and try your best to answer the questions.<br/>'''<br/><br/>prompt = ChatPromptTemplate.from_messages([<br/>    ("system", system_message),<br/>    ("user", "{input}"),<br/>    MessagesPlaceholder(variable_name="agent_scratchpad")<br/>])</span></pre><p id="c706" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s use new LangChain functions to create agents — <code class="cx pq pr ps pt b">create_openai_functions_agent</code> and <code class="cx pq pr ps pt b">create_openai_tools_agent</code>. To create an agent, we need to specify parameters — an LLM model, a list of tools and a prompt template. On top of the agents, we also need to create agent executors.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="0fff" class="qb og fq pt b bg qc qd l qe qf"><br/>from langchain.agents import create_openai_tools_agent, create_openai_functions_agent, AgentExecutor<br/>from langchain_community.chat_models import ChatOpenAI<br/><br/># OpenAI tools agent<br/>agent_tools = create_openai_tools_agent(<br/>    llm = ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview'),<br/>    tools = tools, <br/>    prompt = prompt<br/>)<br/><br/>agent_tools_executor = AgentExecutor(<br/>    agent = agent_tools, tools = tools, <br/>    verbose = True, max_iterations = 10, <br/>    early_stopping_method = 'generate')<br/><br/># OpenAI functions agent<br/>agent_funcs = create_openai_functions_agent(<br/>    llm = ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview'),<br/>    tools = tools, <br/>    prompt = prompt<br/>)<br/><br/>agent_funcs_executor = AgentExecutor(<br/>    agent = agent_funcs, tools = tools, <br/>    verbose = True, max_iterations = 10, <br/>    early_stopping_method = 'generate')</span></pre><p id="a96c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">I used the ChatGPT 4 Turbo model since it’s capable of working with OpenAI tools. We will need some complex reasoning, thus ChatGPT 3.5 will likely be insufficient in our use case.</p><p id="46b0" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We’ve created two agent executors, and it’s time to try them in practice and compare results.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="7310" class="qb og fq pt b bg qc qd l qe qf">user_question = 'What are the absolute numbers and the percentage difference between the number of customers in London and Berlin in December 2023?'<br/><br/>agent_funcs_executor.invoke(<br/>    {'input': user_question, <br/>     'agent_scratchpad': []})<br/><br/><br/>agent_tools_executor.invoke(<br/>    {'input': user_question, <br/>     'agent_scratchpad': []})<br/><br/># In December 2023, the number of customers in London was 4,286, and in Berlin,<br/># it was 2,143. The percentage difference between the number of customers <br/># in London and Berlin is -50.0%, indicating that London had twice <br/># as many customers as Berlin.</span></pre><p id="0644" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Interestingly, the agents returned the same correct result. It’s not so surprising since we used low temperatures.</p><p id="98eb" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Both agents performed well, but let’s compare how they work under the hood. We can switch on debug mode (execute <code class="cx pq pr ps pt b">langchain.debug = True</code> for it) and see the number of LLM calls and tokens used.</p><p id="f129" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">You can see the scheme depicting the calls for two agents below.</p></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qg"><img src="../Images/a63a4a9fcfd1c6bd41445b50fd5d422c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*m7zEijjgC2RZw-BauIa-KA.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Scheme by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="09e6" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The OpenAI functions agent did 4 LLM calls, while the OpenAI tools agent made just 3 ones because it could get MAUs for London and Berlin in one iteration. Overall, it leads to a lower number of used tokens and, hence, lower price:</p><ul class=""><li id="2bff" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pg ph pi bk">OpenAI tools agent — 1 537 tokens</li><li id="7c20" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe pg ph pi bk">OpenAI functions agent — 1 874 tokens (<em class="px">+21.9%</em>).</li></ul><p id="8429" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">So, I would recommend you consider using OpenAI tools agents. You can use it with both ChatGPT 4 Turbo and ChatGPT 3.5 Turbo.</p><p id="7eb3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We’ve revised our previous implementation of an LLM-powered analyst. So, it’s time to move on and teach our agent to pose follow-up questions.</p><h1 id="b392" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">Asking clarifying questions</h1><p id="5001" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">We would like to teach our agent to ask the user clarifying questions. The most reasonable way to teach LLM agents something new is to give them a tool. So, LangChain has a handy tool — <a class="af pj" href="https://python.langchain.com/docs/integrations/tools/human_tools" rel="noopener ugc nofollow" target="_blank">Human</a>.</p><blockquote class="pu pv pw"><p id="ee3f" class="nj nk px nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">There’s no rocket science in it. You can see the implementation <a class="af pj" href="https://api.python.langchain.com/en/latest/_modules/langchain_community/tools/human/tool.html#" rel="noopener ugc nofollow" target="_blank">here</a>. We can easily implement it ourselves, but it’s a good practice to use tools provided by the framework.</p></blockquote><p id="debc" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s initiate such a tool. We don’t need to specify any arguments unless we want to customise something, for example, a tool’s description or input function. See more details in <a class="af pj" href="https://api.python.langchain.com/en/latest/tools/langchain_community.tools.human.tool.HumanInputRun.html#" rel="noopener ugc nofollow" target="_blank">the documentation</a>.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="8605" class="qb og fq pt b bg qc qd l qe qf">from langchain.tools import HumanInputRun<br/>human_tool = HumanInputRun()</span></pre><p id="d67e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can look at the default tool’s description and arguments.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="66aa" class="qb og fq pt b bg qc qd l qe qf">print(human_tool.description)<br/># You can ask a human for guidance when you think you got stuck or <br/># you are not sure what to do next. The input should be a question <br/># for the human. <br/><br/>print(human_tool.args)<br/># {'query': {'title': 'Query', 'type': 'string'}}</span></pre><p id="ca29" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s add this new tool to our agent’s toolkit and reinitialise the agent. I’ve also tweaked the system message to encourage the model to ask follow-up questions when it doesn’t have enough details.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="56e8" class="qb og fq pt b bg qc qd l qe qf"># tweaking the system message<br/>system_message = '''<br/>You are working as a product analyst for the e-commerce company. <br/>Your work is very important, since your product team makes decisions based on the data you provide. So, you are extremely accurate with the numbers you provided. <br/>If you're not sure about the details of the request, you don't provide the answer and ask follow-up questions to have a clear understanding.<br/>You are very helpful and try your best to answer the questions.<br/><br/>If you don't have enough context to answer question, you should ask user the follow-up question to get needed info. <br/>You don't make any assumptions about data requests. For example, if dates are not specified, you ask follow up questions. <br/>Always use tool if you have follow-up questions to the request.<br/>'''<br/>prompt = ChatPromptTemplate.from_messages([<br/>    ("system", system_message),<br/>    ("user", "{input}"),<br/>    MessagesPlaceholder(variable_name="agent_scratchpad")<br/>])<br/><br/># updated list of tools <br/>tools = [get_monthly_active_users, percentage_difference, human_tool]<br/><br/># reinitialising the agent<br/>human_input_agent = create_openai_tools_agent(<br/>    llm = ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview'),<br/>    tools = tools, <br/>    prompt = prompt<br/>)<br/><br/>human_input_agent_executor = AgentExecutor(<br/>    agent = human_input_agent, tools = tools, <br/>    verbose = True, max_iterations = 10, # early stopping criteria<br/>    early_stopping_method = 'generate')</span></pre><p id="f867" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, it’s time to try it out. The agent just returned the output, asking for a specific time period. It doesn’t work as we expected.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="4eee" class="qb og fq pt b bg qc qd l qe qf">human_input_agent_executor.invoke(<br/>    {'input': 'What are the number of customers in London?', <br/>     'agent_scratchpad': []})<br/><br/># {'input': 'What are the number of customers in London?',<br/>#  'agent_scratchpad': [],<br/>#  'output': 'To provide you with the number of customers in London, <br/>#             I need to know the specific time period you are interested in. <br/>#             Are you looking for the number of monthly active users in London <br/>#             for a particular month, or do you need a different metric? <br/>#             Please provide the time frame or specify the metric you need.'}</span></pre><p id="a2e4" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The agent didn’t understand that it needed to use this tool. Let’s try to fix it and change the Human tool’s description so that it is more evident for the agent when it should use this tool.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="dc34" class="qb og fq pt b bg qc qd l qe qf">human_tool_desc = '''<br/>You can use this tool to ask the user for the details related to the request. <br/>Always use this tool if you have follow-up questions. <br/>The input should be a question for the user. <br/>Be concise, polite and professional when asking the questions.<br/>'''<br/><br/>human_tool = HumanInputRun(<br/>    description = human_tool_desc<br/>)</span></pre><p id="8e13" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">After the change, the agent used the Human tool and asked for a specific time period. I provided an answer, and we got the correct result — 4 286 active customers in December 2023 for London.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qh"><img src="../Images/76aae445324f1a27b903fd3c4f1eafa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*32cRA3hWIqmhmZOWDFuxvw.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Screenshot by author</figcaption></figure><p id="a686" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">So, as usual, tweaking the prompt helps. Now, it works pretty well. Remember that creating a good prompt is an iterative process, and it’s worth trying several options and evaluating results.</p><p id="91f1" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We’ve taught our LLM agent to ask for details and take them into account while working on data requests.</p><p id="e712" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">However, it’s only part of the collaboration. In real life, analysts often get follow-up questions after providing any research. Now, our agent can’t keep up the conversation and address the new questions from the user since it doesn’t have any memory. It’s time to learn more about the tools we have to implement memory in LangChain.</p><blockquote class="pu pv pw"><p id="3990" class="nj nk px nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Actually, we already have a concept of memory in the current agent implementation. Our agent stores the story of its interactions with tools in the <code class="cx pq pr ps pt b">agent_scratchpad</code> variable. We need to remember not only interactions with tools but also the conversation with the user.</p></blockquote><h1 id="2e1a" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">Memory in LangChain</h1><p id="cbe4" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">By default, LLMs are stateless and don’t remember previous conversations. If we want our agent to be able to have long discussions, we need to store the chat history somehow. LangChain provides a bunch of different memory implementations. Let’s learn more about it.</p><p id="1701" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><code class="cx pq pr ps pt b">ConversationBufferMemory</code> is the most straightforward approach. It just saves all the context you pushed to it. Let’s try it out: initialise a memory object and add a couple of conversation exchanges.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="0a41" class="qb og fq pt b bg qc qd l qe qf">from langchain.memory import ConversationBufferMemory<br/>memory = ConversationBufferMemory()<br/>memory.save_context(<br/>    {"input": "Hey, how are you? How was your weekend?"}, <br/>    {"output": "Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic."}<br/>)<br/>print(memory.buffer)<br/><br/># Human: Hey, how are you? How was your weekend?<br/># AI: Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.<br/><br/>memory.save_context(<br/>    {"input": "Could you please help me with the urgent request from our CEO. What are the absolute numbers and the percentage difference between the number of customers in London and Berlin in December 2023?"}, <br/>    {"output": "In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin."}<br/>)<br/>print(memory.buffer)<br/><br/># Human: Hey, how are you? How was your weekend?<br/># AI: Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.<br/># Human: Could you please help me with the urgent request from our CEO. What are the absolute numbers and the percentage difference between the number of customers in London and Berlin in December 2023?<br/># AI: In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin.</span></pre><p id="e719" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This approach works well. However, in many cases, it’s not feasible to pass the whole previous conversation to LLM on each iteration because:</p><ul class=""><li id="e0d6" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pg ph pi bk">we might hit the context length limit,</li><li id="fb7f" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe pg ph pi bk">LLMs are not so good at dealing with long texts,</li><li id="8797" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe pg ph pi bk">we are paying for tokens, and such an approach might become quite expensive.</li></ul><p id="8311" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">So there’s another implementation, <code class="cx pq pr ps pt b">ConversationBufferWindowMemory</code>, that can store a limited number of conversation exchanges. So, it will store only the last k iterations.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="35be" class="qb og fq pt b bg qc qd l qe qf">from langchain.memory import ConversationBufferWindowMemory<br/><br/>memory = ConversationBufferWindowMemory(k = 1) <br/><br/>memory.save_context(<br/>    {"input": "Hey, how are you? How was your weekend?"}, <br/>    {"output": "Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic."}<br/>)<br/>print(memory.buffer)<br/><br/># Human: Hey, how are you? How was your weekend?<br/># AI: Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.<br/><br/>memory.save_context(<br/>    {"input": "Could you please help me with the urgent request from our CEO. What are the absolute numbers and the percentage difference between the number of customers in London and Berlin in December 2023?"}, <br/>    {"output": "In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin."}<br/>)<br/>print(memory.buffer)<br/><br/># Human: Could you please help me with the urgent request from our CEO. What are the absolute numbers and the percentage difference between the number of customers in London and Berlin in December 2023?<br/># AI: In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin.</span></pre><blockquote class="pu pv pw"><p id="02bd" class="nj nk px nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We’ve used <code class="cx pq pr ps pt b">k = 1</code> just to show how it works. In real-life use cases, you will likely use much higher thresholds.</p></blockquote><p id="3e75" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This approach can help you to keep chat history size manageable. However, it has a drawback: you can still hit the context size limit since you don’t control the chat history size in tokens.</p><p id="e50f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">To address this challenge, we can use <code class="cx pq pr ps pt b">ConversationTokenBufferMemory</code>. It won’t split statements, so don’t worry about incomplete sentences in the context.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="746a" class="qb og fq pt b bg qc qd l qe qf">from langchain.memory import ConversationTokenBufferMemory<br/><br/>memory = ConversationTokenBufferMemory(<br/>    llm = ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview'), <br/>    max_token_limit=100)<br/><br/>memory.save_context(<br/>    {"input": "Hey, how are you? How was your weekend?"}, <br/>    {"output": "Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic."}<br/>)<br/>print(memory.buffer)<br/><br/># Human: Hey, how are you? How was your weekend?<br/># AI: Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.<br/><br/># &lt;Comment from the author&gt;: the whole info since it fits the memory size <br/><br/>memory.save_context(<br/>    {"input": "Could you please help me with the urgent request from our CEO. What are the absolute numbers and the percentage difference between the number of customers in London and Berlin in December 2023?"}, <br/>    {"output": "In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin."}<br/>)<br/>print(memory.buffer)<br/><br/># AI: In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin.<br/><br/># &lt;Comment from the author&gt;: only the last response from the LLM fit the memory size </span></pre><p id="6e36" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In this case, we need to pass an LLM model to initialise a memory object because LangChain needs to know the model to calculate the number of tokens.</p><p id="d8cb" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In all approaches we’ve discussed above, we stored the exact conversation or at least parts of it. However, we don’t need to do it. For example, people usually don’t remember their conversations exactly. I can’t reproduce yesterday’s meeting’s content word by word, but I remember the main ideas and action items — a summary. Since humans are GI (General Intelligence), it sounds reasonable to leverage this strategy for LLMs as well. LangChain implemented it in <code class="cx pq pr ps pt b">ConversationSummaryBufferMemory</code>.</p><p id="cce1" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s try it in practice: initiate the memory and save the first conversation exchange. We got the whole conversation since our current context hasn’t hit the threshold.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="05bd" class="qb og fq pt b bg qc qd l qe qf">from langchain.memory import ConversationSummaryBufferMemory<br/><br/>memory = ConversationSummaryBufferMemory(<br/>    llm = ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview'), <br/>    max_token_limit=100)<br/><br/>memory.save_context(<br/>    {"input": "Hey, how are you? How was your weekend?"}, <br/>    {"output": "Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic."}<br/>)<br/>print(memory.load_memory_variables({})['history'])<br/><br/># Human: Hey, how are you? How was your weekend?<br/># AI: Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.</span></pre><p id="a75e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s add one more conversation exchange. Now, we’ve hit the limit: the whole chat history exceeds 100 tokens, the specified threshold. So, only the last AI response is stored (it’s within the 100 tokens limit). For earlier messages, the summary has been generated.</p><p id="b29c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The summary is stored with the prefix <code class="cx pq pr ps pt b">System:</code> .</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="2f30" class="qb og fq pt b bg qc qd l qe qf"><br/>memory.save_context(<br/>    {"input": "Could you please help me with the urgent request from our CEO. What are the absolute numbers and the percentage difference between the number of customers in London and Berlin in December 2023?"}, <br/>    {"output": "In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin."}<br/>)<br/>print(memory.load_memory_variables({})['history'])<br/><br/># System: The AI had a good weekend learning about LLM agents and describes it as magical. The human requests assistance with an urgent task from the CEO, asking for the absolute numbers and percentage difference of customers in London and Berlin in December 2023.<br/># AI: In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin.</span></pre><p id="46af" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As usual, it’s interesting to see how it works under the hood, and we can understand it in a debug mode. When the conversation exceeded the limit on the memory size, the LLM call was made with the following prompt:</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="eceb" class="qb og fq pt b bg qc qd l qe qf">Human: Progressively summarize the lines of conversation provided, <br/>adding onto the previous summary returning a new summary.<br/><br/>EXAMPLE<br/>Current summary:<br/>The human asks what the AI thinks of artificial intelligence. The AI <br/>thinks artificial intelligence is a force for good.<br/><br/>New lines of conversation:<br/>Human: Why do you think artificial intelligence is a force for good?<br/>AI: Because artificial intelligence will help humans reach their full <br/>potential.<br/><br/>New summary:<br/>The human asks what the AI thinks of artificial intelligence. The AI thinks <br/>artificial intelligence is a force for good because it will help humans reach <br/>their full potential.<br/>END OF EXAMPLE<br/><br/>Current summary:<br/><br/><br/>New lines of conversation:<br/>Human: Hey, how are you? How was your weekend?<br/>AI: Good morning, I had a wonder time off and spent the whole day learning <br/>about LLM agents. It works like magic.<br/>Human: Could you please help me with the urgent request from our CEO. <br/>What are the absolute numbers and the percentage difference between <br/>the number of customers in London and Berlin in December 2023?<br/><br/>New summary:</span></pre><p id="5c90" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It implements the progressive update of the summary. So, it uses fewer tokens, not passing the whole chat history every time to get an updated summary. That’s reasonable.</p><p id="a8ef" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Also, LangChain has more advanced memory types:</p><ul class=""><li id="df83" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pg ph pi bk">Vector data memory — storing texts’ embeddings in vector stores (similar to what we did in RAG — Retrieval Augmented Generation), then we could retrieve the most relevant bits of information and include them into the conversation. This memory type would be the most useful for long-term conversations.</li><li id="1b5d" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe pg ph pi bk">Entity memories to remember details about specific entities (i.e. people).</li></ul><p id="41de" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">You can even combine different memory types. For example, you can use conversation memory + entity memory to keep details about the tables in the database. To learn more about combined memory, consult <a class="af pj" href="https://python.langchain.com/docs/modules/memory/multiple_memory" rel="noopener ugc nofollow" target="_blank">the documentation</a>.</p><p id="7b84" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We won’t discuss these more advanced approaches in this article.</p><p id="5d7b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We’ve got an understanding of how we can implement memory in LangChain. Now, it’s time to use this knowledge for our agent.</p><h2 id="d9a4" class="qi og fq bf oh qj qk ql ok qm qn qo on ns qp qq qr nw qs qt qu oa qv qw qx qy bk">Adding memory to the agent</h2><p id="a02e" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">Let’s try to see how the current agent implementation works with the follow-up questions from the user.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="85a7" class="qb og fq pt b bg qc qd l qe qf">human_input_agent_executor.invoke(<br/>    {'input': 'What are the number of customers in London in December 2023?', <br/>     'agent_scratchpad': []})</span></pre><p id="849c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">For this call, the agent executed a tool and returned the correct answer: <code class="cx pq pr ps pt b">The number of active customers in London in December 2023 was 4,286.</code></p><p id="750f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We know the number of users for London. It would be interesting to learn about Berlin as well. Let’s ask our agent.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="ef3e" class="qb og fq pt b bg qc qd l qe qf">human_input_agent_executor.invoke(<br/>    {'input': 'And what about Berlin?', <br/>     'agent_scratchpad': []})</span></pre><p id="bb54" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Surprisingly, the agent was able to handle this question correctly. However, it had to clarify the questions using the Human tool, and the user had to provide the same information (not the best customer experience).</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qz"><img src="../Images/9031d4e99965bb6760a04deb221449a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZaRjfCJe3ETcUl-2qlyEiA.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Screenshot by author</figcaption></figure><p id="6507" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, let’s start holding the chart history for the agent. I will use a simple buffer that stores the complete previous conversation, but you could use a more complex strategy.</p><p id="f9b4" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">First, we need to add a placeholder for the chat history to the prompt template. I’ve marked it as optional.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="11e0" class="qb og fq pt b bg qc qd l qe qf">prompt = ChatPromptTemplate.from_messages([<br/>    ("system", system_message),<br/>    MessagesPlaceholder(variable_name="chat_history", optional=True),<br/>    ("user", "{input}"),<br/>    MessagesPlaceholder(variable_name="agent_scratchpad")<br/>])</span></pre><p id="31e6" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Next, let’s initialise a memory and save a small talk (it’s impossible to have a chat without a small talk, you know). Note that we’ve specified the same <code class="cx pq pr ps pt b">memory_key = 'chat_history’</code> as in the prompt template.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="f10d" class="qb og fq pt b bg qc qd l qe qf">memory = ConversationBufferMemory(<br/>    return_messages=True, memory_key="chat_history")<br/><br/>memory.save_context(<br/>    {"input": "Hey, how are you? How was your weekend?"}, <br/>    {"output": "Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic."}<br/>)<br/>print(memory.buffer)</span></pre><p id="19de" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s try the previous use case once again and ask the LLM analyst about the number of users in London.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="3211" class="qb og fq pt b bg qc qd l qe qf">human_input_agent_executor.invoke(<br/>    {'input': 'What is the number of customers in London?'})<br/><br/># {'input': 'What is the number of customers in London?',<br/># 'chat_history': [<br/>#   HumanMessage(content='Hey, how are you? How was your weekend?'),<br/>#   AIMessage(content='Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.'),<br/>#   HumanMessage(content='What is the number of customers in London?'),<br/>#   AIMessage(content='The number of active customers in London for December 2023 is 4,286.')],<br/># 'output': 'The number of active customers in London for December 2023 is 4,286.'}</span></pre><p id="2e1f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">After answering the question, <code class="cx pq pr ps pt b">"Could you please specify the time period for which you would like to know the number of customers in London?"</code>, we got the correct answer and the conversation history between the agent and the user with all the previous statements, including the small talk.</p><p id="c974" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">If we ask the follow-up question about Berlin now, the agent will just return the number for December 2023 without asking for details because it already has it in the context.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="be0d" class="qb og fq pt b bg qc qd l qe qf">human_input_agent_executor.invoke(<br/>    {'input': 'What is the number for Berlin?'})<br/><br/># {'input': 'What is the number for Berlin?',<br/>#  'chat_history': [HumanMessage(content='Hey, how are you? How was your weekend?'),<br/>#    AIMessage(content='Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.'),<br/>#    HumanMessage(content='What is the number of customers in London?'),<br/>#    AIMessage(content='The number of active customers in London for December 2023 is 4,286.'),<br/>#    HumanMessage(content='What is the number for Berlin?'),<br/>#    AIMessage(content='The number of active customers in Berlin for December 2023 is 2,143.')],<br/>#  'output': 'The number of active customers in Berlin for December 2023 is 2,143.'}</span></pre><p id="bcea" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s look at the prompt for the first LLM call. We can see that all chat history was actually passed to the model.</p><pre class="ms mt mu mv mw py pt pz bp qa bb bk"><span id="dfa1" class="qb og fq pt b bg qc qd l qe qf">System: <br/>You are working as a product analyst for the e-commerce company. <br/>Your work is very important, since your product team makes decisions <br/>based on the data you provide. So, you are extremely accurate <br/>with the numbers you provided. <br/>If you're not sure about the details of the request, you don't provide <br/>the answer and ask follow-up questions to have a clear understanding.<br/>You are very helpful and try your best to answer the questions.<br/><br/>If you don't have enough context to answer question, you should ask user <br/>the follow-up question to get needed info. <br/>You don't make any assumptions about data requests. For example, <br/>if dates are not specified, you ask follow up questions. <br/>Always use tool if you have follow-up questions to the request.<br/><br/>Human: Hey, how are you? How was your weekend?<br/>AI: Good morning, I had a wonderful time off and spent the whole day <br/>learning about LLM agents. It works like magic.<br/>Human: What is the number of customers in London?<br/>AI: The number of active customers in London for December 2023 is 4,286.<br/>Human: What is the number for Berlin?</span></pre><p id="cf93" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">So, we’ve added the chat history to our LLM-powered analyst, and now it can handle somewhat long conversations and answer follow-up questions. That’s a significant achievement.</p><blockquote class="pu pv pw"><p id="1ca5" class="nj nk px nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">You can find the complete code on <a class="af pj" href="https://github.com/miptgirl/miptgirl_medium/blob/main/analyst_agent/agent_prototype_collaboration.ipynb" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p></blockquote><h1 id="0ea5" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">Summary</h1><p id="661e" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">In this article, we’ve taught our LLM-powered analyst how to collaborate with users. Now, it can ask clarifying questions if there’s not enough information in the initial request and even answer the follow-up question from the user.</p><p id="de1b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We’ve achieved such a significant improvement:</p><ul class=""><li id="043f" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe pg ph pi bk">by adding a tool — Human input that allows to ask the user,</li><li id="1cca" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe pg ph pi bk">by adding a memory to the agent that can store the chat history.</li></ul><p id="79bc" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Our agent has mastered collaboration now. In one of the following articles, we will try to take the next step and combine LLM agents with RAG (Retrieval Augmented Generation). We’ve understood how to query databases and communicate with the users. The next step is to start using knowledge bases. Stay tuned!</p><blockquote class="pu pv pw"><p id="59f3" class="nj nk px nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Thank you a lot for reading this article. I hope it was insightful to you. If you have any follow-up questions or comments, please leave them in the comments section.</p></blockquote></div></div></div></div>    
</body>
</html>