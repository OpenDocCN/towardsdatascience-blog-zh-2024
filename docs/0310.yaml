- en: An Overview of Contextual Bandits
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文Bandit概述
- en: 原文：[https://towardsdatascience.com/an-overview-of-contextual-bandits-53ac3aa45034?source=collection_archive---------1-----------------------#2024-02-02](https://towardsdatascience.com/an-overview-of-contextual-bandits-53ac3aa45034?source=collection_archive---------1-----------------------#2024-02-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/an-overview-of-contextual-bandits-53ac3aa45034?source=collection_archive---------1-----------------------#2024-02-02](https://towardsdatascience.com/an-overview-of-contextual-bandits-53ac3aa45034?source=collection_archive---------1-----------------------#2024-02-02)
- en: A dynamic approach to treatment personalization
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种治疗个性化的动态方法
- en: '[](https://medium.com/@uguryi?source=post_page---byline--53ac3aa45034--------------------------------)[![Ugur
    Yildirim](../Images/33db36531a170c9621504f466d61334b.png)](https://medium.com/@uguryi?source=post_page---byline--53ac3aa45034--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--53ac3aa45034--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--53ac3aa45034--------------------------------)
    [Ugur Yildirim](https://medium.com/@uguryi?source=post_page---byline--53ac3aa45034--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@uguryi?source=post_page---byline--53ac3aa45034--------------------------------)[![Ugur
    Yildirim](../Images/33db36531a170c9621504f466d61334b.png)](https://medium.com/@uguryi?source=post_page---byline--53ac3aa45034--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--53ac3aa45034--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--53ac3aa45034--------------------------------)
    [Ugur Yildirim](https://medium.com/@uguryi?source=post_page---byline--53ac3aa45034--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--53ac3aa45034--------------------------------)
    ·19 min read·Feb 2, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--53ac3aa45034--------------------------------)
    ·19分钟阅读·2024年2月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Outline
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大纲
- en: '[Introduction](#bc08)'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[介绍](#bc08)'
- en: '[When To Use Contextual Bandits](#0f83)'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[何时使用上下文Bandit](#0f83)'
- en: 2.1\. [Contextual Bandit vs Multi-Armed Bandit vs A/B Testing](#9c98)
  id: totrans-9
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.1\. [上下文Bandit与多臂赌博机（MAB）与A/B测试](#9c98)
- en: 2.2\. [Contextual Bandit vs Multiple MABs](#0b64)
  id: totrans-10
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.2\. [上下文Bandit与多个多臂赌博机（MAB）](#0b64)
- en: 2.3\. [Contextual Bandit vs Multi-Step Reinforcement Learning](#fc98)
  id: totrans-11
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.3\. [上下文Bandit与多步强化学习](#fc98)
- en: 2.4\. [Contextual Bandit vs Uplift Modeling](#e404)
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.4\. [上下文Bandit与提升建模](#e404)
- en: '[Exploration and Exploitation in Contextual Bandits](#6c33)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[上下文Bandit中的探索与利用](#6c33)'
- en: 3.1\. [*ε*-greedy](#635e)
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3.1\. [*ε*-贪婪策略](#635e)
- en: 3.2\. [Upper Confidence Bound (UCB)](#7841)
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3.2\. [上置信界限（UCB）](#7841)
- en: 3.3\. [Thompson Sampling](#31b8)
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3.3\. [汤普森采样](#31b8)
- en: '[Contextual Bandit Algorithm Steps](#49b9)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[上下文Bandit算法步骤](#49b9)'
- en: '[Offline Policy Evaluation in Contextual Bandits](#e7aa)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[上下文Bandit中的离线策略评估](#e7aa)'
- en: 5.1\. [OPE Using Causal Inference Methods](#3f89)
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 5.1\. [使用因果推断方法的OPE](#3f89)
- en: 5.2\. [OPE Using Sampling Methods](#c117)
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 5.2\. [使用采样方法的OPE](#c117)
- en: '[Contextual Bandit in Action](#6403)'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[上下文Bandit实践](#6403)'
- en: '[Conclusion](#92a6)'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[结论](#92a6)'
- en: '[Acknowledgements](#7629)'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[致谢](#7629)'
- en: '[References](#5b07)'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[参考文献](#5b07)'
- en: 1\. Introduction
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: 'Imagine a scenario where you just started an A/B test that will be running
    for the next two weeks. However, after just a day or two, it is becoming increasingly
    clear that version A is working better for certain types of users, whereas version
    B is working better for another set of users. You think to yourself: Perhaps I
    should re-route the traffic such that users are getting more of the version that
    is benefiting them more, and less of the other version. Is there a principled
    way to achieve this?'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下这样的场景：你刚刚开始了一个为期两周的A/B测试。然而，仅仅一两天后，越来越明显地发现版本A对于某些类型的用户效果更好，而版本B对于另一组用户效果更佳。你心想：也许我应该重新引导流量，让用户更多地接触到对他们更有益的版本，而少接触到另一个版本。有没有一种有原则的方法可以实现这一点？
- en: Contextual bandits are a class of one-step reinforcement learning algorithms
    specifically designed for such treatment personalization problems where we would
    like to dynamically adjust traffic based on which treatment is working for whom.
    Despite being incredibly powerful in what they can achieve, they are one of the
    lesser known methods in Data Science, and I hope that this post will give you
    a comprehensive introduction to this amazing topic. Without further ado, let’s
    dive right in!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文赌博者（Contextual Bandits）是一类一阶强化学习算法，专门为这种治疗个性化问题设计，在这种问题中，我们希望根据哪种治疗对谁有效来动态调整流量。尽管它们在可以实现的效果上极为强大，但它们是数据科学中较少为人知的算法之一，我希望这篇文章能为你提供一个全面的介绍，帮助你了解这个令人惊叹的话题。事不宜迟，让我们直接深入了解吧！
- en: 2\. When To Use Contextual Bandits
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 何时使用上下文赌博者
- en: If you are just getting started with contextual bandits, it can be confusing
    to understand how contextual bandits are related to other more widely known methods
    such as A/B testing, and why you might want to use contextual bandits instead
    of those other methods. Therefore, we start our journey by discussing the similarities
    and differences between contextual bandits and related methods.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你刚刚开始了解上下文赌博者，可能会对上下文赌博者与其他更广为人知的方法（如A/B测试）之间的关系感到困惑，以及为什么你可能会选择使用上下文赌博者而不是其他方法。因此，我们将从讨论上下文赌博者与相关方法之间的相似性和差异性开始我们的旅程。
- en: 2.1\. Contextual Bandit vs Multi-Armed Bandit vs A/B Testing
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1\. 上下文赌博者 vs 多臂赌博者 vs A/B 测试
- en: Let us start with the most basic A/B testing setting that allocates traffic
    into treatment and control in a *static* fashion. For example, a data scientist
    might decide to run an A/B test for two weeks with 50% of traffic going to treatment
    and 50% going to control. What this means is that regardless of whether we are
    on the first day of the test or the last, we will be assigning users to control
    or treatment with 50% probability.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最基本的A/B测试设置开始，该设置将流量以*静态*的方式分配到治疗组和控制组。例如，一个数据科学家可能决定进行为期两周的A/B测试，50%的流量分配给治疗组，50%分配给控制组。这意味着无论我们处于测试的第一天还是最后一天，我们都将以50%的概率将用户分配给治疗组或控制组。
- en: On the other hand, if the data scientist were to use a multi-armed bandit (MAB)
    instead of an A/B test in this case, then traffic will be allocated to treatment
    and control in a *dynamic* fashion. In other words, traffic allocations in a MAB
    will change as days go by. For example, if the algorithm decides that treatment
    is doing better than control on the first day, the traffic allocation can change
    from 50% treatment and 50% control to 60% treatment vs 40% control on the second
    day, and so on.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果数据科学家在这种情况下使用多臂赌博者（MAB）而不是A/B测试，那么流量将以*动态*的方式分配到治疗组和控制组。换句话说，MAB中的流量分配将随着时间的推移而变化。例如，如果算法在第一天判断治疗组优于控制组，那么流量分配可能会从第一天的50%治疗和50%控制调整为第二天的60%治疗和40%控制，依此类推。
- en: Despite allocating traffic dynamically, MAB ignores an important fact, which
    is that not all users are the same. This means that a treatment that is working
    for one type of user might not work for another. For example, it might be the
    case that while treatment is working better for core users, control is actually
    better for casual users. In this case, even if treatment is better overall, we
    can actually get more value from our application if we assign more core users
    to treatment and more casual users to control.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管动态分配流量，MAB忽略了一个重要事实，那就是并不是所有用户都一样。这意味着对某一类用户有效的治疗方法可能对另一类用户无效。例如，可能出现这种情况：尽管治疗对核心用户更有效，但对休闲用户而言，控制组实际上更好。在这种情况下，即使治疗方法整体上更有效，我们如果将更多核心用户分配给治疗组，将更多休闲用户分配给控制组，实际上可以从我们的应用中获得更多价值。
- en: This is exactly where contextual bandits (CB) come in. While MAB simply looks
    at whether treatment or control is doing better *overall*, CB focuses on whether
    treatment or control is doing better for a user with a given set of *characteristics*.
    The “context” in contextual bandits precisely refers to these user characteristics
    and is what differentiates it from MAB. For example, CB might decide to increase
    treatment allocation to 60% for core users but decrease treatment allocation to
    40% for casual users after observing first day’s data. In other words, CB will
    dynamically update traffic allocation taking user characteristics (core vs casual
    in this example) into account.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是上下文赌博机（CB）派上用场的地方。尽管多臂赌博机（MAB）只关注治疗组或对照组整体效果如何，*总体*上是否表现更好，CB则关注治疗组或对照组对于具有特定*特征*的用户表现如何。在上下文赌博机中，“上下文”恰恰指的就是这些用户特征，这也是其与多臂赌博机的区别所在。例如，CB可能会在观察到第一天的数据后，决定将核心用户的治疗分配提高到60%，而将普通用户的治疗分配降至40%。换句话说，CB会根据用户特征（在此例中为核心用户与普通用户）动态调整流量分配。
- en: The following table summarizes the key differences between A/B testing, MAB,
    and CB, and the figure that follows visualizes these ideas.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了A/B测试、多臂赌博机和上下文赌博机的关键区别，接下来的图表将可视化这些概念。
- en: '**Table 1: Differences Between A/B Testing, MAB, and CB**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**表1：A/B测试、多臂赌博机和上下文赌博机的区别**'
- en: '![](../Images/b064c54b71fa354aa2c4b4fcaa761b23.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b064c54b71fa354aa2c4b4fcaa761b23.png)'
- en: '**Figure 1: Traffic Allocations in A/B Testing, MAB, and CB**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1：A/B测试、多臂赌博机和上下文赌博机中的流量分配**'
- en: '![](../Images/dd8e19b91a7218c593729019b8ff710d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd8e19b91a7218c593729019b8ff710d.png)'
- en: 2.2\. Contextual Bandit vs Multiple MABs
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2\. 上下文赌博机与多个多臂赌博机
- en: At this point, you might be tempted to think that CB is nothing more than a
    set of multiple MABs running together. In fact, when the context we are interested
    in is a small one (e.g., we are only interested in whether a user is a core user
    or a casual user), we can simply run one MAB for core users and another MAB for
    casual users. However, as the context gets large (core vs casual, age, country,
    time since last active, etc.) it becomes impractical to run a separate MAB for
    each unique context value.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 到此，你可能会产生这样的想法：上下文赌博机（CB）不过是多个多臂赌博机（MAB）同时运行的集合。事实上，当我们关注的上下文较小（例如，我们只关心一个用户是核心用户还是普通用户）时，我们可以简单地为核心用户运行一个MAB，为普通用户运行另一个MAB。然而，当上下文变得庞大（例如核心与普通用户、年龄、国家、上次活跃时间等）时，为每个独特的上下文值运行一个单独的MAB变得不切实际。
- en: The real value of CB emerges in this case through the use of *models* to describe
    the relationship of the experimental conditions in different contexts to our outcome
    of interest (e.g., conversion). As opposed to enumerating through each context
    value and treating them independently, the use of models allows us to share information
    from different contexts and makes it possible to handle large context spaces.
    This idea of a model will be discussed at several different points in this post,
    so keep on reading to learn more.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，上下文赌博机的真正价值体现在通过使用*模型*来描述不同上下文中的实验条件与我们关注的结果（例如转化率）之间的关系。与逐一列举每个上下文值并将其独立处理不同，使用模型可以让我们共享来自不同上下文的信息，从而处理更大的上下文空间。这个模型的概念将在本文的多个部分进行讨论，因此请继续阅读以了解更多内容。
- en: 2.3\. Contextual Bandit vs Multi-Step Reinforcement Learning
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3\. 上下文赌博机与多步强化学习
- en: The introduction referred to CB as a class of one-step reinforcement learning
    (RL) algorithms. So, what exactly is the difference between one-step and multi-step
    RL? And what makes CB one-step? The fundamental difference between CB and multi-step
    RL is that in CB we assume the actions the algorithm takes (e.g., serve treatment
    or control to a specific user) don’t affect the future states of the overall system.
    In other words, the state (or “context” as is more appropriately called in CB)
    affects what action we take, but that action we took does *not* in turn impact
    or change the state. The following figure summarizes this distinction.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 引言中将上下文赌博机（CB）称为一种单步强化学习（RL）算法。那么，单步与多步强化学习到底有什么区别呢？是什么使得CB成为单步学习？上下文赌博机与多步强化学习的根本区别在于，在CB中，我们假设算法所采取的行动（例如，为特定用户提供治疗或对照组）不会影响系统整体的未来状态。换句话说，状态（或在CB中更适当称为“上下文”）影响我们采取的行动，但我们采取的行动*不会*反过来影响或改变状态。下图总结了这一区别。
- en: '**Figure 2: Contextual Bandit vs Multi-Step RL**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2：上下文赌博机与多步强化学习**'
- en: '![](../Images/99dd1e47664a6110fcef41adf27f1326.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99dd1e47664a6110fcef41adf27f1326.png)'
- en: Image by author, inspired by [source](/contextual-bandits-and-reinforcement-learning-6bdfeaece72a)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供，灵感来源于[source](/contextual-bandits-and-reinforcement-learning-6bdfeaece72a)
- en: A few examples should make this distinction clearer. Let’s say that we are building
    a system to decide what ads to show to users based on their age. We would expect
    that users from different age groups may find different ads more relevant to them,
    which means that a user’s age should affect what ads we should show them. However,
    the ad we showed them doesn’t in turn affect their age, so the one-step assumption
    of CB seems to hold. However, if we move one step further and find out that serving
    expensive ads deplete our inventory (and limit which ads we can serve in the future)
    or that the ad we show today affect whether the user will visit our site again,
    then the one-step assumption is indirectly inviolated, so we may want to consider
    developing a full-blown RL system instead.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一些例子应该能让这个区别更加清晰。假设我们正在构建一个系统，根据用户的年龄决定展示哪些广告。我们预期，不同年龄段的用户可能会觉得不同的广告与他们更相关，这意味着用户的年龄应该影响我们展示给他们的广告。然而，我们展示的广告并不会反过来影响他们的年龄，所以CB的单步假设似乎成立。然而，如果我们进一步发现，展示昂贵的广告会消耗我们的库存（并限制未来我们能展示的广告），或者我们今天展示的广告会影响用户是否会再次访问我们的网站，那么单步假设就间接被违反了，因此我们可能需要考虑开发一个完整的强化学习（RL）系统。
- en: 'A note of caution though: While multi-step reinforcement learning is more flexible
    compared to contextual bandits, it’s also more complicated to implement. So, if
    the problem at hand can be accurately framed as a one-step problem (even though
    it looks like a multi-step problem at first glance), contextual bandits could
    be the more practical approach.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是：虽然与上下文赌博机相比，多步强化学习更加灵活，但它的实现也更加复杂。因此，如果当前的问题能够准确地被框定为一个单步问题（即使乍一看像是多步问题），上下文赌博机可能是更实际的解决方案。
- en: 2.4\. Contextual Bandit vs Uplift Modeling
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4. 上下文赌博机与提升建模
- en: Before moving on to discussing different CB algorithms, I would also like to
    briefly touch upon the connection between CB and uplift modeling. An uplift model
    is usually built on top of A/B test data to discover the relationship between
    the treatment effect (uplift) and user characteristics. The results from such
    a model can then be used to personalize treatments in the future. For example,
    if the uplift model discovers that certain users are more likely to benefit from
    a treatment, then only those types of users might be given the treatment in the
    future.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续讨论不同的CB算法之前，我还想简要地提及一下CB与提升建模之间的关系。提升模型通常是基于A/B测试数据构建的，用于发现处理效果（提升）与用户特征之间的关系。然后，可以使用该模型的结果来个性化未来的处理。例如，如果提升模型发现某些用户更可能从某个处理方法中受益，那么未来可能只会将该处理方法提供给这些类型的用户。
- en: Given this description of uplift modeling, it should be clear that both CB and
    uplift modeling are solutions to the personalization problem. The key difference
    between them is that CB approaches this problem in a more dynamic way in the sense
    that personalization happens *on-the-fly* instead of waiting for results from
    an A/B test. At a conceptual level, CB can very loosely be thought of as A/B testing
    and uplift modeling happening concurrently instead of sequentially. Given the
    focus of this post, I won’t be discussing uplift modeling further, but there are
    several great resources to learn more about it such as [[1]](https://www.uber.com/blog/research/uplift-modeling-for-multiple-treatments-with-cost-optimization/).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 给定对提升建模的描述，应该很清楚，CB（上下文赌博机）和提升建模都是个性化问题的解决方案。它们之间的关键区别在于，CB以一种更动态的方式解决这个问题，个性化发生在*即时*的过程中，而不是等待A/B测试的结果。从概念层面上讲，CB可以非常宽泛地被看作是A/B测试和提升建模同时发生，而不是顺序发生。考虑到本文的重点，我不会进一步讨论提升建模，但有几个很好的资源可以了解更多相关内容，例如[[1]](https://www.uber.com/blog/research/uplift-modeling-for-multiple-treatments-with-cost-optimization/)。
- en: 3\. Exploration and Exploitation in Contextual Bandits
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 上下文赌博机中的探索与开发
- en: 'Above we discussed how CB dynamically allocates traffic depending on whether
    treatment or control is doing better for a given group of users at a given point
    in time. This raises an important question: How aggressive do we want to be when
    we are making these traffic allocation changes? For example, if after just one
    day of data we decide that treatment is working better for users from the US,
    should we completely stop serving control to US users?'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 上文我们讨论了CB如何根据某个给定时间点、特定用户群体的治疗组和对照组的表现，动态分配流量。这引出了一个重要的问题：当我们进行这些流量分配调整时，我们希望多么激进？例如，如果在一天的数据之后，我们决定治疗组对美国用户表现更好，是否应该完全停止对美国用户提供对照组？
- en: I’m sure most of you would agree that this would be a bad idea, and you would
    be correct. The main problem with changing traffic allocations this aggressively
    is that making inferences based on insufficient amounts of data can lead to erroneous
    conclusions. For example, it might be that the first day of data we gathered is
    actually not representative of dormant users and that in reality control is better
    for them. If we stop serving control to US users after the first day, we will
    never be able to learn this correct relationship.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信大多数人都会同意，这个做法是一个糟糕的主意，而且你们是对的。过于激进地改变流量分配的主要问题是，基于不足的数据进行推断可能导致错误的结论。例如，可能第一天的数据实际上并不代表沉睡用户的情况，实际上对照组对他们更好。如果我们在第一天之后就停止向美国用户提供对照组，我们将永远无法了解这种正确的关系。
- en: A better approach to dynamically updating traffic allocations is striking the
    right balance between exploitation (serve the best experimental condition based
    on the data so far) and exploration (continue to serve other experimental conditions
    as well). Continuing with the previous example, if data from the first day indicate
    that treatment is better for US users, we can serve treatment to these users with
    an increased probability the next day while still allocating a reduced but non-zero
    fraction to control.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的动态更新流量分配的方法是，在利用（基于目前的数据提供最佳实验条件）和探索（继续为其他实验条件提供服务）之间找到合适的平衡。延续前面的例子，如果第一天的数据表明治疗组对美国用户更好，我们可以在第二天通过更高的概率为这些用户提供治疗，同时仍然为对照组分配一个减少但非零的比例。
- en: There are numerous exploration strategies used in CB (and MAB) as well as several
    variations of them that try to strike this right balance between exploration and
    exploitation. Three popular strategies include ε-greedy, upper confidence bound,
    and Thompson sampling.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在CB（以及MAB）中有许多探索策略，还有一些变种试图在探索和利用之间找到合适的平衡。三种常见的策略包括ε-贪婪策略、上置信界限（UCB）和汤普森采样（Thompson
    sampling）。
- en: 3.1\. ε-greedy
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1. ε-贪婪策略
- en: In this strategy, we first decide which experimental condition is doing better
    for a given group of users at a given point in time. The simplest way to do this
    is by comparing the average target values (*y*) for each experimental condition
    for these users. More formally, we can decide the “winning” condition for a group
    of users by finding the condition *d* that has the higher value for
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个策略中，我们首先决定哪个实验条件在某个给定时间点对于某个特定用户群体表现更好。最简单的做法是通过比较这些用户在每个实验条件下的目标值（*y*）的平均值来实现。更正式地说，我们可以通过找出条件*d*，其值较高，从而决定该组用户的“获胜”条件。
- en: '![](../Images/aacbee75310f29271244345d8dbe6001.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aacbee75310f29271244345d8dbe6001.png)'
- en: where *n_dx* is the number of samples we have so far from users in condition
    *d* with context *x*, and *y_idx* is the target value for a given sample *i* in
    condition *d* with context *x*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*n_dx*是我们到目前为止从条件*d*和上下文*x*中的用户中获取的样本数量，*y_idx*是条件*d*和上下文*x*中第*i*个样本的目标值。
- en: After deciding which experimental condition is currently “best” for these users,
    we serve them that condition with *1-ε* probability (where *ε* is usually a small
    number such as 0.05) and serve a random experimental condition with probability
    *ε*. In reality, we might want to dynamically update our *ε* such that it is large
    at the beginning of the experiment (when more exploration is needed) and gradually
    gets smaller as we collect more and more data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定了哪个实验条件目前对这些用户是“最好的”之后，我们以*1-ε*的概率为他们提供该条件（其中*ε*通常是一个较小的数字，如0.05），并以*ε*的概率提供一个随机的实验条件。实际上，我们可能希望动态更新我们的*ε*，使其在实验开始时较大（此时需要更多的探索），随着我们收集更多数据，*ε*逐渐变小。
- en: Additionally, context *X* might be high-dimensional (country, gender, platform,
    tenure, etc.) so we might want to use a model to get these *y* estimates to deal
    with the curse of dimensionality. Formally, the condition to serve can be decided
    by finding the condition *d* that has the higher value for
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，上下文 *X* 可能是高维的（例如，国家、性别、平台、任期等），因此我们可能需要使用模型来获取这些 *y* 估计值，从而应对维度灾难。形式上，服务的条件可以通过找到条件
    *d* 使其值更高来决定。
- en: '![](../Images/c4eed6bc6b2d15fe9be0299c6dc27931.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4eed6bc6b2d15fe9be0299c6dc27931.png)'
- en: where *x^T* is an *m*-dimensional row-vector of context values and *θ_d* is
    an *m*-dimensional column-vector of learnable parameters associated with condition
    *d*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x^T* 是一个 *m* 维的行向量，表示上下文值，*θ_d* 是一个 *m* 维的列向量，表示与条件 *d* 相关的可学习参数。
- en: 3.2\. Upper Confidence Bound (UCB)
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2. 上置信度界限（UCB）
- en: This strategy decides the next condition to serve by looking at not only which
    condition has a higher *y* estimate but also our precision of (or confidence in)
    that estimate. In a simple MAB setting, precision can be thought to be a function
    of how many times a given condition has already been served so far. In particular,
    a condition that (i) has a high average *y* (so it makes sense to exploit) or
    (ii) has not yet been served many times (so it needs more exploration) is more
    likely to be served next.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个策略通过查看不仅是哪个条件具有更高的 *y* 估计值，还包括我们对该估计值的精确度（或信心），来决定下一个要服务的条件。在一个简单的 MAB 设置中，精确度可以被视为已经服务过多少次某个条件的函数。特别地，一个条件（i）具有较高的平均
    *y*（所以值得利用），或者（ii）还没有被服务过很多次（所以需要更多的探索），更有可能被下一个选择来服务。
- en: We can generalize this idea to the CB setting by keeping track of how many times
    different conditions are served in different contexts. Assuming a simple setting
    with a low-dimensional context *X* such that CB can be thought of as just multiple
    MABs running together, we can select the next condition to serve based on which
    condition *d* has the higher value for
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过跟踪不同条件在不同上下文中被服务的次数，将这一思想推广到 CB 设置。假设在一个简单的设置中，上下文 *X* 是低维的，这样 CB 可以看作是多个
    MAB 的组合运行，我们可以基于哪个条件 *d* 的值更高来选择下一个要服务的条件。
- en: '![](../Images/c81f49f38f4edeb81ae4e4b633191981.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c81f49f38f4edeb81ae4e4b633191981.png)'
- en: where *c* is some constant (to be selected based on how much emphasis we want
    to put on the precision of our estimate when exploring) and *n_x* is the number
    of times context *x* is seen so far.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *c* 是一个常数（根据我们希望在探索时对估计精确度的重视程度来选择），*n_x* 是上下文 *x* 到目前为止被看到的次数。
- en: However, in most cases, the context *X* will be high-dimensional, which means
    that just like in the *ε*-greedy case, we would need to make use of a model. In
    this setting, a condition *d* can be served next if it has the higher value for
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在大多数情况下，上下文 *X* 会是高维的，这意味着就像在 *ε*-贪婪情况下，我们需要使用模型。在这种设置中，如果某个条件 *d* 的值更高，那么它可以被选择为下一个要服务的条件。
- en: '![](../Images/56ecf5df3614fd3b09d19fbaafa735bb.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56ecf5df3614fd3b09d19fbaafa735bb.png)'
- en: where *SE(.)* is the standard error of our estimate (or more generally a metric
    that quantifies our current level of confidence in that estimate).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *SE(.)* 是我们估计的标准误差（或更广泛地说，是量化我们当前对该估计的信心水平的度量）。
- en: Note that there are several versions of UCB, so you will likely come across
    different formulas. A popular UCB method is LinUCB that formalizes the problem
    in a linear model framework (e.g., [[2]](/recommender-systems-using-linucb-a-contextual-multi-armed-bandit-approach-35a6f0eb6c4)).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，UCB 有多个版本，因此你可能会遇到不同的公式。一种流行的 UCB 方法是 LinUCB，它在一个线性模型框架中形式化了问题（例如，[[2]](/recommender-systems-using-linucb-a-contextual-multi-armed-bandit-approach-35a6f0eb6c4)）。
- en: 3.3\. Thompson Sampling
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3. 汤普森采样
- en: 'The third and final exploration strategy to be discussed is Thompson sampling,
    which is a Bayesian approach to solving the exploration-exploitation dilemma.
    Here, we have a model *f(D, X; Θ)* that returns predicted *y* values given experimental
    condition *D*, context *X*, and some set of learnable parameters *Θ*. This function
    gives us access to posterior distributions of expected *y* values for any condition-context
    pair, thus allowing us to choose the next condition to serve according to the
    probability that it yields the highest expected *y* given context. Thompson sampling
    naturally balances exploration and exploitation as we are sampling from the posterior
    and updating our model based on the observations. To make these ideas more concrete,
    here are the steps involved in Thompson sampling:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将要讨论的第三种也是最后一种探索策略是汤普森采样，它是一种解决探索与利用困境的贝叶斯方法。在这里，我们有一个模型*f(D, X; Θ)*，它根据实验条件*D*、上下文*X*和一些可学习的参数*Θ*返回预测的*y*值。这个函数让我们可以访问任何条件-上下文对的预期*y*值的后验分布，从而根据给定上下文下产生最高预期*y*的概率选择下一个要执行的条件。汤普森采样自然地平衡了探索与利用，因为我们是从后验分布中采样并根据观察结果更新我们的模型。为了让这些概念更加具体，以下是汤普森采样涉及的步骤：
- en: '![](../Images/6d027cd3e61c98ab31a0a13d2e6fbd3f.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d027cd3e61c98ab31a0a13d2e6fbd3f.png)'
- en: In practice, instead of having a single function we can also use a different
    function for each experimental condition (e.g., evaluate both *f_c(X; Θ_c)* and
    *f_t(X; Θ_t)* and then select the condition with the higher value). Furthermore,
    the update step usually takes place not after each sample but rather after seeing
    a batch of samples. For more details on Thompson sampling, you can refer to [[3]](https://arxiv.org/pdf/1707.02038.pdf)
    [[4]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7352306).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以使用不同的函数来处理每个实验条件，而不是使用单一的函数（例如，评估*f_c(X; Θ_c)*和*f_t(X; Θ_t)*，然后选择具有较高值的条件）。此外，更新步骤通常不会在每次样本之后进行，而是在看到一批样本之后进行。有关汤普森采样的更多细节，可以参考[[3]](https://arxiv.org/pdf/1707.02038.pdf)
    [[4]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7352306)。
- en: 4\. Contextual Bandit Algorithm Steps
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 上下文强盗算法步骤
- en: 'The previous section (especially the part on Thompson sampling) should already
    give you a pretty good sense of the steps involved in a CB algorithm. However,
    for the sake of completeness, here is a step-by-step description of a standard
    CB algorithm:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节（尤其是关于汤普森采样的部分）应该已经让你对CB算法的步骤有了一个相当清晰的了解。然而，为了完整性，以下是标准CB算法的逐步描述：
- en: A new data point arrives with context *X* (e.g., a core user with an iOS device
    in the US).
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个新的数据点到达，具有上下文*X*（例如，一个在美国使用iOS设备的核心用户）。
- en: Given this data point and the exploration strategy chosen (e.g., *ε*-greedy),
    the algorithm decides on a condition to serve this user (e.g., treatment or control).
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定这个数据点和选择的探索策略（例如，*ε*-贪婪策略），算法决定为该用户执行哪个条件（例如，治疗或对照）。
- en: After the condition is served, we observe the outcome *y* (e.g., whether the
    user made a purchase or not).
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在条件被执行后，我们观察到结果*y*（例如，用户是否进行了购买）。
- en: Update (or fully retrain) the model used in Step 2 after seeing the new data.
    (As mentioned previously, we usually make an update not after every sample but
    after seeing a batch of samples to ensure that updates are less noisy.)
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在看到新数据后更新（或完全重新训练）步骤2中使用的模型。（如前所述，我们通常不是在每次样本后进行更新，而是在看到一批样本后进行更新，以确保更新不那么嘈杂。）
- en: Repeat.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复。
- en: 5\. Offline Policy Evaluation in Contextual Bandits
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 上下文强盗中的离线策略评估
- en: So far we have only discussed how to implement a CB algorithm as *new* data
    come in. An equally important topic to cover is how to evaluate a CB algorithm
    using *old* (or *logged*) data. This is called offline evaluation or offline policy
    evaluation (OPE).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了如何在*新*数据到来时实现CB算法。另一个同样重要的话题是如何使用*旧*（或*已记录*）数据来评估CB算法。这被称为离线评估或离线策略评估（OPE）。
- en: 5.1\. OPE Using Causal Inference Methods
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1\. 使用因果推断方法进行OPE
- en: One way to do OPE is using well-known causal inference techniques such as Inverse
    Propensity Scoring (IPS) or the Doubly Robust (DR) method. Causal inference is
    appropriate here because we are essentially trying to estimate the counterfactual
    of what would have happened if a different policy served a different condition
    to a user. There is already a great Medium article on this topic [[5]](https://edoconti.medium.com/offline-policy-evaluation-run-fewer-better-a-b-tests-60ce8f93fa15),
    so here I will only briefly summarize the main idea from that piece and adapt
    it to our discussion.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 进行OPE的一种方法是使用广为人知的因果推断技术，如逆倾向得分（IPS）或双重稳健（DR）方法。因果推断在这里是合适的，因为我们实际上是在尝试估计一个反事实，即如果采用不同的策略为用户提供不同的条件会发生什么。这个话题已经有一篇很好的Medium文章介绍[[5]](https://edoconti.medium.com/offline-policy-evaluation-run-fewer-better-a-b-tests-60ce8f93fa15)，所以这里我将简要总结文章的主要思想，并将其应用到我们的讨论中。
- en: Taking IPS as an example, doing OPE usually requires us to know not only (i)
    the probability of assigning a given condition to a sample using our new CB algorithm
    but also (ii) the probability with which a given condition was assigned to a sample
    in the logged data. Take the following hypothetical logged data with *X_1-X_3*
    being context, *D* being the experimental condition, *P_O(D)* being the probability
    of assigning *D* to that user, and *y* being the outcome.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以IPS为例，进行OPE通常要求我们不仅知道（i）使用我们新的CB算法为样本分配给定条件的概率，还要知道（ii）在记录数据中样本被分配到给定条件的概率。假设以下是一个假设的记录数据，其中*X_1-X_3*是上下文，*D*是实验条件，*P_O(D)*是将*D*分配给该用户的概率，*y*是结果。
- en: '**Table 2: Example Logged Data From An A/B Test**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**表2：来自A/B测试的示例记录数据**'
- en: '![](../Images/fb236a7c9b9156c8c161d4dedfef5850.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb236a7c9b9156c8c161d4dedfef5850.png)'
- en: As you can see, in this example *P_O(D)* is always 0.6 for *D=1* and 0.4 for
    *D=0* regardless of the context, so the logged data can be assumed to come from
    an A/B test that assigns treatment with probability 0.6\. Now, if we want to test
    how a CB algorithm would have performed had we assigned conditions using a CB
    algorithm rather than a simple A/B test, we can use the following formula to get
    the IPS estimate of the cumulative *y* for CB
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在这个例子中，*P_O(D)* 对于*D=1*始终为0.6，对于*D=0*始终为0.4，无论上下文如何，因此可以假设记录的数据来自一个以0.6的概率分配处理的A/B测试。现在，如果我们想测试CB算法在我们使用CB算法而非简单A/B测试分配条件的情况下会如何表现，我们可以使用以下公式来获取CB的累积*y*的IPS估计值。
- en: '![](../Images/4a6d40fc68b5f0b3dda045a12c2a825a.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a6d40fc68b5f0b3dda045a12c2a825a.png)'
- en: where *n* is the number of samples in the logged data (which is 5 here) and
    *P_N(D_i)* is the probability of serving the logged *D* for *user_i* had we used
    the new CB algorithm instead (this probability will depend on the specific algorithm
    being evaluated).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*n*是记录数据中的样本数量（此处为5），*P_N(D_i)*是如果我们使用新的CB算法，给*user_i*分配记录中的*D*的概率（此概率将取决于被评估的具体算法）。
- en: Once we have this estimate, we can compare that to the observed cumulative *y*
    from the old A/B test (which is 1+0+0+1+1=3 here) to decide if the CB would have
    yielded a higher cumulative *y*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这个估计值，我们可以将其与旧A/B测试中观察到的累积*y*进行比较（在这里为1+0+0+1+1=3），以决定CB是否会产生更高的累积*y*。
- en: For more information on OPE using causal inference methods, please refer to
    the article linked at the beginning of the section. The article also links to
    a nice GitHub repo with lots of OPE implementations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用因果推断方法进行OPE的更多信息，请参考本节开头链接的文章。该文章还链接了一个非常好的GitHub仓库，其中包含许多OPE实现。
- en: A side note here is that this section discussed causal inference methods only
    as a technique used in OPE. However, in reality, one can also apply them while
    the CB algorithm is being run so as to “debias” the training data that the algorithm
    collects along the way. The reason why we might want to apply methods such as
    IPS to our training data is that the CB policy that generates this data is a non-uniform
    random policy by definition, so estimating causal effects from it to decide what
    action to take would benefit from using causal inference methods. If you would
    like to learn more about debiasing, please refer to [[6]](https://arxiv.org/pdf/1802.04064.pdf).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个旁注，本节讨论了因果推断方法作为OPE中的一种技术。然而，实际上，人们也可以在CB算法运行时应用这些方法，以“去偏”算法在过程中收集的训练数据。我们可能希望将像IPS这样的技术应用于训练数据的原因是，生成这些数据的CB策略本质上是一个非均匀的随机策略，因此，从中估计因果效应来决定采取什么行动时，使用因果推断方法会更有帮助。如果你想了解更多关于去偏的信息，请参考[[6]](https://arxiv.org/pdf/1802.04064.pdf)。
- en: 5.2\. OPE Using Sampling Methods
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2\. 使用采样方法进行 OPE
- en: 'Another way to do OPE is through the use of sampling methods. In particular,
    a very simple replay method [[7]](https://arxiv.org/pdf/1003.5956.pdf) can be
    used to evaluate a CB algorithm (or any other algorithm for that matter) using
    logged data from a randomized policy such as an A/B test. In its simplest form
    (where we assume a uniform random logging policy), the method works as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 进行 OPE 的另一种方法是通过使用采样方法。特别地，可以使用一种非常简单的重放方法[[7]](https://arxiv.org/pdf/1003.5956.pdf)来评估
    CB 算法（或任何其他算法），该方法使用来自随机化策略（如 A/B 测试）的日志数据。在最简单的形式下（假设我们使用均匀随机日志策略），该方法的工作原理如下：
- en: Sample the next user with context *X* from the logged data.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从日志数据中采样下一个具有上下文 *X* 的用户。
- en: Decide what condition to assign to that user using the new CB algorithm.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的 CB 算法决定分配给该用户的条件。
- en: If the selected condition matches the actual condition in the logged data, then
    add the observed *y* to the cumulative *y* counter. If it doesn’t match, ignore
    the sample.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果选定的条件与日志数据中的实际条件匹配，则将观察到的 *y* 添加到累积 *y* 计数器中。如果不匹配，则忽略该样本。
- en: Repeat until all samples are considered.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复直到所有样本都被考虑。
- en: If the logging policy doesn’t assign treatments uniformly at random, then the
    method needs to be slightly modified. One modification that the authors themselves
    mention is to use rejection sampling (e.g., [[8]](https://grail.cs.washington.edu/projects/nonstationaryeval/nonstationaryevalExtended.pdf))
    whereby we would accept samples from the majority treatment less often compared
    to the minority treatment in Step 3\. Alternatively, we could consider dividing
    the observed *y* by the propensity in Step 3 to similarly “down-weight” the more
    frequent treatment and “up-weight” the less frequent one.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果日志策略没有均匀随机地分配处理，则需要对该方法稍作修改。作者自己提到的一个修改是使用拒绝采样（例如，[[8]](https://grail.cs.washington.edu/projects/nonstationaryeval/nonstationaryevalExtended.pdf)），在第
    3 步中，我们接受来自多数处理的样本的频率会比少数处理的样本少。或者，我们可以考虑在第 3 步中将观察到的 *y* 除以倾向性，从而“下调”更频繁的处理，并“上调”较少频繁的处理。
- en: In the next section, I employ an even simpler method in my evaluation that uses
    up- and down-sampling with bootstrap to transform the original non-uniform data
    into a uniform one and then apply the method as it is.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我使用了一个更简单的方法来评估，它通过引导法进行上采样和下采样，将原始的不均匀数据转化为均匀数据，然后按原样应用该方法。
- en: 6\. Contextual Bandit in Action
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 上下文赌博机实际应用
- en: To demonstrate contextual bandits in action, I put together a [notebook](https://github.com/uguryi/contextual_bandit/blob/main/ab_vs_mab_vs_cb.ipynb)
    that generates a simulated dataset and compares the cumulative *y* (or “reward”)
    estimates for new A/B, MAB, and CB policies evaluated on this dataset. Many parts
    of the code in this notebook are taken from the Contextual Bandits chapter of
    an amazing book on Reinforcement Learning [[9]](https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python/blob/master/Chapter03/Contextual%20Bandits.ipynb)
    (highly recommended if you would like to dig deeper into Reinforcement Learning
    using Python) and two great posts by James LeDoux [[10]](https://jamesrledoux.com/algorithms/offline-bandit-evaluation/)
    [[11]](https://jamesrledoux.com/algorithms/bandit-algorithms-epsilon-ucb-exp-python/)
    and adapted to the setting we are discussing here.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示上下文赌博机的实际应用，我整理了一个 [notebook](https://github.com/uguryi/contextual_bandit/blob/main/ab_vs_mab_vs_cb.ipynb)，该
    notebook 生成一个模拟数据集并比较新 A/B、MAB 和 CB 策略在该数据集上的累积 *y*（或“奖励”）估计。该 notebook 中的许多代码来自一本关于强化学习的精彩书籍的“上下文赌博机”章节[[9]](https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python/blob/master/Chapter03/Contextual%20Bandits.ipynb)（如果你想深入了解使用
    Python 的强化学习，强烈推荐），以及 James LeDoux 的两篇精彩文章[[10]](https://jamesrledoux.com/algorithms/offline-bandit-evaluation/)
    [[11]](https://jamesrledoux.com/algorithms/bandit-algorithms-epsilon-ucb-exp-python/)，并已根据我们在此讨论的设置进行了调整。
- en: 'The setup is very simple: The original data we have comes from an A/B test
    that assigned treatment to users with probability 0.75 (so *not* uniformly at
    random). Using this randomized logged data, we would like to evaluate and compare
    the following three policies based on their cumulative *y*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 设置非常简单：我们拥有的原始数据来自一个 A/B 测试，该测试以 0.75 的概率分配处理给用户（即*非*均匀随机）。使用这些随机化的日志数据，我们希望基于它们的累积
    *y* 来评估和比较以下三种策略：
- en: A new A/B policy that randomly assigns treatment to users with probability 0.4.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种新的 A/B 策略，它以 0.4 的概率随机将处理分配给用户。
- en: A MAB policy that decides what treatment to assign next using an *ε*-greedy
    policy that doesn’t take context *X* into account.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个 MAB 策略，它使用*ε*-贪心策略决定下一步分配什么治疗，但不考虑上下文 *X*。
- en: A CB policy that decides what treatment to assign next using an *ε*-greedy policy
    that takes context *X* into account.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个 CB 策略，它使用*ε*-贪心策略决定下一步分配什么治疗，同时考虑上下文 *X*。
- en: I modified the original method described in the Li et al. paper such that instead
    of directly sampling from the simulated data (which is 75% treatment and only
    25% control in my example), I first down-sample treatment cases and up-sample
    control cases (both with replacement) to get a new dataset that is exactly 50%
    treatment and 50% control.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我修改了 Li 等人在论文中描述的原始方法，不再直接从模拟数据中采样（在我的例子中，模拟数据是75%治疗和仅25%控制），而是首先对治疗案例进行下采样，对控制案例进行上采样（都使用替换），从而获得一个新的数据集，确保治疗和控制各占50%。
- en: The reason why I start with a dataset that is *not* 50% treatment and 50% control
    is to show that even if the original data doesn’t come from a policy that assigns
    treatment and control uniformly at random, we can still work with that data to
    do offline evaluation after doing up- and/or down-sampling to massage it into
    a 50/50% dataset. As mentioned in the previous section, the logic behind up- and
    down-sampling is similar to rejection sampling and the related idea of dividing
    the observed *y* by the propensity.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我之所以从一个*非*50%治疗和50%控制的数据集开始，是为了展示即使原始数据不是来自一个均匀随机分配治疗和控制的策略，我们仍然可以通过对数据进行上下采样，将其处理成一个50/50%的数据集，并进行离线评估。如前一节所提到的，上下采样的逻辑类似于拒绝采样以及相关的将观察到的
    *y* 除以倾向的概念。
- en: The following figure compares the three policies described above (A/B vs MAB
    vs CB) in terms of their cumulative *y* values.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表比较了上述三种策略（A/B vs MAB vs CB）在其累计 *y* 值上的表现。
- en: '**Figure 3: Cumulative Reward Comparison**'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**图3：累计奖励比较**'
- en: '![](../Images/e6aaafba01f5022e78987f9e49c2f21b.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6aaafba01f5022e78987f9e49c2f21b.png)'
- en: As can be seen in this figure, cumulative *y* increases fastest for CB and slowest
    for A/B with MAB somewhere in between. While this result is based on a simulated
    dataset, the patterns observed here can still be generalized. The reason why A/B
    testing isn’t able to get a high cumulative *y* is because it isn’t changing the
    60/40% allocation at all even after seeing sufficient evidence that treatment
    is better than control overall. On the other hand, while MAB is able to dynamically
    update this traffic allocation, it is still performing worse than CB because it
    isn’t personalizing the treatment vs control assignment based on the context *X*
    being observed. Finally, CB is both dynamically changing the traffic allocation
    and also personalizing the treatment, hence the superior performance.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，累计 *y* 在 CB 策略下增加最快，A/B 策略最慢，而 MAB 则介于两者之间。虽然这个结果基于一个模拟数据集，但这里观察到的模式仍然可以进行概括。A/B
    测试无法获得较高的累计 *y* 的原因在于，即便看到足够的证据表明治疗总体上优于控制，它仍然没有改变 60/40%的分配。另一方面，虽然 MAB 能够动态更新流量分配，但由于没有基于观察到的上下文
    *X* 个性化治疗与控制的分配，它的表现仍然不如 CB。最终，CB 同时动态变化流量分配，并且个性化治疗，因此表现最好。
- en: 7\. Conclusion
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7\. 结论
- en: Congratulations on making it to the end of this fairly long post! We covered
    a lot of ground related to contextual bandits in this post, and I hope that you
    leave this page with an appreciation of the usefulness of this fascinating method
    for online experimentation, especially when treatments need to be personalized.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了这篇相对较长的文章！在这篇文章中，我们讨论了很多与上下文赌博机（CB）相关的内容，我希望你读完这篇文章后，能更加理解这种方法在在线实验中的实用性，特别是当治疗需要个性化时。
- en: If you are interested in learning more about contextual bandits (or want to
    go a step further into multi-step reinforcement learning), I highly recommend
    the book *Mastering Reinforcement Learning with Python* by E. Bilgin. The Contextual
    Bandit chapter of this book was what finally gave me the “aha!” moment in understanding
    this topic, and I kept on reading to learn more about RL in general. As far as
    offline policy evaluation is concerned, I highly recommend the posts by E. Conti
    and J. LeDoux, both of which provide great explanations of the techniques involved
    and provide code examples. Regarding debiasing in contextual bandits, the paper
    by A. Bietti, A. Agarwal, and J. Langford provides a great overview of the techniques
    involved. Finally, while this post exclusively focused on using regression models
    when building contextual bandits, there is an alternative approach called cost-sensitive
    classification, which you can start learning by checking out these lecture notes
    by A. Agarwal and S. Kakade [[12]](https://courses.cs.washington.edu/courses/cse599m/19sp/notes/off_policy.pdf).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对学习更多关于上下文臂（或者想深入了解多步强化学习）感兴趣，我强烈推荐阅读**Mastering Reinforcement Learning with
    Python**一书，作者是E. Bilgin。这本书的上下文臂章节让我最终理解这个主题的“啊哈！”时刻，我继续阅读以了解更多关于RL的知识。至于离线策略评估，我强烈推荐E.
    Conti和J. LeDoux的文章，两者都提供了关于所涉技术的很好的解释并附有代码示例。关于上下文臂中的去偏见问题，A. Bietti、A. Agarwal和J.
    Langford的论文提供了很好的技术概述。最后，虽然本文专注于在构建上下文臂时使用回归模型，但还有一种名为成本敏感分类的替代方法，你可以通过查阅A. Agarwal和S.
    Kakade的这些讲义开始学习[[12]](https://courses.cs.washington.edu/courses/cse599m/19sp/notes/off_policy.pdf)。
- en: Have fun building contextual bandits!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 玩得开心，构建上下文臂！
- en: 8\. Acknowledgements
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8\. 致谢
- en: I would like to thank Colin Dickens for introducing me to contextual bandits
    as well as providing valuable feedback on this post, Xinyi Zhang for all her helpful
    feedback throughout the writing, Jiaqi Gu for a fruitful conversation on sampling
    methods, and Dennis Feehan for encouraging me to take the time to write this piece.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢Colin Dickens向我介绍上下文臂，并在这篇文章中提供了宝贵的反馈，感谢Xinyi Zhang在写作过程中提供的所有有用反馈，感谢Jiaqi
    Gu对采样方法的富有成效的讨论，以及感谢Dennis Feehan鼓励我花时间写这篇文章。
- en: Unless otherwise noted, all images are by the author.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均为作者所有。
- en: 9\. References
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9\. 参考文献
- en: '[1] Z. Zhao and T. Harinen, [Uplift Modeling for Multiple Treatments with Cost
    Optimization](https://www.uber.com/blog/research/uplift-modeling-for-multiple-treatments-with-cost-optimization/)
    (2019), DSAA'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Z. Zhao和T. Harinen，[多治疗方案的提升建模与成本优化](https://www.uber.com/blog/research/uplift-modeling-for-multiple-treatments-with-cost-optimization/)（2019），DSAA'
- en: '[2] Y. Narang, [Recommender systems using LinUCB: A contextual multi-armed
    bandit approach](/recommender-systems-using-linucb-a-contextual-multi-armed-bandit-approach-35a6f0eb6c4)
    (2020), Medium'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Y. Narang，[使用LinUCB的推荐系统：上下文多臂老虎机方法](/recommender-systems-using-linucb-a-contextual-multi-armed-bandit-approach-35a6f0eb6c4)（2020），Medium'
- en: '[3] D. Russo, B. Van Roy, A. Kazerouni, I. Osband, and Z. Wen, [A Tutorial
    on Thompson Sampling](https://arxiv.org/pdf/1707.02038.pdf) (2018), Foundations
    and Trends in Machine Learning'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] D. Russo, B. Van Roy, A. Kazerouni, I. Osband, and Z. Wen，[Thompson抽样教程](https://arxiv.org/pdf/1707.02038.pdf)（2018），Foundations
    and Trends in Machine Learning'
- en: '[4] B. Shahriari, K. Swersky, Z. Wang, R. Adams, and N. de Freitas, [Taking
    the Human Out of the Loop: A Review of Bayesian Optimization](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7352306)
    (2015), IEEE'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] B. Shahriari, K. Swersky, Z. Wang, R. Adams, and N. de Freitas，[摒弃人类的循环：贝叶斯优化综述](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7352306)（2015），IEEE'
- en: '[5] E. Conti, [Offline Policy Evaluation: Run fewer, better A/B tests](https://edoconti.medium.com/offline-policy-evaluation-run-fewer-better-a-b-tests-60ce8f93fa15)
    (2021), Medium'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] E. Conti，[离线策略评估：减少，优化A/B测试](https://edoconti.medium.com/offline-policy-evaluation-run-fewer-better-a-b-tests-60ce8f93fa15)（2021），Medium'
- en: '[6] A. Bietti, A. Agarwal, and J. Langford, [A Contextual Bandit Bake-off](https://arxiv.org/pdf/1802.04064.pdf)
    (2021), ArXiv'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] A. Bietti, A. Agarwal, and J. Langford，[上下文臂竞赛](https://arxiv.org/pdf/1802.04064.pdf)（2021），ArXiv'
- en: '[7] L. Li, W. Chu, J. Langford, and X. Wang, [Unbiased Offline Evaluation of
    Contextual-bandit-based News Article Recommendation Algorithms](https://arxiv.org/pdf/1003.5956.pdf)
    (2011), WSDM'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] L. Li, W. Chu, J. Langford, and X. Wang，[无偏离线评估基于上下文臂的新闻推荐算法](https://arxiv.org/pdf/1003.5956.pdf)（2011），WSDM'
- en: '[8] T. Mandel, Y. Liu, E. Brunskill, and Z. Popovic, [Offline Evaluation of
    Online Reinforcement Learning Algorithms](https://grail.cs.washington.edu/projects/nonstationaryeval/nonstationaryevalExtended.pdf)
    (2016), AAAI'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] T. Mandel, Y. Liu, E. Brunskill, 和 Z. Popovic, [在线强化学习算法的离线评估](https://grail.cs.washington.edu/projects/nonstationaryeval/nonstationaryevalExtended.pdf)
    (2016), AAAI'
- en: '[9] E. Bilgin, [Mastering Reinforcement Learning with Python](https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python/tree/master)
    (2020), Packt Publishing'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] E. Bilgin, [用 Python 精通强化学习](https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python/tree/master)
    (2020), Packt 出版社'
- en: '[10] J. LeDoux, [Offline Evaluation of Multi-Armed Bandit Algorithms in Python
    using Replay](https://jamesrledoux.com/algorithms/offline-bandit-evaluation/)
    (2020), LeDoux’s personal website'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] J. LeDoux, [使用回放在 Python 中离线评估多臂老虎机算法](https://jamesrledoux.com/algorithms/offline-bandit-evaluation/)
    (2020), LeDoux 个人网站'
- en: '[11] J. LeDoux, [Multi-Armed Bandits in Python: Epsilon Greedy, UCB1, Bayesian
    UCB, and EXP3](https://jamesrledoux.com/algorithms/bandit-algorithms-epsilon-ucb-exp-python/)
    (2020), LeDoux’s personal website'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] J. LeDoux, [Python 中的多臂老虎机：Epsilon 贪婪法、UCB1、贝叶斯 UCB 和 EXP3](https://jamesrledoux.com/algorithms/bandit-algorithms-epsilon-ucb-exp-python/)
    (2020), LeDoux 个人网站'
- en: '[12] A. Agarwal and S. Kakade, [Off-policy Evaluation and Learning](https://courses.cs.washington.edu/courses/cse599m/19sp/notes/off_policy.pdf)
    (2019), University of Washington Computer Science Department'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] A. Agarwal 和 S. Kakade, [非策略评估与学习](https://courses.cs.washington.edu/courses/cse599m/19sp/notes/off_policy.pdf)
    (2019), 华盛顿大学计算机科学系'
