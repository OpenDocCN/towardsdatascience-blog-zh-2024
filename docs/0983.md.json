["```py\n**Natural language**: “Get customer names and emails of customers from the US”\n\n**SQL**: \"SELECT name, email FROM customers WHERE country = 'USA'\"\n```", "```py\n**Natural language**: “I am John Doe, phone number is 555–123–4567,\n                   my friends are Anna and Sara”\n\n**JSON**: {name: \"John Doe\",\n       phone_number: \"555–123–5678\",\n       friends: {\n          name: [[\"Anna\", \"Sara\"]]}\n      }\n```", "```py\nfrom transformers import BartForConditionalGeneration, BartTokenizerFast, PreTrainedTokenizer\nfrom transformers.generation.logits_process import LogitsProcessorList, LogitsProcessor\nimport torch\n\nname = 'facebook/bart-large'\ntokenizer = BartTokenizerFast.from_pretrained(name, add_prefix_space=True)\npretrained_model = BartForConditionalGeneration.from_pretrained(name)\n```", "```py\nto_translate = 'customers emails from the us'\nwords = to_translate.split()\ntokenized_text = tokenizer([words], is_split_into_words=True)\n\nout = pretrained_model.generate(\n    torch.tensor(tokenized_text[\"input_ids\"]),\n    max_new_tokens=20,\n)\nprint(tokenizer.convert_tokens_to_string(\n    tokenizer.convert_ids_to_tokens(\n        out[0], skip_special_tokens=True)))\n```", "```py\n'More emails from the us'\n```", "```py\nrules = {'<s>': ['SELECT', 'DELETE'], # beginning of the generation\n 'SELECT': ['name', 'email', 'id'],  # names of columns in our schema\n 'DELETE': ['name', 'email', 'id'],\n 'name': [',', 'FROM'],\n 'email': [',', 'FROM'],\n 'id': [',', 'FROM'],\n ',': ['name', 'email', 'id'],\n 'FROM': ['customers', 'vendors'],  # names of tables in our schema\n 'customers': ['</s>'],\n 'vendors': ['</s>'],  # end of the generation\n}\n```", "```py\ndef convert_token_to_id(token):\n    return tokenizer(token, add_special_tokens=False)['input_ids'][0]\n\nclass SQLLogitsProcessor(LogitsProcessor):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        self.tokenizer = tokenizer\n        self.rules = {convert_token_to_id(k): [convert_token_to_id(v0) for v0 in v] for k,v in rules.items()}\n```", "```py\nclass SQLLogitsProcessor(LogitsProcessor):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        self.tokenizer = tokenizer\n        self.rules = {convert_token_to_id(k): [convert_token_to_id(v0) for v0 in v] for k,v in rules.items()}\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.LongTensor):\n        if not (input_ids == self.tokenizer.bos_token_id).any():\n        # we must allow the start token to appear before we start processing\n            return scores\n        # create a new tensor of -inf\n        new_scores = torch.full((1, self.tokenizer.vocab_size), float('-inf'))\n        # ids of legitimate tokens\n        legit_ids = self.rules[int(input_ids[0, -1])]\n        # place their values in the new tensor\n        new_scores[:, legit_ids] = scores[0, legit_ids]\n        return new_scores\n```", "```py\nto_translate = 'customers emails from the us'\nwords = to_translate.split()\ntokenized_text = tokenizer([words], is_split_into_words=True, return_offsets_mapping=True)\n\nlogits_processor = LogitsProcessorList([SQLLogitsProcessor(tokenizer)])\n\nout = pretrained_model.generate(\n    torch.tensor(tokenized_text[\"input_ids\"]),\n    max_new_tokens=20,\n    logits_processor=logits_processor\n)\nprint(tokenizer.convert_tokens_to_string(\n    tokenizer.convert_ids_to_tokens(\n        out[0], skip_special_tokens=True)))\n```", "```py\n SELECT email , email , id , email FROM customers\n```"]