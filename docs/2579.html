<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Self-Service ML with Relational Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Self-Service ML with Relational Deep Learning</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/self-service-ml-with-relational-deep-learning-beb693a21d5b?source=collection_archive---------9-----------------------#2024-10-22">https://towardsdatascience.com/self-service-ml-with-relational-deep-learning-beb693a21d5b?source=collection_archive---------9-----------------------#2024-10-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="08bd" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Do ML directly on your relational database</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@brechterlaurin?source=post_page---byline--beb693a21d5b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Laurin Brechter" class="l ep by dd de cx" src="../Images/5a68b96bddf86846a2bef9d482ef9dd3.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*QHmK9X-MGra2e-1yZwCa2Q.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--beb693a21d5b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@brechterlaurin?source=post_page---byline--beb693a21d5b--------------------------------" rel="noopener follow">Laurin Brechter</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--beb693a21d5b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj mk"><img src="../Images/fd3ee51120626ff5eccf5c3fc113c1db.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*NXb0TreqAHZgq-ojBBFzjQ.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Relational Schema of our Dataset, <a class="af mx" href="https://www.kaggle.com/datasets/mmohaiminulislam/ecommerce-data-analysis/data" rel="noopener ugc nofollow" target="_blank">source</a>; Image by Author</figcaption></figure><p id="7134" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">In this blog post, we will dive into an interesting new approach to Deep Learning (DL) called Relational Deep Learning (RDL). We will also gain some hands-on experience by doing some RDL on a real-world database (not a dataset!) of an e-commerce company.</p><h1 id="7e92" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">Introduction</h1><p id="6217" class="pw-post-body-paragraph my mz fq na b go oq nc nd gr or nf ng nh os nj nk nl ot nn no np ou nr ns nt fj bk">In the real world, we usually have a relational database against which we want to run some ML task. But especially when the database is highly normalized, this implies lots of time-consuming feature engineering and loss of granularity as we have to do many aggregations. What’s more, there’s a myriad of possible combinations of features that we can construct each of which might yield good performance [2]. That means we are likely to leave some information relevant to the ML task on the table.</p><p id="30f5" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">This is similar to the early days of computer vision, before the advent of deep neural networks where features were hand-crafted from the pixel values. Nowadays, models work directly with the raw pixels instead of relying on this intermediate layer.</p><h1 id="66e2" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">Relational Deep Learning</h1><p id="0278" class="pw-post-body-paragraph my mz fq na b go oq nc nd gr or nf ng nh os nj nk nl ot nn no np ou nr ns nt fj bk">RDL promises to do the same for tabular learning. That is, it removes the extra step of constructing a feature matrix by learning directly on top of your relational database. It does so by transforming the database with its relations into a graph where a row in a table becomes a node and relations between tables become edges. The row values are stored inside the nodes as node features.</p><p id="b876" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><em class="ov">In this blog post, we will be using this </em><a class="af mx" href="https://www.kaggle.com/datasets/mmohaiminulislam/ecommerce-data-analysis/data" rel="noopener ugc nofollow" target="_blank"><em class="ov">e-commerce dataset</em></a><em class="ov"> from kaggle which contains transactional data about an e-commerce platform in a star schema with a central fact table (transactions) and some dimension tables. The full code can be found in this </em><a class="af mx" href="https://github.com/LaurinBrechter/GraphTheory/blob/main/rdl/rdl_ecommerce.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="ov">notebook</em></a><em class="ov">.</em></p><p id="361e" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Throughout this blog post, we will be using the <a class="af mx" href="https://github.com/snap-stanford/relbench" rel="noopener ugc nofollow" target="_blank">relbench</a> library to do <a class="af mx" href="https://relbench.stanford.edu/" rel="noopener ugc nofollow" target="_blank">RDL</a>. The first thing we have to do in relbench is to specify the schema of our relational database. Below is an example of how we can do so for the ‘transactions’ table in the database. We give the table as a pandas dataframe and specify the primary key and the timestamp column. The primary key column is used to uniquely identify the entity. The timestamp ensures that we can only learn from past transactions when we want to forecast future transactions. In the graph, this means that information can only flow from nodes with a lower timestamp (i.e. in the past) to ones with a higher timestamp. Additionally, we specify the foreign keys that exist in the relation. In this case, the transactions table has the column ‘customer_key’ which is a foreign key that points to the ‘customer_dim’ table.</p><pre class="ml mm mn mo mp ow ox oy bp oz bb bk"><span id="254c" class="pa nv fq ox b bg pb pc l pd pe">tables['transactions'] = Table(<br/>            df=pd.DataFrame(t),<br/>            pkey_col='t_id',<br/>            fkey_col_to_pkey_table={<br/>                'customer_key': 'customers',<br/>                'item_key': 'products',<br/>                'store_key': 'stores'<br/>            },<br/>            time_col='date'<br/>        )</span></pre><p id="4491" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The rest of the tables need to be defined in the same way. Note that this could also be automated if you already have a database schema. Since the dataset is from Kaggle, I needed to create the schema manually. We also need to convert the date columns to actual pandas datetime objects and remove any NaN values.</p><pre class="ml mm mn mo mp ow ox oy bp oz bb bk"><span id="7082" class="pa nv fq ox b bg pb pc l pd pe">class EcommerceDataBase(Dataset):<br/>    # example of creating your own dataset: https://github.com/snap-stanford/relbench/blob/main/tutorials/custom_dataset.ipynb<br/><br/>    val_timestamp = pd.Timestamp(year=2018, month=1, day=1)<br/>    test_timestamp = pd.Timestamp(year=2020, month=1, day=1)<br/><br/>    def make_db(self) -&gt; Database:<br/><br/>        tables = {}<br/><br/>        customers = load_csv_to_db(BASE_DIR + '/customer_dim.csv').drop(columns=['contact_no', 'nid']).rename(columns={'coustomer_key': 'customer_key'})<br/>        stores = load_csv_to_db(BASE_DIR + '/store_dim.csv').drop(columns=['upazila'])<br/>        products = load_csv_to_db(BASE_DIR + '/item_dim.csv')<br/>        transactions = load_csv_to_db(BASE_DIR + '/fact_table.csv').rename(columns={'coustomer_key': 'customer_key'})<br/>        times = load_csv_to_db(BASE_DIR + '/time_dim.csv')<br/><br/>        t = transactions.merge(times[['time_key', 'date']], on='time_key').drop(columns=['payment_key', 'time_key', 'unit'])<br/>        t['date'] = pd.to_datetime(t.date)<br/>        t = t.reset_index().rename(columns={'index': 't_id'})<br/>        t['quantity'] = t.quantity.astype(int)<br/>        t['unit_price'] = t.unit_price.astype(float)<br/>        products['unit_price'] = products.unit_price.astype(float)<br/>        t['total_price'] = t.total_price.astype(float)<br/><br/>        print(t.isna().sum(axis=0))<br/>        print(products.isna().sum(axis=0))<br/>        print(stores.isna().sum(axis=0))<br/>        print(customers.isna().sum(axis=0))<br/><br/>        tables['products'] = Table(<br/>            df=pd.DataFrame(products),<br/>            pkey_col='item_key',<br/>            fkey_col_to_pkey_table={},<br/>            time_col=None<br/>        )<br/><br/>        tables['customers'] = Table(<br/>            df=pd.DataFrame(customers),<br/>            pkey_col='customer_key',<br/>            fkey_col_to_pkey_table={},<br/>            time_col=None<br/>        )<br/><br/>        tables['transactions'] = Table(<br/>            df=pd.DataFrame(t),<br/>            pkey_col='t_id',<br/>            fkey_col_to_pkey_table={<br/>                'customer_key': 'customers',<br/>                'item_key': 'products',<br/>                'store_key': 'stores'<br/>            },<br/>            time_col='date'<br/>        )<br/><br/>        tables['stores'] = Table(<br/>            df=pd.DataFrame(stores),<br/>            pkey_col='store_key',<br/>            fkey_col_to_pkey_table={}<br/>        )<br/><br/>        return Database(tables)</span></pre><p id="5a82" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Crucially, the authors introduce the idea of a training table. This training table essentially defines the ML task. The idea here is that we want to predict the future state (i.e. a future value) of some entity in the database. We do this by specifying a table where each row has a timestamp, the identifier of the entity, and some value we want to predict. The id serves to specify the entity, the timestamp specifies at which point in time we need to predict the entity. This will also limit the data that can be used to infer the value of this entity (i.e. only past data). The value itself is what we want to predict (i.e. ground truth).</p><p id="e879" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">In our case, we have an online platform with customers. We want to predict a customer’s revenue in the next 30 days. We can create the training table with a SQL statement executed with DuckDB. This is the big advantage of RDL as we could create any kind of ML task with just SQL. For example, we can define a query to select the number of purchases of buyers in the next 30 days to make a churn prediction.</p><pre class="ml mm mn mo mp ow ox oy bp oz bb bk"><span id="ed42" class="pa nv fq ox b bg pb pc l pd pe">df = duckdb.sql(f"""<br/>            select<br/>                timestamp,<br/>                customer_key,<br/>                sum(total_price) as revenue<br/>            from<br/>                timestamp_df t<br/>            left join<br/>                transactions ta<br/>            on<br/>                ta.date &lt;= t.timestamp + INTERVAL '{self.timedelta}'<br/>                and ta.date &gt; t.timestamp<br/>            group by timestamp, customer_key<br/>        """).df().dropna()</span></pre><p id="29af" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The result will be a table that has the seller_id as the key of the entity that we want to predict, the revenue as the target, and the timestamp as the time at which we need to make the prediction (i.e. we can only use data up until this point to make the prediction).</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="mi mj pf"><img src="../Images/249add07383580218f2c4e9e3b3cf33d.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*xiL9m5CEJvnB4GBMZrgHgA.png"/></div></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Training Table; Image by Author</figcaption></figure><p id="5e53" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Below is the complete code for creating the ‘customer_revenue’ task.</p><pre class="ml mm mn mo mp ow ox oy bp oz bb bk"><span id="95ff" class="pa nv fq ox b bg pb pc l pd pe">class CustomerRevenueTask(EntityTask):<br/>    # example of custom task: https://github.com/snap-stanford/relbench/blob/main/tutorials/custom_task.ipynb<br/><br/><br/>    task_type = TaskType.REGRESSION<br/>    entity_col = "customer_key"<br/>    entity_table = "customers"<br/>    time_col = "timestamp"<br/>    target_col = "revenue"<br/>    timedelta = pd.Timedelta(days=30) # how far we want to predict revenue into the future.<br/>    metrics = [r2, mae]<br/>    num_eval_timestamps = 40<br/><br/>    def make_table(self, db: Database, timestamps: "pd.Series[pd.Timestamp]") -&gt; Table:<br/><br/>        timestamp_df = pd.DataFrame({"timestamp": timestamps})<br/><br/>        transactions = db.table_dict["transactions"].df<br/><br/>        df = duckdb.sql(f"""<br/>            select<br/>                timestamp,<br/>                customer_key,<br/>                sum(total_price) as revenue<br/>            from<br/>                timestamp_df t<br/>            left join<br/>                transactions ta<br/>            on<br/>                ta.date &lt;= t.timestamp + INTERVAL '{self.timedelta}'<br/>                and ta.date &gt; t.timestamp<br/>            group by timestamp, customer_key<br/>        """).df().dropna()<br/><br/>        print(df)<br/><br/>        return Table(<br/>            df=df,<br/>            fkey_col_to_pkey_table={self.entity_col: self.entity_table},<br/>            pkey_col=None,<br/>            time_col=self.time_col,<br/>        )</span></pre><p id="68a2" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">With that, we have done the bulk of the work. The rest of the workflow will be similar, independent of the ML task. I was able to copy most of the code from the <a class="af mx" href="https://github.com/snap-stanford/relbench/blob/main/tutorials/train_model.ipynb" rel="noopener ugc nofollow" target="_blank">example notebook</a> that relbench provides.</p><p id="0dd3" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">For example, we need to encode the node features. Here, we can use glove embeddings to encode all the text features such as the product descriptions and the product names.</p><pre class="ml mm mn mo mp ow ox oy bp oz bb bk"><span id="5a9e" class="pa nv fq ox b bg pb pc l pd pe">from typing import List, Optional<br/>from sentence_transformers import SentenceTransformer<br/>from torch import Tensor<br/><br/><br/>class GloveTextEmbedding:<br/>    def __init__(self, device: Optional[torch.device<br/>                                       ] = None):<br/>        self.model = SentenceTransformer(<br/>            "sentence-transformers/average_word_embeddings_glove.6B.300d",<br/>            device=device,<br/>        )<br/><br/>    def __call__(self, sentences: List[str]) -&gt; Tensor:<br/>        return torch.from_numpy(self.model.encode(sentences))</span></pre><p id="23fc" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">After that, we can apply those transformations to our data and build out the graph.</p><pre class="ml mm mn mo mp ow ox oy bp oz bb bk"><span id="55c6" class="pa nv fq ox b bg pb pc l pd pe">from torch_frame.config.text_embedder import TextEmbedderConfig<br/>from relbench.modeling.graph import make_pkey_fkey_graph<br/><br/>text_embedder_cfg = TextEmbedderConfig(<br/>    text_embedder=GloveTextEmbedding(device=device), batch_size=256<br/>)<br/><br/>data, col_stats_dict = make_pkey_fkey_graph(<br/>    db,<br/>    col_to_stype_dict=col_to_stype_dict,  # speficied column types<br/>    text_embedder_cfg=text_embedder_cfg,  # our chosen text encoder<br/>    cache_dir=os.path.join(<br/>        root_dir, f"rel-ecomm_materialized_cache"<br/>    ),  # store materialized graph for convenience<br/>)</span></pre><p id="5e6b" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The rest of the code will be building the GNN from standard layers, coding the training loop, and doing some evaluations. I will leave this code out of this blog post for brevity since it is very standard and will be the same across tasks. You can check out the notebook <a class="af mx" href="https://github.com/LaurinBrechter/GraphTheory/tree/main/rdl" rel="noopener ugc nofollow" target="_blank">here</a>.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="mi mj pk"><img src="../Images/885dfaed5f9ea9705ffb86ecb8d1fca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z2OfyFaiI7Yr84mXjoi22w.png"/></div></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Result of Training, Image by Author</figcaption></figure><p id="555a" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">As a result, we can train this GNN to reach an r2 of around 0.3 and an MAE of 500. This means that it predicts the seller’s revenue in the next 30 days with an average error of +- $500. Of course, we can’t know if this is good or not, maybe we could have gotten an r2 of 80% with a combination of classical ML and feature engineering.</p><h1 id="d7f5" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">Conclusion</h1><p id="ab02" class="pw-post-body-paragraph my mz fq na b go oq nc nd gr or nf ng nh os nj nk nl ot nn no np ou nr ns nt fj bk">Relational Deep Learning is an interesting new approach to ML especially when we have a complex relational schema where manual feature engineering would be too laborious. It gives us the ability to define an ML task with just SQL which can be especially useful for individuals that are not deep into data science but know some SQL. This also means that we can iterate quickly and experiment a lot with different tasks.</p><p id="1793" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">At the same time, this approach presents its own problems such as the difficulty of training GNNs and constructing the graph from the relational schema. Additionally, the question is to what extent RDL can compete in terms of performance with classical ML models. In the past, we have seen that models such as XGboost have proven to be better than neural networks on tabular prediction problems.</p><h1 id="19e3" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">References</h1><ul class=""><li id="688c" class="my mz fq na b go oq nc nd gr or nf ng nh os nj nk nl ot nn no np ou nr ns nt pl pm pn bk">[1] Robinson, Joshua, et al. “RelBench: A Benchmark for Deep Learning on Relational Databases.” <em class="ov">arXiv</em>, 2024, <a class="af mx" href="https://arxiv.org/abs/2407.20060" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2407.20060</a>.</li><li id="7870" class="my mz fq na b go po nc nd gr pp nf ng nh pq nj nk nl pr nn no np ps nr ns nt pl pm pn bk">[2] Fey, Matthias, et al. “Relational deep learning: Graph representation learning on relational databases.” <em class="ov">arXiv preprint arXiv:2312.04615</em> (2023).</li><li id="f9a9" class="my mz fq na b go po nc nd gr pp nf ng nh pq nj nk nl pr nn no np ps nr ns nt pl pm pn bk">[3] Schlichtkrull, Michael, et al. “Modeling relational data with graph convolutional networks.” <em class="ov">The semantic web: 15th international conference, ESWC 2018, Heraklion, Crete, Greece, June 3–7, 2018, proceedings 15</em>. Springer International Publishing, 2018.</li></ul></div></div></div></div>    
</body>
</html>