<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Running Local LLMs and VLMs on the Raspberry Pi</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Running Local LLMs and VLMs on the Raspberry Pi</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/running-local-llms-and-vlms-on-the-raspberry-pi-57bd0059c41a?source=collection_archive---------0-----------------------#2024-01-14">https://towardsdatascience.com/running-local-llms-and-vlms-on-the-raspberry-pi-57bd0059c41a?source=collection_archive---------0-----------------------#2024-01-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="9da1" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><strong class="al">Get models like Phi-2, Mistral, and LLaVA running locally on a Raspberry Pi with Ollama</strong></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@pyesonekyaw?source=post_page---byline--57bd0059c41a--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Pye Sone Kyaw" class="l ep by dd de cx" src="../Images/907574a7d2de57a4cc0ce36d73234a7a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*ICD1Cj2Bx8Q4zI28knVcxA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--57bd0059c41a--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@pyesonekyaw?source=post_page---byline--57bd0059c41a--------------------------------" rel="noopener follow">Pye Sone Kyaw</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--57bd0059c41a--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">21</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/b138fcad68e649f781cde62d61d153ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pb0-j607x_RI0sPst86M1Q.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Host LLMs and VLMs using Ollama on the Raspberry Pi — Source: Author</figcaption></figure><p id="f089" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Ever thought of running your own large language models (LLMs) or vision language models (VLMs) on your own device? You probably did, but the thoughts of setting things up from scratch, having to manage the environment, downloading the right model weights, and the lingering doubt of whether your device can even handle the model has probably given you some pause.</p><p id="8b58" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s go one step further than that. Imagine operating your own LLM or VLM on a device no larger than a credit card — a Raspberry Pi. Impossible? Not at all. I mean, I’m writing this post after all, so it definitely is possible.</p><h2 id="a3d7" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk"><strong class="al">Possible, yes. But why would you even do it?</strong></h2><p id="f6af" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">LLMs at the edge seem quite far-fetched at this point in time. But this particular niche use case should mature over time, and we will definitely see some cool edge solutions being deployed with an all-local generative AI solution running on-device at the edge.</p><p id="8cf7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It’s also about pushing the limits to see what’s possible. If it can be done at this extreme end of the compute scale, then it can be done at any level in between a Raspberry Pi and a big and powerful server GPU.</p><p id="b6d7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Traditionally, edge AI has been closely linked with computer vision. Exploring the deployment of LLMs and VLMs at the edge adds an exciting dimension to this field that is just emerging.</p><p id="1cde" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Most importantly, I just wanted to do something fun with my recently acquired Raspberry Pi 5.</p><p id="7c31" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, how do we achieve all this on a Raspberry Pi? Using Ollama!</p><h2 id="7e53" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk"><strong class="al">What is Ollama?</strong></h2><p id="720b" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk"><a class="af oy" href="https://ollama.ai/" rel="noopener ugc nofollow" target="_blank">Ollama</a> has emerged as one of the best solutions for running local LLMs on your own personal computer without having to deal with the hassle of setting things up from scratch. With just a few commands, everything can be set up without any issues. Everything is self-contained and works wonderfully in my experience across several devices and models. It even exposes a REST API for model inference, so you can leave it running on the Raspberry Pi and call it from your other applications and devices if you want to.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk oz"><img src="../Images/25665bd9949055ec8e2ed0a29e83ea4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nf5aUbaCc7lVwKFb"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af oy" href="https://ollama.ai/" rel="noopener ugc nofollow" target="_blank">Ollama’s Website</a></figcaption></figure><p id="67f5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There’s also <a class="af oy" href="https://github.com/ollama-webui/ollama-webui" rel="noopener ugc nofollow" target="_blank">Ollama Web UI</a> which is a beautiful piece of AI UI/UX that runs seamlessly with Ollama for those apprehensive about command-line interfaces. It’s basically a local ChatGPT interface, if you will.</p><p id="ca3b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Together, these two pieces of open-source software provide what I feel is the best locally hosted LLM experience right now.</p><p id="e8bd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Both Ollama and Ollama Web UI support VLMs like LLaVA too, which opens up even more doors for this edge Generative AI use case.</p><h2 id="6675" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk"><strong class="al">Technical Requirements</strong></h2><p id="1064" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">All you need is the following:</p><ul class=""><li id="b218" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">Raspberry Pi 5 (or 4 for a less speedy setup) — Opt for the 8GB RAM variant to fit the 7B models.</li><li id="741b" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">SD Card — Minimally 16GB, the larger the size the more models you can fit. Have it already loaded with an appropriate OS such as Raspbian Bookworm or Ubuntu</li><li id="4f96" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">An internet connection</li></ul><p id="f71c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Like I mentioned earlier, running Ollama on a Raspberry Pi is already near the extreme end of the hardware spectrum. Essentially, any device more powerful than a Raspberry Pi, provided it runs a Linux distribution and has a similar memory capacity, should theoretically be capable of running Ollama and the models discussed in this post.</p><h2 id="a4bc" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk"><strong class="al">1. Installing Ollama</strong></h2><p id="6267" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">To install Ollama on a Raspberry Pi, we’ll avoid using Docker to conserve resources.</p><p id="3b63" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the terminal, run</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="cd82" class="pm nz fq pj b bg pn po l pp pq">curl https://ollama.ai/install.sh | sh</span></pre><p id="34e2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You should see something similar to the image below after running the command above.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pr"><img src="../Images/bb3793bb2c4843f497f4e73232983878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*byaVEMGvhZxzYeAp"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Source: Author</figcaption></figure><p id="711c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Like the output says, go to 0.0.0.0:11434 to verify that Ollama is running. It is normal to see the ‘WARNING: No NVIDIA GPU detected. Ollama will run in CPU-only mode.’ since we are using a Raspberry Pi. But if you’re following these instructions on something that is supposed to have a NVIDIA GPU, something did not go right.</p><p id="6bd9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For any issues or updates, refer to the <a class="af oy" href="https://github.com/jmorganca/ollama/tree/main" rel="noopener ugc nofollow" target="_blank">Ollama GitHub repository</a>.</p><h2 id="007b" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk"><strong class="al">2. Running LLMs through the command line</strong></h2><p id="97e8" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">Take a look at <a class="af oy" href="https://ollama.ai/library" rel="noopener ugc nofollow" target="_blank">the official Ollama model library</a> for a list of models that can be run using Ollama. On an 8GB Raspberry Pi, models larger than 7B won’t fit. Let’s use Phi-2, a 2.7B LLM from Microsoft, now under MIT license.</p><p id="5f6e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We’ll use the default Phi-2 model, but feel free to use any of the other tags found <a class="af oy" href="https://ollama.ai/library/phi/tags" rel="noopener ugc nofollow" target="_blank">here</a>. Take a look at the <a class="af oy" href="https://ollama.ai/library/phi" rel="noopener ugc nofollow" target="_blank">model page for Phi-2</a> to see how you can interact with it.</p><p id="9977" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the terminal, run</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="91bd" class="pm nz fq pj b bg pn po l pp pq">ollama run phi</span></pre><p id="9511" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once you see something similar to the output below, you already have a LLM running on the Raspberry Pi! It’s that simple.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ps"><img src="../Images/49c2e2e0adacb151d5df152f9a79421e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cv2ZDo7IN6IOWal3"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Source: Author</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/f998a88c1609414a0df28296d7c24245.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NrA2pZumgzrE0ZMo"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Here’s an interaction with Phi-2 2.7B. Obviously, you won’t get the same output, but you get the idea. | Source: Author</figcaption></figure><p id="5e79" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can try other models like Mistral, Llama-2, etc, just make sure there is enough space on the SD card for the model weights.</p><p id="e75b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Naturally, the bigger the model, the slower the output would be. On Phi-2 2.7B, I can get around 4 tokens per second. But with a Mistral 7B, the generation speed goes down to around 2 tokens per second. A token is roughly equivalent to a single word.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pu"><img src="../Images/1e4a03f08206bd6b1839a3b9bda9da8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*n3kE6CcoMpNS083s"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Here’s an interaction with Mistral 7B | Source: Author</figcaption></figure><p id="cce6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now we have LLMs running on the Raspberry Pi, but we are not done yet. The terminal isn’t for everyone. Let’s get Ollama Web UI running as well!</p><h2 id="7fd4" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk"><strong class="al">3. Installing and Running Ollama Web UI</strong></h2><p id="8d81" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">We shall follow the instructions on the <a class="af oy" href="https://github.com/ollama-webui/ollama-webui" rel="noopener ugc nofollow" target="_blank">official Ollama Web UI GitHub Repository</a> to install it without Docker. It recommends minimally Node.js to be &gt;= 20.10 so we shall follow that. It also recommends Python to be at least 3.11, but Raspbian OS already has that installed for us.</p><p id="1f75" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We have to install Node.js first. In the terminal, run</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="946e" class="pm nz fq pj b bg pn po l pp pq">curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash - &amp;&amp;\<br/>sudo apt-get install -y nodejs</span></pre><p id="f682" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Change the 20.x to a more appropriate version if need be for future readers.</p><p id="8f10" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then run the code block below.</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="aecd" class="pm nz fq pj b bg pn po l pp pq">git clone https://github.com/ollama-webui/ollama-webui.git<br/>cd ollama-webui/<br/><br/># Copying required .env file<br/>cp -RPp example.env .env<br/><br/># Building Frontend Using Node<br/>npm i<br/>npm run build<br/><br/># Serving Frontend with the Backend<br/>cd ./backend<br/>pip install -r requirements.txt --break-system-packages <br/>sh start.sh</span></pre><p id="d168" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It’s a slight modification of what is provided on GitHub. Do take note that for simplicity and brevity we are not following best practices like using virtual environments and we are using the — break-system-packages flag. If you encounter an error like uvicorn not being found, restart the terminal session.</p><p id="4612" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If all goes correctly, you should be able to access Ollama Web UI on port 8080 through <a class="af oy" href="http://0.0.0.0:8080" rel="noopener ugc nofollow" target="_blank">http://0.0.0.0:8080</a> on the Raspberry Pi, or through http://&lt;Raspberry Pi’s local address&gt;:8080/ if you are accessing through another device on the same network.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pv"><img src="../Images/0370778c38e9e9f6934c1657dc6b70b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vYeNHpdwDcm0OYse"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">If you see this, yes, it worked | Source: Author</figcaption></figure><p id="d919" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once you’ve created an account and logged in, you should see something similar to the image below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/d47c4570ae32b131c008cb4fde2e1d02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4jgO2jYS-JpFBGY6"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Source: Author</figcaption></figure><p id="f61d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you had downloaded some model weights earlier, you should see them in the dropdown menu like below. If not, you can go to the settings to download a model.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk px"><img src="../Images/c30e7ce637b4c6e9c2fb346cb3737e84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QM4t08mEiQRDKvQh"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Available models will appear here | Source: Author</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="ab cn cb py"><img src="../Images/320964e9287eebf5ddb47c817225ac9e.png" data-original-src="https://miro.medium.com/v2/format:webp/0*3iSBQ6Nog8RSt-4J"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">If you want to download new models, go to Settings &gt; Models to pull models | Source: Author</figcaption></figure><p id="636e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The entire interface is very clean and intuitive, so I won’t explain much about it. It’s truly a very well-done open-source project.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pz"><img src="../Images/dcb666e351985cb48670b7a30d3effdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rlpz-34lmGCMqOm9"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Here’s an interaction with Mistral 7B through Ollama Web UI | Source: Author</figcaption></figure><h2 id="e671" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk"><strong class="al">4. Running VLMs through Ollama Web UI</strong></h2><p id="9a8f" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">Like I mentioned at the start of this article, we can also run VLMs. Let’s run LLaVA, a popular open source VLM which also happens to be supported by Ollama. To do so, download the weights by pulling ‘llava’ through the interface.</p><p id="16ed" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Unfortunately, unlike LLMs, it takes quite some time for the setup to interpret the image on the Raspberry Pi. The example below took around 6 minutes to be processed. The bulk of the time is probably because the image side of things is not properly optimised yet, but this will definitely change in the future. The token generation speed is around 2 tokens/second.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/4fd80948dfb710e5691d6841cdbd63ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qf5Rj9jzRCGqPsjj"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Query Image Source: Pexels</figcaption></figure><h2 id="445b" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk"><strong class="al">To wrap it all up</strong></h2><p id="8e80" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">At this point we are pretty much done with the goals of this article. To recap, we’ve managed to use Ollama and Ollama Web UI to run LLMs and VLMs like Phi-2, Mistral, and LLaVA on the Raspberry Pi.</p><p id="e1fc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I can definitely imagine quite a few use cases for locally hosted LLMs running on the Raspberry Pi (or another other small edge device), especially since 4 tokens/second does seem like an acceptable speed with streaming for some use cases if we are going for models around the size of Phi-2.</p><p id="856b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The field of ‘small’ LLMs and VLMs, somewhat paradoxically named given their ‘large’ designation, is an active area of research with quite a few model releases recently. Hopefully this emerging trend continues, and more efficient and compact models continue to get released! Definitely something to keep an eye on in the coming months.</p><p id="d8e4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Disclaimer</strong>: I have no affiliation with Ollama or Ollama Web UI. All views and opinions are my own and do not represent any organisation.</p></div></div></div></div>    
</body>
</html>