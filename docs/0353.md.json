["```py\nimport torch\nimport torch.nn as nn\n\nclass NeuralNet(nn.Module):\n    def __init__(self, hidden_size, output_size=1,input_size=1):\n        super(NeuralNet, self).__init__()\n        self.l1 = nn.Linear(input_size, hidden_size)\n        self.relu1 = nn.LeakyReLU()\n        self.l2 = nn.Linear(hidden_size, hidden_size)\n        self.relu2 = nn.LeakyReLU()\n        self.l3 = nn.Linear(hidden_size, hidden_size)\n        self.relu3 = nn.LeakyReLU()\n        self.l4 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        out = self.l1(x)\n        out = self.relu1(out)\n        out = self.l2(out)\n        out = self.relu2(out)\n        out = self.l3(out)\n        out = self.relu3(out)\n        out = self.l4(out)\n        return out\n```", "```py\n# Create the criterion that will be used for the DE part of the loss\ncriterion = nn.MSELoss()\n\n# Define the loss function for the initial condition\ndef initial_condition_loss(y, target_value):\n    return nn.MSELoss()(y, target_value)\n```", "```py\n# Time vector that will be used as input of our NN\nt_numpy = np.arange(0, 5+0.01, 0.01, dtype=np.float32)\nt = torch.from_numpy(t_numpy).reshape(len(t_numpy), 1)\nt.requires_grad_(True)\n\n# Constant for the model\nk = 1\n\n# Instantiate one model with 50 neurons on the hidden layers\nmodel = NeuralNet(hidden_size=50)\n\n# Loss and optimizer\nlearning_rate = 8e-3\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n# Number of epochs\nnum_epochs = int(1e4)\n```", "```py\nfor epoch in range(num_epochs):\n\n    # Randomly perturbing the training points to have a wider range of times\n    epsilon = torch.normal(0,0.1, size=(len(t),1)).float()\n    t_train = t + epsilon\n\n    # Forward pass\n    y_pred = model(t_train)\n\n    # Calculate the derivative of the forward pass w.r.t. the input (t)\n    dy_dt = torch.autograd.grad(y_pred, \n                                t_train, \n                                grad_outputs=torch.ones_like(y_pred), \n                                create_graph=True)[0]\n\n    # Define the differential equation and calculate the loss\n    loss_DE = criterion(dy_dt + k*y_pred, torch.zeros_like(dy_dt))\n\n    # Define the initial condition loss\n    loss_IC = initial_condition_loss(model(torch.tensor([[0.0]])), \n                                     torch.tensor([[1.0]]))\n\n    loss = loss_DE + loss_IC\n\n    # Backward pass and weight update\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```"]