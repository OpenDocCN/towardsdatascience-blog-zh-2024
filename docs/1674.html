<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Doping: A Technique to Test Outlier Detectors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Doping: A Technique to Test Outlier Detectors</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4?source=collection_archive---------2-----------------------#2024-07-09">https://towardsdatascience.com/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4?source=collection_archive---------2-----------------------#2024-07-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="53de" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Using well-crafted synthetic data to compare and evaluate outlier detectors</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@wkennedy934?source=post_page---byline--3f6b847ab8d4--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="W Brett Kennedy" class="l ep by dd de cx" src="../Images/b3ce55ffd028167326c117d47c64c467.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*v8pf2r3SPMLuHoSmF4IwlA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3f6b847ab8d4--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@wkennedy934?source=post_page---byline--3f6b847ab8d4--------------------------------" rel="noopener follow">W Brett Kennedy</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3f6b847ab8d4--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km ld le ab q ee lf lg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lh"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="li k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lj an ao ap ig lk ll lm" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ln cn"><div class="l ae"><div class="ab cb"><div class="lo lp lq lr ls lt ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lj an ao ap ig lu lv lg lw lx ly lz ma s mb mc md me mf mg mh u mi mj mk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lj an ao ap ig lu lv lg lw lx ly lz ma s mb mc md me mf mg mh u mi mj mk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lj an ao ap ig lu lv lg lw lx ly lz ma s mb mc md me mf mg mh u mi mj mk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="543e" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">This article continues my series on outlier detection, following articles on <a class="af nh" href="https://medium.com/towards-data-science/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a" rel="noopener">Counts Outlier Detector</a> and <a class="af nh" href="https://medium.com/towards-data-science/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a" rel="noopener">Frequent Patterns Outlier Factor</a>, and provides another excerpt from my book <a class="af nh" href="https://www.manning.com/books/outlier-detection-in-python" rel="noopener ugc nofollow" target="_blank">Outlier Detection in Python</a>.</p><p id="8c77" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">In this article, we look at the issue of testing and evaluating outlier detectors, a notoriously difficult problem, and present one solution, sometimes referred to as <em class="ni">doping. </em>Using doping, real data rows are modified (usually) randomly, but in such a way as to ensure they are likely an outlier in some regard and, as such, should be detected by an outlier detector. We’re then able to evaluate detectors by assessing how well they are able to detect the doped records.</p><p id="27f6" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">In this article, we look specifically at tabular data, but the same idea may be applied to other modalities as well, including text, image, audio, network data, and so on.</p><h1 id="b3ec" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Testing and Evaluating other Types of Models</h1><p id="94e2" class="pw-post-body-paragraph ml mm fq mn b go of mp mq gr og ms mt mu oh mw mx my oi na nb nc oj ne nf ng fj bk">Likely, if you’re familiar with outlier detection, you’re also familiar, at least to some degree, with predictive models for regression and classification problems. With these types of problems, we have labelled data, and so it’s relatively simple to evaluate each option when tuning a model (selecting the best pre-processing, features, hyper-parameters, and so on); and it’s also relatively easy to estimate a model’s accuracy (how it will perform on unseen data): we simply use a train-validation-test split, or better, use cross validation. As the data is labelled, we can see directly how the model performs on a labelled test data.</p><p id="7374" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">But, with outlier detection, there is no labelled data and the problem is significantly more difficult; we have no objective way to determine if the records scored highest by the outlier detector are, in fact, the most statistically unusual within the dataset.</p><p id="d46e" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">With clustering, as another example, we also have no labels for the data, but it is at least possible to measure the quality of the clustering: we can determine how internally consistent the clusters are and how different the clusters are from each other. Using some distance metric (such as Manhattan or Euclidean distances), we can measure how close records within a cluster are to each other and how far apart clusters are from each other.</p><p id="bf3d" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">So, given a set of possible clusterings, it’s possible to define a sensible metric (such as the Silhouette score) and determine which is the preferred clustering, at least with respect to that metric. That is, much like prediction problems, we can calculate a score for each clustering, and select the clustering that appears to work best.</p><p id="8ff8" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">With outlier detection, though, we have nothing analogous to this we can use. Any system that seeks to quantify how anomalous a record is, or that seeks to determine, given two records, which is the more anomalous of the two, is effectively an outlier detection algorithm in itself.</p><p id="52a5" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">For example, we could use entropy as our outlier detection method, and can then examine the entropy of the full dataset as well as the entropy of the dataset after removing any records identified as strong outliers. This is, in a sense, valid; entropy is a useful measure of the presence of outliers. But we cannot assume entropy is the definitive definition of outliers in this dataset; one of the fundamental qualities of outlier detection is that there is no definitive definition of outliers.</p><p id="51b2" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">In general, if we have any way to try to evaluate the outliers detected by an outlier detection system (or, as in the previous example, the dataset with and without the identified outliers), this is effectively an outlier detection system in itself, and it becomes circular to use this to evaluate the outliers found.</p><p id="7c6b" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Consequently, it’s quite difficult to evaluate outlier detection systems and there’s effectively no good way to do so, at least using the real data that’s available.</p><p id="3d98" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">We can, though, create synthetic test data (in such a way that we can assume the synthetically-created data are predominantly outliers). Given this, we can determine the extent to which outlier detectors tend to score the synthetic records more highly than the real records.</p><p id="aa29" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">There are a number of ways to create synthetic data we cover in the <a class="af nh" href="https://www.manning.com/books/outlier-detection-in-python" rel="noopener ugc nofollow" target="_blank">book</a>, but for this article, we focus on one method, doping.</p><h1 id="38ea" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Doping Data Records</h1><p id="3d6e" class="pw-post-body-paragraph ml mm fq mn b go of mp mq gr og ms mt mu oh mw mx my oi na nb nc oj ne nf ng fj bk">Doping data records refers to taking existing data records and modifying them slightly, typically changing the values in just one, or a small number, of cells per record.</p><p id="8870" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">If the data being examined is, for example, a table related to the financial performance of a company comprised of franchise locations, we may have a row for each franchise, and our goal may be to identify the most anomalous of these. Let’s say we have features including:</p><ul class=""><li id="a99c" class="ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng ok ol om bk">Age of the franchise</li><li id="4d14" class="ml mm fq mn b go on mp mq gr oo ms mt mu op mw mx my oq na nb nc or ne nf ng ok ol om bk">Number of years with the current owner</li><li id="45b2" class="ml mm fq mn b go on mp mq gr oo ms mt mu op mw mx my oq na nb nc or ne nf ng ok ol om bk">Number of sales last year</li><li id="9133" class="ml mm fq mn b go on mp mq gr oo ms mt mu op mw mx my oq na nb nc or ne nf ng ok ol om bk">Total dollar value of sales last year</li></ul><p id="7c34" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">As well as some number of other features.</p><p id="b84b" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">A typical record may have values for these four features such as: 20 years old, 5 years with the current owner, 10,000 unique sales in the last year, for a total of $500,000 in sales in the last year.</p><p id="a776" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">We could create a doped version of this record by adjusting a value to a rare value, for example, setting the age of the franchise to 100 years. This can be done, and will provide a quick smoke test of the detectors being tested — likely any detector will be able to identify this as anomalous (assuming a value is 100 is rare), though we may be able to eliminate some detectors that are not able to detect this sort of modified record reliably.</p><p id="0be9" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">We would not necessarily remove from consideration the type of outlier detector (e.g. kNN, Entropy, or Isolation Forest) itself, but the combination of type of outlier detector, pre-processing, hyperparameters, and other properties of the detector. We may find, for example, that kNN detectors with certain hyperparameters work well, while those with other hyperparameters do not (at least for the types of doped records we test with).</p><p id="6fc6" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Usually, though, most testing will be done creating more subtle outliers. In this example, we could change the dollar value of total sales from 500,000 to 100,000, which may still be a typical value, but the combination of 10,000 unique sales with $100,000 in total sales is likely unusual for this dataset. That is, much of the time with doping, we are creating records that have unusual combinations of values, though unusual single values are sometimes created as well.</p><p id="1782" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">When changing a value in a record, it’s not known specifically how the row will become an outlier (assuming it does), but we can assume most tables have associations between the features. Changing the dollar value to 100,000 in this example, may (as well as creating an unusual combination of number of sales and dollar value of sales) quite likely create an unusual combination given the age of the franchise or the number of years with the current owner.</p><p id="6641" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">With some tables, however, there are no associations between the features, or there are only few and weak associations. This is rare, but can occur. With this type of data, there is no concept of unusual combinations of values, only unusual single values. Although rare, this is actually a simpler case to work with: it’s easier to detect outliers (we simply check for single unusual values), and it’s easier to evaluate the detectors (we simply check how well we are able to detect unusual single values). For the remainder of this article, though, we will assume there are some associations between the features and that most anomalies would be unusual combinations of values.</p><h1 id="c12b" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Working with Doped Data</h1><p id="b626" class="pw-post-body-paragraph ml mm fq mn b go of mp mq gr og ms mt mu oh mw mx my oi na nb nc oj ne nf ng fj bk">Most outlier detectors (with a small number of exceptions) have separate training and prediction steps. In this way, most are similar to predictive models. During the training step, the training data is assessed and the normal patterns within the data (for example, the normal distances between records, the frequent item sets, the clusters, the linear relationships between features, etc.) are identified. Then, during the prediction step, a test set of data (which may be the same data used for training, or may be separate data) is compared against the patterns found during training, and each row is assigned an outlier score (or, in some cases, a binary label).</p><p id="677e" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Given this, there are two main ways we can work with doped data:</p><ol class=""><li id="ece9" class="ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng os ol om bk"><strong class="mn fr">Including doped records in the training data</strong></li></ol><p id="6d5c" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">We may include some small number of doped records in the training data and then use this data for testing as well. This tests our ability to detect outliers in the currently-available data. This is a common task in outlier detection: given a set of data, we often wish to find the outliers in this dataset (though may wish to find outliers in subsequent data as well — records that are anomalous relative to the norms for this training data).</p><p id="5266" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Doing this, we can test with only a small number of doped records, as we do not wish to significantly affect the overall distributions of the data. We then check if we are able to identify these as outliers. One key test is to include both the original and the doped version of the doped records in the training data in order to determine if the detectors score the doped versions significantly higher than the original versions of the same records.</p><p id="8f08" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">We also, though, wish do check that the doped records are generally scored among the highest (with the understanding that some original, unmodified records may legitimately be more anomalous than the doped records, and that some doped records may not be anomalous).</p><p id="e805" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Given that we can test only with a small number of doped records, this process may be repeated many times.</p><p id="c98a" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">The doped data is used, however, only for evaluating the detectors in this way. When creating the final model(s) for production, we will train on only the original (real) data.</p><p id="088e" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">If we are able to reliably detect the doped records in the data, we can be reasonably confident that we are able to identify other outliers within the same data, at least outliers along the lines of the doped records (but not necessarily outliers that are substantially more subtle — hence we wish to include tests with reasonably subtle doped records).</p><p id="77f3" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk"><strong class="mn fr">2. Including doped records only in the testing data</strong></p><p id="52c8" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">It is also possible to train using only the real data (which we can assume is largely non-outliers) and then test with both the real and the doped data. This allows us to train on relatively clean data (some records in the real data will be outliers, but the majority will be typical, and there is no contamination due to doped records).</p><p id="c38d" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">It also allows us to test with the actual outlier detector(s) that may, potentially, be put in production (depending how well they perform with the doped data — both compared to the other detectors we test, and compared to our sense of how well a detector should perform at minimum).</p><p id="644d" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">This tests our ability to detect outliers in future data. This is another common scenario with outlier detection: where we have one dataset that can be assumed to be reasonable clean (either free of outliers, or containing only a small, typical set of outliers, and without any extreme outliers) and we wish to compare future data to this.</p><p id="b578" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Training with real data only and testing with both real and doped, we may test with any volume of doped data we wish, as the doped data is used only for testing and not for training. This allows us to create a large, and consequently, more reliable test dataset.</p><h1 id="a901" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Algorithms to Create Doped Data</h1><p id="7af9" class="pw-post-body-paragraph ml mm fq mn b go of mp mq gr og ms mt mu oh mw mx my oi na nb nc oj ne nf ng fj bk">There are a number of ways to create doped data, including several covered in <a class="af nh" href="https://www.manning.com/books/outlier-detection-in-python" rel="noopener ugc nofollow" target="_blank">Outlier Detection in Python</a>, each with its own strengths and weaknesses. For simplicity, in this article we cover just one option, where the data is modified in a fairly random manner: where the cell(s) modified are selected randomly, and the new values that replace the original values are created randomly.</p><p id="3dd6" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Doing this, it is possible for some doped records to not be truly anomalous, but in most cases, assigning random values will upset one or more associations between the features. We can assume the doped records are largely anomalous, though, depending how they are created, possibly only slightly so.</p><h1 id="a336" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Example</h1><p id="a2cb" class="pw-post-body-paragraph ml mm fq mn b go of mp mq gr og ms mt mu oh mw mx my oi na nb nc oj ne nf ng fj bk">Here we go through an example, taking a real dataset, modifying it, and testing to see how well the modifications are detected.</p><p id="e595" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">In this example, we use a dataset available on OpenML called abalone (<a class="af nh" href="https://www.openml.org/search?type=data&amp;sort=runs&amp;id=42726&amp;status=active" rel="noopener ugc nofollow" target="_blank">https://www.openml.org/search?type=data&amp;sort=runs&amp;id=42726&amp;status=active</a>, available under public license).</p><p id="12e5" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Although other preprocessing may be done, for this example, we one-hot encode the categorical features and use RobustScaler to scale the numeric features.</p><p id="2cca" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">We test with three outlier detectors, Isolation Forest, LOF, and ECOD, all available in the popular <a class="af nh" href="https://github.com/yzhao062/pyod" rel="noopener ugc nofollow" target="_blank">PyOD</a> library (which must be pip installed to execute).</p><p id="e496" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">We also use an Isolation Forest to clean the data (remove any strong outliers) before any training or testing. This step is not necessary, but is often useful with outlier detection.</p><p id="3163" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">This is an example of the second of the two approaches described above, where we train on the original data and test with both the original and doped data.</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="fd44" class="pc nk fq oz b bg pd pe l pf pg">import numpy as np<br/>import pandas as pd<br/>from sklearn.datasets import fetch_openml<br/>from sklearn.preprocessing import RobustScaler<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from pyod.models.iforest import IForest<br/>from pyod.models.lof import LOF<br/>from pyod.models.ecod import ECOD<br/><br/># Collect the data<br/>data = fetch_openml('abalone', version=1) <br/>df = pd.DataFrame(data.data, columns=data.feature_names)<br/>df = pd.get_dummies(df)<br/>df = pd.DataFrame(RobustScaler().fit_transform(df), columns=df.columns)<br/><br/># Use an Isolation Forest to clean the data<br/>clf = IForest() <br/>clf.fit(df)<br/>if_scores = clf.decision_scores_<br/>top_if_scores = np.argsort(if_scores)[::-1][:10]<br/>clean_df = df.loc[[x for x in df.index if x not in top_if_scores]].copy()<br/><br/># Create a set of doped records<br/>doped_df = df.copy() <br/>for i in doped_df.index:<br/>  col_name = np.random.choice(df.columns)<br/>  med_val = clean_df[col_name].median()<br/>  if doped_df.loc[i, col_name] &gt; med_val:<br/>    doped_df.loc[i, col_name] = \   <br/>      clean_df[col_name].quantile(np.random.random()/2)<br/>  else:<br/>    doped_df.loc[i, col_name] = \<br/>       clean_df[col_name].quantile(0.5 + np.random.random()/2)<br/><br/># Define a method to test a specified detector. <br/>def test_detector(clf, title, df, clean_df, doped_df, ax): <br/>  clf.fit(clean_df)<br/>  df = df.copy()<br/>  doped_df = doped_df.copy()<br/>  df['Scores'] = clf.decision_function(df)<br/>  df['Source'] = 'Real'<br/>  doped_df['Scores'] = clf.decision_function(doped_df)<br/>  doped_df['Source'] = 'Doped'<br/>  test_df = pd.concat([df, doped_df])<br/>  sns.boxplot(data=test_df, orient='h', x='Scores', y='Source', ax=ax)<br/>  ax.set_title(title)<br/><br/># Plot each detector in terms of how well they score doped records <br/># higher than the original records<br/>fig, ax = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(10, 3)) <br/>test_detector(IForest(), "IForest", df, clean_df, doped_df, ax[0])<br/>test_detector(LOF(), "LOF", df, clean_df, doped_df, ax[1])<br/>test_detector(ECOD(), "ECOD", df, clean_df, doped_df, ax[2])<br/>plt.tight_layout()<br/>plt.show()</span></pre><figure class="ot ou ov ow ox pk ph pi paragraph-image"><div role="button" tabindex="0" class="pl pm ed pn bh po"><div class="ph pi pj"><img src="../Images/592ffaafb6f576b9d48038f97e15cbc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5_V4ljtfC3pyoK71RYaBA.png"/></div></div></figure><p id="6ae6" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Here, to create the doped records, we copy the full set of original records, so will have an equal number of doped as original records. For each doped record, we select one feature randomly to modify. If the original value is below the median, we create a random value above the median; if the original is below the median, we create a random value above.</p><p id="dbe9" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">In this example, we see that IF does score the doped records higher, but not significantly so. LOF does an excellent job distinguishing the doped records, at least for this form of doping. ECOD is a detector that detects only unusually small or unusually large single values and does not test for unusual combinations. As the doping used in this example does not create extreme values, only unusual combinations, ECOD is unable to distinguish the doped from the original records.</p><p id="7a68" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">This example uses boxplots to compare the detectors, but normally we would use an objective score, very often the AUROC (Area Under a Receiver Operator Curve) score to evaluate each detector. We would also typically test many combinations of model type, pre-processing, and parameters.</p><h1 id="03f4" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Alternative Doping Methods</h1><p id="c86a" class="pw-post-body-paragraph ml mm fq mn b go of mp mq gr og ms mt mu oh mw mx my oi na nb nc oj ne nf ng fj bk">The above method will tend to create doped records that violate the normal associations between features, but other doping techniques may be used to make this more likely. For example, considering first categorical columns, we may select a new value such that both:</p><ol class=""><li id="5565" class="ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng os ol om bk">The new value is different from the original value</li><li id="c1b4" class="ml mm fq mn b go on mp mq gr oo ms mt mu op mw mx my oq na nb nc or ne nf ng os ol om bk">The new value is different from the value that would be predicted from the other values in the row. To achieve this, we can create a predictive model that predicts the current value of this column, for example a Random Forest Classifier.</li></ol><p id="c536" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">With numeric data, we can achieve the equivalent by dividing each numeric feature into four quartiles (or some number of quantiles, but at least three). For each new value in a numeric feature, we then select a value such that both:</p><ol class=""><li id="6897" class="ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng os ol om bk">The new value is in a different quartile than the original</li><li id="5555" class="ml mm fq mn b go on mp mq gr oo ms mt mu op mw mx my oq na nb nc or ne nf ng os ol om bk">The new value is in a different quartile than what would be predicted given the other values in the row.</li></ol><p id="5248" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">For example, if the original value is in Q1 and the predicted value is in Q2, then we can select a value randomly in either Q3 or Q4. The new value will, then, most likely go against the normal relationships among the features.</p><h1 id="f001" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Creating a suite of test datasets</h1><p id="eee2" class="pw-post-body-paragraph ml mm fq mn b go of mp mq gr og ms mt mu oh mw mx my oi na nb nc oj ne nf ng fj bk">There is no definitive way to say how anomalous a record is once doped. However, we can assume that on average the more features modified, and the more they are modified, the more anomalous the doped records will be. We can take advantage of this to create not a single test suite, but multiple test suites, which allows us to evaluate the outlier detectors much more accurately.</p><p id="ca61" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">For example, we can create a set of doped records that are very obvious (multiple features are modified in each record, each to a value significantly different from the original value), a set of doped records that are very subtle (only a single feature is modified, not significantly from the original value), and many levels of difficulty in between. This can help differentiate the detectors well.</p><p id="f178" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">So, we can create a suite of test sets, where each test set has a (roughly estimated) level of difficulty based on the number of features modified and the degree they’re modified. We can also have different sets that modify different features, given that outliers in some features may be more relevant, or may be easier or more difficult to detect.</p><p id="276e" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">It is, though, important that any doping performed represents the type of outliers that would be of interest if they did appear in real data. Ideally, the set of doped records also covers well the range of what you would be interested in detecting.</p><p id="5d4c" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">If these conditions are met, and multiple test sets are created, this is very powerful for selecting the best-performing detectors and estimating their performance on future data. We cannot predict how many outliers will be detected or what levels of false positives and false negatives you will see — these depend greatly on the data you will encounter, which in an outlier detection context is very difficult to predict. But, we can have a decent sense of the types of outliers you are likely to detect and to not.</p><p id="461e" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Possibly more importantly, we are also well situated to create an effective ensemble of outlier detectors. In outlier detection, ensembles are typically necessary for most projects. Given that some detectors will catch some types of outliers and miss others, while other detectors will catch and miss other types, we can usually only reliably catch the range of outliers we’re interested in using multiple detectors.</p><p id="7918" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Creating ensembles is a large and involved area in itself, and is different than ensembling with predictive models. But, for this article, we can indicate that having an understanding of what types of outliers each detector is able to detect gives us a sense of which detectors are redundant and which can detect outliers most others are not able to.</p><h1 id="74cf" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Conclusions</h1><p id="550e" class="pw-post-body-paragraph ml mm fq mn b go of mp mq gr og ms mt mu oh mw mx my oi na nb nc oj ne nf ng fj bk">It is difficult to assess how well any given outlier detector detects outliers in the current data, and even harder to asses how well it may do on future (unseen) data. It is also very difficult, given two or more outlier detectors, to assess which would do better, again on both the current and on future data.</p><p id="aab2" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">There are, though, a number of ways we can estimate these using synthetic data. In this article, we went over, at least quickly (skipping a lot of the nuances, but covering the main ideas), one approach based on doping real records and evaluating how well we’re able to score these more highly than the original data. Although not perfect, these methods can be invaluable and there is very often no other practical alternative with outlier detection.</p><p id="bc0c" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">All images are from the author.</p></div></div></div></div>    
</body>
</html>