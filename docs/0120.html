<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Exploring “Small” Vision-Language Models with TinyGPT-V</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Exploring “Small” Vision-Language Models with TinyGPT-V</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-small-vision-language-models-with-tinygpt-v-499d37a1456d?source=collection_archive---------6-----------------------#2024-01-12">https://towardsdatascience.com/exploring-small-vision-language-models-with-tinygpt-v-499d37a1456d?source=collection_archive---------6-----------------------#2024-01-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="123e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">TinyGPT-V is a “small” vision-language model that can run on a single GPU</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@scottcampit?source=post_page---byline--499d37a1456d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Scott Campit, Ph.D." class="l ep by dd de cx" src="../Images/66a9bc8111e05b8ff2992092a0eb27e9.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*VX-PjenWAYOFKnj4M-wiGg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--499d37a1456d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@scottcampit?source=post_page---byline--499d37a1456d--------------------------------" rel="noopener follow">Scott Campit, Ph.D.</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--499d37a1456d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="46b4" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Summary</h1><p id="a46f" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">AI technologies are continuing to become embedded in our everyday lives. One application of AI includes going multi-modal, such as integrating language with vision models. These vision-language models can be applied towards tasks such as video captioning, semantic searching, and many other problems.</p><p id="9be1" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">This week, I’m going to shed a spotlight towards a recent vision-language model called TinyGPT-V (<a class="af og" href="https://arxiv.org/abs/2312.16862" rel="noopener ugc nofollow" target="_blank">Arxiv</a> | <a class="af og" href="https://github.com/DLYuanGod/TinyGPT-V" rel="noopener ugc nofollow" target="_blank">GitHub</a>). What makes this multimodal language model interesting is that it is very “small” for a large language model, and can be deployed on a single GPU with as little as 8GB of GPU or CPU for inference. This is significant for maximizing the speed, efficiency, and costs of AI models in the wild.</p><p id="aa13" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">I would like to note that I’m not an author or in anyway affiliated with the authors of the model. However, as a researcher and practitioner, I thought it was an intriguing development in AI that is worth examining, especially since having more efficient models will unlock many more applications. Let’s dive in!</p><h1 id="cdba" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">The Problem: Vision-Language Models are Useful But Resource Intensive</h1><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi oj"><img src="../Images/ca5a8701ea7b8c3639e3763e992109c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pZsd9hSjz66pNPal"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Photo by <a class="af og" href="https://unsplash.com/@jpvalery?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jp Valery</a> on <a class="af og" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="a126" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Multi-modal models, such as <a class="af og" href="https://huggingface.co/blog/vision_language_pretraining" rel="noopener ugc nofollow" target="_blank">vision-language models</a>, are achieving record performance in human-aligned responses. As these models continue to improve, we could see companies begin to apply these technologies in real-world scenarios and applications.</p><p id="4548" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">However, many AI models, especially multi-modal models, require substantial computational resources for both model training and inference. This physical constraint of time, hardware resources, and capital is a bottleneck for researchers and practitioners.</p><p id="287f" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Further, these constrains currently prevent multi-modal models from being deployed in certain application interfaces, such as edge-devices. Research and development towards quantized (smaller) and high performance models is needed to address these challenges.</p><h1 id="301e" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">TinyGPT-V: “Small” Vision Language Models</h1><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi pa"><img src="../Images/1bf020bc7c57efbc6dd573e8c313f49b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VQ-FvV_sL5BUF4iB"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Photo by <a class="af og" href="https://unsplash.com/@celinehaeberly?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Céline Haeberly</a> on <a class="af og" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="49a7" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">TinyGPT-V is a 2.8B parameter vision-language model that can be trained on a 24GB GPU and uses 8GB of GPU or CPU for inference. This is significant, because other state-of-the-art “smaller” vision-language models, such as <a class="af og" href="https://huggingface.co/liuhaotian/llava-v1.5-13b" rel="noopener ugc nofollow" target="_blank">LLaVA1.5</a>, are still relatively “big” (7B and 13B parameters).</p><p id="a940" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">When benchmarking against other larger vision-language models, TinyGPT-V achieves similar performance on multiple tasks. Together, this work contributes towards a movement to make AI models more efficient by reducing their computational needs while retaining performance. Balancing these two objectives will enable vision-language models to be served directly on devices, which will offer better user experiences including reduced latency and more robustness.</p><h1 id="4358" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Related Work and Adjacent Technologies Applied in the TinyGPT-V architecture</h1><h2 id="16f7" class="pb mk fq bf ml pc pd pe mo pf pg ph mr no pi pj pk ns pl pm pn nw po pp pq pr bk">Not-So-Large Foundation Vision-Language Models (VLMs)</h2><p id="fb9d" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">VLMs learn the relationship between images/videos and text, which can be applied for many common tasks such as searching for objects within a photo (semantic search), asking questions and receiving answers on videos (VQA), and many more tasks. <strong class="nh fr">LLaVA1.5</strong> and <a class="af og" href="https://minigpt-4.github.io/" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">MiniGPT-4</strong></a> are two multi-modal large language models that are state-of-the-art as of January 2024, and are relatively smaller than similar VL foundation models. However, these VLMs still requires significant GPU usage and training hours. For example, the authors describe the training resources for LLaVA-v1.5 13B parameter model, which uses eight A100 GPUs with 80GB RAM for 25.5 hours of training. This is a barrier towards individuals and institutions that wish to study, develop, and apply these models in the wild.</p><p id="6a94" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">TinyGPT-V is one of the latest VLMs that aims to address this issue. It uses two separate foundation models for the vision and language components: the <a class="af og" href="https://arxiv.org/abs/2211.07636" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">EVA</strong></a> encoder was used as the vision component, while <a class="af og" href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">Phi-2</strong></a> was used as the language model. Briefly, EVA scales up to a 1B parameter vision transformer model that is pre-trained to reconstruct masked image-text features. Phi-2 is a 2.7B parameter language model that was trained on curated synthetic and web datasets. The authors were able to merge these two models and quantize them to have a total parameter size of 2.8B.</p><p id="d74a" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Shown below is the performance of TinyGPT-V compared to other VLMs with various visual language tasks. Notably, TinyGPT-V performs similarly to <a class="af og" href="https://arxiv.org/abs/2301.12597" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">BLIP-2</strong></a>, likely due to the pre-trained Q-Former module that was taken from BLIP-2. Further, it appears that <a class="af og" href="https://arxiv.org/abs/2305.06500" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">InstructBLIP</strong></a> achieved better performance compared to TinyGPT-V, although it is noted that the smallest InstructBLIP model is trained with 4B parameters. Depending on the application, this trade-off may be worth it to a practitioner, and additional analyses would need to be conducted to explain for this difference.</p><p id="07da" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">The following datasets the model is trained with include:</p><ul class=""><li id="2078" class="nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa ps pt pu bk"><a class="af og" href="https://cs.stanford.edu/people/dorarad/gqa/about.html" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">GQA</strong></a>: Real-world visual reasoning and compositional QA</li><li id="19ec" class="nf ng fq nh b go pv nj nk gr pw nm nn no px nq nr ns py nu nv nw pz ny nz oa ps pt pu bk"><a class="af og" href="https://paperswithcode.com/dataset/vsr" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">VSR</strong></a>: text-image pairs in english with spatial relationships</li><li id="ba54" class="nf ng fq nh b go pv nj nk gr pw nm nn no px nq nr ns py nu nv nw pz ny nz oa ps pt pu bk"><a class="af og" href="https://iconqa.github.io/" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">IconQA</strong></a>: visual understanding and reasoning with icon images</li><li id="28ad" class="nf ng fq nh b go pv nj nk gr pw nm nn no px nq nr ns py nu nv nw pz ny nz oa ps pt pu bk"><a class="af og" href="https://www.vizwiz.com/" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">VizWiz</strong></a>: visual queries derived from a photo taken by a visually impaired individual with a smartphone and supplemented with 10 answers.</li><li id="c9f5" class="nf ng fq nh b go pv nj nk gr pw nm nn no px nq nr ns py nu nv nw pz ny nz oa ps pt pu bk"><a class="af og" href="https://hatefulmemeschallenge.com/" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">HM</strong></a>: a multimodal collection designed to detect hateful content in memes.</li></ul><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi qa"><img src="../Images/712ba98abc73a68df1461e3f4618fbef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3TUmHgdUa8Nfn2apIFW2nw.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">TinyGPT-V benchmark performance against similar state-of-the-art “smaller” vision language models (adapted from Figure 1 of <a class="af og" href="https://arxiv.org/abs/2312.16862" rel="noopener ugc nofollow" target="_blank">Yuan et al., 2023</a>). Note that we should assume that the authors denote their model as “TinyGPT-4”. It’s performance is comparable to BLIP-2, which is ~3.1B parameters. InstructBLIP has better performance across different tasks, but is notably ~4B parameters. This is much bigger than TinyGPT-V, which is ~2.1B parameters in size.</figcaption></figure><h2 id="4cf2" class="pb mk fq bf ml pc pd pe mo pf pg ph mr no pi pj pk ns pl pm pn nw po pp pq pr bk">Cross-modal alignment of visual and language features</h2><p id="e71b" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">VLM training consists of several objective functions to optimize for to a) expand the utility of VLMs, b) increase VLM general performance, and c) mitigate the risk of catastrophic forgetting. In addition to different objective functions, there are several model architectures or methods to learn and merge the joint representation of vision and language features. We will discuss the relevant layers for training TinyGPT-V, which are shown below as blocks.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi qb"><img src="../Images/70636cbaddfef36fe846e263bc823d27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R3b0pGl8glKquaDdsij_FA.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">TinyGPT-V training schemes, adapted from Figure 2 (<a class="af og" href="https://arxiv.org/abs/2312.16862" rel="noopener ugc nofollow" target="_blank">Yuan et al., 2023</a>). Stage 1 was a warm-up pre-training stage. The second stage is a pre-training stage to train the LoRA module. The third training stage aims to instruction-tune the model. Finally, the fourth training stage aims to fine-tune the model for various multi-modal tasks.</figcaption></figure><p id="9c0b" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">The <strong class="nh fr">Q-Former</strong> described in BLIP-2 paper was used to learn the joint representation from the aligned image-text data. The Q-Former method optimizes for three objectives to learn the vision-language representation:</p><ol class=""><li id="5153" class="nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa qc pt pu bk"><em class="qd">Image-Text Matching:</em> Learn fine-grained alignment between the image and text representation</li><li id="6dc8" class="nf ng fq nh b go pv nj nk gr pw nm nn no px nq nr ns py nu nv nw pz ny nz oa qc pt pu bk"><em class="qd">Image-Text Contrastive Learning:</em> Align the image and text representation to maximize the mutual information gained</li><li id="aa57" class="nf ng fq nh b go pv nj nk gr pw nm nn no px nq nr ns py nu nv nw pz ny nz oa qc pt pu bk"><em class="qd">Image-Grounded Text Generation:</em> Train the model to generate text, given input images</li></ol><p id="8480" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Following the Q-former layer, they employed a pre-trained linear projection layer from MiniGPT-4 (Vicuna 7B) in order to accelerate learning. Then they apply a linear projection layer to embed these features into the Phi-2 language model.</p><h2 id="cc9f" class="pb mk fq bf ml pc pd pe mo pf pg ph mr no pi pj pk ns pl pm pn nw po pp pq pr bk">Normalization</h2><p id="db92" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Training smaller large-scale language models from different modalities presented significant challenges. During their training process, they found that the model outputs were susceptible to NaN or INF values. Much of this was attributed to the vanishing gradient problem, as the model had a limited number of trainable parameters. To address these issues, they applied several normalization procedures in the Phi-2 model to ensure that the data is in an adequate representation for model training.</p><p id="4a68" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">There are three normalization techniques that are applied throughout the Phi-2 model with minor adjustments from their vanilla implementation. They updated the <strong class="nh fr">LayerNorm</strong> mechanism that is applied within each hidden layer by including a small number for numerical stability. Further they implemented <strong class="nh fr">RMSNorm</strong> as a post-normalization procedure after each Multi-Head Attention Layer. Finally, they incorporated a <strong class="nh fr">Query-Key Normalization</strong> procedure, which they determined as being important in low-resource learning scenarios.</p><h2 id="182d" class="pb mk fq bf ml pc pd pe mo pf pg ph mr no pi pj pk ns pl pm pn nw po pp pq pr bk">Parameter Efficient Fine-Tuning</h2><p id="97c5" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Fine-tuning models is essential to achieve better performance on downstream tasks or domain areas that are not covered in pre-training. This is an essential step to provide huge performance gains compared to out-of-the-box foundation models.</p><p id="57c4" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">One intuitive way to fine-tune a model is to update all pre-trained parameters with the new task or domain area in mind. However, there are issues with this way of fine-tuning large language models, as it requires a full copy of the fine-tuned model for each task. <strong class="nh fr">Parameter Efficient Fine-Tuning (PEFT)</strong> is an active area of research in the AI community, where a smaller number of task-specific parameters are updated while most of the foundation model parameters are frozen.</p><p id="449c" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk"><strong class="nh fr">Low-Rank Adaptation</strong> (<a class="af og" href="https://github.com/microsoft/LoRA" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">LoRA</strong></a>) is a specific PEFT method that was used to fine-tune TinyGPT-V. At a high-level, LoRA freezes the pre-trained model weights, and injects trainable rank decomposition matrices into each layer of a transformer, which reduces the number of trainable parameters for downstream tasks. Shown below is how the LoRA module was applied to the TinyGPT-V model.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi qe"><img src="../Images/7170418f71365ad34f801255abf522a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e3UGQBX9BQ8MNrFbr4LxDw.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Adapted from Figure 3 (<a class="af og" href="https://arxiv.org/abs/2312.16862" rel="noopener ugc nofollow" target="_blank">Yuan et al., 2023</a>). Low-Rank Adaptation (LoRA) was applied to fine-tune TinyGPT-V. Panel c) hows how LoRA was implemented in TinyGPT-V. Panel d) shows the query-key normalization method described in the previous section.</figcaption></figure><h1 id="9e72" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Conclusions and parting thoughts</h1><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi qf"><img src="../Images/8d6eb5e4f2fac6192e6a2b70598621f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DKX4fhGijYuciEGC"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Photo by <a class="af og" href="https://unsplash.com/@mourimoto?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Mourizal Zativa</a> on <a class="af og" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="a95e" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">TinyGPT-V contributes to a body of research for making multi-modal large language models more efficient. Innovations in multiple areas, such as PEFT, quantization methods, and model architectures will be essential to getting models as small as possible while not sacrificing too much performance. As was observed in the pre-print, TinyGPT-V achieves a similar performance to other smaller VLMs. It matches BLIP-2 performance (smallest model is 3.1B parameters), and while it falls short of InstructBLIP’s performance on similar benchmarks, it is still smaller in size (TinyGPT-V is 2.8B parameters versus InstructBLIP’s 4B).</p><p id="661a" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">For future directions, there are certainly aspects that could be explored to improve TinyGPT’s performance. For instance, other PEFT methods could have been applied for fine-tuning. From the pre-print, it is unclear if these model architecture decisions were purely based on empirical performance, or if it was a matter of convenience for implementation. This should be studied further.</p><p id="74a4" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Finally, at the time of this writing the pre-trained model and the model fine-tuned for instruction learning are available, while the multi-task model is currently a test version on GitHub. As developers and users use the model, further improvements could shed insights into additional strengths and weaknesses with TinyGPT-V. But altogether, I thought this was a useful study for designing more efficient VLMs.</p></div></div></div><div class="ab cb qg qh qi qj" role="separator"><span class="qk by bm ql qm qn"/><span class="qk by bm ql qm qn"/><span class="qk by bm ql qm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b719" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">I hope you found this breakdown of TinyGPT-V useful for your own applications! If you want to chat more about AI or if you’re in the Bay Area and just want to grab some coffee, please feel free to reach out on <a class="af og" href="https://www.linkedin.com/in/scottcampit/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>. Otherwise, you can also catch me on <a class="af og" href="https://www.torchstack.ai/" rel="noopener ugc nofollow" target="_blank">torchstack.ai</a>, where we offer custom AI solutions to customers and businesses.</p></div></div></div></div>    
</body>
</html>