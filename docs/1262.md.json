["```py\npip install quixstreams\n```", "```py\nfrom dataclasses import dataclass, asdict # used to define the data schema\nfrom datetime import datetime # used to manage timestamps\nfrom time import sleep # used to slow down the data generator\nimport uuid # used for message id creation\nimport json # used for serializing data\n\nfrom quixstreams import Application\n```", "```py\n app = Application(broker_address='localhost:19092')\n\ndestination_topic = app.topic(name='raw-temp-data', value_serializer=\"json\")\n```", "```py\n@dataclass\nclass Temperature:\n    ts: datetime\n    value: int\n\n    def to_json(self):\n        # Convert the dataclass to a dictionary\n        data = asdict(self)\n        # Format the datetime object as a string\n        data['ts'] = self.ts.isoformat()\n        # Serialize the dictionary to a JSON string\n        return json.dumps(data)\n```", "```py\ni = 0\nwith app.get_producer() as producer:\n    while i < 10000:\n        sensor_id = random.choice([\"Sensor1\", \"Sensor2\", \"Sensor3\", \"Sensor4\", \"Sensor5\"])\n       temperature = Temperature(datetime.now(), random.randint(0, 100))\n        value = temperature.to_json()\n\n        print(f\"Producing value {value}\")\n        serialized = destination_topic.serialize(\n            key=sensor_id, value=value, headers={\"uuid\": str(uuid.uuid4())}\n        )\n        producer.produce(\n            topic=destination_topic.name,\n            headers=serialized.headers,\n            key=serialized.key,\n            value=serialized.value,\n        )\n        i += 1\n        sleep(random.randint(0, 1000) / 1000)\n```", "```py\npython sensor_stream_producer.py\n```", "```py\n[data produced]\n```", "```py\nimport os\nimport random\nimport json\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass\nimport logging\nfrom quixstreams import Application\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n```", "```py\nTOPIC = \"raw-temperature\" # defines the input topic\nSINK = \"agg-temperature\"  # defines the output topic\nWINDOW = 10  # defines the length of the time window in seconds\nWINDOW_EXPIRES = 1 # defines, in seconds, how late data can arrive before it is excluded from the window\n```", "```py\napp = Application(\n    broker_address='localhost:19092',\n    consumer_group=\"quix-stream-processor\",\n    auto_offset_reset=\"earliest\",\n)\n```", "```py\ninput_topic = app.topic(TOPIC, value_deserializer=\"json\")\noutput_topic = app.topic(SINK, value_serializer=\"json\")\n\nsdf = app.dataframe(input_topic)\nsdf = sdf.update(lambda value: logger.info(f\"Input value received: {value}\"))\n```", "```py\ndef custom_ts_extractor(value):\n\n    # Extract the sensor's timestamp and convert to a datetime object\n    dt_obj = datetime.strptime(value[\"ts\"], \"%Y-%m-%dT%H:%M:%S.%f\") # \n\n    # Convert to milliseconds since the Unix epoch for efficent procesing with Quix\n    milliseconds = int(dt_obj.timestamp() * 1000)\n    value[\"timestamp\"] = milliseconds\n    logger.info(f\"Value of new timestamp is: {value['timestamp']}\")\n\n    return value[\"timestamp\"]\n\n# Override the previously defined input_topic variable so that it uses the custom timestamp extractor \ninput_topic = app.topic(TOPIC, timestamp_extractor=custom_ts_extractor, value_deserializer=\"json\") \n```", "```py\ndef initializer(value: dict) -> dict:\n\n    value_dict = json.loads(value)\n    return {\n        'count': 1,\n        'min': value_dict['value'],\n        'max': value_dict['value'],\n        'mean': value_dict['value'],\n    }\n```", "```py\ndef reducer(aggregated: dict, value: dict) -> dict:\n    aggcount = aggregated['count'] + 1\n    value_dict = json.loads(value)\n    return {\n        'count': aggcount,\n        'min': min(aggregated['min'], value_dict['value']),\n        'max': max(aggregated['max'], value_dict['value']),\n        'mean': (aggregated['mean'] * aggregated['count'] + value_dict['value']) / (aggregated['count'] + 1)\n    }\n```", "```py\n### Define the window parameters such as type and length\nsdf = (\n    # Define a tumbling window of 10 seconds\n    sdf.tumbling_window(timedelta(seconds=WINDOW), grace_ms=timedelta(seconds=WINDOW_EXPIRES))\n\n    # Create a \"reduce\" aggregation with \"reducer\" and \"initializer\" functions\n    .reduce(reducer=reducer, initializer=initializer)\n\n    # Emit results only for closed 10 second windows\n    .final()\n)\n\n### Apply the window to the Streaming DataFrame and define the data points to include in the output\nsdf = sdf.apply(\n    lambda value: {\n        \"time\": value[\"end\"], # Use the window end time as the timestamp for message sent to the 'agg-temperature' topic\n        \"temperature\": value[\"value\"], # Send a dictionary of {count, min, max, mean} values for the temperature parameter\n    }\n)\n```", "```py\nsdf = sdf.to_topic(output_topic)\nsdf = sdf.update(lambda value: logger.info(f\"Produced value: {value}\"))\n\nif __name__ == \"__main__\":\n    logger.info(\"Starting application\")\n    app.run(sdf)\n```", "```py\npython sensor_stream_producer.py\n```", "```py\npython sensor_stream_processor.py\n```"]