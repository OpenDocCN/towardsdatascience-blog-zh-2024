- en: Most Data Quality Initiatives Fail Before They Start. Here’s Why.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/most-data-quality-initiatives-fail-before-they-start-heres-why-e66123b7bb3b?source=collection_archive---------6-----------------------#2024-07-23](https://towardsdatascience.com/most-data-quality-initiatives-fail-before-they-start-heres-why-e66123b7bb3b?source=collection_archive---------6-----------------------#2024-07-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Show me your data quality scorecard and I’ll tell you whether you will be successful
    a year from now.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://barrmoses.medium.com/?source=post_page---byline--e66123b7bb3b--------------------------------)[![Barr
    Moses](../Images/4c74558ee692a85196d5a55ac1920718.png)](https://barrmoses.medium.com/?source=post_page---byline--e66123b7bb3b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e66123b7bb3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e66123b7bb3b--------------------------------)
    [Barr Moses](https://barrmoses.medium.com/?source=post_page---byline--e66123b7bb3b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e66123b7bb3b--------------------------------)
    ·7 min read·Jul 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fac1a14f378fe70f85ffa69e64cb0a93.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Braden Collum](https://unsplash.com/@bradencollum?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Every day I talk to organizations ready to dedicate a tremendous amount of time
    and resources towards data quality initiatives doomed to fail.
  prefs: []
  type: TYPE_NORMAL
- en: It’s no revelation that incentives and KPIs drive good behavior. Sales compensation
    plans are scrutinized so closely that they often rise to the topic of board meetings.
    What if we gave the same attention to data quality scorecards?
  prefs: []
  type: TYPE_NORMAL
- en: Even in their heyday, traditional data quality scorecards from the Hadoop era
    were rarely wildly successful. I know this because prior to starting Monte Carlo,
    I spent years as an operations VP trying to create data quality standards that
    drove trust and adoption.
  prefs: []
  type: TYPE_NORMAL
- en: Over the past few years, advances in the cloud and metadata management have
    made organizing silly amounts of data possible.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering processes are starting to trend towards the level of maturity
    and rigor of more longstanding engineering disciplines. And of course, AI has
    the potential to streamline everything.
  prefs: []
  type: TYPE_NORMAL
- en: While this problem isn’t — and probably will never be — completely solved, I
    have seen organizations adopt best practices that are the difference between initiative
    success…and having another kick-off meeting 12 months later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are 4 key lessons for building data quality scorecards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Know what data matters](#cbd9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Measure the machine](#aeb8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Get your carrots and sticks right](#30dd)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automate evaluation and discovery](#0a63)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Know what data matters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most sure way to fail any data related initiative is to assume all data
    is of equal value. And the best only way to determine what matters is to talk
    to the business.
  prefs: []
  type: TYPE_NORMAL
- en: 'Brandon Beidel at Red Ventures [articulates a good place to start](https://www.montecarlodata.com/blog-one-sla-at-a-time-our-data-quality-journey-at-red-digital/):'
  prefs: []
  type: TYPE_NORMAL
- en: '“I’d ask:'
  prefs: []
  type: TYPE_NORMAL
- en: How do you use this table?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When do you look at this data? When do you report this data? Does this data
    need to be up to the minute, hourly, daily?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What purpose does this serve?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who needs to get notified if this data is delayed?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, this may be easier said than done if you work for a sprawling organization
    with tens of thousands of employees distributed across the globe.
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, my recommendation is to s**tart with your most business critical
    data business units** (if you don’t know that, I can’t help you!). Start a discussion
    on requirements and priorities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just remember: **prove the concept first, scale second**. You’d be shocked
    how many people do it the other way around.'
  prefs: []
  type: TYPE_NORMAL
- en: Measure the machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the enduring challenges to this type of endeavor, in a nutshell, is **data
    quality resists standardization**. Quality is, and should be, in the eye of use
    case.
  prefs: []
  type: TYPE_NORMAL
- en: The [six dimensions of data quality](https://www.dataversity.net/data-quality-dimensions/)
    are a vital part of any data quality scorecard and an important starting point,
    but for many teams, that’s just the beginning — and every data product is different.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a financial report may need to be highly accurate with some margin
    for timeliness whereas a machine learning model may be the exact opposite.
  prefs: []
  type: TYPE_NORMAL
- en: From an implementation perspective this means **measuring data quality has typically
    been radically federated**. Data quality is measured on a table-by-table basis
    by different analysts or stewards with wildly different data quality rules given
    wildly different weights.
  prefs: []
  type: TYPE_NORMAL
- en: This makes sense to a degree, but so much gets lost in translation.
  prefs: []
  type: TYPE_NORMAL
- en: Data is multi-use and shared across use cases. Not only is one person’s “yellow”
    quality score another person’s “green,” but it’s often incredibly **difficult
    for data consumers to even understand what a “yellow” score means** or how it’s
    been graded. They also frequently miss the implications of a green table being
    fed data by a red one (you know, garbage in, garbage out…).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d957c3159e4a5ced40d4953fdf140f0.png)'
  prefs: []
  type: TYPE_IMG
- en: What is the meaning of a “yellow” scorecard? Photo by [Keiron Crasktellanos](https://unsplash.com/@crasktellanos?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Surfacing the number of breached rules is important, of course, but you also
    need to:'
  prefs: []
  type: TYPE_NORMAL
- en: Contextualize it as much as possible,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have an aggregated end-to-end data product view,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invest in some strong no-code data profiling, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Realize it’s not sufficient.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So then what else do you need? **You need to measure the machine.**
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the components in the production and delivery of data that generally
    result in high quality. This is much easier to standardize. It’s also easier to
    understand across business units and teams.
  prefs: []
  type: TYPE_NORMAL
- en: '[Airbnb Midas](https://medium.com/airbnb-engineering/data-quality-score-the-next-chapter-of-data-quality-at-airbnb-851dccda19c3)
    is one of the more well known internal data quality score and certification programs
    and rightfully so. They lean heavily into this concept. **They measure data accuracy–
    but reliability, stewardship, and usability actually comprise 60% of the total
    score.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many data teams are still in the process of formalize their own standards,
    but the components we have found to highly correlate to data health include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The previously mentioned six dimensions of data quality** (validity, completeness,
    consistency, timeliness, uniqueness, accuracy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usability & Stewardship**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Documentation**: Some level of semantic meaning for both the data asset,
    its use, and past incidents. One online travel search company scores an asset
    based on how and where it’s cataloged along with the completeness of its metadata
    for two of its 6 categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lineage**: Ability to trace the data’s provenance at the field level across
    systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage**: The number of queries a table receives and the number of data products
    with downstream dependencies. This can be a “key asset score” and it has a flywheel
    effect. You focus your reliability efforts on what’s most utilized, and people
    trust what’s popular.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System Reliability**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Monitoring**: Generally if a data product has strong coverage not only on
    the last mile table but all the way upstream, it indicates a well curated asset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Freshness**: Data freshness requirements will vary by data product type,
    but it is a table level metric where deviations from the norm can be identified
    and surfaced. Many organizations like [Roche Diagnostics](https://www.montecarlodata.com/blog-how-roche-uses-dataops-to-build-data-products-and-data-mesh/)
    will have specific freshness SLAs for their data products and measure the level
    of adherence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volume**: A relatively steady number of rows a table receives is often a
    sign of a well functioning pipeline and data delivery system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Schema**: At the very least you want consumers to have visibility into schema
    changes. For your most critical pipelines, you ideally want some level of [schema
    enforcement or data contract](https://www.montecarlodata.com/blog-data-contracts-explained/)
    so that you know when changes at the source break assets downstream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operational Response:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Ownership**: Does an asset have an owner? Bonus for if it has both a technical
    and business owner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Notification Channels & Communication:** Data delivery is a complex process
    involving multiple handoffs from ingestion to aggregation to consumption. On top
    of that, you ideally have multiple teams using a data asset (or else your mesh
    is more of a silo). The only way to have a reliable data product in this environment
    is to have a central communication channel to highlight and discuss changes and
    incidents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average Time To Fixed**: Arguably the most important indicator of how much
    you can trust a dataset is in how quickly the support team responds and fixes
    incidents that arise. Bad data is inevitable. Great incident response is intentional.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get your carrots and sticks right
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/7dfb99e634365a2601ca0dd9c77aa74e.png)'
  prefs: []
  type: TYPE_IMG
- en: Incentivize quality data for both producers and consumers. Photo by [Jonathan
    Pielmayer](https://unsplash.com/@jonathanpielmayer?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: “Yay, another set of processes we’re required to follow!”… said no one ever.
  prefs: []
  type: TYPE_NORMAL
- en: Remember the purpose of measuring data health isn’t to measure data health.
    The point, as [Clark at Airbnb put it](https://www.youtube.com/watch?v=Lv-bFDSzrqw),
    is to “drive a preference for producing and using high quality data.”
  prefs: []
  type: TYPE_NORMAL
- en: The best practices I’ve seen here are to have a minimum set of requirements
    for data to be on-boarded onto the platform (stick) and a much more stringent
    set of requirements to be certified at each level (carrot).
  prefs: []
  type: TYPE_NORMAL
- en: Certification works as a carrot because producers actually want consumers to
    use their data, and consumers will quickly discern and develop a taste for highly
    reliable data.
  prefs: []
  type: TYPE_NORMAL
- en: Automate evaluation and discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Almost nothing in data management is successful without some degree of automation
    and the ability to self-serve. Airbnb discarded any scoring criteria that 1) wasn’t
    immediately understandable and 2) couldn’t be measured automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Your organization must do the same. Even if it’s the best scoring criteria that
    has ever been conceived, if you do not have a set of solutions that will automatically
    collect and surface it, into the trash bin it must go.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31528d08e2f71cbeffbe29fa4642d848.png)'
  prefs: []
  type: TYPE_IMG
- en: Image courtesy of the author.
  prefs: []
  type: TYPE_NORMAL
- en: The most common ways I’ve seen this done are with [data observability and quality
    solutions, and data catalogs](https://www.thoughtworks.com/en-us/insights/blog/data-strategy/building-an-amazon-com-for-your-data-products).
    Roche, for example, does this and layers on access management as part of creating,
    surfacing and governing trusted data products.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e275902f9bce7f2f7b6c906762f24b90.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://www.montecarlodata.com/blog-how-roche-uses-dataops-to-build-data-products-and-data-mesh/).'
  prefs: []
  type: TYPE_NORMAL
- en: Of course this can also be done by manually stitching together the metadata
    from multiple data systems into a homegrown discoverability portal, but just be
    mindful of the maintenance overhead.
  prefs: []
  type: TYPE_NORMAL
- en: What’s measured is managed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data teams have made big investments into their modern data and AI platforms.
    But to maximize this investment, the organization — both data producers and consumers
    — must fully adopt and trust the data being provided.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the day, what’s measured is managed. And isn’t that what matters?
  prefs: []
  type: TYPE_NORMAL
