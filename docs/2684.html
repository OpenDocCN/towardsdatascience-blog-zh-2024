<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Classify Jira Tickets with GenAI On Amazon Bedrock</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Classify Jira Tickets with GenAI On Amazon Bedrock</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classify-jira-tickets-with-genai-on-amazon-bedrock-69450d4d8b21?source=collection_archive---------4-----------------------#2024-11-04">https://towardsdatascience.com/classify-jira-tickets-with-genai-on-amazon-bedrock-69450d4d8b21?source=collection_archive---------4-----------------------#2024-11-04</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7396" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Replace traditional NLP approaches with prompt engineering and Large Language Models (LLMS) for Jira ticket text classification. A code sample walkthrough</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@tannermcrae?source=post_page---byline--69450d4d8b21--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Tanner McRae" class="l ep by dd de cx" src="../Images/bb80770681d29438860fe83aba8a22fb.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*2P98zdddlhe13kVsJGrxWQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--69450d4d8b21--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@tannermcrae?source=post_page---byline--69450d4d8b21--------------------------------" rel="noopener follow">Tanner McRae</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--69450d4d8b21--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 4, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/e829c8a66e2c4fbdce6ee9ed37ea3f46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xi3VCjVh8ydotKI2"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@anniespratt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Annie Spratt</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="c7c6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Remember the days when classifying text meant embarking on a machine learning journey? If you’ve been in the ML space long enough, you’ve probably witnessed at least one team disappear down the rabbit hole of building the “perfect” text classification system. The story usually goes something like this:</p><ul class=""><li id="14cc" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk"><strong class="nf fr">Month 1</strong>: “We’ll just quickly train a NLP model!”</li><li id="3180" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk"><strong class="nf fr">Month 2:</strong> “We need more training data…”</li><li id="83d8" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk"><strong class="nf fr">Month 3: “</strong>This is good enough”</li></ul><p id="1e90" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For years, text classification has fallen into the realm of classic ML. Early in my career, I remember training a support vector machine (SVM) for email classification. Lots of preprocessing, iteration, data collection, and labeling.</p><p id="9829" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">But here’s the twist: it’s 2024, and generative AI models can <strong class="nf fr">“generally”</strong> classify text out of the box! You can build a robust ticket classification system without, collecting thousands of labeled training examples, managing ML training pipelines, or maintaining custom models.</p><p id="3715" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this post, we’ll go over how to setup a Jira ticket classification system using large language models on Amazon Bedrock and other AWS services.</p><p id="10b3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">DISCLAIMER</strong>: I am a GenAI Architect at AWS and my opinions are my own.</p><h2 id="d2b8" class="oh oi fq bf oj ok ol om on oo op oq or nm os ot ou nq ov ow ox nu oy oz pa pb bk">Why Classify Jira Tickets?</h2><p id="e71d" class="pw-post-body-paragraph nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny fj bk">A common ask from companies is to understand how teams spend their time. Jira has tagging features, but it can sometimes fall short through human error or lack of granularity. By doing this exercise, organizations can get better insights into their team activities, enabling data-driven decisions about resource allocation, project investment, and deprecation.</p><h2 id="ec95" class="oh oi fq bf oj ok ol om on oo op oq or nm os ot ou nq ov ow ox nu oy oz pa pb bk">Why Not Use Other NLP Approaches?</h2><p id="858d" class="pw-post-body-paragraph nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny fj bk">Traditional ML models and smaller transformers like BERT need hundreds (or thousands) of labeled examples, while LLMs can classify text out of the box. In our Jira ticket classification tests, a prompt-engineering approach matched or beat traditional ML models, processing 10k+ annual tickets for ~$10/year using Claude Haiku (excluding other AWS Service costs). Also, prompts are easier to update than retraining models.</p><h1 id="5b07" class="ph oi fq bf oj pi pj gq on pk pl gt or pm pn po pp pq pr ps pt pu pv pw px py bk">Code Sample</h1><p id="00a3" class="pw-post-body-paragraph nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny fj bk">This <a class="af nc" href="https://github.com/aws-samples/jira-ticket-classification" rel="noopener ugc nofollow" target="_blank">github repo</a> contains a sample application that connects to Jira Cloud, classifies tickets, and outputs them in a format that can be consumed by your favorite dashboarding tool (Tableu, Quicksight, or any other tool that supports CSVs).</p><blockquote class="pz qa qb"><p id="98a9" class="nd ne qc nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Important Notice: This project deploys resources in your AWS environment using Terraform. You will incur costs for the AWS resources used. Please be aware of the pricing for services like Lambda, Bedrock, Glue, and S3 in your AWS region.</p></blockquote><p id="e4b8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Pre Requisites</strong></p><p id="9f94" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You’ll need to have terraform installed and the AWS CLI installed in the environment you want to deploy this code from</p><ul class=""><li id="0ff4" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk"><a class="af nc" href="https://github.com/tfutils/tfenv" rel="noopener ugc nofollow" target="_blank">Install Terraform using tfenv</a></li><li id="3b2d" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk"><a class="af nc" href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html" rel="noopener ugc nofollow" target="_blank">Install AWS CLI &amp; configure</a></li></ul><h1 id="86d2" class="ph oi fq bf oj pi pj gq on pk pl gt or pm pn po pp pq pr ps pt pu pv pw px py bk">Architecture</h1><p id="ac5e" class="pw-post-body-paragraph nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny fj bk">The architecture is pretty straight forward. You can find details below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qd"><img src="../Images/7a1dc84efb3bbf496059d6045ca74266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAFhczlUbXHeAtFGylLACQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by the author</figcaption></figure><p id="5397" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Step 1:</strong> An AWS Lambda function is triggered on a cron job to fetch jira tickets based on a time window. Those tickets are then formatted and pushed to an S3 bucket under the <strong class="nf fr">/unprocessed</strong> prefix.</p><p id="710c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Step 2:</strong> A Glue job is triggered off <strong class="nf fr">/unprocessed</strong> object puts. This runs a PySpark deduplication task to ensure no duplicate tickets make their way to the dashboard. The deduplicated tickets are then put to the <strong class="nf fr">/staged</strong> prefix. This is useful for cases where you manually upload tickets as well as rely on the automatic fetch. <strong class="nf fr">If you can ensure no duplicates, you can remove this step.</strong></p><p id="9f61" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Step 3: </strong>A classification task is kicked off on the new tickets by calling Amazon Bedrock to classify the tickets based on a prompt to a large language model (LLM). After classification, the finished results are pushed to the <strong class="nf fr">/processed</strong> prefix. From here, you can pick up the processed CSV using any dashboarding tool you’d like that can consume a CSV.</p><h1 id="fc17" class="ph oi fq bf oj pi pj gq on pk pl gt or pm pn po pp pq pr ps pt pu pv pw px py bk">Getting Started</h1><p id="5d57" class="pw-post-body-paragraph nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny fj bk">To get started, clone the github repo above and move to the /terraform directory</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="a079" class="qi oi fq qf b bg qj qk l ql qm">$ git clone https://github.com/aws-samples/jira-ticket-classification.git<br/><br/>$ cd jira-ticket-classification/terraform</span></pre><p id="8a17" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Run terraform init, plan, &amp; apply. Make sure you have terraform installed on your computer and the AWS CLI configured.</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="0b16" class="qi oi fq qf b bg qj qk l ql qm">$ terraform init<br/><br/>$ terraform plan<br/><br/>$ terraform apply</span></pre><p id="0c19" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Once the infrastructure is deployed into your account, you can navigate to AWS Secrets Manager and update the secret with your Jira Cloud credentials. You’ll need an API key, base url, and email to enable the automatic pull</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qn"><img src="../Images/74f14dbab21c582550d50baf14a9b905.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ziyFfFwkOLrKLzsuAHv2mw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by the author</figcaption></figure><p id="0765" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And that’s it!</p><p id="906d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can (1) wait for the Cron to kick off an automatic fetch, (2) export the tickets to CSV and upload them to the /unprocessed S3 bucket prefix, or (3) manually trigger the Lambda function using a test.</p><h1 id="af41" class="ph oi fq bf oj pi pj gq on pk pl gt or pm pn po pp pq pr ps pt pu pv pw px py bk">How Does It Work?</h1><h2 id="9295" class="oh oi fq bf oj ok ol om on oo op oq or nm os ot ou nq ov ow ox nu oy oz pa pb bk">Jira Fetch:</h2><p id="af6f" class="pw-post-body-paragraph nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny fj bk">Jira fetch uses a Lambda function with a Cloudwatch cron event to trigger it. The Lambda pulls in the AWS Secret and uses a get request in a while loop to retrieve paginated results until the JQL query completes:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="f1e9" class="qi oi fq qf b bg qj qk l ql qm">def fetch_jira_issues(base_url, project_id, email, api_key):<br/>    url = f"{base_url}/rest/api/3/search"<br/><br/>    # Calculate the date 8 days ago<br/>    eight_days_ago = (datetime.now() - timedelta(days=8)).strftime("%Y-%m-%d")<br/>    <br/>    # Create JQL<br/>    jql = f"project = {project_id} AND created &gt;= '{eight_days_ago}' ORDER BY created DESC"<br/><br/>    # Pass into params of request.<br/>    params = {<br/>        "jql": jql,<br/>        "startAt": 0<br/>    }<br/>    all_issues = []<br/><br/>    auth = HTTPBasicAuth(email, api_key)<br/>    headers = {"Accept": "application/json"}<br/><br/>    while True:<br/>        response = requests.get(url, headers=headers, params=params, auth=auth)<br/>        if response.status_code != 200:<br/>            raise Exception(f"Failed to fetch issues for project {project_id}: {response.text}")<br/>        <br/>        data = json.loads(response.text)<br/>        issues = data['issues']<br/>        all_issues.extend(issues)<br/>        <br/>        if len(all_issues) &gt;= data['total']:<br/>            break<br/>        <br/>        params['startAt'] = len(all_issues)<br/><br/>    return all_issues</span></pre><p id="af12" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It then creates a string representation of a CSV and uploads it into S3:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="374d" class="qi oi fq qf b bg qj qk l ql qm">def upload_to_s3(csv_string, bucket, key):<br/>    try:<br/>        s3_client.put_object(<br/>            Bucket=bucket,<br/>            Key=key,<br/>            Body=csv_string,<br/>            ContentType='text/csv'<br/>        )<br/>    except Exception as e:<br/>        raise Exception(f"Failed to upload CSV to S3: {str(e)}")</span></pre><h2 id="900b" class="oh oi fq bf oj ok ol om on oo op oq or nm os ot ou nq ov ow ox nu oy oz pa pb bk">Glue Job</h2><p id="57e9" class="pw-post-body-paragraph nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny fj bk">An S3 event on the /unprocessed prefix kicks off a second lambda that starts an AWS Glue job. This is useful when there’s multiple entry points that Jira tickets can enter the system through. For example, if you want to do a backfill.</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="ef00" class="qi oi fq qf b bg qj qk l ql qm">import boto3 <br/><br/># Initialize Boto3 Glue client<br/>glue_client = boto3.client('glue')<br/><br/>def handler(event, context):<br/>    # Print event for debugging<br/>    print(f"Received event: {json.dumps(event)}")<br/><br/>    # Get bucket name and object key (file name) from the S3 event<br/>    try:<br/>        s3_event = event['Records'][0]['s3']<br/>        s3_bucket = s3_event['bucket']['name']<br/>        s3_key = s3_event['object']['key']<br/>    except KeyError as e:<br/>        print(f"Error parsing S3 event: {str(e)}")<br/>        raise<br/><br/>    response = glue_client.start_job_run(<br/>        JobName=glue_job_name,<br/>        Arguments={<br/>            '--S3_BUCKET': s3_bucket,<br/>            '--NEW_CSV_FILE': s3_key<br/>        }<br/>    )</span></pre><p id="a056" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The Glue job itself is written in PySpark and can be found in the code repo <a class="af nc" href="https://github.com/aws-samples/jira-ticket-classification/blob/main/src/glue/etl_script.py" rel="noopener ugc nofollow" target="_blank">here</a>. The important take away is that it does a <a class="af nc" href="https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-join.html#anti-join" rel="noopener ugc nofollow" target="_blank">leftanti</a> join using the issue Ids on the items in the new CSV against all the Ids in the /staged CSVs.</p><p id="1e34" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The results are then pushed to the <strong class="nf fr">/staged</strong> prefix.</p><h2 id="682b" class="oh oi fq bf oj ok ol om on oo op oq or nm os ot ou nq ov ow ox nu oy oz pa pb bk">Classify Jira Tickets:</h2><p id="c649" class="pw-post-body-paragraph nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny fj bk">This is where it it gets interesting. As it turns out, using prompt engineering can perform on par, if not better, than a text classification model using a couple techniques.</p><ul class=""><li id="98b8" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">You can define the classifications and their descriptions in a prompt,</li><li id="1b17" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Ask the model to think step-by-step <a class="af nc" href="https://www.promptingguide.ai/techniques/cot" rel="noopener ugc nofollow" target="_blank">(Chain of Thought)</a>.</li><li id="467d" class="nd ne fq nf b go oc nh ni gr od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">And then output the classification without having to train a single model. See the prompt below:</li></ul><blockquote class="pz qa qb"><p id="aa5c" class="nd ne qc nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Note: </strong>It’s important to validate your prompt using a human curated subset of classified / labelled tickets. You should run this prompt through the validation dataset to make sure it aligns with how you expect the tickets to be classified</p></blockquote><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="47ac" class="qi oi fq qf b bg qj qk l ql qm">SYSTEM_PROMPT = '''<br/>You are a support ticket assistant. You are given fields of a Jira ticket and your task is to classify the ticket based on those fields<br/><br/>Below is the list of potential classifications along with descriptions of those classifications.<br/>&lt;classifications&gt;<br/>ACCESS_PERMISSIONS_REQUEST: Used when someone doesn't have the write permissions or can't log in to something or they can't get the correct IAM credentials to make a service work.<br/>BUG_FIXING: Used when something is failing or a bug is found. Often times the descriptions include logs or technical information.<br/>CREATING_UPDATING_OR_DEPRECATING_DOCUMENTATION: Used when documentation is out of date. Usually references documentation in the text.<br/>MINOR_REQUEST: This is rarely used. Usually a bug fix but it's very minor. If it seems even remotely complicated use BUG_FIXING.<br/>SUPPORT_TROUBLESHOOTING: Used when asking for support for some engineering event. Can also look like an automated ticket.<br/>NEW_FEATURE_WORK: Usually describes a new feature ask or something that isn't operational.<br/>&lt;/classifications&gt;<br/><br/>The fields available and their descriptions are below.<br/>&lt;fields&gt;<br/>Summmary: This is a summary or title of the ticket<br/>Description: The description of the issue in natural language. The majority of context needed to classify the text will come from this field<br/>&lt;/fields&gt;<br/><br/><br/>&lt;rules&gt;<br/>* It is possible that some fields may be empty in which case ignore them when classifying the ticket<br/>* Think through your reasoning before making the classification and place your thought process in &lt;thinking&gt;&lt;/thinking&gt; tags. This is your space to think and reason about the ticket classificaiton.<br/>* Once you have finished thinking, classify the ticket using ONLY the classifications listed above and place it in &lt;answer&gt;&lt;/answer&gt; tags.<br/>&lt;/rules&gt;'''<br/><br/>USER_PROMPT = '''<br/>Using only the ticket fields below:<br/><br/>&lt;summary_field&gt;<br/>{summary}<br/>&lt;/summary_field&gt;<br/><br/>&lt;description_field&gt;<br/>{description}<br/>&lt;/description_field&gt;<br/><br/>Classify the ticket using ONLY 1 of the classifications listed in the system prompt. Remember to think step-by-step before classifying the ticket and place your thoughts in &lt;thinking&gt;&lt;/thinking&gt; tags.<br/>When you are finished thinking, classify the ticket and place your answer in &lt;answer&gt;&lt;/answer&gt; tags. ONLY place the classifaction in the answer tags. Nothing else.<br/>'''</span></pre><p id="03f9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We’ve added a helper class that threads the calls to Bedrock to speed things up:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="a680" class="qi oi fq qf b bg qj qk l ql qm">import boto3<br/>from concurrent.futures import ThreadPoolExecutor, as_completed<br/>import re<br/>from typing import List, Dict<br/>from prompts import USER_PROMPT, SYSTEM_PROMPT<br/><br/>class TicketClassifier:<br/>    SONNET_ID = "anthropic.claude-3-sonnet-20240229-v1:0"<br/>    HAIKU_ID = "anthropic.claude-3-haiku-20240307-v1:0"<br/>    HYPER_PARAMS = {"temperature": 0.35, "topP": .3}<br/>    REASONING_PATTERN = r'&lt;thinking&gt;(.*?)&lt;/thinking&gt;'<br/>    CORRECTNESS_PATTERN = r'&lt;answer&gt;(.*?)&lt;/answer&gt;'<br/><br/>    def __init__(self):<br/>        self.bedrock = boto3.client('bedrock-runtime')<br/><br/>    def classify_tickets(self, tickets: List[Dict[str, str]]) -&gt; List[Dict[str, str]]:<br/>        prompts = [self._create_chat_payload(t) for t in tickets]<br/>        responses = self._call_threaded(prompts, self._call_bedrock)<br/>        formatted_responses = [self._format_results(r) for r in responses]<br/>        return [{**d1, **d2} for d1, d2 in zip(tickets, formatted_responses)]<br/><br/>    def _call_bedrock(self, message_list: list[dict]) -&gt; str:<br/>        response = self.bedrock.converse(<br/>            modelId=self.HAIKU_ID,<br/>            messages=message_list,<br/>            inferenceConfig=self.HYPER_PARAMS,<br/>            system=[{"text": SYSTEM_PROMPT}]<br/>        )<br/>        return response['output']['message']['content'][0]['text']<br/><br/>    def _call_threaded(self, requests, function):<br/>        future_to_position = {}<br/>        with ThreadPoolExecutor(max_workers=5) as executor:<br/>            for i, request in enumerate(requests):<br/>                future = executor.submit(function, request)<br/>                future_to_position[future] = i<br/>            responses = [None] * len(requests)<br/>            for future in as_completed(future_to_position):<br/>                position = future_to_position[future]<br/>                try:<br/>                    response = future.result()<br/>                    responses[position] = response<br/>                except Exception as exc:<br/>                    print(f"Request at position {position} generated an exception: {exc}")<br/>                    responses[position] = None<br/>        return responses<br/><br/>    def _create_chat_payload(self, ticket: dict) -&gt; dict:<br/>        user_prompt = USER_PROMPT.format(summary=ticket['Summary'], description=ticket['Description'])<br/>        user_msg = {"role": "user", "content": [{"text": user_prompt}]}<br/>        return [user_msg]<br/><br/>    def _format_results(self, model_response: str) -&gt; dict:<br/>        reasoning = self._extract_with_regex(model_response, self.REASONING_PATTERN)<br/>        correctness = self._extract_with_regex(model_response, self.CORRECTNESS_PATTERN)<br/>        return {'Model Answer': correctness, 'Reasoning': reasoning}<br/><br/>    @staticmethod<br/>    def _extract_with_regex(response, regex):<br/>        matches = re.search(regex, response, re.DOTALL)<br/>        return matches.group(1).strip() if matches else None</span></pre><p id="d6c0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Lastly, the classified tickets are converted to a CSV and uploaded to S3</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="840d" class="qi oi fq qf b bg qj qk l ql qm">import boto3<br/>import io<br/>import csv<br/><br/>s3 = boto3.client('s3')<br/><br/>def upload_csv(data: List[Dict[str, str]]) -&gt; None:<br/>      csv_buffer = io.StringIO()<br/>      writer = csv.DictWriter(csv_buffer, fieldnames=data[0].keys())<br/>      writer.writeheader()<br/>      writer.writerows(data)<br/><br/>      current_time = datetime.now().strftime("%Y%m%d_%H%M%S")<br/>      filename = f"processed/processed_{current_time}.csv"<br/><br/>      s3.put_object(<br/>          Bucket=self.bucket_name,<br/>          Key=filename,<br/>          Body=csv_buffer.getvalue()<br/>      )</span></pre><h1 id="9ca7" class="ph oi fq bf oj pi pj gq on pk pl gt or pm pn po pp pq pr ps pt pu pv pw px py bk">Dashboarding</h1><p id="b6ac" class="pw-post-body-paragraph nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny fj bk">The project is dashboard agnostic. Any popular tool/service will work as long as it can consume a CSV. Amazon Quicksight, Tableu or anything in between will do.</p><h1 id="daa7" class="ph oi fq bf oj pi pj gq on pk pl gt or pm pn po pp pq pr ps pt pu pv pw px py bk">Conclusion</h1><p id="689c" class="pw-post-body-paragraph nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny fj bk">In this blog we discussed using Bedrock to automatically classify Jira tickets. These enriched tickets can then be used to create dashboards using various AWS Services or 3P tools. The takeaway, is that classifying text has become much simpler since the adoption of LLMs and what would have taken weeks can now be done in days.</p><p id="9136" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="qc">If you enjoyed this article feel free to connect with me on </em><a class="af nc" href="https://www.linkedin.com/in/tanner-mcrae-aa728358/" rel="noopener ugc nofollow" target="_blank"><em class="qc">LinkedIn</em></a></p></div></div></div></div>    
</body>
</html>