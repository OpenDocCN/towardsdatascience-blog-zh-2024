<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Uncertainty in Markov Decisions Processes: a Robust Linear Programming approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Uncertainty in Markov Decisions Processes: a Robust Linear Programming approach</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/uncertainty-in-markov-decisions-processes-a-robust-linear-programming-approach-b01e6e26e463?source=collection_archive---------2-----------------------#2024-09-18">https://towardsdatascience.com/uncertainty-in-markov-decisions-processes-a-robust-linear-programming-approach-b01e6e26e463?source=collection_archive---------2-----------------------#2024-09-18</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="2c74" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Theoretical derivation of the Robust Counterpart of Markov Decision Processes (MDPs) as a Linear Program (LP)</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@h.fellahi?source=post_page---byline--b01e6e26e463--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Hussein Fellahi" class="l ep by dd de cx" src="../Images/b49c8620d8a490ab078b5d4dfe8d017a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*AN6dPCSfMNT8te_4aWw0cg.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--b01e6e26e463--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@h.fellahi?source=post_page---byline--b01e6e26e463--------------------------------" rel="noopener follow">Hussein Fellahi</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--b01e6e26e463--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 18, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/5ca68bd8a46abe36f974ccfa485f67ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8hTPzD7cugj9O_oA"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@mrnuclear?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">ZHENYU LUO</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="3731" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Introduction</h1><p id="35a0" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Markov Decision Processes are foundational to sequential decision-making problems and serve as the building block for reinforcement learning. They model the dynamic interaction between an agent having to make a series of actions and their environment. Due to their wide applicability in fields such as robotics, finance, operations research and AI, MDPs have been extensively studied in both theoretical and practical contexts.</p><p id="83a3" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Yet, much of the existing MDP literature focuses on the idealized scenarios where model parameters — such as transition probabilities and reward functions — are assumed to be known with certainty. In practice, applying popular methods such as Policy Iteration and Value Iteration require precise estimates of these parameters, often obtained from real-world data. This reliance on data introduces significant challenges: the estimation process is inherently noisy and sensitive to limitations such as data scarcity, measurement errors and variability in the observed environment. Consequently, the performance of standard MDP methods can degrade substantially when applied to problems with uncertain or incomplete data.</p><p id="2ce5" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In this article, we build on the Robust Optimization (RO) literature to propose a generic framework to address these issues. <strong class="oa fr">We provide a Robust Linear Programming (RLP) formulation of MDPs that is capable of handling various sources of uncertainty and adversarial perturbations.</strong></p></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9d73" class="nc nd fq bf ne nf ph gq nh ni pi gt nk nl pj nn no np pk nr ns nt pl nv nw nx bk">MDP definition and LP formulation</h1><p id="1937" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Let’s start by giving a formal definition of MDPs:</p><p id="2ebc" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="pm">A </em><strong class="oa fr"><em class="pm">Markov Decision Process</em></strong><em class="pm"> is a 5-tuple (S, A, R, P, γ) such that:</em></p><ul class=""><li id="e898" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk"><em class="pm">S is the set of </em><strong class="oa fr"><em class="pm">states</em></strong><em class="pm"> the agent can be in</em></li><li id="40d6" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk"><em class="pm">A is the set of </em><strong class="oa fr"><em class="pm">actions</em></strong><em class="pm"> the agent can take</em></li><li id="836e" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk"><em class="pm">R : S </em>x<em class="pm"> A → </em>R<em class="pm"> the </em><strong class="oa fr"><em class="pm">reward</em></strong><em class="pm"> function</em></li><li id="d0a6" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk"><em class="pm">P is the set of </em><strong class="oa fr"><em class="pm">probability distributions</em></strong><em class="pm"> defined such that P(s’|s,a) is the probability of transitioning to state </em><strong class="oa fr"><em class="pm">s’</em></strong><em class="pm"> if the agent takes action </em><strong class="oa fr"><em class="pm">a</em></strong><em class="pm"> in state </em><strong class="oa fr"><em class="pm">s</em></strong><em class="pm">. Note that MDPs are Markov processes, meaning that the Markov property holds on the transition probabilities</em>: P(Sₜ₊₁|S₀, A₀, …, Sₜ, Aₜ) = P(Sₜ₊₁|Sₜ, Aₜ)</li><li id="ff01" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk"><em class="pm">γ ∈ (0, 1] is a </em><strong class="oa fr"><em class="pm">discount factor</em></strong><em class="pm">. While we usually deal with discounted problems (i.e. γ &lt; 1), the formulations presented are also valid for undiscounted MDPs (γ = 1)</em></li></ul><p id="50a9" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We then define the <strong class="oa fr">policy</strong>, i.e. what dictates the agent’s behavior in an MDP:</p><p id="0358" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="pm">A policy π is a </em><strong class="oa fr"><em class="pm">probability measure</em></strong><em class="pm"> over the action space defined as: π(a|s) is the probability of taking action </em><strong class="oa fr"><em class="pm">a</em></strong><em class="pm"> when the agent is in state </em><strong class="oa fr"><em class="pm">s</em></strong><em class="pm">.</em></p><p id="eb54" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We finally introduce the <strong class="oa fr">value function</strong>, i.e. the agent’s objective in an MDP:</p><p id="e683" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="pm">The value function of a policy π is the expected discounted reward under this policy, when starting at a given state </em><strong class="oa fr"><em class="pm">s</em></strong><em class="pm">:</em></p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pv"><img src="../Images/6d82dc5a1a2e1a0b0d5228793fd75781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xd7bPyYhoIxWvu4_MBxekw.png"/></div></div></figure><p id="4391" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="pm">In particular, the value function of the optimal policy π* satisfies the Bellman optimality equation:</em></p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pw"><img src="../Images/7daa90e2dd4bf9c84f0d01a1d142ddc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8iHrp9-FvMoxcKYCTATzfw.png"/></div></div></figure><p id="c38d" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="pm">Which yields the deterministic optimal policy:</em></p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj px"><img src="../Images/25de5914d7227557fccc869698b8a51c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dQDSIbQN8O6jB_HZlaxhkg.png"/></div></div></figure><h2 id="ac9b" class="py nd fq bf ne pz qa qb nh qc qd qe nk oh qf qg qh ol qi qj qk op ql qm qn qo bk">Deriving the LP formulation of MDPs:</h2><p id="64d6" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Given the above definitions, we can start by noticing that any value function V that satisfies</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qp"><img src="../Images/853b1072047df6981994647e5fdb183f.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*MVpfAFxMARLCwRPR-eJJWw.png"/></div></figure><p id="4082" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">is an upper bound on the optimal value function. To see it, we can start by noticing that such value function also satisfies:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qq"><img src="../Images/ead9064be99992fb736efcd2fb7162c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*8lNVKqPFPo-ubHKURMcDGw.png"/></div></figure><p id="25a8" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We recognize the value iteration operator applied to V:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qr"><img src="../Images/addbe4760a5cbd1d16e641580e000de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*b9y1N4-vfBq2OGOLdPAUTQ.png"/></div></figure><p id="994b" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">i.e.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qs"><img src="../Images/529b5be7c352fdaac409965f0a558859.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*j_t-LR21-aOAVVlSra2mcg.png"/></div></figure><p id="a19b" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Also noticing that the H*operator is increasing, we can apply it iteratively to have:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qt"><img src="../Images/1eb2f85b3fef8f0a3946982532d2528c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g6P_zvoskUen2HSaKHR2gw.png"/></div></div></figure><p id="00c1" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">where we used the property of V* being the fixed point of H*.</p><p id="8ca7" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Therefore, finding V* comes down to finding the <strong class="oa fr">tightest upper bound V that obeys the above equation</strong>, which yields the following formulation:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qq"><img src="../Images/dd511b37256a48510693d9294a7427cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*FfEVKXvuSkyl_DHHaPIXIA.png"/></div></figure><p id="848d" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Here we added a weight term corresponding to the probability of starting in state <em class="pm">s</em>. We can see that the above problem is linear in V and can be rewritten as follows:</p></div></div><div class="mq"><div class="ab cb"><div class="ll qu lm qv ln qw cf qx cg qy ci bh"><figure class="ml mm mn mo mp mq ra rb paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qz"><img src="../Images/a95c4fe11f870f446a9cf2a04b8d975e.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*5El-i0UeBSXXAkRqgTFMiQ.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8848" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Further details can be found in [1] and [2].</p></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0928" class="nc nd fq bf ne nf ph gq nh ni pi gt nk nl pj nn no np pk nr ns nt pl nv nw nx bk">Robust Optimization for Linear Programming</h1><p id="a6b5" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Given the above linear program in standard form, the RO framework assumes an adversarial noise in the inputs (i.e. cost vector and constraints). To model this uncertainty, we define an <strong class="oa fr">uncertainty set</strong>:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rc"><img src="../Images/8632027096c26bd218ef09dce04263fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*VqDys6V2Y2vFodiPq-Ku3A.png"/></div></figure><p id="831f" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In short, we want to find the minimum of all linear programs, i.e. for each occurrence in the uncertainty set. Naturally this yields a completely intractable model (potentially an infinite number of LPs) since we did not make any assumption on the form of <em class="pm">U</em>.</p><p id="65c4" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Before addressing these issues, we make the following assumptions — without loss of generality:</p><ul class=""><li id="891b" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk">Uncertainty in <em class="pm">w </em>and <em class="pm">b </em>is equivalent to uncertainty in the constraints for a slightly modified LP — for this reason we consider uncertainty only in <em class="pm">c</em></li><li id="6f07" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">Adversarial noise is applied constraint-wise, i.e. to each constraint individually</li><li id="6385" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">The constraint of the robust problem are in the form:</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rd"><img src="../Images/c3789208dc20154982543af2d706aa66.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*ZFq-JegDfJoLgLOBhUt-UQ.png"/></div></figure><p id="e4e8" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">where: \bar{c} is known as the <em class="pm">nominal constraint vector</em> (e.g. gotten from some estimation), <em class="pm">z </em>the uncertain factor and Q a fixed matrix intuitively corresponding to how the noise is applied to each coefficient of the constraint vector. <em class="pm">Q</em> can be used for instance to model correlation between the noise on difference components of <em class="pm">c</em>. See [3] for more details and proofs.</p><p id="d02e" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Note: we made a slight abuse of notation and dropped the <em class="pm">(s, a)</em> subscripts for readability — yet<em class="pm"> c, \bar{c}, Q</em> and <em class="pm">z</em> are all for a given state and action couple.</p><ul class=""><li id="ed7f" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk">Rather than optimizing for each entry of the uncertainty set, <strong class="oa fr">we optimize the worst case over <em class="pm">U</em>. </strong>In the context of uncertainty on the constraints only, this mean <strong class="oa fr">that the worst case over <em class="pm">U</em> must also be feasible</strong></li></ul><p id="93ea" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">All this leads to the following formulation of the problem:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj re"><img src="../Images/a3909cb7130218128c2279000361bd40.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*E0oA9wIcmIQMIyEoIRDkqw.png"/></div></figure><p id="64b4" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">At this stage, we can make some assumptions on the form of <em class="pm">U</em> in order to further simplify the problem:</p><ul class=""><li id="ea4a" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk">While <em class="pm">z</em> can be a vector of arbitrary dimension <em class="pm">L</em> — as <em class="pm">Q</em> will be a <em class="pm">|S|</em> x <em class="pm">L</em> matrix — we make the simplifying assumption that <em class="pm">z</em> is of size <em class="pm">|S| </em>and<em class="pm"> Q</em> is a square diagonal matrix of size <em class="pm">|S|</em> as well. This will allow to model separately an adversarial noise on each coefficient on the constraint vector (and no correlation between noises)</li><li id="e23b" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">We assume that the uncertainty set is a box of size <em class="pm">2d, </em>i.e. that each coordinate of <em class="pm">z</em> can take any value from the interval <em class="pm">[-d, d]</em>. This is equivalent to saying that the <em class="pm">L∞</em> norm of <em class="pm">z</em> is less than <em class="pm">d</em></li></ul><p id="e9e4" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The optimization problem becomes:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rf"><img src="../Images/419cb06e77e1678dee33007b6e287c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*NhlWhV2DhXSVjFB3Cw8UCw.png"/></div></figure><p id="7f23" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">which is equivalent to:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rg"><img src="../Images/92e0dc89bd906107cf3daa4c57035b35.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*Zb56DPws92jSho2IM_vZuQ.png"/></div></figure><p id="f69a" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Finally, looking closer to the maximization problem in the constraint, we see that it has a closed form. Therefore the final problem can be written as (<strong class="oa fr">robust counterpart of a linear program with box uncertainty</strong>):</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rh"><img src="../Images/1a024026d6db5e420aae02229ede1a5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*Rd5hTKljcW0vmFEaUh37Jg.png"/></div></figure><p id="3ff1" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">A few comments on the above formulation:</p><ul class=""><li id="6ef6" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk">The uncertainty term disappeared — robustness is brought by an additional safety term</li><li id="0360" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">As the L1 norm can be linearized, this is a linear program</li><li id="3339" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">The above formulation does not depend on the form of <em class="pm">Q </em>— the assumption made will be useful in the next section</li></ul><p id="1b37" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">For more details, the interested reader can refer to [3].</p></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="c84c" class="nc nd fq bf ne nf ph gq nh ni pi gt nk nl pj nn no np pk nr ns nt pl nv nw nx bk">The RLP formulation of MDPs</h1><p id="1237" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Starting from the above formulation:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj ri"><img src="../Images/93c5fcdf66e345016b93c434216c00a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RXBzkHXAvjbGfhDapzJdug.png"/></div></figure><p id="8699" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">And finally, linearizing the absolute value in the constraints gives:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rj"><img src="../Images/0f21ead78a1601595332037d985dfdce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*X6m-XSpL2Psa3jXvcA4Srw.png"/></div></figure><p id="2b53" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We notice that robustness translates into an additional <strong class="oa fr">safety term</strong> in the constraints — given the uncertainty on <em class="pm">c</em> (which mainly translates into uncertainty in the MDP’s transition probabilities).</p><p id="cfbb" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">As stated previously, considering uncertainty on the rewards as well can be easily done with a similar derivation. Coming back to the linear program in standard form, we add another noise term on the right hand side of the constraints:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rk"><img src="../Images/1170d90147dfdaf5d285ec368f6caad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*vIshlcZiqVi9EqiraaGbxQ.png"/></div></figure><p id="976e" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">After a similar reasoning as previously done, we have the all-in linear program:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rl"><img src="../Images/304f023eec33f211753050b9b290e5dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*wmDqYpIvtMMtoRQ3MKmVww.png"/></div></figure><p id="acdf" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Again similarly to before, additional robustness with regards to the reward function translates into another safety term in the constraints, which ultimately can yield a less optimal value function and policy but fills the constraints with margin. This tradeoff is controlled both by <em class="pm">Q</em> and the size of the uncertainty box <em class="pm">d</em>.</p></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="97cf" class="nc nd fq bf ne nf ph gq nh ni pi gt nk nl pj nn no np pk nr ns nt pl nv nw nx bk">Conclusion</h1><p id="3b2c" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">While this completes the derivation of the Robust MDP as a linear program, other robust MDP approaches have been developed in the literature, see for instance [4]. These approaches usually take a different route, for instance directly deriving a robust Policy Evaluation operator — which for instance has the advantage of having a better complexity as the LP approach. This proves particularly important when state and action spaces are large. Therefore why would we use such formulation?</p><ul class=""><li id="658e" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk">The RLP formulation allows to benefit from all theoretical properties of linear programming. This entails guarantees of a solution (when the problem is feasible and bounded), as well as known results from duality theory and sensitivity analysis</li><li id="d6ab" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">The LP approach allows to easily use different geometries of uncertainty sets — see [3] for details</li><li id="9c95" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">This formulation allows to integrate naturally additional constraints on the MDP, while keeping the robustness properties</li><li id="b521" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">We can furthermore apply some projection or approximation methods (see for instance [5]) to improve the LP complexity</li></ul></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="3f35" class="py nd fq bf ne pz qa qb nh qc qd qe nk oh qf qg qh ol qi qj qk op ql qm qn qo bk">References:</h2><p id="9d0b" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">[1] M.L. Puterman, <a class="af nb" href="https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887" rel="noopener ugc nofollow" target="_blank">Markov Decision Processes: Discrete Stochastic Dynamic Programming</a> (1996), Wiley</p><p id="94e8" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[2] P. Pouppart, <!-- -->Sequential Decision Making and Reinforcement Learning<!-- --> (2013), University of Waterloo</p><p id="e3bc" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[3] D. Bertsimas, D. Den Hertog, <a class="af nb" href="https://www.dynamic-ideas.com/books/robust-and-adaptive-optimization" rel="noopener ugc nofollow" target="_blank">Robust and Adaptive Optimization</a> (2022), Dynamic Ideas</p><p id="5f34" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[4] W. Wiesemann, D. Kuhn, B. Rustem, <a class="af nb" href="https://pubsonline.informs.org/doi/abs/10.1287/moor.1120.0566?journalCode=moor" rel="noopener ugc nofollow" target="_blank">Robust Markov Decision Processes</a> (2013), INFORMS</p><p id="5cab" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[5] K. Vu, P.-L. Poirion, L. Liberti, <a class="af nb" href="https://pubsonline.informs.org/doi/abs/10.1287/moor.2017.0894" rel="noopener ugc nofollow" target="_blank">Random Projections for Linear Programming</a> (2018), INFORMS</p></div></div></div></div>    
</body>
</html>