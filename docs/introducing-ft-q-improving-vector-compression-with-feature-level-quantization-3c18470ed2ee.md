# 介绍ft-Q：通过特征级量化提升向量压缩

> 原文：[https://towardsdatascience.com/introducing-ft-q-improving-vector-compression-with-feature-level-quantization-3c18470ed2ee?source=collection_archive---------6-----------------------#2024-11-26](https://towardsdatascience.com/introducing-ft-q-improving-vector-compression-with-feature-level-quantization-3c18470ed2ee?source=collection_archive---------6-----------------------#2024-11-26)

## 量化

## 通过特征级量化（ft-Q）推动量化的极限

[](https://medium.com/@ardito.bryan?source=post_page---byline--3c18470ed2ee--------------------------------)[![Michelangiolo Mazzeschi](../Images/9211748ac638d2ed07679ac73ea17296.png)](https://medium.com/@ardito.bryan?source=post_page---byline--3c18470ed2ee--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3c18470ed2ee--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3c18470ed2ee--------------------------------) [Michelangiolo Mazzeschi](https://medium.com/@ardito.bryan?source=post_page---byline--3c18470ed2ee--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3c18470ed2ee--------------------------------) ·阅读时间：10分钟·2024年11月26日

--

****要理解本文，需具备* ***嵌入*** *和* ***基本量化*** *的知识。该算法的实现已在* [*GitHub*](https://github.com/atlantis-nova/ft-Q) *发布，并且完全开源。*

***更新 24–20–12:*** *我注意到一种类似的方法已经存在，并且可以在句子转换器库中通过* [*标定嵌入*](https://sbert.net/docs/package_reference/sentence_transformer/quantization.html) *（请引用正确的作者）。然而，当应用于二值量化时，依然是错误的，因为应该使用* ***中位数*** *来将分布分为两个完全相等的部分*

自从大规模语言模型（LLM）问世以来，量化已成为生产环境中节省内存的最流行技术之一。很快，它也在向量数据库中得到普及，向量数据库开始使用相同的技术压缩不仅是模型，还有用于检索目的的向量。

在本文中，我将展示当前量化算法的局限性，并提出一种**新的量化方法（ft-Q）**来解决这些问题。

# 什么是量化，它是如何工作的？

量化是一种**节省内存的算法**，允许我们使用更少的位数存储数字（无论是内存中还是磁盘上）。默认情况下，当我们在内存中存储任何数字时，我们使用float32：这意味着该数字是通过32位（二进制元素）组合存储的。

例如，整数40在32位对象中的存储方式如下：

![](../Images/3de0fab2d22fc79957fb8000d8d713e9.png)

将数字40存储在32位对象中，**图像来源：作者**

然而，我们可以决定使用更少的位来存储相同的数字（将内存使用量减少一半），例如使用16位对象：

![](../Images/bd0c31063855a84aa94e254701938943.png)

将数字40存储在16位对象中，**图像来源：作者**

通过量化，我们指的是使用较少的位数存储数据（例如，32 -> 16，或32 -> 4），这也称为转换。如果我们存储1GB的数字（默认存储为32位对象），如果我们决定使用16位对象来存储它们（从而应用量化），我们的数据大小将减半，结果是0.5GB。

## 量化是否存在某些陷阱？

节省如此多的存储空间看起来令人难以置信（正如你所理解的，我们可以不断减少，直到达到最小的位数：1位，也就是二进制量化。我们的数据库大小将减少32倍，**从1GB减少到31.25MB！**），但是，正如你可能已经理解的那样，这其中有一个陷阱。

任何数字都可以存储在所有可能组合的位数允许的范围内。使用32位量化时，你可以存储最多2³²个数字。有这么多可能的组合，以至于我们决定在使用32位时包括小数。例如，如果我们在初始数字中添加一个小数并将40.12存储在32位中，它将使用这种1和0的组合：

```py
01000010 00100000 01111010 11100001
```

我们已经理解，在32位存储下（由于它的巨大组合可能性），我们几乎可以编码每个数字，包括其小数点（为了澄清，如果你对量化不熟悉，实数和小数点是没有分开的，40.12作为一个整体会被转换成32个二进制数的组合）。

如果我们不断减少位数，所有可能的组合会呈指数级减少。例如，4位存储的组合数限制为2⁴：我们只能存储16个数字（这没有太多空间来存储小数）。在1位存储中，我们只能存储一个数字，要么是1，要么是0。

为了让这一点更具实际意义，将我们的初始32位数字存储为二进制代码将迫使我们将所有数字（例如40.12）转换为0或1。在这种情况下，这种压缩方式看起来并不理想。

# 如何最大化量化的效果

我们已经看到了量化如何导致信息丢失。那么，究竟如何利用它呢？当你看一个单一数字的量化（例如将40.12转换为1），似乎从如此极端的量化中没有任何价值可言，因为损失实在是太大了。

然而，当我们将这一技术应用于一组数据（例如向量）时，信息丢失并不像应用于单一数字时那么剧烈。向量搜索是一个非常适合以有用方式应用量化的例子。

当我们使用编码器，例如**all-MiniLM-L6-v2**时，我们将每个样本（原本是文本形式）存储为一个向量：一个由384个数字组成的序列。正如你可能已经理解的那样，存储数百万个相似的序列是非常昂贵的，我们可以通过量化大幅减少原始向量的大小。

也许，将我们的向量从32位量化到16位并不会带来太大的损失。但如果是4位甚至二进制量化呢？由于我们的数据集相对较大（每个有384个数字），这种复杂性使我们能够在不造成过多检索损失的情况下达到更高的压缩水平。

**4位量化**

我们执行量化的方式是观察我们展平后的向量的数据分布，并选择映射到一个具有更少位数的等效区间。我最喜欢的例子是4位量化。在这种复杂度下，我们可以存储2⁴ = 16个数字。但是，正如所解释的那样，我们向量中的所有数字都是复杂的，每个数字都有多个小数点：

```py
array([ 2.43655406e-02, -4.33481708e-02, -1.89688837e-03, -3.76498550e-02,
       -8.96364748e-02,  2.96154656e-02, -5.79943173e-02,  1.87652372e-02,
        1.87771711e-02,  6.30387887e-02, -3.23972516e-02, -1.46128759e-02,
       -3.39277312e-02, -7.04369228e-03,  3.87261175e-02, -5.02494797e-02,
        ...
       -1.03239892e-02,  1.83096472e-02, -1.86534156e-03,  1.44851031e-02,
       -6.21072948e-02, -4.46912572e-02, -1.57684386e-02,  8.28376040e-02,
       -4.58770394e-02,  1.04658678e-01,  5.53084277e-02, -2.51113791e-02,
        4.72703762e-02, -2.41811387e-03, -9.09169838e-02,  1.15215247e-02],
      dtype=float32)
```

我们可以做的是将分布中的每个数字映射到一个区间，该区间的范围是[-8, 7]（16个可能的数字）。为了定义区间的极端值，我们可以使用我们正在量化的分布的最小值和最大值。

![](../Images/20c08512fbff9dde9fecc9fdfd93b941.png)

4位量化：灰色区域是整数区间[-8, 7]，不要将其与位数混淆。该区间中的任何数字将会被转换成一个4位的对象，**图片来源：作者**

例如，分布的最小值/最大值为[-0.2, 0.2]。这意味着-0.2将被转换为-8，0.2将被转换为7。分布中的每个数字都会有一个量化后的等效值（例如，示例数组中的第一个数字（0.02436554）将被量化为0，如上图所示）。

```py
array([[-1, -3, -1, ...,  1, -2, -2],
       [-6, -1, -2, ..., -2, -2, -3],
       [ 0, -2, -4, ..., -1,  1, -2],
       ...,
       [ 3,  0, -5, ..., -5,  7,  0],
       [-4, -5,  3, ..., -2, -2, -2],
       [-1,  0, -2, ..., -1,  1, -3]], dtype=int4)
```

**1位量化**

相同的原理也适用于二进制量化，但它要简单得多。规则如下：分布中小于0的每个数字变为0，大于0的每个数字变为1。

![](../Images/47ee5e68eaecf27187bc19d0f422e850.png)

1位量化，**图片来源：作者**

# 不是所有的嵌入（embeddings）都是一样构建的

当前量化技术的主要问题是，它们假设我们所有的值**基于单一的分布**。因此，当我们使用阈值来定义区间（例如最小值和最大值）时，我们只使用从数据总集派生出来的单一集合（这个集合是基于单一分布建模的）。

![](../Images/a6b19bc9727ad03a669bc57815c9d314.png)

展平编码数据集的所有单个样本的分布，**图片来源：作者**

在一项实验中，我将41,963个游戏描述编码成了向量**。**通过观察每个特征的数据分布，我们可以看到尽管作出了努力，仍然没有任何一个特征是完全标准化的：其均值可能偏离目标值0。

![](../Images/c11a41a92ae510cee2c12f27f3c166fe.png)

编码数据集中20个随机特征的分布，**图像来自作者**

简而言之，每个特征都可以用**专用分布**来建模。因为数据**不遵循**单一的巨大分布，我们可以通过在特征层应用量化，利用这种组织方式。此外，嵌入通常会使用相似的值来编码每个特征（否则均值将始终为0），这意味着编码额外数据时**漂移的可能性最小**。

为了更好地解释数学内容，先定义两组值：

**S** = 编码数据集中的所有独立样本（41936 * 384）

**Fₙ** = 编码数据集中属于单一特征的所有独立样本（41936 * 1）

# 特征29：丑小鸭

在我们的样本数据集中，每个向量包含384个特征。然而，通过逐一查看数据，我们可以注意到某些特征并未完美标准化，而是有显著偏斜。以**F**₂₉为例：下图展示了**F**₂₉（41936）在整个编码数据集中的分布。

![](../Images/ab7ea6349f4adddc04c942c09c331cfc.png)

**F**₂₉分布，**图像来自作者**

从图中可以看到，它的分布均值约为-0.07，边缘值为（-0.2, 0.1）。我确信，基于对编码器行为的了解，无论我们输入多少额外数据，**F**₂₉始终会保持一个“丑小鸭”，其分布不会改变。该分布只包含少量正值。

## 常规量化

现在，让我们对**书本**应用**二进制量化**，但仅对**F**₂₉进行。之所以选择二进制方法，是因为大部分信息会丢失，意味着可以通过不同的方法进行改进。

要以二进制方式量化值，我们需要选择一个值作为阈值，将值转换为0或1。最简单的方法是选择0（约为**S**的分布均值）。在处理**F**₂₉的值时，由于大多数值为负数，大多数值会被量化为0，只有少数会被量化为1。

![](../Images/c06df47c158bea08f795a089f742a2e3.png)

应该量化为1但量化为0的样本：**F**₂₉的44%，**图像来自作者**

让我们进一步探索数据：94%的**F**₂₉已转换为0，而在一个完美标准化的分布中，我们的目标是50%。这意味着**F**₂₉的44%（密度分布的红色区域）没有被正确量化。

```py
# we count the number of 0 over the total number of values
>>> 1-quantized_regular[:, 29].sum()/sample_vectors[:, 29].size
0.9424122472338802 
```

## ft-量化

如果我们不使用0作为阈值（从**S**中提取），而是将**F**₂₉分布作为基准呢？再次查看**F**₂₉分布，除了0，我们将使用其均值~ -0.07，并将其极值作为区间的最小/最大值~ [-0.20, 0.10]（请参见下方图像：对图像不准确表示歉意，极值还是有的）。简单来说，ft-Q将**参考量化区间**的位置进行调整，以更好地适应数据的真实分布。

![](../Images/9972f8207b07902d3986059f72446860.png)

ft-Q的可视化，区间已调整以适应特征分布，**图片来源：作者**

> ***通过以下文章，我试图介绍一种新算法，据我所知，至今没有在其他地方找到类似的算法。请注意，该算法不同于用于训练神经网络的FQ（特征量化），这是一种在训练后使用的算法。***
> 
> **我欢迎批评**并**期待任何反馈**。

在对**F**₂₉应用二进制量化后，由于阈值已更新，我们可以看到数据有一半会被量化为0，另一半为1，从而更真实地表示数据。通过比较量化结果，ft-Q将**F**₂₉的47%转换为0，只有3%的值未被正确量化。

```py
# we count the number of 0 over the total number of values
>>> 1-quantized_tfQ[:, 29].sum()/sample_vectors[:, 29].size
0.46809423884013734
```

总结来说，**ft-Q**（或ft-量化）将每个特征单独编码，从而最小化由于非标准化分布所可能产生的错误。

# 何时使用ft-量化

从实际情况来看，没有任何嵌入是完全标准化的，特征分布中会存在一些变异（尽管是微小的）。然而，现在我们已经识别出错误的位置，我们可以使用以下方法进行调整。

**ft-Q**。

## ft-Q可以应用于常规嵌入吗？

当ft-Q应用于常规嵌入时，我们不会看到显著的增强效果。

```py
>>> err_regular = .5-quantized_regular.sum()/sample_vectors.size
>>> err_ftQ = .5-quantized_tfQ.sum()/sample_vectors.size
>>> err_total = abs(err_regular)-abs(err_ftQ)
>>> err_total
0.012901293538566672
```

在**all-MiniLM-L6-v2**的情况下，我们取得了**1.2%的提升**（虽然不显著，但仍然是一次升级）。

## ft-Q的亮点：处理过的嵌入

然而，嵌入并不总是以标准化形式使用。有时，在某些应用场景中，需要对嵌入进行处理（例如，在[covariate encoding](https://github.com/atlantis-nova/simtag)的情况下）。我们可以使用以下理论图表来理解**在哪些情况下**ft-Q能够更好地发挥作用：

![](../Images/0f1ce97a5812a6558b64afc4c9167d7f.png)

特征的偏斜程度越大，ft-Q的效果越明显，**图片来源：作者**

经过额外处理步骤得到的向量不一定是标准化的：我们可以**重新标准化**它们，然后再应用**量化**，但通过使用ft-Q作为单一操作（即使在标准化不完美的情况下仍有小幅提升），我们可以一举两得。

# 结论

总之，本文试图提出一种更细致的量化方法。最初，开发该算法的目的是解决处理后的嵌入向量的性能问题，但经过适当的实验后，它在常规场景中也证明了其有效性。

随着大型语言模型（LLM）和更复杂的向量数据库的普及，内存管理和性能优化在信息检索领域变得越来越重要，因此我们有责任熟悉这些内容，并提出新的、更好的解决方案。

时间将证明，是否会有新的、更智能的数据压缩方法加入其中。至于现在，你可以充分利用该算法。
