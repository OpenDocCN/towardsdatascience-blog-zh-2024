- en: Fine-Tune Tiny Adapters for Llama 3 with VeRA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/fine-tune-tiny-adapters-for-llama-3-with-vera-7c48f4391d84?source=collection_archive---------8-----------------------#2024-06-11](https://towardsdatascience.com/fine-tune-tiny-adapters-for-llama-3-with-vera-7c48f4391d84?source=collection_archive---------8-----------------------#2024-06-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LoRA but 100x smaller
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--7c48f4391d84--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--7c48f4391d84--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7c48f4391d84--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7c48f4391d84--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--7c48f4391d84--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7c48f4391d84--------------------------------)
    ·6 min read·Jun 11, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6fbba9c193cdb65c717fbb23bbe12ae4.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated with DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: LoRA fine-tunes large language models (LLMs) by adding an adapter on top of
    the pre-trained LLM, with only this adapter being trainable while the LLM’s original
    parameters remain frozen. This approach significantly reduces the number of parameters
    that need to be trained, resulting in much smaller optimizer states. Consequently,
    LoRA fine-tuning consumes considerably less memory compared to standard full fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, depending on LoRA’s hyperparameters, such as the rank and the number
    of targeted modules, LoRA may still create very large adapters with hundreds of
    millions of parameters that are too large to be fine-tuned on consumer hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Many alternatives have been proposed to reduce the size of adapters.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I review VeRA, an alternative to LoRA producing adapters 100x
    smaller. I fine-tuned Llama 3 with VeRA for demonstration and compared its performance
    with LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: 'VeRA: Fine-tuning Vectors on top of Random Matrices'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'VeRA is presented in this paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[VeRA: Vector-based Random Matrix Adaptation](https://arxiv.org/abs/2310.11454)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: This is one of my favorite papers presenting an alternative to LoRA.
    It’s very well-written*…'
  prefs: []
  type: TYPE_NORMAL
