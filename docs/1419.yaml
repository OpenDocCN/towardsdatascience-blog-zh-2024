- en: How LLMs Think
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-llms-think-d8754a79017d?source=collection_archive---------1-----------------------#2024-06-07](https://towardsdatascience.com/how-llms-think-d8754a79017d?source=collection_archive---------1-----------------------#2024-06-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Research paper in pills: “Scaling Monosemanticity: Extracting Interpretable
    Features from Claude 3 Sonnet”'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@cristianleo120?source=post_page---byline--d8754a79017d--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--d8754a79017d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d8754a79017d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d8754a79017d--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--d8754a79017d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d8754a79017d--------------------------------)
    ·10 min read·Jun 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7585d40f377e70c9ece698450f690525.png)'
  prefs: []
  type: TYPE_IMG
- en: Image Generated by DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: 'Have you ever wondered how an AI model “thinks”? Imagine peering inside the
    mind of a machine and watching the gears turn. This is exactly what a groundbreaking
    paper from Anthropic explores. Titled “[Scaling Monosemanticity: Extracting Interpretable
    Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)”,
    the research delves into understanding and interpreting the thought processes
    of AI.'
  prefs: []
  type: TYPE_NORMAL
- en: The researchers managed to extract features from the Claude 3 Sonnet model that
    show what it was thinking about famous people, cities, and even security vulnerabilities
    in software. It’s like getting a glimpse into the AI’s mind, revealing the concepts
    it understands and uses to make decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Research Paper Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the paper, the Anthropic team, including Adly Templeton, Tom Conerly, Jonathan
    Marcus, and others, set out to make AI models more transparent. They focused on
    Claude 3 Sonnet, a medium-sized AI model, and aimed to scale monosemanticity —
    essentially making sure that each feature in the model has a clear, single meaning.
  prefs: []
  type: TYPE_NORMAL
- en: But why is scaling monosemanticity so important? And what exactly is monosemanticity?
    We’ll dive into that soon.
  prefs: []
  type: TYPE_NORMAL
