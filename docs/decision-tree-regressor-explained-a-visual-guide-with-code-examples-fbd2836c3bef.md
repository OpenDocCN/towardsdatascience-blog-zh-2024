# å†³ç­–æ ‘å›å½’å™¨è§£é‡Šï¼šå¸¦æœ‰ä»£ç ç¤ºä¾‹çš„å¯è§†åŒ–æŒ‡å—

> åŸæ–‡ï¼š[https://towardsdatascience.com/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=collection_archive---------1-----------------------#2024-10-10](https://towardsdatascience.com/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=collection_archive---------1-----------------------#2024-10-10)

## å›å½’ç®—æ³•

## æ˜æ™ºåœ°ä¿®å‰ªåˆ†æ”¯ï¼Œé€šè¿‡æˆæœ¬å¤æ‚åº¦ä¿®å‰ª

[](https://medium.com/@samybaladram?source=post_page---byline--fbd2836c3bef--------------------------------)[![Samy Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--fbd2836c3bef--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fbd2836c3bef--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fbd2836c3bef--------------------------------) [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--fbd2836c3bef--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fbd2836c3bef--------------------------------) Â·é˜…è¯»æ—¶é—´ï¼š11åˆ†é’ŸÂ·2024å¹´10æœˆ10æ—¥

--

[](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----fbd2836c3bef--------------------------------) [## å†³ç­–æ ‘åˆ†ç±»å™¨è§£é‡Šï¼šå¸¦æœ‰ä»£ç ç¤ºä¾‹çš„åˆå­¦è€…å¯è§†åŒ–æŒ‡å—

### å¯¹æˆ‘ä»¬å–œçˆ±çš„å€’ç«‹æ ‘çš„å…¨æ–°è§†è§’

towardsdatascience.com](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----fbd2836c3bef--------------------------------)

å†³ç­–æ ‘ä¸ä»…ä»…é™äºæ•°æ®åˆ†ç±»â€”â€”å®ƒä»¬åŒæ ·æ“…é•¿é¢„æµ‹æ•°å€¼ï¼åˆ†ç±»æ ‘ç»å¸¸å æ®ç„¦ç‚¹ï¼Œä½†å†³ç­–æ ‘å›å½’å™¨ï¼ˆæˆ–å›å½’æ ‘ï¼‰åœ¨è¿ç»­å˜é‡é¢„æµ‹çš„ä¸–ç•Œä¸­æ˜¯å¼ºå¤§ä¸”å¤šåŠŸèƒ½çš„å·¥å…·ã€‚

è™½ç„¶æˆ‘ä»¬å°†è®¨è®ºå›å½’æ ‘æ„å»ºçš„æœºåˆ¶ï¼ˆè¿™äº›æœºåˆ¶ä¸åˆ†ç±»æ ‘å¤§è‡´ç›¸åŒï¼‰ï¼Œä½†åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¿˜å°†è¶…è¶Šåˆ†ç±»å™¨æ–‡ç« ä¸­ä»‹ç»çš„*å‰*ä¿®å‰ªæ–¹æ³•ï¼Œå¦‚â€œæœ€å°æ ·æœ¬å¶èŠ‚ç‚¹â€å’Œâ€œæœ€å¤§æ ‘æ·±åº¦â€ã€‚æˆ‘ä»¬å°†æ¢ç´¢æœ€å¸¸è§çš„*å*ä¿®å‰ªæ–¹æ³•â€”â€”**æˆæœ¬å¤æ‚åº¦ä¿®å‰ª**ï¼Œå®ƒä¸ºå†³ç­–æ ‘çš„æˆæœ¬å‡½æ•°å¼•å…¥äº†ä¸€ä¸ªå¤æ‚åº¦å‚æ•°ã€‚

![](../Images/af2717ef7b5f2f1e541ab5ba0909511d.png)

æ‰€æœ‰å¯è§†åŒ–å›¾ï¼šä½œè€…ä½¿ç”¨Canva Proåˆ›ä½œã€‚ä¼˜åŒ–ä¸ºç§»åŠ¨ç«¯æ˜¾ç¤ºï¼›åœ¨æ¡Œé¢ç«¯å¯èƒ½æ˜¾å¾—è¿‡å¤§ã€‚

# å®šä¹‰

å›å½’å†³ç­–æ ‘æ˜¯ä¸€ç§ä½¿ç”¨æ ‘çŠ¶ç»“æ„é¢„æµ‹æ•°å€¼çš„æ¨¡å‹ã€‚å®ƒæ ¹æ®å…³é”®ç‰¹å¾æ‹†åˆ†æ•°æ®ï¼Œä»æ ¹é—®é¢˜å¼€å§‹å¹¶åˆ†æ”¯ã€‚æ¯ä¸ªèŠ‚ç‚¹è¯¢é—®ä¸€ä¸ªç‰¹å¾ï¼Œç»§ç»­æ‹†åˆ†æ•°æ®ï¼Œç›´åˆ°è¾¾åˆ°å¶èŠ‚ç‚¹å¹¶ä½œå‡ºæœ€ç»ˆé¢„æµ‹ã€‚è¦è·å¾—ç»“æœï¼Œä½ éœ€è¦ä»æ ¹èŠ‚ç‚¹åˆ°å¶èŠ‚ç‚¹ï¼Œæ²¿ç€åŒ¹é…æ•°æ®ç‰¹å¾çš„è·¯å¾„è¿›è¡Œè·Ÿè¸ªã€‚

![](../Images/4d58bb0bb413cc741a3d2babe2bb0a24.png)

å›å½’å†³ç­–æ ‘é€šè¿‡ä¸€ç³»åˆ—åŸºäºæ•°æ®çš„é—®é¢˜æ¥é¢„æµ‹æ•°å€¼ç»“æœï¼Œé€æ­¥ç¼©å°èŒƒå›´ç›´åˆ°æœ€ç»ˆç»“æœã€‚

# ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†

ä¸ºäº†æ¼”ç¤ºæˆ‘ä»¬çš„æ¦‚å¿µï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[æˆ‘ä»¬çš„æ ‡å‡†æ•°æ®é›†](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629)ã€‚è¯¥æ•°æ®é›†ç”¨äºé¢„æµ‹æŸä¸€å¤©è®¿å®¢é«˜å°”å¤«çƒåœºçš„äººæ•°ï¼ŒåŒ…å«å¤©æ°”å±•æœ›ã€æ¸©åº¦ã€æ¹¿åº¦å’Œé£åŠ›ç­‰å˜é‡ã€‚

![](../Images/be5edbc6b8231b1605ca2447d8c33657.png)

åˆ—ï¼šâ€˜å¤©æ°”å±•æœ›â€™ï¼ˆé€šè¿‡ç‹¬çƒ­ç¼–ç è½¬ä¸ºæ™´å¤©ã€é˜´å¤©ã€é›¨å¤©ï¼‰ï¼Œâ€˜æ¸©åº¦â€™ï¼ˆä»¥åæ°åº¦è¡¨ç¤ºï¼‰ï¼Œâ€˜æ¹¿åº¦â€™ï¼ˆä»¥ç™¾åˆ†æ¯”è¡¨ç¤ºï¼‰ï¼Œâ€˜é£åŠ›â€™ï¼ˆæ˜¯/å¦ï¼‰å’Œâ€˜ç©å®¶æ•°é‡â€™ï¼ˆæ•°å€¼å‹ï¼Œç›®æ ‡ç‰¹å¾ï¼‰

```py
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Create dataset
dataset_dict = {
    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],
    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],
    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],
    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],
    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, 14, 34, 29, 49, 36, 57, 21, 23, 41]
}

df = pd.DataFrame(dataset_dict)

# One-hot encode 'Outlook' column
df = pd.get_dummies(df, columns=['Outlook'],prefix='',prefix_sep='')

# Convert 'Wind' column to binary
df['Wind'] = df['Wind'].astype(int)

# Rearrange columns
column_order = ['sunny', 'overcast', 'rain', 'Temperature', 'Humidity', 'Wind', 'Num_Players']
df = df[column_order]

# Split features and target
X, y = df.drop('Num_Players', axis=1), df['Num_Players']
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)
```

# ä¸»è¦æœºåˆ¶

å›å½’å†³ç­–æ ‘é€šè¿‡é€’å½’åœ°æ ¹æ®æœ€èƒ½å‡å°‘é¢„æµ‹è¯¯å·®çš„ç‰¹å¾æ¥æ‹†åˆ†æ•°æ®ã€‚ä»¥ä¸‹æ˜¯ä¸€èˆ¬è¿‡ç¨‹ï¼š

1.  ä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œä½¿ç”¨æ•´ä¸ªæ•°æ®é›†ã€‚

1.  é€‰æ‹©æœ€å°åŒ–ç‰¹å®šè¯¯å·®æŒ‡æ ‡ï¼ˆå¦‚å‡æ–¹è¯¯å·®æˆ–æ–¹å·®ï¼‰çš„ç‰¹å¾æ¥æ‹†åˆ†æ•°æ®ã€‚

1.  æ ¹æ®æ‹†åˆ†åˆ›å»ºå­èŠ‚ç‚¹ï¼Œæ¯ä¸ªå­èŠ‚ç‚¹è¡¨ç¤ºä¸ç›¸åº”ç‰¹å¾å€¼å¯¹é½çš„æ•°æ®å­é›†ã€‚

1.  å¯¹æ¯ä¸ªå­èŠ‚ç‚¹é‡å¤æ­¥éª¤2â€“3ï¼Œç»§ç»­æ‹†åˆ†æ•°æ®ç›´åˆ°è¾¾åˆ°åœæ­¢æ¡ä»¶ã€‚

1.  ä¸ºæ¯ä¸ªå¶èŠ‚ç‚¹åˆ†é…ä¸€ä¸ªæœ€ç»ˆçš„é¢„æµ‹å€¼ï¼Œé€šå¸¸æ˜¯è¯¥èŠ‚ç‚¹ä¸­ç›®æ ‡å€¼çš„**å¹³å‡å€¼**ã€‚

![](../Images/48ae23f237c31ae05ba52c569f790fb8.png)

# è®­ç»ƒæ­¥éª¤

æˆ‘ä»¬å°†æ¢ç´¢å†³ç­–æ ‘ç®—æ³•CARTï¼ˆåˆ†ç±»ä¸å›å½’æ ‘ï¼‰ä¸­çš„å›å½’éƒ¨åˆ†ã€‚å®ƒæ„å»ºäºŒå‰æ ‘ï¼Œé€šå¸¸éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. ä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œä½¿ç”¨æ‰€æœ‰è®­ç»ƒæ ·æœ¬ã€‚

![](../Images/62033724ca4273f8d77b2f9184658c43.png)

2. å¯¹äºæ•°æ®é›†ä¸­çš„æ¯ä¸ªç‰¹å¾ï¼š

a. æŒ‰å‡åºæ’åºç‰¹å¾å€¼ã€‚

b. å°†ç›¸é‚»å€¼ä¹‹é—´çš„æ‰€æœ‰ä¸­ç‚¹è§†ä¸ºæ½œåœ¨çš„æ‹†åˆ†ç‚¹ã€‚

![](../Images/b37c7a5dead079a3548bbc02b7d9d800.png)

æ€»å…±éœ€è¦æ£€æŸ¥23ä¸ªæ‹†åˆ†ç‚¹ã€‚

3. å¯¹æ¯ä¸ªæ½œåœ¨æ‹†åˆ†ç‚¹ï¼š

a. è®¡ç®—å½“å‰èŠ‚ç‚¹çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€‚

b. è®¡ç®—ç»“æœæ‹†åˆ†çš„åŠ æƒå¹³å‡è¯¯å·®ã€‚

![](../Images/edce98c77731a3a1fb3e871e75d9b0f2.png)

ä¾‹å¦‚ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è®¡ç®—äº†æ‹†åˆ†ç‚¹â€œæ¸©åº¦â€å€¼ä¸º73.5æ—¶çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰çš„åŠ æƒå¹³å‡å€¼ã€‚

```py
def calculate_split_mse(X_train, y_train, feature_name, split_point):
    # Create DataFrame and sort by feature
    analysis_df = pd.DataFrame({
        'feature': X_train[feature_name],
        'y_actual': y_train
    }).sort_values('feature')

    # Split data and calculate means
    left_mask = analysis_df['feature'] <= split_point
    left_mean = analysis_df[left_mask]['y_actual'].mean()
    right_mean = analysis_df[~left_mask]['y_actual'].mean()

    # Calculate squared differences
    analysis_df['squared_diff'] = np.where(
        left_mask,
        (analysis_df['y_actual'] - left_mean) ** 2,
        (analysis_df['y_actual'] - right_mean) ** 2
    )

    # Calculate MSEs and counts
    left_mse = analysis_df[left_mask]['squared_diff'].mean()
    right_mse = analysis_df[~left_mask]['squared_diff'].mean()
    n_left = sum(left_mask)
    n_right = len(analysis_df) - n_left

    # Calculate weighted average MSE
    weighted_mse = (n_left * left_mse + n_right * right_mse) / len(analysis_df)

    # Print results
    print(analysis_df)
    print(f"\nResults for split at {split_point} on feature '{feature_name}':")
    print(f"Left child MSE (n={n_left}, mean={left_mean:.2f}): {left_mse:.2f}")
    print(f"Right child MSE (n={n_right}, mean={right_mean:.2f}): {right_mse:.2f}")
    print(f"Weighted average MSE: {weighted_mse:.2f}")

# Example usage:
calculate_split_mse(X_train, y_train, 'Temperature', 73.5)
```

![](../Images/8302971702ad2e5e18eba4e710464c8a.png)

4. åœ¨è¯„ä¼°æ‰€æœ‰ç‰¹å¾å’Œæ‹†åˆ†ç‚¹åï¼Œé€‰æ‹©å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰åŠ æƒå¹³å‡å€¼æœ€ä½çš„ä¸€ä¸ªã€‚

![](../Images/2bcbe92a762d4f11ca62b9548587d9d7.png)

```py
def evaluate_all_splits(X_train, y_train):
    """Evaluate all possible split points using midpoints for all features"""
    results = []

    for feature in X_train.columns:
        data = pd.DataFrame({'feature': X_train[feature], 'y_actual': y_train})
        splits = [(a + b)/2 for a, b in zip(sorted(data['feature'].unique())[:-1], 
                                          sorted(data['feature'].unique())[1:])]

        for split in splits:
            left_mask = data['feature'] <= split
            n_left = sum(left_mask)

            if not (0 < n_left < len(data)): continue

            left_mean = data[left_mask]['y_actual'].mean()
            right_mean = data[~left_mask]['y_actual'].mean()

            left_mse = ((data[left_mask]['y_actual'] - left_mean) ** 2).mean()
            right_mse = ((data[~left_mask]['y_actual'] - right_mean) ** 2).mean()

            weighted_mse = (n_left * left_mse + (len(data) - n_left) * right_mse) / len(data)

            results.append({'Feature': feature, 'Split_Point': split, 'Weighted_MSE': weighted_mse})

    return pd.DataFrame(results).round(2)

# Example usage:
results = evaluate_all_splits(X_train, y_train)
print(results)
```

![](../Images/1ce048d6bbf06583edef5c90b22e9d5e.png)

5\. æ ¹æ®é€‰æ‹©çš„ç‰¹å¾å’Œåˆ‡åˆ†ç‚¹åˆ›å»ºä¸¤ä¸ªå­èŠ‚ç‚¹ï¼š

- å·¦å­èŠ‚ç‚¹ï¼šç‰¹å¾å€¼ <= åˆ‡åˆ†ç‚¹çš„æ ·æœ¬

- å³å­èŠ‚ç‚¹ï¼šç‰¹å¾å€¼ > åˆ‡åˆ†ç‚¹çš„æ ·æœ¬

![](../Images/87c47d48c4a8dd061010b27bc42362f0.png)

6\. å¯¹æ¯ä¸ªå­èŠ‚ç‚¹é€’å½’é‡å¤æ­¥éª¤2åˆ°5ã€‚ï¼ˆç»§ç»­ç›´åˆ°æ»¡è¶³åœæ­¢å‡†åˆ™ã€‚ï¼‰

![](../Images/60ff1e00eadc6faef39b8fb6a57fe968.png)![](../Images/3ad498b5148c0d1e27ed7cf1a7f885e7.png)

7\. åœ¨æ¯ä¸ªå¶èŠ‚ç‚¹ï¼Œåˆ†é…è¯¥èŠ‚ç‚¹ä¸­æ ·æœ¬çš„å¹³å‡ç›®æ ‡å€¼ä½œä¸ºé¢„æµ‹å€¼ã€‚

![](../Images/aab4fe4bdcce592f0c4e10224a18ad10.png)

```py
from sklearn.tree import DecisionTreeRegressor, plot_tree
import matplotlib.pyplot as plt

# Train the model
regr = DecisionTreeRegressor(random_state=42)
regr.fit(X_train, y_train)

# Visualize the decision tree
plt.figure(figsize=(26,8))
plot_tree(regr, feature_names=X.columns, filled=True, rounded=True, impurity=False, fontsize=16, precision=2)
plt.tight_layout()
plt.show()
```

![](../Images/4c99833d95cd4cda3734cf643d7c2b9d.png)

åœ¨è¿™ä¸ªscikit-learnçš„è¾“å‡ºä¸­ï¼Œå±•ç¤ºäº†å¶èŠ‚ç‚¹å’Œä¸­é—´èŠ‚ç‚¹çš„æ ·æœ¬åŠå…¶å€¼ã€‚

# å›å½’/é¢„æµ‹æ­¥éª¤

è¿™æ˜¯å›å½’æ ‘å¦‚ä½•å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹çš„è¿‡ç¨‹ï¼š

1\. ä»æ ‘çš„é¡¶éƒ¨ï¼ˆæ ¹èŠ‚ç‚¹ï¼‰å¼€å§‹ã€‚

2\. åœ¨æ¯ä¸ªå†³ç­–ç‚¹ï¼ˆèŠ‚ç‚¹ï¼‰ï¼š

- æŸ¥çœ‹ç‰¹å¾å’Œåˆ‡åˆ†å€¼ã€‚

- å¦‚æœæ•°æ®ç‚¹çš„ç‰¹å¾å€¼è¾ƒå°æˆ–ç›¸ç­‰ï¼Œå‘å·¦èµ°ã€‚

- å¦‚æœå®ƒæ›´å¤§ï¼Œå‘å³èµ°ã€‚

3\. ä¸€ç›´å‘ä¸‹ç§»åŠ¨ç›´åˆ°åˆ°è¾¾æ ‘çš„æœ«ç«¯ï¼ˆå¶èŠ‚ç‚¹ï¼‰ã€‚

4\. é¢„æµ‹å€¼æ˜¯è¯¥å¶èŠ‚ç‚¹ä¸­å­˜å‚¨çš„å¹³å‡å€¼ã€‚

![](../Images/ce9e66e2984c1dd8926cee0a62199806.png)

## è¯„ä¼°æ­¥éª¤

![](../Images/b1973f3c301a44f070306fc8def936ce.png)

è¿™ä¸ªRMSEå€¼æ¯”[è™šæ‹Ÿå›å½’å™¨çš„ç»“æœ](https://medium.com/towards-data-science/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629)è¦å¥½å¾—å¤šã€‚

# é¢„å‰ªæä¸åå‰ªæ

åœ¨æ„å»ºå®Œæ ‘ä¹‹åï¼Œæˆ‘ä»¬å”¯ä¸€éœ€è¦æ‹…å¿ƒçš„å°±æ˜¯å¦‚ä½•ä½¿æ ‘å˜å°ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚é€šå¸¸ï¼Œä¿®å‰ªçš„æ–¹æ³•å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š

## é¢„å‰ªæ

é¢„å‰ªæï¼Œä¹Ÿç§°ä¸ºæ—©æœŸåœæ­¢ï¼Œæ˜¯æŒ‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŸºäºæŸäº›é¢„å®šä¹‰çš„æ ‡å‡†ï¼Œåœæ­¢å†³ç­–æ ‘çš„ç”Ÿé•¿ã€‚æ­¤æ–¹æ³•æ—¨åœ¨é˜²æ­¢æ ‘å˜å¾—è¿‡äºå¤æ‚å¹¶å¯¼è‡´è¿‡æ‹Ÿåˆã€‚å¸¸è§çš„é¢„å‰ªææŠ€æœ¯åŒ…æ‹¬ï¼š

1.  **æœ€å¤§æ·±åº¦**ï¼šé™åˆ¶æ ‘çš„æœ€å¤§æ·±åº¦ã€‚

1.  **æ¯æ¬¡åˆ‡åˆ†çš„æœ€å°æ ·æœ¬æ•°**ï¼šè¦æ±‚åˆ‡åˆ†ä¸€ä¸ªèŠ‚ç‚¹æ—¶ï¼Œå¿…é¡»æ»¡è¶³æœ€å°æ ·æœ¬æ•°çš„æ¡ä»¶ã€‚

1.  **æ¯ä¸ªå¶èŠ‚ç‚¹çš„æœ€å°æ ·æœ¬æ•°**ï¼šç¡®ä¿æ¯ä¸ªå¶èŠ‚ç‚¹è‡³å°‘åŒ…å«ä¸€å®šæ•°é‡çš„æ ·æœ¬ã€‚

1.  **æœ€å¤§å¶èŠ‚ç‚¹æ•°**ï¼šé™åˆ¶æ ‘ä¸­å¶èŠ‚ç‚¹çš„æ€»æ•°ã€‚

1.  **æœ€å°çº¯åº¦å‡å°‘**ï¼šä»…å…è®¸é‚£äº›å‡å°‘çº¯åº¦è¾¾åˆ°æŒ‡å®šå€¼çš„åˆ‡åˆ†ã€‚

è¿™äº›æ–¹æ³•åœ¨æ»¡è¶³æŒ‡å®šæ¡ä»¶æ—¶åœæ­¢æ ‘çš„ç”Ÿé•¿ï¼Œæœ‰æ•ˆåœ°åœ¨æ ‘çš„æ„å»ºé˜¶æ®µè¿›è¡Œâ€œä¿®å‰ªâ€ã€‚

([æˆ‘ä»¬ä¹‹å‰å·²ç»è®¨è®ºè¿‡è¿™äº›æ–¹æ³•ï¼Œå®ƒä»¬åœ¨å›å½’é—®é¢˜ä¸­å®Œå…¨ç›¸åŒã€‚](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e))

## åå‰ªæ

åä¿®å‰ªæ–¹æ³•å…è®¸å†³ç­–æ ‘ç”Ÿé•¿åˆ°æœ€å¤§ï¼Œç„¶åä¿®å‰ªå›å»ä»¥å‡å°‘å¤æ‚æ€§ã€‚è¿™ç§æ–¹æ³•é¦–å…ˆæ„å»ºå®Œæ•´çš„æ ‘ï¼Œç„¶åç§»é™¤æˆ–åˆå¹¶é‚£äº›å¯¹æ¨¡å‹è¡¨ç°è´¡çŒ®ä¸å¤§çš„åˆ†æ”¯ã€‚ä¸€ç§å¸¸è§çš„åä¿®å‰ªæŠ€æœ¯è¢«ç§°ä¸º**æˆæœ¬å¤æ‚åº¦ä¿®å‰ª**ã€‚

# æˆæœ¬å¤æ‚åº¦ä¿®å‰ª

## æ­¥éª¤1ï¼šè®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„ä¸çº¯åº¦

å¯¹äºæ¯ä¸ªä¸­é—´èŠ‚ç‚¹ï¼Œè®¡ç®—ä¸çº¯åº¦ï¼ˆå›å½’é—®é¢˜ä¸­çš„MSEï¼‰ã€‚ç„¶åæˆ‘ä»¬å°†è¿™ä¸ªå€¼ä»å°åˆ°å¤§æ’åºã€‚

![](../Images/57e29c193ac2b684411d3da6c0176e9c.png)

```py
# Visualize the decision tree
plt.figure(figsize=(26,8))
plot_tree(regr, feature_names=X.columns, filled=True, rounded=True, impurity=True, fontsize=16, precision=2)
plt.tight_layout()
plt.show()
```

![](../Images/d4244591a1bc3d372a2b2429fc6e24b9.png)

åœ¨è¿™ä¸ªscikit-learnçš„è¾“å‡ºä¸­ï¼ŒèŠ‚ç‚¹çš„ä¸çº¯åº¦æ˜¾ç¤ºä¸ºæ¯ä¸ªèŠ‚ç‚¹çš„â€œsquared_errorâ€ã€‚

![](../Images/fbbc7f5114b6b7959849be45f651aec6.png)

æˆ‘ä»¬ç»™è¿™äº›ä¸­é—´èŠ‚ç‚¹ï¼ˆä»Aåˆ°Jï¼‰å‘½åã€‚ç„¶åæˆ‘ä»¬æ ¹æ®å®ƒä»¬çš„MSEä»å°åˆ°å¤§è¿›è¡Œæ’åºã€‚

## æ­¥éª¤2ï¼šé€šè¿‡ä¿®å‰ªæœ€å¼±çš„é“¾æ¥æ¥åˆ›å»ºå­æ ‘

ç›®æ ‡æ˜¯ä»**å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æœ€å°çš„èŠ‚ç‚¹**å¼€å§‹ï¼Œé€æ¸å°†ä¸­é—´èŠ‚ç‚¹è½¬åŒ–ä¸ºå¶å­èŠ‚ç‚¹ã€‚æˆ‘ä»¬å¯ä»¥åŸºäºæ­¤åˆ›å»ºä¸€ä¸ªä¿®å‰ªè·¯å¾„ã€‚

![](../Images/bd666447562c4b89a0452d15a3a697b9.png)

æˆ‘ä»¬å°†å®ƒä»¬å‘½åä¸ºâ€œå­æ ‘*i*â€ï¼ŒåŸºäºå®ƒè¢«ä¿®å‰ªçš„æ¬¡æ•°(*i*)ã€‚ä»åŸå§‹æ ‘å¼€å§‹ï¼Œæ ‘ä¼šåœ¨å…·æœ‰æœ€ä½MSEçš„èŠ‚ç‚¹ä¸Šä¿®å‰ªï¼ˆä»èŠ‚ç‚¹Jå¼€å§‹ï¼ŒMï¼ˆå·²ç»è¢«Jä¿®å‰ªæ‰ï¼‰ï¼ŒLï¼ŒKï¼Œä¾æ­¤ç±»æ¨ï¼‰ã€‚

## æ­¥éª¤3ï¼šè®¡ç®—æ¯æ£µå­æ ‘çš„æ€»å¶å­ä¸çº¯åº¦

å¯¹äºæ¯ä¸€æ£µå­æ ‘*T*ï¼Œå¯ä»¥è®¡ç®—æ€»å¶å­ä¸çº¯åº¦ï¼ˆ*R*(*T*)ï¼‰ï¼š

*R*(*T*) = (1/*N*) Î£ *I*(*L*) * *n*_*L*

å…¶ä¸­ï¼š

**Â·** *L* éå†æ‰€æœ‰å¶å­èŠ‚ç‚¹

**Â·** *n_L* æ˜¯å¶å­*L*ä¸­æ ·æœ¬çš„æ•°é‡ **Â·** *N* æ˜¯æ ‘ä¸­æ ·æœ¬çš„æ€»æ•°

**Â·** *I*(*L*) æ˜¯å¶å­*L*çš„ä¸çº¯åº¦ï¼ˆMSEï¼‰

![](../Images/db7b97078d3ca41116ba6586413f17a6.png)

æˆ‘ä»¬ä¿®å‰ªå¾—è¶Šå¤šï¼Œæ€»çš„å¶å­ä¸çº¯åº¦å°±è¶Šé«˜ã€‚

## æ­¥éª¤4ï¼šè®¡ç®—æˆæœ¬å‡½æ•°

ä¸ºäº†æ§åˆ¶ä½•æ—¶åœæ­¢å°†ä¸­é—´èŠ‚ç‚¹è½¬åŒ–ä¸ºå¶å­èŠ‚ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ä»¥ä¸‹å…¬å¼æ£€æŸ¥æ¯æ£µå­æ ‘*T*çš„æˆæœ¬å¤æ‚åº¦ï¼š

Cost(*T*) = *R*(*T*) + *Î±* * |*T*|

å…¶ä¸­ï¼š

**Â·** *R*(*T*) æ˜¯æ€»å¶å­ä¸çº¯åº¦

**Â·** |*T*| æ˜¯å­æ ‘ä¸­çš„å¶å­èŠ‚ç‚¹æ•°**Â·** *Î±* æ˜¯å¤æ‚åº¦å‚æ•°

![](../Images/d1e444f5b41346a1628e95d70944d312.png)

## æ­¥éª¤5ï¼šé€‰æ‹©Alpha

*Î±*çš„å€¼æ§åˆ¶æˆ‘ä»¬æœ€ç»ˆå¾—åˆ°å“ªä¸€æ£µå­æ ‘ã€‚**å…·æœ‰æœ€ä½æˆæœ¬çš„å­æ ‘å°†æ˜¯æœ€ç»ˆçš„æ ‘**ã€‚

![](../Images/7f89a62ec5564f5169596ae9b6d2d1d6.png)

å½“*Î±*è¾ƒå°æ—¶ï¼Œæˆ‘ä»¬æ›´å…³å¿ƒå‡†ç¡®æ€§ï¼ˆè¾ƒå¤§çš„æ ‘ï¼‰ã€‚å½“*Î±*è¾ƒå¤§æ—¶ï¼Œæˆ‘ä»¬æ›´å…³å¿ƒç®€æ´æ€§ï¼ˆè¾ƒå°çš„æ ‘ï¼‰ã€‚

è™½ç„¶æˆ‘ä»¬å¯ä»¥è‡ªç”±è®¾å®š*Î±*ï¼Œåœ¨scikit-learnä¸­ï¼Œä½ ä¹Ÿå¯ä»¥è·å–æœ€å°çš„*Î±*å€¼æ¥è·å¾—ç‰¹å®šçš„å­æ ‘ã€‚è¿™è¢«ç§°ä¸º**æœ‰æ•ˆçš„*Î±***ã€‚

![](../Images/d70f61e11dcfad94df3cbd4294949d3d.png)

è¿™ä¸ªæœ‰æ•ˆçš„***Î±*** *ä¹Ÿå¯ä»¥è®¡ç®—å‡ºæ¥*ã€‚

```py
# Compute the cost-complexity pruning path
tree = DecisionTreeRegressor(random_state=42)
effective_alphas = tree.cost_complexity_pruning_path(X_train, y_train).ccp_alphas
impurities = tree.cost_complexity_pruning_path(X_train, y_train).impurities

# Function to count leaf nodes
count_leaves = lambda tree: sum(tree.tree_.children_left[i] == tree.tree_.children_right[i] == -1 for i in range(tree.tree_.node_count))

# Train trees and count leaves for each complexity parameter
leaf_counts = [count_leaves(DecisionTreeRegressor(random_state=0, ccp_alpha=alpha).fit(X_train_scaled, y_train)) for alpha in effective_alphas]

# Create DataFrame with analysis results
pruning_analysis = pd.DataFrame({
    'total_leaf_impurities': impurities,
    'leaf_count': leaf_counts,
    'cost_function': [f"{imp:.3f} + {leaves}Î±" for imp, leaves in zip(impurities, leaf_counts)],
    'effective_Î±': effective_alphas
})

print(pruning_analysis)
```

![](../Images/970ac8efb0ce4a38d8e6f98f7d4a57ee.png)

## æœ€ç»ˆå¤‡æ³¨

é¢„å‰ªææ–¹æ³•é€šå¸¸æ›´å¿«ä¸”æ›´èŠ‚çœå†…å­˜ï¼Œå› ä¸ºå®ƒä»¬ä»ä¸€å¼€å§‹å°±é˜²æ­¢äº†æ ‘å½¢è¿‡å¤§ã€‚

åå‰ªæå¯èƒ½åˆ›å»ºå‡ºæ›´ä¼˜çš„æ ‘å½¢ç»“æ„ï¼Œå› ä¸ºå®ƒä¼šåœ¨åšå‡ºå‰ªæå†³å®šä¹‹å‰è€ƒè™‘æ•´ä¸ªæ ‘çš„ç»“æ„ã€‚ç„¶è€Œï¼Œè¿™å¯èƒ½ä¼šæ¶ˆè€—æ›´å¤šçš„è®¡ç®—èµ„æºã€‚

ä¸¤ç§æ–¹æ³•çš„ç›®æ ‡éƒ½æ˜¯åœ¨æ¨¡å‹å¤æ‚åº¦å’Œæ€§èƒ½ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼Œç›®çš„æ˜¯åˆ›å»ºä¸€ä¸ªèƒ½å¾ˆå¥½åœ°å¯¹æœªè§æ•°æ®è¿›è¡Œæ³›åŒ–çš„æ¨¡å‹ã€‚é€‰æ‹©é¢„å‰ªæè¿˜æ˜¯åå‰ªæï¼ˆæˆ–ä¸¤è€…ç»“åˆï¼‰é€šå¸¸å–å†³äºå…·ä½“çš„æ•°æ®é›†ã€å½“å‰é—®é¢˜ä»¥åŠå¯ç”¨çš„è®¡ç®—èµ„æºã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€šå¸¸ä¼šç»“åˆä½¿ç”¨è¿™äº›æ–¹æ³•ï¼Œæ¯”å¦‚å…ˆåº”ç”¨ä¸€äº›é¢„å‰ªææ ‡å‡†æ¥é˜²æ­¢æ ‘å½¢è¿‡å¤§ï¼Œå†ä½¿ç”¨åå‰ªæå¯¹æ¨¡å‹çš„å¤æ‚åº¦è¿›è¡Œå¾®è°ƒã€‚

# ğŸŒŸ å†³ç­–æ ‘å›å½’å™¨ï¼ˆå¸¦æˆæœ¬å¤æ‚åº¦å‰ªæï¼‰ä»£ç æ€»ç»“

```py
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import root_mean_squared_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import StandardScaler

# Create dataset
dataset_dict = {
    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],
    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],
    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],
    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],
    'Num_Players': [52,39,43,37,28,19,43,47,56,33,49,23,42,13,33,29,25,51,41,14,34,29,49,36,57,21,23,41]
}

df = pd.DataFrame(dataset_dict)

# One-hot encode 'Outlook' column
df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)

# Convert 'Wind' column to binary
df['Wind'] = df['Wind'].astype(int)

# Split data into features and target, then into training and test sets
X, y = df.drop(columns='Num_Players'), df['Num_Players']
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)

# Initialize Decision Tree Regressor
tree = DecisionTreeRegressor(random_state=42)

# Get the cost complexity path, impurities, and effective alpha
path = tree.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
print(ccp_alphas)
print(impurities)

# Train the final tree with the chosen alpha
final_tree = DecisionTreeRegressor(random_state=42, ccp_alpha=0.1)
final_tree.fit(X_train_scaled, y_train)

# Make predictions
y_pred = final_tree.predict(X_test)

# Calculate and print RMSE
rmse = root_mean_squared_error(y_test, y_pred)
print(f"RMSE: {rmse:.4f}")
```

## è¿›ä¸€æ­¥é˜…è¯»

å…³äº[å†³ç­–æ ‘å›å½’å™¨](https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeRegressor.html)ã€[æˆæœ¬å¤æ‚åº¦å‰ªæ](https://scikit-learn.org/1.5/auto_examples/tree/plot_cost_complexity_pruning.html)åŠå…¶åœ¨scikit-learnä¸­çš„å®ç°ï¼Œè¯»è€…å¯ä»¥å‚è€ƒå…¶å®˜æ–¹æ–‡æ¡£ã€‚è¯¥æ–‡æ¡£æä¾›äº†å…³äºä½¿ç”¨æ–¹æ³•å’Œå‚æ•°çš„å…¨é¢ä¿¡æ¯ã€‚

## æŠ€æœ¯ç¯å¢ƒ

æœ¬æ–‡ä½¿ç”¨Python 3.7å’Œscikit-learn 1.5ã€‚è™½ç„¶è®¨è®ºçš„æ¦‚å¿µé€šå¸¸é€‚ç”¨ï¼Œä½†å…·ä½“çš„ä»£ç å®ç°å¯èƒ½ä¼šå› ç‰ˆæœ¬ä¸åŒè€Œç•¥æœ‰å·®å¼‚ã€‚

## å…³äºæ’å›¾

é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾åƒå‡ç”±ä½œè€…åˆ›ä½œï¼Œå¹¶ç»“åˆäº†Canva Proæˆæƒçš„è®¾è®¡å…ƒç´ ã€‚

ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ™ğ™šğ™œğ™§ğ™šğ™¨ğ™¨ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:

![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)

[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fbd2836c3bef--------------------------------)

## å›å½’ç®—æ³•

[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----fbd2836c3bef--------------------------------)5ä¸ªæ•…äº‹![ä¸€ä¸ªæˆ´ç€ç²‰çº¢è‰²å¸½å­ã€æ‰ç€è¾«å­çš„å¡é€šç©å¶ã€‚è¿™åªâ€œè™šæ‹Ÿâ€ç©å¶ï¼Œå‡­å€Ÿå…¶åŸºç¡€çš„è®¾è®¡å’Œå¿ƒå½¢å›¾æ¡ˆçš„è¡¬è¡«ï¼Œå½¢è±¡åœ°è¡¨ç°äº†æœºå™¨å­¦ä¹ ä¸­çš„è™šæ‹Ÿå›å½’å™¨æ¦‚å¿µã€‚å°±åƒè¿™ä¸ªç©å…·èˆ¬çš„å½¢è±¡æ˜¯ä¸€ä¸ªç®€åŒ–ã€é™æ€çš„äººçš„è¡¨ç°ï¼Œè™šæ‹Ÿå›å½’å™¨åˆ™æ˜¯ä½œä¸ºæ›´å¤æ‚åˆ†æåŸºçº¿çš„åŸºæœ¬æ¨¡å‹ã€‚](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)

ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:

![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)

[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fbd2836c3bef--------------------------------)

## åˆ†ç±»ç®—æ³•

[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----fbd2836c3bef--------------------------------) 8 ä¸ªæ•…äº‹ï¼[](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)
