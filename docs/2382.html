<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Support Vector Classifier, Explained: A Visual Guide with Mini 2D Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Support Vector Classifier, Explained: A Visual Guide with Mini 2D Dataset</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=collection_archive---------3-----------------------#2024-10-01">https://towardsdatascience.com/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=collection_archive---------3-----------------------#2024-10-01</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="8202" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">CLASSIFICATION ALGORITHM</h2><div/><div><h2 id="ebc0" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Finding the best “line” to separate the classes? Yeah, sure...</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--62e831e7b9e9--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--62e831e7b9e9--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--62e831e7b9e9--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--62e831e7b9e9--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 1, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">6</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/5a473769a53d065bc213ca926988bd11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aRsuQb-td2NUCRB8uezRUQ.png"/></div></div></figure><p id="c690" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><code class="cx ny nz oa ob b">⛳️ More <a class="af oc" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c" rel="noopener">CLASSIFICATION ALGORITHM</a>, explained: <br/> · <a class="af oc" rel="noopener" target="_blank" href="/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e">Dummy Classifier</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1">K Nearest Neighbor Classifier</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">Bernoulli Naive Bayes</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c">Gaussian Naive Bayes</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">Decision Tree Classifier</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505">Logistic Regression</a> <br/> ▶ <a class="af oc" rel="noopener" target="_blank" href="/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9">Support Vector Classifier</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c">Multilayer Perceptron</a></code></p><p id="ff11" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">“Support Vector Machine (SVM) for classification works on a very basic principle — it tries to find the best line that separates the two classes.” But if I hear that oversimplified explanation <strong class="ne ga">one more time</strong>, I might just scream into a pillow.</p><p id="c328" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While the premise sounds simple, SVM is one of those algorithms packed with mathematical gymnastics that took me an absurd amount of time to grasp. Why is it even called a ‘machine’? Why do we need support vectors? Why are some points suddenly not important? And why does it have to be a straight line — oh wait, a <strong class="ne ga">straight hyperplane</strong>??? Then there’s the optimization formula, which is apparently so tricky that we need another version called the dual form. But hold on, now we need <strong class="ne ga">another</strong> algorithm called SMO to solve that? What’s with all the dual coefficients that scikit-learn just spits out? And if that’s not enough, we’re suddenly pulling off this magic ‘kernel tricks’ when a straight line doesn’t cut it? Why do we even need these tricks? And why do none of the tutorials ever show the actual numbers?!</p><p id="e87d" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this article, I’m trying to stop this Support Vector Madness. After hours and hours trying to really understand this algorithm, I will try to explain what’s ACTUALLY going on with ACTUAL numbers (and of course, its visualization too) but without the complicated maths, perfect for beginners.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/073bec34540009054f6bb609f371fa8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lhCHdLyulB_1lkKBQhloYw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="d214" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Definition</h1><p id="e1c3" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Support Vector Machines are supervised learning models used mainly for classification tasks, though they can be adapted for regression as well. SVMs aim to find the line that best divides a dataset into classes (<em class="pj">sigh…</em>), maximizing the margin between these classes.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/0926d86dbc53835cc82cdbc41023c182.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UaWGTXNfRemqSJgCwAm19Q.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Despite its complexities, SVM can be considered one of the fundamental algorithms in machine learning.</figcaption></figure><p id="0f8d" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">“Support vectors” are the data points that lie closest to the line and can actually define that line as well. And, what’s with the “Machine” then ? While other machine learning algorithms could include “Machine,” SVM’s naming may be partly due to historical context when it was developed. That’s it.</p><h1 id="544f" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">📊 Dataset Used</h1><p id="486a" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">To understand how SVM works, it is a good idea to start from a dataset with few samples and smaller dimension. We’ll use this simple mini 2D dataset (inspired by [1]) as an example.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp pk"><img src="../Images/608af2d7d2e1d635cd2f830d7902f83a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5JBvOLQs_PJ-0NiXQrmAYg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Columns: Temperature (0–3), Humidity (0–3), Play Golf (Yes/No). The training dataset has 2 dimensions and 8 samples.</figcaption></figure><p id="1743" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Instead of explaining the steps of the training process itself, we will walk from keyword to keyword and see how SVM actually works:</p><h1 id="6c0d" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Part 1: Basic Components</h1><h2 id="86c2" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Decision Boundary</h2><p id="1447" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The decision boundary in SVM is the line (or called “hyperplane” in higher dimensions) that the algorithm determines to best separate different classes of data.</p><p id="49ef" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This line would attempt to keep most “YES” points on one side and most “NO” points on the other. However, because for data that isn’t linearly separable, this boundary won’t be perfect — some points might be on the “wrong” side.</p><p id="297e" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once this line is established, any new data can be classified depending on which side of the boundary it falls.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qb"><img src="../Images/475eed31b224801da296d616dfd81eb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B0Oqj5VK4i5jjCjdB8lbLw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">In our golf example, it would be the line that tries to separate the “YES” (play golf) points from the “NO” points. SVM would try to position this line even though a perfect separation isn’t possible with a straight line. At this point, using our eyes, this seems to be a nice line that make a good separator.</figcaption></figure><h2 id="f619" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Linear Separability</h2><p id="fe6b" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Linear separability refers to whether we can draw a straight line that perfectly separates two classes of data points. If data is linearly separable, SVM can find a clear, hard boundary between classes. However, when data isn’t linearly separable (as in our case) SVM needs to use more advanced techniques.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qc"><img src="../Images/c464c383be7459c488a79148a52e09e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eHJz3tCLNluwoqDecDgSKg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">In the training set, no matter how we draw the line, we cannot separate the two classes. If we omit index 1 &amp; 8, now we can.</figcaption></figure><h2 id="34b4" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Margin</h2><p id="71c9" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The margin in SVM is the distance between the decision boundary and the closest data points from each class. These closest points are called support vectors.</p><p id="3673" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">SVM aims to maximize this margin. A larger margin generally leads to better generalization — the ability to correctly classify new, unseen data points.</p><p id="50b1" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, because data in general isn’t perfectly separable, SVM might use a soft margin approach. This allows some points to be within the margin or even on the wrong side of the boundary, trading off perfect separation for a more robust overall classifier.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qd"><img src="../Images/07c9ddad0a6f5df06001f200fdab8947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8x0m5KaR-ACmSkP_8jk2ig.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">SVM would try to position the decision boundary to create the widest possible margin while still separating most “YES” and “NO” instances.</figcaption></figure><h2 id="d645" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Hard Margin vs Soft Margin</h2><p id="7977" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Hard Margin SVM is the ideal scenario where all data points can be perfectly separated by the decision boundary, with no misclassifications. In this case, the margin is “hard” because it doesn’t allow any data points to be on the wrong side of the boundary or within the margin.</p><p id="b06a" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Soft Margin SVM, on the other hand, allows some flexibility. It permits some data points to be misclassified or to lie within the margin. This allows the SVM to find a good balance between:</p><ol class=""><li id="9b1d" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qe qf qg bk">Maximizing the margin</li><li id="5ac9" class="nc nd fq ne b gt qh ng nh gw qi nj nk nl qj nn no np qk nr ns nt ql nv nw nx qe qf qg bk">Minimizing classification errors</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qm"><img src="../Images/ebe390809fdb8c6f9f23aaf70d39dd80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J7WUDLY8SnBtUK2tqtZ5LA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">In our case, a Hard Margin approach isn’t possible because the data isn’t linearly separable. Therefore, a Soft Margin approach is necessary for our dataset. With Soft Margin SVM, you might allow points like ID 1 &amp; ID 8 to be on the “wrong” side of the boundary if it results in a better overall classifier.</figcaption></figure><h2 id="a62d" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk"><strong class="al">Distance Calculation</strong></h2><p id="2a3f" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">In SVM, distance calculations play an important role in both training and classification. The distance of a point <em class="pj">x</em> from the decision boundary is given by:</p><p id="e5dd" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">|<em class="pj">w</em> · <em class="pj">x</em> + <em class="pj">b</em>| / ||<em class="pj">w</em>||</p><p id="0c84" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">where <em class="pj">w</em> is the weight vector perpendicular to the hyperplane, <em class="pj">b</em> is the bias term, and ||<em class="pj">w</em>|| is the Euclidean norm of <em class="pj">w</em>.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/5db8a3f8f5972e138ee8ea808db49c62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C6Plr4wxcRpFf4B5oYXINQ.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">This way, we can see which points are the closest to the hyperplane without drawing it.</figcaption></figure><h2 id="2a34" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Support Vectors</h2><p id="9574" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Support vectors are the data points with closest distance to the hyperplane. They are important because: They “support” the hyperplane, defining its position.</p><p id="70d6" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">What makes Support Vectors special is that they are the only points that matter for determining the decision boundary. All other points could be removed without changing the boundary’s position. This is a key feature of SVM — it bases its decision on the most critical points rather than all data points.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qn"><img src="../Images/93e911910f9e0a7e061da7e7c7ed4412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tmIwycB2roW56eCOuER7Ng.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For this hyperplane, we have 3 support vectors that lies on the margin. The 2 misclassified data points can be regarded as support vectors as well in some situations.</figcaption></figure><h2 id="c5e4" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Slack Variables</h2><p id="cdaf" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Slack Variables are introduced in Soft Margin SVM to quantify the degree of misclassification or margin violation for each data point. They’re called “slack” because they give the model some slack or flexibility in fitting the data.</p><p id="a524" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In SVMs, slack variables<em class="pj"> ξᵢ</em> can be calculated as:</p><p id="32e2" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="pj">ξᵢ</em> = max(0, 1 — <em class="pj">yᵢ</em>(<em class="pj">w</em> · <em class="pj">xᵢ</em> + <em class="pj">b</em>))</p><p id="0887" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">where<br/>· <em class="pj">w</em> is the weight vector<br/>· <em class="pj">b</em> is the bias term<br/>· <em class="pj">xᵢ</em> are the input vectors<br/>· <em class="pj">yᵢ</em> are the corresponding labels</p><p id="da6e" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This formula only works when class labels <em class="pj">yᵢ </em>are in {-1, +1} format. It elegantly handles both classes:<br/>· Correctly classified points beyond margin: <em class="pj">ξᵢ</em> = 0<br/>· Misclassified or margin-violating points: <em class="pj">ξᵢ </em>&gt; 0</p><p id="d960" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using {-1, +1} labels maintains SVM’s mathematical symmetry and simplifies optimization, unlike {0, 1} labels which would require separate cases for each class.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/178df04c97f876c284556ca61937ee1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wb4-RTe_Oyxd8QJZcU-J_w.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">In our golf dataset, the point (3,3) — NO ends up on the “YES” side of our boundary. We’d assign a slack variable to this point to measure how far it is on the wrong side. Similarly, if (2,0) — NO is correctly classified but falls within the margin, it would also get a slack variable.</figcaption></figure><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qo"><img src="../Images/52dbed69530ef9c8757c2de48abc398c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I5X3ux-B0dPLpgOZC_QA2g.png"/></div></div></figure><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qp"><img src="../Images/b5f6ebc65cc316f3365fe3231a85f146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*97AoGj30RNdvr4IqNIAiwA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">In our golf dataset, the point (3,3) — NO ends up on the “YES” side of our boundary. We’d assign a slack variable to this point to measure how far it is on the wrong side. Similarly, if (2,0) — NO is correctly classified but falls within the margin, it would also get a slack variable.</figcaption></figure><h2 id="ac5a" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Primal Form for Hard Margin</h2><p id="9ea7" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The primal form is the original optimization problem formulation for SVMs. It directly expresses the goal of finding the maximum margin hyperplane in the feature space.</p><p id="587f" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In simple terms, the primal form seeks to:</p><ol class=""><li id="5b6d" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qe qf qg bk">Find a hyperplane that correctly classifies all data points.</li><li id="04c1" class="nc nd fq ne b gt qh ng nh gw qi nj nk nl qj nn no np qk nr ns nt ql nv nw nx qe qf qg bk">Maximize the distance between this hyperplane and the nearest data points from each class.</li></ol><p id="d1a5" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Primal form is:</p><p id="72b2" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">minimize</strong>: (1/2) ||<em class="pj">w</em>||²<br/><strong class="ne ga">subject to</strong>: <em class="pj">yᵢ</em>(<em class="pj">w</em> · <em class="pj">xᵢ</em> + <em class="pj">b</em>) ≥ 1 for all i</p><p id="abca" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">where<br/>· <em class="pj">w</em> is the weight vector<br/>· <em class="pj">b</em> is the bias term<br/>· <em class="pj">xᵢ</em> are the input vectors<br/>· <em class="pj">yᵢ</em> are the corresponding labels (+1 or -1)<br/>· ||<em class="pj">w</em>||² is the squared Euclidean norm of <em class="pj">w</em></p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qq"><img src="../Images/5a4e7ae7769876a41d594239f7a087e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sGITtv3OCDDtzcPfM0gRUw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">In the case of index 1 &amp; 8 omitted, we are trying to find the best line that has the bigger the margin.</figcaption></figure><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qr"><img src="../Images/b2f596443ba8eac5e03ae7421a9580e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ATJzxJYqZagJuimgFpgowg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">If we choose hyperplane with smaller margin, it gives higher value of the objective function, which is not what we want.</figcaption></figure><h2 id="c90f" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Primal Form for Soft Margin</h2><p id="e66d" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Remember that the soft margin SVM is an extension of the original (hard margin) SVM that allows for some misclassification? This change is reflected in the primal form. The soft margin SVM primal form becomes:</p><p id="9fdf" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">minimize</strong>: (1/2) ||<em class="pj">w|</em>|² + <em class="pj">C</em> Σ<em class="pj">ᵢ ξᵢ</em><br/><strong class="ne ga">subject to</strong>: <em class="pj">yᵢ</em>(<em class="pj">w</em> · <em class="pj">xᵢ</em> + <em class="pj">b</em>) ≥ 1 — <em class="pj">ξᵢ</em> for all <em class="pj">i</em>, <em class="pj">ξᵢ</em> ≥ 0 for all <em class="pj">i</em></p><p id="ce58" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">where<br/>· <em class="pj">C</em> is the penalty parameter<br/>· <em class="pj">ξᵢ</em> are the slack variables<br/>· All other variables are the same as in the hard margin case</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qs"><img src="../Images/bb835b40fcb1eed9a834078010114b8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j8F00XzqgYfURMHuNsPNVQ.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">The penalty of the wrongly classified data points contributes to the objective function as extra values to minimize.</figcaption></figure><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qt"><img src="../Images/e0a2b3cf3521173665282aab8c4280b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dAHVNd_PMjQnBA8xPOM8_A.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Say we choose another hyperplane that is a bit closer to index 8. The objective value is now higher. The more balance the distance from the wrongly classified ones, the smaller the total penalty.</figcaption></figure><h2 id="c331" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Dual Form</h2><p id="55a1" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Here’s the bad news: The primal form can be slow and hard to solve, especially for complex data.</p><p id="1668" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The dual form provides an alternative way to solve the SVM optimization problem, often leading to computational advantages. It’s formulated as follows:</p><p id="ba5c" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">maximize</strong>: <em class="pj">Σᵢ,ⱼ(αᵢyᵢ) - ½ΣᵢΣⱼ(αᵢαⱼyᵢyⱼ(xᵢ</em> · <em class="pj">xⱼ))<br/></em><strong class="ne ga">subject to:</strong> 0 ≤ <em class="pj">αᵢ</em> ≤ C for all i, Σ<em class="pj">ᵢαᵢyᵢ </em>= 0</p><p id="dd99" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Where:<br/>· <em class="pj">αᵢ</em> are the Lagrange multipliers (dual variables)<br/>· <em class="pj">yᵢ</em> are the class labels (+1 or -1)<br/>· <em class="pj">xᵢ</em> are the input vectors<br/>· <em class="pj">C</em> is the regularization parameter (upper bound for <em class="pj">αᵢ</em>)<br/>· (<em class="pj">xᵢ</em> · <em class="pj">xⱼ</em>) denotes the dot product between <em class="pj">xᵢ</em> and <em class="pj">xⱼ</em></p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qu"><img src="../Images/2e91b4c99d88023d6532f3cdb94d9fe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HMBQaM3B-7w6_4LdWxenTA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Other than the training data itself, the only other components in this dual form is the Lagrange multipliers (<em class="qv">αᵢ</em>).</figcaption></figure><h2 id="be0e" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Lagrange Multipliers</h2><p id="7606" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">As we notice in the dual form, Lagrange multipliers (<em class="pj">αᵢ</em>) show up when we transform the primal problem into its dual form (that’s why they also known as the dual coefficients). If you noticed, the weights &amp; bias are no longer there!</p><p id="019c" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Each data point in the training set has an associated Lagrange multiplier. The good thing is Lagrange multipliers make things much easier to understand:</p><ol class=""><li id="e7bc" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qe qf qg bk"><strong class="ne ga">Interpretation</strong>:<br/>- <em class="pj">αᵢ</em> = 0: The point is correctly classified and outside the margin. This point does not influence the decision boundary.<br/>- 0 &lt; <em class="pj">αᵢ</em> &lt; <em class="pj">C</em>: The point is on the margin boundary. These are called “free” or “unbounded” support vectors.<br/>- <em class="pj">αᵢ</em> = <em class="pj">C</em>: The point is either on or inside the margin (including misclassified points). These are called “bound” support vectors.</li><li id="7f47" class="nc nd fq ne b gt qh ng nh gw qi nj nk nl qj nn no np qk nr ns nt ql nv nw nx qe qf qg bk"><strong class="ne ga">Relationship to decision boundary</strong>: <br/><em class="pj">w</em> = Σ<em class="pj">ᵢ</em>(<em class="pj">αᵢ</em> <em class="pj">yᵢ</em> <em class="pj">xᵢ</em>),<br/><em class="pj">b</em> = <em class="pj">yᵢ</em> — Σ<em class="pj">ⱼ</em>(<em class="pj">αᵢ</em> <em class="pj">yⱼ</em>(<em class="pj">xⱼ</em> · <em class="pj">xᵢ</em>)) <br/>where<em class="pj"> yᵢ</em> is the label of any (unbounded) support vector.<br/>This means the final decision boundary is determined only by points with non-zero <em class="pj">αᵢ </em>!</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qw"><img src="../Images/7cb56db83ca8224714cf4cf1d2faf3b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mNRoSXNHIfRlbu8SgwM-0w.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">It turns out the algorithm decides that our original hyperplane is somehow the best, it just need bigger margin by halving all the weights. This makes all points support vectors somehow, but it’s ok since the dataset itself is small. 😅</figcaption></figure><h2 id="9484" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Sequential Minimal Optimization</h2><p id="6cbe" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Remember that we haven’t really shown how to get the optimal Lagrange multipliers (<em class="pj">αᵢ</em>)? The algorithm to solve this is called Sequential Minimal Optimization (SMO). Here’s a simplified view of how we get these values:</p><ol class=""><li id="a28b" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qe qf qg bk">Start with all αᵢ at zero.</li><li id="fc70" class="nc nd fq ne b gt qh ng nh gw qi nj nk nl qj nn no np qk nr ns nt ql nv nw nx qe qf qg bk">Repeatedly select and adjust two αᵢ at a time to improve the solution.</li><li id="8bc3" class="nc nd fq ne b gt qh ng nh gw qi nj nk nl qj nn no np qk nr ns nt ql nv nw nx qe qf qg bk">Update these pairs quickly using simple math.</li><li id="d84c" class="nc nd fq ne b gt qh ng nh gw qi nj nk nl qj nn no np qk nr ns nt ql nv nw nx qe qf qg bk">Ensure all updates follow SVM constraints.</li><li id="3a2b" class="nc nd fq ne b gt qh ng nh gw qi nj nk nl qj nn no np qk nr ns nt ql nv nw nx qe qf qg bk">Repeat until all <em class="pj">αᵢ</em> are “good enough.”</li><li id="db64" class="nc nd fq ne b gt qh ng nh gw qi nj nk nl qj nn no np qk nr ns nt ql nv nw nx qe qf qg bk">Points with αᵢ &gt; 0 become support vectors.</li></ol><p id="8783" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This approach efficiently solves the SVM optimization without heavy computations, making it practical for large datasets.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qx"><img src="../Images/6795f2deb4d5b41da832a23ec3a3cd74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iqw0SFwvTbVhrxM0rE2oWg.png"/></div></div></figure><h2 id="df90" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Decision Function</h2><p id="1cbb" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">After solving the SVM optimization problem using the dual form and obtaining the Lagrange multipliers, we can define the decision function. This function determines how new, unseen data points are classified by the trained SVM model.</p><p id="0a61" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="pj">f</em>(<em class="pj">x</em>) = Σ<em class="pj">ᵢ</em>(<em class="pj">αᵢyᵢ</em>(<em class="pj">xᵢ</em> · <em class="pj">x</em>)) + <em class="pj">b</em></p><p id="eaa8" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here, <em class="pj">αᵢ</em> are the Lagrange multipliers, <em class="pj">y</em>ᵢ are the class labels (+1 or -1), <em class="pj">xᵢ</em> are the support vectors, and <em class="pj">x</em> is the input vector to be classified. The final classification for a new point x is determined by the sign of <em class="pj">f</em>(<em class="pj">x</em>) (either “+” or “-”).</p><p id="381e" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that this decision function uses only the support vectors (data points with non-zero <em class="pj">αᵢ</em>) to classify new inputs, which is the core principle of the SVM algorithm!</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qy"><img src="../Images/7233f435dd2070b0750e1ef60f117705.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IG-z1hfjlex9bUBY5GFcOw.png"/></div></div></figure><h1 id="7358" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">🌟 Support Vector Classifier Code</h1><p id="2e31" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The results above can be obtained using the following code:</p><pre class="mr ms mt mu mv qz ob ra bp rb bb bk"><span id="e46d" class="rc oj fq ob b bg rd re l rf rg">import numpy as np<br/>import pandas as pd<br/>from sklearn.svm import SVC<br/><br/># Create DataFrame<br/>df = pd.DataFrame({<br/>    '🌞': [0, 1, 1, 2, 3, 3, 2, 3, 0, 0, 1, 2, 3],<br/>    '💧': [0, 0, 1, 0, 1, 2, 3, 3, 1, 2, 3, 2, 1],<br/>    'y': [1, -1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1]<br/>}, index=range(1, 14))<br/><br/># Split into train and test<br/>train_df, test_df = df.iloc[:8].copy(), df.iloc[8:].copy()<br/>X_train, y_train = train_df[['🌞', '💧']], train_df['y']<br/>X_test, y_test = test_df[['🌞', '💧']], test_df['y']<br/><br/># Create and fit SVC model<br/>svc = SVC(kernel='linear', C=2)<br/>svc.fit(X_train, y_train)<br/><br/># Add Lagrange multipliers and support vector status<br/>train_df['α'] = 0.0<br/>train_df.loc[svc.support_ + 1, 'α'] = np.abs(svc.dual_coef_[0])<br/>train_df['Is SV'] = train_df.index.isin(svc.support_ + 1)<br/><br/>print("Training Data, Lagrange Multipliers, and Support Vectors:")<br/>print(train_df)<br/><br/># Print model parameters<br/>w, b = svc.coef_[0], svc.intercept_[0]<br/>print(f"\nModel Parameters:")<br/>print(f"Weights (w): [{w[0]}, {w[1]}]")<br/>print(f"Bias (b): {b}")<br/>print(f"Decision function: f(🌞,💧) = ({w[0]})🌞 + ({w[1]})💧 + ({b})")<br/><br/># Make predictions<br/>test_df['ŷ'] = svc.predict(X_test)<br/><br/>print("\nTest Data and Predictions:")<br/>print(test_df)</span></pre><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp rh"><img src="../Images/dd57dff98a95c2298a2b2520aee3f336.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZezKPhyBOOnGFXBjy4JiA.png"/></div></div></figure><h1 id="999f" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Part 2: Kernel Trick</h1><p id="2fc1" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">As we have seen so far, no matter how we set up the hyperplane, we never could make a perfect separation between the two classes. There are actually some “trick” that we can do to make it separable… even though it is not linearly anymore.</p><h2 id="a0b8" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Input Space vs Feature Space</h2><p id="8ab3" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Input Space refers to the original space of your data features. In our golf dataset, the Input Space is two-dimensional, consisting of temperature and humidity. Each data point in this space represents a specific weather condition where someone decided to play golf or not.</p><p id="005a" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Feature Space, on the other hand, is a transformed version of the Input Space where the SVM actually performs the classification. Sometimes, data that isn’t linearly separable in the Input Space becomes separable when mapped to a higher-dimensional Feature Space.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp ri"><img src="../Images/67548459823e251356184cf343374b7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*83QN6DEtrLax8iC0GZN1_Q.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">As we have tried so far, no matter what hyperplane we choose, we couldn’t separate the two classes linearly. Instead of just using 🌞 and 💧, the Feature Space might include combinations like 🌞², 💧², 🌞×💧. This would turn our 2D Input Space into a 5D Feature Space. If you notice, we can find a hyperplane that now can separate the two classes perfectly!</figcaption></figure><h2 id="4333" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Kernel and Implicit Transformation</h2><p id="d799" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">A kernel is a function that computes the similarity between two data points, implicitly representing them in a higher-dimensional space (the feature space).</p><p id="6d9d" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Say, there’s a function <em class="pj">φ</em>(<em class="pj">x</em>) that transforms each input point <em class="pj">x</em> to a higher-dimensional space. For example: <em class="pj">φ</em> : ℝ² → ℝ³, <em class="pj">φ</em>(<em class="pj">x</em>,<em class="pj">y</em>) = (<em class="pj">x</em>, <em class="pj">y</em>, <em class="pj">x</em>² + <em class="pj">y</em>²)</p><p id="7edf" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">Common Kernels and Their Implicit Transformations: </strong><br/><strong class="ne ga">a. Linear Kernel</strong>: <em class="pj">K</em>(<em class="pj">x</em>,<em class="pj">y</em>) = <em class="pj">x </em>· <em class="pj">y</em><br/>- Transformation: <br/><em class="pj">φ</em>(<em class="pj">x</em>) = <em class="pj">x</em> (identity)<br/>- This doesn’t actually change the space but is useful for linearly separable data.</p><p id="a2c0" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">b. Polynomial Kernel</strong>: <em class="pj">K</em>(<em class="pj">x</em>,<em class="pj">y</em>) = (<em class="pj">x </em>· <em class="pj">y</em> + c)<em class="pj">ᵈ</em><br/>- Transformation (for <em class="pj">d </em>= 2, <em class="pj">c </em>= 1 in ℝ²): <br/><em class="pj">φ</em>(<em class="pj">x</em>₁,<em class="pj">x</em>₂) = (1, √2<em class="pj">x</em>₁, √2<em class="pj">x</em>₂, <em class="pj">x</em>₁², √2<em class="pj">x</em>₁<em class="pj">x</em>₂, <em class="pj">x</em>₂²)<br/>- This captures all polynomial terms up to degree <em class="pj">d</em>.</p><p id="6da6" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">c. RBF Kernel</strong>: <em class="pj">K</em>(<em class="pj">x</em>,<em class="pj">y</em>) = exp(-<em class="pj">γ</em>||<em class="pj">x </em>- <em class="pj">y</em>||²)<br/>- Transformation (as an infinite series): <br/><em class="pj">φ</em>(<em class="pj">x</em>₁,<em class="pj">x</em>₂)= exp(-<em class="pj">γ</em>||<em class="pj">x</em>||²) * (1, √(2<em class="pj">γ</em>)<em class="pj">x</em>₁, √(2<em class="pj">γ</em>)<em class="pj">x</em>₂, …, √(2<em class="pj">γ</em>²/2!)<em class="pj">x</em>₁², √(2<em class="pj">γ</em>²/2!)<em class="pj">x</em>₁<em class="pj">x</em>₂, √(2<em class="pj">γ</em>²/2!)<em class="pj">x</em>₂², …, √(2<em class="pj">γⁿ</em>/<em class="pj">n</em>!)<em class="pj">x</em>₁<em class="pj">ⁿ</em>, …)<br/>- Can be thought of as a similarity measure that decreases with distance.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp rj"><img src="../Images/3ec3cc06460d608cd7214914174c67c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VMEEP__KFQvGjbznlWDbKg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">This is to illustrate how kernel would transform the input space. In reality, the computation of each point in this Feature Space itself is not performed as it is expensive to compute, that’s why it is called implicit.</figcaption></figure><h2 id="89c7" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Kernel Trick</h2><p id="4581" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The “trick” part of the kernel trick is that we can perform operations in this higher-dimensional space solely using the kernel function, without ever explicitly computing the transformation <em class="pj">φ</em>(x).</p><p id="ca91" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Notice that in the dual form, the data points only appear as dot products (<em class="pj">xᵢ</em> · <em class="pj">xⱼ</em>). This is where the kernel trick comes in. We can replace this dot product with a kernel function: (<em class="pj">xᵢ </em>· <em class="pj">xⱼ</em>) → <em class="pj">K</em>(<em class="pj">xᵢ</em>, <em class="pj">xⱼ</em>)</p><p id="9b95" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This process cannot be done if we are just using the primal form, that is one of the main reason why the dual form is preferable!</p><p id="3b04" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This substitution implicitly maps the data to a higher-dimensional space <strong class="ne ga">without explicitly computing the transformation</strong>.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qu"><img src="../Images/a948f98c9e82e1974e89f67b3cd2b244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CL-_Lm60EjFcz8UBYgw9mQ.png"/></div></div></figure><h2 id="9b50" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Decision Function with Kernel Trick</h2><p id="240c" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The resulting decision function for a new point <em class="pj">x</em> becomes:</p><p id="a6a8" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="pj">f </em>(<em class="pj">x</em>) = sign(Σ<em class="pj">ᵢ</em> <em class="pj">αᵢyᵢK</em>(<em class="pj">xᵢ</em>, <em class="pj">x</em>) + <em class="pj">b</em>)</p><p id="47ca" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">where the sum is over all support vectors (points with <em class="pj">αᵢ</em> &gt; 0).</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp rk"><img src="../Images/3916df7fe6b42f1589c29aea40e3598a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lymFR55MVdLSjWNEH6-knw.png"/></div></div></figure><h1 id="51e0" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">🌟 Support Vector Classifier (with Kernel Trick) Code Summary</h1><p id="dcdc" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The results above can be obtained using the following code:</p><pre class="mr ms mt mu mv qz ob ra bp rb bb bk"><span id="ecd3" class="rc oj fq ob b bg rd re l rf rg">import numpy as np<br/>import pandas as pd<br/>from sklearn.svm import SVC<br/><br/># Create DataFrame<br/>df = pd.DataFrame({<br/>    '🌞': [0, 1, 1, 2, 3, 3, 2, 3, 0, 0, 1, 2, 3],<br/>    '💧': [0, 0, 1, 0, 1, 2, 3, 3, 1, 2, 3, 2, 1],<br/>    'y': [1, -1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1]<br/>}, index=range(1, 14))<br/><br/># Split into train and test<br/>train_df, test_df = df.iloc[:8].copy(), df.iloc[8:].copy()<br/>X_train, y_train = train_df[['🌞', '💧']], train_df['y']<br/>X_test, y_test = test_df[['🌞', '💧']], test_df['y']<br/><br/># Create and fit SVC model with polynomial kernel<br/>svc = SVC(kernel='poly', degree=2, coef0=1, C=1)<br/>svc.fit(X_train, y_train)<br/><br/># Add Lagrange multipliers and support vector status<br/>train_df['α'] = 0.0<br/>train_df.loc[svc.support_ + 1, 'α'] = np.abs(svc.dual_coef_[0])<br/>train_df['Is SV'] = train_df.index.isin(svc.support_ + 1)<br/><br/>print("Training Data, Lagrange Multipliers, and Support Vectors:")<br/>print(train_df)<br/><br/># Make predictions<br/>test_df['ŷ'] = svc.predict(X_test)<br/>print("\nTest Data and Predictions:")<br/>print(test_df)</span></pre><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp rl"><img src="../Images/87e23140d1dd4151ec7f60baeb4f8ba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qwWhKXAGBGhvAUE6PVbFXg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Note: Due to some numerical instability in SVC, we cannot make the intercept from scikit-learn and the manual calculation to agree… That’s why I didn’t show how to calculate bias manually (even though it should be the same way as the linear kernel).</figcaption></figure><h1 id="1e6c" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Key Parameters</h1><p id="5f76" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">In SVM, the key parameter would be the penalty/regularization parameter C:</p><ul class=""><li id="ddc5" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx rm qf qg bk">Large C: Tries hard to classify all training points correctly, potentially overfitting</li><li id="afc7" class="nc nd fq ne b gt qh ng nh gw qi nj nk nl qj nn no np qk nr ns nt ql nv nw nx rm qf qg bk">Small C: Allows more misclassifications but aims for a simpler, more general model</li></ul><p id="a582" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Of course, if you are using non-linear kernel, you also need to adjust the degree (and coefficients) related to that kernel.</p><h1 id="2694" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Final Remarks</h1><p id="3e67" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">We’ve gone over a lot of the key concepts in SVMs (and how they work), but the main idea is this: It’s all about finding the right balance. You want your SVM to learn the important patterns in your data without trying too hard on getting every single training data on the correct side of the hyperplane. If it’s too strict, it might miss the big picture. If it’s too flexible, it might see patterns that aren’t really there. The trick is to tune your SVM so it can identify the real trends while still being adaptable enough to handle new data. Get this balance right, and you’ve got a powerful tool that can handle all sorts of classification problems.</p></div></div></div><div class="ab cb rn ro rp rq" role="separator"><span class="rr by bm rs rt ru"/><span class="rr by bm rs rt ru"/><span class="rr by bm rs rt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="8be0" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Further Reading</h2><p id="7b20" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">For a detailed explanation of the <a class="af oc" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank">Support Vector Machine</a> and its implementation in scikit-learn, readers can refer to the official documentation, which provides comprehensive information on its usage and parameters.</p><h2 id="a68a" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Technical Environment</h2><p id="431f" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions.</p><h2 id="b65b" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">About the Illustrations</h2><p id="7503" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><h2 id="b3a1" class="pl oj fq bf ok pm pn po on pp pq pr oq nl ps pt pu np pv pw px nt py pz qa fw bk">Reference</h2><p id="88cd" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">[1] T. M. Mitchell, <a class="af oc" href="https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html" rel="noopener ugc nofollow" target="_blank">Machine Learning</a> (1997), McGraw-Hill Science/Engineering/Math, pp. 59</p><p id="d7a1" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘾𝙡𝙖𝙨𝙨𝙞𝙛𝙞𝙘𝙖𝙩𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:</p><div class="rv rw rx ry rz"><div role="button" tabindex="0" class="ab bx cp kj it sa sb bp sc lw ao"><div class="sd l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by se sf cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l se sf em n ay uq"/></div><div class="sg l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----62e831e7b9e9--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sj hp l"><h2 class="bf ga xo ic it xp iv iw xq iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xr wq wr ws wt lj wu wv vb ii ww wx wy vf vg vh ep bm vi oe" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----62e831e7b9e9--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xs l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="ss dz st it ab su il ed"><div class="ed sm bx sn so"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed sm bx kk sp sq"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx sr sq"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div><p id="595f" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:</p><div class="rv rw rx ry rz"><div role="button" tabindex="0" class="ab bx cp kj it sa sb bp sc lw ao"><div class="sd l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by se sf cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l se sf em n ay uq"/></div><div class="sg l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----62e831e7b9e9--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sj hp l"><h2 class="bf ga xo ic it xp iv iw xq iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xr wq wr ws wt lj wu wv vb ii ww wx wy vf vg vh ep bm vi oe" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----62e831e7b9e9--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xs l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="ss dz st it ab su il ed"><div class="ed sm bx sn so"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This “dummy” doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed sm bx kk sp sq"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx sr sq"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div><div class="rv rw rx ry rz"><div role="button" tabindex="0" class="ab bx cp kj it sa sb bp sc lw ao"><div class="sd l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by se sf cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l se sf em n ay uq"/></div><div class="sg l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----62e831e7b9e9--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sj hp l"><h2 class="bf ga xo ic it xp iv iw xq iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xr wq wr ws wt lj wu wv vb ii ww wx wy vf vg vh ep bm vi oe" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----62e831e7b9e9--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xs l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="ss dz st it ab su il ed"><div class="ed sm bx sn so"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed sm bx kk sp sq"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx sr sq"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>