<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>SageMaker vs Vertex AI for Model Inference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>SageMaker vs Vertex AI for Model Inference</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sagemaker-vs-vertex-ai-for-model-inference-ef0d503cee76?source=collection_archive---------2-----------------------#2024-06-06">https://towardsdatascience.com/sagemaker-vs-vertex-ai-for-model-inference-ef0d503cee76?source=collection_archive---------2-----------------------#2024-06-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="2876" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Comparing the AWS and GCP fully-managed services for ML workflows</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@turc.raluca?source=post_page---byline--ef0d503cee76--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Julia Turc" class="l ep by dd de cx" src="../Images/1ca27d7db36799dec53b8daf4099f5cb.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*kY2xovOt_cQKjc9h9dYeRQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ef0d503cee76--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@turc.raluca?source=post_page---byline--ef0d503cee76--------------------------------" rel="noopener follow">Julia Turc</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ef0d503cee76--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="abab" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If you’re in that exciting stage of product development where you’re looking to deploy your first AI models to production, then take a moment to enjoy this clean slate. The decisions you’re about to make might influence the future of your company, or at least its technical debt going forward. No pressure :) Or at least that’s what I tell myself, now that I am starting to lay down the technical foundations of our company.</p><p id="f7f7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">At <a class="af ne" href="https://storia.ai/" rel="noopener ugc nofollow" target="_blank">Storia</a>, we build and deploy a lot of AI models, so efficient model serving is top of mind. We did a deep dive into two of the most prominent services, <a class="af ne" href="https://aws.amazon.com/pm/sagemaker/" rel="noopener ugc nofollow" target="_blank">SageMaker</a> and <a class="af ne" href="https://cloud.google.com/vertex-ai?hl=en" rel="noopener ugc nofollow" target="_blank">Vertex AI</a>, and are sharing our takeaways here. We liked SageMaker better for our use case. While we tried to stay impartial for the sake of making the best decision for our company, who knows what sort of biases are creeping in. I spent many years at Google, my cofounder at Amazon. Both are offering us free credits through their startup programs and Amazon welcomed us into their <a class="af ne" href="https://aws-startup-lofts.com/amer/program/accelerators/generative-ai" rel="noopener ugc nofollow" target="_blank">generative AI accelerator</a> last year.</p><blockquote class="nf"><p id="67ac" class="ng nh fq bf ni nj nk nl nm nn no nd dx">TL;DR: SageMaker wins overall. If you’re starting from scratch and have no affinity for one cloud provider over another (because of free credits, existing lock-in, or strong familiarity with their tooling), just go for SageMaker. However, if GCP already has you enthralled, stay there: Vertex AI is putting up a good enough fight.</p></blockquote></div></div><div class="np"><div class="ab cb"><div class="ll nq lm nr ln ns cf nt cg nu ci bh"><div class="nv nw nx ny nz ab ke"><figure class="le np oa ob oc od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><img src="../Images/3e1eee4fed051c6b3ae2fa1569a74d04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*pQenS6-sbJHaaCUM"/></div></figure><figure class="le np ok ob oc od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><img src="../Images/239206169cd8817b3efb60141b58dddc.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/0*PwcvTp_yQVYs5NFl"/></div><figcaption class="ol om on oo op oq or bf b bg z dx os ed ot ou">Photos from <a class="af ne" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a> (left: <a class="af ne" href="https://unsplash.com/@christianw?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Christian Wiediger</a>, right: <a class="af ne" href="https://unsplash.com/@kai_wenzel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Kai Wenzel</a>)</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="f3b5" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">What are SageMaker and Vertex AI?</h1><p id="e89f" class="pw-post-body-paragraph mi mj fq mk b go pr mm mn gr ps mp mq mr pt mt mu mv pu mx my mz pv nb nc nd fj bk"><a class="af ne" href="https://aws.amazon.com/pm/sagemaker/" rel="noopener ugc nofollow" target="_blank">SageMaker</a> and <a class="af ne" href="https://cloud.google.com/vertex-ai?hl=en" rel="noopener ugc nofollow" target="_blank">Vertex AI</a> are two competing services from AWS and GCP for training and serving machine learning models. They wrap around cloud primitives (virtual machines, accelerators and storage) to streamline the process of building and deploying ML models. Their goal is to prevent developers from manually and repeatedly setting up operations that are common across most ML workflows.</p><p id="54d6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For instance, building a training pipeline requires a few universal steps: placing the training data in a storage system, bringing up one or more accelerator-enabled virtual machines, ensuring they are not bottlenecked by I/O (i.e. more time is spent propagating gradients than reading training data), checkpointing and evaluating regularly, etc.</p><p id="e0c4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">SageMaker and Vertex AI enable developers to set up such involved workflows with a mere configuration file or a few bash commands. The result is a self-healing system that accomplishes the task without much monitoring needed. This is why they are often referred to as <em class="pw">fully managed services.</em></p><h2 id="fa73" class="px ow fq bf ox py pz qa pa qb qc qd pd mr qe qf qg mv qh qi qj mz qk ql qm qn bk">SageMaker and Vertex AI for model inference</h2><p id="6b1d" class="pw-post-body-paragraph mi mj fq mk b go pr mm mn gr ps mp mq mr pt mt mu mv pu mx my mz pv nb nc nd fj bk">In this article, we compare SageMaker and Vertex AI from the lens of <em class="pw">model inference </em>in particular.<em class="pw"> </em>Here, their main value proposition is to ensure that (a) the inference server is always up and running, and (b) it <em class="pw">autoscales </em>based on incoming traffic. The latter is particularly relevant in today’s era of large models that require powerful accelerators. Since GPUs are scarce and expensive, we cannot afford to have them sit idle, so we need to bring them up and down based on the amount of traffic.</p><p id="a719" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">While we focus on inference in this article, it’s worth acknowledging these services encompass many other parts of the workflow. Notably, in addition to support for model training, they both include notebook-centric offerings for data scientists to analyze the training data (see <a class="af ne" href="https://aws.amazon.com/sagemaker/notebooks/" rel="noopener ugc nofollow" target="_blank">SageMaker Notebooks</a> and <a class="af ne" href="https://cloud.google.com/vertex-ai-notebooks?hl=en" rel="noopener ugc nofollow" target="_blank">Vertex AI Notebooks</a>).</p><h2 id="5fcb" class="px ow fq bf ox py pz qa pa qb qc qd pd mr qe qf qg mv qh qi qj mz qk ql qm qn bk">Developer workflow</h2><p id="fe55" class="pw-post-body-paragraph mi mj fq mk b go pr mm mn gr ps mp mq mr pt mt mu mv pu mx my mz pv nb nc nd fj bk">When using SageMaker or VertexAI for model deployment, developers are expected to perform the following three steps:</p><ol class=""><li id="657c" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd qo qp qq bk">Create a model.</li><li id="f2c4" class="mi mj fq mk b go qr mm mn gr qs mp mq mr qt mt mu mv qu mx my mz qv nb nc nd qo qp qq bk">Configure an endpoint.</li><li id="6ba5" class="mi mj fq mk b go qr mm mn gr qs mp mq mr qt mt mu mv qu mx my mz qv nb nc nd qo qp qq bk">Deploy the model to the endpoint.</li></ol><p id="38a0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">These operations can be performed via the web interface, cloud-specific CLIs, or cloud-specific SDKs for various programming languages.</p><h2 id="e8b4" class="px ow fq bf ox py pz qa pa qb qc qd pd mr qe qf qg mv qh qi qj mz qk ql qm qn bk">Creating a model</h2><p id="ea92" class="pw-post-body-paragraph mi mj fq mk b go pr mm mn gr ps mp mq mr pt mt mu mv pu mx my mz pv nb nc nd fj bk">Creating a model boils down to supplying a <a class="af ne" href="https://www.docker.com/" rel="noopener ugc nofollow" target="_blank">Docker</a> image for an HTTP server that responds to requests for (1) loading model artifacts in memory, (2) making predictions and (3) health checks. Beyond this contract, SageMaker and Vertex AI are relatively unopinionated about what they serve, treating models as black boxes that need to be kept up and running while responding to prediction requests.</p><p id="a279" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Both SageMaker and Vertex AI offer <em class="pw">prebuilt images </em>for various ML frameworks (PyTorch, Tensorflow, Scikit-learn, etc.) and built-in algorithms. For instance, if you simply want to run text-to-image generation with SDXL 1.0, you can grab the image from <a class="af ne" href="https://aws.amazon.com/marketplace/pp/prodview-pe7wqwehghdtm?sr=0-3&amp;ref_=beagle&amp;applicationId=AWSMPContessa" rel="noopener ugc nofollow" target="_blank">Amazon’s Marketplace</a> or from <a class="af ne" href="https://console.cloud.google.com/vertex-ai/publishers/stability-ai/model-garden/stable-diffusion-xl-base?rapt=AEjHL4MQfisT9c5bHE515GsX5Ytk7xRpC6SOyHvaxy3NpApsXgU1UjiMuTlQX8StsIi1PG0K8kSbOXSHJNg-j6baE4RUpxhPi4Asmv2W-3Nao5_FKiafEsg&amp;project=textify-988c3" rel="noopener ugc nofollow" target="_blank">Google Cloud’s Model Garden</a>. Alternatively, they also both support <em class="pw">custom images</em> that allow developers to write their own serving logic and define their own runtime environment, as long as the container exposes an HTTP server with the three endpoints mentioned above.</p><h2 id="d2fd" class="px ow fq bf ox py pz qa pa qb qc qd pd mr qe qf qg mv qh qi qj mz qk ql qm qn bk">Configuring an endpoint</h2><p id="f9a7" class="pw-post-body-paragraph mi mj fq mk b go pr mm mn gr ps mp mq mr pt mt mu mv pu mx my mz pv nb nc nd fj bk">An endpoint configuration associates a model with a set of runtime constraints: machine and accelerator type to run on, minimum and maximum amount of resources to be consumed, and how to handle autoscaling (what metric to monitor and above what threshold to trigger).</p><h2 id="3df8" class="px ow fq bf ox py pz qa pa qb qc qd pd mr qe qf qg mv qh qi qj mz qk ql qm qn bk">Deploying a model</h2><p id="c4f7" class="pw-post-body-paragraph mi mj fq mk b go pr mm mn gr ps mp mq mr pt mt mu mv pu mx my mz pv nb nc nd fj bk">Once these configurations are in place, the developer gives the final green light. SageMaker and Vertex AI then provision the necessary machines, run the container, and call the initial model load method exposed by the inference server. Then, throughout the lifetime of the container, they make regular health checks and restart the container when necessary. Based on traffic, they scale up and down in an attempt to minimize resource consumption and maximize throughput.</p><h1 id="9ac1" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">How do SageMaker and Vertex AI compare?</h1><blockquote class="nf"><p id="014e" class="ng nh fq bf ni nj nk nl nm nn no nd dx">Verdict: SageMaker wins overall. If you’re starting from scratch and have no affinity for one cloud provider over the other (free credits, existing lock-in, or strong familiarity with their tooling), just go for SageMaker. However, if GCP already has you enthralled, stay there: Vertex AI is putting up a good enough fight.</p></blockquote><p id="40e4" class="pw-post-body-paragraph mi mj fq mk b go qw mm mn gr qx mp mq mr qy mt mu mv qz mx my mz ra nb nc nd fj bk">Many times, the answer to this kind of question is <em class="pw">“It depends”</em>. But this is not one of those times. At least in the context of model serving, SageMaker wins by far on most dimensions. Compared to Vertex AI, SageMaker is generally more feature-rich and flexible, without losing sight of its original goal of making ML workflows <em class="pw">easy</em>. This, coupled with AWS’s general customer obsession (which translates into faster customer support and more free credits for startups) makes SageMaker a better choice overall.</p><p id="21ac" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">That being said, Vertex AI can be good enough if your use case is not very sophisticated. If you have a good enough reason to prefer GCP (perhaps you’re already locked in, or have more free credits there), Vertex AI might work for you just fine.</p><h1 id="1bb1" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Autoscaling</h1><blockquote class="nf"><p id="5f83" class="ng nh fq bf ni nj nk nl nm nn no nd dx">SageMaker offers more flexibility when configuring autoscaling. In contrast to Vertex AI, it can scale based on QPS instead of resource usage.</p></blockquote><p id="96f0" class="pw-post-body-paragraph mi mj fq mk b go qw mm mn gr qx mp mq mr qy mt mu mv qz mx my mz ra nb nc nd fj bk">In the context of model inference, autoscaling is one of the main value propositions of fully managed services like SageMaker and Vertex AI. When your traffic increases, they provision extra machines. When it decreases, they remove unnecessary instances. This is particularly important in today’s world, where most models run on accelerators that are too expensive to be kept idle. However, adjusting the allocated resources based on traffic is a non-trivial task.</p><h2 id="1fd7" class="px ow fq bf ox py pz qa pa qb qc qd pd mr qe qf qg mv qh qi qj mz qk ql qm qn bk"><strong class="al">Why is autoscaling difficult?</strong></h2><p id="832e" class="pw-post-body-paragraph mi mj fq mk b go pr mm mn gr ps mp mq mr pt mt mu mv pu mx my mz pv nb nc nd fj bk">A big hinderance is that scaling up is not<em class="pw"> </em>instantaneous. When an extra GPU is needed, the system will provision a new VM, download the Docker image, start the container, and download model artifacts. This can take anywhere between 3 to 8 minutes, depending on the specifics of your deployment. Since it can’t react to fluctuations in traffic quickly enough, the system needs to predict traffic spikes ahead of time by leveraging past information.</p><h2 id="a85f" class="px ow fq bf ox py pz qa pa qb qc qd pd mr qe qf qg mv qh qi qj mz qk ql qm qn bk"><strong class="al">How SageMaker wins the autoscaling game</strong></h2><p id="58a8" class="pw-post-body-paragraph mi mj fq mk b go pr mm mn gr ps mp mq mr pt mt mu mv pu mx my mz pv nb nc nd fj bk">SageMaker offers three types of autoscaling (see <a class="af ne" href="https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html" rel="noopener ugc nofollow" target="_blank">documentation</a>): (1) <em class="pw">target tracking </em>(tracks a designated metric — like CPU usage — and scales up when a predefined threshold is exceeded), (2) <em class="pw">step scaling </em>(supports more complex logic based on multiple tracked metrics) and (3) <em class="pw">scheduled scaling </em>(allows you to hard-code specific times when you expect a traffic increase).</p><p id="bcf2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The recommended method is <em class="pw">target tracking</em>: you pick any metric from <a class="af ne" href="https://aws.amazon.com/pm/cloudwatch/" rel="noopener ugc nofollow" target="_blank">Amazon CloudWatch</a> (or even define a custom one!) and the value that should trigger scaling. There are metrics that reflect resource utilization (e.g. CPU / GPU memory or cycles) and also metrics that measure traffic (e.g. <code class="cx rb rc rd re b">InvocationsPerInstance</code> or <code class="cx rb rc rd re b">ApproximateBacklogSizePerInstance</code>).</p><p id="d4b4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In contrast, Vertex AI provides a lot less control (see <a class="af ne" href="https://cloud.google.com/vertex-ai/docs/general/deployment#scaling" rel="noopener ugc nofollow" target="_blank">documentation</a>). The only option is target tracking, restricted to <a class="af ne" href="https://cloud.google.com/vertex-ai/docs/reference/rest/v1/DedicatedResources#autoscalingmetricspec" rel="noopener ugc nofollow" target="_blank">two metrics</a>: CPU utilization and GPU duty cycles. Note that there is no metric that directly reflects traffic. This is very inconvenient when your model cannot serve multiple requests concurrently (i.e., neither batching nor multi-threading is possible). Without this ability, your CPU or GPU is operating in one of two modes: either <em class="pw">0% utilization </em>(no requests), or a fixed <em class="pw">x% utilization </em>(one or more requests). In this binary reality, CPU or GPU usage does not reflect the true load and is not a good trigger for scaling. Your only option is to scale up whenever utilization is somewhere between <em class="pw">0%</em> and <em class="pw">x%</em>, with the added complexity that <em class="pw">x </em>is accelerator-dependent: if you switch from an NVIDIA T4 to an A100, you’ll have to manually lower the threshold.</p><p id="84b4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For some extra drama, Vertex AI cannot scale down to zero (<a class="af ne" href="https://issuetracker.google.com/issues/206042974" rel="noopener ugc nofollow" target="_blank">see issue</a>); at least one machine must keep running. However, SageMaker allows completely removing all instances for their <a class="af ne" href="https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html" rel="noopener ugc nofollow" target="_blank">asynchronous endpoints</a> (more on this in the next section).</p><p id="41c9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Perhaps the only saving grace for GCP is that it allows you to easily track the autoscaling behavior on their web console, whereas AWS provides no information whatsoever on their web portal (and you’ll have to resort to a bash command in a for loop to monitor it).</p><h1 id="7eb1" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Synchronous vs asynchronous predictions</h1><blockquote class="nf"><p id="34bc" class="ng nh fq bf ni nj nk nl nm nn no nd dx">SageMaker supports both synchronous calls (which block waiting until the prediction is complete) and asynchronous calls (which immediately return a URL that will hold the output once ready). Vertex AI solely supports the former.</p></blockquote><p id="0406" class="pw-post-body-paragraph mi mj fq mk b go qw mm mn gr qx mp mq mr qy mt mu mv qz mx my mz ra nb nc nd fj bk">By default, SageMaker and Vertex AI endpoints are <em class="pw">synchronous — </em>the caller is blocked waiting until the prediction is complete. While this is the easiest client/server communication model to wrap your head around, it can be inconvenient when the model has high latency. Both services will simply timeout after 60 seconds: if a single model call takes longer than that, SageMaker / Vertex AI will simply return a <em class="pw">timeout </em>response. Note that this includes wait times as well. Say that the client issues two requests simultaneously, and each request takes 45 seconds to resolve. If your model doesn’t support parallelism (e.g. via batching), then the second request will timeout (since it would need 90 seconds to get resolved).</p><p id="95f0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To work around this issue, SageMaker supports <a class="af ne" href="https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html" rel="noopener ugc nofollow" target="_blank"><em class="pw">asynchronous endpoints</em></a> — they immediately respond to the client with an S3 URL; the model output will be placed there when completed. It is up to the client to poll the S3 location until available. Since requests are placed in a (best-effort) FIFO queue, the time out is extended to 15 minutes (as opposed to 60 seconds). Unfortunately, Vertex AI does not support asynchronous endpoints; you would have to implement your own queuing and retry logic if you don’t want your requests to simply be dropped after 60 seconds.</p><p id="d449" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Note that both SageMaker and Vertex AI support <em class="pw">batch predictions</em>, which are asynchronous. These are not suitable for live traffic, but rather batch jobs (i.e., running offline predictions over an entire dataset).</p><h1 id="9a99" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Multi-model endpoints (MMEs)</h1><blockquote class="nf"><p id="068f" class="ng nh fq bf ni nj nk nl nm nn no nd dx">SageMaker fully supports multi-model endpoints that share resources among models. Vertex AI’s multi-model endpoints solely share the URL, and don’t translate to any cost savings.</p></blockquote><p id="91da" class="pw-post-body-paragraph mi mj fq mk b go qw mm mn gr qx mp mq mr qy mt mu mv qz mx my mz ra nb nc nd fj bk">Sometimes you want to deploy more than just one model. Maybe you have an entire pipeline where each step requires a different model, like for <a class="af ne" href="https://pytorch.org/blog/amazon-sagemaker-w-torchserve/" rel="noopener ugc nofollow" target="_blank">language-guided image editing</a>. Or maybe you have a collection of independent models with a power law usage (2–3 of them are used frequently, and the long tail only occasionally). Allocating a dedicated machine to each model can get prohibitively expensive. To this end, SageMaker offers <a class="af ne" href="https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html" rel="noopener ugc nofollow" target="_blank"><em class="pw">multi-model endpoints</em></a>, which share the same container and resources among your models. They don’t need to all fit into memory; SageMaker can swap them in and out on demand based on which one is currently requested. The trade-off is the occasional cold start (i.e. if the requested model is not already in memory, your client will have to wait until SageMaker swaps it in). This is tolerable when you have a long tail of rarely used models.</p><p id="ea8f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">One constraint of SageMaker multi-model endpoints is that they require all models to use the same framework (PyTorch, Tensorflow etc.). However, <a class="af ne" href="https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html" rel="noopener ugc nofollow" target="_blank">multi-container endpoints</a> alleviate this restriction.</p><p id="c828" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">While Vertex AI officially allows you to deploy multiple models to an endpoint (see <a class="af ne" href="https://cloud.google.com/vertex-ai/docs/general/deployment#models-endpoint" rel="noopener ugc nofollow" target="_blank">documentation</a>), resources are actually associated with the model, not the endpoint. You don’t get the same advantage of sharing resources and reducing costs, but the mere convenience of being able to gradually transition traffic from a model v1 to a model v2 without changing the endpoint URL. Actually sharing resources is only possible for Tensorflow models that use a pre-built container, which is quite restrictive (see <a class="af ne" href="https://cloud.google.com/vertex-ai/docs/predictions/model-co-hosting" rel="noopener ugc nofollow" target="_blank">documentation</a>).</p><h1 id="f83b" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Quotas and GPU availability</h1><blockquote class="nf"><p id="cb1b" class="ng nh fq bf ni nj nk nl nm nn no nd dx">When it comes to quota and accelerator availability, both providers have their own quirks, which are out-shadowed by the same fundamental challenge: GPUs are expensive.</p></blockquote><ul class=""><li id="1784" class="mi mj fq mk b go qw mm mn gr qx mp mq mr qy mt mu mv qz mx my mz ra nb nc nd rf qp qq bk">With GCP, you can get access to (and pay for) a single A100 GPU. However, AWS forces you to rent 8 at a time (which, depending on your needs, might be an overkill). This situation is particular to A100s and doesn’t apply to lower-tier GPUs; you’re free to request a single GPU of any other type on AWS.</li><li id="07f7" class="mi mj fq mk b go qr mm mn gr qs mp mq mr qt mt mu mv qu mx my mz qv nb nc nd rf qp qq bk">Within GCP, quotas for VMs can be reused for Vertex AI. In other words, you only have to ask for that hot A100 once. However, AWS manages EC2 and SageMaker quotas separately (read more about <a class="af ne" href="https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html" rel="noopener ugc nofollow" target="_blank">AWS service quotas</a>), so make sure to request the quota for the right service.</li><li id="a537" class="mi mj fq mk b go qr mm mn gr qs mp mq mr qt mt mu mv qu mx my mz qv nb nc nd rf qp qq bk">While we have dedicated customer support from both providers (GCP via their <a class="af ne" href="https://cloud.google.com/startup/?hl=en" rel="noopener ugc nofollow" target="_blank">startup program</a> and AWS via their <a class="af ne" href="https://aws-startup-lofts.com/amer/program/accelerators/generative-ai" rel="noopener ugc nofollow" target="_blank">generative AI accelerator</a>), the AWS representatives are generally much more responsive, which also translates to quota requests being resolved quicker.</li></ul><h1 id="e42b" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Limitations</h1><p id="66b5" class="pw-post-body-paragraph mi mj fq mk b go pr mm mn gr ps mp mq mr pt mt mu mv pu mx my mz pv nb nc nd fj bk">In the previous sections, we discussed what limitations the two services have with respect to each other. There are, however, limitations that are common among the two:</p><ol class=""><li id="210c" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd qo qp qq bk"><strong class="mk fr">Payload restrictions</strong>. The model response payload has a maximum size for both services: 1.5 MB for public Vertex AI endpoints, 6 MB for synchronous SageMaker endpoints, and 1 GB for asynchronous endpoints (<a class="af ne" href="https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions" rel="noopener ugc nofollow" target="_blank">source 1</a>, <a class="af ne" href="https://docs.aws.amazon.com/sagemaker/latest/dg/hosting-faqs.html" rel="noopener ugc nofollow" target="_blank">source 2</a>).</li><li id="28e2" class="mi mj fq mk b go qr mm mn gr qs mp mq mr qt mt mu mv qu mx my mz qv nb nc nd qo qp qq bk"><strong class="mk fr">Timeouts</strong>. Prediction requests will be eventually dropped by both services: 60 seconds for Vertex AI and synchronous SageMaker endpoints, 15 minutes for asynchronous SageMaker endpoints (<a class="af ne" href="https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions" rel="noopener ugc nofollow" target="_blank">source 1</a>, <a class="af ne" href="https://docs.aws.amazon.com/sagemaker/latest/dg/hosting-faqs.html" rel="noopener ugc nofollow" target="_blank">source 2</a>).</li><li id="4642" class="mi mj fq mk b go qr mm mn gr qs mp mq mr qt mt mu mv qu mx my mz qv nb nc nd qo qp qq bk"><strong class="mk fr">Scaling down to 0</strong>. This is not supported by Vertex AI and synchronous SageMaker endpoints, but it is possible with SageMaker asynchronous endpoints.</li><li id="288a" class="mi mj fq mk b go qr mm mn gr qs mp mq mr qt mt mu mv qu mx my mz qv nb nc nd qo qp qq bk"><strong class="mk fr">Attaching a shared file system</strong>. Neither SageMaker nor Vertex AI allow mounting an external file storage system (<a class="af ne" href="https://aws.amazon.com/efs/" rel="noopener ugc nofollow" target="_blank">EFS</a> or <a class="af ne" href="https://aws.amazon.com/fsx/" rel="noopener ugc nofollow" target="_blank">FSx</a> in AWS and <a class="af ne" href="https://cloud.google.com/filestore?hl=en" rel="noopener ugc nofollow" target="_blank">Filestore</a> in GCP). This could be convenient to store and share model artifacts across server replicas or implement tricks <a class="af ne" href="https://cloud.google.com/blog/products/containers-kubernetes/stable-diffusion-containers-on-gke" rel="noopener ugc nofollow" target="_blank">like this one</a> for saving space in your Docker image (and reducing launch time). Note that they do support access to regular object storage (S3 and GCS).</li></ol><h1 id="df9d" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Summary</h1><p id="0ff2" class="pw-post-body-paragraph mi mj fq mk b go pr mm mn gr ps mp mq mr pt mt mu mv pu mx my mz pv nb nc nd fj bk">A lot has been said, so here is a neat table that compresses it all:</p><figure class="rh ri rj rk rl np oo op paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="oo op rg"><img src="../Images/5f43e208910b9b59dc8d0385fc44a7ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YGvgvAYp9JGT6elZJay5cw.png"/></div></div><figcaption class="ol om on oo op oq or bf b bg z dx">Image by author. ✅ = supported, ❌ = not supported, ⚠️ = limited support.</figcaption></figure><h1 id="3847" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Alternatives</h1><p id="f185" class="pw-post-body-paragraph mi mj fq mk b go pr mm mn gr ps mp mq mr pt mt mu mv pu mx my mz pv nb nc nd fj bk">SageMaker and Vertex are the most popular solutions for model serving and can satisfy most use cases. If you’re not happy with either, then you’ll have to do a little introspection. Do you want more flexibility? Do you want simplicity at the cost of even less flexibility? Or perhaps you just want to reduce cost at the expense of cold starts?</p><p id="7742" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If flexibility is what you’re craving, then there’s probably no way of avoiding <a class="af ne" href="https://kubernetes.io/" rel="noopener ugc nofollow" target="_blank">Kubernetes</a> — Amazon’s <a class="af ne" href="https://aws.amazon.com/eks/" rel="noopener ugc nofollow" target="_blank">EKS</a> and Google’s <a class="af ne" href="https://cloud.google.com/kubernetes-engine?hl=en" rel="noopener ugc nofollow" target="_blank">GKE</a> are managed Kubernetes services that might be a good start. The additional advantage is that Kubernetes is cloud-agnostic, so you can reuse the same configuration on AWS / GCP / Azure with an infrastructure automation tool like <a class="af ne" href="https://www.terraform.io/" rel="noopener ugc nofollow" target="_blank">Terraform</a>.</p><p id="e81f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In contrast, if you’re aiming for simplicity, there are services like <a class="af ne" href="https://replicate.com/" rel="noopener ugc nofollow" target="_blank">Replicate</a>, <a class="af ne" href="https://www.baseten.co/" rel="noopener ugc nofollow" target="_blank">Baseten</a>, <a class="af ne" href="https://modal.com/" rel="noopener ugc nofollow" target="_blank">Modal</a> or <a class="af ne" href="https://www.mystic.ai/" rel="noopener ugc nofollow" target="_blank">Mystic</a> that are one level of abstraction above SageMaker and Vertex. They come with different trade-offs; for instance, Replicate makes it extremely easy to bring up model endpoints during the experimentation phase, but struggle with significant cold starts.</p><h1 id="88d8" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Contact</h1><p id="9f25" class="pw-post-body-paragraph mi mj fq mk b go pr mm mn gr ps mp mq mr pt mt mu mv pu mx my mz pv nb nc nd fj bk"><em class="pw">If you’re thinking of efficient model serving, we want to hear from you! You can find me on Twitter </em><a class="af ne" href="https://x.com/juliarturc" rel="noopener ugc nofollow" target="_blank"><em class="pw">@juliarturc</em></a><em class="pw"> or </em><a class="af ne" href="https://www.linkedin.com/in/iulia-raluca-turc/" rel="noopener ugc nofollow" target="_blank"><em class="pw">LinkedIn</em></a><em class="pw">.</em></p><h1 id="d9cc" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Further reading</h1><ul class=""><li id="609e" class="mi mj fq mk b go pr mm mn gr ps mp mq mr pt mt mu mv pu mx my mz pv nb nc nd rf qp qq bk"><a class="af ne" href="https://docs.aws.amazon.com/sagemaker/" rel="noopener ugc nofollow" target="_blank">Official SageMaker documentation</a></li><li id="5dcc" class="mi mj fq mk b go qr mm mn gr qs mp mq mr qt mt mu mv qu mx my mz qv nb nc nd rf qp qq bk"><a class="af ne" href="https://cloud.google.com/vertex-ai/docs/predictions/overview" rel="noopener ugc nofollow" target="_blank">Official Vertex AI documentation</a></li><li id="10b2" class="mi mj fq mk b go qr mm mn gr qs mp mq mr qt mt mu mv qu mx my mz qv nb nc nd rf qp qq bk"><a class="af ne" href="https://pytorch.org/blog/amazon-sagemaker-w-torchserve/" rel="noopener ugc nofollow" target="_blank">Accelerate AI models on GPU using Amazon SageMaker multi-model endpoints with TorchServe, saving up to 75% on inference costs</a></li><li id="53a8" class="mi mj fq mk b go qr mm mn gr qs mp mq mr qt mt mu mv qu mx my mz qv nb nc nd rf qp qq bk"><a class="af ne" href="https://medium.com/google-cloud/serving-machine-learning-models-with-google-vertex-ai-5d9644ededa3" rel="noopener">Serving Machine Learning models with Google Vertex AI</a></li><li id="d723" class="mi mj fq mk b go qr mm mn gr qs mp mq mr qt mt mu mv qu mx my mz qv nb nc nd rf qp qq bk"><a class="af ne" href="https://cloud.google.com/blog/products/containers-kubernetes/stable-diffusion-containers-on-gke" rel="noopener ugc nofollow" target="_blank">Improving launch time of Stable Diffusion on Google Kubernetes Engine (GKE) by 4X</a></li></ul></div></div></div></div>    
</body>
</html>