["```py\nfrom metaflow import FlowSpec, Parameter, step, current, batch, S3, environment\n\nclass main_flow(FlowSpec):\n    @step\n    def start(self):\n        \"\"\"\n        Start-up: check everything works or fail fast!\n        \"\"\"\n\n        self.next(self.augment_data_train_model)\n\n    @batch(gpu=1, memory=8192, image='docker.io/tensorflow/tensorflow:latest-gpu', queue=\"job-queue-gpu-metaflow\")\n    @step\n    def augment_data_train_model(self):\n        \"\"\"\n        Code to pull data from S3, augment it, and train our model.\n        \"\"\"\n\n        self.next(self.evaluate_model)\n\n    @step\n    def evaluate_model(self):\n        \"\"\"\n        Code to evaluate our detection model, using Weights & Biases.\n        \"\"\"\n\n        self.next(self.deploy)\n\n    @step\n    def deploy(self):\n        \"\"\"\n        Code to deploy our detection model to a Sagemaker endpoint\n        \"\"\"\n\n        self.next(self.end)\n\n    @step\n    def end(self):\n        \"\"\"\n        The final step!\n        \"\"\"\n\n        print(\"All done. \\n\\n Congratulations! Plants around the world will thank you. \\n\")\n        return\n\nif __name__ == '__main__':\n    main_flow()\n```", "```py\n@pip(libraries={'tensorflow': '2.15', 'keras-cv': '0.9.0', 'pycocotools': '2.0.7', 'wandb': '0.17.3'})\n@batch(gpu=1, memory=8192, image='docker.io/tensorflow/tensorflow:latest-gpu', queue=\"job-queue-gpu-metaflow\")\n@environment(vars={\n    \"S3_BUCKET_ADDRESS\": os.getenv('S3_BUCKET_ADDRESS'),\n    'WANDB_API_KEY': os.getenv('WANDB_API_KEY'),\n    'WANDB_PROJECT': os.getenv('WANDB_PROJECT'),\n    'WANDB_ENTITY': os.getenv('WANDB_ENTITY')})\n@step\ndef augment_data_train_model(self):\n  \"\"\"\n  Code to pull data from S3, augment it, and train our model.\n  \"\"\"\n```", "```py\npinecone==4.0.0\nlangchain==0.2.7\npython-dotenv==1.0.1\npandas==2.2.2\nstreamlit==1.36.0\niso-639==0.4.5\nprefect==2.19.7\nlangchain-community==0.2.7\nlangchain-openai==0.1.14\nlangchain-pinecone==0.1.1\n```", "```py\n@pypi(libraries={'tensorflow': '2.15', 'keras-cv': '0.9.0', 'pycocotools': '2.0.7', 'wandb': '0.17.3'})\n@batch(gpu=1, memory=8192, image='docker.io/tensorflow/tensorflow:latest-gpu', queue=\"job-queue-gpu-metaflow\")\n@environment(vars={\n    \"S3_BUCKET_ADDRESS\": os.getenv('S3_BUCKET_ADDRESS'),\n    'WANDB_API_KEY': os.getenv('WANDB_API_KEY'),\n    'WANDB_PROJECT': os.getenv('WANDB_PROJECT'),\n    'WANDB_ENTITY': os.getenv('WANDB_ENTITY')})\n@step\ndef augment_data_train_model(self):\n  \"\"\"\n  Code to pull data from S3, augment it, and train our model.\n  \"\"\"\n```", "```py\nimage = example[\"images\"]\nself.image = tf.expand_dims(image, axis=0)  # Shape: (1, 416, 416, 3)\n\ny_pred = model.predict(self.image)\n\nconfidence = y_pred['confidence'][0]\nself.confidence = [conf for conf in confidence if conf != -1]\n\nself.y_pred = bounding_box.to_ragged(y_pred)\n```", "```py\nimport matplotlib.pyplot as plt\n\nlatest_run = Flow('main_flow').latest_run\nstep = latest_run['evaluate_model']\nsample_image = step.task.data.image\nsample_image = sample_image[0,:, :, :]\n\none_image_normalized = sample_image / 255\n\n# Display the image using matplotlib\nplt.imshow(one_image_normalized)\nplt.axis('off')  # Hide the axes\nplt.show()\n```", "```py\nlatest_run = Flow('main_flow').latest_run\nstep = latest_run['evaluate_model']\ny_pred = step.task.data.y_pred\nprint(y_pred)\n```"]