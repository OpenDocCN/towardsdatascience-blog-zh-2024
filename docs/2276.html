<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Mystery Behind the PyTorch Automatic Mixed Precision Library</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Mystery Behind the PyTorch Automatic Mixed Precision Library</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-mystery-behind-the-pytorch-automatic-mixed-precision-library-d9386e4b787e?source=collection_archive---------4-----------------------#2024-09-17">https://towardsdatascience.com/the-mystery-behind-the-pytorch-automatic-mixed-precision-library-d9386e4b787e?source=collection_archive---------4-----------------------#2024-09-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="b9ba" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How to get 2X speed up model training using three lines of code</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://mengliuz.medium.com/?source=post_page---byline--d9386e4b787e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mengliu Zhao" class="l ep by dd de cx" src="../Images/0b950a0785fa065db3319ed5be4a91de.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*siAyGzGqa7K3xsa639R_2w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d9386e4b787e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://mengliuz.medium.com/?source=post_page---byline--d9386e4b787e--------------------------------" rel="noopener follow">Mengliu Zhao</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d9386e4b787e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="36c5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Have you ever wished your deep-learning model could run faster?</p><p id="8ca7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The GPU is expensive. The dataset is enormous, and the training session seems endless; you have a million experiments to run and a deadline to hit — all these are good reasons to expect a particular form of training acceleration.</p><p id="5d78" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But which one to choose?</p><p id="0fd7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are already good references on performance tuning for model training from <a class="af nf" href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html" rel="noopener ugc nofollow" target="_blank">PyTorch</a>, <a class="af nf" href="https://huggingface.co/docs/transformers/v4.15.0/performance" rel="noopener ugc nofollow" target="_blank">HuggingFace</a>, and <a class="af nf" href="https://docs.nvidia.com/deeplearning/performance/index.html" rel="noopener ugc nofollow" target="_blank">Nvidia</a>, including asynchronous data loading, buffer checkpointing, distributed data parallelization, and <strong class="ml fr">automatic mixed precision</strong>.</p><p id="0610" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this article, I’ll introduce the automatic mixed precision technique. I’ll start with a brief introduction to Nvidia’s tensor core design, then the groundbreaking work “Mixed Precision Training” paper published in ICLR 2018, and lastly, proceed to a simple example of training a ResNet50 on FashionMNIST and how to speed up the training by 2X while loading 2X a batch size, with <strong class="ml fr">only three extra lines of code</strong>.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/c4fce0287d310bd6839f85ae33144b2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jwaN_w4MP1UQozctr8DvOQ.jpeg"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Image source: <a class="af nf" href="https://pxhere.com/en/photo/872846" rel="noopener ugc nofollow" target="_blank">https://pxhere.com/en/photo/872846</a></figcaption></figure></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="8aed" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk"><strong class="al">Hardware Fundamentals — Nvidia Tensor Cores</strong></h2><p id="7d7d" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">First, let’s recap some of the fundamentals of the GPU design. One of Nvidia GPUS's most popular commercial products is the Volta family, e.g., V100 GPUs, based on the GV100 GPU design. So, we’ll <em class="ph">base our discussions around the GV100 architecture</em> below.</p><p id="1862" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For GV100, the <strong class="ml fr">Streaming Multiprocessor (SM)</strong> is the core design for computation. Each GPU contains 6 GPU Processing Clusters (GPCs) and S84 SMs (or 80 SMs for V100). The overall design looks like the one below.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pi"><img src="../Images/dbf7020857a40f17d2576b5ced5b5123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qJb-jezoq53G3GT0Nncjkg.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Volta GV100 GPU design. Each GPU contains 6 GPCs, and each GPC contains 14 SMs. Image source: <a class="af nf" href="https://arxiv.org/pdf/1803.04014" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1803.04014</a></figcaption></figure><p id="3c32" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For each SM, it contains two types of cores: CUDA cores and Tensor cores. <strong class="ml fr">CUDA cores</strong> were Nvidia’s original design introduced in 2006, which was an essential part of the CUDA platform. The CUDA cores can be divided into three types: FP64 core/unit, FP32 core/unit, and Int32 core/unit. Each GV100 SM contains 32 FP64 cores, 64 FP32 cores, and 64 Int32 cores. <strong class="ml fr">Tensor cores</strong> were introduced in the Volta/Turing (2017) series GPUs to separate from the previous Pascal (2016) series. Each SM on a GV100 contains 8 Tensor cores. A full list of details is <a class="af nf" href="https://datacrunch.io/blog/nvidia-v100-gpu-specs" rel="noopener ugc nofollow" target="_blank">given here</a> for V100 GPUs. A detailed look at the SM design is below.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div class="ng nh pj"><img src="../Images/e200f32cb2b5c8c316fddbb749d81e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*tTxbUQSyCNv6bbef78AHug.png"/></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">A sub-ward of a Streaming Processor (SM). Each SM would contain four sub-warps. Image source: <a class="af nf" href="https://arxiv.org/pdf/1903.03640" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1903.03640</a></figcaption></figure><p id="5ef2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Why Tensor cores? Nvidia Tensor cores are dedicated to performing general matrix multiplication (GEMM) and half-precision matrix multiplication and accumulation (HMMA) operations.</strong> In short, GEMM performs matrix operations in the format of A*B + C, and HMMA converts the operation into the half-precision format. A detailed discussion can be found <a class="af nf" href="https://leimao.github.io/blog/NVIDIA-Tensor-Core-Programming/" rel="noopener ugc nofollow" target="_blank">here</a>. Since deep learning involves MMA heavily, the <strong class="ml fr">Tensor cores are essential in today’s model training and speed-up</strong>.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div class="ng nh pk"><img src="../Images/79b6bd3fc6116e9b9ae38ea680529295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*7wLJwJFLU3SGLnMnsN4IpQ.png"/></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Example of a GEMM operation. For HMMA, A and B are usually converted to FP16, while C and D could be FP16 or FP32. Image source: <a class="af nf" href="https://arxiv.org/pdf/1811.08309" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1811.08309</a></figcaption></figure><p id="4684" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Of course, when switching to mixed precision training, always check the specification of the GPU you’re using. Only the <a class="af nf" href="https://www.wevolver.com/article/tensor-cores-vs-cuda-cores" rel="noopener ugc nofollow" target="_blank">latest GPU series support Tensor cores</a>, and mixed precision training can only be used on these machines.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="c8ca" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk"><strong class="al">Data Format Fundamentals — Single Precision (FP32) vs Half Precision (FP16)</strong></h2><p id="b990" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">Now, let’s take a closer look at FP32 and FP16 formats. The FP32 and FP16 are IEEE formats that represent floating numbers using 32-bit binary storage and 16-bit binary storage. Both formats comprise three parts: a) a sign bit, b) exponent bits, and c) mantissa bits. The FP32 and FP16 differ in the <strong class="ml fr">number of bits allocated to exponent and mantissa</strong>, which result in different value ranges and precisions.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pl"><img src="../Images/215ad72e4c2afa8517c0686351fff244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K8jYnm_Jg8bt_DDrGIqjmA.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Difference between FP16 (IEEE standard), BF16 (Google Brain-standard), FP32 (IEEE-standard), and TF32 (Nvidia-standard). Image source: <a class="af nf" href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Bfloat16_floating-point_format</a></figcaption></figure><p id="dd65" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">How do you convert FP16 and FP32 to real values? According to IEEE-754 standards, the decimal value for FP32 = (-1)^(sign) × 2^(decimal exponent —127 ) × (implicit leading 1 + decimal mantissa), where 127 is the biased exponent value. For FP16, the formula becomes (-1)^(sign) × 2^(decimal exponent — 15) × (implicit leading 1 + decimal mantissa), where 15 is the corresponding biased exponent value. See further details of the biased exponent value <a class="af nf" href="https://en.wikipedia.org/wiki/Exponent_bias" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="6cc0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this sense, the value range for FP32 is approximately [-2¹²⁷, 2¹²⁷] ~[-1.7*1e38, 1.7*1e38], and the value range for FP16 is approximately [-2¹⁵, 2¹⁵]=[-32768, 32768]. Note that the decimal exponent for FP32 is between 0 and 255, and we’re excluding the largest value 0xFF as it represents NAN. That’s why the largest decimal exponent is 254–127 = 127. A similar rule applies to FP16.</p><p id="4d14" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For the precision, note that both the exponent and mantissa contributes to the precision limits (which is also called <strong class="ml fr">denormalization</strong>, see <a class="af nf" href="https://cs.stackexchange.com/questions/101632/understanding-denormalized-numbers-in-floating-point-representation" rel="noopener ugc nofollow" target="_blank">detailed discussion here</a>), so FP32 can represent precision up to 2^(-23)*2^(-126)=2^(-149), and FP16 can represent precision up to 2^(10)*2^(-14)=2^(-24).</p><p id="60ed" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The difference between FP32 and FP16 representations brings the key concerns of mixed precision training, as <strong class="ml fr">different layers/operations of deep learning models are either insensitive or sensitive to value ranges and precision and need to be addressed separately</strong>.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="2bda" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk"><strong class="al">Mixed Precision Training</strong></h2><p id="d0cc" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">Now that we have learnt the hardware foundation for MMA, the concept of Tensor cores, and the key difference between FP32 and FP16, we can further discuss the details for mixed precision training.</p><p id="9500" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The idea of mixed precision training was first proposed in the 2018 ICLR paper “<a class="af nf" href="https://arxiv.org/abs/1710.03740" rel="noopener ugc nofollow" target="_blank">Mixed Precision Training</a>”, which converts deep learning models into half-precision floating point during training without losing model accuracy or modifying hyper-parameters. As mentioned above, since the key difference between FP32 and FP16 are the value ranges and precisions, the paper discussed in detail <strong class="ml fr">why the FP16 causes the gradients to vanish</strong> and how to fix the issue by <strong class="ml fr">loss scaling</strong>. Besides, the paper proposes tricks like using FP32 master weight copy and using FP32 for specific operations like reductions and vector dot-production accumulations.</p><p id="8b9d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Loss scaling</strong>. The paper gives an example of training a Multibox SSD detector network using FP32 precision, as shown below. Without any scaling, the exponent range of the FP16 gradients ≥ 2^(-24), and everything below would become zero, which is insufficient compared to FP32. However, with an experiment, scaling the gradients simply by 2³=8 times can bring the half-precision training accuracy back to match with FP32. In this sense, the authors argue that the extra few percent of gradients between [2^(-27), 2^(-24)] are still important in the training process, while the value below 2^(-27) is not important.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pm"><img src="../Images/7a45fe75b18cd7c86e9d63a193f9f30d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IQLe9G4DeNdi3m6koLMs9w.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Gradient value range using FP32 precision in the Multibox SSD training example. Note that the values between [2^(-27), 2^(-24)] are beyond the FP16 denormalization range and only take a few percent of the total gradients but are still important in the overall training. Image source: <a class="af nf" href="https://arxiv.org/pdf/1710.03740" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1710.03740</a></figcaption></figure><p id="5819" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The way to address this scale difference is to apply loss scaling. According to the chain’s rule, scaling the loss will ensure the same amount scales all the gradients. The gradients need to be unscaled before the final weight update.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="eb81" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk"><strong class="al">Automatic Mixed Precision Training</strong></h2><p id="e037" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">Nvidia first developed Automatic Mixed Precision Training as a PyTorch extension called APEX, which was then widely adopted by mainstream frameworks like PyTorch, TensorFlow, MXNet, etc. See Nvidia <a class="af nf" href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#" rel="noopener ugc nofollow" target="_blank">docs here</a>. We’ll only introduce PyTorch’s automatic mixed precision library for simplicity: <a class="af nf" href="https://pytorch.org/docs/stable/amp.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/amp.html</a>.</p><p id="caf2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The amp library can automatically handle most of the mixed precision training techniques, like the FP32 master weight copy. The users are mainly exposed to <strong class="ml fr">ops autocast</strong> and <strong class="ml fr">gradient/loss scaling</strong>.</p><p id="c90c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Ops autocast. </strong>Although we mentioned that tensor cores could largely improve the performance of GEMM operations, certain operations are unsuitable for half-precision representations.</p><p id="d837" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The amp library gives out a <a class="af nf" href="https://pytorch.org/docs/stable/amp.html#autocast-op-reference" rel="noopener ugc nofollow" target="_blank">list of CUDA ops</a> eligible for half precision. Most matrix multiplication, convolutions, and linear activations are fully covered by the amp.autocast, however, for reduction/sum, softmax, and loss calculations, the calculations are still performed in FP32 as they are more sensitive to data range and precision.</p><p id="c7b6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Gradient/loss scaling.</strong> The amp library provides <a class="af nf" href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#scalefactor" rel="noopener ugc nofollow" target="_blank">automatic gradient scaling</a> techniques so the user doesn’t have to adjust the scaling during training manually. A more detailed algorithm for the scaling factor can be found <a class="af nf" href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#scalefactor" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="6770" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Once the gradient is scaled, it needs to be scaled back before gradient clipping and regularization. More details can be found <a class="af nf" href="https://pytorch.org/docs/stable/amp.html#autocast-op-reference" rel="noopener ugc nofollow" target="_blank">here</a>.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="b000" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk"><strong class="al">A FashionMNIST Training Example</strong></h2><p id="f103" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">The torch.amp library is relatively easy to use and only requires three lines of code to boost your training speed by 2X.</p><p id="cb04" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We start with a very simple task training a ResNet50 model on the FashionMNIST dataset (<a class="af nf" href="https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank">MIT licence</a>) using FP32; we can see the training time is 333 seconds for ten epochs:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pn"><img src="../Images/aa7d6413a871ff3c3b806bff458fcddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dWN6pfltZJUS0-m-prBTsA.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">ResNet50 training on FashionMNIST. Image by author.</figcaption></figure><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh po"><img src="../Images/35c410a68cae78f6b83badcb647fa49e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0MYZV3FK9EUZ50-ieVTxLA.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">The ratio between gradients less than 2**(-24) and the total gradients. We can see that FP16 would turn almost 1/4 of the total gradients into zero. Image by author.</figcaption></figure><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pp"><img src="../Images/aa65d416e3eb94c05cc56ef30eab8e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TElPsG65njPTgYEkSSY5rQ.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Evaluation results. Image by author.</figcaption></figure><p id="26a5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now that we use the amp library. The amp library only requires three extra lines of code for mixed precision training. We can see the training finished within 141 seconds, which is 2.36X speed up than the FP32 training, while achieving the same precision, recall and F1-score.</p><pre class="nj nk nl nm nn pq pr ps bp pt bb bk"><span id="5e62" class="pu oi fq pr b bg pv pw l px py">scaler = torch.cuda.amp.GradScaler()<br/># start your training code<br/># ...<br/>with torch.autocast(device_type="cuda"):<br/>  # training code<br/><br/># wrapping loss and optimizer<br/>scaler.scale(loss).backward()<br/>scaler.step(optimizer)<br/><br/>scaler.update()</span></pre><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pn"><img src="../Images/130adb13d32d608790dbc8b828d9ea5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HUUSnpkPhmoatZhk-V0X7w.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Training code with amp. Image by author.</figcaption></figure><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pz"><img src="../Images/d6afd6d8ec261110cf753fb45cfdbea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T8i5b_RwKMGjtwceCtbUUA.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">The scaling factor during training. The scaling factor only changed atthe first step and kept unchanged. Image by author.</figcaption></figure><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh qa"><img src="../Images/60a8df649cc72aa64cdad00e012f89f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nG2Qa0Q6LgXtofG0Md5tsQ.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Final result comparable to the FP32 training result. Image by author.</figcaption></figure><p id="a621" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The github link for the code above is <a class="af nf" href="https://github.com/adoskk/MachineLearningBasics/blob/main/mixed_precision_training/mixed_precision_training.ipynb" rel="noopener ugc nofollow" target="_blank">here</a>.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="ce48" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">Summary</h2><p id="f2e5" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">Mixed Precision Training is a valuable technique for accelerating deep learning model training. It not only speed up the floating point operations, but also saves the GPU memories as the training batch can be converted to FP16, which saves half the GPU memory. With PyTorch’s amp library, the extra code could be minimized to three additional lines, as the weight copy, loss scaling, operation type casts are all handled by the library internally.</p><p id="d365" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, mixed precision training doesn’t really resolve the GPU memory issue if the model weight size is much larger than the data batch. For one thing, only certain layers of the model is casted into FP16 while the rest are still calculated in FP32; second, weight update need FP32 copies, which still takes much GPU memory; third, parameters from optimizers like Adam takes much GPU memory during training and the mixed precision training keeps the optimizer parameters unchanged. In that sense, more advanced techniques like DeepSpeed’s ZERO algorithm is needed.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="264d" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk"><strong class="al">References</strong></h2><ul class=""><li id="ba30" class="mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne qb qc qd bk">Micikevicius et al., Mixed precision training. ICLR 2018</li><li id="2f44" class="mj mk fq ml b go qe mn mo gr qf mq mr ms qg mu mv mw qh my mz na qi nc nd ne qb qc qd bk">PyTorch AMP library: <a class="af nf" href="https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html</a></li><li id="7256" class="mj mk fq ml b go qe mn mo gr qf mq mr ms qg mu mv mw qh my mz na qi nc nd ne qb qc qd bk">Nvidia CUDA floating point: <a class="af nf" href="https://docs.nvidia.com/cuda/floating-point/index.html" rel="noopener ugc nofollow" target="_blank">https://docs.nvidia.com/cuda/floating-point/index.html</a></li></ul></div></div></div></div>    
</body>
</html>