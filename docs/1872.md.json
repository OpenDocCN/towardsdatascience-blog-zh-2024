["```py\nfor i in range(50):\n    t0 = time.time() # start timer\n    x, y = train_loader.next_batch()\n    x, y = x.to(device), y.to(device)\n    optimizer.zero_grad() \n    logits, loss = model(x, y)\n    loss.backward() \n    optimizer.step()\n    torch.cuda.synchronize() # synchronize with GPU\n    t1 = time.time() # end timer\n    dt = (t1-t0)*1000 # milliseconds difference\n    print(f\"loss {loss.item()}, step {i}, dt {dt:.2f}ms\")\n```", "```py\ntrain_loader = DataLoaderLite(B=16, T=1024)\n```", "```py\ntorch.set_float32_matmul_precision(\"high\")\n```", "```py\nfor i in range(50):\n    t0 = time.time() \n    x, y = train_loader.next_batch()\n    x, y = x.to(device), y.to(device)\n    optimizer.zero_grad() \n    with torch.autocast(device_type=device, dtype=torch.bfloat16): # bf16 change\n        logits, loss = model(x, y)\n    loss.backward() \n    optimizer.step()\n    torch.cuda.synchronize() \n    t1 = time.time()\n    dt = (t1-t0)*1000 \n    print(f\"loss {loss.item()}, step {i}, dt {dt:.2f}ms\")\n    loss_arr.append(loss.item())\n```", "```py\n# ...\nmodel = GPT(GPTConfig(vocab_size=50304))\nmodel.to(device)\nmodel = torch.compile(model) # new line here\n# ...\n```", "```py\nclass TanhGELU(nn.Module):\n    def forward(self, input):\n        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0/math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n```", "```py\ny = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n```", "```py\nmodel = GPT(GPTConfig(vocab_size=50304))\n```"]