<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Run Stable Diffusion with ONNX</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Run Stable Diffusion with ONNX</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-run-stable-diffusion-with-onnx-dafd2d29cd14?source=collection_archive---------4-----------------------#2024-05-13">https://towardsdatascience.com/how-to-run-stable-diffusion-with-onnx-dafd2d29cd14?source=collection_archive---------4-----------------------#2024-05-13</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="bb87" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Addressing compatibility issues during installation | ONNX for NVIDIA GPUs | Hugging Face’s Optimum library</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@turc.raluca?source=post_page---byline--dafd2d29cd14--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Julia Turc" class="l ep by dd de cx" src="../Images/1ca27d7db36799dec53b8daf4099f5cb.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*kY2xovOt_cQKjc9h9dYeRQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--dafd2d29cd14--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@turc.raluca?source=post_page---byline--dafd2d29cd14--------------------------------" rel="noopener follow">Julia Turc</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--dafd2d29cd14--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 13, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="7c17" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This article discusses the <a class="af ne" href="https://onnxruntime.ai/" rel="noopener ugc nofollow" target="_blank">ONNX runtime</a>, one of the most effective ways of speeding up Stable Diffusion inference. On an A100 GPU, running SDXL for 30 denoising steps to generate a 1024 x 1024 image can be as fast as 2 seconds. However, the ONNX runtime depends on multiple moving pieces, and installing the right versions of all of its dependencies can be tricky in a constantly evolving ecosystem. Take this as a high-level debugging guide, where I share my struggles in hopes of saving you time. While the specific versions and commands might quickly become obsolete, the high-level concepts should remain relevant for a longer period of time.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/7ec6eb65d5774713d8de06428cfaae0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WtU39w94qV9b3ThVWPSWIw.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Image by author</figcaption></figure><h1 id="c895" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">What is ONNX?</h1><p id="7c2b" class="pw-post-body-paragraph mi mj fq mk b go ou mm mn gr ov mp mq mr ow mt mu mv ox mx my mz oy nb nc nd fj bk">ONNX can actually refer to two different (but related) parts of the ML stack:</p><ol class=""><li id="44c7" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd oz pa pb bk"><strong class="mk fr">ONNX is a <em class="pc">format</em> for storing machine learning models. </strong>It stands for <em class="pc">Open Neural Network Exchange</em> and, as its name suggests, its main goal is interoperability across platforms. ONNX is a self-contained format: it stores both the model <em class="pc">weights </em>and <em class="pc">architecture</em>. This means that a single .<em class="pc">onnx </em>file contains all the information needed to run inference. No need to write any additional code to define or load a model; instead, you simply pass it to a <em class="pc">runtime </em>(more on this below).</li><li id="7ef7" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd oz pa pb bk"><strong class="mk fr">ONNX is also a <em class="pc">runtime</em> to run model that are in ONNX format</strong>. It literally <em class="pc">runs </em>the model. You can see it as a mediator between the architecture-agnostic ONNX format and the actual hardware that runs inference. There is a separate version of the runtime for each supported accelerator type (see <a class="af ne" href="https://onnxruntime.ai/docs/execution-providers/#summary-of-supported-execution-providers" rel="noopener ugc nofollow" target="_blank">full list here</a>). Note, however, that the ONNX runtime is not the only way to run inference with a model that is in ONNX format — it’s just one way. Manufacturers can choose to build their own runtimes that are hyper-optimized for their hardware. For instance, <a class="af ne" href="https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html" rel="noopener ugc nofollow" target="_blank">NVIDIA’s TensorRT</a> is an alternative to the ONNX runtime.</li></ol><p id="16cf" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This article focuses on running Stable Diffusion models using the <strong class="mk fr">ONNX runtime</strong>. While the high-level concepts are probably timeless, note that the ML tooling ecosystem is in constant change, so the exact workflow or code snippets might become obsolete (this article was written in May 2024). I will focus on the Python implementation in particular, but note that the ONNX runtime can also operate in <a class="af ne" href="https://onnxruntime.ai/docs/tutorials/api-basics" rel="noopener ugc nofollow" target="_blank">other languages</a> like C++, C#, Java or JavaScript.</p><h2 id="92c8" class="pi nz fq bf oa pj pk pl od pm pn po og mr pp pq pr mv ps pt pu mz pv pw px py bk">Pros of the ONNX Runtime</h2><ul class=""><li id="2036" class="mi mj fq mk b go ou mm mn gr ov mp mq mr ow mt mu mv ox mx my mz oy nb nc nd pz pa pb bk"><strong class="mk fr">Balance between inference speed and interoperability</strong>. While the ONNX runtime will not always be the fastest solution for all types of hardware, it is a <em class="pc">fast enough </em>solution for <em class="pc">most </em>types of hardware. This is particularly appealing if you’re serving your models on a heterogeneous fleet of machines and don’t have the resources to micro-optimize for each different accelerator.</li><li id="c189" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pz pa pb bk"><strong class="mk fr">Wide adoption and reliable authorship. </strong>ONNX was open-sourced by Microsoft, who are still maintaining it. It is widely adopted and well integrated into the wider ML ecosystem. For instance, Hugging Face’s <a class="af ne" href="https://huggingface.co/docs/optimum/en/index" rel="noopener ugc nofollow" target="_blank">Optimum library</a> allows you to define and run ONNX model pipelines with a syntax that is reminiscent of their popular transformers and diffusers libraries.</li></ul><h2 id="dca7" class="pi nz fq bf oa pj pk pl od pm pn po og mr pp pq pr mv ps pt pu mz pv pw px py bk">Cons of the ONNX Runtime</h2><ul class=""><li id="ad61" class="mi mj fq mk b go ou mm mn gr ov mp mq mr ow mt mu mv ox mx my mz oy nb nc nd pz pa pb bk"><strong class="mk fr">Engineering overhead</strong>. Compared to the alternative of running inference directly in PyTorch, the ONNX runtime requires compiling your model to the ONNX format (which can take 20–30 minutes for a Stable Diffusion model) and installing the runtime itself.</li><li id="1d35" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pz pa pb bk"><strong class="mk fr">Restricted set of ops. </strong>The ONNX format doesn’t support all PyTorch operations (it is even more restrictive than <a class="af ne" href="https://pytorch.org/docs/stable/jit.html" rel="noopener ugc nofollow" target="_blank">TorchScript</a>). If your model is using an unsupported operation, you will either have to reimplement the relevant portion, or drop ONNX altogether.</li><li id="1f95" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pz pa pb bk"><strong class="mk fr">Brittle installation and setup</strong>. Since the ONNX runtime makes the translation from the ONNX format to architecture-specific instructions, it can be tricky to get the right combination of software versions to make it work. For instance, if running on an NVIDIA GPU, you need to ensure compatibility of (1) operating system, (2) CUDA version, (3) cuDNN version, and (4) ONNX runtime version. There are useful resources like the <a class="af ne" href="https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html" rel="noopener ugc nofollow" target="_blank">CUDA compatibility matrix</a>, but you might still end up wasting hours finding the magic combination that works at a given point in time.</li><li id="64df" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pz pa pb bk"><strong class="mk fr">Hardware limitations</strong>. While the ONNX runtime can run on <a class="af ne" href="https://onnxruntime.ai/docs/execution-providers/#summary-of-supported-execution-providers" rel="noopener ugc nofollow" target="_blank">many architectures</a>, it cannot run on <em class="pc">all </em>architectures like pure PyTorch models can. For instance, there is currently (May 2024) no support for <a class="af ne" href="https://cloud.google.com/tpu?hl=en" rel="noopener ugc nofollow" target="_blank">Google Cloud TPUs</a> or <a class="af ne" href="https://aws.amazon.com/machine-learning/inferentia/" rel="noopener ugc nofollow" target="_blank">AWS Inferentia</a> chips (see <a class="af ne" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/faq/onnx-faq.html" rel="noopener ugc nofollow" target="_blank">FAQ</a>).</li></ul><p id="2192" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">At first glance, the list of cons looks longer than the list of pros, but don’t be discouraged — as shown later on, the improvements in model latency can be significant and worth it.</p><h1 id="fd51" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">How to install the ONNX runtime</h1><h2 id="2fe7" class="pi nz fq bf oa pj pk pl od pm pn po og mr pp pq pr mv ps pt pu mz pv pw px py bk">Option #1: Install from <a class="af ne" href="https://github.com/microsoft/onnxruntime/tree/main" rel="noopener ugc nofollow" target="_blank">source</a></h2><p id="0b8a" class="pw-post-body-paragraph mi mj fq mk b go ou mm mn gr ov mp mq mr ow mt mu mv ox mx my mz oy nb nc nd fj bk">As mentioned above, the ONNX runtime requires compatibility between many pieces of software. If you want to be on the cutting edge, the best way to get the latest version is to follow the instructions in the <a class="af ne" href="https://github.com/microsoft/onnxruntime/tree/main" rel="noopener ugc nofollow" target="_blank">official Github repository</a>. For Stable Diffusion in particular, <a class="af ne" href="https://github.com/microsoft/onnxruntime/tree/main/onnxruntime/python/tools/transformers/models/stable_diffusion" rel="noopener ugc nofollow" target="_blank">this folder</a> contains installation instructions and sample scripts for generating images. Expect building from source to take quite a while (around 30 minutes).</p><p id="4cb8" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">At the time of writing (May 2024), this solution worked seamlessly for me on an Amazon EC2 instance (<code class="cx qa qb qc qd b">g5.2xlarge</code>, which comes with a A10G GPU). It avoids compatibility issues discussed below by using a Docker image that comes with the right dependencies.</p><h2 id="d026" class="pi nz fq bf oa pj pk pl od pm pn po og mr pp pq pr mv ps pt pu mz pv pw px py bk">Option #2: Install via PyPI</h2><p id="7dd8" class="pw-post-body-paragraph mi mj fq mk b go ou mm mn gr ov mp mq mr ow mt mu mv ox mx my mz oy nb nc nd fj bk">In production, you will most likely want a stable version of the ONNX runtime from PyPI, instead of installing the latest version from source. For Python in particular, there are two different libraries (one for CPU and one for GPU). Here is the command to install it for CPU:</p><pre class="ni nj nk nl nm qe qd qf bp qg bb bk"><span id="8f8d" class="qh nz fq qd b bg qi qj l qk ql">pip install onnxruntime</span></pre><p id="a52c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">And here is the command to install it for GPU:</p><pre class="ni nj nk nl nm qe qd qf bp qg bb bk"><span id="b70b" class="qh nz fq qd b bg qi qj l qk ql">pip install onnxruntime-gpu</span></pre><p id="8c02" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">You should never install both</strong>. Having them both might lead to error messages or behaviors that are not easy to track back to this root cause. The ONNX runtime might simply fail to acknowledge the presence of the GPU, which will look surprising given that <code class="cx qa qb qc qd b">onnxruntime-gpu</code> is indeed installed.</p><h1 id="5fb7" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Addressing compatibility issues</h1><p id="45f1" class="pw-post-body-paragraph mi mj fq mk b go ou mm mn gr ov mp mq mr ow mt mu mv ox mx my mz oy nb nc nd fj bk">In an ideal world, <code class="cx qa qb qc qd b">pip install onnxruntime-gpu</code> would be the end of the story. However, in practice, there are strong compatibility requirements between other pieces of software on your machine, including the operating system, the hardware-specific drivers, and the Python version.</p><p id="98b8" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Say that you want to use the latest version of the ONNX runtime (1.17.1) at the time of writing. So what stars do we need to align to make this happen?</p><p id="7ef3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Here are some of the most common sources of incompatibility that can help you set up your environment. The exact details will quickly become obsolete, but the high-level ideas should continue to apply for a while.</p><h2 id="9334" class="pi nz fq bf oa pj pk pl od pm pn po og mr pp pq pr mv ps pt pu mz pv pw px py bk">CUDA compatibility</h2><p id="9cb7" class="pw-post-body-paragraph mi mj fq mk b go ou mm mn gr ov mp mq mr ow mt mu mv ox mx my mz oy nb nc nd fj bk">If you are not planning on using an NVIDIA GPU, you can skip this section. CUDA is a platform for parallel computing that sits on top of NVIDIA GPUs, and is required for machine learning workflows. Each version of the ONNX runtime is compatible with only certain CUDA versions, as you can see in <a class="af ne" href="https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html" rel="noopener ugc nofollow" target="_blank">this compatibility matrix</a>.</p><p id="c21d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">According to this matrix, the latest ONNX runtime version (1.17) is compatible with both CUDA 11.8 and CUDA 12. But you need to pay attention to the fine print: <strong class="mk fr">by default, ONNX runtime 1.17 expects CUDA 11.8</strong>. However, most VMs today (May 2024) come with CUDA 12.1 (you can check the version by running <code class="cx qa qb qc qd b">nvcc --version</code>). For this particular setup, you’ll have to replace the usual <code class="cx qa qb qc qd b">pip install onnxruntime-gpu</code> with:</p><pre class="ni nj nk nl nm qe qd qf bp qg bb bk"><span id="5eff" class="qh nz fq qd b bg qi qj l qk ql">pip install onnxruntime-gpu==1.17.1 --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/</span></pre><p id="0a53" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Note that, instead of being at the mercy of whatever CUDA version happens to be installed on your machine, a cleaner solution is to do your work from within a Docker container. You simply choose the image that has your desired version of Python and CUDA. For instance:</p><pre class="ni nj nk nl nm qe qd qf bp qg bb bk"><span id="cd72" class="qh nz fq qd b bg qi qj l qk ql">docker run --rm -it --gpus all nvcr.io/nvidia/pytorch:23.10-py3</span></pre><h2 id="5646" class="pi nz fq bf oa pj pk pl od pm pn po og mr pp pq pr mv ps pt pu mz pv pw px py bk">OS + Python + pip compatibility</h2><p id="19ff" class="pw-post-body-paragraph mi mj fq mk b go ou mm mn gr ov mp mq mr ow mt mu mv ox mx my mz oy nb nc nd fj bk">This section discusses compatibility issues that are architecture-agnostic (i.e. you’ll encounter them regardless of the target accelerator). It boils down to making sure that your software (operating system, Python installation and <code class="cx qa qb qc qd b">pip</code> installation) are compatible with your desired version of the ONNX runtime library.</p><p id="aef0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Pip version: </strong>Unless you are working with legacy code or systems, your safest bet is to upgrade <code class="cx qa qb qc qd b">pip</code> to the latest version:</p><pre class="ni nj nk nl nm qe qd qf bp qg bb bk"><span id="4c18" class="qh nz fq qd b bg qi qj l qk ql">python -m pip install --upgrade pip</span></pre><p id="8718" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Python version: </strong>As of May 2024, the Python version that is least likely to give you headaches is 3.10 (this is what most VMs come with by default). Again, unless you are working with legacy code, you certainly want at least 3.8 (since 3.7 was deprecated in June 2023).</p><p id="6fe4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Operating system</strong>: The fact that the OS version can also hinder your ability to install the desired library came as a surprise to me, especially that I was using the most standard EC2 instances. And it wasn’t straightforward to figure out that the OS version was the culprit.</p><p id="9635" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Here I will walk you through my debugging process, in the hopes that the workflow itself is longer-lived than the specifics of the versions today. First, I installed <code class="cx qa qb qc qd b">onnxruntime-gpu</code> with the following command (since I had CUDA 12.1 installed on my machine):</p><pre class="ni nj nk nl nm qe qd qf bp qg bb bk"><span id="20f9" class="qh nz fq qd b bg qi qj l qk ql">pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/</span></pre><p id="f489" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">On the surface, this should install the latest version of the library available on PyPI. In reality however, this will install the latest version <strong class="mk fr">compatible with your current setup </strong>(OS + Python version + pip version). For me at the time, that happened to be <code class="cx qa qb qc qd b">onnxruntime-gpu==1.16.0</code> (as opposed to 1.17.1, which is the latest). Unknowingly installing an older version simply manifested in the ONNX runtime being unable to detect the GPU, with no other clues. After somewhat accidentally discovering the version is older than expected, I explicitly asked for the newer one:</p><pre class="ni nj nk nl nm qe qd qf bp qg bb bk"><span id="ab0c" class="qh nz fq qd b bg qi qj l qk ql">pip install onnxruntime-gpu==1.17.1 --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/</span></pre><p id="0ec7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This resulted in a message from <code class="cx qa qb qc qd b">pip</code> complaining that the version I requested is not actually available (despite being <a class="af ne" href="https://pypi.org/project/onnxruntime-gpu/1.17.1/" rel="noopener ugc nofollow" target="_blank">listed on PyPI</a>):</p><pre class="ni nj nk nl nm qe qd qf bp qg bb bk"><span id="05c4" class="qh nz fq qd b bg qi qj l qk ql">ERROR: Could not find a version that satisfies the requirement onnxruntime-gpu==1.17.1 (from versions: 1.12.0, 1.12.1, 1.13.1, 1.14.0, 1.14.1, 1.15.0, 1.15.1, 1.16.0, 1.16.1, 1.16.2, 1.16.3)<br/>ERROR: No matching distribution found for onnxruntime-gpu==1.17.1</span></pre><p id="c6b7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To understand why the latest version is not getting installed, you can pass a flag that makes <code class="cx qa qb qc qd b">pip</code> verbose: <code class="cx qa qb qc qd b">pip install ... -vvv</code>. This reveals all the Python wheels that <code class="cx qa qb qc qd b">pip</code> cycles through in order to find the newest one that is compatible to your system. Here is what the output looked like for me:</p><pre class="ni nj nk nl nm qe qd qf bp qg bb bk"><span id="5ae8" class="qh nz fq qd b bg qi qj l qk ql">Skipping link: none of the wheel's tags (cp35-cp35m-manylinux1_x86_64) are compatible (run pip debug --verbose to show compatible tags): https://files.pythonhosted.org/packages/26/1a/163521e075d2e0c3effab02ba11caba362c06360913d7c989dcf9506edb9/onnxruntime_gpu-0.1.2-cp35-cp35m-manylinux1_x86_64.whl (from https://pypi.org/simple/onnxruntime-gpu/)<br/>Skipping link: none of the wheel's tags (cp36-cp36m-manylinux1_x86_64) are compatible (run pip debug --verbose to show compatible tags): https://files.pythonhosted.org/packages/52/f2/30aaa83bc9e90e8a919c8e44e1010796eb30f3f6b42a7141ffc89aba9a8e/onnxruntime_gpu-0.1.2-cp36-cp36m-manylinux1_x86_64.whl (from https://pypi.org/simple/onnxruntime-gpu/)<br/>Skipping link: none of the wheel's tags (cp37-cp37m-manylinux1_x86_64) are compatible (run pip debug --verbose to show compatible tags): https://files.pythonhosted.org/packages/a2/05/af0481897255798ee57a242d3989427015a11a84f2eae92934627be78cb5/onnxruntime_gpu-0.1.2-cp37-cp37m-manylinux1_x86_64.whl (from https://pypi.org/simple/onnxruntime-gpu/)<br/>Skipping link: none of the wheel's tags (cp35-cp35m-manylinux1_x86_64) are compatible (run pip debug --verbose to show compatible tags): https://files.pythonhosted.org/packages/17/cb/0def5a44db45c6d38d95387f20057905ce2dd4fad35c0d43ee4b1cebbb19/onnxruntime_gpu-0.1.3-cp35-cp35m-manylinux1_x86_64.whl (from https://pypi.org/simple/onnxruntime-gpu/)<br/>Skipping link: none of the wheel's tags (cp36-cp36m-manylinux1_x86_64) are compatible (run pip debug --verbose to show compatible tags): https://files.pythonhosted.org/packages/a6/53/0e733ebd72d7dbc84e49eeece15af13ab38feb41167fb6c3e90c92f09cbb/onnxruntime_gpu-0.1.3-cp36-cp36m-manylinux1_x86_64.whl (from <a class="af ne" href="https://pypi.org/simple/onnxruntime-gpu/)" rel="noopener ugc nofollow" target="_blank">https://pypi.org/simple/onnxruntime-gpu/)</a><br/>...</span></pre><p id="91d1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The tags listed in brackets are <em class="pc">Python platform compatibility tags</em>, and you can read more about them <a class="af ne" href="https://packaging.python.org/en/latest/specifications/platform-compatibility-tags/" rel="noopener ugc nofollow" target="_blank">here</a>. In a nutshell, every Python wheel comes with a tag that indicates what system it can run on. For instance, <code class="cx qa qb qc qd b">cp35-cp35m-manylinux1_x86_64</code> requires CPython 3.5, a set of (older) Linux distributions that fall under the <code class="cx qa qb qc qd b">manylinux1</code> umbrella, and a 64-bit x86-compatible processor.</p><p id="677e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Since I wanted to run Python 3.10 on a Linux machine (hence filtering for <code class="cx qa qb qc qd b">cp310.*manylinux.*</code>, I was left with a single possible wheel for the <code class="cx qa qb qc qd b">onnxruntime-gpu</code> library, with the following tag:</p><pre class="ni nj nk nl nm qe qd qf bp qg bb bk"><span id="ff5b" class="qh nz fq qd b bg qi qj l qk ql">cp310-cp310-manylinux_2_28_x86_64</span></pre><p id="c3f2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">You can get a list of tags that are compatible with your system by running <code class="cx qa qb qc qd b">pip debug --verbose</code>. Here is what part of my output looked like:</p><pre class="ni nj nk nl nm qe qd qf bp qg bb bk"><span id="4d51" class="qh nz fq qd b bg qi qj l qk ql">cp310-cp310-manylinux_2_26_x86_64<br/>cp310-cp310-manylinux_2_25_x86_64<br/>cp310-cp310-manylinux_2_24_x86_64<br/>cp310-cp310-manylinux_2_23_x86_64<br/>cp310-cp310-manylinux_2_22_x86_64<br/>cp310-cp310-manylinux_2_21_x86_64<br/>cp310-cp310-manylinux_2_20_x86_64<br/>cp310-cp310-manylinux_2_19_x86_64<br/>cp310-cp310-manylinux_2_18_x86_64<br/>cp310-cp310-manylinux_2_17_x86_64<br/>...</span></pre><p id="0c87" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In other words, my operating system is just a tad too old (the maximum linux tag that it supports is <code class="cx qa qb qc qd b">manylinux_2_26</code>, while the <code class="cx qa qb qc qd b">onnxruntime-gpu</code> library’s only Python 3.10 wheel requires <code class="cx qa qb qc qd b">manylinux_2_28</code>. Upgrading from Ubuntu 20.04 to Ubuntu 24.04 solved the problem.</p><h1 id="c8dd" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">How to run Stable Diffusion with the ONNX runtime</h1><p id="a81b" class="pw-post-body-paragraph mi mj fq mk b go ou mm mn gr ov mp mq mr ow mt mu mv ox mx my mz oy nb nc nd fj bk">Once the ONNX runtime is (finally) installed, generating images with Stable Diffusion requires two following steps:</p><ol class=""><li id="23e4" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd oz pa pb bk">Export the PyTorch model to ONNX (this can take &gt; 30 minutes!)</li><li id="5ddb" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd oz pa pb bk">Pass the ONNX model and the inputs (text prompt and other parameters) to the ONNX runtime.</li></ol><h2 id="b18a" class="pi nz fq bf oa pj pk pl od pm pn po og mr pp pq pr mv ps pt pu mz pv pw px py bk">Option #1: Using official scripts from Microsoft</h2><p id="19e9" class="pw-post-body-paragraph mi mj fq mk b go ou mm mn gr ov mp mq mr ow mt mu mv ox mx my mz oy nb nc nd fj bk">As mentioned before, using the <a class="af ne" href="https://github.com/microsoft/onnxruntime/tree/main/onnxruntime/python/tools/transformers/models/stable_diffusion" rel="noopener ugc nofollow" target="_blank">official sample scripts</a> from the ONNX runtime repository worked out of the box for me. If you follow their installation instructions, you won’t even have to deal with the compatibility issues mentioned above. After installation, generating an image is a simple as:</p><pre class="ni nj nk nl nm qe qd qf bp qg bb bk"><span id="d9a5" class="qh nz fq qd b bg qi qj l qk ql">python3 demo_txt2img_xl.py "starry night over Golden Gate Bridge by van gogh"</span></pre><p id="3517" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Under the hood, this script defines an SDXL model using Hugging Face’s <a class="af ne" href="https://huggingface.co/docs/diffusers/en/index" rel="noopener ugc nofollow" target="_blank">diffusers</a> library, exports it to ONNX format (which can take up to 30 minutes!), then invokes the ONNX runtime.</p><h2 id="e07c" class="pi nz fq bf oa pj pk pl od pm pn po og mr pp pq pr mv ps pt pu mz pv pw px py bk">Option #2: Using Hugging Face’s <a class="af ne" href="https://huggingface.co/docs/optimum/en/index" rel="noopener ugc nofollow" target="_blank">Optimum</a> library</h2><p id="7663" class="pw-post-body-paragraph mi mj fq mk b go ou mm mn gr ov mp mq mr ow mt mu mv ox mx my mz oy nb nc nd fj bk">The <a class="af ne" href="https://huggingface.co/docs/optimum/en/index" rel="noopener ugc nofollow" target="_blank">Optimum</a> library promises a lot of convenience, allowing you to run models on various accelerators while using the familiar pipeline APIs from the well-known transformers and diffusers libraries. For ONNX in particular, this is what inference code for SDXL looks like (more in <a class="af ne" href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models#text-to-image" rel="noopener ugc nofollow" target="_blank">this tutorial</a>):</p><pre class="ni nj nk nl nm qe qd qf bp qg bb bk"><span id="ca50" class="qh nz fq qd b bg qi qj l qk ql">from optimum.onnxruntime import ORTStableDiffusionXLPipeline<br/><br/>model_id = "stabilityai/stable-diffusion-xl-base-1.0"<br/>base = ORTStableDiffusionXLPipeline.from_pretrained(model_id)<br/>prompt = "sailing ship in storm by Leonardo da Vinci"<br/>image = base(prompt).images[0]<br/><br/># Don't forget to save the ONNX model<br/>save_directory = "sd_xl_base"<br/>base.save_pretrained(save_directory)</span></pre><p id="7fb8" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In practice, however, I struggled a lot with the Optimum library. First, installation is non-trivial; naively following the installation instruction in the <a class="af ne" href="https://github.com/huggingface/optimum" rel="noopener ugc nofollow" target="_blank">README file</a> will run into the incompatibility issues explained above. This is not Optimum’s fault per se, but it does add yet another layer of abstraction on top of an already brittle setup. The Optimum installation might pull a version of <code class="cx qa qb qc qd b">onnxruntime</code> that is conflicting with your setup.</p><p id="042e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Even after I won the battle against compatibility issues, I wasn’t able to run SDXL inference on GPU using Optimum’s ONNX interface. The code snippet above (directly taken from a Hugging Face tutorial) fails with some shape mismatches, perhaps due to bugs in the PyTorch → ONNX conversion:</p><pre class="ni nj nk nl nm qe qd qf bp qg bb bk"><span id="c488" class="qh nz fq qd b bg qi qj l qk ql">[ONNXRuntimeError] : 1 : FAIL : Non-zero status code returned while running Add node.<br/>Name:'/down_blocks.1/attentions.0/Add'<br/>Status Message: /down_blocks.1/attentions.0/Add: left operand cannot broadcast on dim 3 LeftShape: {2,64,4096,10}, RightShape: {2,640,64,64}</span></pre><p id="1cfd" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For a brief second I considered getting into the weeds and debugging the Hugging Face code (at least it’s open source!), but gave up when I realized that Optimum has a backlog of <a class="af ne" href="https://github.com/huggingface/optimum/issues" rel="noopener ugc nofollow" target="_blank">more than 250 issues</a>, with issues going for weeks with no acknowledgement from the Hugging Face team. I decided to move on and simply use Microsoft’s official scripts instead.</p><h1 id="4d5c" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Latency Reduction</h1><p id="1727" class="pw-post-body-paragraph mi mj fq mk b go ou mm mn gr ov mp mq mr ow mt mu mv ox mx my mz oy nb nc nd fj bk">As promised, the effort to get the ONNX runtime working is worth it. On an A100 GPU, the inference time is reduced from 7–8 seconds (when running vanilla PyTorch) to ~2 seconds. This is comparable to <a class="af ne" href="https://developer.nvidia.com/tensorrt-getting-started" rel="noopener ugc nofollow" target="_blank">TensorRT</a> (an NVIDIA-specific alternative to ONNX), and about 1 second faster than <a class="af ne" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="noopener ugc nofollow" target="_blank">torch.compile</a> (PyTorch’s native JIT compilation).</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/7ec6eb65d5774713d8de06428cfaae0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WtU39w94qV9b3ThVWPSWIw.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Image by author</figcaption></figure><p id="f4e9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af ne" href="https://www.baseten.co/blog/sdxl-inference-in-under-2-seconds-the-ultimate-guide-to-stable-diffusion-optimiza/" rel="noopener ugc nofollow" target="_blank">Reportedly</a>, switching to even more performant GPUs (e.g. H100) can lead to even higher gains from running your model with a specialized runtime.</p><h1 id="3254" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Conclusion and further reading</h1><p id="d8d8" class="pw-post-body-paragraph mi mj fq mk b go ou mm mn gr ov mp mq mr ow mt mu mv ox mx my mz oy nb nc nd fj bk">The ONNX runtime promises significant latency gains, but it comes with non-trivial engineering overhead. It also faces the classic trade-off for static compilation: inference is a lot faster, but the graph cannot be dynamically modified (which is at odds with dynamic adapters like <a class="af ne" href="https://huggingface.co/docs/peft/en/index" rel="noopener ugc nofollow" target="_blank">peft</a>). The ONNX runtime and similar compilation methods are worth adding to your pipeline once you’ve passed the experimentation phase, and are ready to invest in efficient production code.</p><p id="f650" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If you’re interested in optimizing inference time, here are some articles that I found helpful:</p><ul class=""><li id="60c7" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pz pa pb bk"><a class="af ne" href="https://www.baseten.co/blog/sdxl-inference-in-under-2-seconds-the-ultimate-guide-to-stable-diffusion-optimiza/" rel="noopener ugc nofollow" target="_blank">SDXL inference in under 2 seconds: the ultimate guide to Stable Diffusion optimization</a></li><li id="aea2" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pz pa pb bk"><a class="af ne" href="https://www.baseten.co/blog/40-faster-stable-diffusion-xl-inference-with-nvidia-tensorrt/" rel="noopener ugc nofollow" target="_blank">40% faster Stable Diffusion XL inference with NVIDIA TensorRT</a></li><li id="f511" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pz pa pb bk"><a class="af ne" href="https://www.baseten.co/blog/unlocking-the-full-power-of-nvidia-h100-gpus-for-ml-inference-with-tensorrt/" rel="noopener ugc nofollow" target="_blank">Unlocking the full power of NVIDIA H100 GPUs for ML inference with TensorRT</a></li><li id="4e09" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pz pa pb bk"><a class="af ne" href="https://www.photoroom.com/inside-photoroom/stable-diffusion-25-percent-faster-and-save-seconds" rel="noopener ugc nofollow" target="_blank">Making stable diffusion 25% faster using TensorRT</a></li><li id="8dfa" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pz pa pb bk"><a class="af ne" href="https://aws.amazon.com/blogs/machine-learning/maximize-stable-diffusion-performance-and-lower-inference-costs-with-aws-inferentia2/" rel="noopener ugc nofollow" target="_blank">Maximize Stable Diffusion performance and lower inference costs with AWS Inferentia2</a></li><li id="d2ef" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pz pa pb bk"><a class="af ne" href="https://huggingface.co/docs/optimum-neuron/en/tutorials/stable_diffusion" rel="noopener ugc nofollow" target="_blank">Generate images with Stable Diffusion models on AWS Inferentia</a></li><li id="04da" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pz pa pb bk"><a class="af ne" href="https://docs.openvino.ai/2022.3/notebooks/225-stable-diffusion-text-to-image-with-output.html" rel="noopener ugc nofollow" target="_blank">Text-to-Image Generation with Stable Diffusion and OpenVINO</a></li><li id="5817" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pz pa pb bk"><a class="af ne" href="https://github.com/huggingface/blog/blob/main/stable-diffusion-inference-intel.md" rel="noopener ugc nofollow" target="_blank">Accelerating Stable Diffusion Inference on Intel CPUs</a></li></ul></div></div></div></div>    
</body>
</html>