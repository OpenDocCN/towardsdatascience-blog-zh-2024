- en: The Meaning of Explainability for AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 解释性的重要性
- en: 原文：[https://towardsdatascience.com/the-meaning-of-explainability-for-ai-d8ae809c97fa?source=collection_archive---------3-----------------------#2024-06-04](https://towardsdatascience.com/the-meaning-of-explainability-for-ai-d8ae809c97fa?source=collection_archive---------3-----------------------#2024-06-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-meaning-of-explainability-for-ai-d8ae809c97fa?source=collection_archive---------3-----------------------#2024-06-04](https://towardsdatascience.com/the-meaning-of-explainability-for-ai-d8ae809c97fa?source=collection_archive---------3-----------------------#2024-06-04)
- en: Do we still care about how our machine learning does what it does?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们现在还关心我们的机器学习是如何做出决策的吗？
- en: '[](https://medium.com/@s.kirmer?source=post_page---byline--d8ae809c97fa--------------------------------)[![Stephanie
    Kirmer](../Images/f9d9ef9167febde974c223dd4d8d6293.png)](https://medium.com/@s.kirmer?source=post_page---byline--d8ae809c97fa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d8ae809c97fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d8ae809c97fa--------------------------------)
    [Stephanie Kirmer](https://medium.com/@s.kirmer?source=post_page---byline--d8ae809c97fa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@s.kirmer?source=post_page---byline--d8ae809c97fa--------------------------------)[![Stephanie
    Kirmer](../Images/f9d9ef9167febde974c223dd4d8d6293.png)](https://medium.com/@s.kirmer?source=post_page---byline--d8ae809c97fa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d8ae809c97fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d8ae809c97fa--------------------------------)
    [Stephanie Kirmer](https://medium.com/@s.kirmer?source=post_page---byline--d8ae809c97fa--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d8ae809c97fa--------------------------------)
    ·8 min read·Jun 4, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d8ae809c97fa--------------------------------)
    ·8分钟阅读·2024年6月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Today I want to get a bit philosophical and talk about how explainability and
    risk intersect in machine learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我想谈谈一点哲学，讨论一下机器学习中的解释性和风险如何交汇。
- en: '![](../Images/dea845c74d9370ff46f3baf0d29920fd.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dea845c74d9370ff46f3baf0d29920fd.png)'
- en: Photo by [Kenny Eliason](https://unsplash.com/@neonbrand?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Kenny Eliason](https://unsplash.com/@neonbrand?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: What do we mean by Explainability?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们所说的解释性是什么意思？
- en: In short, [explainability](https://www.researchgate.net/profile/Kai-Heinrich-3/publication/344357897_White_Grey_Black_Effects_of_XAI_Augmentation_on_the_Confidence_in_AI-based_Decision_Support_Systems/links/5f6ba89392851c14bc922907/White-Grey-Black-Effects-of-XAI-Augmentation-on-the-Confidence-in-AI-based-Decision-Support-Systems.pdf)
    in machine learning is the idea that you could explain to a human user (not necessarily
    a technically savvy one) how a model is making its decisions. A decision tree
    is an example of an easily explainable (sometimes called “white box”) model, where
    you can point to “The model divides the data between houses whose acreage is more
    than one or less than or equal to one” and so on. Other kinds of more complex
    model can be “gray box” or “black box” — increasingly difficult leading to impossible
    for a human user to understand out of the gate.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，机器学习中的[解释性](https://www.researchgate.net/profile/Kai-Heinrich-3/publication/344357897_White_Grey_Black_Effects_of_XAI_Augmentation_on_the_Confidence_in_AI-based_Decision_Support_Systems/links/5f6ba89392851c14bc922907/White-Grey-Black-Effects-of-XAI-Augmentation-on-the-Confidence-in-AI-based-Decision-Support-Systems.pdf)是指你可以向一个人类用户（不一定是技术专家）解释模型是如何做出决策的。例如，决策树就是一个容易解释的（有时被称为“白盒”）模型，你可以指出“模型将数据分为房屋面积大于1的和面积小于或等于1的”等等。其他类型的更复杂的模型可以是“灰盒”或“黑盒”——这些模型对于人类用户来说，理解起来会越来越困难，甚至是不可能的。
- en: The Old School
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 老学校
- en: A foundational lesson in my machine learning education was always that our relationship
    to models (which were usually boosted tree style models) should be, at most, “Trust,
    but verify”. When you train a model, don’t take the initial predictions at face
    value, but spend some serious time kicking the tires. Test the model’s behavior
    on very weird outliers, even when they’re unlikely to happen in the wild. Plot
    the tree itself, if it’s shallow enough. Use techniques like feature importance,
    Shapley values, and [LIME](https://arxiv.org/abs/1602.04938) to test that the
    model is making its inferences using features that correspond to your knowledge
    of the subject matter and logic. Were feature splits in a given tree aligned with
    what you know about the subject matter? When modeling physical phenomena, you
    can also compare your model’s behavior with what we know scientifically about
    how things work. Don’t just trust your model to be approaching the issues the
    right way, but check.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我在机器学习教育中的一个基础性教训是，我们与模型（通常是增强型树模型）之间的关系，最多应该是“信任，但要验证”。当你训练一个模型时，不要仅仅接受初步预测的表面结果，而是要花费一些时间去彻底检验。测试模型在非常异常的离群值上的表现，即便这些情况在现实中不太可能发生。如果树足够浅，可以绘制出树的结构。使用特征重要性、Shapley
    值和 [LIME](https://arxiv.org/abs/1602.04938) 等技术来验证模型是否使用了与你对主题和逻辑的理解相符的特征。给定树中的特征分割是否与你对该领域的了解一致？在建模物理现象时，你还可以将模型的行为与我们对事物如何运作的科学知识进行比较。不要仅仅相信你的模型在正确地处理问题，而是要进行验证。
- en: Don’t just trust your model to be approaching the issues the right way, but
    check.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不要仅仅相信你的模型在正确地处理问题，而是要进行验证。
- en: Enter Neural Networks
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入神经网络
- en: As the relevance of neural networks has exploded, the biggest tradeoff that
    we have had to consider is that this kind of explainability becomes incredibly
    difficult, and changes significantly, because of the way the architecture works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随着神经网络的重要性爆炸式增长，我们不得不考虑的最大权衡之一就是，这种可解释性变得极为困难，并且因为架构的工作方式而发生显著变化。
- en: Neural network models apply functions to the input data at each intermediate
    layer, mutating the data in myriad ways before finally passing data back out to
    the target values in the final layer. The effect of this is that, unlike splits
    of a tree based model, the intermediate layers between input and output are frequently
    not reasonably human interpretable. You may be able to find a specific node in
    some intermediate layer and look at how its value influences the output, but linking
    this back to real, concrete inputs that a human can understand will usually fail
    because of how abstracted the layers of even a simple NN are.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络模型在每一层中都对输入数据应用函数，将数据以多种方式变化，然后最终将数据传递到最终层的目标值。这种作用的结果是，与基于树的模型的分支不同，输入和输出之间的中间层通常无法合理地被人类理解。你也许可以找到某个中间层中的特定节点，并查看其值如何影响输出，但将其与人类能够理解的实际、具体输入联系起来通常会失败，因为即使是一个简单的神经网络，其层次的抽象程度也非常高。
- en: This is easily illustrated by the “husky vs wolf” problem. A convolutional neural
    network was trained to distinguish between photos of huskies and wolves, but upon
    investigation, it was discovered that the model was making choices based on the
    color of the background. Training photos of huskies were less likely to be in
    snowy settings than wolves, so any time the model received an image with a snowy
    background, it predicted a wolf would be present. The model was using information
    that the humans involved had not thought about, and developed its internal logic
    based on the wrong characteristics.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点可以通过“哈士奇与狼”问题来清楚地说明。一个卷积神经网络被训练来区分哈士奇和狼的照片，但经过调查发现，模型是根据背景的颜色做出判断的。哈士奇的训练照片比狼的训练照片更不容易出现在雪地中，因此每当模型接收到一张雪景背景的图像时，它就预测图像中会出现狼。模型使用了人类没有考虑到的信息，并基于错误的特征发展出了其内部逻辑。
- en: This means that the traditional tests of “is this model ‘thinking’ about the
    problem in a way that aligns with physical or intuited reality?” become obsolete.
    We can’t tell how the model is making its choices in that same way, but instead
    we end up relying more on trial-and-error approaches. There are systematic experimental
    strategies for this, essentially testing a model against many counterfactuals
    to determine what kinds and degrees of variation in an input will produce changes
    in an output, but this is necessarily arduous and compute intensive.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着传统的“这个模型是否‘思考’问题的方式与物理或直观现实相符？”的测试变得过时。我们不能像以前那样告诉模型是如何做出选择的，而是我们更多地依赖于试错法。为此，存在系统性的实验策略，本质上是将模型与许多反事实进行测试，以确定输入的哪些种类和程度的变化会引起输出的变化，但这必然是艰巨且计算密集型的。
- en: We can’t tell how the model is making its choices in that same way, but instead
    we end up relying more on trial-and-error approaches.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们不能像以前那样告诉模型是如何做出选择的，而是我们更多地依赖于试错法。
- en: I don’t mean to argue that efforts to understand in some part how neural networks
    do what they do are hopeless. Many scholars are very interested in [explainable
    AI, known as XAI in the literature](https://arxiv.org/pdf/2404.09554). The variations
    in the kinds of model available today mean that there are many approaches that
    we can and should pursue. Attention mechanisms are one technological advancement
    that help us understand what parts of an input the model is paying closest attention
    to/being driven by, which can be helpful. [Anthropic just released a very interesting
    report digging into interpretability for Claude, attempting to understand what
    words, phrases, or images spark the strongest activation for LLMs depending on
    the prompts using sparse autoencoders.](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)
    Tools I described above, [including Shapley](https://skirene.medium.com/demystifying-neural-nets-with-shapley-values-cca29c836089)
    [and LIME](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20images%20-%20Pytorch.ipynb),
    can be applied to some varieties of neural networks too, such as CNNs, although
    the results can be challenging to interpret. But the more we add complexity, by
    definition, the harder it will be for a human viewer or user to understand and
    interpret how the model is working.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我并不是要辩称理解神经网络如何执行其任务的努力完全没有希望。许多学者对[可解释 AI，文献中称为 XAI](https://arxiv.org/pdf/2404.09554)非常感兴趣。今天可用的各种模型意味着我们可以并且应该追求许多方法。注意力机制是一项技术进步，帮助我们理解模型在处理输入时最关注的部分/受到驱动的部分，这非常有帮助。[Anthropic
    最近发布了一份非常有趣的报告，深入探讨了 Claude 的可解释性，试图了解哪些单词、短语或图像根据提示使用稀疏自编码器能引发最大的激活。](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)
    我在上面描述的工具，[包括 Shapley](https://skirene.medium.com/demystifying-neural-nets-with-shapley-values-cca29c836089)
    [和 LIME](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20images%20-%20Pytorch.ipynb)，也可以应用于一些种类的神经网络，比如
    CNNs，尽管结果可能很难解释。但随着我们增加复杂性，按定义，人类观众或用户将更难理解和解释模型是如何工作的。
- en: Considering Randomness
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考虑随机性
- en: An additional element that is important here is to recognize that many neural
    networks incorporate randomness, so you can’t always rely on the model to return
    the same output when it sees the same input. In particular, generative AI models
    intentionally may generate different outputs from the same input, so that they
    seem more “human” or creative — we can increase or decrease the extremity of this
    variation by [tuning the “temperature”](https://medium.com/@harshit158/softmax-temperature-5492e4007f71#:~:text=Temperature%20is%20a%20hyperparameter%20of%20LSTMs%20(and%20neural%20networks%20generally,the%20logits%20before%20applying%20softmax.).
    This means that sometimes our model will choose to return not the most probabilistically
    desirable output, but something “surprising”, which enhances the creativity of
    the results.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这里一个重要的额外因素是要认识到许多神经网络包含随机性，因此你不能总是依赖模型在看到相同输入时返回相同的输出。特别是，生成性 AI 模型有意地可能会从相同输入生成不同的输出，这样它们看起来更“人性化”或富有创造性——我们可以通过[tuning
    the “temperature”](https://medium.com/@harshit158/softmax-temperature-5492e4007f71#:~:text=Temperature%20is%20a%20hyperparameter%20of%20LSTMs%20(and%20neural%20networks%20generally,the%20logits%20before%20applying%20softmax.)来增加或减少这种变化的极端性。这意味着有时我们的模型会选择返回一个不是最具概率性期望的输出，而是一些“令人惊讶”的内容，这增强了结果的创造性。
- en: In these circumstances, we can still do some amount of the trial-and-error approach
    to try and develop our understanding of what the model is doing and why, but it
    becomes exponentially more complex. Instead of the only change to the equation
    being a different input, now we have changes in the input plus an unknown variability
    due to randomness. Did your change of input change the response, or was that the
    result of randomness? It’s often impossible to truly know.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们仍然可以通过试错的方法来尽量发展对模型行为及其原因的理解，但这变得复杂得多。方程式中的唯一变化不再是不同的输入，而是输入的变化加上由于随机性引起的未知变动。你的输入变化是否改变了响应，还是那是随机性的结果？我们常常无法真正知道。
- en: Did your change of input change the response, or was that the result of randomness?
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你的输入变化是否改变了响应，还是那是随机性的结果？
- en: Real World Implications
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现实世界的影响
- en: So, where does this leave us? Why do we want to know how the model did its inference
    in the first place? Why does that matter to us as machine learning developers
    and users of models?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这把我们带到哪里呢？我们为什么要知道模型是如何进行推理的？作为机器学习开发者和模型使用者，这对我们来说有什么意义？
- en: If we build machine learning that will help us make choices and shape people’s
    behaviors, then the accountability for results needs to fall on us. Sometimes
    model predictions go through a human mediator before they are applied to our world,
    but increasingly we’re seeing models being set loose and inferences in production
    being used with no further review. The general public has more unmediated access
    to machine learning models of huge complexity than ever before.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们构建的机器学习模型将帮助我们做出决策并塑造人们的行为，那么结果的责任就应该由我们承担。有时，模型预测会经过人工调解之后再应用于我们的世界，但我们越来越多地看到模型被放开，生产中的推理结果被直接使用而没有进一步审查。公众现在比以往任何时候都更容易接触到这些极其复杂的机器学习模型。
- en: To me, therefore, understanding how and why the model does what it does is due
    diligence just like testing to make sure a manufactured toy doesn’t have lead
    paint on it, or a piece of machinery won’t snap under normal use and break someone’s
    hand. It’s a lot harder to test that, but ensuring I’m not releasing a product
    into the world that makes life worse is a moral stance I’m committed to. If you
    are building a machine learning model, you are responsible for what that model
    does and what effect that model has on people and the world. As a result, to feel
    really confident that your model is safe to use, you need some level of understanding
    about how and why it returns the outputs it does.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对我来说，理解模型如何以及为什么做出它所做的决定，就像测试一个制造的玩具是否没有铅漆，或者一台机器是否在正常使用下不会断裂并伤害到人的手一样，是一种尽职调查。这要比测试更难，但确保我不会将一个使生活更糟的产品推向市场，是我坚定的道德立场。如果你正在构建一个机器学习模型，你就有责任对该模型所做的事以及它对人们和世界的影响负责。因此，要真正有信心你的模型是安全的，你需要一定程度的理解，了解它是如何以及为什么返回它的输出的。
- en: If you are building a machine learning model, you are responsible for what that
    model does and what effect that model has on people and the world.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你正在构建一个机器学习模型，你就有责任对该模型所做的事以及它对人们和世界的影响负责。
- en: As an aside, readers might remember from [my article about the EU AI Act](https://medium.com/towards-data-science/uncovering-the-eu-ai-act-22b10f946174)
    that there are requirements that model predictions be subject to human oversight
    and that they not make decisions with discriminatory effect based on protected
    characteristics. So even if you don’t feel compelled by the moral argument, for
    many of us there is a legal motivation as well.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，读者可能还记得我之前关于[欧盟人工智能法案](https://medium.com/towards-data-science/uncovering-the-eu-ai-act-22b10f946174)的文章，其中提到模型的预测需要经过人工监督，并且不得根据受保护的特征做出具有歧视效应的决策。所以，即使你不被道德论点所打动，对于我们中的许多人来说，法律的动机也是存在的。
- en: Even when we use neural networks, we can still use tools to better understand
    how our model is making choices — we just need to take the time and do the work
    to get there.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们使用神经网络，我们仍然可以使用工具更好地理解我们的模型是如何做出决策的——我们只需要花时间并努力去做到这一点。
- en: But, Progress?
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 但是，进展呢？
- en: Philosophically, we could (and people do) argue that advancements in machine
    learning past a basic level of sophistication require giving up our desire to
    understand it all. This may be true! But we shouldn’t ignore the tradeoffs this
    creates and the risks we accept. Best case, your generative AI model will mainly
    do what you expect (perhaps if you keep the temperature in check, and your model
    is very uncreative) and not do a whole lot of unexpected stuff, or worst case
    you unleash a disaster because the model reacts in ways you had no idea would
    happen. This could mean you look silly, or it could mean the end of your business,
    or it could mean real physical harm to people. When you accept that model explainability
    is unachievable, these are the kind of risks you are taking on your own shoulders.
    You can’t say “oh, models gonna model” when you built this thing and made the
    conscious decision to release it or use its predictions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从哲学角度来看，我们可以（而且人们确实这么做）辩称，机器学习技术达到一定复杂度后，需要放弃完全理解它的愿望。这可能是对的！但我们不应忽视由此带来的权衡和我们所接受的风险。最好的情况是，你的生成性AI模型主要按照预期运行（也许如果你控制了温度，并且你的模型非常缺乏创意），不会做出太多意外的事情；而最坏的情况是，你释放了一个灾难，因为模型以你完全没有预料到的方式做出反应。这可能意味着你看起来很傻，或者可能意味着你的生意结束，或者可能意味着对人们造成真正的身体伤害。当你接受模型的可解释性是无法实现的时，这就是你自己肩负的风险。你不能说“哦，模型就是这样”——当你建造了这个东西并做出了释放它或使用其预测的有意识决定时。
- en: Various tech companies both large and small have accepted that generative AI
    will sometimes produce incorrect, dangerous, discriminatory, and otherwise harmful
    results, and decided that this is worth it for the perceived benefits — we know
    this because generative AI models that routinely behave in undesirable ways have
    been released to the general public. Personally, it bothers me that the tech industry
    has chosen, without any clear consideration or conversation, to subject the public
    to that kind of risk, but the genie is out of the bottle.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 各大大小小的科技公司都已接受生成性AI有时会产生不正确、危险、歧视性以及其他有害的结果，并认为为了获得其感知的好处，这些风险是值得的——我们知道这一点，因为那些经常表现出不良行为的生成性AI模型已经被公开发布。我个人很困扰的是，科技行业在没有任何明确考虑或讨论的情况下，选择将公众置于这种风险之中，但魔
    genie 已经放出来了。
- en: Now what?
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在怎么办？
- en: To me, it seems like pursuing XAI and trying to get it up to speed with the
    advancement of generative AI is a noble goal, but I don’t think we’re going to
    see a point where most people can easily understand how these models do what they
    do, just because the architectures are so complicated and challenging. As a result,
    I think we also need to implement risk mitigation, ensuring that those responsible
    for the increasingly sophisticated models that are affecting our lives on a daily
    basis are accountable for these products and their safety. Because the outcomes
    are so often unpredictable, we need frameworks to protect our communities from
    the worst case scenarios.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，追求XAI并试图让它跟上生成性AI的进步是一个崇高的目标，但我认为我们不会看到大多数人能够轻松理解这些模型是如何运作的，因为它们的架构非常复杂且具有挑战性。因此，我认为我们还需要实施风险缓解措施，确保那些负责日益复杂的模型的人，对这些影响我们日常生活的产品及其安全负责。由于结果往往是不可预测的，我们需要框架来保护我们的社区免受最坏情况的影响。
- en: We shouldn’t regard all risk as untenable, but we need to be clear-eyed about
    the fact that risk exists, and that the challenges of explainability for the cutting
    edge of AI mean that risk of machine learning is harder to measure and anticipate
    than ever before. The only responsible choice is to balance this risk against
    the real benefits these models generate (not taking as a given the projected or
    promised benefits of some future version), and make thoughtful decisions accordingly.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应把所有的风险都视为无法承受，但我们需要清醒地认识到风险的存在，并且由于AI前沿的可解释性挑战，机器学习的风险比以往任何时候都更难以衡量和预见。唯一负责任的选择是将这种风险与这些模型所带来的实际利益进行平衡（而不是将某些未来版本的预期或承诺利益视为理所当然），并据此做出深思熟虑的决策。
- en: Read more of my work at [www.stephaniekirmer.com](http://www.stephaniekirmer.com).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读更多我的作品，请访问 [www.stephaniekirmer.com](http://www.stephaniekirmer.com)。
- en: Further Reading
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)
    (May 21, 2024, Anthropic team)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[扩展单一语义性：从Claude 3 Sonnet中提取可解释特征](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)（2024年5月21日，Anthropic团队）'
- en: '[Explainable Generative AI: A Survey, Conceptualization, and Research Agenda](https://arxiv.org/pdf/2404.09554)
    (April 15, 2024; Johannes Schneider) — this one is a really accessible read, I
    recommend it.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可解释的生成型人工智能：调查、概念化与研究议程](https://arxiv.org/pdf/2404.09554)（2024年4月15日；Johannes
    Schneider）——这篇文章内容非常易读，推荐大家阅读。'
- en: '[An analysis of explainability methods for convolutional neural networks](https://www.sciencedirect.com/science/article/pii/S0952197622005966)
    (January 2023, Von der Haar et al)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[卷积神经网络可解释性方法分析](https://www.sciencedirect.com/science/article/pii/S0952197622005966)（2023年1月，Von
    der Haar等人）'
- en: '[Explainable Convolutional Neural Networks: A Taxonomy, Review, and Future
    Directions](https://dl.acm.org/doi/full/10.1145/3563691) (Feb 2, 2023; Ibrahim
    et al)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可解释的卷积神经网络：分类法、回顾与未来方向](https://dl.acm.org/doi/full/10.1145/3563691)（2023年2月2日；Ibrahim等人）'
- en: '[Google’s AI tells users to add glue to their pizza, eat rocks and make chlorine
    gas](https://www.livescience.com/technology/artificial-intelligence/googles-ai-tells-users-to-add-glue-to-their-pizza-eat-rocks-and-make-chlorine-gas)
    (May 23, 2024)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[谷歌的人工智能告诉用户往比萨饼上加胶水、吃石头并制造氯气](https://www.livescience.com/technology/artificial-intelligence/googles-ai-tells-users-to-add-glue-to-their-pizza-eat-rocks-and-make-chlorine-gas)（2024年5月23日）'
