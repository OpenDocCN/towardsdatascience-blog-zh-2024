<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Explainable Generic ML Pipeline with MLflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Explainable Generic ML Pipeline with MLflow</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explainable-generic-ml-pipeline-with-mlflow-2494ca1b3f96?source=collection_archive---------5-----------------------#2024-11-26">https://towardsdatascience.com/explainable-generic-ml-pipeline-with-mlflow-2494ca1b3f96?source=collection_archive---------5-----------------------#2024-11-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e1b0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">An end-to-end demo to wrap a pre-processor and explainer into an algorithm-agnostic ML pipeline with <code class="cx hd he hf hg b">mlflow.pyfunc</code></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hh hi hj hk hl ab"><div><div class="ab hm"><div><div class="bm" aria-hidden="false"><a href="https://menawang.medium.com/?source=post_page---byline--2494ca1b3f96--------------------------------" rel="noopener follow"><div class="l hn ho by hp hq"><div class="l ed"><img alt="Mena Wang, PhD" class="l ep by dd de cx" src="../Images/eac9fa55026f9fc119bc868439ff311b.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*qUfWZCdwnb6Bn7k4cO0O3Q.jpeg"/><div class="hr by l dd de em n hs eo"/></div></div></a></div></div><div class="ht ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2494ca1b3f96--------------------------------" rel="noopener follow"><div class="l hu hv by hp hw"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hx cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hr by l br hx em n hs eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hy ab q"><div class="ab q hz"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ia ib bk"><a class="af ag ah ai aj ak al am an ao ap aq ar ic" data-testid="authorName" href="https://menawang.medium.com/?source=post_page---byline--2494ca1b3f96--------------------------------" rel="noopener follow">Mena Wang, PhD</a></p></div></div></div><div class="id ie l"><div class="ab if"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ig ih" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ia ib dx"><button class="ii ij ah ai aj ak al am an ao ap aq ar ik il im" disabled="">Follow</button></p></div></div></span></div></div><div class="l in"><span class="bf b bg z dx"><div class="ab cn io ip iq"><div class="ir is ab"><div class="bf b bg z dx ab it"><span class="iu l in">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar ic ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2494ca1b3f96--------------------------------" rel="noopener follow"><p class="bf b bg z iv iw ix iy iz ja jb jc bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ig ih" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="jd je l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju"><div class="h k w ea eb q"><div class="kk l"><div class="ab q kl km"><div class="pw-multi-vote-icon ed iu kn ko kp"><div class=""><div class="kq kr ks kt ku kv kw am kx ky kz kp"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l la lb lc ld le lf lg"><p class="bf b dy z dx"><span class="kr">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kq lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="ll"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj"><div class="lm k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ln an ao ap ik lo lp lq" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lr cn"><div class="l ae"><div class="ab cb"><div class="ls lt lu lv lw lx ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ln an ao ap ik ly lz lk ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ln an ao ap ik ly lz lk ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ln an ao ap ik ly lz lk ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq mr"><img src="../Images/718ec8036048d1449b127442c59434ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rxeyWMHZrwJBbfRVQMv_JA.jpeg"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Photo by <a class="af ni" href="https://unsplash.com/@hannahj236?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Hannah Murrell</a> on <a class="af ni" href="https://unsplash.com/photos/person-holding-ball-focus-on-tree-pTfdcT0hxGc?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="cf1b" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk"><strong class="al">Intro</strong></h1><p id="ab1a" class="pw-post-body-paragraph of og fq oh b go oi oj ok gr ol om on oo op oq or os ot ou ov ow ox oy oz pa fj bk">One common challenge in MLOps is the hassle of migrating between various algorithms or frameworks. To tackle the challenge, this is my second article on the topic of generic model building using <code class="cx hd he hf hg b">mlflow.pyfunc</code>.</p><p id="4fbc" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">In my previous article, I offered a beginner-friendly step-by-step demo on creating a minimalist algorithm-agnostic model wrapper.</p><div class="pg ph pi pj pk pl"><a rel="noopener follow" target="_blank" href="/algorithm-agnostic-model-building-with-mlflow-b106a5a29535?source=post_page-----2494ca1b3f96--------------------------------"><div class="pm ab in"><div class="pn ab co cb po pp"><h2 class="bf fr ia z iv pq ix iy pr ja jc fp bk">Algorithm-Agnostic Model Building with MLflow</h2><div class="ps l"><h3 class="bf b ia z iv pq ix iy pr ja jc dx">A beginner-friendly step-by-step guide to creating generic ML pipelines using mlflow.pyfunc</h3></div><div class="pt l"><p class="bf b dy z iv pq ix iy pr ja jc dx">towardsdatascience.com</p></div></div><div class="pu l"><div class="pv l pw px py pu pz lx pl"/></div></div></a></div><p id="2ecc" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">To further our journey, by the end of this article, we will build a much more sophisticated ML pipeline with the below functionalities:</p><ol class=""><li id="843c" class="of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa qa qb qc bk">This pipeline supports both classification (binary) and regression tasks. It works with scikit-learn models and other algorithms that follow the scikit-learn interface (i.e., fit, predict/predict_proba).</li><li id="8a2d" class="of og fq oh b go qd oj ok gr qe om on oo qf oq or os qg ou ov ow qh oy oz pa qa qb qc bk">Incorporating a fully functional <code class="cx hd he hf hg b">Pre-Processor</code> that can be fitted on train data and then used to transform new data for model consumption. This pre-processor can handle both numeric and categorical features and handle missing values with various imputation strategies.</li><li id="f5c1" class="of og fq oh b go qd oj ok gr qe om on oo qf oq or os qg ou ov ow qh oy oz pa qa qb qc bk">Adding an <code class="cx hd he hf hg b">explainer</code> to shed light on the model’s reasoning, which is invaluable for model selection, monitoring and implementation. This task can be tricky due to the varying implementations of SHAP values across different ML algorithms. But, all good, we will address the challenge in this article. 😎</li></ol><p id="8d36" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">Consistent with the previous article,</p><ol class=""><li id="26ef" class="of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa qa qb qc bk">You will see how easy it is to switch between different customized pre-processors, similar to switching between various ML algorithms.</li><li id="d49a" class="of og fq oh b go qd oj ok gr qe om on oo qf oq or os qg ou ov ow qh oy oz pa qa qb qc bk">This ML pipeline then encapsulates any customized pipeline elements under the hood, yet still offers a unified model representation in <code class="cx hd he hf hg b">pyfunc</code> flavour to simplify model deployment, redeployment, and downstream scoring.</li></ol><p id="ce8d" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">🔗 All code and config are available <a class="af ni" href="https://github.com/MenaWANG/mlflow-demo/blob/main/pyfunc_pipeline.ipynb" rel="noopener ugc nofollow" target="_blank">on GitHub</a>. 🧰</p></div></div></div><div class="ab cb qi qj qk ql" role="separator"><span class="qm by bm qn qo qp"/><span class="qm by bm qn qo qp"/><span class="qm by bm qn qo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="16ad" class="nj nk fq bf nl nm qq gq no np qr gt nr ns qs nu nv nw qt ny nz oa qu oc od oe bk"><strong class="al">The Pre-Processor (V1)</strong></h1><p id="9ad6" class="pw-post-body-paragraph of og fq oh b go oi oj ok gr ol om on oo op oq or os ot ou ov ow ox oy oz pa fj bk">Many machine learning algorithms — such as linear models (e.g., linear regression, SVM), distance-based models (e.g., KNN, PCA), and gradient-based models (e.g., gradient boosting methods or gradient descent optimization) — tend to perform better with scaled input features, because scaling prevents features with larger ranges from dominating the learning process. Additionally, real-world data often contains missing values. Therefore, in this first iteration, we will build a pre-processor that can be trained to scale new data and impute missing values, preparing it for model consumption.</p><p id="7cb6" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">Once this pre-processor is built, I will then demo how to easily plug it into <code class="cx hd he hf hg b">pyfunc</code> ML pipeline. Sounds good? Let’s go. 🤠</p><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="f517" class="qy nk fq hg b bg qz ra l rb rc">class PreProcessor(BaseEstimator, TransformerMixin):<br/>    """<br/>    Custom preprocessor for numeric features.<br/>    <br/>    - Handles scaling of numeric data<br/>    - Performs imputation of missing values<br/>    <br/>    Attributes:<br/>        transformer (Pipeline): Pipeline for numeric preprocessing<br/>        features (List[str]): Names of input features<br/>    """<br/><br/>    def __init__(self):<br/>        """<br/>        Initialize preprocessor.<br/>        <br/>        - Creates placeholder for transformer pipeline<br/>        """<br/>        self.transformer = None<br/><br/>    def fit(self, X, y=None):<br/>        """<br/>        Fits the transformer on the provided dataset.<br/>        <br/>        - Configures scaling for numeric features<br/>        - Sets up imputation for missing values<br/>        - Stores feature names for later use<br/><br/>        Parameters:<br/>            X (pd.DataFrame): The input features to fit the transformer.<br/>            y (pd.Series, optional): Target variable, not used in this method.<br/>        <br/>        Returns:<br/>            PreProcessor: The fitted transformer instance.<br/>        """<br/>        self.features = X.columns.tolist()<br/><br/>        if self.features:<br/>            self.transformer = Pipeline(steps=[<br/>                ('imputer', SimpleImputer(strategy='median')),<br/>                ('scaler', StandardScaler())<br/>            ])<br/>            self.transformer.fit(X[self.features])<br/><br/>        return self<br/><br/>    def transform(self, X):<br/>        """<br/>        Transform input data using fitted pipeline.<br/>        <br/>        - Applies scaling to numeric features<br/>        - Handles missing values through imputation<br/>        <br/>        Parameters:<br/>            X (pd.DataFrame): Input features to transform<br/>        <br/>        Returns:<br/>            pd.DataFrame: Transformed data with scaled and imputed features<br/>        """<br/>        X_transformed = pd.DataFrame()<br/><br/>        if self.features:<br/>            transformed_data = self.transformer.transform(X[self.features])<br/>            X_transformed[self.features] = transformed_data<br/><br/>        X_transformed.index = X.index<br/><br/>        return X_transformed<br/><br/>    def fit_transform(self, X, y=None):<br/>        """<br/>        Fits the transformer on the input data and then transforms it.<br/><br/>        Parameters:<br/>            X (pd.DataFrame): The input features to fit and transform.<br/>            y (pd.Series, optional): Target variable, not used in this method.<br/>        <br/>        Returns:<br/>            pd.DataFrame: The transformed data.<br/>        """<br/>        self.fit(X, y)<br/>        return self.transform(X)</span></pre><p id="64d6" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">This pre-processor can be fitted on train data and then used to process any new data. It will become an element in the ML pipeline below, but of course, we can use or test it independently. Let’s create a synthetic dataset and use the pre-processor to transform it.</p><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="1567" class="qy nk fq hg b bg qz ra l rb rc"># Set parameters for synthetic data<br/>n_feature = 10<br/>n_inform = 4<br/>n_redundant = 0<br/>n_samples = 1000<br/><br/># Generate synthetic classification data<br/>X, y = make_classification(<br/>    n_samples=n_samples,<br/>    n_features=n_feature,<br/>    n_informative=n_inform,<br/>    n_redundant=n_redundant,<br/>    shuffle=False,<br/>    random_state=12<br/>)<br/><br/># Create feature names<br/>feat_names = [f'inf_{i+1}' for i in range(n_inform)] + \<br/>            [f'rand_{i+1}' for i in range(n_feature - n_inform)]<br/><br/># Convert to DataFrame with named features<br/>X = pd.DataFrame(X, columns=feat_names)<br/><br/># Split data into train and test sets<br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    X, y,<br/>    test_size=0.2,<br/>    random_state=22<br/>)</span></pre><p id="0b53" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">Below are screenshots from {sweetViz} reports before vs after scaling; you can see that scaling didn’t change the underlying shape of each feature’s distribution but simply rescaled and shifted it. BTW, it takes two lines to generate a pretty comprehensive EDA report with {sweetViz}, code available in the GitHub repo linked above. 🥂</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq rd"><img src="../Images/7a1b3029a4739afeff34a9d50170c3d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gBkIhQn3geqkBaTCqcbArw.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Screenshots from SweetViz reports before vs after preprocessing</figcaption></figure><h1 id="075e" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk"><strong class="al">ML Pipeline with Pre-Processor</strong></h1><p id="0b99" class="pw-post-body-paragraph of og fq oh b go oi oj ok gr ol om on oo op oq or os ot ou ov ow ox oy oz pa fj bk">Now, let's create an ML pipeline in the <code class="cx hd he hf hg b">mlflow.pyfunc</code> flavour that can encapsulate this preprocessor.</p><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="c15a" class="qy nk fq hg b bg qz ra l rb rc">class ML_PIPELINE(mlflow.pyfunc.PythonModel):<br/>    """<br/>    Custom ML pipeline for classification and regression.<br/>    <br/>    - work with any scikit-learn compatible model<br/>    - Combines preprocessing and model training<br/>    - Handles model predictions<br/>    - Compatible with MLflow tracking<br/>    - Supports MLflow deployment<br/><br/>    Attributes:<br/>        model (BaseEstimator or None): A scikit-learn compatible model instance<br/>        preprocessor (Any or None): Data preprocessing pipeline<br/>        config (Any or None): Optional config for model settings <br/>        task(str): Type of ML task ('classification' or 'regression')<br/>    """<br/><br/>    def __init__(self, model=None, preprocessor=None, config=None):<br/>        """<br/>        Initialize the ML_PIPELINE.<br/>        <br/>        Parameters:<br/>            model (BaseEstimator, optional): <br/>                - Scikit-learn compatible model<br/>                - Defaults to None<br/>            <br/>            preprocessor (Any, optional):<br/>                - Transformer or pipeline for data preprocessing<br/>                - Defaults to None<br/>            <br/>            config (Any, optional):<br/>                - Additional model settings<br/>                - Defaults to None<br/>        """<br/>        self.model = model<br/>        self.preprocessor = preprocessor<br/>        self.config = config<br/>        self.task = "classification" if hasattr(self.model, "predict_proba") else "regression"<br/><br/>    def fit(self, X_train: pd.DataFrame, y_train: pd.Series):<br/>        """<br/>        Train the model on provided data.<br/>        <br/>        - Applies preprocessing to features<br/>        - Fits model on transformed data<br/>        <br/>        Parameters:<br/>            X_train (pd.DataFrame): Training features<br/>            y_train (pd.Series): Target values<br/>        """<br/>        X_train_preprocessed = self.preprocessor.fit_transform(X_train.copy())<br/>        self.model.fit(X_train_preprocessed, y_train)<br/><br/>    def predict(<br/>            self, context: Any, model_input: pd.DataFrame<br/>            ) -&gt; np.ndarray:<br/>        """<br/>        Generate predictions using trained model.<br/>        <br/>        - Applies preprocessing to new data<br/>        - Uses model to make predictions<br/>        <br/>        Parameters:<br/>            context (Any): Optional context information provided <br/>                by MLflow during the prediction phase<br/>            model_input (pd.DataFrame): Input features<br/>        <br/>        Returns:<br/>            Any: Model predictions or probabilities<br/>        """<br/>        processed_model_input = self.preprocessor.transform(model_input.copy())<br/>        if self.task == "classification":<br/>            prediction = self.model.predict_proba(processed_model_input)[:,1]<br/>        elif self.task == "regression":<br/>            prediction = self.model.predict(processed_model_input)<br/>        return prediction</span></pre><p id="238f" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">The ML pipeline defined above takes the preprocessor and ML algorithm as parameters. Usage example below</p><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="6f3b" class="qy nk fq hg b bg qz ra l rb rc"># define the ML pipeline instance with lightGBM classifier<br/>ml_pipeline = ML_PIPELINE(model = lgb.LGBMClassifier(),<br/>                          preprocessor = PreProcessor())</span></pre><p id="fa34" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">It is as simple as that! 🎉 If you want to experiment with another algorithm, just swap it like shown below. As a wrapper, it can encapsulate both regression and classification algorithms. For the latter, predicted probabilities are returned, as shown in the example above.</p><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="495a" class="qy nk fq hg b bg qz ra l rb rc"># define the ML pipeline instance with random forest regressor<br/>ml_pipeline = ML_PIPELINE(model = RandomForestRegressor(),<br/>                          preprocessor = PreProcessor())</span></pre><p id="5503" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">As you can see from the code chunk below, passing hyperparameters to the algorithms is easy, making this ML pipeline a perfect instrument for hyperparameter tuning. I will elaborate on this topic in the following articles.</p><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="1415" class="qy nk fq hg b bg qz ra l rb rc">params = {<br/>    'n_estimators': 100,<br/>    'max_depth': 6,<br/>    'learning_rate': 0.1 <br/>}<br/>model = xgb.XGBClassifier(**params)<br/>ml_pipeline = ML_PIPELINE(model = model,<br/>                          preprocessor = PreProcessor())</span></pre><p id="9300" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">Because this ml pipeline is built in the <code class="cx hd he hf hg b">mlflow.pyfunc</code> flavour. We can log it with rich metadata saved automatically by <code class="cx hd he hf hg b">mlflow</code> for downstream use. When deployed, we can feed the metadata as <code class="cx hd he hf hg b">context</code> for the model in the <code class="cx hd he hf hg b">predict</code> function as shown below. More info and demos are available in my previous article, which is linked at the beginning.</p><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="29e8" class="qy nk fq hg b bg qz ra l rb rc"># train the ML pipeline<br/>ml_pipeline.fit(X_train, y_train)<br/><br/># use the trained pipeline for prediction<br/>y_prob = ml_pipeline.predict(<br/>    context=None, # provide metadata for model in production<br/>    model_input=X_test<br/>)<br/>auc = roc_auc_score(y_test, y_prob)<br/>print(f"auc: {auc:.3f}")</span></pre><h1 id="949b" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Pre-Processor (V2)</h1><p id="b96c" class="pw-post-body-paragraph of og fq oh b go oi oj ok gr ol om on oo op oq or os ot ou ov ow ox oy oz pa fj bk">The above pre-processor has worked well so far, but let’s improve it in two ways below and then demonstrate how to swap between pre-processors easily.</p><ol class=""><li id="57c1" class="of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa qa qb qc bk">Allow users to customize the pre-processing process. For instance, to specify the impute strategy.</li><li id="9df8" class="of og fq oh b go qd oj ok gr qe om on oo qf oq or os qg ou ov ow qh oy oz pa qa qb qc bk">Expand pre-processor capacity to handle categorical features.</li></ol><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="c382" class="qy nk fq hg b bg qz ra l rb rc">    class PreProcessor_v2(BaseEstimator, TransformerMixin):<br/>    """<br/>    Custom transformer for data preprocessing.<br/>    <br/>    - Scales numeric features<br/>    - Encodes categorical features<br/>    - Handles missing values via imputation<br/>    - Compatible with scikit-learn pipeline<br/>    <br/>    Attributes:<br/>        num_impute_strategy (str): Numeric imputation strategy<br/>        cat_impute_strategy (str): Categorical imputation strategy<br/>        num_transformer (Pipeline): Numeric preprocessing pipeline<br/>        cat_transformer (Pipeline): Categorical preprocessing pipeline<br/>        transformed_cat_cols (List[str]): One-hot encoded column names<br/>        num_features (List[str]): Numeric feature names<br/>        cat_features (List[str]): Categorical feature names<br/>    """<br/><br/>    def __init__(self, num_impute_strategy='median', <br/>                 cat_impute_strategy='most_frequent'):<br/>        """<br/>        Initialize the transformer.<br/>        <br/>        - Sets up numeric data transformer<br/>        - Sets up categorical data transformer<br/>        - Configures imputation strategies<br/>        <br/>        Parameters:<br/>            num_impute_strategy (str): Strategy for numeric missing values<br/>            cat_impute_strategy (str): Strategy for categorical missing values<br/>        """<br/>        self.num_impute_strategy = num_impute_strategy<br/>        self.cat_impute_strategy = cat_impute_strategy<br/><br/>    def fit(self, X, y=None):<br/>        """<br/>        Fit transformer on input data.<br/>        <br/>        - Identifies feature types<br/>        - Configures feature scaling<br/>        - Sets up encoding<br/>        - Fits imputation strategies<br/>        <br/>        Parameters:<br/>            X (pd.DataFrame): Input features<br/>            y (pd.Series, optional): Target variable, not used<br/>        <br/>        Returns:<br/>            CustomTransformer: Fitted transformer<br/>        """<br/>        self.num_features = X.select_dtypes(include=np.number).columns.tolist()<br/>        self.cat_features = X.select_dtypes(exclude=np.number).columns.tolist()<br/><br/>        if self.num_features:<br/>            self.num_transformer = Pipeline(steps=[<br/>                ('imputer', SimpleImputer(strategy=self.num_impute_strategy)),<br/>                ('scaler', StandardScaler())<br/>            ])<br/>            self.num_transformer.fit(X[self.num_features])<br/>        <br/>        if self.cat_features:<br/>            self.cat_transformer = Pipeline(steps=[<br/>                ('imputer', SimpleImputer(strategy=self.cat_impute_strategy)),<br/>                ('encoder', OneHotEncoder(handle_unknown='ignore'))<br/>            ])<br/>            self.cat_transformer.fit(X[self.cat_features])<br/>        <br/>        return self<br/><br/>    def get_transformed_cat_cols(self):<br/>        """<br/>        Get transformed categorical column names.<br/>        <br/>        - Creates names after one-hot encoding<br/>        - Combines category with encoded values<br/>        <br/>        Returns:<br/>            List[str]: One-hot encoded column names<br/>        """<br/>        cat_cols = []<br/>        cats = self.cat_features<br/>        cat_values = self.cat_transformer['encoder'].categories_<br/>        for cat, values in zip(cats, cat_values):<br/>            cat_cols += [f'{cat}_{value}' for value in values]<br/>        <br/>        return cat_cols<br/><br/>    def transform(self, X):<br/>        """<br/>        Transform input data.<br/>        <br/>        - Applies fitted scaling<br/>        - Applies fitted encoding<br/>        - Handles numeric and categorical features<br/>        <br/>        Parameters:<br/>            X (pd.DataFrame): Input features<br/>        <br/>        Returns:<br/>            pd.DataFrame: Transformed data<br/>        """<br/>        X_transformed = pd.DataFrame()<br/><br/>        if self.num_features:<br/>            transformed_num_data = self.num_transformer.transform(X[self.num_features])<br/>            X_transformed[self.num_features] = transformed_num_data<br/>        <br/>        if self.cat_features:<br/>            transformed_cat_data = self.cat_transformer.transform(X[self.cat_features]).toarray()<br/>            self.transformed_cat_cols = self.get_transformed_cat_cols()<br/>            transformed_cat_df = pd.DataFrame(transformed_cat_data, columns=self.transformed_cat_cols)<br/>            X_transformed = pd.concat([X_transformed, transformed_cat_df], axis=1)<br/>        <br/>        X_transformed.index = X.index<br/><br/>        return X_transformed<br/><br/>    def fit_transform(self, X, y=None):<br/>        """<br/>        Fit and transform input data.<br/>        <br/>        - Fits transformer to data<br/>        - Applies transformation<br/>        - Combines both operations<br/>        <br/>        Parameters:<br/>            X (pd.DataFrame): Input features<br/>            y (pd.Series, optional): Target variable, not used<br/>        <br/>        Returns:<br/>            pd.DataFrame: Transformed data<br/>        """<br/>        self.fit(X, y)<br/>        return self.transform(X)</span></pre><h1 id="c4b0" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk"><strong class="al">Easy Switch of Custom Pre-Processors</strong></h1><p id="7492" class="pw-post-body-paragraph of og fq oh b go oi oj ok gr ol om on oo op oq or os ot ou ov ow ox oy oz pa fj bk">There you have it: a new preprocessor that is 1) more customizable and 2) handles both numerical and categorical features. Let’s define an ML pipeline instance with it.</p><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="cff3" class="qy nk fq hg b bg qz ra l rb rc"># Define a PreProcessor (V2) instance while specifying impute strategy<br/>preprocessor = PreProcessor_v2(<br/>    num_impute_strategy = 'mean'<br/>)<br/># Define an ML Pipeline instance with this preprocessor<br/>ml_pipeline = ML_PIPELINE(<br/>    model = xgb.XGBClassifier(), # switch ML algorithms<br/>    preprocessor = PreProcessor # switch pre-processors <br/>)</span></pre><p id="40b7" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">Let’s test this new ML pipeline instance with another synthetic dataset containing both numerical and categorical features.</p><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="4330" class="qy nk fq hg b bg qz ra l rb rc"># add missings<br/>np.random.seed(42) <br/>missing_rate = 0.20 <br/>n_missing = int(np.floor(missing_rate * X.size))<br/>rows = np.random.randint(0, X.shape[0], n_missing)<br/>cols = np.random.randint(0, X.shape[1], n_missing)<br/>X.values[rows, cols] = np.nan<br/>actual_missing_rate = X.isna().sum().sum() / X.size<br/>print(f"Target missing rate: {missing_rate:.2%}")<br/>print(f"Actual missing rate: {actual_missing_rate:.2%}")<br/><br/># change X['inf_1] to categorical<br/>percentiles = [0, 0.1, 0.5, 0.9, 1]<br/>labels = ['bottom', 'lower-mid', 'upper-mid', 'top']<br/>X['inf_1'] = pd.qcut(X['inf_1'], q=percentiles, labels=labels)</span></pre><p id="bb05" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">There you have it—the ML pipeline runs smoothly with the new data. As expected, however, if we define the ML pipeline with the previous preprocessor and then run it on this dataset, we will encounter errors because the previous preprocessor was not designed to handle categorical features.</p><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="77a8" class="qy nk fq hg b bg qz ra l rb rc"># create an ML pipeline instance with PreProcessor v1<br/>ml_pipeline = ML_PIPELINE(<br/>    model = lgb.LGBMClassifier(verbose = -1), <br/>    preprocessor = PreProcessor()<br/>)<br/><br/>try:<br/>    ml_pipeline.fit(X_train, y_train)<br/>except Exception as e:<br/>    print(f"Error: {e}")</span></pre><pre class="re qv hg qw bp qx bb bk"><span id="b510" class="qy nk fq hg b bg qz ra l rb rc">Error: Cannot use median strategy with non-numeric data:<br/>could not convert string to float: 'lower-mid'</span></pre><h1 id="95ac" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">The Benefit of An Explainable ML Pipeline</h1><p id="1f58" class="pw-post-body-paragraph of og fq oh b go oi oj ok gr ol om on oo op oq or os ot ou ov ow ox oy oz pa fj bk">Adding an explainer to an ML pipeline can be super helpful in several ways:</p><ol class=""><li id="647f" class="of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa qa qb qc bk"><strong class="oh fr">Model Selection</strong>: It helps us select the best model by evaluating the soundness of its reasoning. Two algorithms may perform similarly on metrics like AUC or precision, but the key features they rely on may differ. Reviewing model reasoning with domain experts to discuss which model makes more sense in such scenarios is a good idea.</li><li id="c0a6" class="of og fq oh b go qd oj ok gr qe om on oo qf oq or os qg ou ov ow qh oy oz pa qa qb qc bk"><strong class="oh fr">Troubleshooting</strong>: One helpful strategy for model improvement is to analyze the reasoning behind mistakes. For example, in classification problems, we can identify false positives where the model was most confident (i.e., produced the highest predicted possibilities) and investigate what went wrong in the reasoning and what key features contributed to the mistakes.</li><li id="0a91" class="of og fq oh b go qd oj ok gr qe om on oo qf oq or os qg ou ov ow qh oy oz pa qa qb qc bk"><strong class="oh fr">Model Monitoring</strong>: Besides the typical monitoring elements such as data drift and performance metrics, it is informative to monitor model reasoning as well. If there is a significant shift in key features that drive the decisions made by a model in production, I want to be alerted.</li><li id="2e23" class="of og fq oh b go qd oj ok gr qe om on oo qf oq or os qg ou ov ow qh oy oz pa qa qb qc bk"><strong class="oh fr">Model Implementation</strong>: In some scenarios, supplying model reasoning along with model predictions can be highly beneficial to our end users. For example, to help a customer service agent best retain a churning customer, we can provide the churn score alongside the customer features that contributed to this score.</li></ol><h1 id="ac3d" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Adding An Explainer to the ML Pipeline</h1><p id="199e" class="pw-post-body-paragraph of og fq oh b go oi oj ok gr ol om on oo op oq or os ot ou ov ow ox oy oz pa fj bk">Because our ML pipeline is algorithm agnostic, it is imperative that the explainer can also work across algorithms.</p><p id="51fb" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">SHAP (SHapley Additive exPlanations) values are an excellent choice for our purpose because they provide theoretically robust explanations based on game theory. They are designed to work consistently across algorithms, including both tree-based and non-tree-based models, with some approximations for the latter. Additionally, SHAP offers rich visualization capabilities and is widely regarded as an industry standard.</p><p id="b4c9" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">In the notebooks below, I have dug into the similarities and differences between SHAP implementations for various ML algorithms.</p><ul class=""><li id="9db5" class="of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa rf qb qc bk"><a class="af ni" href="https://github.com/MenaWANG/ML_toy_examples/blob/main/explain%20models/shap_basic_regression.ipynb" rel="noopener ugc nofollow" target="_blank">SHAP for regressor</a></li><li id="e2e0" class="of og fq oh b go qd oj ok gr qe om on oo qf oq or os qg ou ov ow qh oy oz pa rf qb qc bk"><a class="af ni" href="https://github.com/MenaWANG/ML_toy_examples/blob/main/explain%20models/shap_XGB_classification.ipynb" rel="noopener ugc nofollow" target="_blank">SHAP for XGBoost Classifier</a></li><li id="0586" class="of og fq oh b go qd oj ok gr qe om on oo qf oq or os qg ou ov ow qh oy oz pa rf qb qc bk"><a class="af ni" href="https://github.com/MenaWANG/ML_toy_examples/blob/main/explain%20models/shap_basic_RF_classification.ipynb" rel="noopener ugc nofollow" target="_blank">SHAP for RandomForest Classifier</a></li><li id="ed1e" class="of og fq oh b go qd oj ok gr qe om on oo qf oq or os qg ou ov ow qh oy oz pa rf qb qc bk"><a class="af ni" href="https://github.com/MenaWANG/ML_toy_examples/blob/main/explain%20models/shap_lightgbm_classification.ipynb" rel="noopener ugc nofollow" target="_blank">SHAP for LightGBM Classifier</a></li></ul><p id="0ff7" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">To create a generic explainer for our ML pipeline, the key differences to address are</p><blockquote class="rg rh ri"><p id="8a01" class="of og rj oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk"><strong class="oh fr"><em class="fq">1. Whether the model is directly supported by </em></strong><code class="cx hd he hf hg b"><strong class="oh fr"><em class="fq">shap.Explainer</em></strong></code></p></blockquote><p id="e213" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">The model-specific SHAP explainers are significantly more efficient than the model-agnostic ones. Therefore, the approach we take here is</p><ul class=""><li id="3ba6" class="of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa rf qb qc bk">first attempts to use the direct SHAP explainer for the model type,</li><li id="858d" class="of og fq oh b go qd oj ok gr qe om on oo qf oq or os qg ou ov ow qh oy oz pa rf qb qc bk">If that fails, falls back to a model-agnostic explainer using the predict function.</li></ul><blockquote class="rg rh ri"><p id="3185" class="of og rj oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk"><strong class="oh fr"><em class="fq">2. The shape of SHAP values</em></strong></p></blockquote><p id="5782" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">For binary classification problems, SHAP values can come in two formats/shapes.</p><ul class=""><li id="7035" class="of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa rf qb qc bk"><strong class="oh fr">Format 1</strong>: Only shows impact on positive class</li></ul><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="8030" class="qy nk fq hg b bg qz ra l rb rc">shape = (n_samples, n_features) # 2d array</span></pre><ul class=""><li id="6855" class="of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa rf qb qc bk"><strong class="oh fr">Format 2</strong>: Shows impact on both classes</li></ul><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="ee36" class="qy nk fq hg b bg qz ra l rb rc">shape = (n_samples, n_features, n_classes) # 3d array</span></pre><ul class=""><li id="e245" class="of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa rf qb qc bk">The explainer implementation below always shows the impact on the positive class. When the impact on both classes is available in SHAP values, it selects the ones on the positive class.</li></ul><p id="793c" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">Please see the code below for the implementation of the approach discussed above.</p><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="b0ae" class="qy nk fq hg b bg qz ra l rb rc">class ML_PIPELINE(mlflow.pyfunc.PythonModel):<br/>    """<br/>    Custom ML pipeline for classification and regression.<br/>    <br/>    - Works with scikit-learn compatible models<br/>    - Handles data preprocessing<br/>    - Manages model training and predictions<br/>    - Provide global and local model explanation<br/>    - Compatible with MLflow tracking<br/>    - Supports MLflow deployment<br/><br/>    Attributes:<br/>        model (BaseEstimator or None): A scikit-learn compatible model instance<br/>        preprocessor (Any or None): Data preprocessing pipeline<br/>        config (Any or None): Optional config for model settings <br/>        task(str): Type of ML task ('classification' or 'regression')<br/>        both_class (bool): Whether SHAP values include both classes<br/>        shap_values (shap.Explanation): SHAP values for model explanation<br/>        X_explain (pd.DataFrame): Processed features for SHAP explanation<br/>    """<br/><br/>    # ------- same code as above ---------<br/>    <br/>    def explain_model(self,X):<br/>        """<br/>        Generate SHAP values and plots for model interpretation. <br/>        This method:<br/>        1. Transforms the input data using the fitted preprocessor<br/>        2. Creates a SHAP explainer appropriate for the model type<br/>        3. Calculates SHAP values for feature importance<br/>        4. Generates a summary plot of feature importance<br/>        <br/>        Parameters:<br/>            X : pd.DataFrame<br/>                Input features to generate explanations for. <br/>        <br/>        Returns: None<br/>            The method stores the following attributes in the class:<br/>            - self.X_explain : pd.DataFrame<br/>                Transformed data with original numeric values for interpretation<br/>            - self.shap_values : shap.Explanation<br/>                SHAP values for each prediction<br/>            - self.both_class : bool<br/>                Whether the model outputs probabilities for both classes          <br/>        """<br/>        X_transformed = self.preprocessor.transform(X.copy())<br/>        self.X_explain = X_transformed.copy()<br/>        # get pre-transformed values for numeric features<br/>        self.X_explain[self.preprocessor.num_features] = X[self.preprocessor.num_features]<br/>        self.X_explain.reset_index(drop=True)<br/>        try:<br/>            # Attempt to create an explainer that directly supports the model<br/>            explainer = shap.Explainer(self.model)<br/>        except:<br/>            # Fallback for models or shap versions where direct support may be limited<br/>            explainer = shap.Explainer(self.model.predict, X_transformed)<br/>        self.shap_values = explainer(X_transformed)  <br/><br/>        # get the shape of shap values and extract accordingly<br/>        self.both_class = len(self.shap_values.values.shape) == 3<br/>        if self.both_class:<br/>            shap.summary_plot(self.shap_values[:,:,1])<br/>        elif self.both_class == False:<br/>            shap.summary_plot(self.shap_values)<br/>    <br/>    def explain_case(self,n):<br/>        """<br/>        Generate SHAP waterfall plot for one specific case.<br/>        <br/>        - Shows feature contributions<br/>        - Starts from base value<br/>        - Ends at final prediction<br/>        - Shows original feature values for better interpretability<br/>        <br/>        Parameters:<br/>            n (int): Case index (1-based)<br/>                     e.g., n=1 explains the first case.<br/>        <br/>        Returns:<br/>            None: Displays SHAP waterfall plot<br/>        <br/>        Notes:<br/>            - Requires explain_model() first<br/>            - Shows positive class for binary tasks<br/>        """<br/>        if self.shap_values is None:<br/>            print("""<br/>                  Please explain model first by running<br/>                  `explain_model()` using a selected dataset<br/>                  """)<br/>        else:<br/>            self.shap_values.data = self.X_explain<br/>            if self.both_class:<br/>                shap.plots.waterfall(self.shap_values[:,:,1][n-1])<br/>            elif self.both_class == False:<br/>                shap.plots.waterfall(self.shap_values[n-1]) </span></pre><p id="b41c" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">Now, the updated ML pipeline instance can create explanatory graphs for you in just one line of code. 😎</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq rk"><img src="../Images/130400a4a9c80519ff731a20d3ede222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A0RFODyF9MoBDkzwxxEw2Q.jpeg"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">SHAP plot for global explanation of the model</figcaption></figure><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq rl"><img src="../Images/b20822a6bb527748b1849d31e4cb83d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iLy3ZroANg15VBbb0GwNUQ.jpeg"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">SHAP plot for local explanation of any specific case</figcaption></figure><h1 id="4b77" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk"><strong class="al">Log and Use the Model</strong></h1><p id="1967" class="pw-post-body-paragraph of og fq oh b go oi oj ok gr ol om on oo op oq or os ot ou ov ow ox oy oz pa fj bk">Of course, you can log a trained ML pipeline using <code class="cx hd he hf hg b">mlflow</code> and enjoy all the metadata for model deployment and reproducibility. In the screenshot below, you can see that in addition to the pickled <code class="cx hd he hf hg b">pyfunc</code> model itself, the Python environment, metrics, and hyperparameters have all been logged in just a few lines of code below. To learn more, please refer to my previous article on <code class="cx hd he hf hg b">mlflow.pyfunc</code>, which is linked at the beginning.</p><pre class="ms mt mu mv mw qv hg qw bp qx bb bk"><span id="9d29" class="qy nk fq hg b bg qz ra l rb rc"># Log the model with MLflow<br/>with mlflow.start_run() as run:<br/><br/>    # Log the custom model with auto-captured conda environment<br/>    model_info = mlflow.pyfunc.log_model(<br/>        artifact_path="model",<br/>        python_model=ml_pipeline,<br/>        conda_env=mlflow.sklearn.get_default_conda_env()<br/>    )<br/>    # Log model parameters<br/>    mlflow.log_params(ml_pipeline.model.get_params())<br/>   <br/>    # Log metrics<br/>    mlflow.log_metric("rmse", rmse)<br/>      <br/>    # Get the run ID<br/>    run_id = run.info.run_id</span></pre><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div class="mp mq rm"><img src="../Images/d1ce888009587f800ff1a165eb1fb61a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*GglR5SDpc4QOvfw0gI1Oyw.jpeg"/></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Rich model metadata and artifacts logged with mlflow</figcaption></figure><h1 id="c759" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk"><strong class="al">Conclusions &amp; Next Steps</strong></h1><p id="52ab" class="pw-post-body-paragraph of og fq oh b go oi oj ok gr ol om on oo op oq or os ot ou ov ow ox oy oz pa fj bk">This is it, a generic and explainable ML pipeline that works for both classification and regression algorithms. Take the code and extend it to suit your use case. 🤗 If you find this useful, please give me a clap 👏🥰</p><p id="2fca" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">To further our journey on the <code class="cx hd he hf hg b">mlflow.pyfunc</code> series, below are some topics I am considering. Feel free to leave a comment and let me know what you would like to see. 🥰</p><ul class=""><li id="e8da" class="of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa rf qb qc bk">Feature selection</li><li id="1e00" class="of og fq oh b go qd oj ok gr qe om on oo qf oq or os qg ou ov ow qh oy oz pa rf qb qc bk">Hyperparameter tuning</li><li id="7e62" class="of og fq oh b go qd oj ok gr qe om on oo qf oq or os qg ou ov ow qh oy oz pa rf qb qc bk">If instead of <em class="rj">choosing </em>between off-the-shelf algorithms, one decides to <em class="rj">ensemble </em>multiple algorithms or have highly customized solutions, they can still enjoy a generic model representation and seamless migration via <code class="cx hd he hf hg b">mlflow.pyfunc</code>.</li></ul><p id="a391" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">Stay tuned and follow me on <a class="af ni" href="https://menawang.medium.com/" rel="noopener">Medium</a>. 😁</p><p id="1cff" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">💼<a class="af ni" href="https://www.linkedin.com/in/mena-ning-wang/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a> | 😺<a class="af ni" href="https://github.com/MenaWANG" rel="noopener ugc nofollow" target="_blank">GitHub</a> | 🕊️<a class="af ni" href="https://x.com/mena_wang" rel="noopener ugc nofollow" target="_blank">Twitter/X</a></p></div></div></div><div class="ab cb qi qj qk ql" role="separator"><span class="qm by bm qn qo qp"/><span class="qm by bm qn qo qp"/><span class="qm by bm qn qo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="d874" class="pw-post-body-paragraph of og fq oh b go pb oj ok gr pc om on oo pd oq or os pe ou ov ow pf oy oz pa fj bk">Unless otherwise noted, all images are by the author.</p></div></div></div></div>    
</body>
</html>