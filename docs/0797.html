<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Create Mixtures of Experts with MergeKit</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Create Mixtures of Experts with MergeKit</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/create-mixtures-of-experts-with-mergekit-11b318c99562?source=collection_archive---------2-----------------------#2024-03-27">https://towardsdatascience.com/create-mixtures-of-experts-with-mergekit-11b318c99562?source=collection_archive---------2-----------------------#2024-03-27</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="6a2e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><em class="hd">Combine multiple models into a single MoE</em></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mlabonne?source=post_page---byline--11b318c99562--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Maxime Labonne" class="l ep by dd de cx" src="../Images/a7efdd305e3cc77d5509bbb1076d57d8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*VbPYS4bNf0IrrOF-ZubSGQ.png"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--11b318c99562--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@mlabonne?source=post_page---byline--11b318c99562--------------------------------" rel="noopener follow">Maxime Labonne</a></p></div></div></div><div class="ia ib l"><div class="ab ic"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="id ie" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="if ig ah ai aj ak al am an ao ap aq ar ih ii ij" disabled="">Follow</button></p></div></div></span></div></div><div class="l ik"><span class="bf b bg z dx"><div class="ab cn il im in"><div class="io ip ab"><div class="bf b bg z dx ab iq"><span class="ir l ik">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--11b318c99562--------------------------------" rel="noopener follow"><p class="bf b bg z is it iu iv iw ix iy iz bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="id ie" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="ja jb l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 27, 2024</span></div></span></div></span></div></div></div><div class="ab cp jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr"><div class="h k w ea eb q"><div class="kh l"><div class="ab q ki kj"><div class="pw-multi-vote-icon ed ir kk kl km"><div class=""><div class="kn ko kp kq kr ks kt am ku kv kw km"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kx ky kz la lb lc ld"><p class="bf b dy z dx"><span class="ko">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kn lg lh ab q ee li lj" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lf"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count le lf">8</span></p></button></div></div></div><div class="ab q js jt ju jv jw jx jy jz ka kb kc kd ke kf kg"><div class="lk k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ll an ao ap ih lm ln lo" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lp cn"><div class="l ae"><div class="ab cb"><div class="lq lr ls lt lu lv ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo mp"><img src="../Images/aa74b784e835c9760158fe939603501a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kJ4ALMDzNaq9W88ZtVnhiQ.jpeg"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Image by author</figcaption></figure><p id="9e1c" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Thanks to the release of Mixtral, the <strong class="ni fr">Mixture of Experts</strong> (MoE) architecture has become popular in recent months. This architecture offers an interesting tradeoff: higher performance at the cost of increased VRAM usage. While Mixtral and other MoE architectures are pre-trained from scratch, another method of creating MoE has recently appeared. Thanks to Arcee’s <a class="af oc" href="https://github.com/arcee-ai/mergekit" rel="noopener ugc nofollow" target="_blank">MergeKit</a> library, we now have a new way of creating MoEs by ensembling several pre-trained models. These are often referred to as <strong class="ni fr">frankenMoEs</strong> or <strong class="ni fr">MoErges</strong> to distinguish them from the pre-trained MoEs.</p><p id="560d" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In this article, we will detail how the MoE architecture works and how frankenMoEs are created. Finally, we will make our <a class="af oc" href="https://huggingface.co/mlabonne/Beyonder-4x7B-v3" rel="noopener ugc nofollow" target="_blank">own frankenMoE</a> with MergeKit and evaluate it on several benchmarks. The code is available on Google Colab in a wrapper called <a class="af oc" href="https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y" rel="noopener ugc nofollow" target="_blank">LazyMergeKit</a>.</p><p id="95f9" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Special thanks to <a class="af oc" href="https://github.com/cg123" rel="noopener ugc nofollow" target="_blank">Charles Goddard</a>, the creator of MergeKit, for proofreading this article.</p><h1 id="0c60" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">🔀 Introduction to MoEs</h1><p id="6b7e" class="pw-post-body-paragraph ng nh fq ni b go oz nk nl gr pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob fj bk">A Mixture of Experts is an architecture designed for improved efficiency and performance. It uses multiple specialized subnetworks, known as “<strong class="ni fr">experts</strong>.” Unlike dense models, where the entire network is activated, MoEs only activate relevant experts based on the input. This results in faster training and more efficient inference.</p><p id="01cb" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">There are two components at the core of an MoE model:</p><ol class=""><li id="9e5c" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob pe pf pg bk"><strong class="ni fr">Sparse MoE Layers</strong>: These replace the dense feed-forward network layers in the transformer architecture. Each MoE layer contains several experts, and only a subset of these experts are engaged for a given input.</li><li id="a04d" class="ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob pe pf pg bk"><strong class="ni fr">Gate Network or Router</strong>: This component determines which tokens are processed by which experts, ensuring that each part of the input is handled by the most suitable expert(s).</li></ol><p id="7cd5" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In the following example, we show how a Mistral-7B block is transformed into an MoE block with a sparse MoE layer (feedforward network 1, 2, and 3) and a router. This example represents an MoE with three experts, where two are currently engaged (FFN 1 and FFN 3).</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo pm"><img src="../Images/5ec46b2d6081bda34c1822c37edc0420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gGQxuStcGfSS_Boy9rKvAw.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Image by author</figcaption></figure><p id="8849" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">MoEs also come with their own set of challenges, especially in terms of fine-tuning and memory requirements. The fine-tuning process can be difficult due to the model’s complexity, with the need to <strong class="ni fr">balance expert usage</strong> during training to properly train the gating weights to select the most relevant ones. In terms of memory, even though only a fraction of the total parameters are used during inference, the entire model, including all experts, needs to be <strong class="ni fr">loaded into memory</strong>, which requires high VRAM capacity.</p><p id="263b" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">More specifically, there are two essential parameters when it comes to MoEs:</p><ul class=""><li id="3c74" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob pn pf pg bk"><strong class="ni fr">Number of experts</strong> (<code class="cx po pp pq pr b">num_local_experts</code>): This determines the total number of experts in the architecture (e.g., 8 for Mixtral). The higher the number of experts, the higher the VRAM usage.</li><li id="e19c" class="ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob pn pf pg bk"><strong class="ni fr">Number of experts/token</strong> (<code class="cx po pp pq pr b">num_experts_per_tok</code>): This determines the number of experts that are engaged for each token and each layer (e.g., 2 for Mixtral). There is a tradeoff between a high number of experts per token for accuracy (but diminishing returns) vs. a low number for fast training and inference.</li></ul><p id="313d" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Historically, MoEs have underperformed dense models. However, the release of <a class="af oc" href="https://arxiv.org/abs/2401.04088" rel="noopener ugc nofollow" target="_blank">Mixtral-8x7B</a> in December 2023 shook things up and showed impressive performance for its size. Additionally, GPT-4 is also rumored to be an MoE, which would make sense as it would be a lot cheaper to run and train for OpenAI compared to a dense model. In addition to these recent excellent MoEs, we now have a new way of creating MoEs with MergeKit: frankenMoEs, also called MoErges.</p><h1 id="6159" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">🧟‍♂️ True MoEs vs. frankenMoEs</h1><p id="c8ee" class="pw-post-body-paragraph ng nh fq ni b go oz nk nl gr pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob fj bk">The main difference between true MoEs and frankenMoEs is how they’re trained. In the case of true MoEs, the experts and the router are trained jointly. In the case of frankenMoEs, we upcycle existing models and initialize the router afterward.</p><p id="600e" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In other words, we copy the weights of the layer norm and self-attention layers from a base model, and then copy the weights of the FFN layers found in each expert. This means that besides the FFNs, all the other parameters are shared. This explains why Mixtral-8x7B with eight experts doesn’t have 8*7 = 56B parameters, but about 45B. This is also why using two experts per token gives the inference speed (FLOPs) of a 12B dense model instead of 14B.</p><p id="169d" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">FrankenMoEs are about selecting the most relevant experts and initializing them properly. MergeKit currently implements three ways of initializing the routers:</p><ol class=""><li id="aa12" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob pe pf pg bk"><a class="af oc" href="https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L139-L142" rel="noopener ugc nofollow" target="_blank"><strong class="ni fr">Random</strong></a>: Random weights. Be careful when using it as the same experts might be selected every time (it requires further fine-tuning or <code class="cx po pp pq pr b">num_local_experts = num_experts_per_tok</code>, which means you don't need any routing).</li><li id="b9aa" class="ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob pe pf pg bk"><a class="af oc" href="https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L91C1-L109C37" rel="noopener ugc nofollow" target="_blank"><strong class="ni fr">Cheap embed</strong></a>: It uses the raw embeddings of the input tokens directly and applies the same transformation across all layers. This method is computationally inexpensive and suitable for execution on less powerful hardware.</li><li id="8b74" class="ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob pe pf pg bk"><a class="af oc" href="https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L70-L88" rel="noopener ugc nofollow" target="_blank"><strong class="ni fr">Hidden</strong></a>: It creates hidden representations of a list of positive and negative prompts by extracting them from the last layer of the LLM. They are averaged and normalized to initialize the gates. More information about it is available on <a class="af oc" href="https://goddard.blog/posts/clown-moe/" rel="noopener ugc nofollow" target="_blank">Charles Goddard’s blog</a>.</li></ol><p id="3e62" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">As you can guess, the “hidden” initialization is the most efficient to correctly route the tokens to the most relevant experts. In the next section, we will create our own frankenMoE using this technique.</p><h1 id="8d07" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">💻 Creating a frankenMoE</h1><p id="644d" class="pw-post-body-paragraph ng nh fq ni b go oz nk nl gr pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob fj bk">To create our frankenMoE, we need to select <code class="cx po pp pq pr b">n</code> experts. In this case, we will rely on Mistral-7B thanks to its popularity and relatively small size. However, eight experts like in Mixtral is quite a lot, as we need to fit all of them in memory. For efficiency, I'll only use four experts in this example, with two of them engaged for each token and each layer. In this case, we will end up with a model with 24.2B parameters instead of 4*7 = 28B parameters.</p><p id="d05a" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Here, our goal is to create a well-rounded model that can do pretty much everything: write stories, explain articles, code in Python, etc. We can decompose this requirement into four tasks and select the best expert for each of them. This is how I decomposed it:</p><ul class=""><li id="6118" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob pn pf pg bk"><strong class="ni fr">Chat model</strong>: a general-purpose model that is used in most interactions. I used <a class="af oc" href="https://huggingface.co/mlabonne/AlphaMonarch-7B" rel="noopener ugc nofollow" target="_blank">mlabonne/AlphaMonarch-7B</a>, which perfectly satisfies the requirements.</li><li id="5c21" class="ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob pn pf pg bk"><strong class="ni fr">Code model</strong>: a model capable of generating good code. I don’t have a lot of experience with Mistral-7B-based code models, but I found <a class="af oc" href="https://huggingface.co/beowolx/CodeNinja-1.0-OpenChat-7B" rel="noopener ugc nofollow" target="_blank">beowolx/CodeNinja-1.0-OpenChat-7B</a> particularly good compared to others.</li><li id="433b" class="ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob pn pf pg bk"><strong class="ni fr">Math model</strong>: math is tricky for LLMs, which is why we want a model specialized in math. Thanks to its high MMLU and GMS8K scores, I chose <a class="af oc" href="https://huggingface.co/mlabonne/NeuralDaredevil-7B" rel="noopener ugc nofollow" target="_blank">mlabonne/NeuralDaredevil-7B</a> for this purpose.</li><li id="fcd9" class="ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob pn pf pg bk"><strong class="ni fr">Role-play model</strong>: The goal of this model is to write high-quality stories and conversations. I selected <a class="af oc" href="https://huggingface.co/SanjiWatsuki/Kunoichi-DPO-v2-7B" rel="noopener ugc nofollow" target="_blank">SanjiWatsuki/Kunoichi-DPO-v2–7B</a> because of its good reputation and high MT-Bench score (8.51 vs. 8.30 for Mixtral).</li></ul><p id="c767" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Now that we’ve identified the experts we want to use, we can create the YAML configuration that MergeKit will use to create our frankenMoE. This uses the mixtral branch of MergeKit. You can find more information about how to write the configuration <a class="af oc" href="https://github.com/arcee-ai/mergekit/blob/mixtral/docs/moe.md" rel="noopener ugc nofollow" target="_blank">on this page</a>. Here is our version:</p><pre class="mq mr ms mt mu ps pr pt bp pu bb bk"><span id="1f0e" class="pv oe fq pr b bg pw px l py pz">base_model: mlabonne/AlphaMonarch-7B<br/>experts:<br/>  - source_model: mlabonne/AlphaMonarch-7B<br/>    positive_prompts:<br/>    - "chat"<br/>    - "assistant"<br/>    - "tell me"<br/>    - "explain"<br/>    - "I want"<br/>  - source_model: beowolx/CodeNinja-1.0-OpenChat-7B<br/>    positive_prompts:<br/>    - "code"<br/>    - "python"<br/>    - "javascript"<br/>    - "programming"<br/>    - "algorithm"<br/>  - source_model: SanjiWatsuki/Kunoichi-DPO-v2-7B<br/>    positive_prompts:<br/>    - "storywriting"<br/>    - "write"<br/>    - "scene"<br/>    - "story"<br/>    - "character"<br/>  - source_model: mlabonne/NeuralDaredevil-7B<br/>    positive_prompts:<br/>    - "reason"<br/>    - "math"<br/>    - "mathematics"<br/>    - "solve"<br/>    - "count"</span></pre><p id="92ad" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">For each expert, I provide five basic positive prompts. You can be a bit fancier and write entire sentences if you want. The best strategy consists of using real prompts that should trigger a particular expert. You can also add negative prompts to do the opposite.</p><p id="808d" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Once this is ready, you can save your configuration as <code class="cx po pp pq pr b">config.yaml</code>. In the same folder, we will download and install the <a class="af oc" href="https://github.com/arcee-ai/mergekit" rel="noopener ugc nofollow" target="_blank">mergekit</a> library (mixtral branch).</p><pre class="mq mr ms mt mu ps pr pt bp pu bb bk"><span id="6ab3" class="pv oe fq pr b bg pw px l py pz">git clone -b mixtral https://github.com/arcee-ai/mergekit.git<br/>cd mergekit &amp;&amp; pip install -e .<br/>pip install -U transformers</span></pre><p id="4c16" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">If your computer has enough RAM (roughly 24–32 GB of RAM), you can run the following command:</p><pre class="mq mr ms mt mu ps pr pt bp pu bb bk"><span id="bac8" class="pv oe fq pr b bg pw px l py pz">mergekit-moe config.yaml merge --copy-tokenizer</span></pre><p id="3efc" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">If you don’t have enough RAM, you can shard the models instead as follows (it will take longer):</p><pre class="mq mr ms mt mu ps pr pt bp pu bb bk"><span id="48d5" class="pv oe fq pr b bg pw px l py pz">mergekit-moe config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle</span></pre><p id="ccb1" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">This command automatically downloads the experts and creates the frankenMoE in the <code class="cx po pp pq pr b">merge</code> directory. For the <code class="cx po pp pq pr b">hidden</code> gate mode, you can also use the <code class="cx po pp pq pr b">--load-in-4bit</code> and <code class="cx po pp pq pr b">--load-in-8bit</code> options to compute hidden states with lower precision.</p><p id="fb5b" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Alternatively, you can copy your configuration into <a class="af oc" href="https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y" rel="noopener ugc nofollow" target="_blank">LazyMergekit</a>, a wrapper I made to simplify model merging. In this Colab notebook, you can input your model name, select the <code class="cx po pp pq pr b">mixtral</code> branch, specify your Hugging Face username/token, and run the cells. After creating your frankenMoE, it will also upload it to the Hugging Face Hub with a nicely formatted model card.</p><p id="cc95" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">I called my model <a class="af oc" href="https://huggingface.co/mlabonne/Beyonder-4x7B-v3" rel="noopener ugc nofollow" target="_blank">Beyonder-4x7B-v3</a> and created <a class="af oc" href="https://huggingface.co/mlabonne/Beyonder-4x7B-v3-GGUF" rel="noopener ugc nofollow" target="_blank">GGUF versions</a> of it using <a class="af oc" href="https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu#scrollTo=fD24jJxq7t3k" rel="noopener ugc nofollow" target="_blank">AutoGGUF</a>. If you can’t run GGUF versions on your local machine, you can also perform inference using this <a class="af oc" href="https://colab.research.google.com/drive/1SIfwhpLttmoZxT604LGVXDOI9UKZ_1Aq?usp=sharing" rel="noopener ugc nofollow" target="_blank">Colab notebook</a>.</p><p id="65aa" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">To get a good overview of its capabilities, it has been evaluated on three different benchmarks: Nous’ benchmark suite, EQ-Bench, and the Open LLM Leaderboard. This model is not designed to excel in traditional benchmarks, as the code and role-playing models generally do not apply to those contexts. Nonetheless, it performs remarkably well thanks to strong general-purpose experts.</p><p id="ff3a" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><strong class="ni fr">Nous</strong>: Beyonder-4x7B-v3 is one of the best models on Nous’ benchmark suite (evaluation performed using <a class="af oc" href="https://github.com/mlabonne/llm-autoeval" rel="noopener ugc nofollow" target="_blank">LLM AutoEval</a>) and significantly outperforms the v2. See the entire leaderboard <a class="af oc" href="https://huggingface.co/spaces/mlabonne/Yet_Another_LLM_Leaderboard" rel="noopener ugc nofollow" target="_blank">here</a>.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo qa"><img src="../Images/cc0510c87454a244bd0866f260429868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MTWS0BgqX17yq9qa.png"/></div></div></figure><p id="5f6f" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><strong class="ni fr">EQ-Bench</strong>: It’s also the best 4x7B model on the <a class="af oc" href="https://eqbench.com/" rel="noopener ugc nofollow" target="_blank">EQ-Bench leaderboard</a>, outperforming older versions of ChatGPT and Llama-2–70b-chat. Beyonder is very close to Mixtral-8x7B-Instruct-v0.1 and Gemini Pro, which are (supposedly) much bigger models.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo qb"><img src="../Images/f42a3581a0111fe7994dc95948f26e92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ebG79Y254SJp58KG.png"/></div></div></figure><p id="b923" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><strong class="ni fr">Open LLM Leaderboard</strong>: Finally, it’s also a strong performer on the Open LLM Leaderboard, significantly outperforming the v2 model.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo qc"><img src="../Images/1a42d6294c40a6fc7f7467facc3c08fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RpN_fYTFfzxvSJ66.png"/></div></div></figure><p id="0d17" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">On top of these quantitative evaluations, I recommend checking the model’s outputs in a more qualitative way using a GGUF version on <a class="af oc" href="https://lmstudio.ai/" rel="noopener ugc nofollow" target="_blank">LM Studio</a>. A common way of testing these models is to gather a private set of questions and check their outputs. With this strategy, I found that Beyonder-4x7B-v3 is quite robust to changes in the user and system prompts compared to other models, including AlphaMonarch-7B. This is pretty cool as it improves the usefulness of the model in general.</p><p id="b79d" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">FrankenMoEs are a promising but still experimental approach. The trade-offs, like higher VRAM demand and slower inference speeds, can make it challenging to see their advantage over simpler merging techniques like SLERP or DARE TIES. Especially, when you use frankenMoEs with just two experts, they might not perform as well as if you had simply merged the two models. However, frankenMoEs excel in preserving knowledge, which can result in stronger models, as demonstrated by Beyonder-4x7B-v3. With the right hardware, these drawbacks can be effectively mitigated.</p><h1 id="cd85" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">Conclusion</h1><p id="133a" class="pw-post-body-paragraph ng nh fq ni b go oz nk nl gr pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob fj bk">In this article, we introduced the Mixture of Experts architecture. Unlike traditional MoEs that are trained from scratch, MergeKit facilitates the creation of MoEs by ensembling experts, offering an innovative approach to improving model performance and efficiency. We detailed the process of creating a frankenMoE with MergeKit, highlighting the practical steps involved in selecting and combining different experts to produce a high-quality MoE.</p><p id="b00a" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Thanks for reading this article. I encourage you to try to make your own FrankenMoEs using <a class="af oc" href="https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y" rel="noopener ugc nofollow" target="_blank">LazyMergeKit</a>: select a few models, create your config based Beyonder’s, and run the notebook to create your own models! If you liked this article, please follow me on <a class="af oc" href="https://huggingface.co/mlabonne" rel="noopener ugc nofollow" target="_blank">Hugging Face</a> and X/Twitter <a class="af oc" href="https://twitter.com/maximelabonne" rel="noopener ugc nofollow" target="_blank">@maximelabonne</a>.</p><h1 id="d3b3" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">References</h1><ul class=""><li id="92ee" class="ng nh fq ni b go oz nk nl gr pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob pn pf pg bk"><a class="af oc" href="https://arxiv.org/abs/2401.04088" rel="noopener ugc nofollow" target="_blank">Mixtral of Experts</a> by Jiang et al. (2023)</li><li id="cbd7" class="ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob pn pf pg bk"><a class="af oc" href="https://goddard.blog/posts/clown-moe/" rel="noopener ugc nofollow" target="_blank">Mixture of Experts for Clowns</a> by Charles Goddard (2023)</li><li id="a3d6" class="ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob pn pf pg bk"><a class="af oc" href="https://huggingface.co/blog/moe" rel="noopener ugc nofollow" target="_blank">Mixture of Experts Explained</a> by Sanseviero et al. (2023)</li><li id="8de3" class="ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob pn pf pg bk"><a class="af oc" href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf" rel="noopener ugc nofollow" target="_blank">Adaptive Mixture of Local Experts</a> by Jacobs et al. (1991)</li><li id="c81d" class="ng nh fq ni b go ph nk nl gr pi nn no np pj nr ns nt pk nv nw nx pl nz oa ob pn pf pg bk"><a class="af oc" href="https://arxiv.org/abs/2212.05055" rel="noopener ugc nofollow" target="_blank">Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints</a> by Komatsuzaki et al. (2022)</li></ul></div></div></div><div class="ab cb qd qe qf qg" role="separator"><span class="qh by bm qi qj qk"/><span class="qh by bm qi qj qk"/><span class="qh by bm qi qj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e676" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><em class="ql">Learn more about machine learning and support my work with one click — become a Medium member here:</em></p><div class="qm qn qo qp qq qr"><a href="https://medium.com/@mlabonne/membership?source=post_page-----11b318c99562--------------------------------" rel="noopener follow" target="_blank"><div class="qs ab ik"><div class="qt ab co cb qu qv"><h2 class="bf fr hx z is qw iu iv qx ix iz fp bk">Join Medium with my referral link — Maxime Labonne</h2><div class="qy l"><h3 class="bf b hx z is qw iu iv qx ix iz dx">As a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story…</h3></div><div class="qz l"><p class="bf b dy z is qw iu iv qx ix iz dx">medium.com</p></div></div><div class="ra l"><div class="rb l rc rd re ra rf lv qr"/></div></div></a></div></div></div></div></div>    
</body>
</html>