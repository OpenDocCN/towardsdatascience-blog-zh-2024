<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Training Naive Bayes… Really Fast</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Training Naive Bayes… Really Fast</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/train-naive-bayes-really-fast-7398a404e342?source=collection_archive---------10-----------------------#2024-05-31">https://towardsdatascience.com/train-naive-bayes-really-fast-7398a404e342?source=collection_archive---------10-----------------------#2024-05-31</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/5ca58b4de0a8ed93631227f9f6925e2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xbfAv2U43mpzkVMlIadesQ.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Photo by <a class="af gi" href="https://unsplash.com/de/@marcsm?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Marc Sendra Martorell</a> on <a class="af gi" href="https://unsplash.com/de/fotos/zeitraffer-von-strassenlaternen--Vqn2WrfxTQ?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div/><div><h2 id="8306" class="pw-subtitle-paragraph hi gk gl bf b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx cq dx">Performance tuning in Julia</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hy hz ia ib ic ab"><div><div class="ab id"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@schaetzle.ka?source=post_page---byline--7398a404e342--------------------------------" rel="noopener follow"><div class="l ie if by ig ih"><div class="l ed"><img alt="Roland Schätzle" class="l ep by dd de cx" src="../Images/5d03aad32cda174f2fee595a3fc34a17.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*hDueetwGJg4R5jhnN6_Vnw.jpeg"/><div class="ii by l dd de em n ij eo"/></div></div></a></div></div><div class="ik ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--7398a404e342--------------------------------" rel="noopener follow"><div class="l il im by ig in"><div class="l ed"><img alt="Towards Data Science" class="l ep by br io cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ii by l br io em n ij eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ip ab q"><div class="ab q iq"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ir is bk"><a class="af ag ah ai aj ak al am an ao ap aq ar it" data-testid="authorName" href="https://medium.com/@schaetzle.ka?source=post_page---byline--7398a404e342--------------------------------" rel="noopener follow">Roland Schätzle</a></p></div></div></div><span class="iu iv" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ir is dx"><button class="iw ix ah ai aj ak al am an ao ap aq ar iy iz ja" disabled="">Follow</button></p></div></div></span></div></div><div class="l jb"><span class="bf b bg z dx"><div class="ab cn jc jd je"><div class="jf jg ab"><div class="bf b bg z dx ab jh"><span class="ji l jb">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar it ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--7398a404e342--------------------------------" rel="noopener follow"><p class="bf b bg z jj jk jl jm jn jo jp jq bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="iu iv" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="jr js l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 31, 2024</span></div></span></div></span></div></div></div><div class="ab cp jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki"><div class="h k w ea eb q"><div class="ky l"><div class="ab q kz la"><div class="pw-multi-vote-icon ed ji lb lc ld"><div class=""><div class="le lf lg lh li lj lk am ll lm ln ld"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l lo lp lq lr ls lt lu"><p class="bf b dy z dx"><span class="lf">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao le lv lw ab q ee lx ly" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lz"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q kj kk kl km kn ko kp kq kr ks kt ku kv kw kx"><div class="ma k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mb an ao ap iy mc md me" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mf cn"><div class="l ae"><div class="ab cb"><div class="mg mh mi mj mk gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mb an ao ap iy ml mm ly mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mb an ao ap iy ml mm ly mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mb an ao ap iy ml mm ly mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="1515" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In a recent lecture I demonstrated to my students how a <em class="ny">Multinomial Naive Bayes (MNB) </em>model can be used for document classification. As an example I used the <a class="af gi" href="https://www.cs.cmu.edu/~enron/" rel="noopener ugc nofollow" target="_blank">Enron Email Dataset </a>in order to create a spam filter based on such a model. The version of the dataset used consists of 33,716 emails, categorised as “spam” or “ham” (i.e. no spam).</p><p id="ed6a" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We chose the <code class="cx nz oa ob oc b">MultinomialNBClassifier</code> from the Julia <code class="cx nz oa ob oc b"><a class="af gi" href="https://juliaai.github.io/MLJ.jl/dev/" rel="noopener ugc nofollow" target="_blank">MLJ.jl</a></code>-package and for data preparation the <code class="cx nz oa ob oc b">CountTransformer</code> from the same package. I was quite surprised that it took more than 30 minutes to train this classifier using the whole dataset (on an Apple M3, 16 GB RAM).</p><p id="c6ba" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Typically only a part of a dataset is used for training as the rest is needed for testing. Using just 70% of the dataset for this purpose still took more than 10 minutes. The 33,716 emails are admittedly more than a simple textbook example, but on the other hand NB models are known for low training costs.</p><p id="4443" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Therefore I began to investigate why it takes so long and if there are ways to make things faster. In the following sections I will present the performance tuning measures I’ve applied and the speedup which could be achieved. These measures are not very specific to this problem and thus should also be applicable in other situations.</p><blockquote class="od oe of"><p id="0601" class="nc nd ny ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note: All implementations and benchmarks are done using Julia 1.10.3 on a M3 MacBook Pro with 16 GB RAM. The utilised Julia packages are MLJ 0.20.0, TextAnalysis 0.7.5, Metal 1.1.0, CategoricalArrays 0.10.8 and BenchmarkTools 1.5.0.</p></blockquote><h1 id="2d30" class="og oh gl bf oi oj ok hl ol om on ho oo op oq or os ot ou ov ow ox oy oz pa pb bk">Training a Multinomial Naive Bayes model</h1><p id="99ec" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">But first let me introduce the main steps which are necessary to train a MNB in order to understand which algorithm has to be optimised. There is</p><ul class=""><li id="0505" class="nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ph pi pj bk">a <strong class="ne gm">data preparation</strong> step which converts the documents (in our case the emails) to an adequate data structure (a so-called <em class="ny">document term matrix; DTM)</em> and</li><li id="efb1" class="nc nd gl ne b hj pk ng nh hm pl nj nk nl pm nn no np pn nr ns nt po nv nw nx ph pi pj bk">the <strong class="ne gm">actual training</strong> step where the DTM is aggregated into a vector for each class (spam or ham)</li></ul><h2 id="fbda" class="pp oh gl bf oi pq pr ps ol pt pu pv oo nl pw px py np pz qa qb nt qc qd qe qf bk">Data Preparation</h2><p id="98f3" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Documents for use with an MNB are represented as “bags of words”. I.e. the order of the words within the document is considered irrelevant and only the number of occurrences of each word is stored. So the sentence “the cow eats grass” is in this representation equivalent to “eats the cow grass” or “grass eats the cow”.</p><p id="ac3a" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In order to convert all documents into this form using a memory efficient representation, a <em class="ny">dictionary</em> of all words that occur in the documents is created (it’s basically an array of all words). Let’s say we have the following documents D1, D2 and D3:</p><ul class=""><li id="1998" class="nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ph pi pj bk">D1: “the grey cat lies on the grass”</li><li id="93df" class="nc nd gl ne b hj pk ng nh hm pl nj nk nl pm nn no np pn nr ns nt po nv nw nx ph pi pj bk">D2: “the cow eats grass”</li><li id="984f" class="nc nd gl ne b hj pk ng nh hm pl nj nk nl pm nn no np pn nr ns nt po nv nw nx ph pi pj bk">D3: “the cat is grey”</li></ul><p id="1cc7" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then the dictionary is as follows: [“the”, “grey”, “cat”, “lies”, “on”, “grass”, “cow”, “eats”, “is”] as there are nine different words in the three documents.</p><p id="7b16" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Each document is then represented as an array of the same length as the dictionary and each element within this array is the number of occurrences of the corresponding word in the dictionary. So D1, D2 and D3 will have the form:</p><ul class=""><li id="45f4" class="nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ph pi pj bk">D1: [2, 1, 1, 1, 1, 1, 0, 0, 0] as e.g. the first word in the dictionary (“the”) occurs twice, the second word (“grey”) occurs once and so on</li><li id="9a6d" class="nc nd gl ne b hj pk ng nh hm pl nj nk nl pm nn no np pn nr ns nt po nv nw nx ph pi pj bk">D2: [1, 0, 0, 0, 0, 1, 1, 1, 0]</li><li id="6d16" class="nc nd gl ne b hj pk ng nh hm pl nj nk nl pm nn no np pn nr ns nt po nv nw nx ph pi pj bk">D3: [1, 1, 1, 0, 0, 0, 0, 0, 1]</li></ul><p id="a377" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If we combine these arrays into a matrix — one row for each document, we get the above mentioned <em class="ny">document term matrix (DTM). </em>In our case it is a 3 x 9 matrix, as we have three documents and a dictionary consisting of nine different words.</p><h2 id="01ec" class="pp oh gl bf oi pq pr ps ol pt pu pv oo nl pw px py np pz qa qb nt qc qd qe qf bk">Training</h2><p id="a43d" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">The training of the MNB consists basically of adding all the document vectors, separated by class. I.e. in our spam-example we have to add all document vectors for “spam” and all for “ham” resulting in two vectors, each containing the summarised word frequencies for the respective class.</p><p id="cee0" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If we assume that the documents D1 and D3 are “ham” and D2 is “spam”, we would get the following results:</p><ul class=""><li id="03e6" class="nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ph pi pj bk">“ham” word frequencies: [3, 2, 2, 1, 1, 1, 0, 0, 1]</li><li id="d49e" class="nc nd gl ne b hj pk ng nh hm pl nj nk nl pm nn no np pn nr ns nt po nv nw nx ph pi pj bk">“spam” word frequencies: [1, 0, 0, 0, 0, 1, 1, 1, 0]</li></ul><p id="946a" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In a complete training step for a MNB there is some additional post-processing of these numbers, but the “expensive” part, which we want to optimise, is the aggregation of the DTM as shown here.</p><h1 id="b6c8" class="og oh gl bf oi oj ok hl ol om on ho oo op oq or os ot ou ov ow ox oy oz pa pb bk">Starting with the Enron dataset</h1><h2 id="84ca" class="pp oh gl bf oi pq pr ps ol pt pu pv oo nl pw px py np pz qa qb nt qc qd qe qf bk">Data Preparation</h2><p id="79f5" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">I created the DTM for the Enron dataset using the <code class="cx nz oa ob oc b">CountTransformer</code>, which is part of <code class="cx nz oa ob oc b">MLJ</code> with the following function:</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="3da4" class="qo oh gl oc b bg qp qq l qr qs">function transform_docs(doc_list)<br/>    CountTransformer = @load CountTransformer pkg=MLJText<br/>    trans_machine = machine(CountTransformer(), doc_list)<br/>    fit!(trans_machine)<br/>    return(MLJ.transform(trans_machine, doc_list))<br/>end</span></pre><p id="bad2" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The input <code class="cx nz oa ob oc b">doc_list</code> to this function is an array of the tokenised emails. I.e. each word within a mail got separated into a single string (using <code class="cx nz oa ob oc b">TextAnalysis.tokenize()</code>).</p><p id="41b2" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This results is a 33,716 x 159,093 matrix as there are 33,716 emails and the dictionary consists of 159,093 different words. This is a matrix with more than 5.3 billion elements. Surprisingly the creation of the DTM took less than a minute. So the focus of the performance tuning will be exclusively on the training step.</p><p id="3682" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As the majority of elements of a DTM are 0, a so-called <em class="ny">sparse matrix</em> is used to store them in a memory efficient way (in Julia this is the <code class="cx nz oa ob oc b"><a class="af gi" href="https://docs.julialang.org/en/v1/stdlib/SparseArrays/" rel="noopener ugc nofollow" target="_blank">SparseMatrixCSC</a></code> type).</p><p id="516d" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To be exact, the <code class="cx nz oa ob oc b">CountTransformer</code> produces a data structure of type <code class="cx nz oa ob oc b">LinearAlgebra.Adjoint{Int64,SparseMatrixCSC{Int64, Int64}}</code>. We will come to this special structure later.</p><h2 id="008b" class="pp oh gl bf oi pq pr ps ol pt pu pv oo nl pw px py np pz qa qb nt qc qd qe qf bk">Training</h2><p id="8361" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Training the <code class="cx nz oa ob oc b">MultinomialNBClassifier</code> is then done as follows with <code class="cx nz oa ob oc b">X</code> containing the DTM and <code class="cx nz oa ob oc b">y</code> being the array of spam/ham labels (as a <code class="cx nz oa ob oc b"><a class="af gi" href="https://github.com/JuliaData/CategoricalArrays.jl?tab=readme-ov-file" rel="noopener ugc nofollow" target="_blank">CategoricalArray</a></code> since all MLJ models expect this type):</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="05a5" class="qo oh gl oc b bg qp qq l qr qs">MultinomialNBClassifier = @load MultinomialNBClassifier pkg=NaiveBayes<br/>nb_classifier = MultinomialNBClassifier()<br/>nb_machine = machine(nb_classifier, X, y)<br/>fit!(nb_machine, verbosity=0)</span></pre><p id="9a4c" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The call to <code class="cx nz oa ob oc b">fit!</code> does the actual training and took more than 30 minutes for all Enron mails and more than 10 minutes for a 70%-subset.</p><p id="82ff" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In order to focus on the analysis and optimisation of the training step, I’m starting with my own implementation of a function that does the above mentioned aggregation of all document vectors into two vectors containing the summarised word frequencies for “spam” and “ham”. The respective code of the <code class="cx nz oa ob oc b">MultinomialNBCClassifier</code> has too many dependencies which makes it much less feasible to demonstrate the following optimisation steps.</p><p id="4ec0" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A first baseline approach for this function (called <code class="cx nz oa ob oc b">count_words</code>) looks as follows:</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="73c8" class="qo oh gl oc b bg qp qq l qr qs">function count_words_base(X::AbstractMatrix{Int64},y)<br/>    ndocs = size(X,1)         # number of documents<br/>    nwords = size(X,2)        # number of words in dictionary<br/>    ncats = length(levels(y)) # number of categories in `y`<br/>    wcounts = ones(Int64, ncats, nwords) # matrix for storing the word counts by category<br/>    for col in 1:nwords<br/>        for doc in 1:ndocs<br/>            if y[doc] == “ham”<br/>                wcounts[1,col] += X[doc, col]<br/>            else<br/>                wcounts[2,col] += X[doc, col]<br/>            end<br/>        end<br/>    end<br/>    return(wcounts)<br/>end</span></pre><p id="fc16" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Applied to <code class="cx nz oa ob oc b">X</code> and <code class="cx nz oa ob oc b">y</code> it takes <strong class="ne gm">241.076 seconds </strong>to complete.</p><p id="39ec" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To reduce the runtime of the test runs and to avoid that memory becomes the decisive factor for the runtime, I’ve done all further tests (if not stated otherwise) on a part of the DTM (called <code class="cx nz oa ob oc b">Xpart</code>) limited to the first 10,000 columns (i.e. a 33,716 x 10,000 matrix).</p><p id="6401" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For this reduced DTM <code class="cx nz oa ob oc b">count_words_base</code> needs <strong class="ne gm">20.363 seconds</strong> to complete.</p><h1 id="a11d" class="og oh gl bf oi oj ok hl ol om on ho oo op oq or os ot ou ov ow ox oy oz pa pb bk">OPT1: Use the right data structures in the right way</h1><p id="284d" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">An important aspect of performance tuning are the data structures used and the question if they are used in the most efficient manner.</p><h2 id="9de7" class="pp oh gl bf oi pq pr ps ol pt pu pv oo nl pw px py np pz qa qb nt qc qd qe qf bk">Column-first Storage</h2><p id="b332" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">In this sense <code class="cx nz oa ob oc b">count_words_base</code> already uses an optimisation. In Julia a matrix is stored in a column-first order. I.e. that the elements of each column are stored close to each other in memory. So iterating over all elements in <em class="ny">one</em> <em class="ny">column</em> is faster than iterating over the elements within a <em class="ny">row</em>. Therefore the inner loop in <code class="cx nz oa ob oc b">count_words_base</code> iterates over a column in <code class="cx nz oa ob oc b">X</code>.</p><p id="54c4" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Column-first order storage is common practice in Julia. It also holds e.g. for a <code class="cx nz oa ob oc b">SparseMatrixCSC</code> or a <code class="cx nz oa ob oc b">DataFrame</code>. But it’s always a good idea to check which storage order a data structure uses.</p><h2 id="1c59" class="pp oh gl bf oi pq pr ps ol pt pu pv oo nl pw px py np pz qa qb nt qc qd qe qf bk">CategoricalArrays</h2><p id="2dba" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">The if-statement in <code class="cx nz oa ob oc b">count_words_base</code> is executed for each element of the DTM. So it surely helps to optimise this part of the function.</p><p id="9716" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The parameter <code class="cx nz oa ob oc b">y</code> is not a “normal” array which would store the words “ham” or “spam” 33,716 times. It is a <code class="cx nz oa ob oc b"><a class="af gi" href="https://github.com/JuliaData/CategoricalArrays.jl?tab=readme-ov-file" rel="noopener ugc nofollow" target="_blank">CategoricalArray</a></code> which stores these two words exactly once and uses internally an array of integers to store the 33,716 different “ham” and “spam” values (which are represented by the numbers 1 and 2). We can access this numerical representation using the function <code class="cx nz oa ob oc b">levelcode</code>. So <code class="cx nz oa ob oc b">y[1]</code> results in <code class="cx nz oa ob oc b">"ham"</code>, whereas <code class="cx nz oa ob oc b">levelcode(y[1])</code> gives us 1.</p><p id="526e" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Therefore we can replace the whole if-statement by the following single line (resulting in the first optimised version <code class="cx nz oa ob oc b">count_words_01</code>):</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="daf0" class="qo oh gl oc b bg qp qq l qr qs">wcounts[levelcode(y[doc]),col] += X[doc, col]</span></pre><p id="41e8" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This gives us a runtime of <strong class="ne gm">18.006 s</strong> which is an improvement of about 10%.</p><h2 id="6e43" class="pp oh gl bf oi pq pr ps ol pt pu pv oo nl pw px py np pz qa qb nt qc qd qe qf bk">A more efficient Matrix</h2><p id="22fd" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Often memory efficient data structures are less efficient when it comes to accessing their elements. So I suspected that a (dense) matrix (i.e. a 2-dimensional <code class="cx nz oa ob oc b">Array</code>) might be more performant than the sparse matrix used for the DTM.</p><p id="4c80" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As a <strong class="ne gm">point of reference</strong> I created a <strong class="ne gm">dense matrix</strong> <code class="cx nz oa ob oc b">Xref</code> (filled with random numbers) of the same size as <code class="cx nz oa ob oc b">Xpart</code>: <code class="cx nz oa ob oc b">Xref = rand(0:9, 33716, 10000)</code>.</p><p id="1549" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This matrix has the following runtimes:</p><ul class=""><li id="1ea2" class="nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ph pi pj bk"><code class="cx nz oa ob oc b">count_words_base</code>: <strong class="ne gm">2.378 s</strong></li><li id="ce91" class="nc nd gl ne b hj pk ng nh hm pl nj nk nl pm nn no np pn nr ns nt po nv nw nx ph pi pj bk"><code class="cx nz oa ob oc b">count_words_01</code>: <strong class="ne gm">0.942 s</strong></li></ul><p id="66cb" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So there must be a real problem with the DTM produced by <code class="cx nz oa ob oc b">CountTransformer</code>. Already the baseline implementation gives us a speedup of more than 8x and the optimisation used in <code class="cx nz oa ob oc b">count_words_01</code> is more effective in this case and reduces the runtime to less than half of the baseline number!</p><p id="ade8" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As mentioned above, <code class="cx nz oa ob oc b">CountTransfomer</code> doesn’t produce an actual <code class="cx nz oa ob oc b">SparseMatrixCSC</code> but a <code class="cx nz oa ob oc b">LinearAlgebra.Adjoint{Int64,SparseMatrixCSC{Int64, Int64}}</code>. I.e. the sparse matrix is wrapped in some other structure. This could be a problem. Therefore I tried to extract the actual sparse matrix … which proved to be difficult and expensive: It takes almost 17 s to do this!</p><p id="f39e" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But the resulting <strong class="ne gm">“pure” sparse matrix</strong> is much more efficient:</p><ul class=""><li id="2fd8" class="nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ph pi pj bk"><code class="cx nz oa ob oc b">count_words_base</code>: <strong class="ne gm">3.22 s</strong></li><li id="9d2e" class="nc nd gl ne b hj pk ng nh hm pl nj nk nl pm nn no np pn nr ns nt po nv nw nx ph pi pj bk"><code class="cx nz oa ob oc b">count_words_01</code>: <strong class="ne gm">1.435 s</strong></li></ul><p id="1636" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As we have to add almost 17 s for the extraction to these numbers, this doesn’t really improve the process as a whole. So I was looking for alternatives and found these within the <code class="cx nz oa ob oc b"><a class="af gi" href="https://github.com/JuliaText/TextAnalysis.jl?tab=readme-ov-file" rel="noopener ugc nofollow" target="_blank">TextAnalysis</a></code>-package, which also has a function to create a DTM. The creation is as performant as with <code class="cx nz oa ob oc b">CountTransformer</code>, but it produces a “pure” sparse matrix directly.</p><p id="fb0d" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So we get the runtime numbers for the sparse matrix without having to add another 17 s. This results in a <strong class="ne gm">speedup</strong> at this point of 20.363/1.435 = <strong class="ne gm">14.2.</strong></p><h1 id="9658" class="og oh gl bf oi oj ok hl ol om on ho oo op oq or os ot ou ov ow ox oy oz pa pb bk">OPT2: Multithreading</h1><p id="82f3" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">With Julia it is relatively easy to use multithreading. Especially in our case where we iterate over a data structure and access in each iteration another part of that data structure. So each iteration could potentially be executed within another thread without having to care about data access conflicts.</p><p id="5d14" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this setting we just have to put the macro <code class="cx nz oa ob oc b">@threads</code> in front of the <code class="cx nz oa ob oc b">for</code>-statement and Julia does the rest for us. I.e. it distributes the different iterations to the threads which are available on a particular machine. As the M3 chip has eight kernels, I’ve set the <code class="cx nz oa ob oc b">JULIA_NUM_THREADS</code> environment variable to 8 and changed the for-loop-part of the <code class="cx nz oa ob oc b">count_words</code>-function as follows (resulting in the next optimised version <code class="cx nz oa ob oc b">count_words_02</code>):</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="57d8" class="qo oh gl oc b bg qp qq l qr qs">@threads for col in 1:nwords<br/>    for doc in 1:ndocs<br/>        wcounts[levelcode(y[doc]),col] += X[doc, col]<br/>    end<br/>end</span></pre><p id="d844" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This gives us a runtime of <strong class="ne gm">231 ms</strong> which is a <strong class="ne gm">speedup</strong> of 20.363/0.231 = <strong class="ne gm">88.2.</strong></p><h1 id="ab99" class="og oh gl bf oi oj ok hl ol om on ho oo op oq or os ot ou ov ow ox oy oz pa pb bk">OPT3: GPU and Matrix Operations</h1><p id="5ca2" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Getting even more performance is often achieved by using the GPU. But this can only be done, if the algorithm fits the quite special computing structure of a GPU. Ideally your algorithm should be made up of vector and matrix operations. So let’s explore, if our <code class="cx nz oa ob oc b">count_words</code> function can be adapted this way.</p><h2 id="1dc8" class="pp oh gl bf oi pq pr ps ol pt pu pv oo nl pw px py np pz qa qb nt qc qd qe qf bk">Filtering Rows</h2><p id="152e" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Our example from above with just three documents D1, D2 and D3 is perhaps a good starting point to get a better understanding. <code class="cx nz oa ob oc b">X</code> and <code class="cx nz oa ob oc b">y</code> for that simple example are as follows:</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="aff5" class="qo oh gl oc b bg qp qq l qr qs">X = [2 1 1 1 1 1 0 0 0;               y = ["ham", "spam", "ham"]<br/>     1 0 0 0 0 1 1 1 0;<br/>     1 1 1 0 0 0 0 0 1]</span></pre><p id="2d93" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The function <code class="cx nz oa ob oc b">count_words</code> adds the numbers in the columns, but only for specific rows. In this example, first rows 1 and 3 are added and then we are looking at row 2. I.e. we need sort of a filter for the rows and then we can just sum up columns.</p><p id="bbf0" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In Julia it is possible to index an array using a <code class="cx nz oa ob oc b">BitArray</code>. I.e. <code class="cx nz oa ob oc b">X[[1,0,1],:]</code> will give as rows 1 and 3 of <code class="cx nz oa ob oc b">X</code> and <code class="cx nz oa ob oc b">X[[0,1,0],:]</code> gives us row 2. We can get these “filters”, if we replace “ham” and “spam” in <code class="cx nz oa ob oc b">y</code> by ones and zeros and convert it to the following matrix:</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="33f6" class="qo oh gl oc b bg qp qq l qr qs">yb = [1 0;<br/>      0 1;<br/>      1 0]</span></pre><p id="c345" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So <code class="cx nz oa ob oc b">yb[:,1]</code> would be the first filter and <code class="cx nz oa ob oc b">yb[:,2]</code> the second one.</p><p id="0de2" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For the spam model we can convert the <code class="cx nz oa ob oc b">CategoricalArray</code> <code class="cx nz oa ob oc b">y</code> to such a bit matrix with the following function (<code class="cx nz oa ob oc b">y.refs</code> is the internal representation using just integers):</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="e188" class="qo oh gl oc b bg qp qq l qr qs">function y_as_bitmatrix(y)<br/>    spam = y.refs .== 2<br/>    ham = y.refs .== 1<br/>    return([ham spam]) # Bit-Matrix (one column per category)<br/>end</span></pre><p id="620f" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using this representation of <code class="cx nz oa ob oc b">y</code> we can implement <code class="cx nz oa ob oc b">count_words</code> as follows:</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="f1dd" class="qo oh gl oc b bg qp qq l qr qs">function count_words_03(X::AbstractMatrix{Int64},y::BitMatrix)<br/>    nwords = size(X,2).   # number of words in dictionary<br/>    ncats = size(y,2)     # number of categories in `y`<br/>    wcounts = ones(Int64, ncats, nwords) # matrix for storing the word counts by category<br/>    for cat in 1:ncats<br/>        @threads for col in 1:nwords<br/>            wcounts[cat,col] = sum(X[y[:,cat],col])<br/>        end<br/>    end<br/>    return(wcounts)<br/>end</span></pre><p id="33a6" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This variant has a runtime of <strong class="ne gm">652 ms </strong>(on CPU). So not faster than our last version above, but we are still exploring.</p><h2 id="8864" class="pp oh gl bf oi pq pr ps ol pt pu pv oo nl pw px py np pz qa qb nt qc qd qe qf bk">Dot Product</h2><p id="a78e" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Let’s go back again to the simple three document example:</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="ad5e" class="qo oh gl oc b bg qp qq l qr qs">X = [2 1 1 1 1 1 0 0 0;               yb = [1 0;<br/>     1 0 0 0 0 1 1 1 0;                     0 1;<br/>     1 1 1 0 0 0 0 0 1]                     1 0]</span></pre><p id="245e" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can also achieve our goal, if we compute the <em class="ny">dot product</em> of each column in <code class="cx nz oa ob oc b">X</code> first with the first column of <code class="cx nz oa ob oc b">yb</code> and then doing the same with the second column of <code class="cx nz oa ob oc b">yb</code>. This leads to <code class="cx nz oa ob oc b">count_words_04</code>:</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="c8da" class="qo oh gl oc b bg qp qq l qr qs">function count_words_04(X::AbstractMatrix{Int64},y::BitMatrix)<br/>    nwords = size(X,2)    # number of words in dictionary<br/>    ncats = size(y,2)     # number of categories in `y`<br/>    wcounts = ones(Int64, ncats, nwords) # matrix for storing the word counts by category<br/>    for cat in 1:ncats<br/>        @threads for col in 1:nwords<br/>            wcounts[cat,col] = dot(X[:,col], y[:,cat])<br/>        end<br/>    end<br/>    return(wcounts)<br/>end</span></pre><p id="4e5d" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This results in a runtime of <strong class="ne gm">4.96 ms</strong> (on CPU) which is now a <strong class="ne gm">speedup</strong> of 20.363/0.00496 = <strong class="ne gm">4,105.4!</strong></p><p id="1782" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This drastic improvement needs perhaps some explanation. Two things go here hand in hand:</p><ul class=""><li id="f2e6" class="nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ph pi pj bk">Vector operations like the dot product are super optimised in Julia relying on proven libraries like <a class="af gi" href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms" rel="noopener ugc nofollow" target="_blank">BLAS</a>.</li><li id="7ffb" class="nc nd gl ne b hj pk ng nh hm pl nj nk nl pm nn no np pn nr ns nt po nv nw nx ph pi pj bk">The sparse matrix type is very efficient in this context. Our dense reference matrix <code class="cx nz oa ob oc b">Xref</code> has a runtime of only 455.7 ms in this case.</li></ul><h2 id="85e0" class="pp oh gl bf oi pq pr ps ol pt pu pv oo nl pw px py np pz qa qb nt qc qd qe qf bk">Matrix Multiplication</h2><p id="1913" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Taking the ideas from above a bit further we can represent <code class="cx nz oa ob oc b">yb</code> in its transposed form as follows:</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="4c73" class="qo oh gl oc b bg qp qq l qr qs">ybt = [1 0 1;        X = [2 1 1 1 1 1 0 0 0;<br/>       0 1 0]             1 0 0 0 0 1 1 1 0;<br/>                          1 1 1 0 0 0 0 0 1]                     </span></pre><p id="39a9" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This depiction makes the shortest and probably most elegant version of <code class="cx nz oa ob oc b">count_words</code> more or less obvious. It is just a matrix multiplication:</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="ccc6" class="qo oh gl oc b bg qp qq l qr qs">function count_words_05(X::AbstractMatrix{Int64},y::BitMatrix)<br/>    transpose(Y) * X<br/>end</span></pre><p id="0bb0" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is also the fastest version with <strong class="ne gm">1.105 ms</strong> leading to a speedup of 20.363/0.00105 = <strong class="ne gm">19,393</strong>!</p><p id="e370" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Multithreading is here implicit as the underlying BLAS library is by default multithreaded. The number of threads used can be obtained by <code class="cx nz oa ob oc b">BLAS.get_num_threads()</code>.</p><p id="d238" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Moreover this solution scales well. Applied to the complete dataset, the matrix <code class="cx nz oa ob oc b">X</code> with 33,716 x 159,093 elements, it takes 13.57 ms to complete. This is a speedup of 241.076/0.01357 =<strong class="ne gm"> 17,765.</strong></p><h1 id="5262" class="og oh gl bf oi oj ok hl ol om on ho oo op oq or os ot ou ov ow ox oy oz pa pb bk">OPT4: GPU</h1><p id="40a2" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Finally, applying the last variant to the GPU can be done using the <code class="cx nz oa ob oc b">Metal.jl</code>-package. For this purpose the matrices used have only to be converted to their corresponding metal array type using the <code class="cx nz oa ob oc b">mtl</code>-function:</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="a3ec" class="qo oh gl oc b bg qp qq l qr qs">const mtl_Xpart = mtl(Xpart)<br/>const mtl_yb = mtl(yb)</span></pre><p id="028f" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <code class="cx nz oa ob oc b">count_words</code> variant for the GPU is, apart from the data types, the same as above:</p><pre class="qg qh qi qj qk ql oc qm bp qn bb bk"><span id="de5d" class="qo oh gl oc b bg qp qq l qr qs">function count_words_06(X::MtlMatrix,y::MtlMatrix)<br/>    transpose(y) * X<br/>end</span></pre><p id="a178" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Its runtime is only <strong class="ne gm">0.306 ms</strong>. But copying the data to the GPU (using <code class="cx nz oa ob oc b">mtl</code>) takes much longer than the time gained by running the algorithm on the GPU. So it’s not really faster.</p><p id="c0ca" class="pw-post-body-paragraph nc nd gl ne b hj nf ng nh hm ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Apart from that, the <code class="cx nz oa ob oc b">Metal</code>-package for Apple silicon GPUs is not quite as mature as e.g. <code class="cx nz oa ob oc b">CUDA.jl</code>. This becomes visible when trying to convert the large matrix <code class="cx nz oa ob oc b">X</code> to a metal array: The conversion stops with an error message.</p><h1 id="e02b" class="og oh gl bf oi oj ok hl ol om on ho oo op oq or os ot ou ov ow ox oy oz pa pb bk">Conclusion</h1><p id="b870" class="pw-post-body-paragraph nc nd gl ne b hj pc ng nh hm pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Of course not every algorithm can be converted to such a concise variant as we have in <code class="cx nz oa ob oc b">count_words_05</code>. But even the more “classic” implementation <code class="cx nz oa ob oc b">count_words_04</code> is more than 4,000 times faster than our starting point. Many of the performance tuning measures presented in this article can be applied to other functions too. Apart from this, I would recommend anyone, who wants go get more performance out of a Julia program, to follow the “<a class="af gi" href="https://docs.julialang.org/en/v1/manual/performance-tips/" rel="noopener ugc nofollow" target="_blank">Performance Tips</a>” in the Julia documentation.</p></div></div></div></div>    
</body>
</html>