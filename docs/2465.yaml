- en: Keep the Gradients Flowing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保持梯度流动
- en: 原文：[https://towardsdatascience.com/keep-the-gradients-flowing-5b9bf0098e3d?source=collection_archive---------4-----------------------#2024-10-09](https://towardsdatascience.com/keep-the-gradients-flowing-5b9bf0098e3d?source=collection_archive---------4-----------------------#2024-10-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/keep-the-gradients-flowing-5b9bf0098e3d?source=collection_archive---------4-----------------------#2024-10-09](https://towardsdatascience.com/keep-the-gradients-flowing-5b9bf0098e3d?source=collection_archive---------4-----------------------#2024-10-09)
- en: 'Optimizing Sparse Neural Networks: Understanding Gradient Flow for Faster Training,
    Improved Efficiency, and Better Performance in Deep Learning Models'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化稀疏神经网络：理解梯度流动以加速训练、提高效率，并在深度学习模型中提升性能
- en: '[](https://medium.com/@cristianleo120?source=post_page---byline--5b9bf0098e3d--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--5b9bf0098e3d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5b9bf0098e3d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5b9bf0098e3d--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--5b9bf0098e3d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@cristianleo120?source=post_page---byline--5b9bf0098e3d--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--5b9bf0098e3d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5b9bf0098e3d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5b9bf0098e3d--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--5b9bf0098e3d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5b9bf0098e3d--------------------------------)
    ·22 min read·Oct 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5b9bf0098e3d--------------------------------)
    ·22分钟阅读·2024年10月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e6ffb4275682d74e70e4b11882020c8c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6ffb4275682d74e70e4b11882020c8c.png)'
- en: AI Image generated depicting the gradients flowing in Neural Networks
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能图像生成，展示了神经网络中梯度流动的情况
- en: In recent years, the AI field has been obsessed with building larger and larger
    neural networks, believing that more complexity leads to better performance. Indeed,
    this approach has yielded incredible results, leading to breakthroughs in image
    recognition, language translation, and countless other areas.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，人工智能领域一直痴迷于构建越来越大的神经网络，认为更高的复杂度会带来更好的性能。的确，这一方法已经取得了令人难以置信的成果，带来了图像识别、语言翻译以及其他许多领域的突破。
- en: But there’s a catch. Just like a massive, overly complex machine can be costly
    to build and maintain, these enormous neural networks require significant computational
    resources and time to train. They can be slow, demanding a lot of memory and power,
    making deploying them on devices with limited resources challenging. Plus, they
    often become prone to “memorizing” the training data rather than truly understanding
    the underlying patterns, leading to poor performance on unseen data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但这里有个问题。就像一个庞大且过于复杂的机器既昂贵又难以维护一样，这些巨大的神经网络也需要大量的计算资源和时间来训练。它们可能训练得很慢，需要大量的内存和计算能力，这使得在资源有限的设备上部署它们变得具有挑战性。此外，它们往往更容易“记住”训练数据，而不是真正理解潜在的模式，从而在未见数据上的表现较差。
- en: Sparse neural networks have partly solved the problem above. Think of sparse
    NNs as a leaner version of classic NNs. They carefully remove unnecessary parts
    and connections, resulting in a more efficient and leaner model that still maintains
    its power. They can train faster, require less memory, and are often more robust…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏神经网络在一定程度上解决了上述问题。可以将稀疏神经网络看作是经典神经网络的精简版。它们小心地移除不必要的部分和连接，从而形成一个更高效、更加精简的模型，同时仍保持其强大的能力。它们能够更快地训练，需求更少的内存，而且通常更具鲁棒性……
