["```py\nintel-extension-for-pytorch==2.2\ntransformers==4.35.2\ntorch==2.2.0\nhuggingface_hub\n```", "```py\nfrom huggingface_hub import notebook_login, Repository\n\n# Login to Hugging Face\nnotebook_login()\n```", "```py\nimport torch\nimport intel_extension_for_pytorch as ipex\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n\nModel = 'meta-llama/Meta-Llama-3-8B-Instruct'\n\nmodel = AutoModelForCausalLM.from_pretrained(Model)\ntokenizer = AutoTokenizer.from_pretrained(Model)\n```", "```py\nqconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(\n  weight_dtype=torch.quint4x2, # or torch.qint8\n  lowp_mode=ipex.quantization.WoqLowpMode.NONE, # or FP16, BF16, INT8\n)\ncheckpoint = None # optionally load int4 or int8 checkpoint\n\n# PART 3: Model optimization and quantization\nmodel_ipex = ipex.llm.optimize(model, quantization_config=qconfig, low_precision_checkpoint=checkpoint)\n\ndel model \n```", "```py\nsystem= \"\"\"\\n\\n You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. If you don't know the answer to a question, please don't share false information.\"\"\"\nuser= \"\\n\\n You are an expert in astronomy. Can you tell me 5 fun facts about the universe?\"\nmodel_answer_1 = 'None'\n\nllama_prompt_tempate = f\"\"\"\n<|begin_of_text|>\\n<|start_header_id|>system<|end_header_id|>{system}\n<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>{user}\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>{model_answer_1}<|eot_id|>\n\"\"\"\n\ninputs = tokenizer(llama_prompt_tempate, return_tensors=\"pt\").input_ids\n```", "```py\nstreamer = TextStreamer(tokenizer,skip_prompt=True)\n\nwith torch.inference_mode():\n    tokens = model_ipex.generate(\n        inputs,\n        streamer=streamer,\n        pad_token_id=128001,\n        eos_token_id=128001,\n        max_new_tokens=300,\n        repetition_penalty=1.5,\n)\n```"]