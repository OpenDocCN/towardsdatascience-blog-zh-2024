<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Stein’s Paradox</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Stein’s Paradox</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/steins-paradox-ba493f46e181?source=collection_archive---------3-----------------------#2024-09-30">https://towardsdatascience.com/steins-paradox-ba493f46e181?source=collection_archive---------3-----------------------#2024-09-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="63c3" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Why the Sample Mean Isn’t Always the Best</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@tim.sumner?source=post_page---byline--ba493f46e181--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Tim Sumner" class="l ep by dd de cx" src="../Images/34225cf53f510e5002042bb1be00f423.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*LnDNDjXggGQVqNQQp-HpiQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ba493f46e181--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@tim.sumner?source=post_page---byline--ba493f46e181--------------------------------" rel="noopener follow">Tim Sumner</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ba493f46e181--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">6</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/6b655a6b15a187994596266f9ae44958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BmcYWB2mHtxDsZeGSeC_FQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="45ab" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Averaging is one of the most fundamental tools in statistics, second only to counting. While its simplicity might make it seem intuitive, averaging plays a central role in many mathematical concepts because of its robust properties. Major results in probability, such as the Law of Large Numbers and the Central Limit Theorem, emphasize that averaging isn’t just convenient — it’s often optimal for estimating parameters. Core statistical methods, like Maximum Likelihood Estimators and Minimum Variance Unbiased Estimators (MVUE), reinforce this notion.</p><p id="9d77" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, this long-held belief was upended in 1956[1] when Charles Stein made a breakthrough that challenged over 150 years of estimation theory.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="bd14" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">History</h1><p id="7261" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Averaging has traditionally been seen as an effective method for estimating the central tendency of a random variable’s distribution, particularly in the case of a normal distribution. The normal (or Gaussian) distribution is characterized by its bell-shaped curve and two key parameters: the mean (θ) and the standard deviation (σ). The mean indicates the center of the curve, while the standard deviation reflects the spread of the data.</p><p id="a24d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Statisticians often work backward, inferring these parameters from observed data. Gauss demonstrated that the sample mean maximizes the likelihood of observing the data, making it an unbiased estimator — meaning it doesn’t systematically overestimate or underestimate the true mean (θ).</p><p id="4f45" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Further developments in statistical theory confirmed the utility of the sample mean, which minimizes the expected squared error when compared to other linear unbiased estimators. Researchers like R.A. Fisher and Jerzy Neyman expanded on these ideas by introducing risk functions, which measure the average squared error for different values of θ. They found that while both the mean and the median have constant risk, the mean consistently delivers lower risk, confirming its superiority.</p><p id="98d8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, Stein’s theorem showed that when estimating three or more parameters simultaneously, the sample mean becomes inadmissible. In these cases, biased estimators can outperform the sample mean by offering lower overall risk. Stein’s work revolutionized statistical inference, improving accuracy in multi-parameter estimation.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="42c8" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">The James-Stein Estimator</h1><p id="2ee7" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">The James-Stein[2] estimator is a key tool in the paradox discovered by Charles Stein. It challenges the notion that the sample mean is always the best estimator, particularly when estimating multiple parameters simultaneously. The idea behind the James-Stein estimator is to “shrink” individual sample means toward a central value (the grand mean), which reduces the overall estimation error.</p><p id="f5c9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To clarify this, let’s start by considering a vector <strong class="ne fr">x</strong> representing the sample means of several variables (not necessarily independent). If we take the average of all these means, we get a single value, denoted by <em class="ph">μ</em>, which we refer to as the grand mean. The James-Stein estimator works by moving each sample mean closer to this grand mean, reducing their variance.</p><p id="ae0d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The general formula[3] for the James-Stein estimator is:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pi"><img src="../Images/b6463074c2b0989a789b4f70b3cd145f.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*b9q-8FKrqaPwp1e5HSPMlg.png"/></div></figure><p id="cca3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Where:</p><ul class=""><li id="35cf" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pj pk pl bk"><strong class="ne fr">x </strong>is the sample mean vector.</li><li id="d5e7" class="nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk"><em class="ph">μ</em> is the grand mean (the average of the sample means).</li><li id="e284" class="nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk"><em class="ph">c</em> is a shrinkage factor that lies between 0 and 1. It determines how much we pull the individual means toward the grand mean.</li></ul><p id="e308" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The goal here is to reduce the distance between the individual sample means and the grand mean. For example, if one sample mean is far from the grand mean, the estimator will shrink it toward the center, smoothing out the variation in the data.</p><p id="9c3f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The value of <em class="ph">c</em>, the shrinkage factor, depends on the data and what is being estimated. A sample mean vector follows a multivariate normal distribution, so if this is what we are trying to estimate, the formula becomes:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pr"><img src="../Images/845bc8961fa121fa17e89599bfd7566d.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*5kdtPQC2Cr2fQYwGcaOuIA.png"/></div></figure><p id="feb2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Where:</p><ul class=""><li id="e1da" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pj pk pl bk"><em class="ph">p </em>is the number of parameters being estimated (the length of <strong class="ne fr">x</strong>).</li><li id="cea7" class="nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk"><em class="ph">σ</em>² is the variance of the sample mean vector <strong class="ne fr">x</strong>.</li><li id="da40" class="nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk">The term (<em class="ph">p</em> — 2)/<strong class="ne fr">||x||</strong>² adjusts the amount of shrinkage based on the data’s variance and the number of parameters.</li></ul><p id="34ad" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Key Assumptions and Adjustments</strong></p><p id="3968" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One key assumption for using the James-Stein estimator is that the variance <em class="ph">σ</em>² is the same for all variables, which is often not realistic in real-world data. However, this assumption can be mitigated by standardizing the data, so all variables have the same variance. Alternatively, you can average the individual variances into one pooled estimate. This approach works especially well with larger datasets, where the variance differences tend to diminish as sample size increases.</p><p id="a68e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once the data is standardized or pooled, the shrinkage factor can be applied to adjust each sample mean appropriately.</p><p id="dced" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Choosing the Shrinkage Factor</strong></p><p id="a4fa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The shrinkage factor <em class="ph">c</em> is crucial because it controls how much the sample means are pulled toward the grand mean. A value of <em class="ph">c</em> close to 1 means little to no shrinkage, which resembles the behavior of the regular sample mean. Conversely, a <em class="ph">c</em> close to 0 means significant shrinkage, pulling the sample means almost entirely toward the grand mean.</p><p id="f322" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The optimal value of <em class="ph">c</em> depends on the specific data and the parameters being estimated, but the general guideline is that the more parameters there are (i.e., larger <em class="ph">p</em>), the more shrinkage is beneficial, as this reduces the risk of over fitting to noisy data.</p><p id="4e0a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Implementing the James-Stein Estimator in Code</strong></p><p id="5b8f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here are the James-Stein estimator functions in R, Python, and Julia:</p><pre class="mm mn mo mp mq ps pt pu bp pv bb bk"><span id="8d72" class="pw oh fq pt b bg px py l pz qa">## R ##<br/>james_stein_estimator &lt;- function(Xbar, sigma2 = 1) {<br/>  p &lt;- length(Xbar)<br/>  norm_X2 &lt;- sum(Xbar^2)<br/>  shrinkage_factor &lt;- max(0, 1 - (p - 2) * mean(sigma2) / norm_X2)<br/>  return(shrinkage_factor * Xbar)<br/>}<br/><br/>## Python ##<br/>import numpy as np<br/><br/>def james_stein_estimator(Xbar, sigma2=1):<br/>    p = len(Xbar)<br/>    norm_X2 = np.sum(Xbar**2)<br/>    shrinkage_factor = max(0, 1 - (p - 2) * np.mean(sigma2) / norm_X2)<br/>    return shrinkage_factor * Xbar<br/><br/>## Julia ##<br/>function james_stein_estimator(Xbar, sigma2=1)<br/>    p = length(Xbar)<br/>    norm_X2 = sum(Xbar.^2)<br/>    shrinkage_factor = max(0, 1 - (p - 2) * mean(sigma2) / norm_X2)<br/>    return shrinkage_factor * Xbar<br/>end</span></pre></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3e11" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Example</h1><p id="3933" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">To demonstrate the versatility of this technique, I will generate a 6-dimensional data set with each column containing numerical data from various random distributions. Here are the specific distributions and parameters of each I will be using:</p><p id="d67d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ph">X1 ~ t-distribution </em>(ν = 3)<em class="ph"><br/>X2 ~ Binomial </em>(<em class="ph">n</em> = 10, <em class="ph">p</em> = 0.4)<em class="ph"><br/>X3 ~ Gamma </em>(<em class="ph">α = </em>3, <em class="ph">β =</em> 2)<em class="ph"><br/>X4 ~ Uniform </em>(<em class="ph">a = </em>0, <em class="ph">b</em> = 1)<em class="ph"><br/>X5 ~ Exponential </em>(<em class="ph">λ</em> = 50)<em class="ph"><br/>X6 ~ Poisson </em>(<em class="ph">λ</em> = 2)</p><p id="afb8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note each column in this data set contains independent variables, in that no column should be correlated with another since they were created independently. This is not a requirement to use this method. It was done this way simply for simplicity and to demonstrate the paradoxical nature of this result. If you’re not entirely familiar with any or all of these distributions, I’ll include a simple visual of each of the univariate columns of the randomly generated data. This is simply one iteration of 1,000 generated random variables from each of the aforementioned distributions.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/9f6c629c812641ed53f6c49971e0cd10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0i9XyjFxlH5hp9z2zGjZSQ.png"/></div></div></figure><p id="4710" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It should be clear from the histograms above that not all of these variables follow a normal distribution implying the dataset as a whole is not multivariate normal.</p><p id="f946" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since the true distributions of each are known, we know the true averages of each. The average of this multivariate dataset can be expressed in vector form with each row entry representing the average of the variable respectively. In this example,</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qc"><img src="../Images/fb5b2428002e82d4f2bef3b8b086024e.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*9IkheBie2ezpCOhio7lJpg.png"/></div></figure><p id="0168" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Knowing the true averages of each variable will allow us to be able to measure how close the sample mean, or James Stein estimator gets implying the closer the better. Below is the experiment I ran in R code which generated each of the 6 random variables and tested against the true averages using the Mean Squared Error. This experiment was then ran 10,000 times using four different sample sizes: 5, 50, 500, and 5,000.</p><pre class="mm mn mo mp mq ps pt pu bp pv bb bk"><span id="e56d" class="pw oh fq pt b bg px py l pz qa">set.seed(42)<br/>## Function to calculate Mean Squared Error ##<br/>mse &lt;- function(x, true_value)<br/>  return( mean( (x - true_value)^2 ) )<br/>## True Average ##<br/>mu &lt;- c(0, 4, 1.5, 0.5, 0.02, 2)<br/>## Store Average and J.S. Estimator Errors ##<br/>Xbar.MSE &lt;- list(); JS.MSE &lt;- list()<br/>for(n in c(5, 50, 500, 5000)){ # Testing sample sizes of 5, 30, 200, and 5,000<br/>  for(i in 1:1e4){ # Performing 10,000 iterations<br/>    <br/>    ## Six Random Variables ##<br/>    X1 &lt;- rt(n, df = 3)<br/>    X2 &lt;- rbinom(n, size = 10, prob = 0.4)<br/>    X3 &lt;- rgamma(n, shape = 3, rate = 2)<br/>    X4 &lt;- runif(n)<br/>    X5 &lt;- rexp(n, rate = 50)<br/>    X6 &lt;- rpois(n, lambda = 2)<br/>    <br/>    X &lt;- cbind(X1, X2, X3, X4, X5, X6)<br/>    <br/>    ## Estimating Std. Dev. of Each and Standardizing Data ##<br/>    sigma &lt;- apply(X, MARGIN = 2, FUN = sd)<br/>    <br/>    ## Sample Mean ##<br/>    Xbar &lt;- colMeans(X)<br/>    <br/>    ## J.S. Estimator ##<br/>    JS.Xbar &lt;- james_stein_estimator(Xbar=Xbar, sigma2=sigma/n)<br/><br/>    Xbar.MSE[[as.character(n)]][i] &lt;- mse(Xbar, mu)<br/>    JS.MSE[[as.character(n)]][i] &lt;- mse(JS.Xbar, mu)<br/>    <br/>  }<br/>}<br/>sapply(Xbar.MSE, mean) # Avg. Sample Mean MSE<br/>sapply(JS.MSE, mean) # Avg. James-Stein MSE</span></pre><p id="cffe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">From all 40,000 trails, the total average MSE of each sample size is computed by running the last two lines. The results of each can be seen in the table below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/b3f8fa5c9d6aece3d817aabd2c0191eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*SmKCfVhfPmkjngMr7DcmlA.png"/></div></figure><p id="9ea3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The results of this of this simulation show that the James-Stein estimator is consistently better than the sample mean using the MSE, but that this difference decreases as the sample size increases.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="b575" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk"><strong class="al">Conclusion</strong></h1><p id="9bd2" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">The James-Stein estimator demonstrates a paradox in estimation: it is possible to improve estimates by incorporating information from seemingly independent variables. While the difference in MSE might be negligible for large sample sizes, this result sparked much debate when it was first introduced. The discovery marked a key turning point in statistical theory, and it remains relevant today for multi-parameter estimation.</p><p id="0690" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you’d like to explore more, check out <a class="af qe" href="https://joe-antognini.github.io/machine-learning/steins-paradox" rel="noopener ugc nofollow" target="_blank">this detailed article on Stein’s paradox</a> and other references used to write this document.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="c8ab" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">References</h1><p id="9c1b" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">[1] Stein, C. (1956). <a class="af qe" href="https://www.degruyter.com/document/doi/10.1525/9780520313880-018/html" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr">Inadmissibility of the usual estimator for the mean of a multivariate normal distribution</strong></a>. <em class="ph">Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability</em>, 1, 197–206.</p><p id="8c2b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[2] Stein, C. (1961). <a class="af qe" href="https://link.springer.com/chapter/10.1007/978-1-4612-0919-5_30" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr">Estimation with quadratic loss</strong></a>. <em class="ph">In S. S. Gupta &amp; J. O. Berger (Eds.), Statistical Decision Theory and Related Topics</em> (Vol. 1, pp. 361–379). Academic Press.</p><p id="dfbf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[3] Efron, B., &amp; Morris, C. (1977). <a class="af qe" href="https://link.springer.com/chapter/10.1007/978-0-387-75692-9_7" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr">Stein’s paradox in statistics</strong></a>. <em class="ph">Scientific American</em>, 236(5), 119–127</p></div></div></div></div>    
</body>
</html>