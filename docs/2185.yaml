- en: Reducing the Size of AI Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减小AI模型的尺寸
- en: 原文：[https://towardsdatascience.com/reducing-the-size-of-ai-models-4ab4cfe5887a?source=collection_archive---------3-----------------------#2024-09-07](https://towardsdatascience.com/reducing-the-size-of-ai-models-4ab4cfe5887a?source=collection_archive---------3-----------------------#2024-09-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/reducing-the-size-of-ai-models-4ab4cfe5887a?source=collection_archive---------3-----------------------#2024-09-07](https://towardsdatascience.com/reducing-the-size-of-ai-models-4ab4cfe5887a?source=collection_archive---------3-----------------------#2024-09-07)
- en: Running large AI models on edge devices
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在边缘设备上运行大型AI模型
- en: '[](https://medium.com/@arunnanda?source=post_page---byline--4ab4cfe5887a--------------------------------)[![Arun
    Nanda](../Images/48836e7e13dbe0821bed6902209f2d25.png)](https://medium.com/@arunnanda?source=post_page---byline--4ab4cfe5887a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4ab4cfe5887a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4ab4cfe5887a--------------------------------)
    [Arun Nanda](https://medium.com/@arunnanda?source=post_page---byline--4ab4cfe5887a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@arunnanda?source=post_page---byline--4ab4cfe5887a--------------------------------)[![Arun
    Nanda](../Images/48836e7e13dbe0821bed6902209f2d25.png)](https://medium.com/@arunnanda?source=post_page---byline--4ab4cfe5887a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4ab4cfe5887a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4ab4cfe5887a--------------------------------)
    [Arun Nanda](https://medium.com/@arunnanda?source=post_page---byline--4ab4cfe5887a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4ab4cfe5887a--------------------------------)
    ·9 min read·Sep 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4ab4cfe5887a--------------------------------)
    ·9分钟阅读·2024年9月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a1084f4a571f30f8cc548dcd6d831b94.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1084f4a571f30f8cc548dcd6d831b94.png)'
- en: Image created using Pixlr
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由Pixlr生成
- en: 'AI models, particularly Large Language Models (LLMs), need large amounts of
    GPU memory. For example, in the case of the [LLaMA 3.1 model, released in July
    2024](https://ai.meta.com/blog/meta-llama-3-1/), the [memory requirements](https://huggingface.co/blog/llama31#how-much-memory-does-llama-31-need)
    are:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: AI模型，尤其是大型语言模型（LLMs），需要大量GPU内存。例如，在[2024年7月发布的LLaMA 3.1模型](https://ai.meta.com/blog/meta-llama-3-1/)中，[内存需求](https://huggingface.co/blog/llama31#how-much-memory-does-llama-31-need)如下：
- en: The 8 billion parameter model needs 16 GB memory in 16-bit floating point weights
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 8亿参数的模型需要16GB内存，采用16位浮点数权重
- en: The larger 405 billion parameter model needs 810 GB using 16-bit floats
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更大的4050亿参数模型使用16位浮点数时需要810GB内存
- en: 'In a full-sized machine learning model, the weights are represented as 32-bit
    floating point numbers. Modern models have hundreds of millions to tens (or even
    hundreds) of billions of weights. Training and running such large models is very
    resource-intensive:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个完整的机器学习模型中，权重表示为32位浮点数。现代模型有数百万到数十亿（甚至数百亿）个权重。训练和运行这样的大型模型非常耗费资源：
- en: It takes lots of compute (processing power).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它需要大量计算（处理能力）。
- en: It requires large amounts of GPU memory.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它需要大量GPU内存。
- en: 'It consumes large amounts of energy, In particular, the biggest contributors
    to this energy consumption are:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它消耗大量能源，尤其是导致这种能源消耗的最大因素是：
- en: '- Performing a large number of computations (matrix multiplications) using
    32-bit floats'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 使用32位浮点数执行大量计算（矩阵乘法）'
- en: '- Data transfer — copying the model data from memory to the processing units.'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 数据传输 — 将模型数据从内存复制到处理单元。'
- en: 'Being highly resource-intensive has two main drawbacks:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 高资源消耗有两个主要缺点：
- en: '**Training**: Models with large GPU requirements are expensive and slow to
    train. This limits new research and development to groups with big budgets.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练**：需要大量GPU资源的模型训练昂贵且缓慢。这限制了新的研究和开发只能由预算充足的团队进行。'
- en: '**Inference**: Large models need specialized (and expensive) hardware (dedicated
    GPU servers) to run. They cannot be run on consumer devices like regular laptops
    and mobile phones.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理**：大型模型需要专用的（且昂贵的）硬件（专用GPU服务器）才能运行。它们不能在像普通笔记本电脑和手机这样的消费级设备上运行。'
- en: 'Thus, end-users and personal devices must necessarily access AI models via
    a paid API service. This leads to a suboptimal user experience for both consumer
    apps and their developers:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终用户和个人设备必须通过付费API服务来访问AI模型。这导致了消费者应用程序及其开发者都面临次优的用户体验：
- en: It introduces latency due to network access and server load.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它引入了由于网络访问和服务器负载带来的延迟。
- en: It also introduces budget constraints on developers building AI-based software.
    Being able to run AI models locally — on consumer devices, would mitigate these
    problems.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还介绍了开发AI软件时的预算限制。能够在消费者设备上本地运行AI模型，将有助于缓解这些问题。
- en: Reducing the size of AI models is therefore an active area of research and development.
    This is the first of a series of articles discussing ways of reducing model size,
    in particular by a method called quantization. These articles are based on studying
    the original research papers. Throughout the series, you will find links to the
    PDFs of the reference papers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，减少AI模型的大小是一个活跃的研究和开发领域。这是系列文章中的第一篇，讨论了减少模型大小的不同方法，特别是通过量化这一方法。这些文章基于对原始研究论文的学习。在整个系列中，你会找到参考论文PDF的链接。
- en: The current introductory article gives an overview of different approaches to
    reducing model size. It introduces quantization as the most promising method and
    as a subject of current research.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本文为入门文章，概述了减少模型大小的不同方法。它将量化介绍为最具前景的方法，并且是当前研究的热点。
- en: '[*Quantizing the Weights of AI Models*](https://medium.com/@arunnanda/quantizing-the-weights-of-ai-models-39f489455194)
    illustrates the arithmetics of quantization using numerical examples.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*量化AI模型的权重*](https://medium.com/@arunnanda/quantizing-the-weights-of-ai-models-39f489455194)通过数值实例展示了量化的算术原理。'
- en: '[*Quantizing Neural Network Models*](https://medium.com/@arunnanda/quantizing-neural-network-models-8ce49332f1d3)
    discusses the architecture and process of applying quantization to neural network
    models, including the basic mathematical principles. In particular, it focuses
    on how to train models to perform well during inference with quantized weights.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*量化神经网络模型*](https://medium.com/@arunnanda/quantizing-neural-network-models-8ce49332f1d3)讨论了将量化应用于神经网络模型的架构和过程，包括基本的数学原理。特别是，它着重于如何训练模型，使其在推理时使用量化权重仍能表现良好。'
- en: '[*Different Approaches to Quantization*](https://medium.com/@arunnanda/different-approaches-to-quantization-e3fac905bd5a)
    explains different types of quantization, such as quantizing to different precisions,
    the granularity of quantization, deterministic and stochastic quantization, and
    different quantization methods used during training models.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*量化的不同方法*](https://medium.com/@arunnanda/different-approaches-to-quantization-e3fac905bd5a)解释了不同类型的量化方法，如量化为不同精度、量化粒度、确定性与随机量化，以及在训练模型过程中使用的不同量化方法。'
- en: '[*Extreme Quantization: 1-bit AI Models*](https://medium.com/@arunnanda/extreme-quantization-1-bit-ai-models-07169ee29d96)
    is about binary quantization, which involves reducing the model weights from 32-bit
    floats to binary numbers. It shows the mathematical principles of binary quantization
    and summarizes the approach adopted by the first researchers who implemented binary
    quantization of transformer-based models (BERT).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*极限量化：1位AI模型*](https://medium.com/@arunnanda/extreme-quantization-1-bit-ai-models-07169ee29d96)讲解了二进制量化的内容，涉及将模型权重从32位浮动数减少到二进制数字。它展示了二进制量化的数学原理，并总结了首次实施变换器模型（如BERT）二进制量化的研究者所采用的方法。'
- en: '[*Understanding 1-bit Large Language Models*](https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3)
    presents recent work on quantizing large language models (LLMs) to use 1-bit (binary)
    weights, i.e. {-1, 1}. In particular, the focus is on BitNet, which was the first
    successful attempt to redesign the transformer architecture to use 1-bit weights.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*理解1位大规模语言模型*](https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3)介绍了将大规模语言模型（LLM）量化为使用1位（即二进制）权重（{-1,
    1}）的最新研究成果。特别地，焦点放在了BitNet上，它是首次成功地重新设计变换器架构以使用1位权重的尝试。'
- en: '[*Understanding 1.58-bit Language Models*](https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a)
    discusses the quantization of neural network models, in particular LLMs, to use
    ternary weights ({-1, 0, +1}). This is also referred to as 1.58-bit quantization
    and it has proved to deliver very promising results. This topic has attracted
    much attention in the tech press in the first half of 2024\. The background explained
    in the previous articles helps to get a deeper understanding of how and why LLMs
    are quantized to 1.58 bits.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*理解1.58位语言模型*](https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a)讨论了将神经网络模型，特别是LLM，量化为使用三元权重（{-1,
    0, +1}）的技术。这也被称为1.58位量化，已证明能够提供非常有前景的结果。2024年上半年，关于这一主题的讨论在科技媒体中引起了广泛关注。之前文章中解释的背景有助于深入理解为什么以及如何将LLM量化为1.58位。'
- en: Approaches to Reducing Model Size
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降低模型大小的方法
- en: Not relying on expensive hardware would make AI applications more accessible
    and accelerate the development and adoption of new models. Various methods have
    been proposed and attempted to tackle this challenge of building high-performing
    yet small-sized models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 不依赖昂贵的硬件将使 AI 应用变得更加普及，并加速新模型的开发和应用。为了解决这一构建高性能且小型模型的挑战，已经提出并尝试了多种方法。
- en: Low-rank decomposition
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低秩分解
- en: Neural networks express their weights in the form of high-dimensional tensors.
    It is mathematically possible to decompose a high-ranked tensor into a set of
    smaller-dimensional tensors. This makes the computations more efficient. This
    is known as [Tensor rank decomposition](https://en.wikipedia.org/wiki/Tensor_rank_decomposition).
    For example, in Computer Vision models, weights are typically 4D tensors.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络以高维张量的形式表示其权重。数学上，可以将一个高秩张量分解为一组低维张量，从而使计算更高效。这种方法被称为 [张量秩分解](https://en.wikipedia.org/wiki/Tensor_rank_decomposition)。例如，在计算机视觉模型中，权重通常是
    4D 张量。
- en: Lebedev et al, in their 2014 paper titled [*Speeding-Up Convolutional Neural
    Networks Using Fine-Tuned Cp-Decomposition*](https://arxiv.org/pdf/1412.6553)
    demonstrate that using a common decomposition technique, Canonical Polyadic Decomposition
    (CP Decomposition), convolutions with 4D weight tensors (which are common in computer
    vision models) can be reduced to a series of four convolutions with smaller 2D
    tensors. [Low Rank Adaptation](https://arxiv.org/abs/2106.09685) (LoRA) is a modern
    (proposed in 2021) technique based on a similar approach applied to Large Language
    Models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Lebedev 等人在他们 2014 年的论文 [*使用微调的 Cp 分解加速卷积神经网络*](https://arxiv.org/pdf/1412.6553)
    中展示了使用一种常见的分解技术——典范多元分解（CP 分解），将具有 4D 权重张量的卷积（这种情况在计算机视觉模型中较为常见）简化为一系列使用较小的 2D
    张量的卷积。[低秩自适应](https://arxiv.org/abs/2106.09685)（LoRA）是一种现代技术（提出于 2021 年），其原理基于类似的方法，并应用于大型语言模型。
- en: Pruning
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剪枝
- en: Another way to reduce network size and complexity is by eliminating connections
    from a network. In a 1989 paper titled [Optimal Brain Damage](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html),
    Le Cun et al propose deleting connections with small magnitudes and retraining
    the model. Applied iteratively, this approach reduces half or more of the weights
    of a neural network. The [full paper is available on the website of Le Cun](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf),
    who (as of 2024) is the chief AI scientist at Meta (Facebook).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 降低网络大小和复杂度的另一种方法是通过消除网络中的连接。在 1989 年的一篇名为 [Optimal Brain Damage](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)
    的论文中，Le Cun 等人提出了删除小幅度连接并重新训练模型的方法。通过反复应用，这种方法可以减少神经网络中一半或更多的权重。[Le Cun 的完整论文可以在其网站上查看](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf)，Le
    Cun 目前（截至 2024 年）是 Meta（Facebook）的首席 AI 科学家。
- en: 'In the context of large language models, pruning is especially challenging.
    SparseGPT, first shared by Frantar et al in a 2023 paper titled [*SparseGPT: Massive
    Language Models Can be Accurately Pruned in One-Shot*](https://arxiv.org/pdf/2301.00774),
    is a well-known pruning method that successfully reduces by half the size of LLMs
    without losing much accuracy. Pruning LLMs to a fraction of their original size
    has not yet been feasible. The article, [Pruning for Neural Networks](https://leimao.github.io/article/Neural-Networks-Pruning/),
    by Lei Mao, gives an introduction to this technique.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '在大语言模型的背景下，剪枝尤其具有挑战性。SparseGPT 是由 Frantar 等人在 2023 年发表的论文 [*SparseGPT: 大型语言模型可以通过一次性修剪精确地减小*](https://arxiv.org/pdf/2301.00774)
    中首次提出的一个著名剪枝方法，它成功地将大型语言模型的大小减少了一半，同时几乎不损失准确性。将大型语言模型的大小减少到原始的一小部分仍然不可行。Lei Mao
    的文章 [剪枝与神经网络](https://leimao.github.io/article/Neural-Networks-Pruning/) 介绍了这一技术。'
- en: Knowledge Distillation
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识蒸馏
- en: Knowledge transfer is a way of training a smaller (student) neural network to
    replicate the behavior of a larger and more complex (teacher) neural network.
    In many cases, the student is trained based on the final prediction layer of the
    teacher network. In other approaches, the student is also trained based on the
    intermediate hidden layers of the teacher. Knowledge Distillation has been used
    successfully in some cases, but in general, the student networks are unable to
    generalize to new unseen data. They tend to be overfitted to replicate the teacher’s
    behavior within the training dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 知识迁移是一种训练较小（学生）神经网络以复制较大且更复杂（教师）神经网络行为的方法。在许多情况下，学生网络是基于教师网络的最终预测层进行训练的。在其他方法中，学生网络还会基于教师网络的中间隐藏层进行训练。知识蒸馏在某些情况下取得了成功，但通常情况下，学生网络无法泛化到新的未见过的数据。它们往往过度拟合以在训练数据集内复制教师的行为。
- en: Quantization
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化
- en: In a nutshell, quantization involves starting with a model with 32-bit or 16-bit
    floating point weights and applying various techniques to reduce the precision
    of the weights, to 8-bit integers or even binary (1-bit), without sacrificing
    model accuracy. Lower precision weights have lower memory and computational needs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，量化涉及从一个具有32位或16位浮动点权重的模型开始，应用各种技术来降低权重的精度，至8位整数甚至二进制（1位），而不牺牲模型准确性。低精度权重具有更低的内存和计算需求。
- en: The rest of this article, from the next section onwards, and the rest of this
    series give an in-depth understanding of quantization.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分，从下一节开始，以及本系列的其余部分，提供了对量化的深入理解。
- en: Hybrid
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合
- en: It is also possible to apply different compression techniques in sequence. Han
    et al, in their 2016 paper titled [*Compressing Deep Neural Networks with Pruning,
    Trained Quantization, and Huffman Coding*](https://arxiv.org/pdf/1510.00149)*,*
    apply pruning followed by quantization followed by Huffman coding to compress
    the AlexNet model by a factor of 35X, to reduce the model size from 240 MB to
    6.9 MB without significant loss of accuracy. As of July 2024, such approaches
    have yet to be tried on low-bit LLMs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以按顺序应用不同的压缩技术。Han等人在2016年发表的论文 [*通过剪枝、训练量化和霍夫曼编码压缩深度神经网络*](https://arxiv.org/pdf/1510.00149)*,*
    中，应用了剪枝、量化和霍夫曼编码，成功地将AlexNet模型压缩了35倍，将模型大小从240 MB减少到6.9 MB，且没有显著损失准确度。截至2024年7月，这种方法尚未在低位LLM上进行尝试。
- en: Quantization 101
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化基础
- en: 'The “size” of a model is mainly determined by two factors:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的“大小”主要由两个因素决定：
- en: The number of weights (or parameters)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重（或参数）数量
- en: The size (length in bits) of each parameter.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个参数的大小（位数）。
- en: It is well-established that the number of parameters in a model is crucial to
    its performance — hence, reducing the number of parameters is not a viable approach.
    Thus, attempting to reduce the length of each weight is a more promising angle
    to explore.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 已经确立，模型中的参数数量对其性能至关重要——因此，减少参数数量并不是一个可行的方法。因此，尝试减少每个权重的长度是一个更有前景的研究方向。
- en: Traditionally, LLMs are trained with 32-bit weights. Models with 32-bit weights
    are often referred to as full-sized models. Reducing the length (or precision)
    of model parameters is called quantization. 16-bit and 8-bit quantization are
    common approaches. More radical approaches involve quantizing to 4 bits, 2 bits,
    and even 1 bit. To understand how higher precision numbers are quantized to lower
    precision numbers, refer to [*Quantizing the Weights of AI Models*](https://medium.com/@arunnanda/quantizing-the-weights-of-ai-models-39f489455194),
    with examples of quantizing model weights.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，LLM（大型语言模型）使用32位权重进行训练。具有32位权重的模型通常被称为全尺寸模型。减少模型参数的长度（或精度）被称为量化。16位和8位量化是常见的方式。更激进的方法包括量化到4位、2位，甚至1位。要了解如何将高精度数字量化为低精度数字，请参考
    [*量化AI模型的权重*](https://medium.com/@arunnanda/quantizing-the-weights-of-ai-models-39f489455194)，其中有关于量化模型权重的示例。
- en: Quantization helps with reducing the memory requirements and reducing the computational
    cost of running the model. Typically, model weights are quantized. It is also
    common to quantize the activations (in addition to quantizing the weights). The
    function that maps the floating point weights to their lower precision integer
    versions is called the quantizer, or quantization function.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 量化有助于减少内存需求并降低运行模型的计算成本。通常，模型的权重会被量化。除了量化权重外，还常常对激活进行量化。将浮动点权重映射到其低精度整数版本的函数称为量化器，或量化函数。
- en: Quantization in Neural Networks
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络中的量化
- en: 'Simplistically, the linear and non-linear transformation applied by a neural
    network layer can be expressed as:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，神经网络层应用的线性和非线性变换可以表示为：
- en: '![](../Images/2cb3cb4f2a97e8b82ff63dc9256d958e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cb3cb4f2a97e8b82ff63dc9256d958e.png)'
- en: 'In the above expression:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的表达式中：
- en: z denotes the output of the non-linear function. It is also referred to as the
    activation.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: z 表示非线性函数的输出，也称为激活值。
- en: Sigma is the non-linear activation function. It is often the sigmoid function
    or the tanh function.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigma 是非线性激活函数，通常是sigmoid函数或tanh函数。
- en: W is the weight matrix of that layer
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: W 是该层的权重矩阵
- en: a is the input vector
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: a 是输入向量
- en: B is the bias vector
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B 是偏置向量
- en: The matrix multiplication of the weight and the input is referred to as convolution.
    Adding the bias to the product matrix is called accumulation.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重与输入的矩阵乘法被称为卷积。将偏置加到乘积矩阵上称为累加。
- en: The term passed to the sigma (activation) function is called a Multiply-Accumulate
    (MAC) operation.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传递给sigma（激活）函数的项被称为乘加（MAC）操作。
- en: Most of the computational workload in running neural networks comes from the
    convolution operation — which involves the multiplication of many floating point
    numbers. Large models with many weights have a very large number of convolution
    operations.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 运行神经网络的计算负载大部分来自卷积操作——这涉及到大量浮点数的乘法。拥有大量权重的大型模型会有非常多的卷积操作。
- en: This computational cost could potentially be reduced by doing the multiplication
    in lower-precision integers instead of floating-point numbers. In an extreme case,
    as discussed in [*Understanding 1.58-bit Language Models*](https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a),
    the 32-bit weights could potentially be represented by ternary numbers {-1, 0,
    +1} and the multiplication operations would be replaced by much simpler addition
    and subtraction operations. This is the intuition behind quantization.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用低精度整数而不是浮点数进行乘法，可能会减少计算成本。在极端情况下，正如在[*理解1.58位语言模型*](https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a)中讨论的那样，32位权重可能通过三元数{-1,
    0, +1}来表示，乘法操作将被更简单的加法和减法操作替代。这就是量化背后的直觉。
- en: The computational cost of digital arithmetic is quadratically related to the
    number of bits. As studied by [Siddegowda et al in their paper on Neural Network
    Quantization](https://arxiv.org/pdf/2201.08442) (Section 2.1), using 8-bit integers
    instead of 32-bit floats leads to 16x higher performance, in terms of energy consumption.
    When there are billions of weights, the cost savings are very significant.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 数字运算的计算成本与位数的平方成正比。正如[Siddegowda等人在他们关于神经网络量化的论文](https://arxiv.org/pdf/2201.08442)（第2.1节）中研究的那样，使用8位整数代替32位浮点数会使能效提高16倍。当权重数量达到数十亿时，节省的成本非常可观。
- en: The quantizer function maps the high-precision (typically 32-bit floating point
    weights) to lower-precision integer weights.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 量化函数将高精度（通常是32位浮点数的权重）映射到低精度整数权重。
- en: '![](../Images/c3816b4d4012514ef1e090adb7df803e.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3816b4d4012514ef1e090adb7df803e.png)'
- en: The “knowledge” the model has acquired via training is represented by the value
    of its weights. When these weights are quantized to lower precision, a portion
    of their information is also lost. The challenge of quantization is to reduce
    the precision of the weights while maintaining the accuracy of the model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 模型通过训练所获得的“知识”由其权重的值表示。当这些权重被量化为低精度时，它们的部分信息也会丢失。量化的挑战在于如何在降低权重精度的同时保持模型的准确性。
- en: One of the main reasons some quantization techniques are effective is that the
    relative values of the weights and the statistical properties of the weights are
    more important than their actual values. This is especially true for large models
    with millions or billions of weights. Later articles on quantized [BERT models
    — BinaryBERT and BiBERT](https://medium.com/@arunnanda/extreme-quantization-1-bit-ai-models-07169ee29d96),
    on [BitNet — which is a transformer LLM quantized down to binary weights](https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3),
    and on [BitNet b1.58 — which quantizes transformers to use ternary weights](https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a),
    illustrate the use of successful quantization techniques. [*A Visual Guide to
    Quantization*](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization),
    by Maarten Grootendoorst, has many illustrations and graphic depictions of quantization.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一些量化技术有效的主要原因之一是，权重的相对值和权重的统计特性比它们的实际值更为重要。对于拥有数百万或数十亿个权重的大型模型来说，这一点尤其成立。后续关于量化的文章，如[量化BERT模型——BinaryBERT和BiBERT](https://medium.com/@arunnanda/extreme-quantization-1-bit-ai-models-07169ee29d96)、[BitNet——一种将变压器LLM量化为二进制权重的模型](https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3)，以及[BitNet
    b1.58——将变压器量化为三元权重的模型](https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a)，都展示了成功的量化技术应用。[*量化的视觉指南*](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization)由Maarten
    Grootendoorst编写，包含了许多量化的插图和图示。
- en: Quantized Inference
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化推理
- en: Inference means using an AI model to generate predictions, such as the classification
    of an image, or the completion of a text string. When using a full-precision model,
    the entire data flow through the model is in 32-bit floating point numbers. When
    running inference through a quantized model, many parts — but not all, of the
    data flow are in lower precision.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 推理是指使用AI模型生成预测，例如对图像的分类或对文本串的补全。当使用全精度模型时，整个数据流通过模型时采用32位浮点数表示。使用量化模型进行推理时，许多部分——但并非所有——的数据流采用较低精度表示。
- en: The bias is typically not quantized because the number of bias terms is much
    less than the number of weights in a model. So, the cost savings is not enough
    to justify the overhead of quantization. The accumulator’s output is in high precision.
    The output of the activation is also in higher precision.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置通常不进行量化，因为偏置项的数量远少于模型中的权重数量。因此，节省的成本不足以证明量化开销的合理性。累加器的输出采用高精度格式，激活函数的输出也采用更高精度的格式。
- en: Conclusion
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This article discussed the need to reduce the size of AI models and gave a high-level
    overview of ways to achieve reduced model sizes. It then introduced the basics
    of quantization, a method that is currently the most successful in reducing model
    sizes while managing to maintain an acceptable level of accuracy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论了减少AI模型规模的必要性，并对实现模型规模缩减的方法进行了高层次的概述。接着介绍了量化的基础知识，这是一种目前在缩减模型规模的同时，能够保持可接受的准确度水平的最成功方法。
- en: The goal of this series is to give you enough background to appreciate the extreme
    quantization of language models, starting from [simpler models like BERT](https://medium.com/@arunnanda/extreme-quantization-1-bit-ai-models-07169ee29d96)
    before finally [discussing 1-bit LLMs](https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3)
    and the [recent work on 1.58-bit LLMs](https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a).
    To this end, the next few articles in this series present a semi-technical deep
    dive into the different subtopics like the [mathematical operations behind quantization](https://medium.com/@arunnanda/quantizing-the-weights-of-ai-models-39f489455194)
    and the [process of training quantized models](https://medium.com/@arunnanda/quantizing-neural-network-models-8ce49332f1d3).
    It is important to understand that because this is an active area of research
    and development, there are few standard procedures and different workers adopt
    innovative methods to achieve better results.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列的目标是为您提供足够的背景知识，以便理解语言模型的极端量化，从[像BERT这样的简单模型](https://medium.com/@arunnanda/extreme-quantization-1-bit-ai-models-07169ee29d96)开始，最终[讨论1比特大型语言模型（LLMs）](https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3)以及[近期关于1.58比特LLMs的研究成果](https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a)。为此，本系列接下来的几篇文章将深入探讨不同的子话题，例如[量化背后的数学运算](https://medium.com/@arunnanda/quantizing-the-weights-of-ai-models-39f489455194)和[量化模型训练的过程](https://medium.com/@arunnanda/quantizing-neural-network-models-8ce49332f1d3)。需要理解的是，由于这是一个活跃的研究和开发领域，目前尚无统一的标准程序，不同的研究者采用创新的方法以获得更好的结果。
