- en: How to Parallelize Copy Activities in Azure Data Factory
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在 Azure 数据工厂中并行化复制活动
- en: 原文：[https://towardsdatascience.com/how-to-parallelize-copy-activities-in-azure-data-factory-5d21df7b8562?source=collection_archive---------10-----------------------#2024-10-10](https://towardsdatascience.com/how-to-parallelize-copy-activities-in-azure-data-factory-5d21df7b8562?source=collection_archive---------10-----------------------#2024-10-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-parallelize-copy-activities-in-azure-data-factory-5d21df7b8562?source=collection_archive---------10-----------------------#2024-10-10](https://towardsdatascience.com/how-to-parallelize-copy-activities-in-azure-data-factory-5d21df7b8562?source=collection_archive---------10-----------------------#2024-10-10)
- en: Optimizing data transfer for enterprise data lakes
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化企业数据湖的数据传输
- en: '[](https://rebremer.medium.com/?source=post_page---byline--5d21df7b8562--------------------------------)[![René
    Bremer](../Images/e422c4b84e225d2a949251ebc24dbd2c.png)](https://rebremer.medium.com/?source=post_page---byline--5d21df7b8562--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5d21df7b8562--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5d21df7b8562--------------------------------)
    [René Bremer](https://rebremer.medium.com/?source=post_page---byline--5d21df7b8562--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rebremer.medium.com/?source=post_page---byline--5d21df7b8562--------------------------------)[![René
    Bremer](../Images/e422c4b84e225d2a949251ebc24dbd2c.png)](https://rebremer.medium.com/?source=post_page---byline--5d21df7b8562--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5d21df7b8562--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5d21df7b8562--------------------------------)
    [René Bremer](https://rebremer.medium.com/?source=post_page---byline--5d21df7b8562--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5d21df7b8562--------------------------------)
    ·7 min read·Oct 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5d21df7b8562--------------------------------)
    ·阅读时间：7分钟·2024年10月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4de18df463f08ff5d5c76c52f55ae36d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4de18df463f08ff5d5c76c52f55ae36d.png)'
- en: Skewed data distribution - image by [Vackground.com on Unsplash](https://unsplash.com/@vackground)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分布不均 - 图片来自 [Vackground.com 在 Unsplash](https://unsplash.com/@vackground)
- en: 1\. Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 引言
- en: Azure Data Factory (ADF) is a popular tool for moving data at scale, particularly
    in Enterprise Data Lakes. It is commonly used to ingest and transform data, often
    starting by copying data from on-premises to Azure Storage. From there, data is
    moved through different zones following a medallion architecture. ADF is also
    essential for creating and restoring backups in case of disasters like data corruption,
    malware, or account deletion.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 数据工厂（ADF）是一个广泛使用的数据迁移工具，特别是在企业数据湖中。它通常用于摄取和转换数据，通常通过将数据从本地复制到 Azure 存储开始。之后，数据根据奖章架构（medallion
    architecture）通过不同的区域进行移动。ADF 对于在数据损坏、恶意软件或帐户删除等灾难情况下创建和恢复备份也至关重要。
- en: This implies that ADF is used to move large amounts of data, TBs and sometimes
    even PBs. It is thus important to optimize copy performance and so to limit throughput
    time. A common way to improve ADF performance is to parallelize copy activities.
    However, the parallelization shall happen where most of the data is and this can
    be challenging when the data lake is skewed.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 ADF 被用来迁移大量数据，通常是 TB 级，有时甚至是 PB 级。因此，优化复制性能至关重要，以缩短吞吐时间。一种常见的提升 ADF 性能的方法是并行化复制活动。然而，应该在数据量最多的地方进行并行化，当数据湖的数据分布不均时，这可能会成为一个挑战。
- en: 'In this blog post, different ADF parallelization strategies are discussed for
    data lakes and a project is deployed. The ADF solution project can be found in
    this link: `[https://github.com/rebremer/data-factory-copy-skewed-data-lake](https://github.com/rebremer/data-factory-copy-skewed-data-lake)`.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，讨论了不同的 ADF 并行化策略，适用于数据湖，并且一个项目被部署。可以在以下链接中找到 ADF 解决方案项目：[https://github.com/rebremer/data-factory-copy-skewed-data-lake](https://github.com/rebremer/data-factory-copy-skewed-data-lake)。
- en: 2\. Data lake data distribution
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 数据湖数据分布
- en: 'Data Lakes come in all sizes and manners. It is important to understand the
    data distribution within a data lake to improve copy performance. Consider the
    following situation:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖有各种规模和形式。理解数据湖中的数据分布对于提升复制性能非常重要。考虑以下情况：
- en: An Azure Storage account has N containers.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Azure 存储帐户有 N 个容器。
- en: Each container contains M folders and m levels of sub folders.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个容器包含 M 个文件夹和 m 层子文件夹。
- en: Data is evenly distributed in folders N/M/..
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据在文件夹 N/M/.. 中均匀分布。
- en: 'See also image below:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 参见下图：
- en: '![](../Images/c936540c36846cda676a41e772e8bf64.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c936540c36846cda676a41e772e8bf64.png)'
- en: 2.1 Data lake with uniformly distributed data — image by author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 2.1 数据湖与均匀分布的数据 — 图片由作者提供
- en: In this situation, copy activities can be parallelized on each container N.
    For larger data volumes, performance can be further enhanced by parallelizing
    on folders M within container N. Subsequently, per copy activity it can be configured
    how much [Data Integration Units (DIU)](https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features#data-integration-units)
    and [copy parallelization](https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features#parallel-copy)
    *within* a copy activity is used.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可以在每个容器 N 上并行化复制活动。对于更大的数据量，通过在容器 N 内的文件夹 M 上并行化，可以进一步提高性能。随后，可以为每个复制活动配置使用多少
    [数据集成单元 (DIU)](https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features#data-integration-units)
    和 [复制并行化](https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features#parallel-copy)
    *在* 复制活动中。
- en: 'Now consider the following extreme situation that the last folder Nk and Mk
    has 99% of data, see image below:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑以下极端情况：最后一个文件夹 Nk 和 Mk 拥有 99% 的数据，见下图：
- en: '![](../Images/9ab71d065115517c8f4f210c011abeab.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ab71d065115517c8f4f210c011abeab.png)'
- en: 2.2 Data lake with skewed distributed data — image by author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2 数据湖与倾斜分布的数据 — 图片由作者提供
- en: This implies that parallelization shall be done on the sub folders in Nk/Mk
    where the data is. More advanced logic is then needed to pinpoint the exact data
    locations. An Azure Function, integrated within ADF, can be used to achieve this.
    In the next chapter a project is deployed and are the parallelization options
    discussed in more detail.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着并行化应在包含数据的 Nk/Mk 子文件夹中进行。接下来需要更高级的逻辑来确定确切的数据位置。可以使用集成在 ADF 中的 Azure Function
    来实现这一点。在下一章中，将部署一个项目并更详细地讨论并行化选项。
- en: 3\. Parallelization strategy in ADF project
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. ADF 项目中的并行化策略
- en: 'In this part, the project is deployed and a copy test is run and discussed.
    The entire project can be found in project: `[https://github.com/rebremer/data-factory-copy-skewed-data-lake](https://github.com/rebremer/data-factory-copy-skewed-data-lake)`.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分中，将部署项目并运行并讨论复制测试。整个项目可以在项目中找到：[https://github.com/rebremer/data-factory-copy-skewed-data-lake](https://github.com/rebremer/data-factory-copy-skewed-data-lake)。
- en: '**3.1 Deploy project**'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.1 部署项目**'
- en: 'Run the script `[deploy_adf.ps1](https://github.com/rebremer/data-factory-copy-skewed-data-lake/blob/main/deploy_adf.ps1)`.
    In case ADF is successfully deployed, there are two pipelines deployed:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本 `[deploy_adf.ps1](https://github.com/rebremer/data-factory-copy-skewed-data-lake/blob/main/deploy_adf.ps1)`。如果
    ADF 成功部署，将会部署两个管道：
- en: '![](../Images/33e2f24a37024fac1c86bfb1020eb5c8.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33e2f24a37024fac1c86bfb1020eb5c8.png)'
- en: 3.1.1 Data Factory project with root and child pipeline — image by author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 3.1.1 包含根管道和子管道的数据工厂项目 — 图片由作者提供
- en: Subsequently, run the script `[deploy_azurefunction.ps1](https://github.com/rebremer/data-factory-copy-skewed-data-lake/blob/main/deploy_azurefunction.ps1).`
    In case the Azure Function is successfully deployed, the following code is deployed.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，运行脚本 `[deploy_azurefunction.ps1](https://github.com/rebremer/data-factory-copy-skewed-data-lake/blob/main/deploy_azurefunction.ps1)`。如果
    Azure Function 成功部署，以下代码将被部署。
- en: '![](../Images/b5c986bba658ee55871cf44775f6d265.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5c986bba658ee55871cf44775f6d265.png)'
- en: 3.1.2 Azure Function to find “pockets of data” such that ADF can better parallelize
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 3.1.2 使用 Azure Function 查找“数据口袋”，以便 ADF 可以更好地进行并行化
- en: To finally run the project, make sure that the system assigned managed identity
    of the Azure Function and Data Factory can access the storage account where the
    data is copied from and to.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要最终运行该项目，请确保 Azure Function 和数据工厂的系统分配的托管身份可以访问数据复制的存储帐户。
- en: 3.2 Parallelization used in project
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 项目中使用的并行化
- en: After the project is deployed, it can be noticed that the following tooling
    is deployed to improve the performance using parallelization.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 部署项目后，可以注意到部署了以下工具，以使用并行化来提高性能。
- en: '**Root pipeline:** Root pipeline that lists containers N on storage account
    and triggers child pipeline for each container.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**根管道：** 列出存储帐户中容器 N 并为每个容器触发子管道的根管道。'
- en: '**Child pipeline:** Child pipeline that lists folders M in a container and
    triggers recursive copy activity for each folder.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子管道：** 列出容器中文件夹 M 并为每个文件夹触发递归复制活动的子管道。'
- en: '**Switch:** Child pipeline uses a switch to decide how list folders shall be
    determined. For case “default” (even), Get Metadata is used, for case “uneven”
    an Azure Function is used.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Switch**：子管道使用开关决定如何确定列出文件夹。对于“default”（均匀）情况，使用Get Metadata；对于“uneven”（不均匀）情况，使用Azure
    Function。'
- en: '**Get Metadata:** List all root folders M in a given container N.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Get Metadata**：列出给定容器N中的所有根文件夹M。'
- en: '**Azure Function**: List all folders and sub folders that contain no more than
    X GB of data and shall be copied as a whole.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Function**：列出所有包含不超过X GB数据的文件夹及子文件夹，并作为一个整体进行复制。'
- en: '**Copy activity**: Recursively copy for all data from a given folder.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复制活动**：递归地复制给定文件夹中的所有数据。'
- en: '**DIU**: Number of Data Integration Units per copy activity.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DIU**：每个复制活动的Data Integration Units（数据集成单元）数量。'
- en: '**Copy parallelization:** *Within* a copy activity, number of parallel copy
    threads that can be started. Each thread can copy a file, maximum of 50 threads.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复制并行化**：*在*复制活动中，可以启动的并行复制线程数。每个线程可以复制一个文件，最大可支持50个线程。'
- en: In the uniformly distributed data lake, data is evenly distributed over N containers
    and M folders. In this situation, copy activities can just be parallelized on
    each folder M. This can be done using a Get Meta Data to list folders M, For Each
    to iterate over folders and copy activity per folder. See also image below.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在均匀分布的数据湖中，数据在N个容器和M个文件夹中均匀分布。在这种情况下，复制活动可以仅在每个文件夹M上进行并行化。这可以通过使用Get Metadata列出文件夹M，使用For
    Each遍历文件夹并对每个文件夹执行复制活动来完成。另见下图。
- en: '![](../Images/066f8e743fc246708fa65ffd7ccac3ba.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/066f8e743fc246708fa65ffd7ccac3ba.png)'
- en: 3.2.1 Child pipeline structure focusing on uniformly distributed data
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 3.2.1 关注均匀分布数据的子管道结构
- en: Using this strategy, this would imply that each copy activity is going to copy
    an equal amount of data. A total of N*M copy activities will be run.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种策略意味着每个复制活动将复制相等数量的数据。总共将运行N*M个复制活动。
- en: In the skewed distributed data lake, data is not evenly distributed over N containers
    and M folders. In this situation, copy activities shall be dynamically determined.
    This can be done using an Azure Function to list the data heavy folders, then
    a For Each to iterate over folders and copy activity per folder. See also image
    below.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在偏斜分布的数据湖中，数据在N个容器和M个文件夹中分布不均。在这种情况下，复制活动应动态确定。可以使用Azure Function列出数据量大的文件夹，然后通过For
    Each遍历文件夹并进行每个文件夹的复制活动。另见下图。
- en: '![](../Images/b7d66e1c87a3a493ed1c279bf3a525e6.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7d66e1c87a3a493ed1c279bf3a525e6.png)'
- en: 3.2.2 Child pipeline structure focusing on skewed distributed data
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 3.2.2 关注偏斜分布数据的子管道结构
- en: Using this strategy, copy activities are dynamically scaled in data lake where
    data can be found and parallelization is thus needed most. Although this solution
    is more complex than the previous solution since it requires an Azure Function,
    it allows for copying skewed distributed data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种策略，复制活动将在数据湖中动态扩展，数据可以找到的地方，最需要并行化。尽管该解决方案比前一个更复杂，因为它需要Azure Function，但它可以用于复制偏斜分布的数据。
- en: '3.3: Parallelization performance test'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3：并行化性能测试
- en: 'To compare the performance of different parallelization options, a simple test
    is set up as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较不同并行化选项的性能，设置了如下简单测试：
- en: Two storage accounts and 1 ADF instance using an Azure IR in region westeurope.
    Data is copied from source to target storage account.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用两个存储帐户和1个ADF实例，在westeurope区域使用Azure IR。数据从源存储帐户复制到目标存储帐户。
- en: Source storage account contains three containers with 0.72 TB of data each spread
    over multiple folders and sub folders.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源存储帐户包含三个容器，每个容器有0.72 TB的数据，数据分布在多个文件夹和子文件夹中。
- en: Data is evenly distributed over containers, no skewed data.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据在容器中均匀分布，没有偏斜数据。
- en: 'Test A: Copy 1 container with 1 copy activity using 32 DIU and 16 threads in
    copy activity (both set to auto) => 0.72 TB of data is copied, 12m27s copy time,
    average throughput is 0.99 GB/s'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 测试A：使用32 DIU和16个线程（均设置为自动）复制1个容器，1个复制活动 => 复制0.72 TB数据，复制时间12分27秒，平均吞吐量为0.99
    GB/s
- en: 'Test B: Copy 1 container with 1 copy activity using 128 DIU and 32 threads
    in copy activity => 0.72 TB of data is copied, 06m19s copy time, average throughput
    is 1.95 GB/s.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 测试B：使用128 DIU和32个线程（在复制活动中）复制1个容器，1个复制活动 => 复制0.72 TB数据，复制时间06分19秒，平均吞吐量为1.95
    GB/s。
- en: 'Test C: Copy 1 container with 1 copy activity using 200 DIU and 50 threads
    (max) => test aborted due to throttling, no performance gain compared to test
    B.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 测试C：使用200 DIU和50个线程（最大）复制1个容器，1个复制活动 => 测试因限流被中止，与测试B相比没有性能提升。
- en: 'Test D: Copy 2 containers with 2 copy activities in parallel using 128 DIU
    and 32 threads for each copy activity => 1.44 TB of data is copied, 07m00s copy
    time, average throughput is 3.53 GB/s.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 测试 D：使用 128 DIU 和每个复制活动 32 个线程并行复制 2 个容器 => 复制了 1.44 TB 的数据，复制时间 07 分钟 00 秒，平均吞吐量为
    3.53 GB/s。
- en: 'Test E: Copy 3 containers with 3 copy activities in parallel using 128 DIU
    and 32 threads for each copy activity => 2.17 TB of data is copied, 08m07s copy
    time, average throughput is 4.56 GB/s. See also screenshot below.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 测试 E：使用 128 DIU 和每个复制活动 32 个线程并行复制 3 个容器 => 复制了 2.17 TB 的数据，复制时间 08 分钟 07 秒，平均吞吐量为
    4.56 GB/s。请参见下方截图。
- en: '![](../Images/40bcd1131e29b613192084883172f6ef.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40bcd1131e29b613192084883172f6ef.png)'
- en: '3.3 Test E: Copy throughput of 3 parallel copy activities of 128 DIU and 32
    threads, data size is 3*0.72TB'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 3.3 测试 E：3 个并行复制活动的复制吞吐量，使用 128 DIU 和 32 个线程，数据大小为 3*0.72TB
- en: In this, it shall be noticed that ADF does not immediately start copying since
    there is a startup time. For an Azure IR this is ~10 seconds. This startup time
    is fixed and its impact on throughput can be neglected for large copies. Also,
    maximum ingress of a storage account is [60 Gbps](https://learn.microsoft.com/en-us/azure/storage/common/scalability-targets-standard-account#scale-targets-for-standard-storage-accounts)
    (=7.5 GB/s). There cannot be scaled above this number, unless additional capacity
    is requested on the storage account.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，需要注意的是，ADF 并不会立即开始复制，因为存在启动时间。对于 Azure IR，这大约是 10 秒。这个启动时间是固定的，对于大规模复制来说，它对吞吐量的影响可以忽略不计。此外，存储帐户的最大入口带宽为[60
    Gbps](https://learn.microsoft.com/en-us/azure/storage/common/scalability-targets-standard-account#scale-targets-for-standard-storage-accounts)（即
    7.5 GB/s）。除非请求存储帐户额外的容量，否则无法超出这个值。
- en: 'The following takeaways can be drawn from the test:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从测试中可以得出以下结论：
- en: Significant performance can already be gained by increasing DIU and parallel
    settings *within* copy activity.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过增加复制活动中的 DIU 和并行设置，已经可以显著提高性能。
- en: By running copy pipelines in parallel, performance can be further increased.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过并行运行复制管道，可以进一步提高性能。
- en: In this test, data was uniformly distributed across two containers. If the data
    had been skewed, with all data from container 1 located in a sub folder of container
    2, both copy activities would need to target container 2\. This ensures similar
    performance to Test D.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此测试中，数据均匀分布在两个容器中。如果数据存在倾斜，即容器 1 中的所有数据都位于容器 2 的子文件夹中，则两个复制活动都需要针对容器 2 进行。这将确保与测试
    D 相似的性能。
- en: If the data location is unknown beforehand or deeply nested, an Azure Function
    would be needed to identify the data pockets to make sure the copy activities
    run in the right place.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据位置事先未知或数据深度嵌套，则需要使用 Azure Function 来识别数据存储区，以确保复制活动在正确的位置运行。
- en: 4\. Conclusion
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 结论
- en: Azure Data Factory (ADF) is a popular tool to move data at scale. It is widely
    used for ingesting, transforming, backing up, and restoring data in Enterprise
    Data Lakes. Given its role in moving large volumes of data, optimizing copy performance
    is crucial to minimize throughput time.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 数据工厂（ADF）是一个流行的大规模数据迁移工具。它广泛用于数据湖中数据的摄取、转换、备份和恢复。考虑到它在大规模数据迁移中的角色，优化复制性能对于最小化吞吐时间至关重要。
- en: In this blog post, we discussed the following parallelization strategies to
    enhance the performance of data copying to and from Azure Storage.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们讨论了以下并行化策略，以增强从 Azure 存储复制数据的性能。
- en: '*Within* a copy activity, utilize standard Data Integration Units (DIU) and
    parallelization threads within a copy activity.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在复制活动中，利用标准的数据集成单元（DIU）和并行化线程。
- en: Run copy activities in parallel. If data is known to be evenly distributed,
    standard functionality in ADF can be used to parallelize copy activities across
    each container (N) and root folder (M).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行运行复制活动。如果已知数据均匀分布，可以在 ADF 中使用标准功能将复制活动并行化到每个容器（N）和根文件夹（M）中。
- en: Run copy activities where the data is. In case this is not known on beforehand
    or deeply nested, an Azure Function can be leveraged to locate the data. However,
    incorporating an Azure Function within an ADF pipeline adds complexity and should
    be avoided when not needed.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据所在位置运行复制活动。如果事先未知或数据深度嵌套，可以使用 Azure Function 来定位数据。然而，在 ADF 管道中集成 Azure Function
    会增加复杂性，除非必要，否则应避免使用。
- en: Unfortunately, there is no silver bullet solution and it always requires analyses
    and testing to find the best strategy to improve copy performance for Enterprise
    Data Lakes. This article aimed to give guidance in choosing the best strategy.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，并没有一种灵丹妙药的解决方案，总是需要通过分析和测试来找到最佳策略，以提高企业数据湖的复制性能。本文旨在为选择最佳策略提供指导。
