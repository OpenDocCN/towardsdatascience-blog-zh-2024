# 量化 AI 巨兽

> 原文：[https://towardsdatascience.com/quantizing-the-ai-colossi-017e121a27c5?source=collection_archive---------0-----------------------#2024-04-15](https://towardsdatascience.com/quantizing-the-ai-colossi-017e121a27c5?source=collection_archive---------0-----------------------#2024-04-15)

## 精简巨人 第二部分：神经网络量化

[](https://natecibik.medium.com/?source=post_page---byline--017e121a27c5--------------------------------)[![Nate Cibik](../Images/008c22b715ddf4f1d0f9970142edc09f.png)](https://natecibik.medium.com/?source=post_page---byline--017e121a27c5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--017e121a27c5--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--017e121a27c5--------------------------------) [Nate Cibik](https://natecibik.medium.com/?source=post_page---byline--017e121a27c5--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--017e121a27c5--------------------------------) ·阅读时间 81 分钟 ·2024年4月15日

--

![](../Images/3876a06f9d219ada6c44bf205b2641ab.png)

图片由作者使用 DALL-E 3 制作

近年来，[transformer](https://arxiv.org/abs/1706.03762) 神经网络架构与[将各种问题表述为自监督序列预测任务的形式](https://navigating-the-future-62ea60f27046)之间建立了强大的联盟。这个结合使得研究人员能够利用大量未标记的顺序数据，训练出前所未有的大规模基础模型，这些模型在多个领域展现出了近乎模仿人类智能的惊人能力。随着实践效用的新高度被释放，人工智能（AI）迅速进入了主流生活和讨论领域，今天几乎没有人不知道，曾经只是虚构的硅基智能领域如今已变得非常具体和真实。

然而，伴随着人工智能（AI）能力的爆炸性增长，模型规模的迅速膨胀也紧密相随，模型参数已达到数百亿（在某些情况下甚至是万亿）之巨。一个强大的新技术被传递给了全世界，但它只能通过庞大的硬件集群来服务。回响着早期人工智能时代的挑战，拥有这些强大能力的诱惑在消费者硬件或边缘设备上变得异常强烈，推动了压缩这些预训练庞然大物的动机，这立刻产生了巨大的资金和人才流动，进入了*模型压缩*的研究领域，并重新激活了几种久负盛名的技术，包括剪枝、量化、知识蒸馏和参数高效微调。

在[《简化巨头》系列的第一部分](/streamlining-giants-8a26aa1e91d3)中，我们开始讨论通过模型压缩使大型语言模型（LLM）能够普及化的议题，探索了神经网络*剪枝*的丰富研究遗产，从其起源到最近在包含数十亿或数百亿参数的LLM中的应用。在这一过程中，我们发现这些大型模型可以通过*无结构*或*结构化*地去除网络中最不重要的参数，从而在性能损失最小的情况下显著压缩，同时计算负担也得到显著减轻。我们还看到，虽然剪枝能够生成紧凑的模型，使其能够在资源受限的环境中运行，但这一过程通常需要计算梯度信息和/或重新训练模型以恢复性能。这意味着该方法在历史上只有具备足够计算资源以训练原始模型的人才能使用，而在LLM的情况下，这需要数百万美元的投入。虽然这使得通过剪枝压缩模型的手段在过去远离了那些最需要它的人，但我们看到最近的研究提出了通过低秩梯度或甚至仅使用前向传播信息来进行压缩的高度可接入的方法。此外，随着大模型的重新训练得益于参数高效微调方法的同步进展，剪枝现在也可以在消费级硬件上进行。

在本篇文章中，我们探讨了一种正交的模型压缩方法：*量化*通过降低网络存储和操作数字的精度，来提高模型的计算效率和内存需求，这些数字可能包括权重、激活值，或者两者兼有。虽然量化可以指任何精度的下降，例如从32位浮点数到16位浮点数，但它通常还涉及到进入整数空间，这可以加速操作并在消费级硬件上部署。正如我们将看到的那样，量化是压缩大型语言模型（LLM）的一个极为强大的方法，能够显著减少计算开销和硬件需求，而性能下降仅为轻微，甚至在某些情况下不存在性能下降，这使得它成为当今大型模型世界中最广泛使用的模型压缩技术。此外，通过调整数字精度的级别，我们可以根据具体应用场景调节精度/效率的权衡。

在这个过程中，我们将看到量化与我们之前遇到的*剪枝*技术以及*知识蒸馏*和*参数高效微调*方法相得益彰，这些方法我们还未深入探讨，它们为我们提供了对《简化巨头》系列即将展开的主题的初步了解。有一句流行的格言说：“天下没有免费的午餐”，但正如我们在剪枝研究中看到的那样：当谈到模型压缩时，有时候却真的有。与剪枝类似，量化作为一种正则化形式，已知能够让神经网络更加健壮和具有更强的泛化能力，这意味着这些技术的谨慎应用往往能在压缩模型的同时提高其性能。在本文中，我们将回顾相关文献并展示几个“免费午餐”压缩的例子。到最后，即使是怀疑的读者也应该会发现，网络量化并不一定意味着质量的下降。审视过研究后，我们将探讨如何使用开源软件在自己的工作中应用这些技术。现在，让我们深入探索神经网络量化这一激动人心的领域。

> 注意：对于那些想跳过课程，直接进入加速工作流程的实施指南的读者，请点击[这里](#3761)。

# 量化

![](../Images/dfb0a09962e55d79773bccc5d08d4e92.png)

作者使用 DALL-E 3 生成的图像

作为量化在大规模语言模型（LLM）部署中成功与必要性的证明，如今每个流行的开源 LLM 服务解决方案都提供了量化模型的便捷访问，这些模型通常是默认选项。例如，我最近使用 [Ollama](https://ollama.com/) 创建了[一个开源的语音到语音多语言语言学习助手](/lingonaut-language-assistant-6abe3e8b045c)，它就是建立在 [llama.cpp](https://github.com/ggerganov/llama.cpp) 之上的，这是一个纯 C/C++ 库，旨在使量化 LLM 在消费级硬件上的优化部署成为现实。对于像使用低功耗硬件的机器人这类实时视觉-语言应用，部署量化模型并使用这些硬件优化的服务后端是至关重要的。但究竟什么是量化，它为何能如此有效地压缩神经网络呢？

*量化*是指将一个连续的实数空间映射到一个固定的离散数集合，或者更广泛地说，是将任何数字空间转换为较低精度的表示。例如，32位的“单精度”或“全精度”浮动点值，甚至是高分辨率的64位“双精度”浮点数；这两种数据类型在它们可以携带的小数位数上有一定的精度限制。因此，它们是量化分布的示例，因为在它们的最后一个小数位中的每一个“步长”之间存在无限多个无法表示的值，从而产生了数字世界中独特的“阶梯”模式。事实上，在离散系统中有效地处理连续值的挑战与数字计算机科学本身一样古老。即使是浮动点数也在内部被分解为整数，因为数字计算机处理的是二进制信息，而二进制本质上是离散的。因此，涉及神经网络时，技术上问题并非是“是否量化”，而是“量化到什么程度？”

与其他量化应用（例如信号处理）不同，信号处理的目标是尽可能让量化后的数字与其原始值接近，而神经网络量化的最终目标是以某种方式离散化参数，使其精度尽可能低，同时保持它们的集体相互作用产生相同的输出。由于神经网络具有高度的过度参数化，因此在其损失梯度空间中存在整个最优解流形，包含许多可能的解，在量化过程中，单个权重可以相对远离其原始值，只要它们的集体交互仍保持在这个解流形上，这提供了一个优化模型参数的机会，同时考虑到随后的量化，这被称为*量化感知训练*（QAT）。

![](../Images/d07bc1b92c2a0d1ba70fac00d78fa0f2.png)

来自[Gholami et al. 2021](https://arxiv.org/abs/2103.13630)的图示展示了量化感知训练（QAT）的概念。

执行QAT通常使用*模拟*或*伪*量化，在这种方法中，参数以低精度存储，但操作仍然使用浮动点算术进行。在QAT的情况下，使用浮动点进行数学运算提供了梯度下降的条件（以及“直接通过估算器”或“STE”，它简单地忽略了四舍五入函数对梯度的破坏性影响），但一些方法也会在推理时使用模拟量化，特别是在它们专注于存储效率而非运行时加速时。

模拟量化与*仅整数*或*定点*量化形成对比，在这种量化中，所有操作都是通过低精度算术执行的。仅整数量化通常是在延迟和功耗方面的全面优势所在，但考虑因素会因硬件不同而有所不同，因为现代GPU具有高度优化的浮点单元，而边缘设备使用低功耗硬件，其中整数运算可能更为高效。模拟量化或仅整数量化的使用取决于使用场景；例如，模拟量化将是一个不错的选择，可以用来迭代测试不同网络组件对不同量化水平的敏感性，而无需担心硬件实现，而仅整数量化可能是边缘优化部署的最佳选择。

![](../Images/371c421ef32fee5488169d4db02e0376.png)

来自[Gholami等人 2021年](https://arxiv.org/abs/2103.13630)的图表展示了全精度、模拟量化和仅整数量化之间的区别。

虽然QAT通过将量化效应纳入训练过程来产生最佳结果，但它面临着我们在先前修剪研究中遇到的相同挑战：如果我们希望压缩像LLM这样非常大的模型，我们确实希望避免重新训练阶段，原因有很多，包括无法访问训练数据，或不愿意为所需的GPU时间支付数百万美元。因此，我们被迫放弃QAT显然优越的结果，而转向*后训练量化*（PTQ）方法，它只需要一小部分校准数据，并最终希望*零-shot量化*（ZSQ）方法取得成功，后者探索了完全不使用数据的理想场景。正如我们将看到的，最近的工作将PTQ推向了令人印象深刻的精度水平，即使在低位设置中，也能紧密匹配全精度基准，并且由于开源研究和代码的努力，它变得非常易于访问。

![](../Images/4b9e1c09c588ccae2b160da8a6c2d367.png)

来自[Olivia Weng 2021年量化调查](https://arxiv.org/abs/2112.06126)的流程图简洁地概述了QAT和PTQ。

神经网络量化的好处不仅仅体现在压缩方面。像剪枝一样，量化通过减少独特参数的数量，作为神经网络中的一种正则化形式，从而在谨慎应用时，也能提升性能和泛化能力。通过这种方式，量化与剪枝共同成为神经网络的另一种“免费午餐”压缩方法，因为模型大小和复杂性的显著减少也能反而提供改进的性能（如果正确调优的话）。鉴于离散化的好处，开始出现一种观点，即神经网络在浮点数表示上的形式仅仅是一个发展中的幼态阶段，仅用于在训练过程中为梯度下降创造数学条件，随后的量化不仅仅是模型压缩的后处理技术，而实际上是神经网络开发生命周期中的一个必要成熟阶段。此外，如果研究趋势持续下去，最终可能通过优化的仅使用整数的数学运算来实现等效的训练结果，这将彻底解放我们对高精度神经网络的需求。

量化涉及到大型语言模型（LLM）开发的各个方面，从训练到部署，涵盖了广泛的技术，旨在减少大型模型的内存、功耗和计算效率。例如，训练LLM时使用*混合精度*已经成为一种常见做法，其中对于不那么敏感的计算采用半精度（16位浮点数）而不是完整的32位浮点精度，从而显著减少其内存占用和所需的计算功率，而对结果几乎没有显著影响。这种修改不仅使我们能够更自由地迭代和开发模型，而且在大规模模型训练的环境中具有广泛的环境影响，LLM的训练甚至可能以吨计二氧化碳排放。

事实上，当使用少量资源就能达到等效的数学结果时，没有失败者，反而会带来巨大的收益。这一承诺激发了几十年来神经网络量化领域的丰厚研究成果，并且这一研究势头仍在不断加速，这意味着虽然本文力求全面和自成一体，但为了适应内存的限制，我们将不得不简化一些细节。雄心勃勃的读者可以参考近期的全面综述[Gholami et al. 2021](https://arxiv.org/abs/2103.13630)，或是更加注重历史视角的[Gray & Neuhoff 1998](https://ieeexplore.ieee.org/document/720541)综述。

## 大纲

为了提供最简明的途径，使读者能够深入理解神经网络量化的主题，本文的其余部分将按以下方式展开：首先，我们将熟悉量化背后的数学原理，为讨论奠定基础。接着，我们将探讨神经网络量化研究的起源，追溯到1990年代初，并将其与2012年AlexNet在图像分类任务上取得开创性成功后的“深度学习革命”联系起来。因此，我们将见证量化研究在现代时代首先在计算机视觉领域的快速发展，随后逐渐渗透到自然语言处理领域，届时我们将能够讨论量化在当今大型语言模型（LLM）世界中的应用，并探索将这些方法融入工作流的最佳库和工具。最后，我们将总结我们的发现，并讨论未来研究的方向。

本文分章组织，以便读者能清晰地阅读每个部分。如果读者急于查找信息，可以跳过某些部分，但请记住，文中的术语可能在前面的章节中已经定义。这些部分共同构成了一个相对自成一体的神经网络量化综述，旨在为从爱好者到专业人士的机器学习实践者提供深入的知识，帮助他们优化自己的工作流。文章最后附有LLM量化的实施指南，时间紧迫的读者可以直接跳到该部分。

1.  [**量化机制**](#479f)

    a. [位宽](#b9d1)

    b. [均匀量化](#5e01)

    c. [非均匀量化](#9cc4)

    d. [混合精度量化](#67ba)

    e. [标量与矢量量化](#e326)

    f. [补偿量化效应](#5470)

1.  [**神经网络量化的历史**](#8f81)

    a. [神经网络量化的早期工作](#999e)

    b. [后AlexNet时代的量化](#b7b5)

    • [卷积神经网络（CNN）的量化感知训练](#cc9e)

    • [混合精度量化的兴起](#1fc5)

    • [CNN的训练后量化](#ceaa)

    • [极端量化：二值与三值网络](#a358)

1.  [**LLM的量化**](#5e4a)

    a. [变压器（Transformer）早期的量化](#68d1)

    b. [LLM的训练后量化](#ee78)

    c. [LLM的量化感知训练](#3490)

    • [LLM的极端量化](#b59f)

1.  [**实践者的LLM量化指南**](#3761)a. [LLM量化决策树](#1a5c)

1.  [**结论**](#4f85)a. [未来工作](#6303)

# 量化机制

![](../Images/0c816858fe56724761eb556e0a5124a6.png)

图片由作者使用DALL-E 3生成。

为了深入理解量化过程，我们有必要反思“量化”数字的确切含义。到目前为止，我们讨论了通过量化，我们将一组高精度值映射到较低精度值，以最佳方式保留它们之间的关系，但我们还没有深入探讨这个操作的机制。不出所料，我们发现关于如何将值重新映射到量化空间的过程中有很多细微差别和设计选择，这些选择因使用场景而异。在本节中，我们将努力理解指导量化过程的参数和杠杆，以便我们能更好地理解研究，并为我们的部署做出更有见地的决策。

## 比特宽度

在我们讨论量化的过程中，我们会提到量化值的比特宽度，这代表了表示该值所需的比特数。一个比特只能存储0或1的二进制值，但一组比特可以将其组合解释为递增的整数。例如，2个比特可以表示4种组合（{0, 0}，{0, 1}，{1, 0}，{1, 1}），这些组合可以表示整数范围[0, 3]。随着比特数N的增加，我们得到2的N次方种可能的组合，因此一个8位整数可以表示256个数字。虽然*无符号*整数会从零开始计数直到最大值，*有符号*整数则通过将第一个比特解释为正负号，将零置于范围的中心。因此，一个无符号8位整数的范围是[0, 255]，而一个有符号8位整数的范围是[-128, 127]。

了解比特如何表示信息的基本知识，将帮助我们更好地理解浮点值在我们研究的技术中映射到的数值空间。例如，当我们听到某个网络层被量化为4位时，我们就能理解目标空间有2的4次方（16）个离散值。在量化过程中，这些值不一定代表量化权重的整数值，通常它们指的是量化*级别*的索引——即将输入分布的值映射到的“桶”。每个索引对应一个*代码字*，表示在预定义数值空间内的一个特定量化值。这些代码字共同构成了*代码本*，从代码本中获得的值可以是浮点值或整数值，这取决于要执行的算术类型。定义这些桶的*阈值*依赖于所选择的量化函数，正如我们将要看到的那样。请注意，*代码字*和*代码本*是通用术语，在大多数情况下，代码字与从代码本返回的值是相同的。

## 浮点、定点和仅整数量化

现在我们理解了位宽，接下来我们应当花些时间讨论浮点、定点和仅整数量化之间的区别，以便清楚它们的含义。虽然用二进制位表示整数是直接的，但对具有小数部分的数字进行运算就要复杂一些。浮点数和定点数数据类型都是为此设计的，选择它们之间取决于部署硬件和所需的精度与效率权衡，因为并非所有硬件都支持浮点运算，而定点运算在降低数字范围和精度的代价下，能够提供更高的功率效率。

浮点数将其位分配用来表示三部分信息：*符号位*、*指数部分*和*尾数*，这使得对它们的表示值进行高效的按位操作成为可能。指数部分的位数定义了数字范围的大小，而尾数位的数量定义了精度水平。举个例子，IEEE 754标准中32位浮点数（FP32）将第一个位分配给符号，8个位分配给指数，剩余的23个位分配给尾数。浮点数之所以叫“浮动”是因为它们为每个数字存储一个指数，允许基数点的位置“浮动”，类似于科学记数法在10进制中移动小数点，但不同之处在于计算机使用的是2进制（即二进制）。这种灵活性使得可以精确表示广泛范围的数值，尤其是接近零的数值，这也凸显了在各种应用中归一化的重要性。

相比之下，“定点”精度不使用动态缩放因子，而是将位分配为*符号位*、*整数部分*和*小数部分*（通常仍称为*尾数*）。虽然这意味着更高的效率和节能操作，但动态范围和精度会受到影响。为了理解这一点，假设你想表示一个尽可能接近零的数字。为此，你需要尽可能地把小数点向右移。浮点数可以自由地使用越来越小的指数将小数点推得更左，从而在这种情况下提供额外的分辨率，但定点数值只能依赖于固定数量的小数位精度。

整数可以看作是固定点的一种极端情况，其中没有比特用于小数部分。实际上，固定点的比特可以像整数一样直接操作，计算结果可以通过软件重新缩放以获得正确的固定点结果。由于整数运算在硬件上更节能，因此神经网络量化研究偏好使用*仅整数*量化，将原始浮动值转换为整数，而不是固定点浮动，因为它们的计算最终是等效的，但整数运算可以更加高效并节省能量。这对部署在电池供电的设备上尤其重要，而这些设备通常包含只支持整数运算的硬件。

## 均匀量化

要量化一组数字，我们必须首先定义一个量化函数***Q(r)***，其中***r***是要量化的实数（权重或激活值）。最常见的量化函数如下所示：

![](../Images/92e277d67930ecbf2efd7cb145428294.png)

典型的量化函数。图片来源：作者。

在这个公式中，***Z***表示一个整数*零点*，而***S***是*缩放因子*。在*对称量化*中，***Z***简单地设为零，并从方程中取消掉，而在*非对称量化*中，***Z***用于偏移零点，从而可以将量化范围更多地集中在输入分布的正侧或负侧。这种非对称性在某些情况下非常有用，例如在量化ReLU激活信号时，这些信号只包含正数。**Int(·)**函数将一个缩放后的连续值分配给一个整数，通常通过四舍五入，但在某些情况下会遵循更复杂的过程，正如我们稍后将遇到的那样。

选择正确的缩放因子（***S***）并非易事，需要仔细考虑要量化值的分布。由于量化后的输出空间具有有限的值范围（或*量化* *级别*），用于映射输入值，因此必须建立一个合适的*裁剪范围* [α, β]，以便很好地适应输入值分布。所选的裁剪范围必须在不对极端输入值过度夹紧和不通过为长尾分配过多比特而过度饱和量化级别之间取得平衡。现在，我们考虑*均匀量化*，即量化阈值或*量化步长*是均匀分布的。缩放因子的计算公式如下：

![](../Images/d7c7fb18be4341f52896fb996f9bb06c.png)

基于裁剪范围（*[α, β]*）和所需比特宽度（b）来计算量化函数的缩放因子（*S*）的公式。图片来源：作者。

训练的参数分布的形状在不同的网络之间可能会有很大差异，并且受到多个因素的影响。由这些权重生成的激活信号更加动态和不可预测，这使得关于正确剪裁范围的任何假设都变得困难。这就是为什么我们必须根据我们的模型和数据来*校准*剪裁范围。为了获得最佳准确性，实践者可能会选择在推理过程中在线校准激活的剪裁范围，这就是*动态*量化。如人所料，这会带来额外的计算开销，因此比*静态*量化的使用要少，后者是在推理之前就预先校准好剪裁范围并固定在推理过程中。

**反量化** 在这里我们建立了反向均匀量化操作，它将量化后的值解码回原始数值空间，尽管这种解码并不完美，因为四舍五入操作是不可逆的。我们可以使用以下公式解码我们的近似值：

![](../Images/b03d952b3651983b1e4e9cd26cfafe3e.png)

反量化操作。图像由作者提供。

## 非均匀量化

敏锐的读者可能已经注意到，对任何形状非均匀的输入分布施加均匀间隔的分桶阈值会导致某些位比其他位更饱和，而且调整这些宽度，以便更多地集中在分布密集区域的位上，将更忠实地捕捉输入信号的细微差别。这个概念已经在*非均匀量化*的研究中得到了探讨，并且在信号保真度上确实带来了好处；然而，由*均匀量化*所支持的硬件优化计算使其成为事实上的神经网络量化方法。下面的方程描述了非均匀量化过程：

![](../Images/54b846f331e47b79af4c6de8e3d88591.png)

非均匀量化公式，其中**Xi**是**量化级别**，**∆i**是**量化步长**。图像由作者提供。

许多非均匀量化的研究提到了学习*质心*，它们代表了输入分布中簇的中心，通过量化过程将周围的值映射到这些中心。换句话说，在均匀量化中，阈值均匀地分布在输入分布上，质心就是简单地位于分桶阈值之间的值。

## 混合精度量化

正如我们在剪枝中看到的，经过训练的神经网络在某些层和子模块中的性能对变化更为敏感，而通过测量这些敏感性，整个神经网络的部分可以被移除，而不会显著影响误差。直观地说，量化的不同级别也遵循同样的规律，某些网络组件可以重新映射到比其他组件低得多的位宽。我们已经提到的最基本的例子就是：在不太敏感的网络操作中使用16位浮点数，从而在训练期间大大减少内存占用，但*混合精度量化*可以指的是整个网络中不同量化级别的任何组合。

与混合精度量化概念相关的是量化的*粒度*，它可以是按层、按组、按通道或按子通道划分，并描述了在这些不同量化参数集被校准时所使用的尺度。直观地说，粒度越细，计算开销就越大，这代表了精度与效率之间的权衡。例如，在卷积神经网络（CNN）中，按通道划分的粒度通常是首选，因为按子通道（即按滤波器）量化会过于复杂。

## 标量量化与向量量化

尽管量化领域的大多数研究历来集中在量化矩阵中的单一值，但同样也可以学习多维的质心。这意味着矩阵可以被分割成向量，然后这些向量中的每一个都可以赋予一个指向其最近质心的码字，从而有可能通过单次查找代码书恢复矩阵的整个部分，实际上将一组数字存储为一个单一的值，从而大大增加了压缩比。这种方法被称为*向量量化*，它的优势已引起越来越多的关注。“向量量化”通常指的是将矩阵分割成列向量，但这些向量也可以进一步被分割成子向量，这种做法称为*产品量化*，它在极端情况下可以推广向量量化和标量量化。其核心思想是，使用从代码书中返回的质心向量集合，利用较小的存储码字结构，能够忠实地重建原始的、较大的矩阵。我们将看到，这确实证明是一种非常强大的模型压缩技术。

## 补偿量化效果

我们不能仅仅将神经网络中的所有权重四舍五入到不同的精度并期望一切正常，因此我们必须制定一个计划，来弥补量化过程中所带来的扰动。正如我们上面所学到的那样，通过模拟量化来训练或微调模型，可以大幅提高可执行的量化程度，而不会影响性能，这项技术称为*量化感知训练*（QAT），它还允许在训练过程中学习量化参数。然而，执行 QAT 需要具备必要的硬件和数据来训练模型，而这通常是不可行的，尤其是对于像今天的 LLM（大型语言模型）这样非常庞大的模型。为了解决这个问题，后训练量化（PTQ）技术旨在避免训练，仅需少量无标签数据来校准量化函数，而零样本量化（ZSQ）则探索了理想的“无数据”情境，该情境不需要任何数据进行校准。

在我们通过文献深入探讨这些技术时，我们将更详细地看到每种技术，因此让我们现在乘坐时光旅行车，回到上世纪末，当时研究人员也同样被神经网络的强大能力所吸引，这些网络超出了他们的硬件限制，并开始考虑我们如何希望将这些复杂的模型部署到移动硬件上。

# 神经网络量化的历史

![](../Images/35080b72d42552e80ffc86da605fa28c.png)

图像由作者使用 DALL-E 3 创建。

## 神经网络量化的早期研究

在模型压缩技术家族中，网络量化是与剪枝仅稍年轻的一个“兄弟”，其根源也可以追溯到 1980 年代后期的[反向传播训练神经网络](https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf)的早期阶段。那一时代的计算硬件进步引发了对神经网络研究的复兴，但由于硬件的限制，问题的复杂性仍然需要大大缩小，这实际上排除了神经网络最适用的使用场景。虽然研究人员在几十年来隐性地处理了神经网络中的数值精度问题，但这种日益增长的计算限制感促使人们将注意力转向优化数值精度，产生了1990年左右的广泛研究。

1989年，Baker 和 Hammerstrom首次系统地研究了减少数值精度对网络性能的影响，旨在启用硬件优化。他们的工作《[人工神经网络算法的表征](https://ieeexplore.ieee.org/document/100296)》是一个早期成功的例子，展示了如何使用减少精度计算训练网络的反向传播方法，挑战了32位浮动点运算是保持网络性能所必需的传统观念。[Hollis 等人 1990年](https://direct.mit.edu/neco/article-abstract/2/3/363/5539/The-Effects-of-Precision-Constraints-in-a?redirectedFrom=PDF)进一步研究了精度约束对反向传播网络训练的影响，并同样发现学习能力在大约12位精度时出现了急剧下降。1990年，一年后，Dan Hammerstrom基于之前的工作，[研究了为神经网络中的固定精度计算显式优化的新硬件单元的设计](https://ieeexplore.ieee.org/document/5726581)，证明了在使用16位甚至8位精度时，效率可以获得显著提升，同时性能下降在可容忍范围内，为未来的量化和神经网络硬件优化工作奠定了基础。

一个有趣的事实是，在反向传播算法成为神经网络学习算法的不争冠军之前，已经有研究探讨了利用较低比特宽度和专用硬件来优化神经网络。在1992年的一项开创性工作中，[Hoehfeld 和 Fahlman](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ed7eaefb93994bfee42309bbd7cda541c08696ba)研究了有限数值精度对使用级联相关算法（[Fahlman & Lebiere, 1991](https://www.researchgate.net/publication/2427110_The_Cascade-Correlation_Learning_Architecture)）训练网络的影响，展示了该学习算法在固定精度下也能很好地运行。作为其成功的组成部分，作者概述了动态缩放和概率舍入的技术，这些技术能够在更低精度下（例如他们的研究中为6位）实现收敛，并且可以应用于任何基于梯度的学习算法。

在网络量化探索的这一基础时期，揭示了发现精细技术和硬件特定优化的路径，这些技术和优化充斥着今天庞大的人工智能领域。通过展示低精度计算的可行性和好处，这些早期的工作扩展了神经网络应用的可能性，为开发更高效、更可扩展的AI系统提供了一种强大的方法。如今，随着我们站在人工智能在各平台上普及的前沿，这些开创性努力的遗产比以往任何时候都更具相关性。它们不仅展示了通过量化进行模型压缩和高效计算的潜力，而且激发了对神经网络设计和优化的持续创新探索。

## 后AlexNet时代的量化

2012年，[AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)的作者利用了数据可用性和计算硬件领域重大进展的偶然交汇，超越了以往在ImageNet大规模视觉识别挑战赛（ILSVRC）中的最先进方法。促成这一历史性成功的两个关键因素是：1）[费费·李及其在普林斯顿大学的团队](https://image-net.org/static_files/papers/imagenet_cvpr09.pdf)提供了全球首个大规模精心策划的图像数据集；2）一个偶然的巧合，即由健康的游戏产业收入推动的图形处理单元（GPU）技术的资金支持，恰好推动了能够加速深度学习中矩阵运算的并行计算硬件的诞生。

在这些有利的环境下，Alex Krizhevsky及其团队训练了一个具有6230万个参数的大型卷积神经网络（CNN），并凭借超过10%的准确率领先于第二名，创造了神经网络研究的一个重要时刻，开启了一个持续的深度兴趣和资金投入的时期，这个时期通常被称为“深度学习革命”。然而，焕发活力的神经网络很快就遇到了它们的老对手：硬件限制。尽管GPU加速训练带来了深远的好处，AlexNet的作者也承认，硬件约束对他们方法的成功构成了限制因素，而且随着硬件的改进，结果可能会有所提升。研究界迅速意识到，模型压缩未被解决的潜力可以应对这些限制，并迅速付诸行动。在这个以CNN为重点的时期，关于不同类型网络组件之间的敏感性差异的发现，为未来对变换器（transformer）的研究提供了强有力的基础。

实际上，关于将神经网络部署到边缘设备的愿望早在AlexNet震撼世界之前就已经有了。[Vanhoucke 等人 2011 年的开创性工作](https://research.google/pubs/improving-the-speed-of-neural-networks-on-cpus/)探讨了如何加速在 x86 CPU 上运行的神经网络。在AI社区面临十字路口——在投资GPU还是从传统CPU中挖掘更多性能的讨论中——他们的论文为如何在Intel和AMD CPU上优化神经网络操作提供了关键指导。Vanhoucke 等人在AlexNet引领GPU主导时代之前，展示了CPU通过精心优化所蕴藏的巨大潜力，包括采用定点和整数算术，辅以SIMD指令和内存对齐技术。通过这些优化，作者们实现了显著的性能提升，为未来在CPU硬件上进行神经网络高效训练和部署的研究奠定了基础。

在AlexNet成功之后，CNN成为了量化研究迅速发展的沃土。研究人员与量化不同类型网络层的细节作斗争，这些网络层具有不同的敏感性和提供的优势。例如，大多数CNN的FLOP发生在卷积层，因此对这些层进行量化能带来最大的速度提升；然而，这些层也包含了对特征提取至关重要的参数，使得它们对改变特别敏感。另一方面，完全连接层通常更容易压缩，但这样做主要在存储大小方面具有优势，而在延迟方面则较少影响，因为这些层对整体计算图的贡献较小。

除了提供仅存储与完全效率提升的技术之间的区别外，后者组中的技术也有区分，区分在于一些技术旨在加速训练和推理，而另一些则仅仅致力于加速推理。QAT（量化感知训练）概念就是在这一时期诞生的，尽管许多技术选择在训练过程中使用模拟量化，另一些则更接近网络量化的根源，探索在训练和推理过程中使用定点或仅整数算术，以实现边缘设备上端到端的神经网络开发。

作为两种早期的通过量化进行CNN压缩的不同方法，Denton等人2014年提出的"[利用卷积网络中的线性结构提高计算效率](https://arxiv.org/abs/1404.0736)"方法通过在卷积层应用矩阵分解，作为PTQ或QAT过程，从而提高了计算效率，而Gong等人2014年提出的"[使用向量量化压缩深度卷积网络](https://arxiv.org/abs/1412.6115)"则专注于通过在PTQ环境中使用多种向量量化方法压缩全连接层，以优化存储大小，并指出了乘积量化的明显优越性。

在本节中，我们将看到量化领域如何在AlexNet引发的CNN主导时代中逐渐成型。在这个成长的关键时期，我们将看到QAT、混合精度量化、PTQ以及将量化极限推至1位或2位的极限量化成为明确定义的研究领域，为我们探索这些技术在今天大模型时代的成熟奠定基础。

## 卷积神经网络的量化感知训练

![](../Images/e7fe23e264609bad8ea6ed3634fdefa3.png)

图片由作者使用DALL-E 3生成。

正是在后AlexNet时代，QAT真正发展成为量化研究的一个独立领域。在以前的时代，几乎所有量化工作的重点都放在了使用训练过程来优化权重离散化，因为当时的网络规模相对较小。即使在GPU加速训练推动AI快速发展的时期，模型仍然能够在合理的资源下进行训练，避免重新训练量化网络的需求，更多是出于移动部署和数据访问/隐私问题的考虑，而非训练资源的限制。然而，PTQ的价值在这个时代逐渐显现，两者领域的区别也逐步明确。考虑到合理的训练成本，CNN时代的大多数量化研究仍然坚持基于QAT方法的根本。在本节中，我们回顾了CNN时代QAT方法的发展。

我们从2014年底开始，正是[Courbariaux等人](https://arxiv.org/abs/1412.7024)观察到“乘法器是数字实现深度神经网络中最占用空间和能源的算术运算符”，并专门研究了降低这些运算精度以提高效率的问题，因为它们的成本与位宽成平方关系，而乘法累加器（MAC）操作中其他运算符（加法器和累加器）的成本仅与位宽成线性关系，因此相对较便宜。值得注意的是，他们的研究表明“使用半精度浮点格式对神经网络训练几乎没有影响。”此外，作者还发现“*极低精度足够*不仅可以运行训练好的网络，而且*也足以训练它们*”，尽管在当时，“极低精度”指的是10位乘法，这是该领域变化迅速的一个标志，正如我们将看到的那样。

2015年，IBM的Gupta等人发表了《[使用有限数值精度的深度学习](https://arxiv.org/abs/1502.02551)》一文，提出了一种开创性的深度神经网络训练方法，该方法使用*随机舍入*技术，通过16位定点算术实现，其中舍入一个数值到其最近量化点的概率与这些量化点的接近程度成正比。与传统的四舍五入方法相比，这种舍入方法通过引入噪声，使得量化误差的期望值（偏差）趋近于零。与通常依赖于全精度浮点运算的量化感知训练（QAT）方法不同，Gupta等人的策略是在训练过程中执行所有计算时使用较低的精度。使用定点算术使得可以使用更快速、更节能且更节省空间的计算单元，作者还通过展示一种新型的节能硬件加速器，探讨了硬件协同设计。重要的是，使用随机舍入确保了即使是较小的梯度值也能对训练过程产生贡献，从而减少了对梯度近似方法（如直通估计器（STE））的依赖。

![](../Images/54ebdc5549bb68480edcc27c76864b40.png)

来自[Gupta等人2015年](https://arxiv.org/abs/1502.02551)的结果显示了随机舍入的强大作用，16位定点训练中，14位分数长度（FL）几乎与浮点训练曲线匹配，而使用“四舍五入到最近”定点算术则导致训练发散。

Han 等人 2015 年的 “[Deep Compression](https://arxiv.org/abs/1510.00149)” 是一种基于字典（即代码本）的方法，通过混合量化和剪枝技术，在不牺牲性能的情况下，实现在神经网络中达到极高压缩比的基础方法。作者的动机是将计算机视觉领域的突破应用到移动设备中，他们对当时的主流卷积神经网络（CNN）进行了实验，分别在 AlexNet 和 VGG-16 上实现了 35 倍和 49 倍的压缩率，且精度没有下降。这一成果是通过一个三阶段管道实现的，该管道依次进行剪枝、量化，最后对网络权重应用 [Huffman 编码](https://webspace.science.uu.nl/~leeuw112/huffman.pdf)（一种无损数据压缩方法）。首先，使用 Han 等人 2015 年先前工作 “[Learning both Weights and Connections for Efficient Neural Networks](https://arxiv.org/abs/1506.02626)” 中的开创性无结构方法，识别并剪除模型中不重要的权重，从而实现 9 倍到 13 倍的压缩，且没有引入误差。其次，将剩余的权重从 32 位量化为 5 位，之后进行一轮再训练以恢复性能，最后对量化后的权重进行 Huffman 编码，实现额外的 20%-30% 存储空间减少。

![](../Images/b9b78627877b8cd85b5564a5dcd18152.png)

来自 “[Deep Compression](https://arxiv.org/abs/1510.00149)” 的流程图展示了他们的三阶段压缩技术。

“[Deep Compression](https://arxiv.org/abs/1510.00149)” 的结果令人震惊，该方法将 CNN 压缩至原始大小的不到 1/35，同时在精度上与基准参考相比相等或更优。然而，值得注意的是，AlexNet 和 VGG 架构是故意进行了过度参数化，以最大化性能，它们都包含了参数密集但相对不敏感的全连接层，这些层可以被大量压缩。虽然该方法主要关注增强模型的存储占用，但作者指出，较小的存储也意味着加速操作，因为存储和提取的权重更少，因此减少了内存带宽需求，特别是当模型大小足够小以至于能够存储在片上静态随机存取存储器（SRAM）中，而不是在动态随机存取存储器（DRAM）和 SRAM 之间来回交换时。此外，作者还介绍了高效推理引擎（EIE）的概念，这是一种旨在利用剪枝所带来的稀疏性的硬件加速器，相关内容将在他们的 [未来出版物](https://arxiv.org/abs/1602.01528) 中讨论。

![](../Images/de7c16ed30076f776fbdf1db36a00f82.png)

来自 “[Deep Compression](https://arxiv.org/abs/1510.00149)” 的结果显示，在不损失精度的情况下，取得了令人印象深刻的压缩率。

2017年初，Zhou 等人提出的增量网络量化（[INQ](https://arxiv.org/abs/1702.03044)）方法超越了深度压缩（Deep Compression）所见的压缩水平。作者同样采用了剪枝和量化的组合，但不使用哈夫曼编码，成功地在 AlexNet 上实现了 53 倍的压缩，同时没有损失 top-1 准确率，在仅有轻微损失（<1.5%）的情况下，成功达到了 89 倍的压缩。其策略是逐步量化网络权重的一部分，重新训练其余的全精度权重以补偿引入的误差，并反复迭代直到所有权重都被量化。权重被约束为零或 2 的幂（可以是负幂）。正如作者所解释的那样，这种策略的优势在于“原始的浮点乘法操作可以被更便宜的二进制位移操作替代，运行在如 FPGA 这样的专用硬件上。”为了实现这一点，他们采用了可变长度编码方法，使用一个比特表示零值，剩余的比特共同表示一个代码字，用于索引给定比特宽度和缩放因子的可能量化值，后者被设定为每层权重的最大绝对值。他们的方法在 5 位精度下略微超越了基准的 FP32 AlexNet 性能，并且在 3 位精度下显示了相当可比的准确率，具体见下图。INQ 的代码[可在 GitHub 上获取](https://github.com/AojunZhou/Incremental-Network-Quantization)。

![](../Images/ac1277c6ef2dfc28015553f8875039f3.png)

2017年底，[Jacob 等人](https://arxiv.org/abs/1712.05877)来自 Google 关注于在移动设备 CPU 上实现高效的整数-only 推理。作者指出，使用像 AlexNet 和 VGG 这样故意过度参数化的模型来基准化模型压缩技术，容易成为攻击目标，因此他们选择使用 MobileNets 来测试他们的方法。由于这些紧凑型模型已经设计为最大化参数效率，存在较少的“死重”可被轻易压缩，而且它们的参数对扰动更为敏感。被认为是量化感知训练（QAT）领域的开创性工作，Jacob 等人的方法将权重和激活量化为 8 位整数，将偏差（需要更高精度）量化为 32 位整数。他们的方法在训练过程中使用浮点运算，作为使用模拟量化进行 QAT 的早期示例。作者避免使用需要查找表的量化方案，因为这些方案通常比在 SIMD 硬件上进行纯算术运算的性能差，而选择对权重进行仿射变换到整数空间。为了补充他们的 QAT 方法，作者共同设计了一个框架，用于转换并在仅支持整数的硬件上运行经过训练的模型，并且比许多先前的工作更进一步，证明了他们在实际边缘硬件上的效率提升。

![](../Images/d8b8e559b7fffb812c612a161ccb886b.png)

Jacob 等人 2017 年的结果比较了参考浮动点与他们的 8 位量化 MobileNets 在两种类型的移动 CPU 上的延迟与准确度权衡。请注意，对于更优化浮动点运算的 Snapdragon 821，8 位量化的优势并不那么显著。

到目前为止，我们看到的技术将模型的各层量化到统一的精度水平，这是硬件加速的最佳条件，尤其是在低功耗硬件的边缘设备上。然而，正如我们在之前对剪枝的探讨中了解到的那样，某些网络层比其他层对改变的敏感度较低，因此可以在不影响性能的情况下更加激进地压缩。因此，*混合精度量化*方法根据网络组件（通常按层级粒度）对变化的敏感度使用不同级别的压缩，从而实现更小的内存占用，减少在边缘设备上运行模型时的数据传输和功耗成本。在下一节中，我们将看到敏感度分析的熟悉主题如何影响在神经网络组件间分配数值精度，从而通过量化最大化模型压缩。

## 混合精度量化的崛起

![](../Images/20bffca32ba68b021ed373907c712da9.png)

图片由作者使用 DALL-E 3 生成。

为了考虑网络层之间对量化引起的扰动的敏感度差异，*混合精度量化*方法应运而生，这种方法根据网络组件的敏感度量身定制精度水平，已成为通过量化在深度网络中最大化压缩程度的流行方法。挑战来自混合精度设置的广阔搜索空间，该空间随着网络层数的增加呈指数级增长。此外，对于旨在寻找层级量化和微调的最佳顺序的方法，复杂度变得是组合性的。因此，研究人员提出了各种有效的算法来自动化搜索和选择深度网络的最优混合精度设置，避免了暴力搜索的不可行性。正如我们将看到的，研究人员已经采用了神经架构搜索（NAS）、强化学习（RL）、二阶海森矩阵信息以及其他类型的求解器来解决这一问题，取得了令人印象深刻的成果，并建立了通过量化获得效率提升的新水平。

2018年底，吴等人发布了“通过可微神经架构搜索进行卷积网络的混合精度量化”([DNAS](https://arxiv.org/abs/1812.00090))方法，用于解决混合精度问题。他们的方法创建了一组网络架构参数*θ*，该参数定义了一个完全可微的超级网络中的子网，该超级网络包含每一层的所有可能精度设置。*θ*参数控制在该子网图中采样每条边的概率，并可以通过梯度下降进行直接优化。使用这一技术，作者创建了具有极高压缩率的模型（在某些层中权重压缩至1位），并能够超越全精度基准。DNAS方法生成的ResNet模型在精度上与全精度模型相当，但其内存占用小至21.1倍，计算成本低至103.9倍。作者提到，他们的方法是一个通用的架构搜索框架，可以扩展到其他网络参数化，但这部分内容将留待未来研究。

![](../Images/56043278c12d98051565d81eb6b3c995.png)

[DNAS](https://arxiv.org/abs/1812.00090)的结果显示了按准确率排序的前三个搜索架构的计算成本压缩率。请注意，他们的方法在arch-1列中实现了33–40倍的“免费午餐”计算压缩。

王等人于2018年发表的“[HAQ: 硬件感知自动量化与混合精度](https://arxiv.org/abs/1811.08886)”放弃了使用代理度量（如FLOPs）来提高计算效率，而是选择使用硬件模拟器提供的信号，输入强化学习（RL）算法，自动搜索最佳混合精度设置。尽管他们的方法推动了智能模型压缩的极限，但在训练RL策略来预测特定网络架构和硬件设备的正确混合精度设置时，仍然面临与复杂性、计算开销和泛化能力不足相关的弱点。

![](../Images/2fa11df3a2f1b8f9d918d8769dcb1cd8.png)

HAQ结果表格将他们的方法与深度压缩进行比较。请注意，在约4位设置下，两种方法几乎匹配全精度基准模型。

在2019年，Hessian AWare Quantization（[HAWQ](https://arxiv.org/abs/1905.03696)）论文解决了混合精度量化中的两个重要挑战：一是确定每个网络层的最佳精度级别所需的指数级搜索空间，二是确定在这些层之间进行QAT（量化感知训练）的最佳顺序的阶乘复杂度，这使得在深度网络中暴力搜索这些值变得不可行。作者展示了来自二阶Hessian信息的主特征值可以提供敏感度分析，从而为正确的量化级别和跨网络层的优化微调顺序提供指导。这类似于基于LeCun等人1989年提出的[Optimal Brain Damage](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)的剪枝方法，在这种方法中，Hessian信息被用作衡量网络组件显著性（即敏感度）的标准，较大的值表示对给定网络组件的剪枝或量化对性能的影响更大。HAWQ能够在使用2位权重和4位激活的情况下超越DNAS的性能，从而实现了更高的整体压缩效果。

在2019年HAWQ发布七个月后，Dong等人发布了[HAWQ-V2](https://arxiv.org/abs/1911.03852)，以解决他们在前期工作中发现的三个主要限制：1）仅使用主特征值进行敏感度分析，忽略了Hessian谱中的其余部分，2）仅确定相对的层级敏感度度量，导致需要手动分配精度，3）未考虑激活的混合精度量化。针对第一个问题，他们发现，对所有Hessian特征值取平均是更好的层级敏感度度量方法。针对第二个问题，作者提出使用基于Pareto前沿的方法来自动选择精确的层级位精度。为了解决第三个问题，作者“将Hessian分析扩展到混合精度激活量化”。通过这些调整，HAWQ-V2在CNN量化中设立了新的最先进的基准。

![](../Images/1bef5d60effcafaa9567fb8f4f6ff62d.png)

使用ResNet50在ImageNet上进行HAWQ-V2结果的表格。

一年后的2020年底，HAWQ的作者发布了第三个版本，[HAWQ-V3](https://arxiv.org/abs/2011.10680)。这项工作通过确保每个网络操作中的整数运算，包括批量归一化层和残差连接，改进了之前的方法，后者为了保持准确性，之前一直使用float32，从而导致无法在常见的仅支持整数的边缘设备硬件上部署。为了确定每一层的比特精度，HAWQ-V3采用了“新颖的硬件感知混合精度量化方案，该方案使用整数线性规划（ILP）问题”，它“平衡了模型扰动与约束（例如内存占用和延迟）之间的权衡”。他们的方法生成了一个在8位整数推理中表现优异的模型，超越了最强的全精度基准，展示了“免费午餐”压缩，并保持在低至4位精度时的高准确性。

![](../Images/1b24f0795b1aaa76351b911e51bef5b5.png)

来自[HAWQ-V3](https://arxiv.org/abs/2011.10680)论文的结果，使用ResNet50在ImageNet上的测试。注意，所有8位量化的HAWQ-V3超越了全精度基准。“Int”表示仅整数，“Uni”表示均匀，“BL”表示基准（这些在不同的文献中有所不同，他们选择最强的基准进行研究）。“Dist”指的是知识蒸馏的使用。

本节中介绍的技术展示了基于敏感性分析调整逐层精度分配的强大功能，对不敏感的层进行更激进的量化，并允许在敏感层中保留更高的精度以保持表示能力。然而，值得注意的是，混合精度量化方案在边缘设备上实现可能更具挑战性，因为它们可能无法高效运行。因此，看到HAWQ-V3在上述结果中提供了强有力的固定精度基准是令人放心的。在下一节中，我们将讨论一些技术，旨在对经过训练的全精度CNN进行量化，而无需重新训练它们，这为我们即将讨论的LLM量化提供了一个高度相关的前置条件，因为在LLM的情况下，重新训练和访问训练数据通常是不可能的。

## CNN的后训练量化

![](../Images/c4264da9bd33c2feb203b1f0845ea0b2.png)

图片由作者使用DALL-E 3生成。

聪明的读者会注意到，本节中关于CNN时代量化的多数研究属于QAT类别，因为在这一时期研究的模型足够简单，可以在量化设置中进行微调。然而，早在神经网络规模即将爆炸之前，研究人员就已经对PTQ的实际优势表现出浓厚兴趣，因为PTQ承诺解放那些开发量化模型的人，不再需要访问原始训练数据（在许多情况下这可能是不可能的），并且节省了重新训练所需的时间和资源。因此，关于PTQ研究的兴趣浪潮大约在2019年时兴起，为后续聚焦大型语言模型的研究奠定了基础。

[Krishnamoorthi](https://arxiv.org/abs/1806.08342)在2018年中期发布了一篇开创性的白皮书，领导了CNN量化的研究。他们的方法采用了权重的通道级非对称均匀量化，并对激活进行逐层量化，固定精度为8位，同时保持准确度在基准值的2%以内。作者观察到，仅对网络的权重进行8位量化可以作为一种压缩存储空间的简便方法，但为了实现高效推理，激活也必须进行量化，这需要使用校准数据来计算网络各层激活的动态范围，从而发现合适的逐层量化参数。在下面的图表中，作者提供了逐层和逐通道权重量化方案对各种CNN影响的比较。请注意，右侧较大的、过度参数化的CNN对逐层量化参数的低粒度更为敏感，而效率导向的MobileNet（左侧）则不那么敏感。

![](../Images/161dfb9aa79084e5317d823602e609a1.png)

来自[Krishnamoorthi 2018](https://arxiv.org/abs/1806.08342)的结果显示了不同W8A8 PTQ方案在不同CNN架构中的效果。“Mv1”和“Mv2”分别表示MobileNet v1和v2，在使用每层权重量化时，准确度出现了灾难性的下降。

在2018年10月，Banner等人提出的“[卷积网络的后训练4位量化以实现快速部署](https://arxiv.org/abs/1810.05723)”旨在将PTQ的可用性扩展到低于8位的精度。他们的方法高效地实现了4位无数据的混合精度PTQ，并且具有可接受的性能下降。为了实现这一点，作者利用了神经网络分布通常在均值周围呈钟形分布的知识，从而调节他们的量化方案，最小化张量级别的均方量化误差，避免了重新训练的需求。为了更好地将知识转移到量化空间，作者采取了以下措施：1）使用他们提出的整数量化的解析剪切技术（ACIQ）根据最佳饱和点来限制激活张量的异常值，从而减少在谱的密集区域中舍入误差；2）通过解析方法确定每个通道的最佳位宽分配，发现给定通道的最佳量化步长“与其范围的2/3次方成正比”；3）提出了一种简单的*偏置修正*方法，通过将量化参数中考虑到的通道均值和方差的预期变化来补偿量化后引入的权重偏差。

ACIQ方法需要对小型校准集上的网络激活进行统计分析，因此，尽管不需要访问训练数据，但必须确保校准集能够代表在运行时会遇到的分布，否则存在将量化参数过拟合到错误分布的风险。此外，需要注意的是，使用按通道的位宽会给实际应用带来许多问题，因为硬件和软件都必须支持在通道级别进行混合精度运算，否则运行量化后的网络可能效率低下甚至无法执行。尽管如此，提出一种封闭形式的解析解来直接计算网络组件的最佳位宽，标志着量化研究中的一个重要里程碑。此外，他们针对偏置修正参数的封闭形式PTQ解法，以及将这些参数高效吸收到现有计算中的方法，也是另一个重要贡献。Banner等人方法的代码可以在[GitHub上找到](https://github.com/submission2019/cnn-quantization)。

![](../Images/5b549fcb99ff767c89f1af3a0ba29aab.png)

来自[Banner等人 2019](https://arxiv.org/abs/1810.05723)的图表展示了他们4位PTQ方法的相对性能。

Nagel等人于2019年提出的“通过权重均衡和偏置修正进行无数据量化”（[DFQ](https://arxiv.org/abs/1906.04721)）介绍了一种突破性的无数据PTQ方法，使深度网络能够在无需校准数据、微调或超参数调优的情况下，高效地量化到8位。作者通过缩放适应权重，使其“更适合量化”，提出了一种修正量化引入的偏置的方法，并指出他们的方法可以作为QAT的补充预处理步骤使用。与上述Krishnamoorthi和Banner等人的方法不同，后者需要为每个通道存储量化参数，DFQ只需为每一层的权重张量存储一个单独的缩放值和偏移量值，通过确定最大化该层每通道精度的值来实现。DFQ利用ReLU激活函数的尺度等变性，并通过将缩放和引入的偏置吸收到下一层，保持整体数学等效性。作者展示了，通过使用模型训练时保存的批量归一化统计数据来估计层级预期量化误差，可以避免量化激活时需要校准数据，并通过从层偏置参数中减去这一预期误差来补偿量化引入的偏置。虽然作者没有明确使用这个术语，但DFQ可以视为*零-shot量化*（ZSQ）中的一项奠基性工作，因为它是一种无需校准数据的PTQ方法。

![](../Images/93b173d62760e28e6387ecc037918a70.png)

来自[DFQ](https://arxiv.org/abs/1906.04721)的图表显示了在没有应用特殊技术的情况下，PTQ性能如何迅速下降，在低于12位时出现灾难性的损失。即便是DFQ方法，在低于8位时也无法维持效果。

Choukroun等人2019年提出的[OMSE](https://arxiv.org/abs/1902.06822)方法通过找到每个卷积核的量化参数，最小化量化后的权重/激活张量与原始张量之间的均方误差（MSE），标志着首个实现4位量化并保持较小精度损失（在ImageNet顶级1分类上为3%下降）的PTQ方法。作者选择使用对称均匀量化以提高效率，避免了使用偏移量所带来的额外计算。由于给定卷积核的量化引起的MSE与缩放因子之间的关系是非凸的，作者使用线搜索方法来发现最优值。为了避免在敏感网络层中使用混合精度来保持表示能力，作者建议用多个低精度张量表示这些“关键”层，但他们警告说，这种方法的复杂性要求仅用于小型张量，在卷积神经网络（CNN）中效果很好，因为最敏感的部分是卷积核，但对于具有大规模显著组件的架构，该方法不适用。

在2020年初，HAWQ论文的作者发布了[ZeroQ](https://arxiv.org/abs/2001.00281)：一种无数据的PTQ方法，超越了DFQ设定的先前最先进的ZSQ基准。该方法通过一种新颖的帕累托前沿方法实现了混合精度量化，自动确定最佳的混合精度设置，无需手动搜索。ZeroQ方法不需要访问训练或校准数据，而是生成一个合成数据集，旨在匹配批量归一化层中的统计信息，称为“蒸馏数据”，然后使用该数据生成的激活值来校准量化参数并执行逐层敏感性分析。这些敏感性值会反馈给帕累托前沿选择过程，从而找到适合给定模型大小或所需精度级别的最佳设置。作者指出，大多数PTQ工作通常仅对图像分类准确性进行基准测试，而不考虑更复杂的任务，因此他们还证明了该方法在更具挑战性的目标检测任务中保持了性能。ZeroQ是[开源](https://github.com/amirgholami/ZeroQ)的，且计算效率极高，为网络量化提供了较低的入门门槛。

![](../Images/29acd6b57f938f8140976dcc50d926cc.png)

来自[ZeroQ论文](https://github.com/amirgholami/ZeroQ)的结果表明，他们的方法优于以前最先进的PTQ方法。“No D”表示“无数据”（无数据或零-shot），而“No FT”表示不需要微调（PTQ）。请注意，8位ZeroQ在没有任何数据或重新训练的情况下，提供了非常接近全精度基准的性能。

2020年晚些时候，[AdaRound](https://arxiv.org/abs/2004.10568)的作者指出，尽管四舍五入到最近的方式在量化的前期工作中占主导地位，但它仍然是一种次优的四舍五入方案。相反，他们提出了一个框架来分析四舍五入的影响，这个框架考虑了输入数据和任务损失的特性，并将四舍五入公式化为每层的二次无约束二进制优化（QUBO）问题。他们使用二阶泰勒级数展开，近似任务损失对权重扰动的变化，采用其他工作中常见的方式，利用海森矩阵信息来度量灵敏度，最早可以追溯到1989年的[Optimal Brain Damage](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)（OBD）。类似于[Optimal Brain Surgeon](https://www.researchgate.net/publication/3568764_Optimal_Brain_Surgeon_and_general_network_pruning)（OBS），他们扩展了他们的方法，从而能从对海森矩阵非对角元素的更深入理论分析中受益。他们认为，天真的四舍五入到最近的方式相当于仅考虑海森矩阵的对角线，其中扰动被假设为对任务损失的贡献没有共变性，因此只有各自幅度的减少才重要（即四舍五入到最近）。然而，权重扰动的影响是相互关联的，因此非对角线信息是重要的，因为它能表明当扰动组合实际上对损失有利时。AdaRound仅需要少量的未标记数据，并在CNN的PTQ（后训练量化）中设定了新的最先进技术，能够将ResNet模型压缩到4位，同时保持在其基准性能的1%以内。

为了生动地展示他们的观点，AdaRound的作者使用Gupta等人2015年提出的随机四舍五入方法，生成了100个随机扰动集，针对ResNet的第一层，并将这些扰动与四舍五入到最近的扰动进行了比较。在这100个抽样的层扰动中，48个表现优于四舍五入到最近的方式，其中一些甚至提供了超过10%的准确度提升。这表明在量化空间中存在许多更好的解决方案，并清楚地表明应该有方法能够针对它们，以提高PTQ的准确性。

![](../Images/4ee2b95ec4efe02b2d5eea55dff66b3c.png)

来自[AdaRound](https://arxiv.org/abs/2004.10568)的图表显示了在与四舍五入到最近方案进行比较时，随机抽样的扰动在性能上的分布，表明有许多更好的解决方案，而且这些更好的解决方案与二阶泰勒级数项高度相关。

在上图中，我们可以清楚地看到二阶泰勒级数项与量化后精度下降之间的关联，表明这一项可以很好地作为优化量化任务损失的代理。然而，即使忽略了网络权重之间的跨层交互，Hessian仍然在大规模网络层计算时代价高昂。下面的结果显示，AdaRound的W4A8配置接近于FP32基准和W8A8 DFQ的性能。然而，在这个比较中，值得注意的是，AdaRound并不是像DFQ那样的数据无关或“零-shot”方法。

![](../Images/fa01a1f6c698f06e32420a0e8fce430c.png)

来自[AdaRound](https://arxiv.org/abs/2004.10568)的结果表明，该方法能够保持CNN性能，在W4A8精度下比以往的方法更好。

在2020年中期AdaRound发布几个月后，[AdaQuant](https://arxiv.org/abs/2006.10518)进一步提出了一种新的PTQ方法，该方法可以将权重和激活量化到4位，同时保持性能，使用ResNet50时ImageNet的Top-1精度下降不到1%。作者通过使用小型校准集，以逐层的方式使用通道粒度最小化权重和激活的逐层量化均方误差，从而绕过了AdaRound的局限性。他们指出，量化过程会引入固有的偏差和方差，影响批归一化的统计数据，并提出通过重新估算这些统计数据来恢复性能下降，这就是他们提出的批归一化调优（Batch Normalization Tuning，BNT）方法。作者提供了AdaQuant的混合精度和固定精度变种，代码可在[GitHub上获取](https://github.com/itayhubara/CalibTIP)。

![](../Images/553e39581e08c66868624ccaeef38a80.png)

[AdaQuant](https://arxiv.org/abs/2006.10518)的结果显示，在不同的校准数据集规模下，AdaQuant相较于AdaRound和量化感知知识蒸馏（QAT-KLD）在ImageNet的Top-1精度上表现更为优秀。每种配置的方差是通过5次实验计算得出的。

在这一部分中，我们看到了在后AlexNet时代PTQ方法的日益关注。尽管在此期间，PTQ的主要驱动力通常是边缘部署和数据隐私的考虑，但这里讨论的工作为即将到来的依赖PTQ方法奠定了重要基础，这些方法将随着未来几年模型规模的急剧扩大而兴起。在我们告别CNN时代之前，还有一个在这一时期逐渐成型的研究趋势需要讨论，这一趋势同样受到希望在边缘设备上运行的驱动：将神经网络极端量化到仅1位（二进制网络）或2位（三进制网络）。

## 极端量化：二进制和三进制网络

![](../Images/eec8ff7d1201ddb2d5799d2bc9bc2c9e.png)

图片由作者使用DALL-E 3生成。

极限量化是指将模型压缩到≤2位的精度，这意味着要么是三值（2位），要么是二值（1位）。将模型有效压缩到如此低的精度显然带来了显著的好处，因为这些模型比其FP32对应物小16到32倍，能够部署到较小的边缘设备上，消耗更少的功率，同时为优化计算速度节省宝贵的芯片空间。不足为奇的是，神经网络精度如此降低带来了同样极端的挑战，因为表示能力会丧失。然而，网络计算中代价高昂的乘加（MAC）操作在二值和三值网络中可以被更为节能的加法/减法以及位移操作完全替代，从而使得潜在收益极为显著，并激励研究人员去应对相关挑战。在这一节中，我们将观察到在CNN时代，极限网络量化这一发展领域带来的引人注目的成果，并发现为何它后来成为如此具有吸引力且频繁被引用的领域：低位网络带来的非凡效率提升是难以忽视的。此外，我们还将看到，二值网络可以通过集成使用与定点网络等效数量的比特，超越定点网络的性能，同时保持二值运算的所有优势。首先，让我们回到2014年，以便逐步建立我们对低精度网络的理解。

作为极限量化领域的开创性工作，2014年来自首尔国立大学的黄和成研究人员发表的《[使用+1、0和-1权重的定点前馈深度神经网络设计](https://www.semanticscholar.org/paper/Fixed-point-feedforward-deep-neural-network-design-Hwang-Sung/a4db2d26b5d169de6b64de361dc7d4fd5b1f61a3)》是一种量化感知训练（QAT）方法，该方法通过获得三值（2位）权重和3位激活信号，几乎没有性能损失。相反，作者观察到，为了保持性能，偏置项必须分配更高的精度，即8位。量化阈值最初选择是为了最小化原始张量和量化张量之间的均方误差（MSE），然后在每一层逐一执行穷举搜索，以调节这些初步提议，直到它们达到能够最小化网络输出误差的最优值。最后，通过使用针对量化进行修改的定点反向传播方案来微调量化后的权重。虽然作者在极限神经网络量化领域奠定了基础性先例，但他们采用的穷举搜索方法也证明了当时模型尺寸较小，而对于未来日益增长的模型规模，更多可扩展的解决方案变得更加必要。

在2015年，Courbariaux 等人提出了 “[BinaryConnect](https://arxiv.org/abs/1511.00363)” 算法。顾名思义，他们的方法生成一个二值化网络，其中权重被限制为-1或1。作者指出，噪声权重，即使用随机舍入方案离散化的权重，“与随机梯度下降（SGD）非常兼容”，正如我们之前在 Gupta 等人2015年关于随机舍入的研究中看到的那样。与 Hwang & Sung 上述方法类似，作者将由量化权重生成的损失梯度应用于更新全精度权重（这些权重是单独存储的），量化权重则是根据每次前向传播时全精度权重的当前状态得出的。作者展示了 *确定性二值化*（简单地取权重符号）和 *随机二值化*（使用权重大小推导概率）作为正则化机制（类似于 dropout）在所研究的网络中都能很好地工作，展示了比非正则化的全精度基线更慢的收敛曲线和更低的最终验证误差。尽管这些结果非常令人兴奋，但需要考虑的是，CIFAR-10 数据集和他们研究中训练的CNN按今天的标准并不复杂，因此目前尚不清楚这些结果是否能在更深的网络或更具挑战性的任务上保持有效。BinaryConnect 的代码 [可在 GitHub 上获取](https://github.com/MatthieuCourbariaux/BinaryConnect)。

![](../Images/379a1cdc07bb590d1a78f48ffefea042.png)

来自 [BinaryConnect](https://arxiv.org/abs/1511.00363) 的结果表明，网络权重的二值化作为一种正则化形式，实际上改善了CNN和研究任务的训练结果。

后来在2015年，Lin 等人提出的 “[Neural Networks with Few Multiplications](https://arxiv.org/abs/1510.03009)” 扩展了 BinaryConnect 的工作和代码库，并在 [原始代码库的分支](https://github.com/hantek/BinaryConnect) 中加入了他们的随机二值化方法的三值变体，称为“TernaryConnect”，并引入了 *量化反向传播*（QBP）方案，其中网络激活值被量化为二的整数次方，这样在反向传播过程中昂贵的乘法操作就可以被高效的位移操作所替代，进一步提高了训练效率，同时也完成了在网络操作中可二值化的另一个项目。

![](../Images/b2b8aa901338cae2878b97e3bb07ca36.png)

来自 [Lin 等人 2015](https://arxiv.org/abs/1510.03009) 在 CIFAR-10 上的结果显示，使用 QBP 可以得到与 BinaryConnect 等效的结果，而且三值网络的训练结果略好。所有量化方法都因随机量化的正则化效应而超越了基线表现。

在2016年，Courbariaux及其同事在此基础上进行了第三次改进，使得在训练过程中也能实现激活函数的二值化。在他们的开创性论文《[Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1](https://arxiv.org/abs/1602.02830)》（BNN，或常常称为他们的第一版草稿和[GitHub 仓库](https://github.com/MatthieuCourbariaux/BinaryNet)的“[BinaryNet](https://arxiv.org/abs/1602.02830v1)”）中，作者再次对比了确定性和随机二值化，观察到尽管从理论和实证上讲，随机二值化优于确定性二值化，但由于训练过程中无需生成随机位，确定性二值化在简化方面具有很大的吸引力。因此，作者采用了一种“取长补短”的方法，其中在训练过程中仅对激活函数使用随机二值化。为了寻求进一步的改进，作者解决了神经网络中批量归一化（[BN](https://arxiv.org/abs/1502.03167)）层所需的高昂乘法操作，提出了一种方法来避免这些昂贵的运算，同时保持网络操作的二值化特性，这就是*基于移位的批量归一化*（SBN），它通过廉价的位移（以2的幂次为基础的缩放）操作代替乘法运算，近似了批量归一化的效果。同时，作者提出了一种*基于移位的 AdaMax 优化算法*，以绕过[Adam优化器](https://arxiv.org/abs/1412.6980)所需的乘法操作，并证明训练结果不会受到这两种近似方法引入的噪声的影响。在对CIFAR-10数据集的基本CNN模型进行测试时，作者使用二值化的权重和激活函数训练神经网络，展示了有力的实验结果。用于训练这些二值化神经网络（BNN）的代码，包括定制的GPU内核，已[在线发布](https://github.com/MatthieuCourbariaux/BinaryNet)。

![](../Images/5179de1eee7c48ee8f6673f04cc3231e.png)

BNN 论文的结果表明，二值化网络在验证误差上几乎与基线相当。

Rastegari 等人 2016 年的 [XNOR-Net](https://arxiv.org/abs/1603.05279) 论文首次在大规模的 ImageNet 数据集上测试了 CNN 的二值化，结果表明 BinaryConnect 和 BNN（即 BinaryNet）在这些规模下的表现不如预期。作者通过引入一种新的权重二值化方法，该方法结合了权重缩放，超过了 BNN 在 ImageNet top-1 准确率上的 16.3% 的成绩，并发现给定权重矩阵的最优缩放因子是其绝对值的平均值。与 BinaryConnect 和 BNN 相似，作者使用 STE 从量化的前向传递中计算梯度来更新一组独立的全精度权重，尽管他们没有选择在 BNN 论文中提供的基于移位的 BN 和 Adam 近似方法。从他们的 Binary-Weight-Network (BWN) 配置到 XNOR-Net 配置的性能下降突显了在像 ImageNet 这样复杂任务中离散化激活信号的难度，但二值化这些表示尤其具有挑战性。然而，权重单独二值化能够达到与基准 CNN 相同的性能，这为“免费午餐”压缩提供了另一个有希望的机会，且 XNOR-Net 版本即使在完全信号二值化的情况下仍能取得合理的表现，这一点令人信服。XNOR-Net 的代码也可以在 [GitHub](https://github.com/allenai/XNOR-Net) 上找到。

![](../Images/c50043468789da3545338108bea3109f.png)

来自 [XNOR-Net](https://arxiv.org/abs/1603.05279) 的图展示了他们提出的两种方法的准确度权衡。请注意，没有二值化输入的 Binary Weight Network 达到与基准相同的性能，但并未实现 XNOR-Net 配置中的显著计算节省。

![](../Images/921453b1d03e79854734fb25605ce599.png)

来自 [XNOR-Net](https://arxiv.org/abs/1603.05279) 论文的结果显示，XNOR-Net 在复杂的 ImageNet 分类基准上超越了 BNN（在全端到端二值化中）以及 BWN 超过了 BinaryConnect（在仅权重二值化中）的表现。请注意，对于 XNOR-Net 的 top-1 分数，评估分数似乎在中途饱和，这可能表明使用二进制信号时表现能力的严重缺乏。

Li 等人于2016年发表的《三值权重网络》（[TWN](https://arxiv.org/abs/1605.04711)）是一种量化感知训练（QAT）方法，旨在从头开始训练三值网络。他们的量化方案通过使用学习得到的逐层缩放因子和三值化阈值（设定为每个权重张量的平均值的3/4），尽可能精确地逼近全精度权重，旨在最小化量化权重和原始权重之间的欧几里得距离。作者观察到三值网络“比二值精度的网络具有更好的表达能力”，并通过二值和三值3x3卷积滤波器的示例演示了这一点，后者分别可以有512和19683种不同的模板。实验结果证明，这种额外的表达能力对各种任务（包括MNIST、CIFAR-10、ImageNet和[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/)目标检测任务）都有益。在下面的结果中，我们可以看到，三值网络在更加复杂的ImageNet和Pascal VOC任务中表现出更强的优势，这可能是一个信号，表明我们在较为简单的模型和任务中看到的二值化结果所带来的乐观情绪，在复杂性增加时可能无法保持。

![](../Images/069e18c2d9d41d6908d90268d4b8a2c3.png)

[TWN](https://arxiv.org/abs/1605.04711)论文中的结果表明，三值网络的额外表达能力是有益的，特别是在更具挑战性的ImageNet和Pascal VOC任务中。请记住，BNN 和 XNOR-Net 对激活信号进行二值化，而TWN方法像BinaryConnect和BWN一样，仅专注于权重的量化，这相对较少具有挑战性。

2016年末，Hou、Yao 和 Kwok 发表了《深度网络的损失感知二值化》（[LAB](https://arxiv.org/abs/1611.01600)），填补了前人方法未能基于其对代价函数的直接影响优化二值化过程的研究空白。为了以最小化代价函数的方式对网络进行二值化，作者通过使用 Adam 优化器中捕获的二阶梯度信息，求解了一个[近端牛顿算法](https://arxiv.org/abs/1206.1623)，从而高效地提取了对角Hessian近似，而不是直接计算Hessian。作者展示了他们的方法“对宽而深的网络更具鲁棒性”，并将他们的研究扩展到使用循环神经网络（RNN）处理自然语言处理（NLP）任务。随后在2018年初，Hou 和 Kwok 将他们的 LAB 算法扩展到“[深度网络的损失感知权重量化](https://arxiv.org/abs/1802.08635)”（LAQ，或者在三值情况中为 LAT），展示了该方法在比 LAB 二值化更高精度的网络上的表现。

![](../Images/40776c34364cc2b4d70a98bfafe78ed8.png)

来自[LAQ](https://arxiv.org/abs/1802.08635)论文的结果显示与LAB、BinaryConnect和BWN的比较。

2017年，[Dong等人](https://arxiv.org/abs/1708.01001)提出了*随机量化*算法：一种量化训练方法，用于极端量化，在每次训练步骤中，只有部分网络元素/滤波器（与量化误差成反比）被量化，并与全精度权重分开更新。随着训练的进行，最终所有权重都会被量化，最终得到的低位网络在准确度上显著优于等效的BWN和TWN模型。

![](../Images/689145e348d27fbe93c913d7afe9910f.png)

来自[Dong等人 2017](https://arxiv.org/abs/1708.01001)的结果表明，随机量化（SQ）相较于基线模型有显著的改进。

2017年提出的增量网络量化（[INQ](https://arxiv.org/abs/1702.03044)）论文，我们之前在卷积神经网络（CNNs）的量化训练（QAT）部分看到过，超越了2015年由深度压缩（Deep Compression）设定的模型压缩最前沿，还探讨了他们的方法在创建三值网络中的可行性。下图中，我们可以看到他们的方法在ResNet-18上训练ImageNet分类任务时明显优于TWN，错误率降低了超过4%。从上面来看，我们还可以看到，它比随机量化TWN（SQ-TWN）的36.18% top-1错误率低了超过2%。

![](../Images/ae4567246cee4f0d86a9421fdda6f8a9.png)

2017年，Lin等人的“迈向准确的二值卷积神经网络”（[ABC-Net](https://arxiv.org/abs/1711.11294)）论文试图通过结合多个二值权重或激活集来克服二值网络在表示能力上的不足，以更忠实地表示高精度值，展示了使用3到5个权重基和5个二值激活时，ImageNet上的精度下降可从基线减少到5%。作者指出，尽管这需要使用更多的位数，但该方案比使用更高位数的定点表示更可取，因为它仍然避免了更复杂的算术运算，位运算仍然在二进制中进行。他们的工作标志着二值神经网络首次在ImageNet上达到了与全精度基准相当的性能，但他们的解决方案将基线BNN的复杂度增加了*O(k * l)*，其中*k*是使用的权重基数数量，*l*是使用的激活基数数量，因此在效率上有明显的损失。

![](../Images/668c6f21802010eb3ab5911c79826819.png)

朱等人2018年发表的论文《二元集成神经网络：每个网络更多比特，还是每比特更多网络？》([BENN](https://arxiv.org/abs/1806.07550))提出，BNN的局限性无法通过进一步优化二值化过程来解决，因为这些局限性根源于二进制空间缺乏表示能力。作者的目标是通过创建多个BNN的集成，利用提升（boosting）或装袋（bagging）方法来减少预测方差并提高对噪声的鲁棒性。实验表明，集成分类器的统计性质得到了改善，性能也因此大幅提升。执行这些集成的额外复杂度仅为*O(k)*，比ABC-Net高出*l*倍，并且在ImageNet上显著超越了它。此外，由于集成可以并行化，这个解决方案的额外复杂度可以是*O(1)*，并且运行速度与基准BNN一样快。

![](../Images/e6af35d1cf27a599a1517cd4c9a0c41c.png)

来自[BENN](https://arxiv.org/abs/1806.07550)的结果表明，BNN集成能够显著提升二值化网络的性能。值得注意的是，他们将其与仅使用1比特权重和激活函数的ABC-Net进行了比较，这一选择颇为奇特，因为他们将其与3个或6个BNN的集成进行了比较。由于这些集成的复杂度在没有并行化的情况下分别为O(3)和O(6)，因此将它们与具有类似复杂度的ABC-Net配置进行比较更为公平。正如我们上面所见，ABC-Net的复杂度范围从49.1%（3比特/1比特）到54.1%（5比特/1比特），尽管这些结果仍然较低，但提供了更多有意义的比较。再者，INQ产生的三元网络在使用更少的比特时就超过了这些结果，但并行化的二元网络集成概念仍然极具吸引力。

在本节中，我们见证了极端神经网络量化领域的兴趣爆炸，尤其是在大型CNN问世之后，这一趋势始于2012年AlexNet的巨大成功。在这个时代，部署具有新高度建模能力的神经网络于低功耗硬件的边缘设备上的诱人需求是不可抗拒的，因为这是深度学习的许多实际应用所在。需要注意的是，本节中的大多数方法是从头开始训练低比特网络，而不是量化预训练权重，这在完全量化方法中通过其卓越的效率更容易实现。稍后，我们将看到，在极端量化方法的蓬勃发展期过后，未来的突破将在这一领域的肥沃土壤中孕育而生。

现在，我们结束了关于 CNN 时代的章节，因为我们已经看到了几个量化研究领域在 AlexNet 成功后蓬勃发展的过程，这一成功带来了大量的人才和资金进入深度学习领域。我们已经看到，QAT 方法通过微调量化权重，达到了令人印象深刻的压缩效果而不损失性能，混合精度技术通过将敏感性分析融入量化过程，达到了新的压缩水平，PTQ 方法在 8 位甚至 4 位精度下，与基准性能匹配且无需重新训练（在 ZSQ 的情况下，甚至不需要使用任何校准数据），最后，我们看到了极限量化研究的兴起。现在，我们将焦点从 CNN 时代转移到由 2017 年变压器架构的成功引发的自然语言处理（NLP）研究浪潮。

# 大型语言模型（LLMs）的量化

![](../Images/8c8e644f6f6d58e0d7d07ac93e3621f7.png)

图片由作者使用 DALL-E 3 生成。

现在我们已经了解了量化的功能细节和历史，我们可以继续讨论今天大型语言模型（LLMs）的量化。正如我们在剪枝中看到的那样，进入大型模型的世界伴随着对需要模型训练的压缩技术（如 QAT）的希望减弱，除了最大的机构之外，几乎没有人能采用这些方法。因此，我们将看到研究焦点转向更加轻量化的 PTQ 方法，尽管在大规模操作中的独特挑战并没有阻止研究社区的创造力，他们找到了利用 QAT 的方法，正如我们将看到的那样。在本节中，我们首先回顾了从变压器架构在 2017 年的开创性论文《[Attention is All You Need](https://arxiv.org/abs/1706.03762)》发布到 2020 年标志着 175B（十亿参数）[GPT-3](https://arxiv.org/abs/2005.14165)发布的 LLM 时代曙光之间的量化努力。接着，我们回顾了 LLM 时代 PTQ 方法的普及，随后将焦点转向 LLM 的 QAT 方法，最后通过回顾 LLM 的极限量化，结束我们的研究。本节将完成我们在量化方面的学习，为我们在下一节中进行实现指南的学习做好准备，并开始在我们自己的工作流程中应用神经网络量化。

## 变压器早期时代的量化

正如我们在上一节中所见，AlexNet在计算机视觉中的成功引发了对卷积神经网络（CNN）进行量化以实现高效部署的研究热潮，但对语言模型中量化应用的研究直到几年后才开始加速，催化剂时刻是变换器架构的显著成功。在2020年GPT-3发布后，语言模型规模的爆炸性增长之前，曾有一段时间，基于变换器的自然语言处理（NLP）探索相对适中，尽管这些模型随着规模的不断增大，性能持续提升的趋势很快就变得明显。在这里，我们回顾了变换器基础的语言模型量化研究的形成时期，这一时期发生在变换器的出现和多十亿参数变换器网络的兴起之间。正如我们将看到的，这种新型网络架构在量化方面提出了独特的挑战，特别是在低位宽下，研究人员迅速行动，力求理解并克服这些挑战。

由谷歌于2018年末发布的开创性双向编码表示变换器（[BERT](https://arxiv.org/abs/1810.04805)）继续作为一种基于变换器的语言模型，对行业产生着深远影响。BERT通过掩蔽语言模型（MLM）目标进行训练，学习如何编码双向上下文，生成能够同时考虑输入序列中给定输入标记之前和之后信息的嵌入，从而产生包含深刻上下文理解的表示，这对情感分类和问答等任务非常有用。尽管BERT使用变换器架构的仅编码器变体来创建这些双向编码表示，但与此相对，OpenAI的生成式预训练变换器（[GPT](https://openai.com/research/language-unsupervised)）模型则采用仅解码器变换器架构，在字节对编码（[BPE](https://arxiv.org/abs/1508.07909)）文本上执行自回归建模任务，只考虑序列中前面的标记，并通过逐步预测接下来的标记。关于这些自回归仅解码器变换器及其无标签预训练数据集的潜在可扩展性的发现，最终启发了2020年发布前所未有的大型175B [GPT-3](https://arxiv.org/abs/2005.14165)模型的诞生，但正如我们所见，变换器量化的研究已经在那时取得了长足的进展。

2019年底，来自加州大学伯克利分校的Shen等人发布了[Q-BERT](https://arxiv.org/abs/1909.05840)，这是一种QAT方法，它扩展了HAWQ基于Hessian的灵敏度分析，额外考虑了训练数据子样本中Hessian谱的方差，而不仅仅是均值。Q-BERT利用这一改进的灵敏度度量，建立了一个逐层混合精度量化方案，然后使用组粒度对每一层进行量化，其中矩阵被拆分成子单元，每个子单元都有自己的量化范围和查找表。使用他们的方法，作者实现了权重的13倍压缩、激活大小缩小4倍、嵌入大小缩小4倍，且准确度从基线BERT最大下降2.3%。像大多数QAT技术一样，Q-BERT使用STE来近似通过不可微的量化函数的梯度。

与Q-BERT专注于使用混合精度最大化压缩不同，2019年底Intel的Zafrir等人提出的[Q8BERT](https://arxiv.org/abs/1910.06188)则专注于在模型和激活函数上应用统一的8位量化，这从硬件优化的角度来看更具优势，因为混合精度操作往往会增加开销，并且不利于通用硬件加速。他们的方法将QAT作为预训练BERT模型的微调阶段，使用基于Jacob等人2017年论文中介绍的模拟量化方法（如我们在CNN量化部分看到的那样），以实现4倍压缩，并且准确度损失最小，不超过1%。该方法使用STE进行梯度近似。权重的缩放因子通过最大绝对权重幅度进行校准，而激活的缩放因子则基于在训练过程中累积的指数加权移动平均。

在2020年，Fan等人提出了[Quant-Noise](https://arxiv.org/abs/2004.07320)，这是一种QAT技术，通过在每次训练中的前向传播过程中仅随机量化一部分网络权重，来实现高压缩率。这样大部分权重能够在没有STE近似引入的误差的情况下得到更新，从而使它们的值能够更准确地调整，以减少量化子集的影响。随着时间的推移，这种方法的效果比对所有网络权重同时进行QAT要更好。Quant-Noise研究了*产品量化*（PQ）的使用，其中多个权重被一起量化成单一的代码字，这允许在性能下降较小的情况下实现非常高的压缩比。这种方法利用了由网络结构引起的权重之间的相关性。

2020年，来自多伦多大学的Zadeh等人发布了[GOBO](https://arxiv.org/abs/2005.03842v2)，这是一种基于字典的PTQ方法，适用于基于注意力的模型，可以使用非均匀量化将BERT中的几乎所有FP32权重压缩到3位而不损失精度。异常值以全精度保留以保护准确性，并通过高斯分布拟合自动检测，而其余权重则使用3位代码字存储，这些代码字索引一小组（在3位的情况下为8个）代表性的FP32质心。由于典型硬件无法直接对3位值进行操作（正如Q-BERT的作者在其研究中也指出的），GOBO的作者开发了一种新型硬件架构，能够高效加速3位计算，以补充他们的量化方法。尽管加速效益只有在使用专用硬件时才能完全实现，但3位量化模型减少的内存占用将减少更多通用硬件上的内存存储和流量，因此仍能在推理延迟和能耗方面提供一定程度的改进。GOBO方法的灵感来源于“深度压缩”中将霍夫曼编码权重存储在少量代表性值（质心）字典中的方式，但与此不同的是，它不需要微调，通过不量化权重异常值显著提高了准确性，并使用了一种新型的“质心选择算法，该算法收敛速度是K均值的9倍，并始终将所需质心的数量减少一半。”

张等人于2020年发布的[TernaryBERT](https://arxiv.org/abs/2009.12812)将知识蒸馏（灵感来源于[TinyBERT](https://arxiv.org/abs/1909.10351)）与三值量化结合，提出了他们所称的*蒸馏感知三值化*，将三值化模型视为全精度、相同大小的教师模型的学生模型。他们的蒸馏方法包括在嵌入层、变换器层的输出和注意力得分、以及预测层的logits与软交叉熵之间的MSE最小化。TernaryBERT在性能上与全精度基准相当，但体积缩小了14.9倍。作者发现，最适合BERT中激活值的量化方法是min-max 8位量化，因为他们发现BERT激活值的分布倾向于负值，特别是在早期层。他们的QAT过程从全精度模型初始化权重，并通过量化函数使用STE来近似梯度。

在2020年，[BinaryBERT](https://arxiv.org/abs/2012.15701)尝试将BERT量化推向极限，但发现由于其高度不规则的损失函数，二值化BERT模型难以直接训练。因此，作者选择训练一个半大小的三值网络，通过他们提出的*三值权重分裂*方法将其转换为二值网络，然后对结果进行微调，以在性能下降最小的情况下实现BERT的24倍压缩。他们将这一概念扩展到包括*自适应分裂*，即将更重要的层分裂，而较不敏感的层则以二进制方式表示，从而为不同的模型大小约束提供灵活性。像大多数QAT方法一样，BinaryBERT使用STE进行训练。

![](../Images/da4e3b2496c0fe7dffc0a264185d4226.png)

[BinaryBERT](https://arxiv.org/abs/2012.15701)论文的结果提供了关于BERT压缩技术及其相对大小的全面视角，并展示了他们的二值化方法所保持的令人印象深刻的表示能力。请记住，GOBO是一种PTQ方法，这在一定程度上是一个显著的障碍。

## 大型语言模型的后训练量化（PTQ）

![](../Images/5462a05b6bb2e0b7363dd619c6be0fcc.png)

对于许多阅读本文的读者来说，这将是最相关的研究部分。在这里，我们回顾LLMs的PTQ，这可能是我们大多数人希望在这些预训练大模型上使用的唯一量化类型，尽管我们可能会有惊讶。随着我们最终转向LLM量化，重要的是要记住，由于LLM的规模如此庞大，“全精度”的定义在这一时代已经改变，意味着FP16，因为这些庞大的模型通常是在FP16精度下训练的，因为使用FP32会使存储需求加倍，而性能提升却微乎其微。因此，当我们讨论LLM的压缩率时，8位整数量化相比于“全精度”基准仅提供2倍的内存占用减少。此外，或许并不令人意外的是，我们会看到，这些规模的网络具有使其量化更困难的特性，即在激活中出现少数但极为重要的异常值。因此，尽管LLM和更容易压缩的大型CNN一样，都是过度参数化的，但它们的特性使得量化变得更加困难。幸运的是，研究界迅速诊断出了这些问题，并提出了可以解除LLM高效且准确的PTQ的公式。

在2022年中期，即OpenAI于2020年发布闭源的175B GPT-3之后的两年，最大的开源模型是GPT-J 6B和GPT-NeoX 20B，而175B规模的OPT和BLOOM模型尚未发布。在此期间，微软的姚等人发表了[ZeroQuant](https://arxiv.org/abs/2206.01861)，对GPT-3风格的变换器模型量化进行了早期研究。他们的方法通过对权重和激活值使用INT8量化，能够在效率提高5.2倍的同时，达到与FP16模型相似的准确度。作者还提出了一种高效的逐层知识蒸馏（LKD）方法，通过不需要训练数据的方式，进一步将不那么敏感的权重压缩到4位，从而使内存占用比FP16模型小3倍。为了研究在将PTQ应用于大型变换器模型时观察到的性能急剧下降，作者考察了在其各层之间出现的激活（令牌）中的高度动态范围，并指出通过按最小/最大层级激活值缩放的均匀量化范围来应用，将导致在这些分布的密集区域内表现能力不佳。同样，权重矩阵具有长尾范围，这将导致在少数均匀分布的量化级别中丧失细粒度的细节。下图展示了这些逐层特性，说明为什么使用均匀网格切分这些范围会丢失大量信息。

![](../Images/84e6bd353692b8485b184f73e48db23b.png)

来自[ZeroQuant论文](https://arxiv.org/abs/2206.01861)的图表展示了GPT-3 350M中变换器层间注意力输出矩阵中的令牌激活和权重范围。

为了处理这些范围内的方差，作者提议使用权重的组别量化和激活的令牌量化。作者还指出，由于这些大型变换器模型的激活范围存在很大的方差，在*静态量化*设置下使用校准数据集对这些范围进行离线校准很可能会引发问题，因此他们选择在推理过程中动态校准令牌的最小/最大范围。为了克服令牌量化引入的数据移动开销，作者利用内核融合技术构建了优化的推理后端，将每个量化操作符与其前一个操作符（例如层归一化）融合在一起。作者将其LKD方法调整为解决大规模模型知识蒸馏的三个限制，即：需要在内存中存储两个模型，需要完全训练学生模型，以及需要原始训练数据。为了解决这些问题，作者一次优化量化模型的单层，并使用相同的教师激活来为量化层输入数据，并通过相应的全精度层生成伪标签。由于仅使用这些层级伪标签，因此LKD过程不需要标注数据，作者还展示了使用原始训练数据并非必需的。ZeroQuant作为[DeepSpeed库的一部分](https://github.com/microsoft/DeepSpeed/blob/548d37b1611e84448c5ec917f86080e65299e79d/docs/_tutorials/model-compression.md#12-weight-quantization)开源，[集成](https://huggingface.co/docs/transformers/en/main_classes/deepspeed)到Hugging Face的[Transformers](https://github.com/huggingface/transformers)库中。

![](../Images/127ea3b142b66437dbf919c1830d2e52.png)

GPT-3 350M的ZeroQuant结果表明，W8A8配置的基准性能是匹配的。对于较低的精度，LKD步骤显著提升了性能，但即使是在这里展示的数十亿参数规模的模型中，仍未恢复到基准精度。请注意，在所有设置中，从8位到4位的权重性能大幅下降，即使激活值采用更高精度，这也展示了2022年中期LLM的低比特PTQ的难度。

在2022年，Tim Dettmers及其同事介绍了[LLM.int8()](https://arxiv.org/abs/2208.07339)，即GPT3.int8()：一种用于≥175B参数规模LLM的8位整数PTQ方法，通过采用双重量化过程来保持其准确性。首先，他们使用向量化量化粒度量化大多数特征，并为矩阵乘法中的每一行/列内积使用不同的归一化常数。其次，作者发现，现有的PTQ方法在处理包含≥6.7B参数的transformer时失败的原因是，在这些规模下，transformer层中的极端离群值会大量增加，这些离群值对变化极为敏感，并且在移除时会严重影响模型性能。因此，作为双重量化过程的第二部分，作者选择通过*混合精度分解*单独处理这些离群值，并证明这能够将8位量化模型的性能保持在数百亿参数规模。代码已在[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)库中开源，并已集成到Hugging Face生态系统中。

![](../Images/99fdf5ff3397d97ba4da2c3556380e6a.png)

GPTQ（即OPTQ）的结果表明，他们对系统性离群值的单独处理能够保持≥6.7B规模transformer的准确性。

在2022年，Frantar等人发布了[GPTQ](https://arxiv.org/abs/2210.17323)（有时也使用OpenReview草稿标题[OPTQ](https://openreview.net/pdf?id=tcbBPnfwxS)引用），这是一项开创性的LLM PTQ工作，它能够在无需任何重新训练的情况下，保持预训练模型在权重量化为3位或4位精度时的准确性。作者旨在克服ZeroQuant和LLM.int8()在使用“基础的四舍五入量化变种”时出现的限制，这种方法仅在8位精度下有效。相比之下，GPTQ通过几小时的GPU计算，能够高效地将数百亿参数的LLM压缩到3位或4位精度，并且几乎不损失准确性。为了实现这一点，作者逐个量化网络中的权重，并使用近似的二阶信息来调整每一步中剩余的未量化权重，从而弥补量化当前权重时引入的误差。这个过程会一直重复，直到每个权重都被量化，而量化后的权重在算法进展时保持冻结状态。

由于GPTQ仅量化权重，因此在乘法操作中并未带来效率提升，这是因为与未量化的激活进行混合精度交互。然而，减少内存占用对于减小推理所需的GPU硬件规模至关重要，作者们能够首次在单个A100 GPU上运行开源的GPT-3等效模型：[OPT-175B](https://arxiv.org/abs/2205.01068)（由Meta AI在2022年中期慷慨提供）。虽然乘法操作没有加速，但作者们发布了定制的GPU内核，通过减少内存流量和改进并行性，使推理速度在所测试的Nvidia GPU上大约加快了4倍，这得益于将权重的反量化下游转移到与后续操作相同的GPU核心上。GPTQ是[开源](https://github.com/IST-DASLab/gptq)的，已广泛应用于多个流行的LLM部署框架，包括Hugging Face。

![](../Images/4688696edc785b8c77eb3162694fa9ef.png)

来自[GPTQ](https://arxiv.org/abs/2210.17323)的结果显示，在4位和3位量化场景下，GPTQ相比RTN（LLM.int8）展现了更好的一致性和性能。

在2022年底，麻省理工学院（MIT）与英伟达（Nvidia）合作的研究中，[SmoothQuant](https://arxiv.org/abs/2211.10438)展示了一种方法，能够在大型模型中匹配LLM.int8()的8位PTQ性能，同时实现完全量化的激活，从而避免了通过将高精度离群值的影响转移到权重中来维持高精度离群值的需求。与此相比，权重更加均匀且更易量化。下图提供了这一概念的直观视觉参考。

![](../Images/0584e0504b55dcb665ee1b2688e873fd.png)

来自[SmoothQuant](https://arxiv.org/abs/2211.10438)的图示展示了将激活离群值转移到权重中的过程，从而使其更适合量化。

SmoothQuant的作者认为，LLM中的按标记量化几乎没有益处，因为额外的粒度与LLM中出现的激活离群值并不一致。他们证明，当存在离群值时，标记会有较大的内部方差，因为这些大值只会出现在少数几个通道中；然而，离群值出现的通道的方差却在标记间很低。换句话说，离群值具有通道级的规律性，这意味着LLM的准确度可以仅通过使用通道级量化来大致保持。然而，由于通道级量化与硬件加速的INT8通用矩阵乘法（GEMM）内核不兼容，作者们利用这种可预测的通道级离群值表现来调整相应的权重，从而实现等效的数学结果，同时减少离群值，使激活张量更适合量化。

![](../Images/b7cd1c97aae8df2d4ae1993533b6a056.png)

来自[SmoothQuant](https://arxiv.org/abs/2211.10438)的图表清楚地展示了激活中异常值的通道规律，这些异常值可以被吸收到权重中。

对SmoothQuant进行了三种配置的测试：O1-O3，它们都使用张量级权重量化，但在量化激活方式上有所不同。O3是最简单的，使用每个张量的*静态量化*来量化激活，其中缩放因子是离线校准的。O2也使用张量级激活量化，但通过使用*动态量化*来在线校准运行时统计数据，性能得到提升。O1探索更复杂的标记级动态量化，但仅比O2略好一些。请注意，在SmoothQuant方法中，LayerNorm和SoftMax层仍然以FP16计算，因此不仅限于整数，该方法在8位以下不起作用。尽管如此，它标志着克服使其量化困难的LLMs的特殊性的重要进展。

![](../Images/c7d050751c2893abf221441d82e8dda6.png)

[SmoothQuant](https://arxiv.org/abs/2211.10438)的结果表明，与LLM.int8()相比，无需对激活信号进行混合精度分解即可实现相同的性能。

在2023年中期，ZeroQuant的作者推出了ZeroQuant-V2，他们提出通过使用一种称为*块-k量化*的细粒度量化（FGQ）方法来增加粒度，从而减少量化误差，其中在权重矩阵行中为每个长度为*k*的子向量设置缩放因子和/或零点。此外，作者使用低秩补偿（LoRC）进一步恢复模型性能，使用小的可训练低秩矩阵来抵消逐层量化误差。

![](../Images/9726430d450237fb245c0834c14e504c.png)

ZeroQuant-V2的结果显示了LLMs上各种PTQ方法的困惑度分数。请注意，ZeroQuant-V2与GPTQ之间的真正差异在于OPT风格模型的W4A8量化，我们现在知道由于SmoothQuant论文，这些模型包含敏感的异常值，因此ZeroQuant-V2相对于GPTQ的改进性能可能是块状粒度更好地保留激活通道中发生的异常值的能力。

林等人于2023年中期提出的《针对LLM压缩与加速的激活感知权重量化》([AWQ](https://arxiv.org/abs/2306.00978))是一种仅基于权重的低位量化方法。作者指出，尽管GPTQ非常有效，但它可能会对校准集发生过拟合，这对于像LLM这样的通用模型来说是一个问题。作者强调，并非所有权重都同等重要，而不对0.1%到1%高度显著的网络权重进行量化，可以显著减少因量化引起的性能下降。为了识别显著权重，作者着重于激活信号而非权重本身，并采用按通道缩放的方法来最小化量化误差。作者指出，“AWQ不依赖于任何反向传播或重建，因此能够在不对校准集过拟合的情况下，良好地保持LLM在各种领域和模态上的泛化能力。”作者展示了这种泛化对于保持像Vicuna这样的指令调优LLM和像OpenFlamingo这样的多模态语言模型（LMM）的性能至关重要。他们的方法使得Llama-2–70B模型可以在64GB Jetson Orin AGX上运行，或者使13B规模的模型在8GB RTX-4070 GPU上以“每秒30个token的交互速度”运行。AWQ的有效性使其在流行的开源LLM服务软件中得到了广泛应用（更多细节将在即将发布的实现指南中介绍）。

![](../Images/adf8963d2b116b320afdb3713c4c7e8f.png)

来自[AWQ](https://arxiv.org/abs/2306.00978)的结果，显示其在困惑度评分上优于RTN（LLM.int8）和GPTQ。

在2023年中，[SqueezeLLM](https://arxiv.org/abs/2306.07629)超越了GPTQ和AWQ的性能。作者提出了一种*基于灵敏度的非均匀量化*方法，利用二阶信息来搜索最佳位精度分配，并提出了一个密集-稀疏分解方法，该方法“以高效的稀疏格式存储异常值和敏感的权重值”。在3位设置下，SqueezeLLM将与其他最先进的PTQ方法相比的困惑度上升降低了一半。此外，量化模型的推理速度比FP16基准快2.3倍。如我们所见，大多数方法使用由不同粒度参数控制的均匀量化，这是由于它提供的简化应用。然而，正如我们所看到的，以及SqueezeLLM的作者所指出的，“LLM中的权重分布表现出明显的非均匀模式”，这表明非均匀量化自然会提供更好的表示。此外，作者认为，由于大多数工作只对权重进行量化并在FP16中执行算术运算，因此均匀量化的优势并未完全实现，而且非均匀量化更可取，因为它对异常值的影响较小，从而在较低精度下保留了表示能力。此外，作者将权重异常值分解并隔离在高效的稀疏表示中，从而使得这些分布更适合量化。SqueezeLLM的代码可以在[GitHub上找到](https://github.com/SqueezeAILab/SqueezeLLM)。

![](../Images/8cd2393e0bbf51a8d9b70c29cf046582.png)

来自[SqueezeLLM](https://arxiv.org/abs/2306.07629)的图表比较了它们的量化模型与相同大小的FP16模型，结果显示在相同的内存占用下，量化模型提供了显著更好的性能。这一图表有力地展示了网络量化的不可争议的优势。

![](../Images/1ab4595db245f9ab2a0d88c2c3418a2d.png)

来自[SqueezeLLM](https://arxiv.org/abs/2306.07629)的结果表明，在给定的位数限制下，指令调优的结果与AWQ大致相当。

2023 年末，Mobius Labs GmbH 开源了一种极为高效且精确的零-shot PTQ 方法，称为半二次量化（[HQQ](https://mobiusml.github.io/hqq_blog/)）。该方法能够将大型 LLaMA-2–70B 模型的量化速度提高 50 倍，比 GPTQ 快，且在测试的 GPU 上仅需 5 分钟。作者指出，GPTQ 和 AWQ 的限制在于需要校准数据，以最小化层输出之间的误差，这可能导致过拟合校准集，并且需要大量的计算时间和资源。为了解决这个问题，HQQ 通过最小化权重而非激活的量化误差，采用了一种更准确捕捉异常值影响的方式，使用了一个鼓励稀疏性的损失函数，并通过一个[*半二次求解器*](https://ieeexplore.ieee.org/document/120331)将其分解为两个可解的子问题，迭代地达到关于组-*k* 量化参数的闭式最优解，无需数据，方法通过交替优化两个子问题来实现。与流行的、无数据的 LLM.int8() 方法（来自 bitsandbytes）相比，HQQ 始终生成具有较低困惑度的模型。此外，尽管 HQQ 的执行速度比 GPTQ 快 50 倍，比 AWQ 快约 25 倍，但其结果优于或与这些依赖数据的方法相当，尤其是在精度小于 4 位的情况下。HQQ 的[代码已开源](https://github.com/mobiusml/hqq)，一些预量化的模型也在[Hugging Face hub](https://huggingface.co/mobiuslabsgmbh)上提供。虽然在 Hugging Face 生态系统中使用 HQQ 模型非常简单，但 HQQ 在优化过的[vLLM](https://github.com/vllm-project/vllm)推理引擎中的集成在写作时仍然是“实验性”的。

![](../Images/e191338797e8904e9e87bc77804f6c74.png)

计算在大型 LLaMA-2–70B 模型上，[HQQ](https://mobiusml.github.io/hqq_blog/) 与 GPTQ 和 AWQ 的时间对比。

![](../Images/4cf01ad30e6ca78fd480557325873055.png)

来自 [HQQ](https://mobiusml.github.io/hqq_blog/) 的图表显示，HQQ 在给定内存预算下，比其他最先进的方法提供了更低或相等的困惑度，同时速度更快。BNB 指的是 bitsandbytes，即无数据的 LLM.int8() 方法。

2023年12月，[SmoothQuant+](https://arxiv.org/abs/2312.03788)在4位均匀PTQ领域设立了新的最先进技术。作者们解决了AWQ的局限性，AWQ的搜索空间随着层数的增加而扩展，并且在搜索过程中没有考虑到误差积累问题，这使得搜索速度较慢，且模型的准确性下降。进一步地，他们指出，权重量化误差会被激活值的异常值放大，因此通过权重调整平滑这些异常值，可以大大减少整体的量化误差。作者提供了适用于流行的vLLM推理引擎的自定义W4A16内核，并设计了SmoothQuant+的实现，使得用户无需预处理步骤，即可直接从Hugging Face hub加载FP16模型，在将模型移到GPU时自动应用量化过程。不幸的是，截至目前，SmoothQuant+算法代码尚未发布到[公开的GitHub页面](https://github.com/adlik/smoothquant+)，并且其在vLLM中的集成也尚未上线。

![](../Images/2d22156bac4cd17642bdd92d59f77165.png)

[SmoothQuant+](https://arxiv.org/abs/2312.03788)的结果显示，在大模型中，其性能优于RTN和AWQ，并超越了FP16基准。

2024年4月，Park等人发布了他们最新版本的[LUT-GEMM](https://arxiv.org/abs/2206.09557)论文。在这项工作中，作者指出，与同时量化权重和激活值相比，仅量化权重通常能够在给定准确度下实现更高的压缩率，因为内存流量主要由权重主导，而激活值对量化更为敏感。然而，正如我们所讨论的，这也会失去仅整数的优势，并且需要在乘法前对权重进行去量化。为了解决这个问题，LUT-GEMM是一个内核，旨在实现量化权重与未量化的FP16激活值之间的直接矩阵乘法，而无需去量化步骤。

![](../Images/0045b8e8f419d9a96749878eea98462b.png)

来自[LUT-GEMM](https://arxiv.org/abs/2206.09557)的图表显示，通过使用LUT系统避免高成本的去量化步骤带来的好处。

LUT-GEMM扩展了XNOR-Net中使用的二进制编码量化（BCQ）格式，从而实现了简单的算术运算。尽管BCQ最初是为非均匀量化设计的，作者通过添加偏置项证明它同样适用于均匀量化，这大大增加了表示能力。作者构建了一个潜在子向量乘积的查找表（LUT），因为可能的组合数量有限，然后通过在运行时使用遇到的子向量来索引这些子向量，而不是执行生成它们的操作，从而大幅提高了效率。使用他们的量化方案，他们在单个GPU上展示了比GPTQ快2.1倍的推理加速。LUT-GEMM是强大的[MLC-LLM](https://llm.mlc.ai/)推理引擎所使用的默认量化方案，稍后我们将进一步了解它。

![](../Images/1bc692d04df6c5b49e70dc2794e4eca9.png)

来自[LUT-GEMM](https://arxiv.org/abs/2206.09557)的结果显示，他们的内核和量化方法在推理延迟方面优于AWQ和GPTQ。

在本节中，我们见证了PTQ的遗产在LLM时代的实现。这些庞大的模型训练成本极为昂贵，因此前所未有地需要创新高效的一次性或零-shot量化方法。因此，研究人员向开源社区提供了极为有效的方法，bitsandbytes（LLM.int8）、GPTQ和AWQ变得特别流行，最近发布的HQQ提供了一种强大的数据无关替代方案。在实现指南中，我们将权衡这些量化算法之间的比较优缺点。但首先，让我们通过勇敢探索LLM的QAT来完成我们的研究之旅，因为我们知道QAT本质上能实现更优的量化效果，也许我们会惊讶于这些方法的可接近性。

## LLM的量化感知训练（QAT）

![](../Images/3ff3effb6c85428a789613bcbed34194.png)

作者使用DALL-E 3生成的图像。

在我们的最后研究部分，我们回顾了针对LLMs的QAT方法这一令人生畏的领域。有人说，训练LLaMA-2的费用超过了2000万美元，那么我们是否应该涉足这一领域？尽管这里的一些方法对于我们个人可能无法接触，但它们对于更广泛地理解可能的技术仍然是非常重要的。一个问题是，如果QAT是生成高效、低位数模型的最有效方法，并且通过将量化纳入训练过程中不会影响模型的性能，那么为什么那些花费数千万美元训练开源基础模型的公司不应该将这些技术作为发布策略的一部分呢？有多少LLaMA-2的用户在使用全精度权重？既然我们知道绝大多数部署都将使用量化技术，难道不应该让大模型更适合量化吗？这一部分可能有助于我们在这些问题上形成更有见地的看法。

刘等人在2023年中期的“[LLM-QAT](https://arxiv.org/abs/2305.17888)”是首个针对LLM的QAT探索。作者指出，目前最先进的PTQ方法在精度低于8位时未能保护准确性，而且尽管QAT可能是保持低精度下性能的必要方法，但之前的研究没有探讨LLM的QAT，原因我们已经了解：需要大量的数据和计算资源，以及一些更微妙的原因，例如，复制指令调优LLM的训练过程的难度。为了绕过这些挑战，作者提出了一种*无数据知识蒸馏*方法，利用预训练模型生成的样本作为训练数据，这比仅使用原始训练数据的大部分子集训练要更好地保留输出分布。

![](../Images/d4255652644f48733a6891840831a8a8.png)

来自[LLM-QAT](https://arxiv.org/abs/2305.17888)的图表提供了一个直观的视觉参考，展示了变换器层中权重、激活和KV的量化。

LLM-QAT的作者发现，即使是一个相对较小的10万个合成样本集，也足以提炼量化模型，这可以通过合理的计算资源完成。他们发现，在每次生成步骤中随机采样最可能的标记对于给数据添加噪声至关重要。与大多数之前的变换器量化研究不同，LLM-QAT研究了存储在KV缓存中的中间激活向量的量化，尤其在长序列中尤为有利。作者将他们的研究限制在LLaMA模型≤30B参数，这反映了QAT的基础开销；话虽如此，作者在他们的研究中基准了多种混合精度配置，显示他们的方法在低比特设置下能够显著优于GPTQ和SmoothQuant，保持LLM的性能。然而，在较高精度下，结果大致相当，这表明，当较高精度是可容忍的时，PTQ方法更具优势，因为它们更简单。尽管如此，LLM-QAT能够高效地在没有训练数据的情况下重新训练LLM以适应低比特设置并保持性能，这一点非常令人兴奋，尤其是在LLM变得更加参数高效的背景下。该代码[可以在GitHub上找到](https://github.com/facebookresearch/LLM-QAT)。

![](../Images/abd8d25e28ef4c5f38b61c87f4f0ed87.png)

[LLM-QAT](https://arxiv.org/abs/2305.17888)的结果显示，在低比特设置下，他们的方法具有优势。比特值按W-A-KV顺序显示。困惑度被认为是一个严格的评估指标。

在2023年，Xi 等人简明扼要地发布了题为“[使用4位整数训练变压器](https://arxiv.org/abs//2306.11987)”的方法，提出了一种通过将所有乘法操作都在4位整数算术中进行来训练变压器模型的方案。为实现这一目标，作者在前向传播过程中通过他们提出的*Hadamard量化器*来处理激活中的重要异常值，该量化器对每个激活矩阵进行Hadamard矩阵变换的块对角化量化。此变换之所以有效，是因为它将异常值信息分散到相邻的矩阵条目中，从而减少了其数值范围，使矩阵更适合量化。在反向传播过程中，作者提出通过不计算激活中的小梯度行来节省计算，并利用节省下来的计算将最有信息量的梯度分为低4位和高4位，利用两行共同保留细节，创建一个8位表示。通过这种方法，作者实现了使用全4位算术的精确变压器训练，而无需像FP4这样的自定义数值格式，这意味着该方法可以在当代硬件上实现。作者展示了他们的INT4矩阵乘法操作比FP16训练快2.2倍，并减少了35.1%的总训练时间，同时在各种任务和变压器模型上接近基线性能。虽然这项研究集中于BERT和视觉变压器（ViT），而非大型语言模型（LLMs），但它在低精度训练大规模变压器模型领域标志着一个重要的里程碑，展示了这一技术的潜力。此外，毫无疑问，这种技术应用于LLMs只是时间问题。作者已在[GitHub](https://github.com/xijiu9/Train_Transformers_with_INT4)发布了他们的代码。

2023年，Tim Dettmers及其华盛顿大学的同事们通过[QLoRA](https://arxiv.org/abs/2305.14314)开创了新的里程碑，该技术使用高效的[低秩适应](https://arxiv.org/abs/2106.09685)微调方法，将冻结的4位量化大语言模型（LLM）中的梯度反向传播，通过可训练的低秩矩阵进行微调，只需在单个48GB GPU上训练24小时，即可微调一个65B模型，同时保持基线FP16性能。QLoRA大幅减少了微调LLM所需的硬件，为这一技术的普及奠定了重要基础。为实现这一目标，作者发现使用一种新型的4位NormalFloat（NF4）数据类型，其理论上最适合量化正态分布，相较于INT4或FP4类型，能够提供更好的实际效果。作者还探讨了*双重量化*的应用，在这种方法中，量化参数本身也被量化，平均每个参数节省0.37位（在65B模型中节省3GB内存）。为了突出其方法的高效性，作者对超过1000个模型进行了广泛的测试，使用了多个指令调优数据集。作者利用QLoRA方法训练了一系列[Guanaco](https://huggingface.co/timdettmers/guanaco-65b)模型。使用任何LoRA变体的一个显著优势是，预训练的模型保持不变，微调的内容完全体现在低秩适配器矩阵中。这意味着可以为多个应用场景训练多组适配器，而无需进行完整的模型重训。[通过Hugging Face Transformers进行QLoRA微调](https://huggingface.co/blog/4bit-transformers-bitsandbytes)，可以使用[bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index)库，且[原始QLoRA仓库](https://github.com/artidoro/qlora)已在GitHub上发布。

![](../Images/a82bae8da1a301165c03254db841cea9.png)

来自[QLoRA](https://arxiv.org/abs/2305.14314)论文的表格展示了，通过对4位量化模型应用QLoRA微调，量化造成的性能损失可以完全恢复。

## LLM的极限量化

最近，极限量化的魅力已进入LLM研究领域。正如我们在卷积神经网络（CNN）量化部分所看到的那样，神经网络在使用二进制或三进制权重时表现出惊人的效果，显著减少了内存流量和计算复杂度，因为权重占用的位数大大减少，乘法运算可以被加法和减法替代。虽然LLM的极限量化仍处于初期阶段，但最近的研究成果展示了令人信服的结果，必将激励研究界将这种方法发展成熟。在本节中，我们将看到低比特LLM训练的曙光。

在2023年，[BitNet](https://arxiv.org/abs/2310.11453)的作者提出了一种适用于LLM的1位变压器架构。他们引入了一个`BitLinear`层，可以替换`nn.Linear`层，并从头开始训练二进制权重，以实现与全精度基准相竞争的性能，发现得到的变压器也表现出与其全精度对手相似的扩展规律，同时使用的操作所需的功率要小得多。激活量化为8位，优化器状态和梯度仍然以全精度进行。作者承认，由于他们的实现是在FP16/BF16中执行操作，因此没有实际的训练加速，但他们提议可以使用低精度FP8 GEMM CUDA内核来加速前向和反向传播，尽管他们将此留待未来工作中进行。

![](../Images/4080589deca9dbe34c6ef2c99b76e2da.png)

图表显示了[BitNet](https://arxiv.org/abs/2310.11453)的性能与全精度变压器的性能扩展类似。请注意，左侧的图表展示了通过减少一个数量级的能耗，可以实现相同的性能。

2024年，BitNet的作者发布了[BitNet b1.58](https://arxiv.org/abs/2402.17764)，将他们的工作扩展到了使用三值{-1, 0, 1}权重，通过调整γ阈值参数，将小值限制为零，从而将零值引入到之前的1位BitNet系统中。正如我们在三值卷积神经网络（CNNs）中学到的那样，在权重中加入零值不会增加操作复杂度，因为乘法仍然可以通过加法/减法替代，但它显著提高了性能，使得权重能够执行“特征过滤”，虽然这意味着每个权重的位数从1位增加到2位。无论如何，由于权重更小，从DRAM到SRAM的传输更加容易，内存流量也减少了。作者使用对称量化来简化卷积核优化，将激活量化为8位，将KV缓存量化为4位。采用这种方法，作者从零开始训练三值网络，在困惑度和任务性能上都能与基准FP16性能匹配。

有关BitNet b1（双精度）和b1.58（三值）方法的代码和实现的详细信息，读者可以参考[训练提示、代码和常见问题解答PDF文件](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf)。然而，那里提供的代码并未详细说明推理时使用的低位核函数。幸运的是，开源社区已经开发了这个[GitHub仓库](https://github.com/kyegomez/BitNet/tree/main)，提供了完整的实现，包含自定义核函数和pip安装。需要注意的是，尽管BitNet在极端量化LLMs方面迈出了重要一步，但在这些实现中，二值化和三值化并未得到充分利用，因为训练过程中乘法仍然采用半精度，而加速推理的自定义核函数使用的是FP8精度。然而，我们之前了解到，使用二值/三值权重的最显著优势是，矩阵乘法可以用更节能的整数加法/减法运算替代，从而直接提高计算速度，但在这里，无论是训练还是推理，都未能探索这一点，留下了未来工作的空间。

![](../Images/aeb9879d5421194fb27a15e8cc766e90.png)

来自BitNet b1.58的结果显示，它与具有相同参数数量的FP16 LLaMA模型非常接近，甚至超越它们。请注意，他们将实验的参数上限设置为3.9B，可能是因为他们的训练过程未能在FP16基础上提高效率，因此成本非常高。

# 实践者的LLM量化指南

![](../Images/98327cf41a99daba4f1d0c1312af6009.png)

作者使用DALL-E 3生成的图像。

在本节中，我们将讨论量化的实际应用，以优化我们的工作流程。正如我们所看到的，量化可以在训练过程中应用，或者作为模型开发中的后处理步骤，选择哪种方式取决于模型和使用场景。为了成为量化工作流程的实践者，我们将首先回顾当前可用的工具集，以及它们的优缺点。然后，我们将根据所学的内容，制定一个决策树，帮助我们将目标与可用方法对齐。

一种流行且简便的尝试变换器开发和推理的方式是使用[Hugging Face Transformers](https://github.com/huggingface/transformers)库（以及他们的[模型库](https://huggingface.co/models)，该库类似于一个深度学习模型的GitHub仓库，允许你轻松推送模型检查点并将其拉取到其他机器上，只需几行代码。虽然这是一个非常适合能够操作Python代码的人的接口，但它没有提供用于模型的非代码用户界面（UI），而且虽然这个库可以作为一个强大的推理后端使用，但通常更优化的库提供更快的后端服务，正如我们将看到的那样。然而，在训练方面，[bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index)库的集成，通过Hugging Face的Transformers库的直观界面实现高效的量化训练，并支持[QLoRA](https://arxiv.org/abs/2305.14314)，使其成为一个非常强大的模型开发工具。

从2019年7月开始，Nvidia提供了[FasterTransformer](https://github.com/NVIDIA/FasterTransformer)库，用于优化变换器模型的推理吞吐量（最初专注于BERT），并用它支持TensorRT SDK以适配其设备。随后，该项目发展成了[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0)（也称为TRT-LLM），这是一个类似于PyTorch的Python API，支持GPTQ、AWQ以及[SmoothQuant](https://arxiv.org/abs/2211.10438)技术的实现。这些库专门设计用于优化Nvidia硬件或其[Triton推理服务器](https://developer.nvidia.com/triton-inference-server)上的推理，因此，对于使用这些硬件的用户来说，它非常有效，但它并不是一个通用的量化库。话虽如此，对于那些希望在Nvidia硬件上使用多GPU设置的用户来说，它可能是一个合适的选择。该库包括许多流行开源[模型](https://github.com/NVIDIA/TensorRT-LLM?tab=readme-ov-file#Models)的预构建版本，但用户也可以使用API量化自定义模型。

与此同时，[Georgi Gerganov](https://ggerganov.com/) 的机器学习库 ([GGML](https://github.com/ggerganov/ggml))，这是一个旨在加速 Apple 设备上 LLM 推理的纯 C 库，创建于 2022 年 9 月，并自那时以来稳步发展。GGML 使用量化来创建结构化的二进制模型文件，这些文件可以用于在各种硬件上执行优化的张量计算。虽然该库专为 Apple Silicon 量身定制，但现在也支持加速 x86 架构和 GPU 上的推理。GGML 库为广受欢迎的 llama.cpp 推理库提供后端支持，后者又为 [Ollama](https://ollama.com/) 和 [LM Studio](https://lmstudio.ai/) 等前端库提供后端支持。GGUF 是 GGML 库提供的全新改进文件格式。使用这些压缩格式的缺点是它们强制要求使用 llama.cpp 进行推理，这对于许多硬件架构，特别是非 Apple 设备来说并不理想。然而，llama.cpp 已增加对多 GPU/CUDA 硬件设置的支持，因此像 Ollama 这样用户友好的工具仍然非常有效，即使它们可能不是最快的。

[vLLM](https://github.com/vllm-project/vllm) 是一个强大的推理引擎，首次在 [Kwon 等，2023](https://arxiv.org/abs/2309.06180) 中提出，它使用优化的 CUDA 内核加速 Nvidia 和 AMD GPU 上的性能。作者指出，LLM 的逐个生成 token 的过程是受内存限制的，因此未充分利用 GPU 的计算能力。为了解决这个问题，他们在其提出的 *PagedAttention* 算法基础上构建了 vLLM 服务引擎，该算法提高了 KV 缓存的内存效率，以避免浪费空间，这对于增加给定硬件上的最大批处理大小至关重要。与 FasterTransformer 相比，吞吐量提高了 2–4 倍，且随着模型和序列规模的增大，这一优势更加明显。vLLM 无缝集成了 Hugging Face 模型，并支持多 GPU 工作流。该引擎与 GPTQ、AWQ、SqueezeLLM 和 FP8 KV 缓存量化兼容，允许通过这些方法量化的模型受益于 PagedAttention 的速度提升。SmoothQuant+ 集成即将推出。

[MLC-LLM](https://github.com/mlc-ai/mlc-llm) 是一个强大且通用的部署解决方案，用于在多种硬件上优化量化模型的本地部署，包括 Apple、Nvidia、AMD、Intel 和移动设备。根据目前的情况，它似乎是最快且最通用的服务引擎，且因其卓越的吞吐量而受到 [Jetson AI Lab](https://www.jetson-ai-lab.com/) 的青睐。MLC-LLM 库提供了一 [组预构建的模型](https://llm.mlc.ai/docs/prebuilt_models.html)，以及[编译新模型](https://llm.mlc.ai/docs/compilation/compile_models.html)的选项，以便与该库一起使用。

![](../Images/707119d410ddf78b53001650292ab9d9.png)

MLC-LLM的结果显示，在两块NVIDIA RTX 4090上，4位CodeLlama-34B和Llama2–70B的吞吐量非常高。我们可以看到，任何专用推理引擎都能比使用HF Transformers提供显著的性能提升，但对于这些NVIDIA显卡，MLC-LLM始终优于其他引擎。

## LLM量化决策树

在本文中，我们涵盖了很多方法论，这些方法论为量化设计提供了一个令人眼花缭乱的选择列表。幸运的是，一些开源推理引擎已经以一些直观的形式将这些最有用的工具提供给我们。这棵决策树可以作为一个非常基本的经验法则，根据预期的使用场景和部署环境来使用，但它并不是一个详尽无遗的考虑因素列表。它旨在为从业者提供实施量化的起点。

+   **问：你是否在CPU或边缘设备上部署了预训练模型？**

    答：是的 — 对于苹果用户，可以选择基于GGML的推理（[llama.cpp](https://github.com/ggerganov/llama.cpp)，[Ollama](https://ollama.com/)，[LM Studio](https://lmstudio.ai/)）。对于安卓和x86硬件，使用[MLC-LLM](https://github.com/mlc-ai/mlc-llm)。

    否 — 请转到下一个问题。

+   **问：你会同时处理批量请求吗？** 答：是的 — 使用[vLLM](https://github.com/vllm-project/vllm)，它专门为此进行了优化。

    否 — 请转到下一个问题。

+   **问：你是否在GPU上部署了预训练模型？**

    答：是的 — MLC-LLM在Nvidia和AMD GPU上似乎提供了最先进的吞吐量，虽然它也支持苹果GPU，但llama.cpp在优化苹果硬件方面有优势，因此值得进行比较。对于Nvidia和AMD GPU，vLLM库中即将集成的SmoothQuant+将在发布时值得一试。

    否 — 请转到下一个问题。

+   **问：你正在微调一个量化的LLM吗？**

    答：是的 — 使用[QLoRA](https://github.com/artidoro/qlora)。这是使量化的LLM执行新任务或恢复由于量化丢失的性能的最有效方式。[它可以轻松应用于Hugging Face Transformers](https://huggingface.co/blog/4bit-transformers-bitsandbytes)。

    否 — 请转到下一个问题。

+   **问：你正在训练一个基础模型吗？**

    答：是的 — 尝试使用[BitNet](https://github.com/kyegomez/BitNet)来原型化或开发基础模型，因为它的训练成本要低得多，并且提供与全精度基准相竞争的性能。更好的是，尝试将这些低位模型进行集成。

这个清单应该为普通用户提供一个可靠的指南，帮助他们根据自己的情况找到适合的方案。若想获得更全面的LLM推理框架指南，可以参考[Sergei Savvov的这篇文章](https://betterprogramming.pub/frameworks-for-serving-llms-60b7f7b23407)。

# 结论

在本文中，我们见证了一些量化研究中的令人难以置信的成就。我们看到无损8位量化逐渐成为一个平凡的基准，二进制卷积神经网络（CNN）几乎可以与其全精度模型相媲美，训练在4位精度下的大型变换器模型，以及在大规模语言模型（LLM）中极端量化的兴起。我们发现INT8的后训练量化（PTQ）已经完全成熟，能够轻松与全精度基准对齐，并广泛集成到大多数开源库中，因此可以说，没有人应该使用大于8位的浮动点数据类型来服务模型。在大规模语言模型的案例中，这些模型通常以FP16格式发布，而无损INT8量化将内存占用减少一半，并提供了“免费午餐”般的压缩效果。此外，低精度量化已经在最小的性能损失下实现，对于许多实际应用场景，适度的精度下降换来加速和无缝的用户体验，将是一个受欢迎的权衡。我们在BENN论文中看到的极低位网络集成的理念非常有吸引力，特别是如今我们看到LLM中极端量化的崛起，未来这种方法可能会受益于这种方法。

我们看到了一系列量化方法，满足不同应用场景的需求，具有各种复杂性，并见证了它们随时间的成长，揭示了哪些方法在开源社区中得到了普及。尽管PTQ和ZSQ已经变得非常有效，QAT仍然是从根本上最具性能的方法，因为它将量化融入了网络训练过程。虽然这种方法资源消耗较大，并且似乎排除了普通从业者在大规模语言模型上使用它，但QLoRA的巧妙设计使我们能够模糊PTQ和QAT之间的界限，通过仅训练低秩矩阵中的少量参数来恢复量化带来的损失。我们看到，PTQ最近已经通过HQQ方法有效地实现了2位精度量化，并且我们还看到二进制和三进制LLM可以从头开始训练，几乎达到与全精度模型相媲美的性能。

恭喜你坚持到了这一点。在本文中，我们已经了解了所讨论的一些工作，现在你已经掌握了神经网络量化的专业知识。这项调查并不适合胆小的人，但也没有必要对2024年的量化方法做一个肤浅的总结。为了进行正确的机器学习实践，我们需要一个全面的知识基础，以便做出明智的决策，权衡所有潜在的选项以及我们所选方法的利弊。神经网络的量化是一个极具活力的研究领域，建立在丰富的遗产之上，给那些希望实现全面理解的研究者带来了巨大的挑战。希望本文能为该领域的从业人员提供一个可访问、独立且全面的资源，并为他们的常见应用场景提供一些有效的工具选择建议。

## 未来工作

BitNet 在训练过程中不使用低精度计算，也没有充分利用其权重的二值化或三值化，因为它没有用加法/减法操作替代乘法运算。这些因素限制了其实现真正价值的能力。极端量化卷积神经网络（CNNs）领域的遗产应该应用于对实际数学进行二值化，并使 BitNet 使用的操作更加高效。此外，一旦为 BitNet 建立了优化的内核，我们应该开始研究这些低位 LLMs 的集成，因为我们在 BENN 论文中看到这在 CNNs 中非常有效。

HQQ 是一种非常高效的准确的无数据（零样本）PTQ 算法，但迄今为止，仅有一个优化的推理引擎（vLLM）开始为运行这些模型提供实验性支持。此外，目前没有关于使用 HQQ 量化的模型与其他方法相比的推理吞吐量基准。这两项内容是未来工作的开放领域。
