# ä¼˜åŒ–åœ¨è®¾è®¡æ¶‰åŠå°æ ·æœ¬å®éªŒä¸­çš„åŠ›é‡

> åŸæ–‡ï¼š[https://towardsdatascience.com/the-power-of-optimization-in-designing-experiments-involving-small-samples-de87f9783d3b?source=collection_archive---------6-----------------------#2024-10-21](https://towardsdatascience.com/the-power-of-optimization-in-designing-experiments-involving-small-samples-de87f9783d3b?source=collection_archive---------6-----------------------#2024-10-21)

## ä½¿ç”¨Pythonä¸­çš„ä¼˜åŒ–æŠ€æœ¯è®¾è®¡æ›´ç²¾ç¡®å®éªŒçš„é€æ­¥æŒ‡å—

[](https://medium.com/@leandro.magga?source=post_page---byline--de87f9783d3b--------------------------------)[![Leandro Magga](../Images/75d2b6b31635ac2bd409bfb91d151ac4.png)](https://medium.com/@leandro.magga?source=post_page---byline--de87f9783d3b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--de87f9783d3b--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--de87f9783d3b--------------------------------) [Leandro Magga](https://medium.com/@leandro.magga?source=post_page---byline--de87f9783d3b--------------------------------)

Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--de87f9783d3b--------------------------------) Â·10åˆ†é’Ÿé˜…è¯»Â·2024å¹´10æœˆ21æ—¥

--

å®éªŒæ˜¯ç”¨ç§‘å­¦ä¸¥è°¨çš„æ–¹æ³•æ£€éªŒå‡è®¾çš„åŸºç¡€ã€‚åœ¨åŒ»å­¦é¢†åŸŸï¼Œå®éªŒç”¨äºè¯„ä¼°æ–°ç–—æ³•å¯¹æ‚£è€…çš„æ•ˆæœï¼›è€Œåœ¨æ•°å­—ä¸–ç•Œä¸­ï¼Œåƒ[Amazon](https://www.amazon.jobs/en/teams/aeo)ã€[Netflix](https://netflixtechblog.com/experimentation-is-a-major-focus-of-data-science-across-netflix-f67923f8e985)å’Œ[Uber](https://www.uber.com/en-CL/blog/xp/)è¿™æ ·çš„ç§‘æŠ€å·¨å¤´æ¯å¹´éƒ½ä¼šè¿›è¡Œæˆåƒä¸Šä¸‡æ¬¡çš„å®éªŒï¼Œä»¥ä¼˜åŒ–å’Œæ”¹è¿›ä»–ä»¬çš„å¹³å°ã€‚

å¯¹äºå¤§è§„æ¨¡å®éªŒï¼Œé€šå¸¸ä½¿ç”¨éšæœºåˆ†é…ï¼Œå¹¶ä¸”è¢«è®¤ä¸ºæ˜¯â€œé»„é‡‘æ ‡å‡†â€ã€‚åœ¨æ•°æ®é‡å……è¶³çš„æƒ…å†µä¸‹ï¼Œéšæœºæ€§å¾€å¾€èƒ½å¤Ÿç”Ÿæˆå¯æ¯”è¾ƒçš„ç»„ï¼Œå…¶ä¸­é‡è¦çš„å®éªŒå‰å› ç´ è¢«å¹³è¡¡ï¼Œå¹¶ä¸”äº¤æ¢æ€§å‡è®¾å¾—ä»¥æˆç«‹ã€‚

ç„¶è€Œï¼Œå½“å®éªŒçš„æ ·æœ¬éå¸¸å°çš„æ—¶å€™ï¼Œéšæœºåˆ†é…é€šå¸¸æ— æ³•åˆ›å»ºç»Ÿè®¡ä¸Šç­‰æ•ˆçš„ç»„ã€‚å› æ­¤ï¼Œ**æˆ‘ä»¬å¦‚ä½•é«˜æ•ˆåœ°åœ¨å¤„ç†ç»„å’Œæ§åˆ¶ç»„ä¹‹é—´åˆ†é…å•å…ƒå‘¢ï¼Ÿ**

## ä½ å°†å­¦åˆ°çš„å†…å®¹ï¼š

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†è§£é‡Šä¸€ç§åŸºäºä¼˜åŒ–çš„æ–¹æ³•ï¼Œç”¨äºæ„å»ºç­‰æ•ˆçš„å®éªŒç»„ï¼Œè¿™æ˜¯Bertsimasç­‰äººåœ¨[è¿™ç¯‡æ–‡ç« ](https://medium.com/r?url=https%3A%2F%2Fpubsonline.informs.org%2Fdoi%2F10.1287%2Fopre.2015.1361)ä¸­æå‡ºçš„ã€‚

é€šè¿‡Pythonä¸­çš„ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ï¼š

+   è®¾è®¡ä¸€ä¸ªåŸºäºä¼˜åŒ–çš„å®éªŒã€‚

+   ä½¿ç”¨è‡ªåŠ©æ³•æŠ€æœ¯è¿›è¡Œæ¨æ–­ã€‚

+   åœ¨Pythonä¸­å®ç°ä»£ç ï¼Œä»¥ä¾¿ä½ è‡ªå·±çš„å®éªŒä½¿ç”¨

# ä¼˜åŒ–çš„åŠ›é‡

åœ¨æ·±å…¥ç ”ç©¶æˆ‘ä»¬çš„ä¾‹å­å’ŒPythonä»£ç ä¹‹å‰ï¼Œé¦–å…ˆè®¨è®ºä¸€ä¸‹ä½¿ç”¨åŸºäºä¼˜åŒ–çš„æ–¹æ³•æ¥è®¾è®¡å®éªŒçš„å¥½å¤„ã€‚

> ä¼˜åŒ–ä½¿å¾—ç»Ÿè®¡ç»“æœæ›´åŠ ç²¾ç¡®ï¼Œä»è€Œå¯ä»¥è¿›è¡Œæ›´å¼ºæœ‰åŠ›çš„æ¨æ–­ã€‚

åŸºäºä¼˜åŒ–çš„æ–¹æ³•**å°†å®éªŒç»„è¿›è¡ŒåŒ¹é…ï¼Œä»¥æœ€å°åŒ–æ•´ä½“å‡å€¼å’Œæ–¹å·®çš„å·®å¼‚**ã€‚è¿™æ ·å¯ä»¥ä½¿ç»Ÿè®¡ç»“æœæ›´åŠ ç²¾ç¡®ï¼Œå°†å…¶ç´§å¯†é›†ä¸­åœ¨åä¹‰å€¼å‘¨å›´ï¼ŒåŒæ—¶ä»ç„¶ä¿æŒæ— åçš„ä¼°è®¡ã€‚è¿™ç§ç²¾åº¦çš„æé«˜ä½¿å¾—æ¨æ–­èƒ½åŠ›æ›´å¼ºï¼ˆæˆ‘ä»¬ç¨åä¼šä»‹ç»ä½¿ç”¨çš„è‡ªåŠ©æ³•ç®—æ³•ï¼‰ã€‚

è¿™ä½¿å¾—ç ”ç©¶äººå‘˜èƒ½å¤Ÿé€šè¿‡æ›´å°‘çš„æ•°æ®å¾—å‡ºç»Ÿè®¡ä¸Šæœ‰æ•ˆçš„ç»“è®ºï¼Œä»è€Œå‡å°‘å®éªŒæˆæœ¬â€”â€”è¿™æ˜¯è‚¿ç˜¤å­¦ç ”ç©¶ç­‰å­¦ç§‘ä¸­çš„ä¸€ä¸ªé‡è¦ä¼˜åŠ¿ï¼Œå› ä¸ºåœ¨å°é¼ ç™Œç—‡æ¨¡å‹ä¸­æµ‹è¯•åŒ–ç–—è¯ç‰©æ—¢ç¹çåˆæ˜‚è´µã€‚æ­¤å¤–ï¼Œä¸å…¶ä»–é€‚ç”¨äºå°æ ·æœ¬é‡çš„æ–¹æ³•ç›¸æ¯”ï¼Œä¼˜åŒ–æ–¹æ³•å·²ç»è¯æ˜åœ¨æ•ˆæœä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæ­£å¦‚æˆ‘ä»¬ç¨åé€šè¿‡æ¨¡æ‹Ÿå®éªŒçœ‹åˆ°çš„é‚£æ ·ã€‚

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ·±å…¥ä¸€ä¸ªä¾‹å­ï¼Œçœ‹çœ‹å¦‚ä½•ä½¿ç”¨Pythonåº”ç”¨è¿™ç§æ–¹æ³•ï¼

# åœ¨Pythonä¸­å®ç°ç®—æ³•

## æ¡ˆä¾‹ç ”ç©¶ï¼š20åªå°é¼ çš„è¯ç‰©å®éªŒ

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª20åªå°é¼ çš„å®éªŒï¼Œç ”ç©¶è¯ç‰©å¯¹è‚¿ç˜¤ç”Ÿé•¿çš„å½±å“ï¼Œè‚¿ç˜¤å¤§å°åˆå§‹å€¼ä¸åŒã€‚å‡è®¾åˆå§‹è‚¿ç˜¤å¤§å°å‘ˆæ­£æ€åˆ†å¸ƒï¼Œå‡å€¼ä¸º200 mgï¼Œæ ‡å‡†å·®ä¸º300 mgï¼ˆä¸ºäº†ç¡®ä¿å€¼ä¸ºéè´Ÿæ•°è€Œè¿›è¡Œäº†æˆªæ–­ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹Pythonä»£ç ç”Ÿæˆå°é¼ ç§ç¾¤ï¼š

```py
import numpy as np
import pandas as pd
import scipy.stats as stats

def create_experiment_data(n_mice, mu, sigma, seed):
      lower, upper = 0, np.inf
      initial_weights = stats.truncnorm(
          (lower - mu) / sigma,
          (upper - mu) / sigma,
          loc=mu,
          scale=sigma
      ).rvs(size=n_mice, random_state=seed)
      return pd.DataFrame({
          'mice': list(range(1, n_mice+1)),
          'initial_weight': initial_weights,
      })

tumor_data = create_experiment_data(n_mice=20, mu=200, sigma=300, seed=123)
print(tumor_data.head())
```

```py
>    mice  initial_weight
0     1      424.736888
1     2      174.691035
2     3      141.016478
3     4      327.518749
4     5      442.239789
```

ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦å°†20åªå•®é½¿åŠ¨ç‰©åˆ†æˆä¸¤ç»„ï¼Œæ¯ç»„10åªâ€”â€”ä¸€ç»„å°†æ¥å—æ²»ç–—ï¼Œå¦ä¸€ç»„å°†æ¥å—å®‰æ…°å‰‚ã€‚æˆ‘ä»¬å°†é€šè¿‡ä¼˜åŒ–æ¥å®Œæˆè¿™ä¸€ä»»åŠ¡ã€‚

æˆ‘ä»¬è¿˜å‡è®¾è‚¿ç˜¤ç”Ÿé•¿æ˜¯åœ¨ä¸€å¤©å†…è§‚å¯Ÿçš„ï¼Œå¹¶ä¸”éµå¾ªGompertzæ¨¡å‹ï¼ˆè¯¦è§[*ã€Šç™Œç—‡ç ”ç©¶ä¸­çš„æ•°å­¦æ¨¡å‹ã€‹*](https://medium.com/r?url=https%3A%2F%2Fwww.semanticscholar.org%2Fpaper%2FMathematical-models-in-cancer-research-Wheldon%2F9e3176627ff018f369b87b1aa8c97df203020f76)ï¼‰ã€‚å‡è®¾æ²»ç–—å…·æœ‰ç¡®å®šæ€§æ•ˆæœï¼Œå¯ä»¥å‡å°‘è‚¿ç˜¤å¤§å°250 mgã€‚

## å®éªŒè®¾è®¡

æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†ç­‰æ•ˆç»„çš„åˆ›å»ºè¡¨è¿°ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œå…¶ä¸­ç›®æ ‡æ˜¯æœ€å°åŒ–åˆå§‹è‚¿ç˜¤é‡é‡çš„å‡å€¼å’Œæ–¹å·®å·®å¼‚ã€‚

ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬éœ€è¦éµå¾ªä¸‰ä¸ªæ­¥éª¤ï¼š

## ç¬¬1æ­¥ï¼šè§„èŒƒåŒ–åˆå§‹è‚¿ç˜¤é‡é‡

é¦–å…ˆï¼Œæ•´ä¸ªæ ·æœ¬å¿…é¡»è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ä¸”åº¦é‡æ ‡å‡†åº”è¯¥è§„èŒƒåŒ–ï¼Œä½¿å…¶å‡å€¼ä¸ºé›¶ï¼Œæ–¹å·®ä¸ºå•ä½ï¼š

```py
mean = tumor_data['initial_weight'].mean()
std = tumor_data['initial_weight'].std()

tumor_data['norm_initial_weight'] = (tumor_data['initial_weight'] - mean) / std
```

## ç¬¬2æ­¥ï¼šä½¿ç”¨ä¼˜åŒ–åˆ›å»ºç»„

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å®ç°ä¸€ä¸ªé€šç”¨çš„ä¼˜åŒ–æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ„å»ºå…·æœ‰*kä¸ªå—è¯•è€…*çš„*mç»„*ï¼Œå¹¶æœ€å°åŒ–*ä»»æ„ä¸¤ç»„ä¹‹é—´çš„æœ€å¤§å·®å¼‚*ï¼ˆæœ‰å…³æ¨¡å‹å˜é‡çš„å®Œæ•´æè¿°ï¼Œè¯·å‚è§[æ–‡ç« ](https://pubsonline.informs.org/doi/10.1287/opre.2015.1361)ï¼‰ï¼Œå¹¶ä¼ é€’å½’ä¸€åŒ–åº¦é‡ï¼š

![](../Images/73f213fb1e811cde82574a44e4a94e8a.png)

åˆ›å»ºæ¯ä¸ªå«æœ‰kä¸ªå•ä½çš„mç»„çš„ä¼˜åŒ–æ¨¡å‹ï¼ˆæ¥è‡ª[Bertsimas et al. 2015](https://pubsonline.informs.org/doi/10.1287/opre.2015.1361)ï¼‰ã€‚

è¯¥æ•°å­¦æ¨¡å‹å¯ä»¥ä½¿ç”¨Pythonä¸­çš„ortoolsåº“å’ŒSCIPæ±‚è§£å™¨å®ç°ï¼Œå…·ä½“å¦‚ä¸‹ï¼š

```py
from ortools.linear_solver import pywraplp
from typing import Union

class SingleFeatureOptimizationModel:
    """
    Implements the discrete optimization model proposed by Bertsimas et al. (2015) in "The Power of 
    Optimization Over Randomization in Designing Experiments Involving Small Samples". 
    See: https://doi.org/10.1287/opre.2015.1361.
    """

    def __init__(self, data: pd.DataFrame, n_groups: int, units_per_group: int, metric: str, unit: str):
        self.data = data.reset_index(drop=True)
        self.parameters = {
            'rho': 0.5,
            'groups': range(n_groups),
            'units': range(len(self.data)),
            'unit_list': self.data[unit].tolist(),
            'metric_list': self.data[metric].tolist(),
            'units_per_group': units_per_group,
        }
        self._create_solver()
        self._add_vars()
        self._add_constraints()
        self._set_objective()

    def _create_solver(self):
        self.model = pywraplp.Solver.CreateSolver("SCIP")
        if self.model is None:
            raise Exception("Failed to create SCIP solver")

    def _add_vars(self):
        self.d = self.model.NumVar(0, self.model.infinity(), "d")
        self.x = {}
        for i in self.parameters['units']:
            for p in self.parameters['groups']:
                self.x[i, p] = self.model.IntVar(0, 1, "")

    def _set_objective(self):
        self.model.Minimize(self.d)

    def _add_constraints(self):
        self._add_constraints_d_bounding()
        self._add_constraint_group_size()
        self._add_constraint_all_units_assigned()

    def _add_constraints_d_bounding(self):
        rho = self.parameters['rho']
        for p in self.parameters['groups']:
            for q in self.parameters['groups']:
                if p < q:
                    self.model.Add(self.d >= self._mu(p) - self._mu(q) + rho * self._var(p) - rho * self._var(q))
                    self.model.Add(self.d >= self._mu(p) - self._mu(q) + rho * self._var(q) - rho * self._var(p))
                    self.model.Add(self.d >= self._mu(q) - self._mu(p) + rho * self._var(p) - rho * self._var(q))
                    self.model.Add(self.d >= self._mu(q) - self._mu(p) + rho * self._var(q) - rho * self._var(p))

    def _add_constraint_group_size(self):
        for p in self.parameters['groups']:
            self.model.Add(
                self.model.Sum([
                    self.x[i,p] for i in self.parameters['units']
                ]) == self.parameters['units_per_group']
                )

    def _add_constraint_all_units_assigned(self):
        for i in self.parameters['units']:
            self.model.Add(
                self.model.Sum([
                    self.x[i,p] for p in self.parameters['groups']
                ]) == 1
                )

    def _add_contraint_symmetry(self):
        for i in self.parameters['units']:
            for p in self.parameters['units']: 
                if i < p:
                    self.model.Add(
                        self.x[i,p] == 0 
                        )

    def _mu(self, p):
        mu = self.model.Sum([
            (self.x[i,p] * self.parameters['metric_list'][i]) / self.parameters['units_per_group']
            for i in self.parameters['units']
            ])
        return mu

    def _var(self, p):
        var = self.model.Sum([
            (self.x[i,p]*(self.parameters['metric_list'][i])**2) / self.parameters['units_per_group']
            for i in self.parameters['units']
            ])
        return var

    def optimize(
            self,
            max_run_time: int = 60,
            max_solution_gap: float = 0.05,
            max_solutions: Union[int, None] = None,
            num_threads: int = -1,
            verbose: bool = False
    ):
        """
        Runs the optimization model.

        Args:
            max_run_time: int
                Maximum run time in minutes.
            max_solution_gap: float
                Maximum gap with the LP relaxation solution.
            max_solutions: int
                Maximum number of solutions until stop.
            num_threads: int
                Number of threads to use in solver.
            verbose: bool
                Whether to set the solver output.

        Returns: str
            The status of the solution.
        """
        self.model.SetTimeLimit(max_run_time * 60 * 1000)
        self.model.SetNumThreads(num_threads)

        if verbose:
            self.model.EnableOutput()

        self.model.SetSolverSpecificParametersAsString(f"limits/gap = {max_solution_gap}")
        self.model.SetSolverSpecificParametersAsString(f"limits/time = {max_run_time * 60}")

        if max_solutions:
            self.model.SetSolverSpecificParametersAsString(f"limits/solutions = {max_solutions}")

        status = self.model.Solve()

        if verbose:
            if status == pywraplp.Solver.OPTIMAL:
                print("Optimal Solution Found.")
            elif status == pywraplp.Solver.FEASIBLE:
                print("Feasible Solution Found.")
            else:
                print("Problem infeasible or unbounded.")

        self._extract_solution()
        return status

    def _extract_solution(self):
        tol = 0.01
        self.assignment = {}
        for i in self.parameters['units']:
            for p in self.parameters['groups']:
                if self.x[i,p].solution_value() > tol:
                    self.assignment.setdefault(p, []).append(self.parameters['unit_list'][i])

    def get_groups_list(self):
        return list(self.assignment.values())
```

```py
model = SingleFeatureOptimizationModel(
    data = tumor_data,
    n_groups = 2,
    units_per_group = 10,
    unit = 'mice',
    metric = 'norm_initial_weight',

)

status = model.optimize()
optimized_groups = model.get_groups_list()
print(f"The optimized mice groups are: {optimized_groups}")
```

```py
> The optimized mice groups are: [
   [1, 4, 5, 6, 8, 12, 14, 16, 17, 18], 
   [2, 3, 7, 9, 10, 11, 13, 15, 19, 20]
  ]
```

> ***æ³¨æ„ï¼š*** *å‚æ•°* rho *æ§åˆ¶æœ€å°åŒ–ä¸€é˜¶çŸ©å’ŒäºŒé˜¶çŸ©ä¹‹é—´å·®å¼‚çš„æŠ˜è¡·ï¼Œç”±ç ”ç©¶äººå‘˜é€‰æ‹©ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å‡è®¾* rho *ç­‰äº0.5ã€‚*

## Paso 3ï¼šéšæœºåŒ–åˆ†é…å“ªç»„æ¥å—å“ªç§æ²»ç–—

æœ€åï¼Œæˆ‘ä»¬éšæœºå†³å®šå“ªä¸€ç»„å®éªŒå°é¼ å°†æ¥å—è¯ç‰©ï¼Œå“ªä¸€ç»„å°†æ¥å—å®‰æ…°å‰‚ï¼š

```py
import random

def random_shuffle_groups(group_list, seed):
  random.seed(seed)
  random.shuffle(group_list)
  return group_list

randomized_groups = random_shuffle_groups(optimized_groups, seed=123)
treatment_labels = ["Placebo", "Treatment"]
treatment_dict = {treatment_labels[i]: randomized_groups[i] for i in range(len(randomized_groups))}
print(f"The treatment assignment is: {treatment_dict}")
```

```py
> The treatment assignment is: {
   'Placebo': [2, 3, 7, 9, 10, 11, 13, 15, 19, 20], 
   'Treatment': [1, 4, 5, 6, 8, 12, 14, 16, 17, 18]
}
```

è®©æˆ‘ä»¬é€šè¿‡åˆ†æä¸¤ç»„åˆå§‹è‚¿ç˜¤æƒé‡çš„å‡å€¼å’Œæ–¹å·®æ¥æŸ¥çœ‹ç»“æœçš„è´¨é‡ï¼š

```py
mice_assignment_dict = {inx: gp for gp, indices in treatment_dict.items() for inx in indices}
tumor_data['treatment'] = tumor_data['mice'].map(mice_assignment_dict)

print(tumor_data.groupby('treatment').agg(
    avg_initial_weight = ('initial_weight', 'mean'),
    std_initial_weight = ('initial_weight', 'std'),
).round(2))
```

```py
>          avg_initial_weight  std_initial_weight
treatment                                        
Placebo                302.79              202.54
Treatment              303.61              162.12
```

**è¿™æ˜¯é¢„å®éªŒè‚¿ç˜¤æƒé‡å‡å€¼å’Œæ–¹å·®æœ€å°çš„ç»„åˆ†é…**ï¼ç°åœ¨è®©æˆ‘ä»¬è¿›è¡Œå®éªŒå¹¶åˆ†æç»“æœã€‚

# ä½¿ç”¨è‡ªåŠ©æ³•è¿›è¡Œæ¨æ–­

## **æ¨¡æ‹Ÿè‚¿ç˜¤ç”Ÿé•¿**

æ ¹æ®å·²ç¡®å®šçš„æ²»ç–—åˆ†é…ï¼Œå‡è®¾æ²»ç–—ç»„çš„æ•ˆæœä¸º-250æ¯«å…‹ï¼Œè‚¿ç˜¤ç”Ÿé•¿åœ¨ä¸€å¤©å†…ä½¿ç”¨Gompertzæ¨¡å‹è¿›è¡Œæ¨¡æ‹Ÿï¼š

```py
import numpy as np
from scipy.integrate import odeint

# Gompertz model parameters
a = 1 
b = 5

# Critical weight
wc = 400

def gompex_growth(w, t):
    """
    Gomp-ex differential equation model based on the initial weight.
    """
    growth_rate = a + max(0, b * np.log(wc / w))
    return w * growth_rate

def simulate_growth(w_initial, t_span):
    """
    Simulate the tumor growth using the Gomp-ex model.
    """
    return odeint(gompex_growth, w_initial, t_span).flatten()

def simulate_tumor_growth(data: pd.DataFrame, initial_weight: str, treatment_col: str, treatment_value: str, treatment_effect: float):
    """
    Simulate the tumor growth experiment and return the dataset.
    """
    t_span = np.linspace(0, 1, 2)
    final_weights = np.array([simulate_growth(w, t_span)[-1] for w in data[initial_weight]])

    experiment_data = data.copy()
    mask_treatment = data[treatment_col] == treatment_value
    experiment_data['final_weight'] = np.where(mask_treatment, final_weights + treatment_effect, final_weights)

    return experiment_data.round(2)

experiment_data = simulate_tumor_growth(
    data = tumor_data, 
    initial_weight = 'initial_weight', 
    treatment_col = 'treatment', 
    treatment_value = 'Treatment', 
    treatment_effect = -250
)

print(experiment_data.head())
```

```py
>   mice  initial_weight  norm_initial_weight  treatment  final_weight
0     1          424.74                 0.68  Treatment        904.55
1     2          174.69                -0.72    Placebo        783.65
2     3          141.02                -0.91    Placebo        754.56
3     4          327.52                 0.14  Treatment        696.60
4     5          442.24                 0.78  Treatment        952.13
```

```py
mask_tr = experiment_data.group == 'Treatment'
mean_tr = experiment_data[mask_tr]['final_weight'].mean()
mean_co = experiment_data[~mask_tr]['final_weight'].mean()
print(f"Mean difference between treatment and control: {round(mean_tr - mean_co)} mg")
```

```py
> Mean difference between treatment and control: -260 mg
```

ç°åœ¨æˆ‘ä»¬å·²ç»å¾—åˆ°äº†æœ€ç»ˆçš„è‚¿ç˜¤æƒé‡ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ²»ç–—ç»„çš„å¹³å‡æœ€ç»ˆè‚¿ç˜¤é‡é‡æ¯”å¯¹ç…§ç»„ä½260æ¯«å…‹ã€‚ç„¶è€Œï¼Œä¸ºäº†ç¡®å®šè¿™ä¸ªå·®å¼‚æ˜¯å¦å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ï¼Œæˆ‘ä»¬éœ€è¦åº”ç”¨ä»¥ä¸‹è‡ªåŠ©æ³•æœºåˆ¶æ¥è®¡ç®—på€¼ã€‚

## **åŸºäºä¼˜åŒ–è®¾è®¡çš„è‡ªåŠ©æ³•æ¨æ–­**

> â€œåœ¨åŸºäºä¼˜åŒ–çš„è®¾è®¡ä¸­ï¼Œè¯¸å¦‚ç»„é—´å¹³å‡å·®å¼‚ä¹‹ç±»çš„ç»Ÿè®¡æ•°æ®å˜å¾—æ›´åŠ ç²¾ç¡®ï¼Œä½†ä¸å†éµå¾ªå¸¸è§„åˆ†å¸ƒã€‚å› æ­¤ï¼Œåº”ä½¿ç”¨è‡ªåŠ©æ³•æ¨æ–­æ–¹æ³•æ¥å¾—å‡ºæœ‰æ•ˆç»“è®ºã€‚â€

[Bertsimas et al. (2015)](https://medium.com/r?url=https%3A%2F%2Fpubsonline.informs.org%2Fdoi%2F10.1287%2Fopre.2015.1361) æå‡ºçš„è‡ªåŠ©æ³•æ¨æ–­æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨å¸¦æ”¾å›çš„æŠ½æ ·æ„å»ºä¼°è®¡é‡çš„åŸºå‡†åˆ†å¸ƒã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œä½¿ç”¨ä¼˜åŒ–è¿›è¡Œç»„åˆ†é…ï¼Œæœ€åå¾—å‡º*på€¼*ï¼Œå…·ä½“å¦‚ä¸‹ï¼š

```py
from tqdm import tqdm
from typing import Any, List

def inference(data: pd.DataFrame, unit: str, outcome: str, treatment: str, treatment_value: Any = 1, n_bootstrap: int = 1000) -> pd.DataFrame:
    """
    Estimates the p-value using bootstrap for two groups.

    Parameters
    -----------
    data (pd.DataFrame): The experimental dataset with the observed outcome.
    unit (str): The experimental unit column.
    outcome (str): The outcome metric column.
    treatment (str): The treatment column.
    treatment_value (Any): The value referencing the treatment (other will be considered as control).
    n_bootstrap (int): The number of random draws with replacement to use.

    Returns
    -----------
    pd.DataFrame: The dataset with the results.

    Raise
    ------------
    ValueException: if there are more than two treatment values.
    """
    responses = data[outcome].values
    mask_tr = (data[treatment] == treatment_value).values
    delta_obs = _compute_delta(responses[mask_tr], responses[~mask_tr])
    deltas_B = _run_bootstrap(data, unit, outcome, n_bootstrap)
    pvalue = _compute_pvalue(delta_obs, deltas_B)
    output_data = pd.DataFrame({
        'delta': [delta_obs],
        'pvalue': [pvalue],
        'n_bootstrap': [n_bootstrap],
        'avg_delta_bootstrap': [np.round(np.mean(deltas_B), 2)],
        'std_delta_bootstrap': [np.round(np.std(deltas_B), 2)]
    })
    return output_data

def _run_bootstrap(data: pd.DataFrame, unit: str, outcome: str, B: int = 1000) -> List[float]:
    """
    Runs the bootstrap method and returns the bootstrapped deltas.

    Parameters
    -----------
    data (pd.DataFrame): The dataframe from which sample with replacement.
    outcome (str): The outcome metric observed in the experiment.
    B (int): The number of random draws with replacement to perfrom.

    Returns
    -----------
    List[float]: The list of bootstrap deltas.
    """
    deltas_bootstrap = []
    for i in tqdm(range(B), desc="Bootstrap Progress"):
        sample_b = _random_draw_with_replacement(data, unit)
        responses_b, mask_tr_b = _optimal_treatment_control_split(sample_b, unit, outcome, seed=i)
        delta_b = _compute_delta(responses_b[mask_tr_b], responses_b[~mask_tr_b])
        deltas_bootstrap.append(delta_b)

    return deltas_bootstrap

def _compute_delta(response_tr, responses_co):
    delta = np.mean(response_tr) - np.mean(responses_co)
    return delta

def _compute_pvalue(obs_delta, bootstrap_deltas):
    count_extreme = sum(1 for delta_b in bootstrap_deltas if abs(delta_b) >= abs(obs_delta))
    p_value = (1 + count_extreme) / (1 + len(bootstrap_deltas))
    return p_value

def _random_draw_with_replacement(data: pd.DataFrame, unit: str):
    sample = data.sample(frac=1, replace=True)
    sample[unit] = range(1, len(sample) + 1)
    return sample

def _optimal_treatment_control_split(data: pd.DataFrame, unit: str, outcome: str, seed: int):
    result = _sample(
        data = data, 
        unit = unit,
        normalized_feature = 'norm_initial_weight',
        seed = seed
    )
    treatment_dict = {inx: gp for gp, indices in result.items() for inx in indices}
    treatment = data[unit].map(treatment_dict)
    mask_tr = (treatment == 'Treatment').values
    responses = data[outcome].values
    return responses, mask_tr

def _sample(data: pd.DataFrame, unit: str, normalized_feature: str, seed: int):
    model = SingleFeatureOptimizationModel(
        data, 
        n_groups = 2, 
        units_per_group = 10, 
        unit = unit, 
        metric = normalized_feature,
    )
    status = model.optimize()
    optimized_groups = model.get_groups_list()
    randomized_groups = random_shuffle_groups(optimized_groups, seed=seed)
    treatment_labels = ["Placebo", "Treatment"]
    return {treatment_labels[i]: randomized_groups[i] for i in range(len(randomized_groups))}
```

```py
infer_result = inference(
    data = experiment_data, 
    unit = 'mice',
    outcome = 'final_weight',
    treatment = 'group',
    treatment_value = 'Treatment',
    n_bootstrap = 1000
)

print(infer_result)
```

```py
>     delta    pvalue  n_bootstrap  avg_delta_bootstrap  std_delta_bootstrap
0 -260.183  0.001998         1000                 2.02               112.61
```

ç»„é—´è§‚å¯Ÿåˆ°çš„-260æ¯«å…‹çš„å·®å¼‚åœ¨5%çš„æ˜¾è‘—æ€§æ°´å¹³ä¸‹æ˜¯æ˜¾è‘—çš„ï¼ˆpå€¼å°äº0.05ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ‹’ç»é›¶å‡è®¾ï¼ˆå‡å€¼ç›¸ç­‰ï¼‰ï¼Œå¹¶å¾—å‡ºç»“è®ºï¼šæ²»ç–—å…·æœ‰ç»Ÿè®¡å­¦æ˜¾è‘—æ•ˆåº”ã€‚

# **1000åªå°é¼ å®éªŒçš„ç»“æœ**

æˆ‘ä»¬å¯ä»¥å¤šæ¬¡æ¨¡æ‹Ÿå®éªŒï¼Œç”Ÿæˆå…·æœ‰ä¸åŒåˆå§‹è‚¿ç˜¤é‡é‡çš„å°é¼ ç¾¤ä½“ï¼Œè¿™äº›å°é¼ æ¥è‡ªç›¸åŒçš„æ­£æ€åˆ†å¸ƒï¼Œå‡å€¼ä¸º200 mgï¼Œæ ‡å‡†å·®ä¸º300 mgã€‚

è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†åŸºäºä¼˜åŒ–çš„è®¾è®¡ä¸å…¶ä»–å®éªŒè®¾è®¡è¿›è¡Œæ¯”è¾ƒã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œæˆ‘å°†ä¼˜åŒ–æ–¹æ³•ä¸ç®€å•éšæœºåˆ†é…å’Œåˆ†å±‚éšæœºåˆ†é…è¿›è¡Œäº†æ¯”è¾ƒï¼ˆå…¶ä¸­ï¼Œåˆ†å±‚æ˜¯ä½¿ç”¨åŸºäºåˆå§‹è‚¿ç˜¤é‡é‡çš„k-meansç®—æ³•åˆ›å»ºçš„ï¼‰ï¼š

![](../Images/95504d0add5f8dacff5e62d5f7f6d1e5.png)

1000æ¬¡æ¨¡æ‹Ÿå®éªŒçš„ç»“æœï¼Œç”¨äºæ£€æµ‹-250 mgçš„æ•ˆåº”ï¼ˆå›¾ç‰‡æ¥æºï¼šä½œè€…ï¼‰ã€‚

åœ¨ä»–ä»¬çš„æ–‡ç« ä¸­ï¼Œä½œè€…è¿˜å°†åŸºäºä¼˜åŒ–çš„æ–¹æ³•ä¸é‡æ–°éšæœºåŒ–å’Œé…å¯¹åŒ¹é…åœ¨ä¸åŒæ•ˆåº”å¤§å°å’Œç»„å¤§å°ä¸‹è¿›è¡Œäº†æ¯”è¾ƒã€‚å¦‚æœä½ æœ‰å…´è¶£æ·±å…¥æ¢ç´¢ç»†èŠ‚ï¼Œæˆ‘å¼ºçƒˆæ¨èé˜…è¯»å®Œæ•´çš„æ–‡ç« ï¼

æ­å–œï¼ä½ å·²ç»é˜…è¯»å®Œæ¯•ğŸ‰å¦‚æœä½ è§‰å¾—è¿™ç¯‡æ–‡ç« æœ‰è¶£ï¼Œå¯ä»¥è€ƒè™‘å…³æ³¨æˆ‘ã€‚æˆ‘ç»å¸¸åˆ†äº«å…³äºä¼˜åŒ–å’Œå› æœæ¨æ–­çš„æƒ³æ³•ã€‚
