["```py\nvoid AddTwoVectors(flaot A[], float B[], float C[]) {\n    for (int i = 0; i < N; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```", "```py\n#include <stdio.h>\n\nvoid AddTwoVectors(flaot A[], float B[], float C[]) {\n    for (int i = 0; i < N; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n\nint main() {\n    ...\n    AddTwoVectors(A, B, C);\n    ...\n}\n```", "```py\n#include <stdio.h>\n\n// Kernel definition\n__global__ void AddTwoVectors(float A[], float B[], float C[]) {\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main() {\n    ...\n    // Kernel invocation with N threads\n    AddTwoVectors<<<1, N>>>(A, B, C);\n    ...\n}\n```", "```py\n#include <stdio.h>\n\n// Kernel definition\n__global__ void AddTwoVectors(float A[], float B[], float C[]) {\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main() {\n\n    int N = 1000; // Size of the vectors\n    float A[N], B[N], C[N]; // Arrays for vectors A, B, and C\n\n    ...\n\n    float *d_A, *d_B, *d_C; // Device pointers for vectors A, B, and C\n\n    // Allocate memory on the device for vectors A, B, and C\n    cudaMalloc((void **)&d_A, N * sizeof(float));\n    cudaMalloc((void **)&d_B, N * sizeof(float));\n    cudaMalloc((void **)&d_C, N * sizeof(float));\n\n    // Copy vectors A and B from host to device\n    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);\n\n    // Kernel invocation with N threads\n    AddTwoVectors<<<1, N>>>(d_A, d_B, d_C);\n\n    // Copy vector C from device to host\n    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);\n\n}\n```", "```py\n#include <stdio.h>\n\n// Kernel definition\n__global__ void AddTwoVectors(float A[], float B[], float C[]) {\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main() {\n\n    int N = 1000; // Size of the vectors\n    float A[N], B[N], C[N]; // Arrays for vectors A, B, and C\n\n    // Initialize vectors A and B\n    for (int i = 0; i < N; ++i) {\n        A[i] = 1;\n        B[i] = 3;\n    }\n\n    float *d_A, *d_B, *d_C; // Device pointers for vectors A, B, and C\n\n    // Allocate memory on the device for vectors A, B, and C\n    cudaMalloc((void **)&d_A, N * sizeof(float));\n    cudaMalloc((void **)&d_B, N * sizeof(float));\n    cudaMalloc((void **)&d_C, N * sizeof(float));\n\n    // Copy vectors A and B from host to device\n    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);\n\n    // Kernel invocation with N threads\n    AddTwoVectors<<<1, N>>>(d_A, d_B, d_C);\n\n    // Copy vector C from device to host\n    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);\n\n    // Free device memory\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n}\n```", "```py\n#include <stdio.h>\n\n// Kernel definition\n__global__ void AddTwoVectors(float A[], float B[], float C[]) {\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main() {\n\n    int N = 1000; // Size of the vectors\n    float A[N], B[N], C[N]; // Arrays for vectors A, B, and C\n\n    // Initialize vectors A and B\n    for (int i = 0; i < N; ++i) {\n        A[i] = 1;\n        B[i] = 3;\n    }\n\n    float *d_A, *d_B, *d_C; // Device pointers for vectors A, B, and C\n\n    // Allocate memory on the device for vectors A, B, and C\n    cudaMalloc((void **)&d_A, N * sizeof(float));\n    cudaMalloc((void **)&d_B, N * sizeof(float));\n    cudaMalloc((void **)&d_C, N * sizeof(float));\n\n    // Copy vectors A and B from host to device\n    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);\n\n    // Kernel invocation with N threads\n    AddTwoVectors<<<1, N>>>(d_A, d_B, d_C);\n\n    // Check for error\n    cudaError_t error = cudaGetLastError();\n    if(error != cudaSuccess) {\n        printf(\"CUDA error: %s\\n\", cudaGetErrorString(error));\n        exit(-1);\n    }\n\n    // Waits untill all CUDA threads are executed\n    cudaDeviceSynchronize();\n\n    // Copy vector C from device to host\n    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);\n\n    // Free device memory\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n}\n```", "```py\n%%shell\nnvcc example.cu -o compiled_example # compile\n./compiled_example # run\n\n# you can also run the code with bug detection sanitizer\ncompute-sanitizer --tool memcheck ./compiled_example \n```", "```py\nint device;\ncudaDeviceProp props;\ncudaGetDevice(&device);\ncudaGetDeviceProperties(&props, device);\nprintf(\"Maximum threads per block: %d\\n\", props.maxThreadsPerBlock);\n```", "```py\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n```", "```py\n#include <stdio.h>\n\n// Kernel definition\n__global__ void AddTwoVectors(float A[], float B[], float C[], int N) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) // To avoid exceeding array limit\n        C[i] = A[i] + B[i];\n}\n\nint main() {\n    int N = 500000; // Size of the vectors\n    int threads_per_block;\n    int device;\n    cudaDeviceProp props;\n    cudaGetDevice(&device);\n    cudaGetDeviceProperties(&props, device);\n    threads_per_block = props.maxThreadsPerBlock;\n    printf(\"Maximum threads per block: %d\\n\", threads_per_block); // 1024\n\n    float A[N], B[N], C[N]; // Arrays for vectors A, B, and C\n\n    // Initialize vectors A and B\n    for (int i = 0; i < N; ++i) {\n        A[i] = 1;\n        B[i] = 3;\n    }\n\n    float *d_A, *d_B, *d_C; // Device pointers for vectors A, B, and C\n\n    // Allocate memory on the device for vectors A, B, and C\n    cudaMalloc((void **)&d_A, N * sizeof(float));\n    cudaMalloc((void **)&d_B, N * sizeof(float));\n    cudaMalloc((void **)&d_C, N * sizeof(float));\n\n    // Copy vectors A and B from host to device\n    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);\n\n    // Kernel invocation with multiple blocks and threads_per_block threads per block\n    int number_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n    AddTwoVectors<<<number_of_blocks, threads_per_block>>>(d_A, d_B, d_C, N);\n\n    // Check for error\n    cudaError_t error = cudaGetLastError();\n    if (error != cudaSuccess) {\n        printf(\"CUDA error: %s\\n\", cudaGetErrorString(error));\n        exit(-1);\n    }\n\n    // Wait until all CUDA threads are executed\n    cudaDeviceSynchronize();\n\n    // Copy vector C from device to host\n    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);\n\n    // Free device memory\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n\n}\n```", "```py\n#include <stdio.h>\n\n// Kernel definition\n__global__ void AddTwoMatrices(float A[N][N], float B[N][N], float C[N][N]) {\n    int i = threadIdx.x;\n    int j = threadIdx.y;\n    C[i][j] = A[i][j] + B[i][j];\n}\n\nint main() {\n    ...\n    // Kernel invocation with 1 block of NxN threads\n    dim3 threads_per_block(N, N);\n    AddTwoMatrices<<<1, threads_per_block>>>(A, B, C);\n    ...\n}\n```", "```py\n#include <stdio.h>\n\n// Kernel definition\n__global__ void AddTwoMatrices(float A[N][N], float B[N][N], float C[N][N]) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        C[i][j] = A[i][j] + B[i][j];\n    }\n}\n\nint main() {\n    ...\n    // Kernel invocation with 1 block of NxN threads\n    dim3 threads_per_block(32, 32);\n    dim3 number_of_blocks((N + threads_per_block.x - 1) ∕ threads_per_block.x, (N + threads_per_block.y - 1) ∕ threads_per_block.y);\n    AddTwoMatrices<<<number_of_blocks, threads_per_block>>>(A, B, C);\n    ...\n}\n```", "```py\n#include <math.h>\n\n// Sigmoid function\n__device__ float sigmoid(float x) {\n    return 1 / (1 + expf(-x));\n}\n\n// Kernel definition for applying sigmoid function to a vector\n__global__ void sigmoidActivation(float input[], float output[]) {\n    int i = threadIdx.x;\n    output[i] = sigmoid(input[i]);\n\n}\n```", "```py\n// GPU version\n\n__global__ void matMul(float A[M][N], float B[N][P], float C[M][P]) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < M && col < P) {\n        float C_value = 0;\n        for (int i = 0; i < N; i++) {\n            C_value += A[row][i] * B[i][col];\n        }\n        C[row][col] = C_value;\n    }\n}\n```", "```py\n// CPU version\n\nvoid matMul(float A[M][N], float B[N][P], float C[M][P]) {\n    for (int row = 0; row < M; row++) {\n        for (int col = 0; col < P; col++) {\n            float C_value = 0;\n            for (int i = 0; i < N; i++) {\n                C_value += A[row][i] * B[i][col];\n            }\n            C[row][col] = C_value;\n        }\n    }\n}\n```"]