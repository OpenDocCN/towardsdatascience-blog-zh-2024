<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Deep Learning At Scale: Parallel Model Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Deep Learning At Scale: Parallel Model Training</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-at-scale-parallel-model-training-d7c22904b5a4?source=collection_archive---------9-----------------------#2024-04-05">https://towardsdatascience.com/deep-learning-at-scale-parallel-model-training-d7c22904b5a4?source=collection_archive---------9-----------------------#2024-04-05</a></blockquote><div><div class="em ff fg fh fi fj"/><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><div/><div><h2 id="34ec" class="pw-subtitle-paragraph go fq fr bf b gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cq dx">Concept and a Pytorch Lightning example</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@caroline.arnold_63207?source=post_page---byline--d7c22904b5a4--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Caroline Arnold" class="l ep by dd de cx" src="../Images/fb13ba36e302d8161b67c4888d0601e4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*rMa21dr4FkLAt8NY-pdWRw.png"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d7c22904b5a4--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@caroline.arnold_63207?source=post_page---byline--d7c22904b5a4--------------------------------" rel="noopener follow">Caroline Arnold</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d7c22904b5a4--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk ld le ab q ee lf lg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lc"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lb lc">1</span></p></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lh k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al li an ao ap ie lj lk ll" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lm cn"><div class="l ae"><div class="ab cb"><div class="ln lo lp lq lr ls ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml mm"><img src="../Images/9889f32b020f8d3fbe4ebe73db1d32fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9iJhwaGBgmB8O_PYKzEpnQ.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Image created by the author using Midjourney.</figcaption></figure><p id="cffa" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Parallel training on a large number of GPUs is state of the art in deep learning. The open source image generation algorithm Stable Diffusion <a class="af nz" href="https://the-decoder.com/training-cost-for-stable-diffusion-was-just-600000-and-that-is-a-good-sign-for-ai-progress/" rel="noopener ugc nofollow" target="_blank">was trained</a> on a cluster of 256 GPUs. Meta’s <a class="af nz" href="https://www.siliconrepublic.com/machines/meta-is-using-two-nvidia-gpu-clusters-to-train-llama-3" rel="noopener ugc nofollow" target="_blank">AI Research SuperCluster</a> contains more than 24,000 NVIDIA H100 GPUs that are used to train models such as Llama 3.</p><p id="dcc2" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">By using multiple GPUs, machine learning experts reduce the <em class="oa">wall time</em> of their training runs. Training Stable Diffusion <a class="af nz" href="https://the-decoder.com/training-cost-for-stable-diffusion-was-just-600000-and-that-is-a-good-sign-for-ai-progress/" rel="noopener ugc nofollow" target="_blank">took 150,000 GPU hours</a>, or more than 17 years. Parallel training reduced that to 25 days.</p><p id="9d7d" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">There are two types of <a class="af nz" href="https://arxiv.org/abs/1802.09941" rel="noopener ugc nofollow" target="_blank">parallel deep learning</a>:</p><ul class=""><li id="73f0" class="nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny ob oc od bk"><em class="oa">Data parallelism</em>, where a large dataset is distributed across multiple GPUs.</li><li id="d8d5" class="nd ne fr nf b gp oe nh ni gs of nk nl nm og no np nq oh ns nt nu oi nw nx ny ob oc od bk"><em class="oa">Model parallelism</em>, where a deep learning model that is too large to fit on a single GPU is distributed across multiple devices.</li></ul><p id="5c34" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">We will focus here on data parallelism, as model parallelism <a class="af nz" href="https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html" rel="noopener ugc nofollow" target="_blank">only becomes relevant</a> for very large models beyond 500M parameters.</p><p id="44b0" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Beyond reducing wall time, there is an economic argument for parallel training: Cloud compute providers such as <a class="af nz" href="https://docs.aws.amazon.com/batch/latest/userguide/gpu-jobs.html" rel="noopener ugc nofollow" target="_blank">AWS offer single machines</a> with up to 16 GPUs. Parallel training can take advantage of all available GPUs, and you get more value for your money.</p></div></div></div><div class="ab cb oj ok ol om" role="separator"><span class="on by bm oo op oq"/><span class="on by bm oo op oq"/><span class="on by bm oo op"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h2 id="b6a5" class="or os fr bf ot ou ov ow ox oy oz pa pb nm pc pd pe nq pf pg ph nu pi pj pk pl bk">Parallel computing</h2><p id="e2d8" class="pw-post-body-paragraph nd ne fr nf b gp pm nh ni gs pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fk bk"><a class="af nz" href="https://en.wikipedia.org/wiki/Parallel_computing" rel="noopener ugc nofollow" target="_blank">Parallelism is the dominant paradigm</a> in high performance computing. Programmers identify tasks that can be executed independently of each other and distribute them across a large number of devices. The serial parts of the program distribute the tasks and gather the results.</p><p id="9e11" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Parallel computing reduces computation time. A program parallelized across four devices could ideally run four times faster than the same program running on a single device. In practice, communication overhead limits this scaling.</p><p id="c9ef" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk"><em class="oa">As an analogy, think of a group of painters painting a wall. The communication overhead occurs when the foreman tells everyone what color to use and what area to paint. Painting can be done in parallel, and only the finishing is again a serial task.</em></p><h2 id="b615" class="or os fr bf ot ou ov ow ox oy oz pa pb nm pc pd pe nq pf pg ph nu pi pj pk pl bk">Training deep learning models</h2><p id="86d6" class="pw-post-body-paragraph nd ne fr nf b gp pm nh ni gs pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fk bk">Training a deep learning algorithm requires data and an optimization routine like <a class="af nz" rel="noopener" target="_blank" href="/stochastic-gradient-descent-math-and-python-code-35b5e66d6f79">stochastic gradient descent</a>.</p><p id="1b36" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The training data is shuffled and split into batches of fixed size. Batch by batch, the training routine calculates the loss between the actual and predicted labels. The model parameters are adjusted according to the gradient of the loss with respect to the model parameters.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml pr"><img src="../Images/9f2d4393572ca20cfa46824bd8c9d985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DQcppTLYdSrPtzFmC4PpAQ.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Stochastic gradient descent. Image created by the author.</figcaption></figure><p id="5701" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">An <em class="oa">epoch</em> of training is complete when the model has seen all training data batches once.</p></div></div></div><div class="ab cb oj ok ol om" role="separator"><span class="on by bm oo op oq"/><span class="on by bm oo op oq"/><span class="on by bm oo op"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h2 id="6638" class="or os fr bf ot ou ov ow ox oy oz pa pb nm pc pd pe nq pf pg ph nu pi pj pk pl bk">Distributed data parallelism</h2><p id="9f59" class="pw-post-body-paragraph nd ne fr nf b gp pm nh ni gs pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fk bk">The following diagram describes data parallelism for training a deep learning model using 3 GPUs. A replica of the untrained model is copied to each GPU. The data set is split into 3 parts, each part is processed in parallel on a separate GPU.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml ps"><img src="../Images/2f7994943c709cc560310884b328f8f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zf8G2dHy8X5zcnc6lBm9Nw.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Distributed data, same model. Image created by the author.</figcaption></figure><p id="1b72" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Note that each model replica sees different subsets of the training data. After completing the epoch, the weights and biases in each model replica are different.</p><p id="bb56" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The three model replicas are harmonized by averaging the weights across all model instances. The updated model is broadcast across all 3 GPUs, and training continues with the next epoch.</p><p id="9bad" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Distributed training changes the effective batch size to <code class="cx pt pu pv pw b">number of devices * original batch size</code> . This can affect training, convergence, and model performance.</p></div></div></div><div class="ab cb oj ok ol om" role="separator"><span class="on by bm oo op oq"/><span class="on by bm oo op oq"/><span class="on by bm oo op"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h2 id="39f4" class="or os fr bf ot ou ov ow ox oy oz pa pb nm pc pd pe nq pf pg ph nu pi pj pk pl bk">Implementation in Pytorch Lightning</h2><p id="fe1c" class="pw-post-body-paragraph nd ne fr nf b gp pm nh ni gs pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fk bk">We need several ingredients for data parallelism:</p><ul class=""><li id="a05f" class="nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny ob oc od bk">A dataloader that can handle distributed training</li><li id="64cd" class="nd ne fr nf b gp oe nh ni gs of nk nl nm og no np nq oh ns nt nu oi nw nx ny ob oc od bk">An <em class="oa">all-reduce</em> function that harmonizes the model replicas</li><li id="e647" class="nd ne fr nf b gp oe nh ni gs of nk nl nm og no np nq oh ns nt nu oi nw nx ny ob oc od bk">A framework for the different parallel parts to communicate with each other</li></ul><p id="e5e1" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In PyTorch Lightning, the <a class="af nz" href="https://lightning.ai/docs/pytorch/stable/common/trainer.html" rel="noopener ugc nofollow" target="_blank">Lightning Trainer</a> handles the entire training process. It can perform distributed data parallel training <em class="oa">out of the box</em> by specifying the following keyword arguments:</p><ul class=""><li id="7d8b" class="nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny ob oc od bk">Number of nodes — the machines in the cloud compute cluster that you want to use</li><li id="4fba" class="nd ne fr nf b gp oe nh ni gs of nk nl nm og no np nq oh ns nt nu oi nw nx ny ob oc od bk">Number of devices (GPUs) <em class="oa">per node</em></li><li id="74ae" class="nd ne fr nf b gp oe nh ni gs of nk nl nm og no np nq oh ns nt nu oi nw nx ny ob oc od bk">Training strategy, here distributed data parallelism. Different <a class="af nz" href="https://lightning.ai/docs/pytorch/stable/api_references.html#strategies" rel="noopener ugc nofollow" target="_blank">strategies</a> are available for distributed training, they take care of adjusting the training procedure.</li></ul><p id="8645" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">For distributed data parallelism (<em class="oa">ddp</em>) training on 2 nodes with 2 GPUs each, use:</p><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="5b1a" class="qa os fr pw b bg qb qc l qd qe">import lightning as L<br/><br/>trainer = L.Trainer(nodes=2, devices=2, strategy='ddp')<br/>trainer.fit(model, dataloader)</span></pre><p id="cd67" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">By default, the Lightning Trainer uses all available GPUs in the context of the computing environment. This implies that you need to specify explicitly if you want to use only a single GPU on a multi-GPU machine.</p><h2 id="08c7" class="or os fr bf ot ou ov ow ox oy oz pa pb nm pc pd pe nq pf pg ph nu pi pj pk pl bk">Experiment: Parallel training with the Country211 dataset</h2><p id="b2d1" class="pw-post-body-paragraph nd ne fr nf b gp pm nh ni gs pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fk bk">The <a class="af nz" href="https://github.com/openai/CLIP/blob/main/data/country211.md" rel="noopener ugc nofollow" target="_blank">Country211 dataset</a> consists of geo-tagged images from 211 countries. In my experiment, I use a pre-trained vision transformer and fine tune this model on the Country211 dataset. The code is available on <a class="af nz" href="https://github.com/crlna16/pretrained-vision-transformer" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p><div class="qf qg qh qi qj qk"><a rel="noopener follow" target="_blank" href="/how-to-fine-tune-a-pretrained-vision-transformer-on-satellite-data-d0ddd8359596?source=post_page-----d7c22904b5a4--------------------------------"><div class="ql ab ih"><div class="qm ab co cb qn qo"><h2 class="bf fs hx z ip qp ir is qq iu iw fq bk">How to Fine-Tune a Pretrained Vision Transformer on Satellite Data</h2><div class="qr l"><h3 class="bf b hx z ip qp ir is qq iu iw dx">A step-by-step tutorial in PyTorch Lightning</h3></div><div class="qs l"><p class="bf b dy z ip qp ir is qq iu iw dx">towardsdatascience.com</p></div></div><div class="qt l"><div class="qu l qv qw qx qt qy ls qk"/></div></div></a></div><p id="a537" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">There are 31,650 training images. I used a batch size of 256 throughout and <a class="af nz" href="https://lightning.ai/docs/fabric/stable/guide/multi_node/cloud.html" rel="noopener ugc nofollow" target="_blank">experimented with the number of GPUs</a>, going from a single GPU on a single node up to eight GPUs distributed across four nodes. I applied an early stopping condition, where training stops if the validation accuracy does not improve for five epochs.</p><p id="284f" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The training wall time, which is the total time it took to train the model, decreases with the number of GPUs used. On a single GPU, the wall time is 45 minutes, which drops to 11 minutes when I use eight GPUs.</p><p id="cd7d" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The ideal scaling would be 45 minutes / 8 GPUs = 5.6 minutes, but communication overhead and a larger number of epochs when training on more GPUs keep us from reaching this optimum.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml qz"><img src="../Images/e288058bf56d71bc57ef28cd0c8acbe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*APRfj3DIkR78UmxO4xNe6A.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Wall time for different numbers of devices. Image created by the author.</figcaption></figure><p id="69d6" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">To compare the generalization capabilities, I calculated the test set accuracy with all trained models. The best test set accuracy was obtained when using a single GPU on a single device, but even there, the model had issues with generalization: The test set accuracy is only 15%.</p><p id="84fa" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The panel shows the decrease in accuracy, relative to the optimal result, for different numbers of GPUs. The more devices were used in training, the less accurate the model became when applied to the test set.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml qz"><img src="../Images/28b987cf516ae749486725807cc19f5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TY3L-1F-C-yjXVH3YQ-jLQ.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Test set accuracy. Image created by the author.</figcaption></figure><p id="6bc6" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The changing accuracy could be due to the changing effective batch size — note that we do not get exactly the same model depending on the training strategy.</p><p id="6b66" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In this experiment, there is a tradeoff between generalization accuracy and speed. Training on 8 GPUs was four times faster, but 11% less accurate than training on a single device.</p><h2 id="ef75" class="or os fr bf ot ou ov ow ox oy oz pa pb nm pc pd pe nq pf pg ph nu pi pj pk pl bk">A look at parallel training</h2><p id="23a7" class="pw-post-body-paragraph nd ne fr nf b gp pm nh ni gs pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fk bk">In the training log file, you will see lines like these:</p><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="dcd4" class="qa os fr pw b bg qb qc l qd qe">Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4<br/>Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4<br/>Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4<br/>Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4<br/>----------------------------------------------------------------------------------------------------<br/>distributed_backend=nccl<br/>All distributed processes registered. Starting with 4 processes<br/>----------------------------------------------------------------------------------------------------</span></pre><pre class="ra px pw rb rc ay rd bk"><span id="f40e" class="or os fr pw b hx re rf l in qe">LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]<br/>LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]<br/>LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]<br/>LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]</span></pre><p id="43ea" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The parallel training processes must be able to communicate with each other. The communication backend, here the NVIDIA Collective Communications Library (<em class="oa">nccl</em>), is automatically initialized depending on the platform.</p><p id="d883" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">We have requested two nodes with two GPUs each, for a total of four parallel processes. They are enumerated by their <em class="oa">global rank</em> (0–3). On each node, the <em class="oa">local rank</em> identifies the GPU that is used (0 or 1).</p><p id="db16" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">When the model is large, the <a class="af nz" href="https://en.wikipedia.org/wiki/Collective_operation#All-Reduce" rel="noopener ugc nofollow" target="_blank"><em class="oa">all-reduce</em> operation</a> can take a long time. Compute clusters connect the GPUs with efficient networks, but sending a full model replica out puts a strain on communication.</p><h2 id="f75a" class="or os fr bf ot ou ov ow ox oy oz pa pb nm pc pd pe nq pf pg ph nu pi pj pk pl bk">Model parallelism</h2><p id="3a12" class="pw-post-body-paragraph nd ne fr nf b gp pm nh ni gs pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fk bk"><a class="af nz" href="https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html" rel="noopener ugc nofollow" target="_blank">Lightning provides strategies</a> for model parallelism. Note that this should only be explored for very large models with billions of parameters, where the model parameters and gradients exceed GPU memory.</p></div></div></div><div class="ab cb oj ok ol om" role="separator"><span class="on by bm oo op oq"/><span class="on by bm oo op oq"/><span class="on by bm oo op"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h2 id="58d9" class="or os fr bf ot ou ov ow ox oy oz pa pb nm pc pd pe nq pf pg ph nu pi pj pk pl bk">Summary</h2><p id="b716" class="pw-post-body-paragraph nd ne fr nf b gp pm nh ni gs pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fk bk">Parallel training can speed up the training process and help you make the most of your computational resources.</p><p id="61db" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">We distinguish between data parallelism, where replicas of the model see subsets of the data during training, and model parallelism, where the model itself is distributed across multiple devices.</p><p id="bfe4" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">With PyTorch Lightning, data parallel training is as simple as specifying the <em class="oa">distributed data parallelism</em> strategy and the number of nodes and devices (GPUs) per node.</p><p id="971c" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Data parallel training reduces the wall time and changes the effective batch size. Models trained on different numbers of devices may perform differently, and careful evaluation is recommended.</p><p id="3c84" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Model parallel training should only be considered for very large models exceeding 500M parameters.</p><p id="f01d" class="pw-post-body-paragraph nd ne fr nf b gp ng nh ni gs nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The code to replicate the experiment can be found on my GitHub:</p><div class="qf qg qh qi qj qk"><a href="https://github.com/crlna16/pretrained-vision-transformer?source=post_page-----d7c22904b5a4--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ql ab ih"><div class="qm ab co cb qn qo"><h2 class="bf fs hx z ip qp ir is qq iu iw fq bk">GitHub - crlna16/pretrained-vision-transformer: Pretrained Vision Transformer with PyTorch…</h2><div class="qr l"><h3 class="bf b hx z ip qp ir is qq iu iw dx">Pretrained Vision Transformer with PyTorch Lightning - crlna16/pretrained-vision-transformer</h3></div><div class="qs l"><p class="bf b dy z ip qp ir is qq iu iw dx">github.com</p></div></div><div class="qt l"><div class="rg l qv qw qx qt qy ls qk"/></div></div></a></div></div></div></div><div class="ab cb oj ok ol om" role="separator"><span class="on by bm oo op oq"/><span class="on by bm oo op oq"/><span class="on by bm oo op"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h2 id="faf9" class="or os fr bf ot ou ov ow ox oy oz pa pb nm pc pd pe nq pf pg ph nu pi pj pk pl bk">References</h2><ul class=""><li id="c1d4" class="nd ne fr nf b gp pm nh ni gs pn nk nl nm po no np nq pp ns nt nu pq nw nx ny ob oc od bk">Tal Ben-Nun <em class="oa">et al</em>, “Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis”, 2018, <a class="af nz" href="https://arxiv.org/abs/1802.09941" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1802.09941</a></li><li id="0239" class="nd ne fr nf b gp oe nh ni gs of nk nl nm og no np nq oh ns nt nu oi nw nx ny ob oc od bk">OpenAI Country211 dataset: <a class="af nz" href="https://github.com/openai/CLIP/blob/main/data/country211.md" rel="noopener ugc nofollow" target="_blank">source</a> (subset of <a class="af nz" href="https://dx.doi.org/10.1145/2812802" rel="noopener ugc nofollow" target="_blank">YFCC100M</a>) and <a class="af nz" href="https://multimediacommons.wordpress.com/yfcc100m-core-dataset/#yfcc100m" rel="noopener ugc nofollow" target="_blank">license</a></li><li id="7b47" class="nd ne fr nf b gp oe nh ni gs of nk nl nm og no np nq oh ns nt nu oi nw nx ny ob oc od bk">Vision transformer model: Dosovitskiy et al, <em class="oa">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em>, ICLR, 2021. <a class="af nz" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank">Paper</a> and code</li><li id="5100" class="nd ne fr nf b gp oe nh ni gs of nk nl nm og no np nq oh ns nt nu oi nw nx ny ob oc od bk">PyTorch Lightning project for data-parallel training: <a class="af nz" href="https://lightning.ai/docs/pytorch/stable/accelerators/gpu_intermediate.html" rel="noopener ugc nofollow" target="_blank">docs</a></li><li id="213a" class="nd ne fr nf b gp oe nh ni gs of nk nl nm og no np nq oh ns nt nu oi nw nx ny ob oc od bk">Will Falcon on distributed training <a class="af nz" rel="noopener" target="_blank" href="/9-tips-for-training-lightning-fast-neural-networks-in-pytorch-8e63a502f565">(TDS, 2019)</a></li></ul></div></div></div></div>    
</body>
</html>