<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Art of Tokenization: Breaking Down Text for AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Art of Tokenization: Breaking Down Text for AI</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-art-of-tokenization-breaking-down-text-for-ai-43c7bccaed25?source=collection_archive---------0-----------------------#2024-09-26">https://towardsdatascience.com/the-art-of-tokenization-breaking-down-text-for-ai-43c7bccaed25?source=collection_archive---------0-----------------------#2024-09-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a7e7" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Demystifying NLP: From Text to Embeddings</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@murilogustineli?source=post_page---byline--43c7bccaed25--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Murilo Gustineli" class="l ep by dd de cx" src="../Images/2a56c10e79b4810c7bf5e511300bfc34.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*4qAuEaLDWBpWwTGfSSiSzQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--43c7bccaed25--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@murilogustineli?source=post_page---byline--43c7bccaed25--------------------------------" rel="noopener follow">Murilo Gustineli</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--43c7bccaed25--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/10347347f8af7b1b60b09cc2d9a55932.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QVXvydRMEWTWiUP42bYBAg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Tokenization example generated by Llama-3-8B. Each colored subword represents a distinct token.</figcaption></figure><h1 id="3d0d" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk"><strong class="al">What is tokenization?</strong></h1><p id="1762" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk ou"><span class="l ov ow ox bo oy oz pa pb pc ed">In</span> computer science, we refer to human languages, like English and Mandarin, as “natural” languages. In contrast, languages designed to interact with computers, like Assembly and LISP, are called “machine” languages, following strict syntactic rules that leave little room for interpretation. While computers excel at processing their own highly structured languages, they struggle with the messiness of human language.</p><p id="91ed" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Language — especially text — makes up most of our communication and knowledge storage. For example, the internet is mostly text. Large language models like <a class="af pi" href="https://openai.com/chatgpt/" rel="noopener ugc nofollow" target="_blank">ChatGPT</a>, <a class="af pi" href="https://www.anthropic.com/claude" rel="noopener ugc nofollow" target="_blank">Claude</a>, and <a class="af pi" href="https://www.llama.com/" rel="noopener ugc nofollow" target="_blank">Llama </a>are trained on enormous amounts of text — essentially all the text available online — using sophisticated computational techniques. However, computers operate on numbers, not words or sentences. So, how do we bridge the gap between human language and machine understanding?</p><p id="1040" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">This is where <strong class="oa fr">Natural Language Processing (NLP)</strong> comes into play. NLP is a field that combines linguistics, computer science, and artificial intelligence to enable computers to understand, interpret, and generate human language. Whether translating text from English to French, summarizing articles, or engaging in conversation, NLP allows machines to produce meaningful outputs from textual inputs.</p><p id="94ff" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">The first critical step in NLP is transforming raw text into a format that computers can work with effectively. This process is known as <strong class="oa fr">tokenization</strong>. Tokenization involves breaking down text into smaller, manageable units called <strong class="oa fr"><em class="pj">tokens</em></strong>, which can be words, subwords, or even individual characters. Here’s how the process typically works:</p><ul class=""><li id="5f87" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pk pl pm bk"><strong class="oa fr">Standardization:</strong> Before tokenizing, the text is standardized to ensure consistency. This may include converting all letters to lowercase, removing punctuation, and applying other normalization techniques.</li><li id="f207" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><strong class="oa fr">Tokenization:</strong> The standardized text is then split into tokens. For example, the sentence <code class="cx ps pt pu pv b">“The quick brown fox jumps over the lazy dog”</code> can be tokenized into words:</li></ul><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="d78f" class="pz nd fq pv b bg qa qb l qc qd">["the", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"]</span></pre><ul class=""><li id="82e9" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pk pl pm bk"><strong class="oa fr">Numerical representation:</strong> Since computers operate on numerical data, each token is converted into a numerical representation. This can be as simple as assigning a unique identifier to each token or as complex as creating multi-dimensional vectors that capture the token’s meaning and context.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qe"><img src="../Images/404ea4ba951265efc54d7a42d444724f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mkvzSPMiX5FZcuQjFe2B6w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="qf">Illustration inspired by “Figure 11.1 From text to vectors” from</em> <a class="af pi" href="https://www.manning.com/books/deep-learning-with-python-second-edition" rel="noopener ugc nofollow" target="_blank"><strong class="bf ne">Deep Learning with Python</strong> <em class="qf">by François Chollet</em></a></figcaption></figure><p id="4eb4" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Tokenization is more than just splitting text; it’s about preparing language data in a way that preserves meaning and context for computational models. Different tokenization methods can significantly impact how well a model understands and processes language.</p><p id="111e" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">In this article, we focus on text standardization and tokenization, exploring a few techniques and implementations. We’ll lay the groundwork for converting text into numerical forms that machines can process — a crucial step toward advanced topics like word embeddings and language modeling that we’ll tackle in future articles.</p><h1 id="96b4" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk"><strong class="al">Text standardization</strong></h1><p id="bab1" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Consider these two sentences:</p><blockquote class="qg qh qi"><p id="4c34" class="ny nz pj oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk"><em class="fq">1. </em><code class="cx ps pt pu pv b">“dusk fell, i was gazing at the Sao Paulo skyline. Isnt urban life vibrant??”</code></p><p id="11aa" class="ny nz pj oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk"><em class="fq">2. </em><code class="cx ps pt pu pv b">“Dusk fell; I gazed at the São Paulo skyline. Isn’t urban life vibrant?”</code></p></blockquote><p id="e5cc" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">At first glance, these sentences convey a similar meaning. However, when processed by a computer, especially during tasks like tokenization or encoding, they can appear vastly different due to subtle variations:</p><ul class=""><li id="dd11" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pk pl pm bk"><strong class="oa fr">Capitalization:</strong> <code class="cx ps pt pu pv b">“dusk”</code> vs. <code class="cx ps pt pu pv b">“Dusk”</code></li><li id="020f" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><strong class="oa fr">Punctuation:</strong> Comma vs. semicolon; presence of question marks</li><li id="f76a" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><strong class="oa fr">Contractions:</strong> <code class="cx ps pt pu pv b">“Isnt”</code> vs. <code class="cx ps pt pu pv b">“Isn’t”</code></li><li id="9512" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><strong class="oa fr">Spelling and Special Characters:</strong> <code class="cx ps pt pu pv b">“Sao Paulo”</code> vs. <code class="cx ps pt pu pv b">“São Paulo”</code></li></ul><p id="aeb6" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">These differences can significantly impact how algorithms interpret the text. For example, <code class="cx ps pt pu pv b">“Isnt”</code> without an apostrophe may not be recognized as the contraction of <code class="cx ps pt pu pv b">“is not”</code>, and special characters like <code class="cx ps pt pu pv b">“ã”</code> in <code class="cx ps pt pu pv b">“São”</code> may be misinterpreted or cause encoding issues.</p><p id="56a8" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk ou"><span class="l ov ow ox bo oy oz pa pb pc ed">T</span><strong class="oa fr">ext standardization</strong> is a crucial preprocessing step in NLP that addresses these issues. By standardizing text, we reduce irrelevant variability and ensure that the data fed into models is consistent. This process is a form of feature engineering where we eliminate differences that are not meaningful for the task at hand.</p><p id="6697" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">A simple method for text standardization includes:</p><ul class=""><li id="b722" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pk pl pm bk"><strong class="oa fr">Converting to lowercase</strong>: Reduces discrepancies due to capitalization.</li><li id="72b0" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><strong class="oa fr">Removing punctuation</strong>: Simplifies the text by eliminating punctuation marks.</li><li id="6e8b" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><strong class="oa fr">Normalizing special characters</strong>: Converts characters like <code class="cx ps pt pu pv b">“ã”</code> to their standard forms (<code class="cx ps pt pu pv b">“a”</code>).</li></ul><p id="79f7" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Applying these steps to our sentences, we get:</p><blockquote class="qg qh qi"><p id="d9d5" class="ny nz pj oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk"><em class="fq">1. </em><code class="cx ps pt pu pv b">“dusk fell i was gazing at the sao paulo skyline isnt urban life vibrant”</code></p><p id="1d8c" class="ny nz pj oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk"><em class="fq">2. </em><code class="cx ps pt pu pv b">“dusk fell i gazed at the sao paulo skyline isnt urban life vibrant”</code></p></blockquote><p id="fd6e" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Now, the sentences are more uniform, highlighting only the meaningful differences in word choice (e.g., <code class="cx ps pt pu pv b">“was gazing at”</code> vs. <code class="cx ps pt pu pv b">“gazed at”</code>).</p><p id="22bb" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">While there are more advanced standardization techniques like <a class="af pi" href="https://en.wikipedia.org/wiki/Stemming" rel="noopener ugc nofollow" target="_blank"><strong class="oa fr">stemming</strong></a> (reducing words to their root forms) and <a class="af pi" href="https://en.wikipedia.org/wiki/Lemmatization" rel="noopener ugc nofollow" target="_blank"><strong class="oa fr">lemmatization</strong> </a>(reducing words to their dictionary form), this basic approach effectively minimizes superficial differences.</p><h2 id="f756" class="qj nd fq bf ne qk ql qm nh qn qo qp nk oh qq qr qs ol qt qu qv op qw qx qy qz bk"><strong class="al">Python implementation of text standardization</strong></h2><p id="299a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Here’s how you can implement basic text standardization in Python:</p><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="d5ee" class="pz nd fq pv b bg qa qb l qc qd">import re<br/>import unicodedata<br/><br/>def standardize_text(text:str) -&gt; str:<br/>    # Convert text to lowercase<br/>    text = text.lower()<br/>    # Normalize unicode characters to ASCII<br/>    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')<br/>    # Remove punctuation<br/>    text = re.sub(r'[^\w\s]', '', text)<br/>    # Remove extra whitespace<br/>    text = re.sub(r'\s+', ' ', text).strip()<br/>    return text<br/><br/># Example sentences<br/>sentence1 = "dusk fell, i was gazing at the Sao Paulo skyline. Isnt urban life vibrant??"<br/>sentence2 = "Dusk fell; I gazed at the São Paulo skyline. Isn't urban life vibrant?"<br/><br/># Standardize sentences<br/>std_sentence1 = standardize_text(sentence1)<br/>std_sentence2 = standardize_text(sentence2)<br/>print(std_sentence1)<br/>print(std_sentence2)</span></pre><p id="871b" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk"><strong class="oa fr">Output:</strong></p><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="11f4" class="pz nd fq pv b bg qa qb l qc qd">dusk fell i was gazing at the sao paulo skyline isnt urban life vibrant<br/>dusk fell i gazed at the sao paulo skyline isnt urban life vibrant</span></pre><p id="7b82" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">By standardizing the text, we’ve minimized differences that could confuse a computational model. The model can now focus on the variations between the sentences, such as the difference between <code class="cx ps pt pu pv b">“was gazing at”</code> and <code class="cx ps pt pu pv b">“gazed at”</code>, rather than discrepancies like punctuation or capitalization.</p><h1 id="f766" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Tokenization</h1><p id="0469" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk ou"><span class="l ov ow ox bo oy oz pa pb pc ed">A</span>fter text standardization, the next critical step in natural language processing is <strong class="oa fr">tokenization</strong>. Tokenization involves breaking down the standardized text into smaller units called <strong class="oa fr"><em class="pj">tokens</em></strong>. These tokens are the building blocks that models use to understand and generate human language. Tokenization prepares the text for vectorization, where each token is converted into numerical representations that machines can process.</p><p id="d180" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">We aim to convert sentences into a form that computers can efficiently and effectively handle. There are three common methods for tokenization:</p><h2 id="28c9" class="qj nd fq bf ne qk ql qm nh qn qo qp nk oh qq qr qs ol qt qu qv op qw qx qy qz bk"><strong class="al">1. Word-level tokenization</strong></h2><p id="3091" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Splits text into individual words based on spaces and punctuation. It’s the most intuitive way to break down text.</p><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="bf95" class="pz nd fq pv b bg qa qb l qc qd">text = "dusk fell i gazed at the sao paulo skyline isnt urban life vibrant"<br/>tokens = text.split()<br/>print(tokens)</span></pre><p id="033d" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk"><strong class="oa fr">Output:</strong></p><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="14e3" class="pz nd fq pv b bg qa qb l qc qd">['dusk', 'fell', 'i', 'gazed', 'at', 'the', 'sao', 'paulo', 'skyline', 'isnt', 'urban', 'life', 'vibrant']</span></pre><h2 id="9368" class="qj nd fq bf ne qk ql qm nh qn qo qp nk oh qq qr qs ol qt qu qv op qw qx qy qz bk"><strong class="al">2. Character-level tokenization</strong></h2><p id="1468" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Breaks text into individual characters, including letters and sometimes punctuation.</p><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="409b" class="pz nd fq pv b bg qa qb l qc qd">text = "Dusk fell"<br/>tokens = list(text)<br/>print(tokens)</span></pre><p id="87f8" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk"><strong class="oa fr">Output:</strong></p><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="a76a" class="pz nd fq pv b bg qa qb l qc qd">['D', 'u', 's', 'k', ' ', 'f', 'e', 'l', 'l']</span></pre><h2 id="0834" class="qj nd fq bf ne qk ql qm nh qn qo qp nk oh qq qr qs ol qt qu qv op qw qx qy qz bk"><strong class="al">3. Subword tokenization</strong></h2><p id="2a60" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Splits words into smaller, meaningful subword units. This method balances the granularity of character-level tokenization with the semantic richness of word-level tokenization. Algorithms like <strong class="oa fr">Byte-Pair Encoding (BPE)</strong> and <strong class="oa fr">WordPiece</strong> fall under this category. For instance, the <a class="af pi" href="https://huggingface.co/docs/transformers/v4.44.2/en/model_doc/bert#transformers.BertTokenizer" rel="noopener ugc nofollow" target="_blank">BertTokenizer</a> tokenizes <code class="cx ps pt pu pv b">“I have a new GPU!”</code> as follows:</p><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="357c" class="pz nd fq pv b bg qa qb l qc qd">from transformers import BertTokenizer<br/><br/>text = "I have a new GPU!"<br/>tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")<br/>tokens = tokenizer.tokenize(text)<br/>print(tokens)</span></pre><p id="5ad7" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk"><strong class="oa fr">Output:</strong></p><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="2633" class="pz nd fq pv b bg qa qb l qc qd">['i', 'have', 'a', 'new', 'gp', '##u', '!']</span></pre><p id="4cbb" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Here, <code class="cx ps pt pu pv b">“GPU”</code> is split into <code class="cx ps pt pu pv b">“gp”</code> and <code class="cx ps pt pu pv b">“##u”</code>, where <code class="cx ps pt pu pv b">“##”</code> indicates that <code class="cx ps pt pu pv b">“u”</code> is a continuation of the previous subword.</p><p id="167d" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Subword tokenization offers a balanced approach between vocabulary size and semantic representation. By decomposing rare words into common subwords, it maintains a manageable vocabulary size without sacrificing meaning. Subwords carry semantic information that aids models in understanding context more effectively. This means models can process new or rare words by breaking them down into familiar subwords, increasing their ability to handle a wider range of language inputs.</p><p id="d63f" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">For example, consider the word <code class="cx ps pt pu pv b">“annoyingly”</code> which might be rare in a training corpus. It can be decomposed into the subwords <code class="cx ps pt pu pv b">“annoying”</code> and <code class="cx ps pt pu pv b">“ly”</code>. Both <code class="cx ps pt pu pv b">“annoying”</code> and <code class="cx ps pt pu pv b">“ly”</code> appear more frequently on their own, and their combined meanings retain the essence of <code class="cx ps pt pu pv b">“annoyingly”</code>. This approach is especially beneficial in <a class="af pi" href="https://en.wikipedia.org/wiki/Agglutinative_language" rel="noopener ugc nofollow" target="_blank">agglutinative languages</a> like Turkish, where words can become exceedingly long by stringing together subwords to convey complex meanings.</p><p id="2baa" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Notice that the standardization step is often integrated into the tokenizer itself. Large language models use tokens as both inputs and outputs when processing text. Here’s a visual representation of tokens generated by Llama-3–8B on <a class="af pi" href="https://tiktokenizer.vercel.app/" rel="noopener ugc nofollow" target="_blank">Tiktokenizer</a>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ra"><img src="../Images/9dc6a0f02ad1b56bfc550734d2ff6555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h5G4o3Nhh0BzvYBX3SsBtw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf ne">Tiktokenizer</strong> example using <strong class="bf ne">Llama-3–8B</strong>. Each token is represented by a different color.</figcaption></figure><p id="97d0" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Additionally, Hugging Face provides an excellent <a class="af pi" href="https://huggingface.co/docs/transformers/en/tokenizer_summary" rel="noopener ugc nofollow" target="_blank">summary of the tokenizers</a> guide, in which I use some of its examples in this article.</p><p id="01e1" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Let’s now explore how different subword tokenization algorithms work. Note that all of those tokenization algorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained on.</p><h1 id="0073" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Byte-Pair Encoding (BPE)</h1><p id="6e24" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk ou"><span class="l ov ow ox bo oy oz pa pb pc ed">B</span><strong class="oa fr">yte-Pair Encoding</strong> is a subword tokenization method introduced in <a class="af pi" href="https://arxiv.org/abs/1508.07909" rel="noopener ugc nofollow" target="_blank">Neural Machine Translation of Rare Words with Subword Units</a> by Sennrich et al. in 2015. BPE starts with a base vocabulary consisting of all unique characters in the training data and iteratively merges the most frequent pairs of symbols — which can be characters or sequences of characters — to form new subwords. This process continues until the vocabulary reaches a predefined size, which is a hyperparameter you choose before training.</p><p id="4a0a" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Suppose we have the following words with their frequencies:</p><ul class=""><li id="ba9e" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“hug”</code> (10 occurrences)</li><li id="ddf1" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“pug”</code> (5 occurrences)</li><li id="8fdb" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“pun”</code> (12 occurrences)</li><li id="b5f7" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“bun”</code> (4 occurrences)</li><li id="13c8" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“hugs”</code> (5 occurrences)</li></ul><p id="50f5" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Our initial base vocabulary consists of the following characters: <code class="cx ps pt pu pv b">[“h”, “u”, “g”, “p”, “n”, “b”, “s”]</code>.</p><p id="02e6" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">We split the words into individual characters:</p><ul class=""><li id="2adc" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“h” “u” “g”</code> (hug)</li><li id="437a" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“p” “u” “g”</code> (pug)</li><li id="fd6f" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“p” “u” “n”</code> (pun)</li><li id="ed2e" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“b” “u” “n”</code> (bun)</li><li id="0f7f" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“h” “u” “g” “s”</code> (hugs)</li></ul><p id="553e" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Next, we count the frequency of each symbol pair:</p><ul class=""><li id="f2c5" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“h u”</code>: 15 times (from <code class="cx ps pt pu pv b">“hug”</code> and <code class="cx ps pt pu pv b">“hugs”</code>)</li><li id="46ba" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“u g”</code>: 20 times (from <code class="cx ps pt pu pv b">“hug”</code>, <code class="cx ps pt pu pv b">“pug”</code>, <code class="cx ps pt pu pv b">“hugs”</code>)</li><li id="bef5" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“p u”</code>: 17 times (from <code class="cx ps pt pu pv b">“pug”</code>, <code class="cx ps pt pu pv b">“pun”</code>)</li><li id="8f9f" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“u n”</code>: 16 times (from <code class="cx ps pt pu pv b">“pun”</code>, <code class="cx ps pt pu pv b">“bun”</code>)</li></ul><p id="f8b7" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">The most frequent pair is <code class="cx ps pt pu pv b">“u g”</code> (20 times), so we merge <code class="cx ps pt pu pv b">“u”</code> and <code class="cx ps pt pu pv b">“g”</code> to form <code class="cx ps pt pu pv b">“ug”</code> and update our words:</p><ul class=""><li id="67dd" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“h” “ug”</code> (hug)</li><li id="7c78" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“p” “ug”</code> (pug)</li><li id="da00" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“p” “u” “n”</code> (pun)</li><li id="693f" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“b” “u” “n”</code> (bun)</li><li id="b621" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><code class="cx ps pt pu pv b">“h” “ug” “s”</code> (hugs)</li></ul><p id="546f" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">We continue this process, merging the next most frequent pairs, such as <code class="cx ps pt pu pv b">“u n”</code> into <code class="cx ps pt pu pv b">“un”</code>, until we reach our desired vocabulary size.</p><p id="4c05" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">BPE controls the vocabulary size by specifying the number of merge operations. Frequent words remain intact, reducing the need for extensive memorization. And, rare or unseen words can be represented through combinations of known subwords. It’s used in models like <a class="af pi" href="https://openai.com/index/language-unsupervised/" rel="noopener ugc nofollow" target="_blank"><strong class="oa fr">GPT</strong></a> and <a class="af pi" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"><strong class="oa fr">RoBERTa</strong></a>.</p><p id="2fd1" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">The Hugging Face tokenizers library provides a fast and flexible way to train and use tokenizers, including BPE.</p><h2 id="e479" class="qj nd fq bf ne qk ql qm nh qn qo qp nk oh qq qr qs ol qt qu qv op qw qx qy qz bk">Training a BPE Tokenizer</h2><p id="f64d" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Here’s how to train a BPE tokenizer on a sample dataset:</p><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="9761" class="pz nd fq pv b bg qa qb l qc qd">from tokenizers import Tokenizer<br/>from tokenizers.models import BPE<br/>from tokenizers.trainers import BpeTrainer<br/>from tokenizers.pre_tokenizers import Whitespace<br/><br/># Initialize a tokenizer<br/>tokenizer = Tokenizer(BPE())<br/><br/># Set the pre-tokenizer to split on whitespace<br/>tokenizer.pre_tokenizer = Whitespace()<br/><br/># Initialize a trainer with desired vocabulary size<br/>trainer = BpeTrainer(vocab_size=1000, min_frequency=2, special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])<br/><br/># Files to train on<br/>files = ["path/to/your/dataset.txt"]<br/><br/># Train the tokenizer<br/>tokenizer.train(files, trainer)<br/><br/># Save the tokenizer<br/>tokenizer.save("bpe-tokenizer.json")</span></pre><p id="04fb" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk"><strong class="oa fr">Using the trained BPE Tokenizer:</strong></p><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="d9a2" class="pz nd fq pv b bg qa qb l qc qd">from tokenizers import Tokenizer<br/><br/># Load the tokenizer<br/>tokenizer = Tokenizer.from_file("bpe-tokenizer.json")<br/><br/># Encode a text input<br/>encoded = tokenizer.encode("I have a new GPU!")<br/>print("Tokens:", encoded.tokens)<br/>print("IDs:", encoded.ids)</span></pre><p id="776f" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk"><strong class="oa fr">Output:</strong></p><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="6935" class="pz nd fq pv b bg qa qb l qc qd">Tokens: ['I', 'have', 'a', 'new', 'GP', 'U', '!']<br/>IDs: [12, 45, 7, 89, 342, 210, 5]</span></pre><h1 id="4704" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">WordPiece</h1><p id="92d0" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk ou"><span class="l ov ow ox bo oy oz pa pb pc ed">W</span><strong class="oa fr">ordPiece </strong>is another subword tokenization algorithm, outlined by <a class="af pi" href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf" rel="noopener ugc nofollow" target="_blank">Schuster and Nakajima in 2012</a> and popularized by models like <a class="af pi" href="https://arxiv.org/abs/1810.04805#" rel="noopener ugc nofollow" target="_blank"><strong class="oa fr">BERT</strong></a>. Similar to BPE, WordPiece starts with all unique characters but differs in how it selects which symbol pairs to merge.</p><p id="1601" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Here’s how WordPiece works:</p><ol class=""><li id="29bb" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot rb pl pm bk"><strong class="oa fr">Initialization</strong>: Start with a vocabulary of all unique characters.</li><li id="3535" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot rb pl pm bk"><strong class="oa fr">Pre-tokenization</strong>: Split the training text into words.</li><li id="1a3e" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot rb pl pm bk"><strong class="oa fr">Building the Vocabulary</strong>: Iteratively add new symbols (subwords) to the vocabulary.</li><li id="18af" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot rb pl pm bk"><strong class="oa fr">Selection Criterion</strong>: Instead of choosing the most frequent symbol pair, WordPiece selects the pair that maximizes the likelihood of the training data when added to the vocabulary.</li></ol><p id="16cb" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Using the same word frequencies as before, WordPiece evaluates which symbol pair, when merged, would most increase the probability of the training data. This involves a more probabilistic approach compared to BPE’s frequency-based method.</p><p id="73e9" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Similar to BPE, we can train a WordPiece tokenizer using the <code class="cx ps pt pu pv b">tokenizers</code> library.</p><h2 id="b0da" class="qj nd fq bf ne qk ql qm nh qn qo qp nk oh qq qr qs ol qt qu qv op qw qx qy qz bk">Training a WordPiece Tokenizer</h2><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="c514" class="pz nd fq pv b bg qa qb l qc qd">from tokenizers import Tokenizer<br/>from tokenizers.models import WordPiece<br/>from tokenizers.trainers import WordPieceTrainer<br/>from tokenizers.pre_tokenizers import Whitespace<br/><br/># Initialize a tokenizer<br/>tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))<br/><br/># Set the pre-tokenizer<br/>tokenizer.pre_tokenizer = Whitespace()<br/><br/># Initialize a trainer<br/>trainer = WordPieceTrainer(vocab_size=1000, min_frequency=2, special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])<br/><br/># Train the tokenizer<br/>tokenizer.train(files, trainer)<br/><br/># Save the tokenizer<br/>tokenizer.save("wordpiece-tokenizer.json")</span></pre><p id="9d19" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk"><strong class="oa fr">Using the trained WordPiece tokenizer:</strong></p><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="b5f0" class="pz nd fq pv b bg qa qb l qc qd">from tokenizers import Tokenizer<br/><br/># Load the tokenizer<br/>tokenizer = Tokenizer.from_file("wordpiece-tokenizer.json")<br/><br/># Encode a text input<br/>encoded = tokenizer.encode("I have a new GPU!")<br/>print("Tokens:", encoded.tokens)<br/>print("IDs:", encoded.ids)</span></pre><p id="53a8" class="pw-post-body-paragraph ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk"><strong class="oa fr">Output:</strong></p><pre class="mm mn mo mp mq pw pv px bp py bb bk"><span id="ba9b" class="pz nd fq pv b bg qa qb l qc qd">Tokens: ['I', 'have', 'a', 'new', 'G', '##PU', '!']<br/>IDs: [10, 34, 5, 78, 301, 502, 8]</span></pre><h1 id="a07a" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk"><strong class="al">Conclusion</strong></h1><p id="1319" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk ou"><span class="l ov ow ox bo oy oz pa pb pc ed">T</span>okenization is a foundational step in NLP that prepares text data for computational models. By understanding and implementing appropriate tokenization strategies, we enable models to process and generate human language more effectively, setting the stage for advanced topics like word embeddings and language modeling.</p><blockquote class="qg qh qi"><p id="2ad4" class="ny nz pj oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">All the code in this article is also available on my GitHub repo: <a class="af pi" href="https://github.com/murilogustineli/nlp-medium" rel="noopener ugc nofollow" target="_blank"><strong class="oa fr">github.com/murilogustineli/nlp-medium</strong></a></p></blockquote><h2 id="277e" class="qj nd fq bf ne qk ql qm nh qn qo qp nk oh qq qr qs ol qt qu qv op qw qx qy qz bk">Other Resources</h2><ul class=""><li id="f3ff" class="ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot pk pl pm bk"><a class="af pi" href="https://www.youtube.com/watch?v=zduSFxRajkE" rel="noopener ugc nofollow" target="_blank">Let’s build the GPT Tokenizer | Andrej Karpathy on YouTube</a></li><li id="d016" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><a class="af pi" href="https://docs.mistral.ai/guides/tokenization/" rel="noopener ugc nofollow" target="_blank">Tokenization | Mistral AI Large Language Models</a></li><li id="b0a8" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><a class="af pi" href="https://huggingface.co/docs/transformers/en/tokenizer_summary" rel="noopener ugc nofollow" target="_blank">Summary of the tokenizers | Hugging Face</a></li><li id="2cb3" class="ny nz fq oa b go pn oc od gr po of og oh pp oj ok ol pq on oo op pr or os ot pk pl pm bk"><a class="af pi" href="https://huggingface.co/learn/nlp-course/en/chapter6/8" rel="noopener ugc nofollow" target="_blank">Building a tokenizer, block by block | Hugging Face</a></li></ul><blockquote class="qg qh qi"><p id="31c3" class="ny nz pj oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot fj bk">Unless otherwise noted, all images are created by the author.</p></blockquote></div></div></div></div>    
</body>
</html>