- en: 'LLMs for Everyone: Running the HuggingFace Text Generation Inference in Google
    Colab'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/llms-for-everyone-running-the-huggingface-text-generation-inference-in-google-colab-5adb3218a137?source=collection_archive---------3-----------------------#2024-01-13](https://towardsdatascience.com/llms-for-everyone-running-the-huggingface-text-generation-inference-in-google-colab-5adb3218a137?source=collection_archive---------3-----------------------#2024-01-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Experimenting with Large Language Models for free (Part 3)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dmitryelj.medium.com/?source=post_page---byline--5adb3218a137--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page---byline--5adb3218a137--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5adb3218a137--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5adb3218a137--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page---byline--5adb3218a137--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5adb3218a137--------------------------------)
    ¬∑7 min read¬∑Jan 13, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7f6ab5d005b7e9089c8aa882f7f43e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Markus Spiske, [Unsplash](https://unsplash.com/@markusspiske)
  prefs: []
  type: TYPE_NORMAL
- en: In the [first part](/llms-for-everyone-running-langchain-and-a-mistralai-7b-model-in-google-colab-246ca94d7c4d)
    of the story, we used a free Google Colab instance to run a Mistral-7B model and
    extract information using the FAISS (Facebook AI Similarity Search) database.
    In the [second part](/llms-for-everyone-running-the-llama-13b-model-and-langchain-in-google-colab-68d88021cf0b)
    of the story, we used a LLaMA-13B model and a LangChain library to make a chat
    with text summarization and other features. In this part, I will show how to use
    a HuggingFace ü§ó [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index)
    (TGI). TGI is a toolkit that allows us to run a large language model (LLM) as
    a service. As in the previous parts, we will test it in the Google Colab instance,
    completely for free.
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Text Generation Inference (TGI) is a production-ready toolkit for deploying
    and serving large language models (LLMs). Running LLM as a service allows us to
    use it with different clients, from Python notebooks to mobile apps. It is interesting
    to test the TGI‚Äôs functionality, but it turned out that its system requirements
    are pretty high, and not everything works as smoothly as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: A free Google Colab instance provides only 12.7 GB of RAM, which is often not
    enough to load a 13B or even 7B model ‚Äúin one piece.‚Äù‚Ä¶
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
