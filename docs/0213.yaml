- en: Enhancing Cancer Detection with StyleGAN-2 ADA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用StyleGAN-2 ADA增强癌症检测
- en: 原文：[https://towardsdatascience.com/enhancing-cancer-detection-with-stylegan-2-ada-aee55ef99c5b?source=collection_archive---------5-----------------------#2024-01-22](https://towardsdatascience.com/enhancing-cancer-detection-with-stylegan-2-ada-aee55ef99c5b?source=collection_archive---------5-----------------------#2024-01-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/enhancing-cancer-detection-with-stylegan-2-ada-aee55ef99c5b?source=collection_archive---------5-----------------------#2024-01-22](https://towardsdatascience.com/enhancing-cancer-detection-with-stylegan-2-ada-aee55ef99c5b?source=collection_archive---------5-----------------------#2024-01-22)
- en: Data augmentation for data-deficient deep neural networks.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 针对数据匮乏的深度神经网络进行数据增强。
- en: '[](https://medium.com/@ianstebbs?source=post_page---byline--aee55ef99c5b--------------------------------)[![Ian
    Stebbins](../Images/50ece59dc136f7d41e02c046ea1216e4.png)](https://medium.com/@ianstebbs?source=post_page---byline--aee55ef99c5b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--aee55ef99c5b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--aee55ef99c5b--------------------------------)
    [Ian Stebbins](https://medium.com/@ianstebbs?source=post_page---byline--aee55ef99c5b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ianstebbs?source=post_page---byline--aee55ef99c5b--------------------------------)[![Ian
    Stebbins](../Images/50ece59dc136f7d41e02c046ea1216e4.png)](https://medium.com/@ianstebbs?source=post_page---byline--aee55ef99c5b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--aee55ef99c5b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--aee55ef99c5b--------------------------------)
    [Ian Stebbins](https://medium.com/@ianstebbs?source=post_page---byline--aee55ef99c5b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--aee55ef99c5b--------------------------------)
    ·8 min read·Jan 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--aee55ef99c5b--------------------------------)
    ·8分钟阅读·2024年1月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*By:* [*Ian Stebbins*](https://www.linkedin.com/in/ian-stebbins-244a1722b/)*,*
    [*Benjamin Goldfried*](https://www.linkedin.com/in/benjamin-goldfried/)*,* [*Ben
    Maizes*](https://www.linkedin.com/in/benjamin-maizes/)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者：* [*Ian Stebbins*](https://www.linkedin.com/in/ian-stebbins-244a1722b/)*,*
    [*Benjamin Goldfried*](https://www.linkedin.com/in/benjamin-goldfried/)*,* [*Ben
    Maizes*](https://www.linkedin.com/in/benjamin-maizes/)'
- en: '**Intro**'
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**简介**'
- en: Often for many domain-specific problems, a lack of data can hinder the effectiveness
    and even disallow the use of deep neural networks. Recent architectures of Generative
    Adversarial Networks (GANs), however, allow us to synthetically augment data,
    by creating new samples that capture intricate details, textures, and variations
    in the data distribution. This synthetic data can act as additional training input
    for deep neural networks, thus making domain tasks with limited data more feasible.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多领域特定的问题，数据的匮乏可能会妨碍深度神经网络的有效性，甚至使其无法使用。然而，生成对抗网络（GAN）的最新架构使我们能够通过创建捕捉数据分布中细节、纹理和变异的新样本来合成增强数据。这些合成数据可以作为深度神经网络的额外训练输入，从而使数据稀缺的领域任务变得更加可行。
- en: 'In this project, we applied NVIDIA StyleGAN-2 with Adaptive Discriminator Augmentation
    (ADA) to a small [Chest CT-Scan Dataset](https://www.kaggle.com/datasets/mohamedhanyyy/chest-ctscan-images/code?datasetId=839140&sortBy=voteCount)
    (Licensed under [Database: Open Database, Contents: © Original Authors](http://opendatacommons.org/licenses/odbl/1.0/))[1].
    Additionally, we built a CNN classifier to distinguish normal scans from those
    with tumors. By injecting varying proportions of synthetically generated data
    into the training of different models, we were able to evaluate the performance
    differences between models with all real data and those with a real-synthetic
    mix.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们使用了带有自适应判别器增强（ADA）的NVIDIA StyleGAN-2，应用于一个小型的[胸部CT扫描数据集](https://www.kaggle.com/datasets/mohamedhanyyy/chest-ctscan-images/code?datasetId=839140&sortBy=voteCount)（许可协议：[数据库：开放数据库，内容：©
    原作者](http://opendatacommons.org/licenses/odbl/1.0/)）[1]。此外，我们构建了一个CNN分类器，用于区分正常扫描与肿瘤扫描。通过将不同比例的合成生成数据注入到不同模型的训练过程中，我们能够评估仅使用真实数据与使用真实-合成混合数据模型之间的性能差异。
- en: '**StyleGAN-2 ADA**'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**StyleGAN-2 ADA**'
- en: 'StyleGAN-2 with ADA was first introduced by NVIDIA in the NeurIPS 2020 paper:
    [“Training Generative Adversarial Networks with Limited Data”](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/ada-paper.pdf)
    [2]. In the past, training GANs on small datasets typically led to the network
    discriminator overfitting. Thus rather than learning to distinguish between real
    and generated data, the discriminator tended to memorize the patterns of noise
    and outliers of the training set, rather than learn the general trends of the
    data distribution. To combat this, ADA dynamically adjusts the strength of data
    augmentation based on the degree of overfitting observed during training. This
    helps the model to generalize better and leads to better GAN performance on smaller
    datasets.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: StyleGAN-2与ADA首次由NVIDIA在2020年NeurIPS论文中提出：[“在有限数据上训练生成对抗网络”](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/ada-paper.pdf)
    [2]。过去，在小数据集上训练GAN通常会导致网络判别器过拟合。因此，判别器往往不是学习区分真实数据和生成数据，而是倾向于记住训练集中的噪声和异常值的模式，而不是学习数据分布的一般趋势。为了解决这个问题，ADA根据训练中观察到的过拟合程度动态调整数据增强的强度。这有助于模型更好地进行泛化，并在小数据集上实现更好的GAN性能。
- en: '**Augmenting The Dataset**'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**增强数据集**'
- en: To use the StyleGAN-2 ADA model, we used the official NVIDIA model implementation
    from GitHub, which can be found [here](https://github.com/NVlabs/stylegan3). *Note
    that this is the StyleGAN-3 repo but StyleGAN-2 can still be run.*
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用StyleGAN-2 ADA模型，我们使用了来自GitHub的官方NVIDIA模型实现，具体可以在[这里](https://github.com/NVlabs/stylegan3)找到。*请注意，这是StyleGAN-3的仓库，但仍然可以运行StyleGAN-2。*
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Depending on your setup you may have to install dependencies and do some other
    preprocessing. For example, we chose to resize and shrink our dataset images to
    224x224 since we only had access to a single GPU, and using larger image sizes
    is much more computationally expensive. We chose to use 224x224 because ResNet,
    the pre-trained model we chose for the CNN, is optimized to work with this size
    of image.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的设置，你可能需要安装依赖项并进行一些其他预处理。例如，我们选择将数据集图像调整大小并缩小为224x224，因为我们只有一个GPU，使用更大的图像尺寸在计算上更加昂贵。我们选择使用224x224的图像尺寸，因为我们为CNN选择的预训练模型ResNet优化了这种图像大小。
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To begin the training process, navigate to the directory where you cloned the
    repo and then run the following.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练过程，首先导航到你克隆的仓库目录，然后运行以下命令。
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**SNAP** refers to the number of Ticks (training steps where information is
    displayed) after which you would like to take a snapshot of your network and save
    it to a pickle file.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**SNAP**指的是你希望在多少个训练步骤（信息显示的训练步数）后拍摄网络快照并将其保存到pickle文件中。'
- en: '**KIMG** refers to the number of thousands of images you want to feed into
    your GAN.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**KIMG**指的是你希望输入到GAN中的成千上万的图像数量。'
- en: '**GAMMA d**etermines how strongly the regularization affects the discriminator.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**GAMMA d**决定正则化对判别器的影响强度。'
- en: '![](../Images/b34210233f78736ace829dafc60eb73c.png)![](../Images/6a3268bd615e1c08b05b12fa8d74ea19.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b34210233f78736ace829dafc60eb73c.png)![](../Images/6a3268bd615e1c08b05b12fa8d74ea19.png)'
- en: Initial Generated Images
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 初始生成的图像
- en: '![](../Images/86495efb421750130e2efe65762c8570.png)![](../Images/948a9ccdfd5f21abf3313d44279cfa62.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86495efb421750130e2efe65762c8570.png)![](../Images/948a9ccdfd5f21abf3313d44279cfa62.png)'
- en: Generated Images During Training
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 训练期间生成的图像
- en: Once your model has finished training (this can take multiple hours depending
    on your compute resources) you can now use your trained network to generate images.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的模型完成训练（根据你的计算资源，这可能需要多个小时），你就可以使用训练好的网络生成图像。
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/43a52ee7c15a0a27c8aee5c3c86606fd.png)![](../Images/7496b11702ef63c7c146d11d852d35d2.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43a52ee7c15a0a27c8aee5c3c86606fd.png)![](../Images/7496b11702ef63c7c146d11d852d35d2.png)'
- en: Normal Real Image (Left) vs Normal Generated Image (Right)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正常真实图像（左）与正常生成图像（右）
- en: '**Transfer Learning & Convolutional Neural Network**'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**迁移学习与卷积神经网络**'
- en: To benchmark the effectiveness of our synthetically generated data, we first
    trained a CNN model on our original data. Once we had a benchmark accuracy on
    the test set, we re-trained the model with increasing amounts of synthetic data
    in the training mix.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们合成生成数据的有效性，我们首先在原始数据上训练了一个CNN模型。一旦我们得到了测试集上的基准准确度，我们就用逐渐增加的合成数据重新训练了模型。
- en: To feed our data into the model we used Keras data generators which flow the
    samples directly from a specified directory into the model. The original dataset
    has 4 classes for different types of cancer, however, for simplicity, we turned
    this into a binary classification problem. The two classes we decided to work
    with from the original Kaggle dataset were the normal and squamous classes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据输入模型，我们使用了Keras数据生成器，它将样本直接从指定目录流入模型。原始数据集有4个类别，分别代表不同类型的癌症，但为了简化问题，我们将其转化为二分类问题。我们决定从原始Kaggle数据集中选择正常类和鳞状类进行处理。
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To build our model, we began by using the ResNet50 base architecture and model
    weights. We chose to use ResNet50 due to its moderate-size architecture, good
    documentation, and ease of use through Keras. After importing ResNet50 with the
    Imagenet model weights, we then froze the ResNet50 layers and added trainable
    dense layers on top to help the network learn our specific classification task.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的模型，我们首先使用了ResNet50基础架构和模型权重。我们选择使用ResNet50是因为它具有适中的架构大小、良好的文档支持，并且通过Keras易于使用。在导入带有Imagenet模型权重的ResNet50后，我们冻结了ResNet50的层，并在其上添加了可训练的密集层，帮助网络学习我们特定的分类任务。
- en: We also chose to incorporate batch normalization, which can lead to faster convergence
    and more stable training by normalizing layer inputs and reducing internal covariate
    shift [3]. Additionally, it can provide a regularization effect that can help
    prevent overfitting in our added trainable dense layers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还选择了引入批量归一化（batch normalization），通过对层输入进行归一化并减少内部协变量偏移，能够加速收敛并使训练更加稳定[3]。此外，它还可以提供一种正则化效果，有助于防止我们添加的可训练密集层发生过拟合。
- en: '![](../Images/563757ec5c2f1f0f7c1b6a808ac3f16b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/563757ec5c2f1f0f7c1b6a808ac3f16b.png)'
- en: Our Model Architecture
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型架构
- en: Originally, our model was not performing well. We solved this issue by switching
    our activation function from ReLU to leaky ReLU. This suggested that our network
    may have been facing the dying ReLU or dead neuron problem. In short, since the
    gradient of ReLU will always be zero for negative numbers, this can lead to neurons
    “dying” and not contributing to the network [4][5]. Since leaky ReLU is nonzero
    for negative values, using it as an activation function can help combat this issue.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，我们的模型表现不佳。我们通过将激活函数从ReLU切换到leaky ReLU解决了这个问题。这表明我们的网络可能正面临着ReLU死亡或神经元失效的问题。简而言之，由于ReLU对负数的梯度始终为零，这可能导致神经元“死亡”，从而无法对网络作出贡献[4][5]。由于leaky
    ReLU对负值不为零，使用它作为激活函数有助于解决这个问题。
- en: '**Results**'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结果**'
- en: To test our synthetic data, we trained the above CNN on 5 separate instances
    with 0%, 25%, 50%, 75%, and 100% additional synthetic samples. For example, 0%
    synthetic samples meant that the data was all original, while 100% meant the training
    set contained equal amounts of original and synthetic data. For each network,
    we then evaluated the performance using an accuracy metric on a real set of unseen
    test data. The plot below visualizes how different proportions of synthetic data
    affect the testing accuracy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的合成数据，我们在5个不同实例上训练了上述CNN，分别使用了0%、25%、50%、75%和100%的附加合成样本。例如，0%的合成样本意味着数据全部为原始数据，而100%则意味着训练集包含相等数量的原始数据和合成数据。对于每个网络，我们随后使用准确度指标在一组未见过的实际测试数据上评估了性能。下图可视化了不同合成数据比例对测试准确率的影响。
- en: '![](../Images/9a3fef522d3001511884da078b5b2dc7.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a3fef522d3001511884da078b5b2dc7.png)'
- en: Test Accuracy on Binary (Normal vs Squamous Tumor) Classification
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类（正常与鳞状肿瘤）分类的测试准确率
- en: Training the model was unstable, thus we ruled out iterations where the accuracy
    was 1.0 or extremely low. This helped us avoid training iterations that were under
    or over fit.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型时不稳定，因此我们排除了准确度为1.0或极低的迭代。这帮助我们避免了过拟合或欠拟合的训练迭代。
- en: We can see that from 0 to 25% we see a sharp increase in the testing accuracy,
    suggesting that even augmenting the dataset by a small amount can have a large
    impact on problems where the data is initially minimal.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，从0%到25%时，测试准确度出现了急剧上升，这表明即使是通过少量的数据增强，也能对数据最初较少的问题产生较大影响。
- en: Since we only trained our GAN on 80 KIMG (due to compute limitations) the quality
    of our synthetic data could have potentially been better, given more GAN training
    iterations. Notably, an increase in synthetic data quality could also influence
    the graph above. We hypothesize that an increase in synthetic quality will also
    lead to an increase in the optimal proportion of synthetic data used in training.
    Further, if the synthetic images were better able to fit the real distribution
    of our training data, we could incorporate more of them in model training without
    overfitting.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们仅在80 KIMG上训练了我们的GAN（由于计算限制），因此如果有更多的GAN训练迭代，我们的合成数据质量可能会更好。值得注意的是，合成数据质量的提高也可能影响上述图表。我们假设，合成数据质量的提高将导致在训练中使用合成数据的最佳比例增加。此外，如果合成图像更好地适应我们训练数据的真实分布，我们就可以在模型训练中融入更多的合成图像而不至于过拟合。
- en: '**Conclusion**'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In this project, using GANs for the augmentation of limited data has shown to
    be an effective technique for expanding training sets and more importantly, improving
    classification accuracy. While we opted for a small and basic problem, this could
    easily be upscaled in a few ways. Future work may include using more computational
    resources to get better synthetic samples, introducing more classes into the classification
    task (making it a multi-class problem), and experimenting with newer GAN architectures.
    Regardless, using GANs to augment small datasets can now bring many previously
    data-limited problems into the scope of deep neural networks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，使用GAN进行有限数据的增强被证明是扩展训练集并且更重要的是提高分类精度的有效技术。尽管我们选择了一个小而基础的问题，但这一技术可以通过几种方式轻松扩展。未来的工作可能包括使用更多的计算资源来获取更好的合成样本，引入更多的类别到分类任务中（使其成为一个多分类问题），并实验更新的GAN架构。不管怎样，使用GAN增强小数据集现在可以将许多以前受数据限制的问题纳入深度神经网络的范畴。
- en: Kaggle Dataset
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kaggle 数据集
- en: We compiled our augmented and resized images into the following [Kaggle dataset](https://www.kaggle.com/datasets/benjaminmaizes/formatted-and-augmented-chest-ct-scan-images).
    This contains 501 normal and 501 squamous 224x224 synthetic images which can be
    used for further experimentation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将增强和重新调整大小的图像汇编成以下[Kaggle数据集](https://www.kaggle.com/datasets/benjaminmaizes/formatted-and-augmented-chest-ct-scan-images)。该数据集包含501张正常和501张鳞状的224x224合成图像，可用于进一步的实验。
- en: '[Our *GitHub Repo*](https://github.com/istebbins/Enhancing-Cancer-Detection-with-StyleGAN-2-ADA)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[我们的*GitHub 仓库*](https://github.com/istebbins/Enhancing-Cancer-Detection-with-StyleGAN-2-ADA)'
- en: '**Citations**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**引用**'
- en: '[1] Hany, Mohamed, [Chest CT-Scan images Dataset](https://www.kaggle.com/datasets/mohamedhanyyy/chest-ctscan-images/code?datasetId=839140&sortBy=voteCount),
    Kaggle (2020).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Hany, Mohamed， [胸部CT扫描图像数据集](https://www.kaggle.com/datasets/mohamedhanyyy/chest-ctscan-images/code?datasetId=839140&sortBy=voteCount)，Kaggle（2020）。'
- en: '[2] Karras, Tero, et al, [Training Generative Adversarial Networks with Limited
    Data](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/ada-paper.pdf) (2020), Advances
    in neural information processing systems 2020.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Karras, Tero，等， [使用有限数据训练生成对抗网络](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/ada-paper.pdf)（2020），《神经信息处理系统进展》2020。'
- en: '[3] Ioffe, Sergey, and Christian Szegedy, [Batch normalization: Accelerating
    deep network training by reducing internal covariate shift](http://proceedings.mlr.press/v37/ioffe15.pdf),
    (2015), *International conference on machine learning*. pmlr, 2015.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Ioffe, Sergey, 和 Christian Szegedy， [批量归一化：通过减少内部协变量偏移加速深度网络训练](http://proceedings.mlr.press/v37/ioffe15.pdf)，（2015），*国际机器学习会议*。pmlr，2015。'
- en: '[4] He, Kaiming, et al, [Delving deep into rectifiers: Surpassing human-level
    performance on imagenet classification](https://arxiv.org/pdf/1502.01852.pdf),
    (2015), *Proceedings of the IEEE international conference on computer vision*.
    2015.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] He, Kaiming，等， [深入探讨修正器：在imagenet分类中超越人类级别的表现](https://arxiv.org/pdf/1502.01852.pdf)，（2015），*IEEE国际计算机视觉会议论文集*。2015。'
- en: '[5]Bai, Yuhan, [RELU-function and derived function review](https://www.shs-conferences.org/articles/shsconf/pdf/2022/14/shsconf_stehf2022_02006.pdf),
    (2022), *SHS Web of Conferences*. Vol. 144\. EDP Sciences, 2022.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Bai, Yuhan， [RELU函数及其导函数回顾](https://www.shs-conferences.org/articles/shsconf/pdf/2022/14/shsconf_stehf2022_02006.pdf)，（2022），*SHS
    Web of Conferences*。第144卷。EDP Sciences，2022。'
