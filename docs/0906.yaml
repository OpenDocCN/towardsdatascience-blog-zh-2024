- en: 5 Ways to Serve Open Source LLMs (With Code Examples)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5种服务开源LLM的方法（带代码示例）
- en: 原文：[https://towardsdatascience.com/5-ways-to-serve-open-source-llms-with-code-examples-39e02cdd4a70?source=collection_archive---------8-----------------------#2024-04-09](https://towardsdatascience.com/5-ways-to-serve-open-source-llms-with-code-examples-39e02cdd4a70?source=collection_archive---------8-----------------------#2024-04-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/5-ways-to-serve-open-source-llms-with-code-examples-39e02cdd4a70?source=collection_archive---------8-----------------------#2024-04-09](https://towardsdatascience.com/5-ways-to-serve-open-source-llms-with-code-examples-39e02cdd4a70?source=collection_archive---------8-----------------------#2024-04-09)
- en: Code and Instructions for each method applied to Llama 2 7B
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适用于Llama 2 7B的每种方法的代码和说明
- en: '[](https://medium.com/@CVxTz?source=post_page---byline--39e02cdd4a70--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--39e02cdd4a70--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--39e02cdd4a70--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--39e02cdd4a70--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--39e02cdd4a70--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@CVxTz?source=post_page---byline--39e02cdd4a70--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--39e02cdd4a70--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--39e02cdd4a70--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--39e02cdd4a70--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--39e02cdd4a70--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--39e02cdd4a70--------------------------------)
    ·10 min read·Apr 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--39e02cdd4a70--------------------------------)
    ·阅读时间：10分钟·2024年4月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/41df84ad14cb436bcece2b94c2995562.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41df84ad14cb436bcece2b94c2995562.png)'
- en: Photo by [Kyaw Tun](https://unsplash.com/@kyawthutun?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/blue-siamese-fighting-fish-k6BHLfw_jUg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Kyaw Tun](https://unsplash.com/@kyawthutun?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)拍摄，来自[Unsplash](https://unsplash.com/photos/blue-siamese-fighting-fish-k6BHLfw_jUg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: In the ever-evolving realm of Large Language Models (LLMs), the tools and techniques
    for serving them are advancing at a pace as swift as the models themselves. Unlike
    conventional models like xgboost or MNIST classifier CNN, LLMs are vast in size
    and complexity, demanding more meticulous attention to deploy effectively.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速发展的大型语言模型（LLMs）领域，服务它们的工具和技术正以前所未有的速度进步，几乎与这些模型本身的速度一样快。与xgboost或MNIST分类器CNN等传统模型不同，LLM模型在规模和复杂性上都要庞大得多，因此在部署时需要更加细致的关注。
- en: In this blog post, our spotlight falls on Open Source LLMs, which stand out
    as perhaps the most advantageous due to their tunability and hackability, allowing
    anyone to contribute and drive progress in the field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的焦点集中在开源LLM上，它们因可调性和可操作性而脱颖而出，可能是最具优势的选择，因为它们允许任何人参与并推动这一领域的进步。
- en: My aim here is to guide you through various methods of serving LLMs, catering
    to diverse use cases. I’ll present five distinct options, each accompanied by
    comprehensive instructions for replication, and a thorough examination of their
    respective pros and cons.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我的目标是引导你了解多种服务LLM的方法，满足不同的使用场景。我将介绍五种不同的选项，每种选项都附有详细的复制指南，并对其各自的优缺点进行全面分析。
- en: We’ll explore options for both local deployment and utilizing managed services.
    What’s more, the services we’ll discuss offer generous free credits, enabling
    you to experiment without spending a penny.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨本地部署和使用托管服务的选项。更重要的是，我们将讨论的这些服务提供了丰厚的免费额度，帮助你在不花一分钱的情况下进行实验。
- en: 'Here’s what’s on the menu:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可用的选项：
- en: 'Local Server: Anaconda + CPU'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本地服务器：Anaconda + CPU
- en: 'Local Server: Anaconda + GPU'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本地服务器：Anaconda + GPU
