- en: 5 Ways to Serve Open Source LLMs (With Code Examples)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/5-ways-to-serve-open-source-llms-with-code-examples-39e02cdd4a70?source=collection_archive---------8-----------------------#2024-04-09](https://towardsdatascience.com/5-ways-to-serve-open-source-llms-with-code-examples-39e02cdd4a70?source=collection_archive---------8-----------------------#2024-04-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Code and Instructions for each method applied to Llama 2 7B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@CVxTz?source=post_page---byline--39e02cdd4a70--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--39e02cdd4a70--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--39e02cdd4a70--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--39e02cdd4a70--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--39e02cdd4a70--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--39e02cdd4a70--------------------------------)
    ·10 min read·Apr 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41df84ad14cb436bcece2b94c2995562.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Kyaw Tun](https://unsplash.com/@kyawthutun?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/blue-siamese-fighting-fish-k6BHLfw_jUg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: In the ever-evolving realm of Large Language Models (LLMs), the tools and techniques
    for serving them are advancing at a pace as swift as the models themselves. Unlike
    conventional models like xgboost or MNIST classifier CNN, LLMs are vast in size
    and complexity, demanding more meticulous attention to deploy effectively.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, our spotlight falls on Open Source LLMs, which stand out
    as perhaps the most advantageous due to their tunability and hackability, allowing
    anyone to contribute and drive progress in the field.
  prefs: []
  type: TYPE_NORMAL
- en: My aim here is to guide you through various methods of serving LLMs, catering
    to diverse use cases. I’ll present five distinct options, each accompanied by
    comprehensive instructions for replication, and a thorough examination of their
    respective pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll explore options for both local deployment and utilizing managed services.
    What’s more, the services we’ll discuss offer generous free credits, enabling
    you to experiment without spending a penny.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what’s on the menu:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Local Server: Anaconda + CPU'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Local Server: Anaconda + GPU'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
