<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>From MOCO v1 to v3: Towards Building a Dynamic Dictionary for Self-Supervised Learning — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>From MOCO v1 to v3: Towards Building a Dynamic Dictionary for Self-Supervised Learning — Part 1</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-moco-v1-to-v3-towards-building-a-dynamic-dictionary-for-self-supervised-learning-part-1-745dc3b4e861?source=collection_archive---------9-----------------------#2024-07-04">https://towardsdatascience.com/from-moco-v1-to-v3-towards-building-a-dynamic-dictionary-for-self-supervised-learning-part-1-745dc3b4e861?source=collection_archive---------9-----------------------#2024-07-04</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c79e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A gentle recap on the momentum contrast learning framework</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://mengliuz.medium.com/?source=post_page---byline--745dc3b4e861--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mengliu Zhao" class="l ep by dd de cx" src="../Images/0b950a0785fa065db3319ed5be4a91de.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*siAyGzGqa7K3xsa639R_2w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--745dc3b4e861--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://mengliuz.medium.com/?source=post_page---byline--745dc3b4e861--------------------------------" rel="noopener follow">Mengliu Zhao</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--745dc3b4e861--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 4, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="c26c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Have we reached the era of self-supervised learning?</strong></p><p id="ec99" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Data is flowing in every day. People are working 24/7. Jobs are distributed to every corner of the world. But still, so much data is left unannotated, waiting for the possible use by a new model, a new training, or a new upgrade.</p><p id="d8ca" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Or, it will never happen. It will never happen when the world is running in a supervised fashion.</p><p id="b4ed" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The rise of self-supervised learning in recent years has unveiled a new direction. Instead of creating annotations for all tasks, self-supervised learning breaks tasks into pretext/pre-training (see my previous post on pre-training <a class="af ne" href="https://medium.com/towards-data-science/from-masked-image-modeling-to-autoregressive-image-modeling-d9a3cadf72a1" rel="noopener">here</a>) tasks and downstream tasks. The pretext tasks focus on extracting representative features from the whole dataset without the guidance of any ground truth annotations. Still, this task requires labels generated automatically from the dataset, usually by extensive data augmentation. Hence, we use the terminologies <strong class="mk fr">unsupervised learning</strong> (dataset is unannotated) and <strong class="mk fr">self-supervised learning</strong> (tasks are supervised by self-generated labels) interchangeably in this article.</p><p id="2a90" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Contrastive learning is a major category of self-supervised learning</strong>. It uses unlabelled datasets and contrastive information-encoded losses (e.g., contrastive loss, InfoNCE loss, triplet loss, etc.) to train the deep learning network. Major contrastive learning includes SimCLR, SimSiam, and the MOCO series.</p><p id="7f91" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">MOCO — the word is an abbreviation for “momentum contrast.” The core idea was written in the first MOCO paper, suggesting the understanding of a computer vision self-supervised learning problem, as follows:</p><p id="c842" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="nf">“[quote from </em><a class="af ne" href="https://arxiv.org/pdf/1911.05722" rel="noopener ugc nofollow" target="_blank"><em class="nf">original paper</em></a><em class="nf">] Computer vision, in contrast, further concerns dictionary building, as the raw signal is in a continuous, high-dimensional space and is not structured for human communication… Though driven by various motivations, these (note: recent visual representation learning) methods can be thought of as building dynamic dictionaries… </em><strong class="mk fr"><em class="nf">Unsupervised learning trains encoders to perform dictionary look-up: an encoded ‘query’ should be similar to its matching key and dissimilar to others</em></strong><em class="nf">. Learning is formulated as minimizing a contrastive loss.”</em></p><p id="3b7b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this article, we’ll do a gentle review of MOCO v1 to v3:</p><ul class=""><li id="a593" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ng nh ni bk">v1 — the paper “<a class="af ne" href="http://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html" rel="noopener ugc nofollow" target="_blank">Momentum contrast for unsupervised visual representation learning</a>” was published in CVPR 2020. The paper proposes a momentum update to key ResNet encoders using sample queues with InfoNCE loss.</li><li id="0c9b" class="mi mj fq mk b go nj mm mn gr nk mp mq mr nl mt mu mv nm mx my mz nn nb nc nd ng nh ni bk">v2 — the paper “ Improved baselines with momentum contrastive learning” came out immediately after, implementing two SimCLR architecture improvements: a) replacing the FC layer with a 2-layer MLP and b) extending the original data augmentation by including blur.</li><li id="9f19" class="mi mj fq mk b go nj mm mn gr nk mp mq mr nl mt mu mv nm mx my mz nn nb nc nd ng nh ni bk">v3 — the paper “An empirical study of training self-supervised vision transformers” was published in ICCV 2021. The framework extends the key-query pair to two key-query pairs, which were used to form a SimSiam-style symmetric contrastive loss. The backbone also got extended from ResNet-only to both ResNet and ViT.</li></ul><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np nq"><img src="../Images/dda2a08a360af0f07bd3728dd432aa2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aRw0h2SZQc-s9luMZVU7gg.jpeg"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">Image source: <a class="af ne" href="https://pxhere.com/en/photo/760197" rel="noopener ugc nofollow" target="_blank">https://pxhere.com/en/photo/760197</a></figcaption></figure></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="7151" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">MOCO V1</strong></p><p id="8d99" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The framework starts at a core self-supervised learning concept: <strong class="mk fr">query and keys</strong>. Here, query refers to the representation vector of the query image or patches (x^query), while keys refer to the representation vectors of the sample image/patch dictionaries ({x_0^key, x_1^key, …}). The query vector q is generated by a trainable “main” encoder with regular gradient backpropagation. The key vectors, stored in a dictionary queue, are generated by a trainable encoder, which doesn’t do gradient backpropagation directly but <strong class="mk fr">only updates the weights in a momentum fashion using the main encoder’s weights</strong>. See the update style below:</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div class="no np op"><img src="../Images/88f5658f2edadbd8df803bc6d4bc3be0.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*qwGB73ojlSZpHwOHLhHMug.png"/></div><figcaption class="oc od oe no np of og bf b bg z dx">Image source: <a class="af ne" href="https://arxiv.org/pdf/1911.05722" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1911.05722</a></figcaption></figure><p id="cd21" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Instance discrimination task and InfoNCE loss</strong></p><p id="330e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">A specific task is needed since the dataset does not have labels at the pretext/pre-training stage. The paper adopted the instance discrimination task proposed in <a class="af ne" href="https://arxiv.org/abs/1805.01978" rel="noopener ugc nofollow" target="_blank">this CVPR 2018 paper</a>. Unlike the original design, where the similarity between feature vectors in the memory bank was calculated using a non-parametric classifier, the MOCO paper used the positive +&lt;query, key&gt; pair and negative -&lt;query, key&gt; pair to supervise the learning process. A pair is considered positive when the query and key image are augmented from the same image. Otherwise, it is negative. The training loss is the <a class="af ne" href="https://arxiv.org/pdf/1807.03748" rel="noopener ugc nofollow" target="_blank">InfoNCE loss</a>, which can be considered as the negative logarithm of the softmax of the query/key pairs:</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div class="no np oq"><img src="../Images/3a804eee3aace41b596106edfe1ef9ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*nrSIxkTP4fa2yl5XdquUhA.png"/></div><figcaption class="oc od oe no np of og bf b bg z dx">Equation source: <a class="af ne" href="https://arxiv.org/pdf/1911.05722" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1911.05722</a></figcaption></figure><p id="4657" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Momentum update</strong></p><p id="c79f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The authors claim that copying the main query encoder to the key encoder would likely cause poor results because a rapidly changing encoder will reduce the key representation dictionary’s consistency. Instead, only the main query encoder is trained at each step, but the weights of the key encoder are updated using a momentum weight m:</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div class="no np or"><img src="../Images/0b9fd60b95b8d1c3aa764b82ab6ffd3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*ioDgazJs-OItoEUh4qAw2w.png"/></div><figcaption class="oc od oe no np of og bf b bg z dx">Equation source: <a class="af ne" href="https://arxiv.org/pdf/1911.05722" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1911.05722</a></figcaption></figure><p id="91a7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The momentum weight is kept large during the training, e.g., 0.999 rather than 0.9, which validates the authors’ guess that the key encoder’s consistency and stability affect the contrastive learning performance.</p><p id="3636" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Pseudocode</strong></p><p id="3068" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The less-than-20-lines pseudo code is a quick outline of the whole training process. Consistent with the InfoLoss shown above, it is worth noting that the positive logit is a single scale per sample, and the negative logit is a K-element vector per sample corresponding to the K keys.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div class="no np os"><img src="../Images/dc25876e6c6a6be41a2118cf31b3f12b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*jskmo4x5VTSd6pyq72PbQQ.png"/></div><figcaption class="oc od oe no np of og bf b bg z dx">Pseudocode from: <a class="af ne" href="https://arxiv.org/pdf/1911.05722" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1911.05722</a></figcaption></figure></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="46f5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">MOCO V2</strong></p><p id="1c73" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Released immediately after MOCO, the 2-page v2 paper proposed minor changes to version 1 by adopting two successful architecture changes from SimCLR:</p><ul class=""><li id="4a95" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ng nh ni bk">replacing the fully connected layer of the ResNet encoder with a 2-layer MLP</li><li id="e3bc" class="mi mj fq mk b go nj mm mn gr nk mp mq mr nl mt mu mv nm mx my mz nn nb nc nd ng nh ni bk">extending the original augmentation set with blur augmentation</li></ul><p id="e982" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Interestingly, even with one simple architecture tweak, the performance boost seems significant:</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np ot"><img src="../Images/6d842ba1e192a83219ecafec8975afdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T-TroOdkGH9qJqEtCd6GLA.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">Image source: <a class="af ne" href="https://arxiv.org/pdf/2003.04297" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2003.04297</a></figcaption></figure></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="1fb2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">MOCO V3</strong></p><p id="cadb" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Version 3 proposed major improvements by adopting a symmetric contrastive loss, extra projection head, and ViT encoder.</p><p id="5c0f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Symmetric contrastive loss</strong></p><p id="2d2f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Inspired by the SimSiam work, which takes two randomly augmented views and switches them in the negative cosine similarity computation to obtain a symmetric loss, MOCO v3 augments the sample twice. It feeds them separately to the query and key encoders.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np ou"><img src="../Images/1e16f89f803634cee1f310530d1272d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cM-ut0dIMbz3YAxxDi8BKA.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">Image source: <a class="af ne" href="https://arxiv.org/pdf/1911.05722" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1911.05722</a> and <a class="af ne" href="https://arxiv.org/abs/2104.02057" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2104.02057</a></figcaption></figure><p id="e0ed" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The symmetric contrastive loss is based on the simple assumption — all positive pairs are in the diagonal of the N*N query-key matrix since they are the augmentations of the same image; all negative pairs are on other locations of the N*N query-key matrix as they are augmentations (might be the same augmentation) from different samples:</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div class="no np ov"><img src="../Images/e7553907cd14dce1395aed58eb832258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*wXCtY20yEpoTZbBvZloGag.png"/></div><figcaption class="oc od oe no np of og bf b bg z dx">Image source: <a class="af ne" href="https://arxiv.org/abs/2104.02057" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2104.02057</a></figcaption></figure><p id="15a1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this sense, the dynamic key dictionary is much simpler as it’s calculated on the fly within the minibatch and doesn’t need to keep a memory queue. This can be validated by the stability analysis over batch sizes below (note the authors explained that the 6144-batch performance decreased because of the <a class="af ne" href="https://arxiv.org/pdf/2104.02057" rel="noopener ugc nofollow" target="_blank">partial failure phenomenon during training time</a>):</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np ow"><img src="../Images/3ef8e39e874c7e70617019ce85f83725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r0Ml9yfzoI1gCAyNZ7v2vQ.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">Image source: <a class="af ne" href="https://arxiv.org/pdf/2104.02057" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2104.02057</a></figcaption></figure><p id="1227" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">ViT encoder</strong></p><p id="9e17" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The performance boost by the ViT encoder is shown below:</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div class="no np ox"><img src="../Images/218a3243c511c15382c311f2927ab0d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*aLXFdTpUpk7PMnq6nTLM9Q.png"/></div><figcaption class="oc od oe no np of og bf b bg z dx">Image source: <a class="af ne" href="https://arxiv.org/pdf/2104.02057" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2104.02057</a></figcaption></figure></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="550e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Comparisons and summary</strong></p><p id="cf79" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The MOCO v3 paper gives a performance comparison among v1-v3 using ResNet50 (R50) encoder:</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np oy"><img src="../Images/2303db653ef6c32e0a45f98b02e24994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UBOwTQo8tscntle5TpUQrA.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">Image source: <a class="af ne" href="https://arxiv.org/pdf/2104.02057" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2104.02057</a></figcaption></figure><p id="babe" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In summary, MOCO v1-v3 gave a clear transformation of the following elements:</p><ul class=""><li id="12e7" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ng nh ni bk"><strong class="mk fr">Encoder</strong>: ResNet → ResNet + MLP layers → ResNet/ViT + MLP layers</li><li id="fce3" class="mi mj fq mk b go nj mm mn gr nk mp mq mr nl mt mu mv nm mx my mz nn nb nc nd ng nh ni bk"><strong class="mk fr">Key dictionary</strong>: global key vector queues → minibatch keys based on augmentation</li><li id="0b2b" class="mi mj fq mk b go nj mm mn gr nk mp mq mr nl mt mu mv nm mx my mz nn nb nc nd ng nh ni bk"><strong class="mk fr">Contrastive loss</strong>: asymmetric contrastive loss → symmetric contrastive loss</li></ul><p id="0ea5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">But there is more. In the next article, I will dive deep into the <a class="af ne" href="https://github.com/facebookresearch/moco-v3/tree/main" rel="noopener ugc nofollow" target="_blank">MOCO v3 code</a> to implement data augmentation and the momentum update. Stay tuned!</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4023" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">References</strong></p><ul class=""><li id="61d5" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ng nh ni bk">Wu et al., Unsupervised feature learning via non-parametric instance discrimination. CVPR 2018. github: <a class="af ne" href="https://github.com/zhirongw/lemniscate.pytorch" rel="noopener ugc nofollow" target="_blank">https://github.com/zhirongw/lemniscate.pytorch</a></li><li id="3ea7" class="mi mj fq mk b go nj mm mn gr nk mp mq mr nl mt mu mv nm mx my mz nn nb nc nd ng nh ni bk">Oord et al., Representation learning with contrastive predictive coding. arXiv preprint 2018.</li><li id="07d9" class="mi mj fq mk b go nj mm mn gr nk mp mq mr nl mt mu mv nm mx my mz nn nb nc nd ng nh ni bk">Chen et al., A simple framework for contrastive learning of visual representations. PMLR 2020. github: <a class="af ne" href="https://github.com/sthalles/SimCLR" rel="noopener ugc nofollow" target="_blank">https://github.com/sthalles/SimCLR</a></li><li id="a4b7" class="mi mj fq mk b go nj mm mn gr nk mp mq mr nl mt mu mv nm mx my mz nn nb nc nd ng nh ni bk">He et al., Momentum contrast for unsupervised visual representation learning. CVPR 2020.</li><li id="4bbd" class="mi mj fq mk b go nj mm mn gr nk mp mq mr nl mt mu mv nm mx my mz nn nb nc nd ng nh ni bk">Chen et al., Improved baselines with momentum contrastive learning. arXiv preprint 2020.</li><li id="7f16" class="mi mj fq mk b go nj mm mn gr nk mp mq mr nl mt mu mv nm mx my mz nn nb nc nd ng nh ni bk">Chen et al., Exploring simple siamese representation learning. CVPR 2021. github: <a class="af ne" href="https://github.com/facebookresearch/simsiam" rel="noopener ugc nofollow" target="_blank">https://github.com/facebookresearch/simsiam</a></li><li id="6a5d" class="mi mj fq mk b go nj mm mn gr nk mp mq mr nl mt mu mv nm mx my mz nn nb nc nd ng nh ni bk">Chen et al., An Empirical Study of Training Self-Supervised Vision Transformers. ICCV 2021. github: <a class="af ne" href="https://github.com/facebookresearch/moco-v3" rel="noopener ugc nofollow" target="_blank">https://github.com/facebookresearch/moco-v3</a></li></ul></div></div></div></div>    
</body>
</html>