<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Interpret GPT2-Small</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Interpret GPT2-Small</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-interpret-gpt2-small-76e0536a588a?source=collection_archive---------8-----------------------#2024-03-22">https://towardsdatascience.com/how-to-interpret-gpt2-small-76e0536a588a?source=collection_archive---------8-----------------------#2024-03-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="6d7f" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Mechanistic Interpretability on prediction of repeated tokens</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@vanillaxiangshuyang?source=post_page---byline--76e0536a588a--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Shuyang Xiang" class="l ep by dd de cx" src="../Images/36a5fd18fd9b7b88cb41094f09b83882.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Q-6F64L3h4jxYNYPiqHVaQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--76e0536a588a--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@vanillaxiangshuyang?source=post_page---byline--76e0536a588a--------------------------------" rel="noopener follow">Shuyang Xiang</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--76e0536a588a--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="b34d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The development of large-scale language models, especially ChatGPT, has left those who have experimented with it, myself included, astonished by its remarkable linguistic prowess and its ability to accomplish diverse tasks. However, many researchers, including myself, while marveling at its capabilities, also find themselves perplexed. Despite knowing the model’s architecture and the specific values of its weights, we still struggle to comprehend why a particular sequence of inputs leads to a specific sequence of outputs.</p><p id="7d46" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this blog post, I will attempt to demystify GPT2-small using mechanistic interpretability on a simple case: the prediction of repeated tokens.</p><h1 id="bb8b" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Mechanistic Interpretability</h1><p id="e006" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Traditional mathematical tools for explaining machine learning models aren’t entirely suitable for language models.</p><p id="57c9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Consider SHAP, a helpful tool for explaining machine learning models. It’s proficient at determining which feature significantly influenced the prediction of a good quality wine. However, it’s important to remember that language models make predictions at the token level, while SHAP values are mostly computed at the feature level, making them potentially unfit for tokens.</p><p id="d7f1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Moreover, Language Models (LLMs) have numerous parameters and inputs, creating a high-dimensional space. Computing SHAP values is costly even in low-dimensional spaces, and even more so in the high-dimensional space of LLMs.</p><p id="509b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Despite tolerating the high computational costs, the explanations provided by SHAP can be superficial. For instance, knowing that the term “potter” most influenced the output prediction due to the earlier mention of “Harry” doesn’t provide much insight. It leaves us uncertain about the part of the model or the specific mechanism responsible for such a prediction.</p><p id="4181" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Mechanistic Interpretability offers a different approach. It doesn’t just identify important features or inputs for a model’s predictions. Instead, it sheds light on the underlying mechanisms or reasoning processes, helping us understand how a model makes its predictions or decisions.</p><h1 id="1096" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Prediction of repeated tokens by GPT2-Small</h1><p id="c21c" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">We will be using GPT2-small for a simple task: predicting a sequence of repeated tokens. The library we will use is <a class="af of" href="https://neelnanda-io.github.io/TransformerLens/index.html" rel="noopener ugc nofollow" target="_blank">TransformerLens</a>, which is designed for <a class="af of" href="https://distill.pub/2020/circuits/zoom-in/" rel="noopener ugc nofollow" target="_blank">mechanistic interpretability</a> of GPT-2 style language models.</p><pre class="og oh oi oj ok ol om on bp oo bb bk"><span id="09e2" class="op nf fq om b bg oq or l os ot">gpt2_small: HookedTransformer = HookedTransformer.from_pretrained("gpt2-small")</span></pre><p id="7655" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We use the code above to load the GPT2-Small model and predict tokens on a sequence generated by a specific function. This sequence includes two identical token sequences, followed by the bos_token. An example would be “ABCDABCD” + bos_token when the seq_len is 3. For clarity, we refer to the sequence from the beginning to the seq_len as the first half, and the remaining sequence, excluding the bos_token, as the second half.</p><pre class="og oh oi oj ok ol om on bp oo bb bk"><span id="2d8e" class="op nf fq om b bg oq or l os ot">def generate_repeated_tokens(<br/>    model: HookedTransformer, seq_len: int, batch: int = 1<br/>) -&gt; Int[Tensor, "batch full_seq_len"]:<br/>    '''<br/>    Generates a sequence of repeated random tokens<br/><br/>    Outputs are:<br/>        rep_tokens: [batch, 1+2*seq_len]<br/>    '''<br/>    bos_token = (t.ones(batch, 1) * model.tokenizer.bos_token_id).long()  # generate bos token for each batch<br/><br/>    rep_tokens_half = t.randint(0, model.cfg.d_vocab, (batch, seq_len), dtype=t.int64)<br/>    rep_tokens = t.cat([bos_token,rep_tokens_half,rep_tokens_half], dim=-1).to(device)<br/>    return rep_tokens</span></pre><p id="f3f7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">When we allow the model to run on the generated token, we find an interesting observation: the model performs significantly better on the second half of the sequence than on the first half. This is measured by the log probabilities on the correct tokens. To be precise, the performance on the first half is -13.898, while the performance on the second half is -0.644.</p><figure class="og oh oi oj ok ox ou ov paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="ou ov ow"><img src="../Images/0aecbb799f225fbde4fee8e95c671884.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqUpWEjf8hK9G6XjAKqplA.png"/></div></div><figcaption class="pd pe pf ou ov pg ph bf b bg z dx">Image for author: Log probs on correct tokens</figcaption></figure><p id="cd2c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We can also calculate prediction accuracy, defined as the ratio of correctly predicted tokens (those identical to the generated tokens) to the total number of tokens. The accuracy for the first half sequence is 0.0, which is unsurprising since we’re working with random tokens that lack actual meaning. Meanwhile, the accuracy for the second half is 0.93, significantly outperforming the first half.</p><h1 id="b212" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Induction Circuits</h1><h2 id="41ae" class="pi nf fq bf ng pj pk pl nj pm pn po nm mr pp pq pr mv ps pt pu mz pv pw px py bk">Finding induction head</h2><p id="bbba" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">The observation above might be explained by the existence of an induction circuit. This is a circuit that scans the sequence for prior instances of the current token, identifies the token that followed it previously, and predicts that the same sequence will repeat. For instance, if it encounters an ‘A’, it scans for the previous ‘A’ or a token very similar to ‘A’ in the embedding space, identifies the subsequent token ‘B’, and then predicts the next token after ‘A’ to be ‘B’ or a token very similar to ‘B’ in the embedding space.</p><figure class="og oh oi oj ok ox ou ov paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="ou ov pz"><img src="../Images/ee98d87987179fb90146a1571297455d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cg94T3U_3ApQ-yuh6zNT3Q.png"/></div></div><figcaption class="pd pe pf ou ov pg ph bf b bg z dx">Image by author: Induction circuit</figcaption></figure><p id="5543" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This prediction process can be broken down into two steps:</p><ol class=""><li id="a2c2" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd qa qb qc bk">Identify the previous same (or similar) token. Every token in the second half of the sequence should “pay attention” to the token ‘seq_len’ places before it. For instance, the ‘A’ at position 4 should pay attention to the ‘A’ at position 1 if ‘seq_len’ is 3. We can call the attention head performing this task the “<strong class="mk fr">induction head</strong>.”</li><li id="47fd" class="mi mj fq mk b go qd mm mn gr qe mp mq mr qf mt mu mv qg mx my mz qh nb nc nd qa qb qc bk">Identify the following token ‘B’. This is the process of copying information from the previous token (e.g., ‘A’) into the next token (e.g., ‘B’). This information will be used to “reproduce” ‘B’ when ‘A’ appears again. We can call the attention head performing this task the “<strong class="mk fr">previous token head</strong>.”</li></ol><p id="d949" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">These two heads constitute a complete induction circuit. Note that sometimes the term “induction head” is also used to describe the entire “induction circuit.” For more introduction of induction circuit, I highly recommend the article <a class="af of" href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html" rel="noopener ugc nofollow" target="_blank">In-context learning and induction head</a> which is a master piece!</p><p id="a136" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Now, let’s identify the attention head and previous head in GPT2-small.</p><p id="d3e4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The following code is used to find the induction head. First, we run the model with 30 batches. Then, we calculate the mean value of the diagonal with an offset of seq_len in the attention pattern matrix. This method lets us measure the degree of attention the current token gives to the one that appears seq_len beforehand.</p><pre class="og oh oi oj ok ol om on bp oo bb bk"><span id="1e62" class="op nf fq om b bg oq or l os ot">def induction_score_hook(<br/>    pattern: Float[Tensor, "batch head_index dest_pos source_pos"],<br/>    hook: HookPoint,<br/>):<br/>    '''<br/>    Calculates the induction score, and stores it in the [layer, head] position of the `induction_score_store` tensor.<br/>    '''<br/>    induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1-seq_len) # src_pos, des_pos, one position right from seq_len<br/>    induction_score = einops.reduce(induction_stripe, "batch head_index position -&gt; head_index", "mean")<br/>    induction_score_store[hook.layer(), :] = induction_score<br/><br/>seq_len = 50<br/>batch = 30<br/>rep_tokens_30 = generate_repeated_tokens(gpt2_small, seq_len, batch)<br/>induction_score_store = t.zeros((gpt2_small.cfg.n_layers, gpt2_small.cfg.n_heads), device=gpt2_small.cfg.device)<br/><br/><br/>    rep_tokens_30,<br/>    return_type=None, <br/>        pattern_hook_names_filter,<br/>        induction_score_hook<br/>    )]<br/>)</span></pre><p id="acc2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Now, let’s examine the induction scores. We’ll notice that some heads, such as the one on layer 5 and head 5, have a high induction score of 0.91.</p><figure class="og oh oi oj ok ox ou ov paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="ou ov qi"><img src="../Images/8a8ae188ca0dd0f8b9fd56e99b55491c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YAnk4CoUXGQgyeCiLN2r8Q.png"/></div></div><figcaption class="pd pe pf ou ov pg ph bf b bg z dx">Image by author: Induction head scores</figcaption></figure><p id="58be" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We can also display the attention pattern of this head. You will notice a clear diagonal line up to an offset of seq_len.</p><figure class="og oh oi oj ok ox ou ov paragraph-image"><div class="ou ov qj"><img src="../Images/cdf95b0714b8e8577791fd751fc7c31a.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*iE3_JZKil0J-LrpH_Cst-w.png"/></div><figcaption class="pd pe pf ou ov pg ph bf b bg z dx">Image by author: layer 5, head 5 attention pattern</figcaption></figure><p id="b25a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Similarly, we can identify the preceding token head. For instance, layer 4 head 11 demonstrates a strong pattern for the previous token.</p><figure class="og oh oi oj ok ox ou ov paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="ou ov qi"><img src="../Images/59d5f94ef7520b604a4296d27da7c44e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KAeTFr7TBm3887wPEi0UsA.png"/></div></div><figcaption class="pd pe pf ou ov pg ph bf b bg z dx">Image by author: previous token head scores</figcaption></figure><h2 id="a3a3" class="pi nf fq bf ng pj pk pl nj pm pn po nm mr pp pq pr mv ps pt pu mz pv pw px py bk">How do MLP layers attribute?</h2><p id="cf89" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Let’s consider this question: do MLP layers count? We know that GPT2-Small contains both attention and MLP layers. To investigate this, I propose using an ablation technique.</p><p id="ab01" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Ablation, as the name implies, systematically removes certain model components and observes how performance changes as a result.</p><p id="2d7b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We will replace the output of the MLP layers in the second half of the sequence with those from the first half, and observe how this affects the final loss function. We will compute the difference between the loss after replacing the MLP layer outputs and the original loss of the second half sequence using the following code.</p><pre class="og oh oi oj ok ol om on bp oo bb bk"><span id="af5a" class="op nf fq om b bg oq or l os ot">def patch_residual_component(<br/>    residual_component,<br/>    hook,<br/>    pos,<br/>    cache,<br/>):<br/>    residual_component[0,pos, :] = cache[hook.name][pos-seq_len, :]<br/>    return residual_component<br/><br/>ablation_scores = t.zeros((gpt2_small.cfg.n_layers, seq_len), device=gpt2_small.cfg.device)<br/><br/>gpt2_small.reset_hooks()<br/>logits = gpt2_small(rep_tokens, return_type="logits")<br/>loss_no_ablation = cross_entropy_loss(logits[:, seq_len: max_len],rep_tokens[:, seq_len: max_len])<br/><br/>for layer in tqdm(range(gpt2_small.cfg.n_layers)):<br/>  for position in range(seq_len, max_len):<br/>    hook_fn = functools.partial(patch_residual_component, pos=position, cache=rep_cache)<br/>    ablated_logits = gpt2_small.run_with_hooks(rep_tokens, fwd_hooks=[<br/>              (utils.get_act_name("mlp_out", layer), hook_fn)<br/>    ])<br/>    loss = cross_entropy_loss(ablated_logits[:, seq_len: max_len], rep_tokens[:, seq_len: max_len])<br/>    ablation_scores[layer, position-seq_len] = loss - loss_no_ablation</span></pre><p id="5075" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We arrive at a surprising result: aside from the first token, the ablation does not produce a significant logit difference. This suggests that the MLP layers may not have a significant contribution in the case of repeated tokens.</p><figure class="og oh oi oj ok ox ou ov paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="ou ov qi"><img src="../Images/29f50de7de46432cb97d79853870184d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6dtICd7bWoxvBbOt2WIsKg.png"/></div></div><figcaption class="pd pe pf ou ov pg ph bf b bg z dx">Image by author: loss different before and after ablation of mlp layers</figcaption></figure><h1 id="a07f" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk"><strong class="al">One induction circuit</strong></h1><p id="5092" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Given that the MLP layers don’t significantly contribute to the final prediction, we can manually construct an induction circuit using the head of layer 5, head 5, and the head of layer 4, head 11. Recall that these are the induction head and the previous token head. We do it by the following code:</p><pre class="og oh oi oj ok ol om on bp oo bb bk"><span id="995f" class="op nf fq om b bg oq or l os ot">def K_comp_full_circuit(<br/>    model: HookedTransformer,<br/>    prev_token_layer_index: int,<br/>    ind_layer_index: int,<br/>    prev_token_head_index: int,<br/>    ind_head_index: int<br/>) -&gt; FactoredMatrix:<br/>    '''<br/>    Returns a (vocab, vocab)-size FactoredMatrix,<br/>    with the first dimension being the query side<br/>    and the second dimension being the key side (going via the previous token head)<br/><br/>    '''<br/>    W_E = gpt2_small.W_E<br/>    W_Q = gpt2_small.W_Q[ind_layer_index, ind_head_index]<br/>    W_K = model.W_K[ind_layer_index, ind_head_index]<br/>    W_O = model.W_O[prev_token_layer_index, prev_token_head_index]<br/>    W_V = model.W_V[prev_token_layer_index, prev_token_head_index]<br/><br/>    Q = W_E @ W_Q<br/>    K = W_E @ W_V @ W_O @ W_K<br/>    return FactoredMatrix(Q, K.T)</span></pre><p id="4fbf" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Computing the top 1 accuracy of this circuit yields a value of 0.2283. This is quite good for a circuit constructed by only two heads!</p><p id="a385" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For detailed implementation, please check my <a class="af of" href="https://colab.research.google.com/drive/1_Qx67oPB2ZNeKa1ANYhFa9tkKGSZhX6a#scrollTo=ss1yF3e8PNYD" rel="noopener ugc nofollow" target="_blank"><em class="qk">notebook</em></a>. And many thanks to Neel Nanda who developed the wonderful <a class="af of" href="https://neelnanda-io.github.io/TransformerLens/index.html" rel="noopener ugc nofollow" target="_blank">TransformerLen</a> as a great tool for Mechanistic Interpretability of LLMs!</p></div></div></div></div>    
</body>
</html>