- en: An Overview of the LoRA Family
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA家族概述
- en: 原文：[https://towardsdatascience.com/an-overview-of-the-lora-family-515d81134725?source=collection_archive---------1-----------------------#2024-03-10](https://towardsdatascience.com/an-overview-of-the-lora-family-515d81134725?source=collection_archive---------1-----------------------#2024-03-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/an-overview-of-the-lora-family-515d81134725?source=collection_archive---------1-----------------------#2024-03-10](https://towardsdatascience.com/an-overview-of-the-lora-family-515d81134725?source=collection_archive---------1-----------------------#2024-03-10)
- en: LoRA, DoRA, AdaLoRA, Delta-LoRA, and more variants of low-rank adaptation.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoRA、DoRA、AdaLoRA、Delta-LoRA 等低秩适应的变体。
- en: '[](https://medium.com/@doriandrost?source=post_page---byline--515d81134725--------------------------------)[![Dorian
    Drost](../Images/1795395ad0586eafd83d3e2f7b975ca8.png)](https://medium.com/@doriandrost?source=post_page---byline--515d81134725--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--515d81134725--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--515d81134725--------------------------------)
    [Dorian Drost](https://medium.com/@doriandrost?source=post_page---byline--515d81134725--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@doriandrost?source=post_page---byline--515d81134725--------------------------------)[![Dorian
    Drost](../Images/1795395ad0586eafd83d3e2f7b975ca8.png)](https://medium.com/@doriandrost?source=post_page---byline--515d81134725--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--515d81134725--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--515d81134725--------------------------------)
    [Dorian Drost](https://medium.com/@doriandrost?source=post_page---byline--515d81134725--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--515d81134725--------------------------------)
    ·17 min read·Mar 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--515d81134725--------------------------------)
    ·阅读时长17分钟·2024年3月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3d2b918df5f4ec4113e0afb3b88111f5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d2b918df5f4ec4113e0afb3b88111f5.png)'
- en: LoRA comes in different shapes and varieties. Photo by [Lucas George Wendt](https://unsplash.com/@lucasgwendt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 有多种不同的形式和变体。照片由[Lucas George Wendt](https://unsplash.com/@lucasgwendt?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
- en: '**Lo**w-**R**ank **A**daptation (**LoRA**) can be considered a major breakthrough
    towards the ability to train large language models for specific tasks efficiently.
    It is widely used today in many applications and has inspired research on how
    to improve upon its main ideas to achieve better performance or train models even
    faster.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**Lo**w-**R**ank **A**daptation (**LoRA**) 可以被认为是朝着高效训练大型语言模型以完成特定任务的一项重大突破。它今天广泛应用于许多领域，并激发了关于如何改进其主要思想以实现更好的性能或更快速训练模型的研究。'
- en: In this article, I want to give an overview of some variants of LoRA, that promise
    to improve LoRAs capabilities in different ways. I will first explain the basic
    concept of LoRA itself, before presenting **LoRA+**, **VeRA**, **LoRA-FA**, **LoRA-drop**,
    **AdaLoRA**, **DoRA,** and **Delta-LoRA**. I will introduce the basic concepts
    and main ideas each, and show, how these approaches deviate from the original
    LoRA. I will spare technical details, unless they are important for the basic
    concepts, and will also not discuss evaluations in detail. For readers interested,
    I linked the original papers at the end.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将概述一些LoRA的变体，它们承诺在不同方面提高LoRA的能力。我将首先解释LoRA本身的基本概念，然后介绍**LoRA+**、**VeRA**、**LoRA-FA**、**LoRA-drop**、**AdaLoRA**、**DoRA**
    和 **Delta-LoRA**。我将介绍每种方法的基本概念和主要思想，并展示这些方法如何偏离原始的LoRA。我会省略技术细节，除非它们对基本概念至关重要，并且也不会详细讨论评估。对于有兴趣的读者，我在文末提供了原始论文的链接。
- en: Lora
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lora
- en: '![](../Images/93b5a2ec564b17edc789c7d34596032c.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93b5a2ec564b17edc789c7d34596032c.png)'
- en: The main idea of LoRA is to add two smaller tunable matrices A and B next to
    the pre-trained weight matrix W, without changing the parameters of W. Image from
    [1].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 的主要思想是将两个较小的可调矩阵 A 和 B 添加到预训练的权重矩阵 W 旁边，而不改变 W 的参数。图片来源[1]。
- en: Low-Rank Adaption (**LoRA**) [1] is a technique, that is widely used today to
    train large language models (LLMs). Large language models come with the capability
    to predict tokens of natural language given a natural language input. This is
    an astonishing capability, but for solving many problems this is not enough. Most
    of the time, you want to train an LLM on a given downstream task, such as classifying
    sentences or generating answers to given questions. The most straightforward way
    of doing that is fine-tuning, where you train some of the layers of the LLM with
    data of the desired task. That means training very big models with millions to
    billions of parameters though.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应（**LoRA**）[1]是一种今天广泛用于训练大型语言模型（LLMs）的技术。大型语言模型具有根据自然语言输入预测自然语言令牌的能力。这是一个令人惊叹的能力，但在解决许多问题时，仅凭此能力往往不够。大多数情况下，你希望在给定的下游任务上训练LLM，例如对句子进行分类或生成给定问题的答案。实现这一点最直接的方式是微调，即使用目标任务的数据训练LLM的部分层。
    但这意味着需要训练具有数百万到数十亿参数的非常大模型。
- en: LoRA gives an alternative way of training that is much faster and easier to
    conduct due to a drastically reduced number of parameters. Next to the parameter
    weights of an already pre-trained LLM layer, LoRA introduces two matrices A and
    B, that are called *adapters* and that are much smaller. If the original matrix
    of parameters W is of size *d x d*, the matrices A and B are of size *d x r* and
    *r x d*, where *r* is much smaller (typically below 100). The parameter *r* is
    called the *rank*. That is, if you use LoRA with a rank of *r=16*, these matrices
    are of shape *16 x d*. The higher the rank, the more parameters you train. That
    can lead to better performance on the one hand but needs more computation time
    on the other.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA提供了一种替代的训练方式，由于大幅减少了参数数量，因此训练速度更快，实施更为简便。除了已经预训练的LLM层的参数权重外，LoRA引入了两个矩阵A和B，这些矩阵被称为*适配器*，它们的尺寸要小得多。如果原始的参数矩阵W的大小是*d
    x d*，那么矩阵A和B的大小分别为*d x r*和*r x d*，其中*r*要小得多（通常小于100）。参数*r*被称为*秩*。也就是说，如果你使用秩为*r=16*的LoRA，这些矩阵的形状就是*16
    x d*。秩越高，训练的参数越多。这一方面可能带来更好的性能，另一方面也需要更多的计算时间。
- en: Now that we have these new matrices A and B, what happens with them? The input
    fed to W is also given to B*A, and the output of B*A is added to the output of
    the original matrix W. That is, you train some parameters on top and add their
    output to the original prediction, which allows you to influence the model’s behavior.
    You don’t train W anymore, which is why we sometimes say that W is *frozen*. Importantly,
    the addition of A and B is not only done at the very end (which would just add
    a layer on top) but can be applied to layers deep inside the neural network.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了这些新的矩阵A和B，那么它们会发生什么呢？输入给W的数据同样也会输入到B*A中，B*A的输出会加到原始矩阵W的输出上。也就是说，你在原有基础上训练了一些参数，并将它们的输出添加到原始预测中，这使得你可以影响模型的行为。你不再训练W，这也是为什么我们有时会说W是*冻结*的。重要的是，A和B的加法不仅仅发生在最后一层（那样只是增加一层），而是可以应用于神经网络内部的深层。
- en: 'That is the main idea of LoRA, and its biggest advantage is, that you have
    to train fewer parameters than in fine-tuning, but still get comparable performance.
    One more technical detail I want to mention at this place: At the beginning, the
    matrix A is initialized with random values of mean zero, but with some variance
    around that mean. The matrix B is initialized as a matrix of complete zeros. This
    ensures, that the LoRA matrices don’t change the output of the original W in a
    random fashion from the very beginning. The update of A and B on W’s output should
    rather be an addition to the original output, once the parameters of A and B are
    being tuned in the desired direction. However, we will later see that some approaches
    deviate from this idea for different reasons.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是LoRA的主要思想，它的最大优势是，训练的参数比微调时少，但仍能获得相似的性能。这里我还想提到一个技术细节：一开始，矩阵A用均值为零但方差较小的随机值进行初始化。矩阵B则初始化为一个全零矩阵。这确保了LoRA矩阵从一开始就不会以随机的方式改变原始W的输出。当A和B的参数被调优到期望方向时，A和B的更新对W的输出应当是一个加法操作，而不是随机改变原始输出。然而，我们稍后会看到，由于不同的原因，某些方法偏离了这一思想。
- en: LoRA as just explained is used very often with today's LLMs. However, by now
    there are many variants of LoRA that deviate from the original method in different
    ways and aim at improving speed, performance, or both. Some of these I want to
    present to you in the following.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LoRA在当今的LLM中被广泛使用。然而，到现在为止，已经有许多LoRA的变种，它们在不同的方面偏离了原始方法，旨在提高速度、性能，或两者兼顾。接下来我将向你介绍其中一些。
- en: LoRA+
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA+
- en: '![](../Images/881b71fcbde54ca06cf2e34bc537c163.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/881b71fcbde54ca06cf2e34bc537c163.png)'
- en: LoRA+ introduces different learning rates for the two matrices A and B, here
    indicated by the parameter λ. Image from [2].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA+为两个矩阵A和B引入了不同的学习率，这里通过参数λ来表示。图片来源于[2]。
- en: '**LoRA+** [2]introduces a more efficient way of training LoRA adapters by introducing
    different learning rates for matrices A and B. Most of the time, when training
    a neural network, there is just one learning rate that is applied to all weight
    matrices the same way. However, for the adapter matrices used in LoRA, the authors
    of LoRA+ can show, that it is suboptimal to have that single learning rate. The
    training becomes more efficient by setting the learning rate of matrix B much
    higher than that of matrix A.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**LoRA+** [2]通过为矩阵A和B引入不同的学习率，提供了一种更高效的LoRA适配器训练方法。在大多数情况下，当训练神经网络时，只有一个学习率应用于所有的权重矩阵。但是，LoRA+的作者展示了，对于LoRA中使用的适配器矩阵，使用单一学习率是次优的。通过将矩阵B的学习率设置得比矩阵A高得多，训练变得更加高效。'
- en: There is a theoretical argument to justify that approach, that mainly bases
    on numerical caveats of a neural network’s initialization if the model becomes
    very wide in terms of the number of its neurons. However, the math required to
    prove that is quite complicated (if you are really into it, feel free to take
    a look at the original paper [2]). Intuitively, you may think that matrix B, which
    is initialized with zero, could use bigger update steps than the randomly initialized
    matrix A. In addition, there is empirical evidence for an improvement by that
    approach. By setting the learning rate of matrix B 16 times higher than that of
    matrix A, the authors have been able to gain a small improvement in model accuracy
    (around 2%), while speeding up the training time by factor two for models such
    as RoBERTa or Llama-7b.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法背后有一个理论依据，主要基于神经网络初始化的数值陷阱，尤其是当模型在神经元数量上变得非常宽时。然而，证明这一点的数学推导相当复杂（如果你对这一点非常感兴趣，可以查看原文[2]）。直观上，你可能会认为矩阵B在初始化为零后，可以使用比随机初始化的矩阵A更大的更新步长。此外，实证研究也证明了这种方法的改进。通过将矩阵B的学习率设置为矩阵A的16倍，作者在像RoBERTa或Llama-7b这样的模型上获得了小幅度的精度提升（大约2%），同时将训练时间提高了两倍。
- en: VeRA
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VeRA
- en: '![](../Images/dc446d5c9e0b98bf45daf99f16076d84.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc446d5c9e0b98bf45daf99f16076d84.png)'
- en: VeRA doesn’t train A and B, but initializes them to a random projection and
    trains additional vectors d and b instead. Image from [3].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: VeRA不训练A和B，而是将其初始化为一个随机投影，随后训练额外的向量d和b。图片来源于[3]。
- en: With **VeRA** (**Ve**ctor-based **R**andom Matrix **A**daptation) [3], the authors
    introduce an approach to drastically reduce the parameter size of the LoRA adapters.
    Instead of training the matrices A and B, which is the core idea of LoRA in the
    first place, they initialize these matrices with shared random weights (i.e. all
    the matrices A and B in all the layers have the same weights) and add two new
    vectors d and b. Only these vectors d and b are trained in the following.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**VeRA**（**Ve**ctor-based **R**andom Matrix **A**daptation）[3]中，作者提出了一种方法，极大地减少了LoRA适配器的参数规模。与其训练矩阵A和B（这是LoRA的核心思想），他们用共享随机权重初始化这些矩阵（即所有层中的矩阵A和B具有相同的权重），并添加了两个新的向量d和b。接下来只训练这些向量d和b。'
- en: You may wonder how this can work at all. A and B are matrices of random weights.
    How should they contribute anything to the model’s performance, if they are not
    trained at all? This approach is based on an interesting field of research on
    so-called random projections. There is quite some research that indicates that
    in a large neural network only a small fraction of the weights is used to steer
    the behavior and lead to the desired performance on the task the model was trained
    for. Due to the random initialization, some parts of the model (or sub-networks)
    are contributing more towards the desired model behavior from the very beginning.
    During the training, all parameters are trained though, as it is now known which
    are the important subnetworks. That makes training very costly, as most of the
    parameters that are updated don’t add any value to the model’s prediction.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，这怎么可能有效呢？A和B是随机权重的矩阵。如果它们完全没有经过训练，怎么能为模型的性能做出贡献呢？这种方法基于一个有趣的研究领域——所谓的随机投影。许多研究表明，在大型神经网络中，只有一小部分权重被用来引导模型行为，并产生模型在训练任务上的预期表现。由于随机初始化，模型的某些部分（或子网络）从一开始就更有可能贡献于预期的模型行为。在训练过程中，所有参数都会被训练，因为现在已经知道了哪些子网络是重要的。这使得训练非常昂贵，因为大多数更新的参数并未对模型的预测产生任何价值。
- en: Based on this idea, there are approaches to only train these relevant sub-networks.
    A similar behavior can be obtained by not training the sub-networks themselves,
    but by adding projection vectors after the matrix. Due to the multiplication of
    the matrix with the vector, this can lead to the same output as tuning some sparse
    parameters in the matrix would. That is exactly what the authors of VeRA propose
    by introducing the vectors d and b, which are trained, while the matrices A and
    B are frozen. Also, in contrast to the original LoRa approach, matrix B is not
    set to zero anymore but is initialized randomly just as matrix A.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一思想，有一些方法仅训练这些相关的子网络。通过在矩阵后添加投影向量，而不是直接训练子网络本身，也可以获得类似的行为。由于矩阵与向量的乘法，这可以产生与调整矩阵中的一些稀疏参数相同的输出。这正是VeRA的作者提出的，通过引入向量d和b来实现训练，而矩阵A和B则被冻结。此外，与原始的LoRa方法不同，矩阵B不再被设为零，而是像矩阵A一样随机初始化。
- en: This approach naturally leads to a number of parameters that is much smaller
    than the full matrices A and B. For example, if you introduce LoRA-layers of rank
    16 to GPT-3, you would have 75.5M parameters. With VeRA, you only have 2.8M (a
    reduction of 97%). But how is the performance with such a small number of parameters?
    The authors of VeRA performed an evaluation with some common benchmarks such as
    GLUE or E2E and with models based on RoBERTa and GPT2 Medium. Their results suggest,
    that the VeRA model yields performance that is only marginally lower than models
    that are fully finetuned or that use the original LoRa technique.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法自然导致的参数数量远小于完整的矩阵A和B。例如，如果你为GPT-3引入秩为16的LoRA层，你将拥有7550万个参数。而使用VeRA时，你只有280万个参数（减少了97%）。但是，参数数量如此之小，性能如何呢？VeRA的作者使用了一些常见的基准，如GLUE或E2E，并使用基于RoBERTa和GPT2
    Medium的模型进行了评估。他们的结果表明，VeRA模型的性能仅比完全微调或使用原始LoRa技术的模型略低。
- en: LoRA-FA
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA-FA
- en: '![](../Images/45b48efbeb4e02ad27c047b823646bd0.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45b48efbeb4e02ad27c047b823646bd0.png)'
- en: LoRA-FA freezes matrix A and only trains matrix B. Image from [4].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA-FA冻结矩阵A，只训练矩阵B。图片来自[4]。
- en: Another approach, **LoRA-FA** [4], which stands for LoRA with **F**rozen-**A,**
    is going in a similar direction as VeRA. In LoRA-FA, the matrix A is frozen after
    initialization and hence serves as a random projection. Instead of adding new
    vectors, matrix B is trained though, after being initialized with zeros (just
    as in the original LoRA). This halves the number of parameters while having comparable
    performance to normal LoRA.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法，**LoRA-FA** [4]，即带有**F**rozen-**A**的LoRA，朝着与VeRA类似的方向发展。在LoRA-FA中，矩阵A在初始化后被冻结，因此作为一个随机投影存在。不同于添加新向量的是，矩阵B在初始化为零后被训练（就像原始LoRA一样）。这样可以将参数数量减半，同时保持与普通LoRA相当的性能。
- en: LoRa-drop
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRa-drop
- en: '![](../Images/ed58052c832ec80fefa83a5cc137b93a.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed58052c832ec80fefa83a5cc137b93a.png)'
- en: LoRA-drop uses the output of B*A to decide, which LoRA-layers are worth to be
    trained at all. Image from [5].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA-drop使用B*A的输出决定哪些LoRA层值得训练。图片来自[5]。
- en: In the beginning, I explained, that you can add Lora matrices to any layer in
    the neural network. **LoRA-drop** [5] introduces an algorithm to decide which
    layers are worth to be enhanced by LoRA, and for which this is not worth the effort.
    Even if training LoRA adapters is much cheaper than finetuning the whole model,
    the more LoRA adapters you add, the more expensive is the training, still.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，我解释过，你可以将 LoRA 矩阵添加到神经网络的任何一层。**LoRA-drop** [5] 提出了一种算法，用来决定哪些层值得通过 LoRA
    进行增强，哪些则不值得这么做。即使训练 LoRA 适配器比微调整个模型便宜得多，但你添加的 LoRA 适配器越多，训练的成本仍然会越高。
- en: LoRA-drop consists of two steps. In the first step, you sample a subset of the
    data and train the LoRA adapters for a few iterations. Then you calculate the
    importance of each LoRA adapter as B*A*x, where A and B are the LoRA matrices,
    and x is the input. That is simply the output of the LoRA adapters that is added
    to the output of the frozen layer each. If this output is big, it changes the
    behavior of the frozen layer more drastically. If it is small, this indicates
    that the LoRA adapter has only little influence on the frozen layer and could
    as well be omitted.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA-drop 包含两个步骤。第一步，你从数据中抽取一个子集并训练 LoRA 适配器若干迭代。然后，你计算每个 LoRA 适配器的重要性，计算公式为
    B*A*x，其中 A 和 B 是 LoRA 矩阵，x 是输入。这只是 LoRA 适配器的输出，它会加到每个冻结层的输出中。如果这个输出很大，它会显著改变冻结层的行为。如果输出很小，这表明
    LoRA 适配器对冻结层的影响微乎其微，完全可以省略。
- en: Given that importance, you now select the LoRA layers that are most important.
    The are different ways of doing that. You can sum up the importance values until
    you reach a threshold, which is controlled by a hyperparameter, or you just take
    the top n LoRA layers with the highest importance for a fixed n. In any case,
    in the next step, you conduct the full training on the whole dataset (remember
    that you used a subset of data for the previous steps) but only on those layers
    that you just selected. The other layers are fixed to a shared set of parameters
    that won’t be changed anymore during training.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这一重要性，你现在可以选择最重要的 LoRA 层。有多种方法可以做到这一点。你可以将重要性值加总，直到达到一个由超参数控制的阈值，或者直接选择前 n
    个具有最高重要性的 LoRA 层，其中 n 是固定的。无论哪种方式，在下一步中，你将在整个数据集上进行完整训练（记住，你在前几步使用的是数据的子集），但只针对你刚才选择的那些层。其他层将固定为一组共享的参数，在训练过程中不再改变。
- en: The algorithm of LoRA-drop hence allows to training a model with just a subset
    of the LoRA layers. The authors propose empirical evidence that indicates only
    marginal changes in accuracy, compared to training all LoRA layers, but at reduced
    computation time due to the smaller number of parameters that have to be trained.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LoRA-drop 算法允许仅使用部分 LoRA 层进行模型训练。作者提供了实证证据，表明与训练所有 LoRA 层相比，准确率变化仅为微不足道，但由于需要训练的参数数量减少，计算时间大大缩短。
- en: AdaLoRA
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaLoRA
- en: '![](../Images/58b22f87680fb826da33ff3e6d46b248.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58b22f87680fb826da33ff3e6d46b248.png)'
- en: AdaLoRA allows to adapt the rank of the LoRA matrices dynamically. Photo by
    [Hasmik Ghazaryan Olson](https://unsplash.com/@find_something_pretty_everyday?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: AdaLoRA 允许动态调整 LoRA 矩阵的秩。照片来自 [Hasmik Ghazaryan Olson](https://unsplash.com/@find_something_pretty_everyday?utm_source=medium&utm_medium=referral)
    上传于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
- en: 'There are alternative ways how to decide which LoRA parameters are more important
    than others. In this section, I will present **AdaLoRA** [6], which stands for
    **Ada**ptive LoRa. What part of LoRA is adaptive here? It is the rank (i.e. the
    size) of the LoRA matrices. The main problem is the same as in the previous section:
    It may not be worth adding LoRA matrices A and B to each layer, but for some layers,
    the LoRA training may be more important (i.e. may lead to more change in the model’s
    behavior) than for others. To decide on that importance, the authors of AdaLoRA
    propose to consider the singular values of the LoRA matrices as indicators of
    their importance.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他方式可以决定哪些 LoRA 参数比其他参数更重要。在这一部分，我将介绍 **AdaLoRA** [6]，它代表了 **自适应 LoRA**。这里
    LoRA 的哪个部分是自适应的？就是 LoRA 矩阵的秩（即大小）。主要问题与上一部分相同：并非每一层都值得添加 LoRA 矩阵 A 和 B，但对于某些层，LoRA
    训练可能比其他层更为重要（即可能会对模型行为产生更大的影响）。为了决定这种重要性，AdaLoRA 的作者提出考虑 LoRA 矩阵的奇异值作为其重要性的指标。
- en: 'What is meant by that? First, we have to understand, that a matrix multiplication
    can also be seen as applying a function to a vector. When dealing with neural
    networks, this is quite obvious: Most of the time you use neural networks as functions,
    i.e. you give an input (say, a matrix of pixel values) and obtain a result (say,
    a classification of an image). Under the hood, this function application is powered
    by a sequence of matrix multiplications. Now, let’s say you want to reduce the
    number of parameters in such a matrix. That will change the function’s behavior,
    but you want it to change as little as possible. One way to do that is to compute
    the eigenvalues of the matrix, which tell you how much variance is captured by
    the rows of the matrix each. You may then decide to set some rows to zero, that
    capture only a small fraction of the variance, and hence don’t add much information
    to the function. This is the main idea of AdaLoRA since the aforementioned singular
    values are exactly the square roots of the eigenvalues. That is, based on the
    singular values, AdaLoRA decides which rows of which LoRA matrices are more important,
    and which can be omitted. This effectively shrinks the rank of some matrices,
    which have many rows that don’t contribute much. However, note an important difference
    to LoRA-drop from the previous section: In LoRA-drop, the adapter of a layer is
    selected to either be trained fully, or not trained at all. AdaLoRA can also decide
    to keep adaptors for some layers but with lower rank. That means, in the end,
    different adaptors can have different ranks (whereas in the original LoRA approach,
    all adaptors have the same rank).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是什么意思呢？首先，我们必须理解，矩阵乘法也可以看作是对向量应用一个函数。在处理神经网络时，这一点是非常明显的：大多数时候你将神经网络视作一个函数，也就是说，你给定一个输入（例如，一组像素值的矩阵），然后得到一个结果（比如，图像的分类）。在幕后，这个函数的应用是通过一系列矩阵乘法来实现的。现在，假设你想要减少矩阵中的参数数量。这样会改变函数的行为，但你希望它的变化尽可能小。一个方法是计算矩阵的特征值，它们告诉你每一行矩阵捕获了多少方差。然后，你可以决定将一些捕获较小方差的行设为零，这些行对函数贡献不大。AdaLoRA
    的主要思想就是基于上述的奇异值，它们正是特征值的平方根。也就是说，基于奇异值，AdaLoRA 决定了哪些 LoRA 矩阵的行更重要，哪些可以被省略。这有效地缩小了一些矩阵的秩，这些矩阵中有许多行并没有太大贡献。然而，需要注意的是，与上一节的
    LoRA-drop 存在一个重要的区别：在 LoRA-drop 中，层的适配器要么被完全训练，要么根本不训练。而 AdaLoRA 还可以决定保留某些层的适配器，但使用较低的秩。这意味着，最终不同的适配器可以有不同的秩（而在原始的
    LoRA 方法中，所有适配器的秩是相同的）。
- en: 'There are some more details to the AdaLoRA approach, which I omitted for brevity.
    I want to mention two of them though: First, the AdaLoRA approach does not calculate
    the singular values explicitly all the time (as that would be very costly), but
    decomposes the weight matrices with a singular value decomposition. This decomposition
    is another way of representing the same information as in a single matrix, but
    it allows to get the singular values directly, without costly computation needed.
    Second, AdaLoRA does not decide on the singular values alone but also takes into
    account the sensitivity of the loss to certain parameters. If setting a parameter
    to zero has a large influence on the loss, this parameter is said to have high
    sensitivity. When deciding where to shrink the rank, the mean sensitivity of a
    row’s elements is taken into consideration in addition to the singular value.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: AdaLoRA 方法还有一些细节，我为了简洁起见省略了它们。不过，我想提到其中的两个：首先，AdaLoRA 方法并不是每次都显式地计算奇异值（因为那样做会非常昂贵），而是通过奇异值分解来分解权重矩阵。这种分解是另一种表示相同信息的方式，但它允许直接获取奇异值，而无需进行昂贵的计算。其次，AdaLoRA
    不仅仅依赖奇异值，还考虑了损失函数对某些参数的敏感性。如果将某个参数设为零对损失有很大影响，那么这个参数就被认为具有较高的敏感性。在决定压缩秩时，除了奇异值外，还会考虑一行元素的平均敏感性。
- en: Empirical evidence for the value of the approach is given by comparing AdaLoRA
    with standard LoRA of the same rank budget. That is, both approaches have the
    same number of parameters in total, but these are distributed differently. In
    LoRA, all matrices have the same rank, while in AdaLoRA, some have a higher and
    some have a lower rank, which leads to the same number of parameters in the end.
    In many scenarios, AdaLoRA yields better scores than the standard LoRA approach,
    indicating a better distribution of trainable parameters on parts of the model,
    that are of particular importance for the given task. The following plot gives
    an example, of how AdaLoRA distributed the ranks for a given model. As we see,
    it gives higher ranks to the layers towards the end of the model, indicating that
    adapting these is more important.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较AdaLoRA与标准LoRA（在相同秩预算下）的结果，可以获得该方法有效性的实证证据。也就是说，这两种方法总参数数量相同，但分布方式不同。在LoRA中，所有矩阵的秩相同，而在AdaLoRA中，部分矩阵的秩较高，部分较低，最终导致相同数量的参数。在许多场景中，AdaLoRA比标准LoRA方法取得更好的分数，表明它能更好地分配模型的可训练参数，尤其是在对于给定任务至关重要的部分。以下图表展示了AdaLoRA如何为给定模型分配秩。如图所示，AdaLoRA将较高的秩分配给模型后面的层，表明调整这些层更为重要。
- en: '![](../Images/44a262227e359e20f92231767812dc76.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44a262227e359e20f92231767812dc76.png)'
- en: On different layers of the network, the LoRA matrices are given different ranks.
    On later layers, the ranks are higher, in general. Image from [6].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络的不同层中，LoRA矩阵被赋予不同的秩。通常，在后面的层中，秩会更高。图片来源：[6]。
- en: DoRA
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DoRA
- en: '![](../Images/e1d08a8374ad6a7ad5343beaa7e121c6.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1d08a8374ad6a7ad5343beaa7e121c6.png)'
- en: In DoRA, the weight matrix W is decomposed into magnitude m and direction V,
    which are tuned independently. Image from [7].
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在DoRA中，权重矩阵W被分解为大小m和方向V，这两个部分是独立调整的。图片来源：[7]。
- en: 'Another approach to modify LoRa to get better performance is Weight-**D**ecomposed
    L**o**w-**R**ank **A**daption, or **DoRA** [7]. DoRA starts with the idea, that
    each matrix can be decomposed into the product of a magnitude and a direction.
    For a vector in 2D space, you can easily visualize that: A vector is nothing else
    than an arrow starting at the position of zero and ending at a certain point in
    the vector space. With the vector’s entries, you specify that point, e.g. by saying
    x=1 and y=1, if your space has two dimensions x and y. Alternatively, you could
    describe the very same point in a different way by specifying a magnitude and
    an angle (i.e. a direction), such as m=√2 and a=45°. That means that you start
    at point zero and move in the direction of 45° with an arrow length of √2\. That
    will lead you to the same point (x=1,y=1).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 改进LoRA以获得更好性能的另一种方法是权重-**分**解低秩**适**应（Weight-**D**ecomposed L**o**w-**R**ank
    **A**daption），简称**DoRA**[7]。DoRA的核心思想是，每个矩阵都可以分解为大小和方向的乘积。对于二维空间中的一个向量，你可以很容易地可视化这一点：一个向量不过是从零位置出发，指向向量空间中某个点的箭头。通过向量的条目，你可以指定那个点，例如，如果你的空间有x和y两个维度，你可以说x=1和y=1。或者，你也可以通过指定大小和角度（即方向）来以不同的方式描述这个点，比如m=√2和a=45°。这意味着你从零点开始，沿着45°方向移动，箭头长度为√2，最终会到达相同的点（x=1，y=1）。
- en: 'This decomposition into magnitude and direction can also be done with matrices
    of higher order. The authors of DoRA apply this to the weight matrices that describe
    the updates within the training steps for a model trained with normal fine-tuning
    and a model trained with LoRA adapters. A comparison of these two techniques we
    see in the following plot:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种大小和方向的分解方法同样可以应用于更高阶的矩阵。DoRA的作者将此方法应用于描述模型训练步骤中更新的权重矩阵，适用于普通微调训练的模型和使用LoRA适配器训练的模型。我们在下图中可以看到这两种技术的对比：
- en: '![](../Images/52fa2292e0c07ee023f4f62c7b3edd1d.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52fa2292e0c07ee023f4f62c7b3edd1d.png)'
- en: Finetuning and LoRA differ in the relationship between the changes in magnitude
    and direction. Image from [7].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 微调和LoRA在大小和方向的变化关系上有所不同。图片来源：[7]。
- en: We see two plots, one for a fine-tuned model (left) and one for a model trained
    with LoRA adapters (right). On the x-axis, we see the change in direction, on
    the y-axis we see the change in magnitude, and each scatter point in the plots
    belongs to one layer of the model. There is an important difference between the
    two ways of training. In the left plot, there is a small negative correlation
    between the update in direction and the update in magnitude, while in the right
    plot, there is a positive relationship, which is much stronger. You may wonder
    which is better, or whether this has any meaning at all. Remember, that the main
    idea of LoRA is to achieve the same performance as finetuning, but with fewer
    parameters. That means, ideally we want LoRA’s training to share as many properties
    with fine-tuning as possible, as long as this does not increase the costs. If
    the correlation between direction and magnitude is slightly negative in fine-tuning,
    this may be a desirable property for LoRA as well, if it is achievable. In other
    words, if the relationship between direction and magnitude is different in LoRA
    compared to full fine-tuning, this may be one of the reasons why LoRA sometimes
    performs less well than fine-tuning.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到两个图表，一个是微调模型（左）的图表，一个是使用LoRA适配器训练的模型（右）的图表。在x轴上，我们看到方向的变化，在y轴上，我们看到幅度的变化，图中的每个散点代表模型的一层。两种训练方式之间有一个重要的区别。在左图中，方向更新与幅度更新之间存在较小的负相关，而在右图中，存在更强的正相关关系。你可能会想，哪种方式更好，或者这是否有什么意义。请记住，LoRA的主要理念是以较少的参数实现与微调相同的性能。这意味着，理想情况下，我们希望LoRA的训练与微调共享尽可能多的特性，只要这不会增加成本。如果微调中方向与幅度之间的相关性稍微是负的，这对于LoRA来说可能是一个理想的特性，前提是它可以实现。换句话说，如果LoRA中方向与幅度的关系与全微调不同，这可能是LoRA有时表现不如微调的原因之一。
- en: 'The authors of DoRA introduce a method to train magnitude and direction independently
    by separating the pretrained matrix W into a magnitude vector m of size *1 x d*
    and a direction matrix V. The direction matrix V is then enhanced by B*A, as known
    from the standard LoRA approach, and m is trained as it is, which is feasible
    because it has just one dimension. While LoRA tends to change both magnitude and
    direction together (as indicated by the high positive correlation between these
    two), DoRA can more easily adjust the one without the other, or compensate changes
    in one with negative changes in the other. We can see the relationship between
    direction and magnitude is more like the one in finetuning:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: DoRA的作者提出了一种方法，通过将预训练矩阵W分解为幅度向量m（大小为*1 x d*）和方向矩阵V，独立地训练幅度和方向。然后，方向矩阵V通过B*A进行增强，正如标准LoRA方法所示，m则保持不变进行训练，因为它只有一个维度。尽管LoRA倾向于同时改变幅度和方向（如这两者之间的高度正相关所示），但DoRA可以更轻松地单独调整一个，而不影响另一个，或通过负向变化来补偿一个的变化。我们可以看到方向和幅度之间的关系更像是在微调中的关系：
- en: '![](../Images/1a08898cca3e316d5e4d73cbebfb9d8c.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a08898cca3e316d5e4d73cbebfb9d8c.png)'
- en: For DoRA, the relationship between magnitude and direction is more like that
    in finetuning. Image from [7].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DoRA，幅度和方向之间的关系更像是在微调中的关系。图片来自[7]。
- en: On several benchmarks, DoRA outperforms LoRA in accuracy. Decomposing the weight
    updates into magnitude and direction may allow DoRA to perform a training that
    is closer to the training done in fine-tuning, while still using the smaller parameters
    space introduced by LoRA.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个基准测试中，DoRA在准确性上优于LoRA。将权重更新分解为幅度和方向可能使DoRA执行更接近于微调中的训练，同时仍使用LoRA引入的较小参数空间。
- en: Delta-LoRA
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Delta-LoRA
- en: '![](../Images/ee2078075952954884f5c48df562c9e7.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee2078075952954884f5c48df562c9e7.png)'
- en: Delta-LoRA doesn’t freeze the matrix W but updates it with the gradient obtained
    from B*A. Image from [8].
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Delta-LoRA并不冻结矩阵W，而是通过从B*A获得的梯度更新它。图片来自[8]。
- en: '**Delta-LoRA** [8]introduces yet another idea to improve LoRA. This time, the
    pre-trained matrix W comes into play again. Remember that the main idea in LoRA
    is to not (!) tune the pre-trained matrix W, as that is too costly (and that would
    be normal fine-tuning). That is why LoRA introduced new smaller matrices A and
    B. However, those smaller matrices have less capability to learn the downstream
    task, which is why the performance of a LoRA-trained model is often lower than
    the performance of a fine-tuned model. Tuning W during training would be great,
    but how can we afford that?'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**Delta-LoRA** [8]引入了另一个改进LoRA的想法。这一次，预训练矩阵W再次发挥了作用。记住，LoRA的主要思想是不要（！）调整预训练矩阵W，因为那样代价太高（这将是正常的微调）。这就是为什么LoRA引入了新的较小矩阵A和B。然而，这些较小的矩阵在学习下游任务时的能力较弱，这也是为什么LoRA训练的模型性能通常低于微调模型性能的原因。在训练过程中调整W是非常理想的，但我们如何负担得起呢？'
- en: 'The authors of Delta-LoRA propose to update the matrix W by the gradients of
    A*B, which is the difference between A*B in two consecutive time steps. This gradient
    is scaled with some hyperparameter λ, which controls, how big the influence of
    the new training on the pre-trained weights should be, and is then added to W
    (while α and r (the rank) are hyperparameters from the original LoRA setup):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Delta-LoRA的作者提议通过A*B的梯度来更新矩阵W，即A*B在两个连续时间步之间的差异。这个梯度会通过某个超参数λ进行缩放，λ控制新训练对预训练权重的影响程度，然后将其加到W中（同时α和r（秩）是原始LoRA设置中的超参数）：
- en: '![](../Images/ac8506bf2baf8c7b83f8340357e8f40c.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac8506bf2baf8c7b83f8340357e8f40c.png)'
- en: W is updated with the difference of AB in two consecutive steps. Image from
    [8].
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: W通过两步之间AB的差值来更新。图片来源于[8]。
- en: That introduces more parameters to be trained at almost no computational overhead.
    We do not have to calculate the gradient for the whole matrix W, as we would within
    finetuning, but update it with a gradient we already got in the LoRA training
    anyway. The authors compared this method on a number of benchmarks using models
    like RoBERTA and GPT-2 and found a boost in performance over the standard LoRA
    approach.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这引入了更多的参数进行训练，而几乎没有计算开销。我们不需要像在微调中那样为整个矩阵W计算梯度，而是利用在LoRA训练过程中已经得到的梯度来更新它。作者在多个基准测试中使用RoBERTA和GPT-2等模型对比了这种方法，并发现其在性能上优于标准的LoRA方法。
- en: Summary
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: '![](../Images/52b06c7aabc1374011d32be954221f02.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52b06c7aabc1374011d32be954221f02.png)'
- en: Congrats. You’ve made it to the end. Photo by [david Griffiths](https://unsplash.com/@itscakefortea?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，已经读完了。图片来源：[david Griffiths](https://unsplash.com/@itscakefortea?utm_source=medium&utm_medium=referral)来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'We just saw a number of approaches, that vary the core idea of LoRA to reduce
    computation time or improve performance (or both). In the end, I will give a short
    summary of the different approaches:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了多种方法，它们在LoRA的核心思想上有所变化，目的是减少计算时间或提高性能（或两者兼具）。最后，我将简要总结这些不同的方法：
- en: '**LoRA** introduces low-rank matrices A and B that are trained, while the pre-trained
    weight matrix W is frozen.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LoRA**引入了低秩矩阵A和B进行训练，同时预训练的权重矩阵W保持冻结。'
- en: '**LoRA+** suggests having a much higher learning rate for B than for A.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LoRA+**建议B的学习率远高于A。'
- en: '**VeRA** does not train A and B, but initializes them randomly and trains new
    vectors d and b on top.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VeRA**不训练A和B，而是随机初始化它们，并在其上训练新的向量d和b。'
- en: '**LoRA-FA** only trains matrix B.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LoRA-FA**仅训练矩阵B。'
- en: '**LoRA-drop** uses the output of B*A to determine, which layers are worth to
    be trained at all.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LoRA-drop**使用B*A的输出来确定哪些层值得进行训练。'
- en: '**AdaLoRA** adapts the ranks of A and B in different layers dynamically, allowing
    for a higher rank in these layers, where more contribution to the model’s performance
    is expected.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AdaLoRA**动态地调整A和B在不同层中的秩，允许在这些层中使用更高的秩，尤其是在期望对模型性能贡献较大的层中。'
- en: '**DoRA** splits the LoRA adapter into two components of magnitude and direction
    and allows to train them more independently.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DoRA**将LoRA适配器分为幅度和方向两个组件，并允许它们更加独立地进行训练。'
- en: '**Delta-LoRA** changes the weights of W by the gradient of A*B.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Delta-LoRA**通过A*B的梯度来改变W的权重。'
- en: The field of research on LoRA and related methods is very rich and vivid, with
    new contributions every other day. In this article, I wanted to explain the core
    ideas of some approaches. Naturally, that was only a selection of such, that is
    far away from being a complete review.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA及相关方法的研究领域非常丰富且生动，几乎每天都有新的贡献。在本文中，我想解释一些方法的核心思想。当然，这只是其中的一部分，远远不能算作完整的综述。
- en: I hope that I have been able to share some knowledge with you and possibly inspire
    you to new ideas. LoRA and related methods are a field of research with great
    potential, as we saw. New breakthroughs in improving performance or computation
    time in training large language models can be expected soon, I suppose.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望我能够与您分享一些知识，并可能激发您产生新的想法。正如我们所看到的，LoRA及相关方法是一个具有巨大潜力的研究领域。预计在提升大规模语言模型训练性能或计算时间方面，新的突破很快就会出现。
- en: References and Further Reading
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献及进一步阅读
- en: 'These are the papers on the concepts explained in this article:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是本文中解释的概念相关的论文：
- en: '**[1]** **LoRA**: [Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y.,
    Wang, S., … & Chen, W. (2021). Lora: Low-rank adaptation of large language models.
    *arXiv preprint arXiv:2106.09685*.](https://arxiv.org/pdf/2106.09685.pdf)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[1]** **LoRA**: [Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y.,
    Wang, S., … & Chen, W. (2021). LoRA: 大规模语言模型的低秩适应. *arXiv预印本 arXiv:2106.09685*.](https://arxiv.org/pdf/2106.09685.pdf)'
- en: '**[2] LoRA**+: [Hayou, S., Ghosh, N., & Yu, B. (2024). LoRA+: Efficient Low
    Rank Adaptation of Large Models. *arXiv preprint arXiv:2402.12354*.](https://arxiv.org/pdf/2402.12354.pdf)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[2] LoRA**+: [Hayou, S., Ghosh, N., & Yu, B. (2024). LoRA+: 大规模模型的高效低秩适应.
    *arXiv预印本 arXiv:2402.12354*.](https://arxiv.org/pdf/2402.12354.pdf)'
- en: '**[3] VeRA**: [Kopiczko, D. J., Blankevoort, T., & Asano, Y. M. (2023). Vera:
    Vector-based random matrix adaptation. *arXiv preprint arXiv:2310.11454*.](https://arxiv.org/pdf/2310.11454.pdf)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[3] VeRA**: [Kopiczko, D. J., Blankevoort, T., & Asano, Y. M. (2023). Vera:
    基于向量的随机矩阵自适应. *arXiv预印本 arXiv:2310.11454*.](https://arxiv.org/pdf/2310.11454.pdf)'
- en: '**[4]: LoRA-FA**: [Zhang, L., Zhang, L., Shi, S., Chu, X., & Li, B. (2023).
    Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning.
    *arXiv preprint arXiv:2308.03303*.](https://arxiv.org/pdf/2308.03303.pdf)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[4]: LoRA-FA**: [Zhang, L., Zhang, L., Shi, S., Chu, X., & Li, B. (2023).
    Lora-fa: 针对大规模语言模型微调的内存高效低秩适应. *arXiv预印本 arXiv:2308.03303*.](https://arxiv.org/pdf/2308.03303.pdf)'
- en: '**[5] LoRA-drop**: [Zhou, H., Lu, X., Xu, W., Zhu, C., & Zhao, T. (2024). LoRA-drop:
    Efficient LoRA Parameter Pruning based on Output Evaluation. *arXiv preprint arXiv:2402.07721*.](https://arxiv.org/pdf/2402.07721.pdf)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[5] LoRA-drop**: [Zhou, H., Lu, X., Xu, W., Zhu, C., & Zhao, T. (2024). LoRA-drop:
    基于输出评估的高效LoRA参数剪枝. *arXiv预印本 arXiv:2402.07721*.](https://arxiv.org/pdf/2402.07721.pdf)'
- en: '**[6] AdaLoRA**: [Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen,
    W., & Zhao, T. (2023). Adaptive budget allocation for parameter-efficient fine-tuning.
    *arXiv preprint arXiv:2303.10512*.](https://arxiv.org/pdf/2303.10512.pdf)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[6] AdaLoRA**: [Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen,
    W., & Zhao, T. (2023). 参数高效微调的自适应预算分配. *arXiv预印本 arXiv:2303.10512*.](https://arxiv.org/pdf/2303.10512.pdf)'
- en: '**[7] DoRA**: [Liu, S. Y., Wang, C. Y., Yin, H., Molchanov, P., Wang, Y. C.
    F., Cheng, K. T., & Chen, M. H. (2024). DoRA: Weight-Decomposed Low-Rank Adaptation.
    *arXiv preprint arXiv:2402.09353*.](https://arxiv.org/pdf/2402.09353.pdf)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[7] DoRA**: [Liu, S. Y., Wang, C. Y., Yin, H., Molchanov, P., Wang, Y. C.
    F., Cheng, K. T., & Chen, M. H. (2024). DoRA: 权重分解低秩适应. *arXiv预印本 arXiv:2402.09353*.](https://arxiv.org/pdf/2402.09353.pdf)'
- en: '**[8]: Delta-LoRA**: [Zi, B., Qi, X., Wang, L., Wang, J., Wong, K. F., & Zhang,
    L. (2023). Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank
    matrices. *arXiv preprint arXiv:2309.02411*.](https://openreview.net/pdf?id=FAO4VS9QRV)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[8]: Delta-LoRA**: [Zi, B., Qi, X., Wang, L., Wang, J., Wong, K. F., & Zhang,
    L. (2023). Delta-lora: 用低秩矩阵的增量微调高秩参数. *arXiv预印本 arXiv:2309.02411*.](https://openreview.net/pdf?id=FAO4VS9QRV)'
- en: 'For some core ideas on random projection, as mentioned in the section on VeRA,
    this is one of the major contributions to the field:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于关于随机投影的一些核心思想，正如在VeRA一节中提到的，这篇文章是该领域的主要贡献之一：
- en: '[Frankle, J., & Carbin, M. (2018). The lottery ticket hypothesis: Finding sparse,
    trainable neural networks. *arXiv preprint arXiv:1803.03635*.](https://arxiv.org/pdf/1803.03635.pdf)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Frankle, J., & Carbin, M. (2018). 彩票票据假设：寻找稀疏的可训练神经网络. *arXiv预印本 arXiv:1803.03635*.](https://arxiv.org/pdf/1803.03635.pdf)'
- en: 'For a more fine-grained explanation of LoRA and DoRA, I can recommend this
    article:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 若需要更细致地解释LoRA和DoRA，我可以推荐这篇文章：
- en: '[https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch](https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch](https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch)'
- en: '*Like this article?* [*Follow me*](https://medium.com/@doriandrost) *to be
    notified of my future posts.*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*喜欢这篇文章吗？* [*关注我*](https://medium.com/@doriandrost) *以便接收我未来文章的通知。*'
