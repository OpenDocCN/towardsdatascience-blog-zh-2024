["```py\nval sparkConnectUri: Option[String] = Option(System.getenv(\"SPARK_CONNECT_URI\"))\n\nval isSparkConnectMode: Boolean = sparkConnectUri.isDefined\n\ndef createSparkSession(): SparkSession = {\n  if (isSparkConnectMode) {\n    createRemoteSparkSession()\n  } else {\n    SparkSession.builder\n      // Whatever you need to do to configure SparkSession for a separate \n      // Spark application.\n      .getOrCreate\n  }\n}\n\nprivate def createRemoteSparkSession(): SparkSession = {\n  val uri = sparkConnectUri.getOrElse(throw new Exception(\n    \"Required environment variable 'SPARK_CONNECT_URI' is not set.\"))\n\n  val builder = SparkSession.builder\n  // Reflection is used here because the regular SparkSession API does not \n  // contain these methods. They are only available in the SparkSession API \n  // version for Spark Connect.\n  classOf[SparkSession.Builder]\n    .getDeclaredMethod(\"remote\", classOf[String])\n    .invoke(builder, uri)\n\n  // A set of identifiers for this application (to be used later).\n  val scAppId = s\"spark-connect-${UUID.randomUUID()}\"\n  val airflowTaskId = Option(System.getenv(\"AIRFLOW_TASK_ID\"))\n    .getOrElse(\"unknown_airflow_task_id\")\n  val session = builder\n    .config(\"spark.joom.scAppId\", scAppId)\n    .config(\"spark.joom.airflowTaskId\", airflowTaskId)\n    .getOrCreate()\n\n  // If the client application uses your Scala code (e.g., custom UDFs), \n  // then you must add the jar artifact containing this code so that it \n  // can be used on the Spark Connect server side.\n  val addArtifact = Option(System.getenv(\"ADD_ARTIFACT_TO_SC_SESSION\"))\n    .forall(_.toBoolean)\n\n  if (addArtifact) {\n    val mainApplicationFilePath = \n      System.getenv(\"SPARK_CONNECT_MAIN_APPLICATION_FILE_PATH\")\n    classOf[SparkSession]\n      .getDeclaredMethod(\"addArtifact\", classOf[String])\n      .invoke(session, mainApplicationFilePath)\n  }\n\n  Runtime.getRuntime.addShutdownHook(new Thread() {\n    override def run(): Unit = {\n      session.close()\n    }\n  })\n\n  session\n}\n```", "```py\nFROM openjdk:11-jre-slim\n\nWORKDIR /app\n\n# Here, we copy the common artifacts required for any of our Spark Connect \n# clients (primarily spark-connect-client-jvm, as well as spark-hive, \n# hadoop-aws, scala-library, etc.).\nCOPY build/libs/* /app/\n\nCOPY src/main/docker/entrypoint.sh /app/\nRUN chmod +x ./entrypoint.sh\nENTRYPOINT [\"./entrypoint.sh\"]\n```", "```py\n#!/bin/bash\nset -eo pipefail\n\n# This variable will also be used in the SparkSession builder within \n# the application code.\nexport SPARK_CONNECT_MAIN_APPLICATION_FILE_PATH=\"/tmp/$(uuidgen).jar\"\n\n# Download the JAR with the code and specific dependencies of the client \n# application to be run. All such JAR files are stored in S3, and when \n# creating a client Pod, the path to the required JAR is passed to it \n# via environment variables.\njava -cp \"/app/*\" com.joom.analytics.sc.client.S3Downloader \\ \n    ${MAIN_APPLICATION_FILE_S3_PATH} ${SPARK_CONNECT_MAIN_APPLICATION_FILE_PATH}\n\n# Launch the client application. Any MAIN_CLASS initializes a SparkSession \n# at the beginning of its execution using the code provided above.\njava -cp ${SPARK_CONNECT_MAIN_APPLICATION_FILE_PATH}:\"/app/*\" ${MAIN_CLASS} \"$@\"\n```", "```py\ndef delete(path: Path, recursive: Boolean = true)\n          (implicit hadoopConfig: Configuration): Boolean = {\n  val fs = path.getFileSystem(hadoopConfig)\n  fs.delete(path, recursive)\n}\n```", "```py\nimport org.apache.spark.SparkConf\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.spark.sql.hive.StandaloneHiveExternalCatalog\nimport org.apache.spark.sql.catalyst.catalog.{ExternalCatalogWithListener, SessionCatalog}\n\n// This is just an example of what the required properties might look like. \n// All of them should already be set for existing Spark applications in one \n// way or another, and their complete list can be found in the UI of any\n// running separate Spark application on the Environment tab.\nval sessionCatalogConfig = Map(\n  \"spark.hadoop.hive.metastore.uris\" -> \"thrift://metastore.spark:9083\",\n  \"spark.sql.catalogImplementation\" -> \"hive\",\n  \"spark.sql.catalog.spark_catalog\" -> \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n)\n\nval hadoopConfig = Map(\n  \"hive.metastore.uris\" -> \"thrift://metastore.spark:9083\",\n  \"fs.s3.impl\" -> \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n  \"fs.s3a.aws.credentials.provider\" -> \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\",\n  \"fs.s3a.endpoint\" -> \"s3.amazonaws.com\",\n  // and others...\n)\n\ndef createStandaloneSessionCatalog(): (SessionCatalog,  Configuration) = {\n  val sparkConf = new SparkConf().setAll(sessionCatalogConfig)\n  val hadoopConfiguration = new Configuration()\n  hadoopConfig.foreach { \n    case (key, value) => hadoopConfiguration.set(key, value) \n  }\n\n  val externalCatalog = new StandaloneHiveExternalCatalog(\n    sparkConf, hadoopConfiguration)\n  val sessionCatalog = new SessionCatalog(\n    new ExternalCatalogWithListener(externalCatalog)\n  )\n  (sessionCatalog, hadoopConfiguration)\n}\n```", "```py\npackage org.apache.spark.sql.hive\n\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.spark.SparkConf\n\nclass StandaloneHiveExternalCatalog(conf: SparkConf, hadoopConf: Configuration) \n  extends HiveExternalCatalog(conf, hadoopConf)\n```", "```py\nval session = builder\n  .config(\"spark.joom.scAppId\", scAppId)\n  .config(\"spark.joom.airflowTaskId\", airflowTaskId)\n  .getOrCreate()\n```", "```py\nclass StatsReportingSparkListener extends SparkListener {\n\n  override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = {\n    val stageId = stageSubmitted.stageInfo.stageId\n    val stageAttemptNumber = stageSubmitted.stageInfo.attemptNumber()\n    val scAppId = stageSubmitted.properties.getProperty(\"spark.joom.scAppId\")\n    // ...\n  }\n}\n```", "```py\n// Using dynamicAllocation is important for the Spark Connect server \n// because the workload can be very unevenly distributed over time.\nspark.dynamicAllocation.enabled: true  // default: false\n\n// This pair of parameters is responsible for the timely removal of idle \n// executors:\nspark.dynamicAllocation.cachedExecutorIdleTimeout: 5m  // default: infinity\nspark.dynamicAllocation.shuffleTracking.timeout: 5m  // default: infinity\n\n// To create new executors only when the existing ones cannot handle \n// the received tasks for a significant amount of time. This allows you \n// to save resources when a small number of tasks arrive at some point \n// in time, which do not require many executors for timely processing. \n// With increased schedulerBacklogTimeout, unnecessary executors do not \n// have the opportunity to appear by the time all incoming tasks are \n// completed. The time to complete the tasks increases slightly with this, \n// but in most cases, this increase is not significant.\nspark.dynamicAllocation.schedulerBacklogTimeout: 30s  // default: 1s\n\n// If, for some reason, you need to stop the execution of a client \n// application (and free up resources), you can forcibly terminate the client. \n// Currently, even explicitly closing the client SparkSession does not \n// immediately end the execution of its corresponding Jobs on the server. \n// They will continue to run for a duration equal to 'detachedTimeout'. \n// Therefore, it may be reasonable to reduce it.\nspark.connect.execute.manager.detachedTimeout: 2m  // default: 5m\n\n// We have encountered a situation when killed tasks may hang for \n// an unpredictable amount of time, leading to bad consequences for their \n// executors. In this case, it is better to remove the executor on which \n// this problem occurred.\nspark.task.reaper.enabled: true // default: false\nspark.task.reaper.killTimeout: 300s  // default: -1\n\n// The Spark Connect server can run for an extended period of time. During \n// this time, executors may fail, including for reasons beyond our control \n// (e.g., AWS Spot interruptions). This option is needed to prevent \n// the entire server from failing in such cases.\nspark.executor.maxNumFailures: 1000\n\n// In our experience, BroadcastJoin can lead to very serious performance \n// issues in some cases. So, we decided to disable broadcasting. \n// Disabling this option usually does not result in a noticeable performance \n// degradation for our typical applications anyway.\nspark.sql.autoBroadcastJoinThreshold: -1 // default: 10MB\n\n// For many of our client applications, we have to add an artifact to \n// the client session (method sparkSession.addArtifact()). \n// Using 'useFetchCache=true' results in double space consumption for \n// the application JAR files on executors' disks, as they are also duplicated \n// in a local cache folder. Sometimes, this even causes disk overflow with \n// subsequent problems for the executor.\nspark.files.useFetchCache: false   // default: true\n\n// To ensure fair resource allocation when multiple applications are \n// running concurrently.\nspark.scheduler.mode: FAIR  // default: FIFO\n```"]