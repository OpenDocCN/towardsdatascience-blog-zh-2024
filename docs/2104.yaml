- en: 'Mistral-NeMo: 4.1x Smaller with Quantized Minitron'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mistral-nemo-4-1x-smaller-with-quantized-minitron-9d6ad7b70981?source=collection_archive---------7-----------------------#2024-08-29](https://towardsdatascience.com/mistral-nemo-4-1x-smaller-with-quantized-minitron-9d6ad7b70981?source=collection_archive---------7-----------------------#2024-08-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How pruning, knowledge distillation, and 4-bit quantization can make advanced
    AI models more accessible and cost-effective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--9d6ad7b70981--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--9d6ad7b70981--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9d6ad7b70981--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9d6ad7b70981--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--9d6ad7b70981--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9d6ad7b70981--------------------------------)
    ·9 min read·Aug 29, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/adf13515087f8d860ba67eab0cb2f9c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author — Made with an illustration from [Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA’s Minitron compresses large language models (LLMs) by pruning the least
    important weights, followed by retraining through knowledge distillation. This
    approach significantly reduces model sizes while preserving their accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[NVIDIA released Minitron versions of Llama 3.1 and Mistral-NeMo](https://developer.nvidia.com/blog/mistral-nemo-minitron-8b-foundation-model-delivers-unparalleled-accuracy/?ncid=ref-inor-263670%2F),
    reducing their number of parameters from 8B to 4B and 12B to 8B, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Why is this important?*'
  prefs: []
  type: TYPE_NORMAL
- en: While Mistral-NeMo can’t run on a consumer GPU, its Minitron version can. A
    24 GB GPU would be enough. However, this could also be achieved by quantizing
    Mistral-NeMo. 4-bit quantization methods are now accurate enough.
  prefs: []
  type: TYPE_NORMAL
- en: '*But what if we could also quantize a Minitron model? Is quantization still
    accurate enough for a model that has been pruned with Minitron?*'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a 4-bit version of Mistral-NeMo-Minitron would run on an 8 GB
    GPU, significantly bringing down inference costs.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I review the Minitron approach, exploring how to compress LLMs
    through pruning and knowledge distillation. We will…
  prefs: []
  type: TYPE_NORMAL
