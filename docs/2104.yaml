- en: 'Mistral-NeMo: 4.1x Smaller with Quantized Minitron'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Mistral-NeMo: 通过量化Minitron减少4.1倍大小'
- en: 原文：[https://towardsdatascience.com/mistral-nemo-4-1x-smaller-with-quantized-minitron-9d6ad7b70981?source=collection_archive---------7-----------------------#2024-08-29](https://towardsdatascience.com/mistral-nemo-4-1x-smaller-with-quantized-minitron-9d6ad7b70981?source=collection_archive---------7-----------------------#2024-08-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mistral-nemo-4-1x-smaller-with-quantized-minitron-9d6ad7b70981?source=collection_archive---------7-----------------------#2024-08-29](https://towardsdatascience.com/mistral-nemo-4-1x-smaller-with-quantized-minitron-9d6ad7b70981?source=collection_archive---------7-----------------------#2024-08-29)
- en: How pruning, knowledge distillation, and 4-bit quantization can make advanced
    AI models more accessible and cost-effective
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剪枝、知识蒸馏和4位量化如何使先进的AI模型变得更加易于访问和具备成本效益
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--9d6ad7b70981--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--9d6ad7b70981--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9d6ad7b70981--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9d6ad7b70981--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--9d6ad7b70981--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--9d6ad7b70981--------------------------------)[![本杰明·马里](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--9d6ad7b70981--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9d6ad7b70981--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9d6ad7b70981--------------------------------)
    [本杰明·马里](https://medium.com/@bnjmn_marie?source=post_page---byline--9d6ad7b70981--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9d6ad7b70981--------------------------------)
    ·9 min read·Aug 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9d6ad7b70981--------------------------------)
    ·9分钟阅读·2024年8月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/adf13515087f8d860ba67eab0cb2f9c9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adf13515087f8d860ba67eab0cb2f9c9.png)'
- en: Image by the author — Made with an illustration from [Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片 — 制作自[Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)的插图
- en: NVIDIA’s Minitron compresses large language models (LLMs) by pruning the least
    important weights, followed by retraining through knowledge distillation. This
    approach significantly reduces model sizes while preserving their accuracy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA的Minitron通过剪枝最不重要的权重来压缩大型语言模型（LLMs），然后通过知识蒸馏进行重新训练。这种方法显著减少了模型的大小，同时保持了其准确性。
- en: '[NVIDIA released Minitron versions of Llama 3.1 and Mistral-NeMo](https://developer.nvidia.com/blog/mistral-nemo-minitron-8b-foundation-model-delivers-unparalleled-accuracy/?ncid=ref-inor-263670%2F),
    reducing their number of parameters from 8B to 4B and 12B to 8B, respectively.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[NVIDIA发布了Llama 3.1和Mistral-NeMo的Minitron版本](https://developer.nvidia.com/blog/mistral-nemo-minitron-8b-foundation-model-delivers-unparalleled-accuracy/?ncid=ref-inor-263670%2F)，分别将其参数数量从8B减少到4B，以及从12B减少到8B。'
- en: '*Why is this important?*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么这很重要？*'
- en: While Mistral-NeMo can’t run on a consumer GPU, its Minitron version can. A
    24 GB GPU would be enough. However, this could also be achieved by quantizing
    Mistral-NeMo. 4-bit quantization methods are now accurate enough.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Mistral-NeMo无法在消费级GPU上运行，但其Minitron版本可以。一个24 GB的GPU就足够了。然而，这也可以通过对Mistral-NeMo进行量化来实现。4位量化方法现在已经足够准确。
- en: '*But what if we could also quantize a Minitron model? Is quantization still
    accurate enough for a model that has been pruned with Minitron?*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*但是，如果我们也能对Minitron模型进行量化呢？对于一个经过Minitron剪枝的模型，量化是否仍然足够准确？*'
- en: For instance, a 4-bit version of Mistral-NeMo-Minitron would run on an 8 GB
    GPU, significantly bringing down inference costs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Mistral-NeMo-Minitron的4位版本可以在8 GB的GPU上运行，显著降低推理成本。
- en: In this article, I review the Minitron approach, exploring how to compress LLMs
    through pruning and knowledge distillation. We will…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我回顾了Minitron方法，探讨了如何通过剪枝和知识蒸馏来压缩大型语言模型（LLMs）。我们将…
