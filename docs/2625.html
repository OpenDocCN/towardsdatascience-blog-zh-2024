<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>GSM-Symbolic: Analyzing LLM Limitations in Mathematical Reasoning and Potential Solutions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>GSM-Symbolic: Analyzing LLM Limitations in Mathematical Reasoning and Potential Solutions</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gsm-symbolic-analyzing-llm-limitations-in-mathematical-reasoning-and-potential-solutions-363b82370a26?source=collection_archive---------8-----------------------#2024-10-28">https://towardsdatascience.com/gsm-symbolic-analyzing-llm-limitations-in-mathematical-reasoning-and-potential-solutions-363b82370a26?source=collection_archive---------8-----------------------#2024-10-28</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a0ff" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">What The Paper on LLM Reasoning Got Right — And What It Missed.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@zredlined?source=post_page---byline--363b82370a26--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Alexander Watson" class="l ep by dd de cx" src="../Images/aea574d9652ea8b1b91d4ec8a9c88ef8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*DxaGXFaUaIhZEn22D3yg-A.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--363b82370a26--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@zredlined?source=post_page---byline--363b82370a26--------------------------------" rel="noopener follow">Alexander Watson</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--363b82370a26--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 28, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="9467" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Co-authors: Alex Watson, Yev Meyer, Dane Corneil, Maarten Van Segbroeck (Gretel.ai)</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf ng"><img src="../Images/8a06f7b26698ac70c2f8d5922ee6ca19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mPeRWwM5dxMNvQCeUydo3Q.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Source: Gretel.ai</figcaption></figure><h1 id="37d0" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Introduction</h1><p id="fafd" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">Large language models (LLMs) have recently made significant strides in AI reasoning, including mathematical problem-solving. However, a recent paper titled “<a class="af oy" href="https://arxiv.org/pdf/2410.05229" rel="noopener ugc nofollow" target="_blank">GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models</a>” by Mirzadeh et al. raises questions about the true capabilities of these models when it comes to mathematical reasoning. We have reviewed the paper and found it to be a valuable contribution to the ongoing discussion about AI capabilities and limitations, however, our analysis suggests that its conclusions may not fully capture the complexity of the issue.</p><h1 id="46c6" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">The GSM-Symbolic Benchmark</h1><p id="eb4b" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">The authors introduce GSM-Symbolic, an enhanced benchmark derived from the popular GSM8K dataset. This new benchmark allows for the generation of diverse question variants, enabling a more nuanced evaluation of LLMs’ performance across various setups. The study’s large-scale analysis of 25 state-of-the-art open and closed models provides significant insights into how these models behave when faced with mathematical reasoning tasks.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf oz"><img src="../Images/c6e00b2fb8155010c499955f50509297.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Xvj0SF8x-Aj5iM_3"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Figure 1: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models (Source: <a class="af oy" href="https://arxiv.org/abs/2410.05229" rel="noopener ugc nofollow" target="_blank">Mirzadeh et al., GSM-Symbolic Paper</a>)</figcaption></figure><h1 id="d21e" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Performance Variability and Model Comparisons</h1><p id="7ceb" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">One of the most surprising findings is the high variability in model performance across different instantiations of the same question. All models exhibit “significant variability in accuracy” when tested on GSM-Symbolic. This variability raises concerns about the reliability of currently reported metrics on the <a class="af oy" href="https://huggingface.co/datasets/openai/gsm8k" rel="noopener ugc nofollow" target="_blank">GSM8K</a> benchmark, which relies on single point-accuracy responses.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf pa"><img src="../Images/51313010f4883f6faa35b8d5b355484b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ADI_t3okxemVZhH6"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Figure 3: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models (Source: <a class="af oy" href="https://arxiv.org/abs/2410.05229" rel="noopener ugc nofollow" target="_blank">Mirzadeh et al., GSM-Symbolic Paper</a>)</figcaption></figure><p id="7c20" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Not all models are created equal.</strong> <code class="cx pb pc pd pe b">Llama-3–8b</code> and <code class="cx pb pc pd pe b">GPT-4o</code> are clear outliers in that they don’t exhibit as significant of a drop on the new benchmark as other models like <code class="cx pb pc pd pe b">gemma-2–9b</code>, <code class="cx pb pc pd pe b">phi-3</code>, <code class="cx pb pc pd pe b">phi-3.5</code> and <code class="cx pb pc pd pe b">mathstral-7b</code>. This observations suggests two important points:</p><ol class=""><li id="f98a" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pf pg ph bk"><code class="cx pb pc pd pe b">Llama-3–8b</code> and <code class="cx pb pc pd pe b">GPT-4o</code> generally demonstrate a more robust understanding of mathematical concepts, although they are still not immune to performance variations.</li><li id="e543" class="mi mj fq mk b go pi mm mn gr pj mp mq mr pk mt mu mv pl mx my mz pm nb nc nd pf pg ph bk">The training data for <code class="cx pb pc pd pe b">Llama-3–8b</code> and <code class="cx pb pc pd pe b">GPT-4o</code> likely has not been contaminated (or at least not to the same extent) with GSM8K data. In this context, data contamination refers to the unintentional inclusion of test or benchmark data in a model’s training set, leading to artificially inflated model performance during evaluation. If contamination had occurred, as the authors hypothesize for some models, we would expect to see very high performance on GSM8K but significantly lower performance on even slight variations of these problems.</li></ol><p id="260a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">These findings highlight a opportunity for improvement through the use of synthetic data, where properly designed synthetic datasets can address both of these points for anyone training models:</p><ol class=""><li id="3c83" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pf pg ph bk">To mitigate potential data contamination issues, there’s no need to use the original GSM8K data in training when high-quality synthetic versions can be generated (<a class="af oy" href="https://gretel.ai/blog/teaching-ai-to-think-a-new-approach-with-synthetic-data-and-reflection" rel="noopener ugc nofollow" target="_blank">blog link</a>). These synthetic datasets retain the mathematical reasoning challenges of GSM8K without reusing the exact problems or solutions, thus preserving the integrity of the model’s evaluation.</li><li id="3330" class="mi mj fq mk b go pi mm mn gr pj mp mq mr pk mt mu mv pl mx my mz pm nb nc nd pf pg ph bk">Even more importantly, it’s possible to generate synthetic data that surpass the quality of both the OpenAI GSM8K and Apple GSM-Symbolic datasets. This approach can lead to a more robust understanding of mathematical concepts, addressing the performance variability observed in current models.</li></ol><h1 id="e944" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Sensitivity to Changes and Complexity</h1><p id="d83e" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">The authors show that LLMs are more sensitive to changes in numerical values than to changes in proper names within problems, suggesting that the models’ understanding of the underlying mathematical concepts may not be as robust as previously thought. As the complexity of questions increases (measured by the number of clauses), the performance of all models degrades, and the variance in their performance increases. This highlights the importance of using diverse data in training, and this is something that synthetics can help with. As the authors demonstrate, there is logically no reason why a AI model should perform worse on a given set of problems, with just a simple change in numbers or a slight variation in the number of clauses.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf pn"><img src="../Images/b953ae222b4dcb1370c9695ed3b319d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yKgRCPR1C0NZUtiU"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Figure 4: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models (Source: <a class="af oy" href="https://arxiv.org/abs/2410.05229" rel="noopener ugc nofollow" target="_blank">Mirzadeh et al., GSM-Symbolic Paper</a>)</figcaption></figure><h1 id="2a14" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">The GSM-NoOp Challenge</h1><p id="b8a6" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">Perhaps the most concerning finding is the introduction of GSM-NoOp, a dataset designed to challenge the reasoning capabilities of LLMs. By adding seemingly relevant but ultimately inconsequential information to problems, the authors observed substantial performance drops across all models — up to 65% for some. The authors propose that this points to current LLMs relying more on a type of pattern matching than true logical reasoning</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf po"><img src="../Images/a1b1f43f0cd7b8ca1a64325f5456e1a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qyqxRWyna2fec1et"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Figure 6: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models (Source: <a class="af oy" href="https://arxiv.org/abs/2410.05229" rel="noopener ugc nofollow" target="_blank">Mirzadeh et al., GSM-Symbolic Paper</a>)</figcaption></figure><h1 id="e9d7" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">A Critical Perspective on the Paper’s Conclusions</h1><p id="2782" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">While the GSM-Symbolic study provides valuable insights into the performance of LLMs on mathematical reasoning tasks, it’s important to critically examine the paper’s conclusions. The authors argue that the observed limitations suggest LLMs are not capable of true logical reasoning. However, this interpretation may be oversimplifying a complex issue.</p><p id="02d6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The paper’s argument for LLMs relying on pattern matching rather than reasoning seems less definitive when examined closely. It’s clear that these models are not perfect reasoners — if they were, they would achieve 100% accuracy on GSM8K. But the leap from imperfect performance to a lack of reasoning capability is not necessarily justified.</p><p id="32ce" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">There are at least two potential explanations for why LLMs, like humans, sometimes get questions wrong</strong>:</p><ol class=""><li id="1d1a" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pf pg ph bk">The model tries to strictly pattern match a problem to something it has seen before, and fails if it can’t.</li><li id="e97a" class="mi mj fq mk b go pi mm mn gr pj mp mq mr pk mt mu mv pl mx my mz pm nb nc nd pf pg ph bk">The model tries to follow a logical program but has a certain (compounding) probability of making an error at each step, as expected based on the fact that it literally samples tokens.</li></ol><p id="7128" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The paper seems to lean towards explanation (1), but doesn’t make a convincing case for why this should be preferred over explanation (2). In fact, (2) is more akin to human-like reasoning and potentially more interesting from a research perspective.</p><p id="8317" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Let’s examine each main finding of the paper through this critical lens:</strong></p><p id="e714" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="pp">GSM-Symbolic Performance</em></p><p id="6708" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The GSM-Symbolic approach is a valuable method for dataset expansion, validating the potential of synthetic data generation techniques like those used by Gretel. However, it’s worth noting that model performance doesn’t completely fall apart on these new variants — it just gets somewhat worse. If the models were strictly pattern matching, we might expect performance to drop to near zero on these new variants. The observed behavior seems more consistent with a model that can generalize to some degree but makes more errors on unfamiliar problem structures.</p><p id="5827" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Even human experts are not infallible. On the MATH benchmark, for instance, former math olympians typically scored 18/20 or 19/20, making small arithmetic errors. This suggests that error-prone reasoning, rather than a lack of reasoning capability, might be a more accurate description of both human and LLM performance.</p><p id="3b5e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="pp">Varying Difficulty</em></p><p id="6cda" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The paper’s findings on performance degradation with increasing question complexity are consistent with the idea of compounding errors in a multi-step reasoning process. As the number of steps increases, so does the probability of making an error at some point in the chain. This behavior is observed in human problem-solving as well and doesn’t necessarily indicate a lack of reasoning ability.</p><p id="b84b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="pp">GSM-NoOp Challenge</em></p><p id="80d0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The GSM-NoOp results, may not be as directly related to reasoning capability as the paper suggests. In real-world scenarios, we typically assume that all information provided in a problem statement is relevant. For instance, in the example question in Figure 7, a reasonable human might infer (like the LLMs did) that the size of the kiwis was only mentioned because they were discarded.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf pq"><img src="../Images/b20402670fc477c1e3f36a39ecb5f8db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZXFhuyiNQd6XmbLn"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Figure 7: GSM-Symbolic: Example GSM No-Op question. (Source: <a class="af oy" href="https://arxiv.org/abs/2410.05229" rel="noopener ugc nofollow" target="_blank">Mirzadeh et al., GSM-Symbolic Paper</a>)</figcaption></figure><p id="ddcd" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The ability to discern relevant information from irrelevant information, especially when the irrelevant information is inserted with the intent to be misleading (i.e. <em class="pp">seemingly</em> relevant), is a separate skill from pure mathematical reasoning.</p><p id="bc4a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The authors include a follow-up experiment (NoOp-NoOp) in which the models are implicitly “warned” of the misleading intent: they use few-shot examples that also contain irrelevant information. The subset of models illustrated with this experiment still show a drop in performance. <strong class="mk fr">Several follow-up experiments could serve to better understand the phenomenon</strong>:</p><ol class=""><li id="767a" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pf pg ph bk">Expand the NoOp-NoOp experiment to more models;</li><li id="ca73" class="mi mj fq mk b go pi mm mn gr pj mp mq mr pk mt mu mv pl mx my mz pm nb nc nd pf pg ph bk">Measure how well models perform when <em class="pp">explicitly</em> warned that some information may be irrelevant in the prompt;</li><li id="8f28" class="mi mj fq mk b go pi mm mn gr pj mp mq mr pk mt mu mv pl mx my mz pm nb nc nd pf pg ph bk">Fine-tune models on synthetic training examples that include irrelevant information in addition to examples that contain entirely relevant information.</li></ol><h1 id="3395" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Opportunities for Improvement: The Promise of Synthetic Data</h1><p id="66ac" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">While the paper by Mirzadeh et al. highlights important limitations in current LLMs, at Gretel we have developed datasets that address many of the challenges identified in the paper:</p><ol class=""><li id="76f1" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pf pg ph bk"><strong class="mk fr">Synthetic GSM8K </strong>Dataset: Available on HuggingFace at <a class="af oy" href="https://huggingface.co/datasets/gretelai/synthetic-gsm8k-reflection-405b" rel="noopener ugc nofollow" target="_blank">gretelai/synthetic-gsm8k-reflection-405b</a>, this dataset focuses on generating more complex, multi-step reasoning versions of problems than what existed in the original human generated dataset from OpenAI. It incorporates advanced prompting techniques, including Reflection and other cognitive models, to capture detailed reasoning processes. This approach has shown significant improvements, particularly for very hard problems, demonstrating its potential to enhance AI’s ability to handle complex, multi-step reasoning tasks. As covered in our blog, Gretel’s synthetic data created using these techniques achieved a <a class="af oy" href="https://gretel.ai/blog/teaching-ai-to-think-a-new-approach-with-synthetic-data-and-reflection" rel="noopener ugc nofollow" target="_blank">92.3% win-rate on problem complexity and an 82.7% win-rate for educational value over the standard Llama 3.1 405B parameter model outputs</a>, using these advanced techniques as judged by <code class="cx pb pc pd pe b">GPT-4o</code>- demonstrating that LLM reasoning can further be unlocked with more sophisticated training data examples and prompting techniques than the basic Chain-of-Thought used in the paper.</li></ol><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf pr"><img src="../Images/a28512feacedaced6552c69262035702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MU2kPk-UvS8C-WUg"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Source: <a class="af oy" href="https://gretel.ai/blog/teaching-ai-to-think-a-new-approach-with-synthetic-data-and-reflection" rel="noopener ugc nofollow" target="_blank">https://gretel.ai/blog/teaching-ai-to-think-a-new-approach-with-synthetic-data-and-reflection</a></figcaption></figure><p id="8670" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">2. Synthetic Text-to-SQL</strong> Dataset: Generated by Gretel to help improve LLMs ability to interact with SQL-based databases/warehouses &amp; lakes, available at <a class="af oy" href="https://huggingface.co/datasets/gretelai/synthetic_text_to_sql" rel="noopener ugc nofollow" target="_blank">gretelai/synthetic_text_to_sql</a>, has proven highly effective in improving model performance on Text-to-SQL tasks. When used to fine-tune CodeLlama models, <a class="af oy" href="https://gretel.ai/blog/fine-tuning-codellama-on-gretel-aws-sagemaker-jumpstart" rel="noopener ugc nofollow" target="_blank">it led to 36%+ improvements on the BIRD benchmark</a>, a challenging cross-domain Text-to-SQL evaluation platform. Further supporting the theory about today’s LLMs being trained on data that is too simple and leading to memorization, a single epoch of fine-tuning the <a class="af oy" href="https://youtu.be/jn6FuG4WA1c?feature=shared&amp;t=2420" rel="noopener ugc nofollow" target="_blank">Phi-3 and Llama 3.1 models on this dataset yielded a 300%+ improvement</a> on BIRD benchmark problems labeled as “very hard”.</p><p id="162e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">These results demonstratethat high-quality synthetic data can be a powerful tool in addressing the limitations of current LLMs in complex reasoning tasks.</p><h1 id="30cd" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Future Directions</h1><p id="ba26" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">In conclusion, the GSM-Symbolic paper provides valuable insights into the current limitations of LLMs in mathematical reasoning tasks. However, its conclusions should be approached critically. The observed behavior of LLMs could be interpreted in multiple ways, and it’s possible that the paper’s emphasis on pattern matching over reasoning may be oversimplifying a more complex issue.</p><p id="81a5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The limitations identified by the study are real and significant. The variability in performance, sensitivity to numerical changes, and struggles with irrelevant information all point to areas where current LLMs can be improved.</p><p id="f651" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">However, as demonstrated by more advanced models such as GPT-4o and Llama 3.1 above- by synthesizing diverse, challenging problem sets that push the boundaries of what AI models can tackle, we can develop LLMs that exhibit more robust, human-like reasoning capabilities.</p><h1 id="90d4" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">References</h1><ol class=""><li id="e192" class="mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd pf pg ph bk">I. Mirzadeh, K. Alizadeh, H. Shahrokhi, O. Tuzel, S. Bengio, and M. Farajtabar. <a class="af oy" href="https://arxiv.org/pdf/2410.05229" rel="noopener ugc nofollow" target="_blank">GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models.</a> 2024.</li></ol></div></div></div></div>    
</body>
</html>