- en: Integrating Multimodal Data into a Large Language Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/integrating-multimodal-data-into-a-large-language-model-d1965b8ab00c?source=collection_archive---------0-----------------------#2024-10-17](https://towardsdatascience.com/integrating-multimodal-data-into-a-large-language-model-d1965b8ab00c?source=collection_archive---------0-----------------------#2024-10-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Developing a context-retrieval, multimodal RAG using advanced parsing, semantic
    & keyword search, and re-ranking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@umairali.khan?source=post_page---byline--d1965b8ab00c--------------------------------)[![Umair
    Ali Khan](../Images/a6674b39315b20726aad1ba58b64ba12.png)](https://medium.com/@umairali.khan?source=post_page---byline--d1965b8ab00c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d1965b8ab00c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d1965b8ab00c--------------------------------)
    [Umair Ali Khan](https://medium.com/@umairali.khan?source=post_page---byline--d1965b8ab00c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d1965b8ab00c--------------------------------)
    ·15 min read·Oct 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '***If you are not Medium member, you can read the full article from*** [***this
    link***](/integrating-multimodal-data-into-a-large-language-model-d1965b8ab00c?sk=1d82aba6a10afb5f0a47d673307098d8)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) have a knowledge cutoff date and cannot answer
    queries to specific data not present in their knowledge base. For instance, LLMs
    cannot answer queries about data regarding a company’s meeting minutes from the
    last year. Similarly, LLMs are prone to hallucinate and provide plausible-looking
    wrong answers.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this issue, Retrieval Augment Generation (RAG) solutions are becoming
    increasingly popular. The main idea of an RAG is to integrate external documents
    into LLMs and guide its behavior to answer questions only from the external knowledge
    base. This is done by chunking the document(s) into smaller chunks, computing
    each chunk’s embeddings (numerical representations), and storing the embeddings
    as an index in a specialized vector database.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7574a6bb8213a8f8d798e7df3aa81427.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The RAG workflow: a query is converted to embeddings, matched with a vector
    database by a retrieval model, and combined with retrieved data to produce a response
    via an LLM (image by author).'
  prefs: []
  type: TYPE_NORMAL
- en: Contextual Retrieval RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The process of matching the user’s query with the small chunks in the vector
    database usually works well; however, it has the following issues:'
  prefs: []
  type: TYPE_NORMAL
