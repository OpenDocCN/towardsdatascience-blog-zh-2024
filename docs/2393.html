<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Under-trained and Unused tokens in Large Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Under-trained and Unused tokens in Large Language Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/under-trained-and-unused-tokens-in-large-language-models-db5fa17589ec?source=collection_archive---------14-----------------------#2024-10-01">https://towardsdatascience.com/under-trained-and-unused-tokens-in-large-language-models-db5fa17589ec?source=collection_archive---------14-----------------------#2024-10-01</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="3a3e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Existence of under-trained and unused tokens and Identification Techniques using GPT-2 Small as an Example</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@vanillaxiangshuyang?source=post_page---byline--db5fa17589ec--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Shuyang Xiang" class="l ep by dd de cx" src="../Images/36a5fd18fd9b7b88cb41094f09b83882.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Q-6F64L3h4jxYNYPiqHVaQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--db5fa17589ec--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@vanillaxiangshuyang?source=post_page---byline--db5fa17589ec--------------------------------" rel="noopener follow">Shuyang Xiang</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--db5fa17589ec--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 1, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj mk"><img src="../Images/44f9ba6050c2fa14c0bd03bb05f6f2fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*LdYdFjBlpjlJ2MzFxD9yUw.jpeg"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Image generated by <a class="af mx" href="https://deepai.org/machine-learning-mode" rel="noopener ugc nofollow" target="_blank">deepai</a> from the text: under-trained tokenization of LLMs</figcaption></figure><h1 id="21ac" class="my mz fq bf na nb nc gq nd ne nf gt ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Introduction: Unused and Under-trained Tokens</h1><p id="97bc" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">We have observed the existence of both unused and under-trained tokens in exploration of transformer based large language models (LLMs) such as ChatGPT, of which the tokenization and the model training stay as two separate processes. Unused tokens and under-trained tokens have the following different behaviors:</p><ul class=""><li id="ed53" class="nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op ov ow ox bk"><strong class="nw fr">Unused Tokens</strong> exist in the LLM’s vocabulary and were included during the process training but were not sufficiently seen.</li><li id="71c6" class="nu nv fq nw b go oy ny nz gr oz ob oc od pa of og oh pb oj ok ol pc on oo op ov ow ox bk"><strong class="nw fr">Under-Trained Tokens</strong> may or may not exist in the LLM’s vocabulary and were not at all represented in the training data.</li></ul><p id="0859" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">Ideally, the two types of tokens would have very low probabilities to be generated, or equivalently, have extremely negative logit values, so that they should not be generated by the LLMs. However, in practice, users have still found some unused tokens with important logits and the model can sometimes unfortunately predict them. This can lead to undesirable behaviors in LLMs.</p><p id="da64" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">Let us consider an LLM which unexpectedly generates nonsensical or inappropriate text because of some tokens that was never trained during the model training. Such occurrences can sometimes cause serious consequences, such as hallucination, leading to lack of accuracy and appropriateness.</p><p id="1fc3" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">We claim this issue is due to the separation between tokenization and the training process of LLMs. In general, these two aspects are never trained together and it did happen that a token in the model’s vocabulary fails to be trained and appears randomly in the output of the model.</p><p id="4b96" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">In this article, we will demonstrate the existence of unused tokens, including under-trained ones, with some simple experiments using GPT-2 Small . We will also discuss techniques for identifying under-trained tokens.</p><h1 id="0572" class="my mz fq bf na nb nc gq nd ne nf gt ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Existence of Unused Tokens: Experiments on GPT-2 Small</h1><p id="21b9" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">In many LLMs, including GPT-2 Small on which our experiments are executed, there exist unused tokens, that is, tokens existing in the LLM’s vocabulary and were included during the process training but were not sufficiently seen.</p><p id="ab78" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">In the following examples, we give two cases proving the existence of unused tokens:</p><h2 id="28a9" class="pd mz fq bf na pe pf pg nd ph pi pj ng od pk pl pm oh pn po pp ol pq pr ps pt bk">Example 1: Reproducing Unused Tokens</h2><p id="4af2" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">In this experiment, we aim to show how GPT-2 Small struggles to reproduce unused tokens, even with very straightforward instructions. Let us now consider the following unused token:<code class="cx pu pv pw px b">"ú"</code> (<code class="cx pu pv pw px b">\u00fa</code>). We would like to instruct GPT-2 small to repeat the token exactly as given by the input.</p><p id="8e40" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">This is a very simple task: For the given input token <code class="cx pu pv pw px b">"ú"</code>, the model have to give the same token as output.</p><pre class="ml mm mn mo mp py px pz bp qa bb bk"><span id="30ca" class="qb mz fq px b bg qc qd l qe qf">from transformers import GPT2LMHeadModel, GPT2Tokenizer</span></pre><pre class="qg py px qh qi ay qj bk"><span id="4737" class="pd mz fq px b hw qk ql l im qf"># Load pre-trained model (GPT-2 Small) and tokenizer<br/>model_name = "gpt2"  # GPT-2 Small (124M parameters)<br/>model = GPT2LMHeadModel.from_pretrained(model_name)<br/>tokenizer = GPT2Tokenizer.from_pretrained(model_name)<br/>tokenizer.pad_token = tokenizer.eos_token</span><span id="7f3e" class="pd mz fq px b hw qm ql l im qf"># Configure the model's `pad_token_id`<br/>model.config.pad_token_id = model.config.eos_token_id</span><span id="9926" class="pd mz fq px b hw qm ql l im qf"># Encode a prompt to generate text<br/>token= "\u00fa"<br/>prompt_text = "Rpeats its input exactly" + ', '.join([f"Input: {token}, Output: {token}" for _ in range(20)])+ f"Input: {token}, Output: "<br/>inputs = tokenizer(prompt_text, return_tensors="pt", padding=True)</span><span id="25f8" class="pd mz fq px b hw qm ql l im qf"># Generate text with attention mask<br/>output = model.generate(<br/>    inputs['input_ids'],<br/>    attention_mask=inputs['attention_mask'],  # Explicitly pass attention_mask<br/>    max_new_tokens=10,  # Maximum length of the generated text<br/>    num_return_sequences=1,  # Number of sequences to return<br/>    no_repeat_ngram_size=2,  # Prevent repeating n-grams<br/>    do_sample=True,  # Enable sampling<br/>    top_k=50,  # Limit sampling to top k choices<br/>    top_p=0.95,  # Use nucleus sampling<br/>)</span></pre><p id="f6ac" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">As you can see in the code above, we have designed a prompt as n-shot examples, instructing the model to give exactly the same specific token <code class="cx pu pv pw px b">"ú"</code> . What we see is that the model fails to predict this token: it gives some grabled text as <code class="cx pu pv pw px b">"Output: - ß, *- *-, "</code> . In contrast, when we tested the same task with common tokens such as <code class="cx pu pv pw px b">"a"</code> , the model successfully predicted the correct output, showing the stark difference in performance between frequently encountered and unused tokens.</p><h2 id="404b" class="pd mz fq bf na pe pf pg nd ph pi pj ng od pk pl pm oh pn po pp ol pq pr ps pt bk">Example 2: Token Repetition</h2><p id="d14f" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">We now consider the range of unused tokens from indices 177 to 188, the range of unused tokens for GPT2 [1].</p><p id="c0fa" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">Our goal now is to generate sequences of repeated random tokens and evaluate the model’s performance on the repeated sequencee. As discussed in my previous blog post, <strong class="nw fr">“</strong><a class="af mx" href="https://medium.com/towards-data-science/how-to-interpret-gpt2-small-76e0536a588a" rel="noopener"><strong class="nw fr">How to Interpret GPT-2 Small: Mechanistic Interpretability on Prediction of Repeated Tokens,</strong></a><strong class="nw fr">”</strong> transformer-based LLMs have a strong ability to recognize and predict repeated patterns, even for small models such as GPT2 small.</p><p id="98e8" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">For example, when the model encounters an ‘A’, it searches for the previous occurrence of ‘A’ or a token closely related to ‘A’ in the embedding space. It then identifies the subsequent token, ‘B’, and predicts that the next token following ‘A’ will be ‘B’ or a token similar to ‘B’ in the embedding space.</p><p id="bd36" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">We begin by defining a function, <code class="cx pu pv pw px b">generate_repeated_tokens</code> which generated a sequence whose second half repeats the first half.</p><pre class="ml mm mn mo mp py px pz bp qa bb bk"><span id="be79" class="qb mz fq px b bg qc qd l qe qf">import torch as t<br/>from typing import Tuple<br/># Assuming HookedTransformer and other necessary components are defined elsewhere.<br/>t.manual_seed(42)<br/>def generate_repeated_tokens(<br/>    model: HookedTransformer, seq_len: int, batch: int = 1<br/>) -&gt; Int[Tensor, "batch full_seq_len"]:<br/>    '''<br/>    Generates a sequence of repeated random tokens<br/>    Outputs are:<br/>        rep_tokens: [batch, 1+2*seq_len]<br/>    '''<br/>    bos_token = (t.ones(batch, 1) * model.tokenizer.bos_token_id).long()  # generate bos token for each batch<br/>    rep_tokens_half = t.randint(177, 188, (batch, seq_len), dtype=t.int64)<br/>    rep_tokens = t.cat([bos_token, rep_tokens_half, rep_tokens_half], dim=-1).to(device)<br/>    return rep_tokens</span></pre><p id="554e" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">Next, we define the <code class="cx pu pv pw px b">run_and_cache_model_repeated_tokens</code> function, which runs the model on the generated repeated tokens, returning the logits and caching the activations. We will use only logits here.</p><pre class="ml mm mn mo mp py px pz bp qa bb bk"><span id="87f8" class="qb mz fq px b bg qc qd l qe qf">def run_and_cache_model_repeated_tokens(model: HookedTransformer, seq_len: int, batch: int = 1) -&gt; Tuple[t.Tensor, t.Tensor, ActivationCache]:<br/>    '''<br/>    Generates a sequence of repeated random tokens, and runs the model on it, returning logits, tokens and cacheShould use the `generate_repeated_tokens` function above<br/>    Outputs are:<br/>        rep_tokens: [batch, 1+2*seq_len]<br/>        rep_logits: [batch, 1+2*seq_len, d_vocab]<br/>        rep_cache: The cache of the model run on rep_tokens<br/>    '''<br/>    rep_tokens = generate_repeated_tokens(model, seq_len, batch)<br/>    rep_logits, rep_cache = model.run_with_cache(rep_tokens)<br/>    return rep_tokens, rep_logits, rep_cache</span></pre><p id="3d1f" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">Now we run the model using the defined <code class="cx pu pv pw px b">run_and_cache_model_repeated_tokens</code> function, generating both the tokens and the associated logits with the following code:</p><pre class="ml mm mn mo mp py px pz bp qa bb bk"><span id="067b" class="qb mz fq px b bg qc qd l qe qf">seq_len = 25<br/>batch = 1<br/>(rep_tokens, rep_logits, rep_cache) = run_and_cache_model_repeated_tokens(gpt2_small, seq_len, batch)<br/>rep_cache.remove_batch_dim()<br/>rep_str = gpt2_small.to_str_tokens(rep_tokens)<br/>gpt2_small.reset_hooks()<br/>log_probs = get_log_probs(rep_logits, rep_tokens).squeeze()Copy co</span></pre><p id="cdc0" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">After running the model, we analyze the log probabilities of the predicted tokens for both halves of the repeated sequences. We observed a mean log probability of -17.270 for the first half and -19.675 for the second half for token sequences varying in indices 177 to 188.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="qo qp ed qq bh qr"><div class="mi mj qn"><img src="../Images/97825741fd58c75f3218c336e64f899b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ieSDh1wzkFwHbksNcx8EhA.png"/></div></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Image by author: log prob of repeated tokens ranging in 177 to 188</figcaption></figure><p id="3be8" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">On the other hand, doing the same experiment with a commonly used range of tokens gives different results: When examining token indices 100 to 110, we observe significantly better performance in the second half, with log probabilities of -0.971 compared to -7.327 in the first half.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="qo qp ed qq bh qr"><div class="mi mj qs"><img src="../Images/41a683197974c901ef65f126648227cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QpFuktPDIoiNxhvmWii1HQ.png"/></div></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Image by author: log prob of repeated tokens ranging in 100 to 111</figcaption></figure><h1 id="cdb1" class="my mz fq bf na nb nc gq nd ne nf gt ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Under-trained Tokens</h1><p id="5e98" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">The world of LLM would ideally have less surprise if all unused tokens had significantly negative logits and the model would therefore never produce weird texts.</p><p id="3d8b" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">The reality is, unfortunately much more complex. The fact that the creation of the tokenizer and the training of LLM do not happen at the same time lead sometimes to undertrained tokens which are, the the culprits of unexpected behaviors of LLMs.</p><p id="61ff" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">An example of undertrained tokens is:<code class="cx pu pv pw px b">_SolidGoldMagikarp</code>[1] which was seen sometimes in ChatGPT’s outputs. Now we would like to prove the existence of under-trained tokens in the case of GPT-2 Small.</p><h2 id="cfee" class="pd mz fq bf na pe pf pg nd ph pi pj ng od pk pl pm oh pn po pp ol pq pr ps pt bk">Example: Reproducing Unused Tokens</h2><p id="8035" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">In our former experiment of reproducing unused tokens within the GPT-2 Small model, we proved that the token <code class="cx pu pv pw px b">"ú"</code> has hardly any chance to be generated by the model.</p><p id="9aa9" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">Now we slice the logits tensor after runing the model to isolate the outputs to the unused token indices ranging from 177 to 188:</p><pre class="ml mm mn mo mp py px pz bp qa bb bk"><span id="f7e0" class="qb mz fq px b bg qc qd l qe qf">sliced_tensor = gpt2_logits[0, :, 177:188]</span></pre><p id="6026" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">Interestingly, we have observed that the logit values for some tokens in this “unused” range reached approximately -1.7, which is to say, there is a probability of around 0.18 for some unused tokens being generated.</p><p id="69ae" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">This finding highlights the model’s possiblity to assign non-negligible probabilities to some unused tokens, despite they are uncommonly used in most of the context.</p><h1 id="f397" class="my mz fq bf na nb nc gq nd ne nf gt ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Identifying Under-Trained Tokens</h1><p id="954c" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">In recent years, researchers have proposed techniques to automatically identify under-trained tokens in LLMs. Works in this area include those by Watkins and Rumbelow (2023), and Fell (2023) among which one very interesting approach to identifying under-trained tokens involves analyzing the output embeddings E_{out} generated by the model:</p><p id="34cf" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">The the method computes the average embedding vector of the unused tokens and uses cosine distances to measure how the vector is similar to all tokens’e embedding vector of the model. Tokens with cosine distances close to the mean embeddings are thus marked as candidates of under-trained tokens. Please check more details in [1].</p><h1 id="3352" class="my mz fq bf na nb nc gq nd ne nf gt ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Conclusion</h1><p id="95ce" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">In conclusion, this blog posts discusses the under-trained tokens LLMs. We do some experiments with GPT-2 Small to illustrate that under-trained tokens can unexpectedly affect model outputs, giving sometimes unpredictable and undesirable behaviors. Recent researches propose methods in identifying under-trained tokens accordingly. For those interested in more details of my implementation, you can check my accompanying <a class="af mx" href="https://colab.research.google.com/drive/13h_X8uvkai49yUWW2cndKDVMg-8RqlIb#scrollTo=ckKDzTnF1r3l" rel="noopener ugc nofollow" target="_blank">notebook</a>.</p><h1 id="9760" class="my mz fq bf na nb nc gq nd ne nf gt ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Reference</h1><p id="d397" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">[1] Land, S., &amp; Bartolo, M. (2024). <em class="qt">Fishing for Magikarp: Automatically detecting under-trained tokens in large language models</em>. arXiv. <a class="af mx" href="https://doi.org/10.48550/arXiv.2405.05417" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.2405.05417</a>.</p><p id="b968" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">[2] Jessica Rumbelow and Matthew Watkins. 2023. SolidGoldMagikarp (plus, prompt generation). Blog Post.</p><p id="96e9" class="pw-post-body-paragraph nu nv fq nw b go oq ny nz gr or ob oc od os of og oh ot oj ok ol ou on oo op fj bk">[3] Martin Fell. 2023. A search for more ChatGPT / GPT3.5 / GPT-4 “unspeakable” glitch tokens. Blog post.</p></div></div></div></div>    
</body>
</html>