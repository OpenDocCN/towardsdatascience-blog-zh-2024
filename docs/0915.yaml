- en: Scaling AI Models Like You Mean It
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 像您所期望的那样扩展 AI 模型
- en: 原文：[https://towardsdatascience.com/scaling-ai-models-like-you-mean-it-3afa56c1e14b?source=collection_archive---------5-----------------------#2024-04-10](https://towardsdatascience.com/scaling-ai-models-like-you-mean-it-3afa56c1e14b?source=collection_archive---------5-----------------------#2024-04-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/scaling-ai-models-like-you-mean-it-3afa56c1e14b?source=collection_archive---------5-----------------------#2024-04-10](https://towardsdatascience.com/scaling-ai-models-like-you-mean-it-3afa56c1e14b?source=collection_archive---------5-----------------------#2024-04-10)
- en: Strategies for Overcoming the Challenges of Scaling Open-Source AI Models in
    Production
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克服在生产环境中扩展开源 AI 模型挑战的策略
- en: '[](https://medium.com/@ssheng?source=post_page---byline--3afa56c1e14b--------------------------------)[![Sean
    Sheng](../Images/ae58cf760ce5c482e7a6614995b8b8e1.png)](https://medium.com/@ssheng?source=post_page---byline--3afa56c1e14b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3afa56c1e14b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3afa56c1e14b--------------------------------)
    [Sean Sheng](https://medium.com/@ssheng?source=post_page---byline--3afa56c1e14b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ssheng?source=post_page---byline--3afa56c1e14b--------------------------------)[![Sean
    Sheng](../Images/ae58cf760ce5c482e7a6614995b8b8e1.png)](https://medium.com/@ssheng?source=post_page---byline--3afa56c1e14b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3afa56c1e14b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3afa56c1e14b--------------------------------)
    [Sean Sheng](https://medium.com/@ssheng?source=post_page---byline--3afa56c1e14b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3afa56c1e14b--------------------------------)
    ·11 min read·Apr 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3afa56c1e14b--------------------------------)
    ·11分钟阅读·2024年4月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: If you’re reading this article, you probably need no introduction to the advantages
    of deploying open-source models. Over the past couple of years, we have seen incredible
    growth in the both the quantity and quality of open source models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在阅读这篇文章，可能不需要我再介绍部署开源模型的优势。在过去几年里，我们已经见证了开源模型在数量和质量上的惊人增长。
- en: Platforms such as Hugging Face have democratized access to a wide array of models,
    including Large Language Models (LLMs) and diffusion models, empowering developers
    to innovate freely and efficiently.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像 Hugging Face 这样的平台使得各种模型的访问变得更加民主化，包括大型语言模型（LLMs）和扩散模型，这使得开发者能够自由高效地进行创新。
- en: Developers enjoy greater autonomy, as they can fine-tune and combine different
    models at will, leading to innovative approaches like Retrieval-Augmented Generation
    (RAG) and the creation of advanced agents.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发者享有更大的自主性，因为他们可以自由地微调和组合不同的模型，从而推动创新方法的出现，比如检索增强生成（RAG）和创建先进的智能体。
- en: From an economic perspective, open-source models provide substantial cost savings,
    enabling the use of smaller, specialized models that are more budget-friendly
    compared to general-purpose models like GPT-4.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从经济角度来看，开源模型提供了可观的成本节省，使得使用较小、专用的模型成为可能，这些模型相较于像 GPT-4 这样的通用模型更加节省预算。
- en: Open-source models present an attractive solution, but what’s the next hurdle?
    Unlike using a model endpoint like OpenAI, where the model is a scalable black
    box behind the API, deploying your own open-source models introduces scaling challenges.
    It’s crucial to ensure that your model scales effectively with production traffic
    and maintains a seamless experience during traffic spikes. Additionally, it’s
    important to manage costs efficiently, so you only pay for what you use and avoid
    any financial surprises at the end of the month.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 开源模型呈现出一种有吸引力的解决方案，但下一个难题是什么呢？与使用像 OpenAI 这样的模型端点不同，后者的模型是 API 背后的可扩展黑箱，部署自己的开源模型则引入了扩展问题。确保你的模型能够有效扩展以应对生产流量，并在流量激增期间保持无缝体验，至关重要。此外，合理管理成本也很重要，这样你只需为实际使用的部分付费，避免在月底遇到财务上的意外。
- en: 'True north: Serverless functions for GPUs'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真正的方向：面向 GPU 的无服务器函数
- en: Interestingly, this sounds like a challenge that modern serverless architectures,
    like AWS Lambda, have already solved — a solution that have existed for almost
    a decade. However, when it comes to AI model deployment, this isn’t quite the
    case.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这听起来像是现代无服务器架构（如 AWS Lambda）已经解决的挑战——这是一个已经存在近十年的解决方案。然而，当涉及到 AI 模型部署时，情况并非完全如此。
- en: The limitations of serverless functions for AI deployments are multifaceted.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器函数在AI部署中的局限性是多方面的。
- en: '**No GPU support**. Platforms like AWS Lambda don’t support GPU. This isn’t
    merely a technical oversight; it’s rooted in architectural and practical considerations.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**没有GPU支持**。像AWS Lambda这样的平台不支持GPU。这不仅仅是一个技术疏漏，而是由于架构和实际考虑所致。'
- en: '**GPUs cannot be easily shared.** GPUs, while highly parallelizable as devices,
    is not as flexible in handling multiple inference tasks on different models simultaneously.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU无法轻松共享**。虽然GPU作为设备具有高度并行性，但它在同时处理不同模型的多个推理任务时，灵活性不如其他技术。'
- en: '**GPUs are expensive**. They’re exceptional for model inferencetasks but costly
    to maintain, especially if not utilized continuously.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU昂贵**。GPU在模型推理任务中表现出色，但维护成本高昂，尤其是在不被持续使用的情况下。'
- en: Next, let’s take a look at our scaling journey and the important lessons we
    have learned along the way.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来看看我们的扩展历程以及我们在过程中学到的重要经验。
- en: The cold start problem
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 冷启动问题
- en: 'Before we could even begin to work on scaling, we have the notorious “cold
    start” problem. This issue presents itself in three different stages:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始扩展工作之前，首先面临的是臭名昭著的“冷启动”问题。这个问题在三个不同的阶段表现出来：
- en: '![](../Images/a91758d3ec08a5076eb6b8d716db9fef.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a91758d3ec08a5076eb6b8d716db9fef.png)'
- en: Breakdown of the cold start problem. Image by the author.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 冷启动问题的分析。图像来源：作者。
- en: '**Cloud provisioning**: This phase involves the time it takes for a cloud provider
    to allocate an instance and integrate it into our cluster. This process varies
    widely, ranging from as quick as 30 seconds to several minutes, and in some cases,
    even hours, especially for high-demand instances like the Nvidia A100 and H100
    GPUs.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**云资源配置**：这一阶段涉及云提供商为我们分配实例并将其整合进集群所需的时间。这个过程差异很大，通常从30秒到几分钟不等，有时甚至会延长到几个小时，特别是对于需求量大的实例，如Nvidia
    A100和H100 GPU。'
- en: '**Container image pulling**: Unlike simple Python job images, AI model serving
    images are very complex, due to the dependencies and custom libraries they require.
    Although cloud providers boast multi-gigabit network bandwidth, our experience
    often saw download speeds far below them, with image pulling time about 3 minutes.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**容器镜像拉取**：与简单的Python作业镜像不同，AI模型服务镜像非常复杂，因其依赖关系和所需的自定义库。尽管云提供商宣称拥有多吉比特的网络带宽，但我们的经验表明，下载速度往往远低于这一标准，镜像拉取时间大约为3分钟。'
- en: '**Model loading**. The time required here is largely dependent on the model’s
    size, with larger models like LLMs and diffusion models taking significantly longer
    time due to their billions of parameters. For example, loading a 5GB model like
    Stable Diffusion 2 might take approximately 1.3 minutes with 1Gbps network bandwidth,
    while larger models like Llama 13B and Mixtral 8x7B could require 3.5 minutes
    and 12.5 minutes respectively.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型加载**。所需时间主要取决于模型的大小，像LLM和扩散模型这样的大型模型由于拥有数十亿个参数，需要显著更长的时间。例如，加载一个5GB的模型（如Stable
    Diffusion 2）可能需要大约1.3分钟，前提是网络带宽为1Gbps，而更大的模型如Llama 13B和Mixtral 8x7B则分别需要3.5分钟和12.5分钟。'
- en: Each phase of the cold start issue demands specific strategies to minimize delays.
    In the following sections, we’ll explore each of them in more detail, sharing
    our strategies and solutions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 冷启动问题的每个阶段都需要采取特定的策略来最小化延迟。在接下来的章节中，我们将更详细地探讨每个阶段，分享我们的策略和解决方案。
- en: Cloud provisioning
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云资源配置
- en: In contrast to the homogeneous environment of serverless CPUs, managing a diverse
    range of compute instance types is crucial when dealing with GPUs, each tailored
    for specific use cases. For instance, IO-bound LLMs require high GPU memory bandwidth
    and capacity, while generative models need more powerful GPU compute.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与无服务器CPU的同质化环境不同，管理多种计算实例类型在处理GPU时至关重要，因为每种GPU都针对特定的使用案例进行优化。例如，IO密集型的LLM模型需要较高的GPU内存带宽和容量，而生成式模型则需要更强大的GPU计算能力。
- en: Ensuring availability during peak traffic by maintaining all GPU instance types
    could lead to prohibitively high costs. To avoid the financial strain of idle
    instances, we implemented a “standby instances” mechanism. Rather than preparing
    for the maximum potential load, we maintained a calculated number of standby instances
    that match the incremental scaling step sizes. For example, if we scale by two
    GPUs at a time, we need to have two standby instances ready. This allows us to
    quickly add resources to our serving fleet as demand surges, significantly reducing
    wait time, while keeping cost manageable.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在流量高峰期间确保可用性，通过保持所有GPU实例类型可能会导致不可承受的高成本。为了避免空闲实例带来的财务压力，我们实施了“备用实例”机制。我们没有为最大潜在负载做准备，而是维护了一个与增量扩展步长相匹配的计算备用实例数量。例如，如果我们一次扩展两个GPU，我们就需要准备两个备用实例。这使我们能够在需求激增时迅速向服务队列中添加资源，显著减少等待时间，同时保持成本可控。
- en: '![](../Images/f06c07c4b29d12ce233597218a55b698.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f06c07c4b29d12ce233597218a55b698.png)'
- en: Image by the author.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者。
- en: In a multi-tenant environment, where multiple teams or, in our case, multiple
    organizations, share a common resource pool, we can achieve more efficient utilization
    rates. This shared environment allows us to balance varying resource demands,
    contributing to improved cost efficiency. However, managing multi-tenancy introduces
    challenges, such as enforcing quotas and ensuring network isolation, which can
    add complexity to the cluster.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在多租户环境中，多个团队或在我们的案例中，多个组织共享一个公共资源池，我们可以实现更高效的资源利用率。这个共享环境使我们能够平衡不同的资源需求，有助于提高成本效率。然而，管理多租户环境也带来了挑战，例如强制执行配额和确保网络隔离，这可能会增加集群的复杂性。
- en: Container image pulling
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器镜像拉取
- en: Serverless CPU workloads often use lightweight images, like the Python slim
    image (around 154 MB). In stark contrast, a container image built for serving
    an LLM can be much larger (6.7 GB); the bulk of this size comes from the various
    dependencies required to run the AI model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器CPU工作负载通常使用轻量级镜像，如Python slim镜像（约154 MB）。相比之下，为了提供LLM服务的容器镜像可能要大得多（6.7 GB）；其中大部分大小来自于运行AI模型所需的各种依赖项。
- en: '![](../Images/46e47cc296d52e7e9f4085be53b44b34.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46e47cc296d52e7e9f4085be53b44b34.png)'
- en: Image by the author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者。
- en: Despite high-bandwidth networks advertised by cloud providers, the reality often
    falls short, with actual download speeds being a fraction of the promised rates.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管云服务提供商宣传了高带宽网络，实际情况往往远低于预期，实际下载速度仅为承诺速度的一个小部分。
- en: Practically, a significant portion of the files were never used. One way is
    to optimize the container image itself, but that quickly proved to be unmanageable.
    Instead, we shifted our focus to an on-demand file pulling approach. Specifically,
    we first downloaded only the image metadata, with the actual remote files being
    fetched later as needed. In addition, we leveraged peer-to-peer networking within
    the cluster to dramatically increase pulling efficiency.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，很多文件根本没有被使用。一种方式是优化容器镜像本身，但这很快证明是不可管理的。相反，我们将注意力转向按需文件拉取方法。具体来说，我们首先只下载镜像元数据，实际的远程文件则在需要时再拉取。此外，我们利用集群中的点对点网络来大幅提高拉取效率。
- en: '![](../Images/ae31e765f9e62607e7e19c9024344b48.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae31e765f9e62607e7e19c9024344b48.png)'
- en: Container image metadata can be pull in seconds. Image by the author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 容器镜像元数据可以在几秒钟内拉取。图片来源：作者。
- en: With these optimizations, we reduced the image pulling time from several minutes
    to mere seconds. However, we all know this measurement is “cheating” since the
    actual files are not pulled at this stage. The real file pulling occurs when the
    service runs. Therefore, it’s crucial to have a service framework that allows
    you to define behaviors at various lifecycle stages, such as initialization and
    serving. By doing all of the bootstrapping during initialization, we can ensure
    that all file dependencies are pulled. This way, when it comes to serving time,
    there are no delays caused by file pulling.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些优化，我们将镜像拉取时间从几分钟减少到几秒钟。然而，大家都知道，这个测量实际上是在“作弊”，因为实际的文件并没有在此阶段被拉取。真正的文件拉取发生在服务运行时。因此，拥有一个允许在各生命周期阶段定义行为的服务框架至关重要，例如初始化和服务。在初始化阶段完成所有引导工作后，我们可以确保拉取所有文件依赖项。这样，在服务时，就不会因拉取文件而产生延迟。
- en: '![](../Images/afd20effc8387a63201471c4453e6382.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afd20effc8387a63201471c4453e6382.png)'
- en: Service framework that enables service initialization and API definitions. Image
    by the author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 启动服务框架并定义API的服务框架。图片来源：作者。
- en: In the above example, model loading is done during the initialization lifecycle
    within `__init__` and serving happens within the `@bentoml.api` named `txt2img`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，模型加载是在`__init__`初始化生命周期中完成的，服务则在`@bentoml.api`命名的`txt2img`中进行。
- en: Model loading
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型加载
- en: Initially, the most straightforward method for model loading was to fetch it
    directly from a remote store like Hugging Face. Using Content Delivery Networks
    (CDNs), NVMe SSDs, and shared memory, we could remove some of the bottlenecks.
    While this worked, it was far from optimal.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，最直接的模型加载方法是从远程存储（如Hugging Face）中直接获取。通过使用内容分发网络（CDN）、NVMe SSD和共享内存，我们能够消除一些瓶颈。尽管这种方法可行，但远未达到最优。
- en: To improve this process, we considered using in-region network bandwidth. We
    seeded models in our distributed file systems and broke them into smaller chunks,
    allowing for parallel downloads. This drastically improved performance, but we
    still encountered cloud provider’s network bandwidth bottlenecks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善这一过程，我们考虑了使用区域内网络带宽。我们在分布式文件系统中预先加载模型，并将其分割成较小的块，允许并行下载。这大大提高了性能，但我们仍然遇到了云服务商网络带宽的瓶颈。
- en: In response, we further optimized to leverage in-cluster network bandwidth by
    using peer-to-peer sharing and tapping into local caches. While the improvements
    were substantial, they added a layer of complexity to the process, which we need
    to abstract away from the developers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们进一步优化了利用集群内网络带宽的方法，通过使用点对点共享和利用本地缓存。虽然这些改进显著，但它们增加了处理过程的复杂性，我们需要将其从开发人员的操作中抽象出来。
- en: '![](../Images/2ed77644411ae062984ed93fc5af4986.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ed77644411ae062984ed93fc5af4986.png)'
- en: Image by the author.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者。
- en: 'Even with the above practices, we still suffer from a sequential bottleneck:
    the need to wait for each step to complete before proceeding with the next. Models
    had to be downloaded to persistent drive entirely before loading into CPU memory,
    and then into the GPU.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 即使采用了上述做法，我们仍然面临着一个顺序瓶颈：每个步骤完成后才能进行下一个步骤的等待问题。模型必须完全下载到持久化驱动器中，然后加载到CPU内存中，再加载到GPU内存。
- en: '![](../Images/d24c7294db86b83eabfdd2ec71787907.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d24c7294db86b83eabfdd2ec71787907.png)'
- en: Image by the author.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者。
- en: We turned to a stream-based method for loading model weights, using the distributed
    file cache system we had in place. This system allows programs to operate as if
    all files were logically available on disk. In reality, the required data is fetched
    on-demand from remote storage therefore bypassed disk writing. By leveraging a
    format like [Safetensors](https://github.com/huggingface/safetensors), we can
    efficiently load the model weights into the main memory through memory mapping
    (mmap) before loading to the GPU memory in a streaming fashion.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们转向了一种基于流的模型权重加载方法，利用我们现有的分布式文件缓存系统。该系统允许程序像所有文件都逻辑上存储在磁盘上一样进行操作。实际上，所需的数据是按需从远程存储中获取，从而避免了磁盘写入。通过利用像[Safetensors](https://github.com/huggingface/safetensors)这样的格式，我们可以通过内存映射（mmap）将模型权重高效加载到主内存中，然后以流式方式加载到GPU内存中。
- en: Moreover, we adopted asynchronous writing to disk. By doing so, we created a
    faster-access cache layer on the local disk. Thus, new deployments with only code
    changes could bypass the slower remote storage fetch phase, reading the model
    weights from local cache directly.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还采用了异步磁盘写入。通过这样做，我们在本地磁盘上创建了一个更快速访问的缓存层。因此，只有代码更改的新部署可以绕过较慢的远程存储获取阶段，直接从本地缓存读取模型权重。
- en: 'To summarize, we managed to optimize the cold start time and we were happy
    with the results:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们成功优化了冷启动时间，并对结果感到满意：
- en: '**No cloud provision delay** with standby instances.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**没有云提供商延迟**，配备待命实例。'
- en: '**Faster container image pulling** with on-demand and peer-to-peer streaming.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更快的容器镜像拉取**，通过按需和点对点流式传输。'
- en: '**Accelerated model loading** time with distributed file systems, peer-to-peer
    caching, and streamed loading to GPU memory.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过分布式文件系统、点对点缓存和流式加载到GPU内存中，加速了模型加载**。'
- en: '**Parallelized** image pulling and model loading enabled by service framework.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务框架启用了**并行**图像拉取和模型加载。
- en: Scaling metrics
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展指标
- en: Next, we need to identify the most indicative signal for scaling AI model deployments
    on GPUs.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要确定在GPU上扩展AI模型部署的最具指示性的信号。
- en: Resource utilization metrics
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源利用率指标
- en: Initially, we considered CPU utilization. It’s straightforward and has an intuitive
    default threshold, such as 80%. However, the obvious drawback is that CPU metrics
    don’t capture GPU utilization. Additionally, the Global Interpreter Lock (GIL)
    in Python limits parallelism, preventing high CPU utilization on multi-core instances,
    making CPU utilization a less feasible metric.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们考虑了CPU利用率。它直观且有一个默认的阈值，例如80%。然而，显而易见的缺点是，CPU度量无法捕捉GPU的利用情况。此外，Python中的全局解释器锁（GIL）限制了并行性，阻止了多核实例上的高CPU利用率，这使得CPU利用率成为一个不太可行的度量。
- en: 'We also explored GPU utilization as a more direct measure of our models’ workloads.
    However, we encountered an issue: the GPU utilization reported by tools like `nvml`
    didn''t accurately represent the actual utilization of the GPU. This metric samples
    kernel usage over a period of time, and a GPU is considered utilized if at least
    one kernel is executing. This aligns with our observation that better performance
    can often be achieved through improved batching, even though the GPU device was
    already reported as having high utilization.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探索了GPU利用率作为衡量模型工作负载的更直接指标。然而，我们遇到了一个问题：工具如`nvml`报告的GPU利用率并未准确反映GPU的实际利用情况。该度量是通过一定时间段内对内核使用情况的采样来计算的，如果至少有一个内核在执行，GPU就被认为是被利用的。这与我们的观察一致，即通过改进批处理，尽管GPU设备已经报告为高利用率，但通常可以实现更好的性能。
- en: '***Note****: According to* [*the NVIDIA documentation*](https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries)*,
    utilization.gpu means “Percent of time over the past sample period during which
    one or more kernels was executing on the GPU. The sample period may be between
    1 second and 1/6 second depending on the product”.*'
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***注意****：根据[*NVIDIA文档*](https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries)*，utilization.gpu表示“过去采样周期内，一个或多个内核在GPU上执行的时间百分比。采样周期可能是1秒到1/6秒，具体取决于产品。”*'
- en: 'Resource-based metrics are inherently retrospective as they only reflect usage
    after the resources have been consumed. They’re also capped at 100%, which presents
    a problem: when scaling based on these metrics, the maximum ratio for adjustment
    is typically the current utilization over the desired threshold (see scaling formula
    below). This results in a conservative scale-up behavior that doesn’t necessarily
    match the actual demand of production traffic.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基于资源的度量本质上是回顾性的，因为它们只反映在资源被消耗后才会出现的使用情况。它们也被限制在100%，这带来了一个问题：当根据这些度量进行扩展时，调整的最大比例通常是当前利用率与所需阈值之间的比例（见下面的扩展公式）。这导致了一种保守的扩展行为，未必能准确匹配生产流量的实际需求。
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Request-based metrics
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于请求的度量
- en: We turned to request-based metrics for more proactive signaling that are also
    not capped at a 100%.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们转向了基于请求的度量，作为更主动的信号，它们也不受100%限制。
- en: QPS is a widely recognized metric for its simplicity. However, its application
    in generative AI, such as with LLMs, is still a question. QPS is not easy to configure
    and due to the variable cost per request, which depends on the number of tokens
    processed and generated, using QPS as a scaling metric can lead to inaccuracies.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: QPS是一个被广泛认可的度量，因其简单性。然而，它在生成式AI中的应用，如LLM，仍然存在疑问。QPS的配置并不容易，并且由于每个请求的成本变化，取决于处理和生成的标记数量，使用QPS作为扩展度量可能会导致不准确。
- en: 'Concurrency, on the other hand, has proven to be an ideal metric for reflecting
    the actual load on the system. It represents the number of active requests either
    queued or being processed. This metric:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，并发性被证明是反映系统实际负载的理想度量。它代表了排队或正在处理的活跃请求的数量。这个度量：
- en: Precisely reflects the load on the system. [Little’s Law](https://en.wikipedia.org/wiki/Little%27s_law),
    which states that QPS multiplied by average latency equals concurrency, provides
    an elegant way to understand the relationship between QPS and concurrency. In
    practice, the average latency per request is rather unknown in model serving.
    However, by measuring concurrency, we don’t need to calculate average latency.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确反映系统的负载。[小洛特法则](https://en.wikipedia.org/wiki/Little%27s_law)指出，QPS乘以平均延迟等于并发性，提供了一种优雅的方式来理解QPS和并发性之间的关系。在实际操作中，模型服务中每个请求的平均延迟是相对未知的。然而，通过测量并发性，我们无需计算平均延迟。
- en: Accurately calculate the desired replicas using the scaling formula. Allowing
    the deployment to directly scale to the ideal size without intermediate steps.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用扩展公式准确计算所需的副本数。允许部署直接扩展到理想规模，而无需中间步骤。
- en: Easy to configure based on batch size. For non-batchable models, it’s simply
    the number of GPUs, since each can only handle one generation task at a time.
    For models that support batching, the batch size determines the concurrency level.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于批次大小易于配置。对于不能批处理的模型，只需配置 GPU 数量，因为每个 GPU 一次只能处理一个生成任务。对于支持批处理的模型，批次大小决定了并发级别。
- en: For concurrency to work, we need the support from the service framework to automatically
    instrument concurrency as a metric and serve it as a scaling signal for the deployment
    platform. We must also establish right scaling policies to help against overzealous
    scale-up during a traffic spike or premature scale-down when traffic is sparse.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使并发有效，我们需要服务框架的支持，自动将并发作为度量标准并将其作为扩展信号提供给部署平台。我们还必须建立正确的扩展策略，以防在流量激增时过度扩展，或者在流量稀少时过早缩减。
- en: Request queue
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 请求队列
- en: A another important mechanism we integrated with concurrency is the request
    queue. It acts as a buffer and an orchestrator, ensuring that incoming requests
    are handled efficiently and without overloading any single server replica.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们与并发集成的重要机制是请求队列。它充当缓冲区和调度器，确保传入的请求得到高效处理，并且不会使任何单一服务器副本过载。
- en: In a scenario without a request queue, all incoming requests are dispatched
    directly to the server (6 requests in the image below). If multiple requests arrive
    simultaneously, and there’s only one active server replica, it becomes a bottleneck.
    The server tries to process each request in a first-come-first-serve manner, often
    leading to timeouts and a bad client experience.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有请求队列的情况下，所有传入请求都会直接分配给服务器（如下图所示，6 个请求）。如果多个请求同时到达，而只有一个活跃的服务器副本，那么它会成为瓶颈。服务器会尝试按照先来先服务的方式处理每个请求，这通常会导致超时并影响客户端体验。
- en: '![](../Images/fbca33197fc380dd90ab8b0fc7f2b698.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbca33197fc380dd90ab8b0fc7f2b698.png)'
- en: Image by the author.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Conversely, with a request queue in place, the server consumes requests at an
    optimal rate, processing at a rate based on the *concurrency* defined for the
    service. When additional server replicas scale up, they too begin to pull from
    the queue. This mechanism prevents any single server from becoming overwhelmed
    and allows for a smoother, more manageable distribution of requests across the
    available infrastructure.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，使用请求队列时，服务器按最优速率消费请求，基于为服务定义的*并发*速率处理。当额外的服务器副本扩展时，它们也会开始从队列中拉取请求。该机制防止任何单一服务器过载，并且能够更平滑、更可管理地分配请求到可用的基础设施上。
- en: Conclusions
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Our journey in exploring AI model scaling solutions has been an adventure, which
    has led us to ultimately create the scaling experience on BentoCloud — a platform
    that encapsulates all our learnings.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在探索 AI 模型扩展解决方案的过程中经历了一次冒险，这最终使我们创造了 BentoCloud 上的扩展体验——一个囊括了我们所有学习成果的平台。
- en: To avoid the impression of a promotion, we’ll illustrate our point with a picture
    that’s worth a thousand words. The monitoring dashboard below demonstrates the
    correlation between incoming requests and the scaling up of server instances.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免给人宣传的印象，我们将通过一张千言万语的图片来说明我们的观点。下面的监控仪表板展示了传入请求与服务器实例扩展之间的关系。
- en: Equally important to scaling up is the ability to scale down. As the requests
    waned to zero, the deployment reduced the number of active instances accordingly.
    This ability ensures that no unnecessary costs are incurred for unused resources,
    aligning expenditure with actual usage.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 与扩展能力同样重要的是缩放的能力。当请求减少到零时，部署会相应地减少活跃实例的数量。这一能力确保了不会因未使用的资源而产生不必要的成本，使开支与实际使用情况相符。
- en: '![](../Images/7aedb48ff2a14a63c2d7a79d5738bd22.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7aedb48ff2a14a63c2d7a79d5738bd22.png)'
- en: BentoCloud monitoring dashboard. Image by the author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: BentoCloud 监控仪表板。图片由作者提供。
- en: We hope the takeaway is that scaling for model deployments should be considered
    an important aspect of production applications. Unlike scaling CPU workloads,
    scaling model deployments on GPUs presents unique challenges, including cold start
    times, configuring scaling metrics, and orchestrating requests. When evaluating
    deployment platforms, their solutions to these challenges should be thoroughly
    assessed.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望大家能从中获得的启示是，模型部署的扩展应被视为生产应用中的一个重要方面。与扩展 CPU 工作负载不同，在 GPU 上扩展模型部署面临独特的挑战，包括冷启动时间、配置扩展指标和调度请求等。在评估部署平台时，应彻底评估它们应对这些挑战的解决方案。
