<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Towards Generalization on Graphs: From Invariance to Causality</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Towards Generalization on Graphs: From Invariance to Causality</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/towards-generalization-on-graphs-from-invariance-to-causality-c81a174ac37b?source=collection_archive---------6-----------------------#2024-07-18">https://towardsdatascience.com/towards-generalization-on-graphs-from-invariance-to-causality-c81a174ac37b?source=collection_archive---------6-----------------------#2024-07-18</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="343b" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><em class="hd">This blog post shares recent papers on out-of-distribution generalization on graph-structured data</em></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@qitianwu228?source=post_page---byline--c81a174ac37b--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Qitian Wu" class="l ep by dd de cx" src="../Images/363e62ead857be0af76f9654c19f2b8c.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*yDv_ZcBFZlSHeIFCJvoWCw.jpeg"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--c81a174ac37b--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@qitianwu228?source=post_page---byline--c81a174ac37b--------------------------------" rel="noopener follow">Qitian Wu</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--c81a174ac37b--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 18, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk ld le ab q ee lf lg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lc"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lb lc">1</span></p></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lh k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al li an ao ap ie lj lk ll" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lm cn"><div class="l ae"><div class="ab cb"><div class="ln lo lp lq lr ls ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml mm"><img src="../Images/eb13a4c4ace10b85ad4a680e17242b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UhfBzLVPqqQHf8kielkgig.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Image generated by GPT-4</figcaption></figure><p id="24d4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="nz">This blog post introduces recent advances in out-of-distribution generalization on graphs, an important yet under-explored problem in machine learning. We will first introduce the problem formulation and typical scenarios involving distribution shifts on graphs. Then we present an overview of three recently published papers (where I am the author):</em></p><blockquote class="oa ob oc"><p id="b34d" class="nd ne nz nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af od" href="https://arxiv.org/pdf/2202.02466" rel="noopener ugc nofollow" target="_blank">Handling Distribution Shifts on Graphs: An Invariance Perspective</a>, ICLR2022.</p><p id="8ebe" class="nd ne nz nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af od" href="https://arxiv.org/pdf/2402.11494" rel="noopener ugc nofollow" target="_blank">Graph Out-of-Distribution Generalization via Causal Intervention</a>, WWW2024.</p><p id="32e0" class="nd ne nz nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af od" href="https://arxiv.org/pdf/2406.04963" rel="noopener ugc nofollow" target="_blank">Learning Divergence Fields for Shift-Robust Graph Representations</a>, ICML2024.</p></blockquote><p id="4689" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="nz">These works focus on generalization on graphs through the lens of invariance principle and causal intervention. Moreover, we will compare these methods and discuss potential future directions in this area.</em></p></div></div></div><div class="ab cb oe of og oh" role="separator"><span class="oi by bm oj ok ol"/><span class="oi by bm oj ok ol"/><span class="oi by bm oj ok"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="1c82" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Graph machine learning remains a popular research direction, especially with the wave of AI4Science driving increasingly diverse applications of graph data. Unlike general image and text data, graphs stand as a mathematical abstraction that describes the attributes of entities and their interactions within a system. In this regard, graphs can not only represent real-world physical systems of different scales (such as molecules, protein interactions, social networks, etc.), but also describe certain abstract topological relationships (such as scene graphs, industrial processes, chains of thought, etc.).</p><p id="995c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">How to build universal foundation models for graph data is a research question that has recently garnered significant attention. Despite the powerful representation capabilities demonstrated by existing methods such as Graph Neural Networks (GNNs) and Graph Transformers, the generalization of machine learning models on graph-structured data remains an underexplored open problem [1, 2, 3]. On the one hand, the non-Euclidean space and geometric structures involved in graph data significantly increase the difficulty of modeling, making it challenging for existing methods aimed at enhancing model generalization to succeed [4, 5, 6]. On the other hand, the distribution shift in graph data, i.e., the difference in distribution between training and testing data, arises from more complex guiding factors (such as topological structures) and external context, making this problem even more challenging to study [7, 8].</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml om"><img src="../Images/ba1c3e0d24e963ef3fdbe2526ec318ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Cxu6FsVEnZd_gZzv.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">The generalization challenge aims at handling distribution shifts from training to testing.</figcaption></figure><h1 id="57a3" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Problem and Motivation</h1><h2 id="a89f" class="pj oo fq bf op pk pl pm os pn po pp ov nm pq pr ps nq pt pu pv nu pw px py pz bk">Distribution Shifts in An Open World</h2><p id="8b27" class="pw-post-body-paragraph nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">The issue of generalization is crucial because models in real-world scenarios often need to interact with an open, dynamic, and complex environment. In practical situations, due to limited observation and resources, training data cannot encompass all possible environments, and the model cannot foresee all potential future circumstances during the training process. At the testing stage, however, the model is likely to encounter samples that are not aligned with the training distribution. <em class="nz">The key focus of the out-of-distribution generalization (OOD) problem targets how machine learning models perform on test data outside the training distribution.</em></p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml om"><img src="../Images/22a358ef7dbb045f049ced6606b78efc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QnInuuoRM3rCu6x8.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Typical scenarios involving distribution shifts on graphs require machine learning models to generalize from limited training data to new test distributions. Images from Medium blogs: <a class="af od" rel="noopener" target="_blank" href="/temporal-graph-networks-ab8f327f2efe">Temporal Graph Networks</a> and <a class="af od" href="https://medium.com/towards-data-science/topological-generalisation-with-advective-diffusion-transformers-70f263a5fec7" rel="noopener">Advective Diffusion Transformers</a></figcaption></figure><p id="053a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this setting, since the test data/distribution is strictly unseen/unknown during the training process, structural assumptions about the data generation are necessarily required as a premise. Conversely, without any data assumptions, out-of-distribution generalization is impossible (no-free lunch theorem). Therefore, it is important to clarify upfront that the research goal of the OOD problem is <em class="nz">not </em>to eliminate all assumptions <em class="nz">but</em> to 1) maximize the model’s generalization ability under reasonable assumptions, and 2) properly add/reduce assumptions to ensure the model’s capability to handle certain distribution shifts.</p><h2 id="cadb" class="pj oo fq bf op pk pl pm os pn po pp ov nm pq pr ps nq pt pu pv nu pw px py pz bk">Out-of-Distribution Generalization on Graphs</h2><p id="7429" class="pw-post-body-paragraph nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">The general out-of-distribution (OOD) problem can be simply described as:</p><blockquote class="oa ob oc"><p id="583f" class="nd ne nz nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">How to design effective machine learning methods when p(x,y|train)≠p(x,y|test)?</p></blockquote><p id="b302" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here, we follow the commonly used setting in the literature, assuming that the data distribution is controlled by an underlying environment. Thus, under a given environment e, the data generation can be written as (x,y)∼p(x,y|e). Then for the OOD problem, training and test data can be assumed to be generated from different environments. Consequently, the problem can be further elaborated as</p><blockquote class="oa ob oc"><p id="5449" class="nd ne nz nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">How to learn a predictor model f such that it performs (equally) well across all environments e∈E?</p></blockquote><p id="a757" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Specifically, for graph-structured data, the input data also contains structural information. In this regard, depending on the form in which graph structures exist, the problem can be further categorized into two types: node-level tasks and graph-level tasks. The following figure presents the formulation of the OOD problem under the two types of tasks.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml om"><img src="../Images/41cb47be812bc54b61c448f24c059046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*45Xjk13LBf6O4V98.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">The formulation of OOD generalization on graphs, where we further distinguish between graph-level and node-level tasks which vary in the form of graph structures. Specifically, for node-level tasks, due to the inter-dependence introduced by the graph structures among node instances, [5] proposes to divide a whole graph into node-centered ego-graphs that can be considered as independent inputs.</figcaption></figure><p id="5634" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As previously mentioned, the OOD problem requires certain assumptions about data generation which pave the way for building generalizable machine learning methods. Below, we will specifically introduce two classes of methods that utilize the invariance principle and causal intervention, respectively, to achieve out-of-distribution generalization on graphs.</p><h1 id="4870" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Generalization by Invariance Principle</h1><p id="692c" class="pw-post-body-paragraph nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">Learning methods based on the invariance principle, often referred to as <em class="nz">invariant learning </em>[9, 10, 11], aim to design new learning algorithms that guide machine learning models to leverage the <em class="nz">invariant relations</em> in data. Invariant relations particularly refer to the predictive relations from input x and label y that universally hold across all environments. Therefore, when a predictor model f (e.g., a neural network) successfully learns such invariant relations, it can generalize across data from different environments. On the contrary, if the model learns <em class="nz">spurious correlations</em>, which particularly refer to the predictive relations from x and y that hold only in some environments, then excessively improving training accuracy would mislead the predictor to overfit the data.</p><p id="4012" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In light of the above illustration, we notice that invariant learning relies on the <em class="nz">invariant assumption </em>in data generation, i.e., there exists a predictive relation between x and y that remains invariant across different environments. Mathematically, this can be formulated as:</p><blockquote class="oa ob oc"><p id="daa2" class="nd ne nz nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There exists a mapping c such that z=c(x) satisfies p(y|z,e)=p(y|z), ∀e∈E.</p></blockquote><p id="b4e5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this regard, we naturally have two follow-up questions: i) how can the invariant assumption be defined on graphs? and ii) is this a reasonable assumption for common graph data?</p><p id="d920" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We next introduce the recent paper [5], <em class="nz">Wu et al., “</em><a class="af od" href="https://arxiv.org/pdf/2202.02466" rel="noopener ugc nofollow" target="_blank"><em class="nz">Handling Distribution Shifts on Graphs: An Invariance Perspective</em></a><em class="nz">” (ICLR2022)</em>. This paper proposes applying the invariance principle to out-of-distribution generalization on graphs and poses the invariance assumption for graph data.</p><h2 id="b51f" class="pj oo fq bf op pk pl pm os pn po pp ov nm pq pr ps nq pt pu pv nu pw px py pz bk">Invariant Assumption on Graphs</h2><p id="b69f" class="pw-post-body-paragraph nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">Inspired by the Weisfeiler-Lehman algorithm for graph isomorphism testing, [5] considers ego-graphs centered on each node and characterizes the contributions of all the nodes’ features within the ego-graph to the label of the central node. The latter is specifically decomposed into <em class="nz">invariant features</em> and <em class="nz">spurious features</em>. This definition accommodates the topological structures and also allows enough flexibility. The following figure illustrates the invariant assumption as defined in [5] and provides an example of a citation network.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml om"><img src="../Images/d61ef3d9b14b6f635324bc8bca0152a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YTOJB3HNYAI2jMCC.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">The invariant assumption on graphs (left) and an example of a citation network (right). In the citation network, each node represents a paper, and the label y to be predicted is the research field of the paper. The node features x include the paper’s published venue (x1) and its citation index (x2), with the environment (e) being the publication time. In this example, x1 is an invariant feature because its relationship with y is independent of the environment. Conversely, x2 is a spurious feature; although it is strongly correlated with y, this correlation changes over time. Therefore, in this case, an ideal predictor should utilize the information in x1 to achieve generalization across different environments. Image from <a class="af od" href="https://arxiv.org/pdf/2202.02466" rel="noopener ugc nofollow" target="_blank">the paper</a>.</figcaption></figure><h2 id="9745" class="pj oo fq bf op pk pl pm os pn po pp ov nm pq pr ps nq pt pu pv nu pw px py pz bk">Proposed Method: Explore-to-Extrapolate Risk Minimization</h2><p id="f8b2" class="pw-post-body-paragraph nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">Under the invariance assumption, a natural approach is to regularize the loss difference across environments to facilitate learning invariant relations. However, real-world data typically lack environment labels, i.e., the correspondence between each instance and its environment is unknown, making it impossible to directly compute differences in loss across different environments. To address this challenge, [5] proposes Exploration-Extrapolation Risk Minimization (EERM), which involves introducing K context generators to augment and diversify the input data, thereby simulating input data from different environments. Through theoretical analysis, [5] proves that the new learning objective can guarantee an optimal solution for the formulated out-of-distribution generalization problem.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml om"><img src="../Images/af67d937affec052f992998fa894ac9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bA8O6MM6J_HXxRf_.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Explore-to-Extrapolate Risk Minimization (EERM) proposed by [5], where the inner objective is to maximize the “diversity” of data generated by K context generators and the outer objective involves computing the mean and variance of losses using data from the K generated (virtual) environments for training the predictor. Image from <a class="af od" href="https://arxiv.org/pdf/2202.02466" rel="noopener ugc nofollow" target="_blank">the paper</a>.</figcaption></figure><p id="86e7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Apart from generating (virtual) environments, another recent study [12] proposes inferring latent environments from observed data and introduces an additional model for environment inference, iteratively optimizing it alongside the predictor during training. Meanwhile, [13] approaches OOD generalization with data augmentation, using the invariance principle to guide the data augmentation process that preserves invariant features.</p><h1 id="00b3" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Generalization by Causal Intervention</h1><p id="10de" class="pw-post-body-paragraph nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">Invariant learning requires assuming the existence of invariant relations in data that can be learned. This to some extent limits the applicability of such methods, as the model can only generalize reliably on test data that shares certain invariance with training data. For out-of-distribution test data that violates this condition, the model’s generalization performance remains unknown.</p><p id="a4a8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Next, we introduce another approach proposed by recent work [14], <em class="nz">Wu et al., “</em><a class="af od" href="https://arxiv.org/pdf/2402.11494" rel="noopener ugc nofollow" target="_blank"><em class="nz">Graph Out-of-Distribution Generalization via Causal Intervention</em></a><em class="nz">” (WWW2024)</em>. This paper aims to tackle out-of-distribution generalization through the lens of causal intervention. Unlike invariant learning, this approach does not rely on the invariant assumption in data generation. Instead, it guides the model to learn causality from x to y through the learning algorithm.</p><h2 id="974c" class="pj oo fq bf op pk pl pm os pn po pp ov nm pq pr ps nq pt pu pv nu pw px py pz bk">A Causal Perspective for Graph Learning</h2><p id="71db" class="pw-post-body-paragraph nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">Firstly, let us consider the causal dependency among variables typically induced by machine learning models such as graph neural networks. We have the input G (e.g., ego-graphs centered on each node in a graph), the label Y, and the environment E influencing the data distribution. After training with the standard supervised learning objective (e.g., empirical risk minimization or equivalently, maximum likelihood estimation), their dependencies are illustrated in the diagram below.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml om"><img src="../Images/b8939081e9596e8c38629ad55d204ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ltTQaHk-C3SNTZ-T.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">In the causal graph, there are three dependence paths: i) from G to Y, induced by the predictor; ii) from E to G, given by definition of data generation; iii) from E to Y, led by the model training.</figcaption></figure><p id="bebc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The causal graph above reveals the limitation of traditional training methods, specifically their inability to achieve out-of-distribution generalization. Here, both the input G and the label Y are outcomes of the environment E, suggesting that they are correlated due to this <em class="nz">confounder</em>. During training, the model continuously fits the training data, causing the predictor f to learn the spurious correlation between inputs and labels specific to a particular environment.</p><p id="400d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[14] introduces an example of a social network to illustrate this learning process. Suppose we need to predict the interests of users (nodes) in a social network, where notice that user interests are significantly influenced by factors such as age and social circles. Therefore, if a predictor is trained on data from a university social network, it might easily predict a user’s interest in “basketball” because within a university environment, there is a higher proportion of users interested in basketball due to the environment itself. However, this predictive relation may not hold when the model is transferred to LinkedIn’s social network, where user ages and interests are more diverse. This example highlights that an ideal model needs to learn the causal relations between inputs and labels to generalize across different environments.</p><p id="e1fb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To this end, a common approach is <em class="nz">causal intervention</em>, which involves cutting off the dependence path between E and G in the causal graph. This is achieved by disrupting how the environment influences the inputs and labels, thereby guiding the model to learn causality. The diagram below illustrates this approach. In causal inference terminology [15], such interventions, aimed at removing dependence paths to a specific variable, can be represented using the do-operator. Therefore, if we aim to enforce cutting off the dependence path between E and G during training, it effectively means replacing the traditional optimization objective p(Y|G) (the likelihood of observed data) with p(Y|do(G)).</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml om"><img src="../Images/ab6865bfb02c094f00308f3fc481fe28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HfqR2se7n32zkpmy.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">The learning objective based on causal intervention. As one step further, utilizing the backdoor adjustment from causal inference [15], we can derive the explicit form of the objective from the causal graph.</figcaption></figure><p id="9be1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">However, computing this learning objective requires observed environment information in data, specifically the correspondence between each sample G and its environment E. In practice, however, environments are often unobservable.</p><h2 id="f8ea" class="pj oo fq bf op pk pl pm os pn po pp ov nm pq pr ps nq pt pu pv nu pw px py pz bk">Proposed Method: Variational Context Adjustment</h2><p id="1f72" class="pw-post-body-paragraph nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">To make the above approach feasible, [14] derives a variational lower bound for the causal intervention objective, using a data-driven approach that infers the latent environments from data to address the issue of unobservable environments. Particularly, [14] introduces a variational distribution q(E|G), resulting in a surrogate learning objective depicted in the following figure.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml om"><img src="../Images/0e36ba25a7486a54d6723529fb018953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ITYJCYuB7dQL1Y3x.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">The variational lower bound of the original causal intervention objective and the specific instantiations of three terms in the final learning objective proposed by [14]. Image from <a class="af od" href="https://arxiv.org/pdf/2402.11494" rel="noopener ugc nofollow" target="_blank">the paper</a>.</figcaption></figure><p id="5963" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The new learning objective is comprised of three components. [14] instantiates them as an environment inference model, a GNN predictor, and a (non-parametric) prior distribution of the environment. The first two models contain trainable parameters and are jointly optimized during training.</p><p id="fadf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To validate the effectiveness of the proposed method, [14] applies the model to various real-world graph datasets with distribution shifts. Specifically, because the proposed method CaNet does not depend on specific backbone models, [14] uses GCN and GAT as the backbone, respectively, and compares the model with state-of-the-art OOD methods (including the previously-introduced approach EERM). The table below shows some of the experimental results.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml om"><img src="../Images/74193aab006dc46f879205a144029993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9OTaE6zExA0h9Dv4.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Experimental results of testing Accuracy (resp. ROC-AUC) on Arxiv (resp. Twitch), where the distribution shifts are introduced by splitting the data according to publication years (resp. subgraphs).</figcaption></figure><h2 id="b904" class="pj oo fq bf op pk pl pm os pn po pp ov nm pq pr ps nq pt pu pv nu pw px py pz bk">Implicit Assumptions in Causal Intervention</h2><p id="50a9" class="pw-post-body-paragraph nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">So far, we have introduced the method of causal intervention that shows competitiveness for out-of-distribution generalization on graphs. As mentioned earlier in this blog, achieving guaranteed generalization requires necessary assumptions about how the data is generated. This triggers a natural inquiry: <em class="nz">What assumptions does causal intervention require for generalization?</em> Unlike invariant learning, causal intervention does not start from explicit assumptions but instead relies on implicit assumptions during modeling and analysis:</p><blockquote class="oa ob oc"><p id="ac64" class="nd ne nz nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There exists only one confounding factor (the environment) between the inputs and the labels.</p></blockquote><p id="fa48" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This assumption simplifies the analysis of the real system to some extent but introduces approximation errors. For more complex scenarios, there remains significant exploration space in the future.</p><h1 id="807c" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Generalization with Implicit Graph Structures</h1><p id="3a94" class="pw-post-body-paragraph nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">In the previous discussion, we assumed that the structural information of input data is observed and complete. For more general graph data, structural information may be partially observed or even completely unknown. Such data is referred to as implicit graph structures. Moreover, distribution shifts on graphs may involve underlying structures that impact data distribution, posing unresolved challenges in characterizing the influence of geometry on data distribution.</p><p id="e6c4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To address this, recent work [16], <em class="nz">Wu et al., “</em><a class="af od" href="https://arxiv.org/pdf/2406.04963" rel="noopener ugc nofollow" target="_blank"><em class="nz">Learning Divergence Fields for Shift-Robust Graph Representations</em></a><em class="nz">” (ICML2024)</em>, leverages the inherent connection between continuous diffusion equations and message passing mechanisms, integrating the causal intervention approach introduced earlier. This design aims to develop a learning method that is applicable for both explicit and implicit graph structures where distribution shifts pose the generalization challenge.</p><h2 id="e485" class="pj oo fq bf op pk pl pm os pn po pp ov nm pq pr ps nq pt pu pv nu pw px py pz bk">From Message Passing to Diffusion Equations</h2><p id="ee7b" class="pw-post-body-paragraph nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">Message Passing mechanism serves as a foundational design in modern graph neural networks and graph Transformers, propagating information from other nodes in each layer to update the representation of the central node. Essentially, if we view the layers of a neural network as discretized approximations of continuous time, then message passing can be seen as a discrete form of diffusion process on graphs [17, 18]. The following diagram illustrates their analogy. (We refer the readers interested in more details along this line to <a class="af od" href="https://michael-bronstein.medium.com/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774" rel="noopener">recent blogs by Prof. Michael Bronstein et al.</a>).</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml om"><img src="../Images/18aaa6a57b7e1d69b6bfa3900007361f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nyo9IFm-sUQRiKeo.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Message passing (the inter-layer updates in GNNs and Transformers) can be viewed as discrete iterations of a continuous diffusion equation through the analogy: nodes in the graph are mapped to locations on a manifold, node embeddings are represented by heat signals, layer-wise updates of embeddings correspond to changes in heat signals over time, and interactions between nodes in each layer are reflected by interactions between positions on the manifold.</figcaption></figure><p id="b1d3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Particularly, the diffusivity (denoted by d_u) in the diffusion equation controls the interactions between nodes during the diffusion process. When adopting local or global diffusion forms, the discrete iterations of the diffusion equation respectively lead to the layer-wise update formulas of Graph Neural Networks [18] and Transformers [19].</p><p id="6744" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">However, the deterministic diffusivity cannot model the multi-faceted effects and uncertainties in interactions between instances. Therefore, [16] proposes defining the diffusivity as a random sample from a probability distribution. The corresponding diffusion equation will yield a stochastic trajectory (as shown in the figure below).</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml om"><img src="../Images/5a55342e17812d86519433c30936b880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nX84Nz114FWq5hxn.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">After defining the diffusivity d_u as a random variable, the divergence field of the diffusion equation at each time (i.e., the change in node embeddings at the current layer) will become stochastic. This enables modeling the uncertainty in interactions between nodes.</figcaption></figure><p id="207a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Even so, if the traditional supervised learning objective is directly applied for training, the model described above can not generalize well with distribution shifts. This issue is echoed by the causal perspective of graph learning discussed earlier. Specifically, in the diffusion models considered here, the input x (such as a graph) and the output y (such as node labels in the graph) are associated by diffusivity. The diffusivity can be seen as an embodiment of the environment specific to the dataset, determining the interdependencies among instances. Therefore, the model trained on limited training data tends to learn specific interdependent patterns specific to the training set, making it unable to generalize to new test data.</p><h2 id="ead4" class="pj oo fq bf op pk pl pm os pn po pp ov nm pq pr ps nq pt pu pv nu pw px py pz bk">Causality-guided Divergence Field Learning</h2><p id="c823" class="pw-post-body-paragraph nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">To address this challenge, we once again employ causal intervention to eliminate the dependency between the diffusivity d and the input x during training. Unlike previous work [14] where the mapping from input to output was given by a predictor, here the dependence path from x to y involves a multi-step diffusion process (corresponding to multiple layers of updates in GNNs/Transformers). Therefore, causal intervention is needed at each step of the diffusion process. However, since the diffusivity is an abstract notion for modeling and cannot be directly observed (similar to the environment discussed earlier), [16] extends the variational approach used in [14] to derive a variational lower bound for the learning objective pertaining to the diffusion process. This serves as an approximate objective for causal intervention at each step of the diffusion process.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml om"><img src="../Images/adc9aad2a68adae832796aeea7619a98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Q-4XFz1meNlwEqef.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">The learning approach proposed in [16] estimates the diffusivity for each step of the diffusion model and applies causal intervention. This approach guides the model to learn stable causal relations from inputs to outputs, thereby enhancing its ability to generalize under distribution shifts. Image from <a class="af od" href="https://arxiv.org/pdf/2406.04963" rel="noopener ugc nofollow" target="_blank">the paper</a>.</figcaption></figure><p id="fe23" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As an implementation of the aforementioned method, [16] introduces three specific model designs:</p><ul class=""><li id="6f90" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qf qg qh bk"><strong class="nf fr">GLIND-GCN</strong>: Considers the diffusivity as a constant matrix instantiated by the normalized graph adjacency matrix;</li><li id="f25f" class="nd ne fq nf b go qi nh ni gr qj nk nl nm qk no np nq ql ns nt nu qm nw nx ny qf qg qh bk"><strong class="nf fr">GLIND-GAT</strong>: Considers the diffusivity as a time-dependent matrix implemented by graph attention networks;</li><li id="ea81" class="nd ne fq nf b go qi nh ni gr qj nk nl nm qk no np nq ql ns nt nu qm nw nx ny qf qg qh bk"><strong class="nf fr">GLIND-Trans</strong>: Considers the diffusivity as a time-dependent matrix implemented by global all-pair attention networks.</li></ul><p id="359c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Particularly, for GLIND-Trans, to address the quadratic complexity issue in global attention computations, [16] further adopts the linear attention function design from DIFFormer [19]. (We also refer the readers interested in how to achieve linear complexity for all-pair attentions to this <a class="af od" href="https://medium.com/towards-data-science/how-to-build-graph-transformers-with-o-n-complexity-d507e103d30a" rel="noopener">Blog</a>).</p><p id="cbd4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The table below presents partial experimental results in scenarios involving implicit structures.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml qn"><img src="../Images/79cdd9decfaf711c8f75a1ee1572cb71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*k56uYysh2_Y9NGWd.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Experimental results of testing Accuracy on CIFAR and STL, where the original datasets contain no structural information and we use k-nearest-neighbor to construct graphs. Furthermore, for CIFAR and STL, we introduce distribution shifts by adding rotation angles (that change the similarity function for k-nearest-neighbor) and using different k, respectively.</figcaption></figure><h1 id="06e6" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Summary and Discussion</h1><p id="872c" class="pw-post-body-paragraph nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">This blog briefly introduces recent advances in out-of-distribution (OOD) generalization, focusing primarily on three published papers [5, 14, 16]. These works approach the problem from the perspectives of invariant learning and causal intervention, proposing methods applicable to both explicit and implicit graph structures. As mentioned earlier, we note that OOD problems require assumptions about the data generation as a prerequisite for effective solutions. Based on this, future research could focus on refining existing methods or analyzing the limits of generalization under the well-established assumptions. It could also explore how to achieve generalization under other assumption conditions.</p><p id="ded9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Another challenge closely related to OOD generalization is <em class="nz">Out-of-Distribution Detection</em> [20, 21, 22]. Unlike OOD generalization, OOD detection aims to investigate how to equip models during training to recognize out-of-distribution samples appearing during the testing phase. Future research could also focus on extending the methods in this blog to OOD detection or exploring the intersection of these two problems.</p><h1 id="616c" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">References</h1><p id="4bf4" class="pw-post-body-paragraph nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">[1] Garg et al., Generalization and Representational Limits of Graph Neural Networks, ICLR 2020.</p><p id="ae39" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[2] Koh et al., WILDS: A Benchmark of in-the-Wild Distribution Shifts, ICML 2021</p><p id="c5d8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[3] Morris et al., Position: Future Directions in the Theory of Graph Machine Learning, ICML 2024.</p><p id="d079" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[4] Zhu et al., Shift-Robust GNNs: Overcoming the Limitations of Localized Graph Training Data, NeurIPS 2021.</p><p id="3850" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[5] Wu et al., Handling Distribution Shifts on Graphs: An Invariance Perspective, ICLR 2022.</p><p id="f058" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[6] Li et al., OOD-GNN: Out-of-Distribution Generalized Graph Neural Network, TKDE 2022.</p><p id="6369" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[7] Yehudai et al., From Local Structures to Size Generalization in Graph Neural Networks, ICML 2021.</p><p id="9653" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[8] Li et al., Size Generalization of Graph Neural Networks on Biological Data:<br/>Insights and Practices from the Spectral Perspective, Arxiv 2024.</p><p id="efb5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[9] Arjovsky, et al., Invariant Risk Minimization, Arxiv 2019.</p><p id="5c27" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[10] Rojas-Carulla, et al., Invariant Models for Causal Transfer Learning, JMLR 2018.</p><p id="0e36" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[11] Krueger et al., Out-of-Distribution Generalization via Risk Extrapolation, ICML 2021.</p><p id="4b37" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[12] Yang et al., Learning Substructure Invariance for Out-of-Distribution Molecular Representations, NeurIPS 2022.</p><p id="3667" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[13] Sui et al., Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift, NeurIPS 2023.</p><p id="d2e4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[14] Wu et al., Graph Out-of-Distribution Generalization via Causal Intervention, WWW 2024.</p><p id="39c8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[15] Pearl et al., Causal Inference in Statistics: A Primer, 2016.</p><p id="e6b6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[16] Wu et al., Learning Divergence Fields for Shift-Robust Graph Representations, ICML 2024.</p><p id="abbd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[17] Freidlin et al., Diffusion Processes on Graphs and the Averaging Principle, The Annals of probability 1993.</p><p id="c466" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[18] Chamberlain et al., GRAND: Graph Neural Diffusion, ICML 2021.</p><p id="9208" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[19] Wu et al., DIFFormer: Scalable (Graph) Transformers Induced by<br/>Energy Constrained Diffusion, ICLR 2023.</p><p id="e5ea" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[20] Wu et al., Energy-based Out-of-Distribution Detection for Graph Neural Networks, ICLR 2023.</p><p id="943d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[21] Liu et al., GOOD-D: On Unsupervised Graph Out-Of-Distribution Detection, WSDM 2023.</p><p id="1fb9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[22] Bao et al., Graph Out-of-Distribution Detection Goes Neighborhood Shaping, ICML 2024.</p></div></div></div></div>    
</body>
</html>