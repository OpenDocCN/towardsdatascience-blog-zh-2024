<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Machine Learning Guide for Predictive Accuracy: Interpolation and Extrapolation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Machine Learning Guide for Predictive Accuracy: Interpolation and Extrapolation</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-machine-learning-guide-for-predictive-accuracy-interpolation-and-extrapolation-45dd270ee871?source=collection_archive---------4-----------------------#2024-07-04">https://towardsdatascience.com/the-machine-learning-guide-for-predictive-accuracy-interpolation-and-extrapolation-45dd270ee871?source=collection_archive---------4-----------------------#2024-07-04</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8500" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Evaluating machine learning models beyond training data</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://rkiuchir.medium.com/?source=post_page---byline--45dd270ee871--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ryota Kiuchi, Ph.D." class="l ep by dd de cx" src="../Images/5459c434848898345d932320c4a01312.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*BEoGVPFLLJJv49YGqJGBgA@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--45dd270ee871--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://rkiuchir.medium.com/?source=post_page---byline--45dd270ee871--------------------------------" rel="noopener follow">Ryota Kiuchi, Ph.D.</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--45dd270ee871--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 4, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="4820" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Introduction</h1><p id="d5cf" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">In recent years, data-driven approaches such as machine learning (ML) and deep learning (DL) have been applied to a wide range of tasks including machine translation and personal customized recommendations. These technologies reveal some patterns within the given training dataset by analyzing numerous data. However, if the given dataset has some biases and does not include the data that you want to know or predict, it might be difficult to get the correct answer from the trained model.</p><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob oc"><img src="../Images/941658a16eabebab03560ebfa237dbf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0p4_HIzOqa1aT58X"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Photo by <a class="af ot" href="https://unsplash.com/@dawson2406?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Stephen Dawson</a> on <a class="af ot" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="a1d4" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">Let’s think about the case of ChatGPT. The latest version of ChatGPT at this time is ChatGPT 4o, and this model is trained on data until June 2023 (at the period of this article). Therefore, if you ask about something that happened in 2024 not included in the training data, you will not get an accurate answer. This is well-known as “hallucination,” and OpenAI added the preprocessing procedure to return a fixed answer as “unanswerable” for such kinds of questions. On the other hand, ChatGPT’s training data is also basically based on documents written in English, so it is not good at local domain knowledge outside of English-native countries such as Japan and France. Therefore, many companies and research groups put a lot of effort into customizing their LLM by including the region or domain-specific knowledge using RAG (Retrieval-Augmented Generation) or fine-tuning.</p><p id="1d3d" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">Hence, identifying what training data is used is important for understanding the applicability and limitations of AI models. On the other hand, one of the biggest challenges in data-driven approaches is that these technologies often need to perform beyond the range of the training dataset. These demands are typically seen in new product development in material science, predicting the effects of new pharmaceutical compounds, and predicting consumer behavior when launching products in the markets. These scenarios require the correct predictions in the sparse area and outside of the training data, which refer to interpolation and extrapolation.</p><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob oz"><img src="../Images/4fc75309a8d53d6ae33f3b7315e6bf84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RLYeAHST6L6gEY_2"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Photo by <a class="af ot" href="https://unsplash.com/@elevatebeer?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Elevate</a> on <a class="af ot" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="9c54" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">Interpolation involves making predictions within the known data range. If the training data is densely and uniformly distributed, accurate predictions can be obtained within that range. However, in practice, preparing such data is uncommon. On the other hand, extrapolation refers to making predictions outside the known data points’ range. Although predictions in such areas are highly desired, data-driven approaches typically struggle the most. Consequently, it is significantly important to understand the performance of both interpolation and extrapolation for each algorithm.</p><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pa"><img src="../Images/60d5eb36eebb5fa24ed0c4f4c92269d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kFy08MaOKkjmgkphtZgmpw.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Created by author</figcaption></figure><p id="68d2" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">This article examines various machine learning algorithms for their interpolation and extrapolation capabilities. We prepare an artificial training dataset and evaluate these capabilities by visualizing each model’s prediction results. The target of machine learning algorithms are as follows:</p><ul class=""><li id="2828" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pb pc pd bk">Symbolic Regressor</li><li id="6196" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">SVR (Support Vector Regression)</li><li id="9693" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Gaussian Process Regressor (GPR)</li><li id="fca9" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Decision Tree Regressor</li><li id="9b61" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Random Forest Regressor</li><li id="454d" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">XGBoost</li><li id="5798" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">LightGBM</li></ul><p id="16c8" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">In addition, we also evaluate ensemble models such as Voting Regressor and Stacking Regressor.</p><h2 id="df42" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">Codes</h2><p id="50e6" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Full of codes are available from below:</p><div class="qa qb qc qd qe qf"><a href="https://github.com/rkiuchir/blog_TDS/tree/main/02_compare_regression?source=post_page-----45dd270ee871--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">blog_TDS/02_compare_regression at main · rkiuchir/blog_TDS</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Blog Contents for Towards Data Science. Contribute to rkiuchir/blog_TDS development by creating an account on GitHub.</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">github.com</p></div></div><div class="qo l"><div class="qp l qq qr qs qo qt lq qf"/></div></div></a></div></div></div></div><div class="ab cb qu qv qw qx" role="separator"><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="f642" class="mi mj fq bf mk ml rc gq mn mo rd gt mq mr re mt mu mv rf mx my mz rg nb nc nd bk">Data Generation and Preprocessing</h1><p id="ad4e" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Firstly, we generate the artificial data using a simple nonlinear function that is slightly modified from the <a class="af ot" href="https://gplearn.readthedocs.io/en/stable/examples.html" rel="noopener ugc nofollow" target="_blank">symbolic regressor’s tutorial in gplearn</a> by adding the exponential term. This function consists of linear, quadratic, and exponential terms, defined as follows:</p><figure class="od oe of og oh oi oa ob paragraph-image"><div class="oa ob rh"><img src="../Images/0172ad132b748c46722140f59278115d.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*jJ-owBwIUPZhmIjugA6Z_g.png"/></div></figure><p id="c68b" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">where <em class="ri">x₀</em> and <em class="ri">x₁</em> take a range of -1 to 1. The plane of ground truth is as shown below:</p></div></div><div class="oi"><div class="ab cb"><div class="ll rj lm rk ln rl cf rm cg rn ci bh"><figure class="od oe of og oh oi"><div class="ro io l ed"><div class="rp rq l"/></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c85c" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">Since we examine the performance of each ML model in terms of interpolation and extrapolation, different datasets will be needed for each case.</p><p id="c496" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">For interpolation, we evaluate the model performance within the same range as with the training dataset. Thus, each model will be trained with discretized data points within the range of -1 to 1 and evaluated the predicted surface within the same range.</p><p id="c448" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">On the other hand, for extrapolation, the capability of the model within the range outside of the training dataset will be required. We will train the model using the discretized data points within the range of -0.5 to 1 for both <em class="ri">x₀</em> and <em class="ri">x₁</em> and assess the predicted surface within the range of -1 to 1. Consequently, the difference between the ground truth and predicted surface in the range of -1 to -0.5 for both <em class="ri">x₀</em> and <em class="ri">x₁</em> reveals the model capability in terms of extrapolation.</p><p id="e0ac" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">In this article, the impact of the number of points for the training dataset will also be evaluated by examining two cases: 20 and 100 points.</p><p id="a9bb" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">For example, 100 data points are generated as follows:</p><pre class="od oe of og oh rr rs rt bp ru bb bk"><span id="96d3" class="rv mj fq rs b bg rw rx l ry rz">import numpy as np<br/><br/>def target_function(x0, x1):<br/>    return x0**2 - x1**2 + x1 + np.exp(x0) - 1<br/><br/># Generate training data for interpolation<br/>X_train = rng.uniform(-1, 1, 200).reshape(100, 2)<br/>y_train = target_function(X_train[:, 0], X_train[:, 1])<br/><br/># Generate training data for extrapolation<br/>X_train = rng.uniform(-0.5, 1, 200).reshape(100, 2)<br/>y_train = target_function(X_train[:, 0], X_train[:, 1])</span></pre></div></div></div><div class="ab cb qu qv qw qx" role="separator"><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="10e6" class="mi mj fq bf mk ml rc gq mn mo rd gt mq mr re mt mu mv rf mx my mz rg nb nc nd bk">Introduction to Machine Learning Algorithms</h1><p id="99a4" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">In this article, we evaluate the performance of interpolation and extrapolation for the major 7 machine learning algorithms. In addition, the 6 ensemble models using 7 algorithms are also considered. Each algorithm has different structures and aspects, that introduce pros and cons for predicting performance. Here we summarize the characteristics of each algorithm as follows:</p><ol class=""><li id="8198" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz sa pc pd bk"><strong class="ng fr">Symbolic Regression</strong></li></ol><ul class=""><li id="047d" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pb pc pd bk">A trained model is expressed as the mathematical expressions fitted based on the genetic algorithms</li><li id="16a6" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Model is defined as the function, contributing high interpretability</li><li id="10d0" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Appropriate for the task that target variable can be expressed as a function of features</li><li id="4285" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Good at interpolation but may have some potential in extrapolation</li></ul><div class="qa qb qc qd qe qf"><a rel="noopener follow" target="_blank" href="/find-hidden-laws-within-your-data-with-symbolic-regression-ebe55c1a4922?source=post_page-----45dd270ee871--------------------------------"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Find Hidden Laws Within Your Data with Symbolic Regression</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Automatically discover fundamental formulas like Kepler and Newton</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">towardsdatascience.com</p></div></div><div class="qo l"><div class="sb l qq qr qs qo qt lq qf"/></div></div></a></div><p id="e1e0" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr">2. Support Vector Regression (SVR)</strong></p><ul class=""><li id="2cc8" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pb pc pd bk">Based on a Support Vector Machine (SVM) that can efficiently handle the nonlinear relationship in the high dimensional spaces using the kernel method</li><li id="088c" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Using different types of kernels such as linear, RBF, polynomial, and sigmoid kernels, a model can express complex data patterns</li><li id="854d" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Good at interpolation but less stable in extrapolation</li></ul><div class="qa qb qc qd qe qf"><a rel="noopener follow" target="_blank" href="/the-complete-guide-to-support-vector-machine-svm-f1a820d8af0b?source=post_page-----45dd270ee871--------------------------------"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">The Complete Guide to Support Vector Machine (SVM)</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Understand its inner workings and implement SVMs in four different scenarios</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">towardsdatascience.com</p></div></div><div class="qo l"><div class="sc l qq qr qs qo qt lq qf"/></div></div></a></div><p id="fc08" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr">3. Gaussian Process Regression (GPR)</strong></p><ul class=""><li id="42f4" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pb pc pd bk">Based on the Bayesian method, the prediction is expressed as the probability which includes the predicted value and its uncertainty</li><li id="6bb6" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Thanks to the uncertainty estimation, GPR used for Bayesian Optimization</li><li id="441a" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Using different types of kernels such as linear, RBF, polynomial, and sigmoid kernels, a model can express complex data patterns</li><li id="9c3c" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Good at interpolation, and some potential for extrapolation selecting appropriate kernel selection</li></ul><div class="qa qb qc qd qe qf"><a rel="noopener follow" target="_blank" href="/quick-start-to-gaussian-process-regression-36d838810319?source=post_page-----45dd270ee871--------------------------------"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Quick Start to Gaussian Process Regression</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">A quick guide to understanding Gaussian process regression (GPR) and using scikit-learn’s GPR package</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">towardsdatascience.com</p></div></div><div class="qo l"><div class="sd l qq qr qs qo qt lq qf"/></div></div></a></div><p id="72be" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr">4. Decision Tree</strong></p><ul class=""><li id="e11f" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pb pc pd bk">Simple tree-shape algorithm which successively splits the data</li><li id="c388" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Easy to understand and interpret but tends to overfit</li><li id="451a" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Step-like estimation for interpolation and not good at extrapolation</li></ul><div class="qa qb qc qd qe qf"><a rel="noopener follow" target="_blank" href="/decision-tree-in-machine-learning-e380942a4c96?source=post_page-----45dd270ee871--------------------------------"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Decision Tree in Machine Learning</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">A decision tree is a flowchart-like structure in which each internal node represents a test on a feature (e.g. whether…</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">towardsdatascience.com</p></div></div><div class="qo l"><div class="se l qq qr qs qo qt lq qf"/></div></div></a></div><p id="6003" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr">5. Random Forest</strong></p><ul class=""><li id="5cd6" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pb pc pd bk">An ensemble-based algorithm which is called “Bagging” consisting of multiple decision trees</li><li id="52da" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">By combining multiple diverse trees, this algorithm can reduce overfitting risk and have a high interpolation performance</li><li id="ed41" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">More stable predictions than single decision trees but not good at extrapolation</li></ul><div class="qa qb qc qd qe qf"><a rel="noopener follow" target="_blank" href="/understanding-random-forest-58381e0602d2?source=post_page-----45dd270ee871--------------------------------"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Understanding Random Forest</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">How the Algorithm Works and Why it Is So Effective</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">towardsdatascience.com</p></div></div><div class="qo l"><div class="sf l qq qr qs qo qt lq qf"/></div></div></a></div><p id="73ef" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr">6. XGBoost</strong></p><ul class=""><li id="9b77" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pb pc pd bk">An ensemble-based algorithm which is called “Boosting” combines multiple decision trees by sequentially reducing errors</li><li id="f9ec" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Commonly used for competition such as Kaggle because of the good prediction performance</li><li id="c8ba" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">More stable predictions than single decision trees but not good at extrapolation</li></ul><div class="qa qb qc qd qe qf"><a href="https://medium.com/sfu-cspmp/xgboost-a-deep-dive-into-boosting-f06c9c41349?source=post_page-----45dd270ee871--------------------------------" rel="noopener follow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">XGBoost: A Deep Dive into Boosting</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Every day we hear about the breakthroughs in Artificial Intelligence. However, have you wondered what challenges it…</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">medium.com</p></div></div><div class="qo l"><div class="sg l qq qr qs qo qt lq qf"/></div></div></a></div><p id="3bb5" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr">7. LightGBM</strong></p><ul class=""><li id="5add" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pb pc pd bk">Similar to XGBoost, but with faster training speed and memory efficiency, which is more suitable for the larger datasets</li><li id="0d02" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">More stable predictions than single decision trees but not good at extrapolation</li></ul><div class="qa qb qc qd qe qf"><a href="https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc?source=post_page-----45dd270ee871--------------------------------" rel="noopener follow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">What is LightGBM, How to implement it? How to fine tune the parameters?</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Hello,</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">medium.com</p></div></div><div class="qo l"><div class="sh l qq qr qs qo qt lq qf"/></div></div></a></div><p id="a42b" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr">8. Voting Regressor</strong></p><ul class=""><li id="6ace" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pb pc pd bk">An ensemble learning method combining predictions from multiple models</li><li id="a179" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Mixing different model characteristics, which contribute to more robust predictions than a single model</li><li id="565d" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Evaluated in three combinations in this article:<br/>– Support Vector Regressor + Random Forest<br/>– Gaussian Process Regressor + Random Forest<br/>– Random Forest + XGBoost</li></ul><div class="qa qb qc qd qe qf"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html?source=post_page-----45dd270ee871--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">VotingRegressor</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Gallery examples: Plot individual and voting regression predictions</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">scikit-learn.org</p></div></div><div class="qo l"><div class="si l qq qr qs qo qt lq qf"/></div></div></a></div><p id="12cc" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr">9. Stacking Regressor</strong></p><ul class=""><li id="a3e7" class="ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz pb pc pd bk">An ensemble learning method that uses predictions from multiple models as input for a final prediction model, “meta-model”</li><li id="34ec" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Meta model covers individual model weaknesses and combines each model's strengths</li><li id="61a6" class="ne nf fq ng b go pe ni nj gr pf nl nm nn pg np nq nr ph nt nu nv pi nx ny nz pb pc pd bk">Evaluated in three combinations in this article:<br/>– Base model: Support Vector Regressor + Random Forest; Meta-model: Random Forest<br/>– Base model: Gaussian Process Regressor + Random Forest; Meta-model: Random Forest<br/>– Base model: Random Forest + XGBoost; Meta-model: Random Forest</li></ul><div class="qa qb qc qd qe qf"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html?source=post_page-----45dd270ee871--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">StackingRegressor</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Gallery examples: Combine predictors using stacking</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">scikit-learn.org</p></div></div><div class="qo l"><div class="sj l qq qr qs qo qt lq qf"/></div></div></a></div><p id="1e7d" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">Using these algorithms, we will evaluate both interpolation and extrapolation performance with the dataset we generated earlier. In the following sections, the training methods and evaluation approaches for each model will be explained.</p></div></div></div><div class="ab cb qu qv qw qx" role="separator"><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="919e" class="mi mj fq bf mk ml rc gq mn mo rd gt mq mr re mt mu mv rf mx my mz rg nb nc nd bk">Model Training and Evaluation</h1><h2 id="f2c6" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">Preprocessing</h2><p id="5138" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Basically, except for tree-based approaches such as Random Forest, XGBoost, and LightGBM, most machine learning algorithms require feature scaling. However, since we only use two features such as <em class="ri">x₀</em> and <em class="ri">x₁</em> which take the same range, -1 to 1 (interpolation) or -0.5 to 1 (extrapolation) in this practice, we will skip the feature scaling.</p><h2 id="b91e" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">Model Training</h2><p id="b93a" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">For simplicity, the default hyper parameters are used for all algorithms except LightGBM of which default parameters are suitable for the larger dataset.</p><p id="ba63" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">As introduced in the earlier section, we will use different datasets for the evaluation of interpolation and extrapolation during model training.</p><h2 id="f683" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">Evaluation and Visualization</h2><p id="8ad3" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">After model training, we will predict using very finely discretized data. Based on these predicted values, the prediction surface will be drawn using the <a class="af ot" href="https://plotly.com/python/3d-surface-plots/" rel="noopener ugc nofollow" target="_blank">Plotly surface function</a>.</p><p id="62be" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">These procedures are done by the following code:</p><pre class="od oe of og oh rr rs rt bp ru bb bk"><span id="2606" class="rv mj fq rs b bg rw rx l ry rz">class ModelFitterAndVisualizer:<br/>    def __init__(self, X_train, y_train, y_truth, scaling=False, random_state=41):<br/>        """<br/>        Initialize the ModelFitterAndVisualizer class with training and testing data.<br/><br/>        Parameters:<br/>            X_train (pd.DataFrame): Training data features<br/>            y_train (pd.Series): Training data target<br/>            y_truth (pd.Series): Ground truth for predictions<br/>            scaling (bool): Flag to indicate if scaling should be applied<br/>            random_state (int): Seed for random number generation<br/>        """<br/>        self.X_train = X_train<br/>        self.y_train = y_train<br/>        self.y_truth = y_truth<br/>        <br/>        self.initialize_models(random_state)<br/>        <br/>        self.scaling = scaling<br/><br/>    # Initialize models<br/>    # -----------------------------------------------------------------<br/>    def initialize_models(self, random_state):<br/>        """<br/>        Initialize the models to be used for fitting and prediction.<br/><br/>        Parameters:<br/>            random_state (int): Seed for random number generation<br/>        """<br/>                <br/>        # Define kernel for GPR<br/>        kernel = 1.0 * RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0)<br/>        <br/>        # Define Ensemble Models Estimator<br/>        # Decision Tree + Kernel Method<br/>        estimators_rf_svr = [<br/>            ('rf', RandomForestRegressor(n_estimators=30, random_state=random_state)),<br/>            ('svr', SVR(kernel='rbf')),<br/>        ]<br/>        estimators_rf_gpr = [<br/>            ('rf', RandomForestRegressor(n_estimators=30, random_state=random_state)),<br/>            ('gpr', GaussianProcessRegressor(kernel=kernel, normalize_y=True, random_state=random_state))<br/>        ]<br/>        # Decision Trees<br/>        estimators_rf_xgb = [<br/>            ('rf', RandomForestRegressor(n_estimators=30, random_state=random_state)),<br/>            ('xgb', xgb.XGBRegressor(random_state=random_state)),<br/>        ]<br/>        <br/>        self.models = [<br/>            SymbolicRegressor(random_state=random_state),<br/>            SVR(kernel='rbf'),<br/>            GaussianProcessRegressor(kernel=kernel, normalize_y=True, random_state=random_state),<br/>            DecisionTreeRegressor(random_state=random_state),<br/>            RandomForestRegressor(random_state=random_state),<br/>            xgb.XGBRegressor(random_state=random_state),<br/>            lgbm.LGBMRegressor(n_estimators=50, num_leaves=10, min_child_samples=3, random_state=random_state),<br/>            VotingRegressor(estimators=estimators_rf_svr),<br/>            StackingRegressor(estimators=estimators_rf_svr, <br/>                              final_estimator=RandomForestRegressor(random_state=random_state)),<br/>            VotingRegressor(estimators=estimators_rf_gpr),<br/>            StackingRegressor(estimators=estimators_rf_gpr, <br/>                              final_estimator=RandomForestRegressor(random_state=random_state)),<br/>            VotingRegressor(estimators=estimators_rf_xgb),<br/>            StackingRegressor(estimators=estimators_rf_xgb, <br/>                              final_estimator=RandomForestRegressor(random_state=random_state)),<br/>        ]<br/>        <br/>        # Define graph titles<br/>        self.titles = [<br/>            "Ground Truth", "Training Points", <br/>            "SymbolicRegressor", "SVR", "GPR",<br/>            "DecisionTree", "RForest", <br/>            "XGBoost", "LGBM", <br/>            "Vote_rf_svr", "Stack_rf_svr__rf",<br/>            "Vote_rf_gpr", "Stack_rf_gpr__rf",<br/>            "Vote_rf_xgb", "Stack_rf_xgb__rf",<br/>        ] <br/><br/>    def fit_models(self):<br/>        """<br/>        Fit the models to the training data.<br/><br/>        Returns:<br/>            self: Instance of the class with fitted models<br/>        """<br/>        if self.scaling:<br/>            scaler_X = MinMaxScaler()<br/>            self.X_train_scaled = scaler_X.fit_transform(self.X_train)<br/>        else:<br/>            self.X_train_scaled = self.X_train.copy()<br/>        <br/>        for model in self.models:<br/>            model.fit(self.X_train_scaled, self.y_train)<br/>        return self<br/><br/>    def visualize_surface(self, x0, x1, width=400, height=500,<br/>                          num_panel_columns=5,<br/>                          vertical_spacing=0.06, horizontal_spacing=0,<br/>                          output=None, display=False, return_fig=False):<br/>        """<br/>        Visualize the prediction surface for each model.<br/><br/>        Parameters:<br/>            x0 (np.ndarray): Meshgrid for feature 1<br/>            x1 (np.ndarray): Meshgrid for feature 2<br/>            width (int): Width of the plot<br/>            height (int): Height of the plot<br/>            output (str): File path to save the plot<br/>            display (bool): Flag to display the plot<br/>        """<br/>        <br/>        num_plots = len(self.models) + 2<br/>        num_panel_rows = num_plots // num_panel_columns<br/>        <br/>        whole_width = width * num_panel_columns<br/>        whole_height = height * num_panel_rows<br/><br/>        specs = [[{'type': 'surface'} for _ in range(num_panel_columns)] for _ in range(num_panel_rows)]<br/>        fig = make_subplots(rows=num_panel_rows, cols=num_panel_columns, <br/>                            specs=specs, subplot_titles=self.titles,<br/>                            vertical_spacing=vertical_spacing, <br/>                            horizontal_spacing=horizontal_spacing)<br/><br/>        for i, model in enumerate([None, None] + self.models):<br/>            # Assign the subplot panels<br/>            row = i // num_panel_columns + 1<br/>            col = i % num_panel_columns + 1<br/>            <br/>            # Plot training points<br/>            if i == 1:<br/>                fig.add_trace(go.Scatter3d(x=self.X_train[:, 0], y=self.X_train[:, 1], z=self.y_train,<br/>          mode='markers', marker=dict(size=2, color='darkslategray'),<br/>          name='Training Data'), row=row, col=col) <br/>                <br/>                surface = go.Surface(z=self.y_truth, x=x0, y=x1, <br/>                                     showscale=False, opacity=.4)<br/>                fig.add_trace(surface, row=row, col=col)<br/>            <br/>            # Plot predicted surface for each model and ground truth<br/>            else:<br/>                y_pred = self.y_truth if model is None else model.predict(np.c_[x0.ravel(), x1.ravel()]).reshape(x0.shape)<br/>                surface = go.Surface(z=y_pred, x=x0, y=x1, <br/>                                     showscale=False)<br/>                fig.add_trace(surface, row=row, col=col)<br/><br/><br/>            fig.update_scenes(dict(<br/>                xaxis_title='x0',<br/>                yaxis_title='x1',<br/>                zaxis_title='y',<br/>                ), row=row, col=col)<br/>                      <br/>        fig.update_layout(title='Model Predictions and Ground Truth', <br/>                          width=whole_width, <br/>                          height=whole_height)<br/>        <br/>        <br/>        # Change camera angle<br/>        camera = dict(<br/>         up=dict(x=0, y=0, z=1),<br/>         center=dict(x=0, y=0, z=0),<br/>         eye=dict(x=-1.25, y=-1.25, z=2)<br/>        )<br/>        for i in range(num_plots):<br/>            fig.update_layout(**{f'scene{i+1}_camera': camera})<br/><br/>                <br/>        if display:<br/>            fig.show()<br/>        <br/>        if output:<br/>            fig.write_html(output)<br/>            <br/>        if return_fig:<br/>            return fig</span></pre></div></div></div><div class="ab cb qu qv qw qx" role="separator"><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="26e9" class="mi mj fq bf mk ml rc gq mn mo rd gt mq mr re mt mu mv rf mx my mz rg nb nc nd bk">Evaluation of Interpolation Performance</h1><p id="0d6b" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The prediction surfaces for each algorithm are shown for training data cases of 100 and 20 points respectively.</p><h2 id="3371" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">100 Training Points:</h2><p id="2eed" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The original interactive figures can be viewed from <a class="af ot" href="https://chart-studio.plotly.com/~rkiuchi/87" rel="noopener ugc nofollow" target="_blank">here</a></p></div></div><div class="oi bh"><figure class="od oe of og oh oi bh paragraph-image"><img src="../Images/d6738a6729ec30307118a6d37ce47b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:3588/format:webp/1*nmrD_X6Y6ckgRAG1cCiRKg.png"/></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="5913" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">20 Training Points:</h2><p id="c88b" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The original interactive figures can be viewed from <a class="af ot" href="https://chart-studio.plotly.com/~rkiuchi/89" rel="noopener ugc nofollow" target="_blank">here</a></p></div></div><div class="oi bh"><figure class="od oe of og oh oi bh paragraph-image"><img src="../Images/a5f138d525a873a3a2daeb3f6986afb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:3576/format:webp/1*txBOjszDtbPnH5q2Q3L31Q.png"/></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="802f" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">Here are the summarized features for each algorithm:</p><h2 id="4a3d" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">Symbolic Regressor</h2><p id="2adc" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">This algorithm performs almost perfectly in interpolation with 100 data points, but moderately good with 20 data points. This is because the Symbolic Regressor approximates the mathematical expressions and the simple functional form is used in this practice. Thanks to this feature, the predicted surface is notably smooth which is different from the tree-based algorithms explained later.</p><h2 id="3d26" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">Support Vector Regressor (SVR), Gaussian Process Regressor (GPR)</h2><p id="09a9" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">For kernel-based algorithms SVR and GPR, although the predicted surfaces slightly differ from the ground truth, interpolation performance is generally good with 100 data points. In addition, the prediction surface obtained from these models is smooth similar to one estimated by Symbolic Regressor. However, in the case of 20 points, there is a significant difference between the predicted surface and the ground truth especially for SVR.</p><h2 id="fc88" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">Decision Tree, Random Forest, XGBoost, LightGBM</h2><p id="b905" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Firstly, the prediction surfaces estimated by these five tree-based models are not smooth but more step-like shapes. This characteristic arises from the structure and learning method of decision trees. Decision trees split the data recursively based on a threshold for one of the features. Each data point is assigned to some leaf nodes whose values are represented as the average value of the data points in that node. Therefore, the prediction values are constant within each leaf node, resulting in a step-like prediction surface.</p><p id="2237" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">The estimates of a single decision tree clearly show this characteristic. On the other hand, ensemble methods like Random Forests, XGBoost, and LightGBM, which consist of many decision trees within a single model, generate relatively smoother prediction surfaces due to the more different thresholds based on the many different shapes of decision trees.</p><h2 id="1d60" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">Voting Regressor, Stacking Regressor</h2><p id="421b" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The Voting Regressor combines the results of two algorithms by averaging them. For combinations like Random Forest + SVR, and Random Forest + GPR, the prediction surfaces reflect characteristics that mix the kernel-based and tree-based models. On the other hand, the combination of tree-based models like Random Forest and XGBoost relatively reduces the step-like shape prediction surface than one estimated from the single model.</p><p id="9c8e" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">The Stacking Regressor, which uses a meta-model to compute final predictions based on the outputs of multiple models, also shows step-like surfaces, because of the Random Forest used as the meta-model. This characteristic will be changed if kernel-based algorithms like SVR or GPR are used as the meta-model.</p></div></div></div><div class="ab cb qu qv qw qx" role="separator"><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0193" class="mi mj fq bf mk ml rc gq mn mo rd gt mq mr re mt mu mv rf mx my mz rg nb nc nd bk">Evaluation of Extrapolation Performance</h1><p id="a7dc" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">As explained earlier, each model is trained with data ranging from -0.5 to 1 for both <em class="ri">x₀</em> and <em class="ri">x₁</em> and those performances will be evaluated within the range of -1 to 1. Therefore, we get to know the extrapolation ability to inspect the prediction surface with the range of -1 to -0.5 for both <em class="ri">x₀</em> and <em class="ri">x₁.</em></p><p id="1087" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">The prediction surfaces for each algorithm are shown for training data cases of 100 and 20 points respectively.</p><h2 id="23cb" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">100 Training Points:</h2><p id="da61" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The original interactive figures can be viewed from <a class="af ot" href="https://chart-studio.plotly.com/~rkiuchi/91" rel="noopener ugc nofollow" target="_blank">here</a></p></div></div><div class="oi bh"><figure class="od oe of og oh oi bh paragraph-image"><img src="../Images/a2352079e0966edba848a71e21f702ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:3578/format:webp/1*nq0O-sow0cWl5EsqUrExCA.png"/></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="e38d" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">20 Training Points:</h2><p id="cf38" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The original interactive figures can be viewed from <a class="af ot" href="https://chart-studio.plotly.com/~rkiuchi/93" rel="noopener ugc nofollow" target="_blank">here</a></p></div></div><div class="oi bh"><figure class="od oe of og oh oi bh paragraph-image"><img src="../Images/8b2bef5db2a4e1cf26276c879f3848cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:3562/format:webp/1*nPmyXtd9lTw5sa4Ie8Z0lA.png"/></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="f2bd" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">Symbolic Regressor</h2><p id="31f4" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The predicted surface within the area of extrapolation obtained by the Symbolic Regressor which is trained with 100 data points is almost accurately estimated similar to the interpolation evaluation. However, with only 20 training data points used, the predicted surface differs from the ground truth especially in the edge of the surface, indicating that the obtained functional form is not well estimated.</p><h2 id="2e83" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">Support Vector Regressor (SVR), Gaussian Process Regressor (GPR)</h2><p id="b7e5" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Although both SVR and GPR are kernel-based algorithms, the obtained results are totally different. For both of 20 and 100 data points, while the predicted surface from SVR is well not estimated, GPR predicts almost perfectly even within the range of extrapolation.</p><h2 id="426a" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">Decision Tree, Random Forest, XGBoost, LightGBM</h2><p id="ae6e" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Although there are some differences among the results from these tree-based models, the predicted surfaces are constant in the range of extrapolation. This is because that decision trees rely on splits and no splits are generated in extrapolation regions, which cause constant values.</p><h2 id="8d4a" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">Voting Regressor, Stacking Regressor</h2><p id="435b" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">As seen above, the kernel-based algorithms have better performance compared to the tree-based ones. The Voting Regressor with the combination of Random Forest and XGBoost, and all three Stacking Regressors whose meta-model is Random Forest predict constant in the range of extrapolation. On the other hand, the prediction surfaces derived from the Voting Regressor with the combination of Random Forest + SVR, and Random Forest + GPR have the blended characteristics of kernel-based and tree-based models.</p></div></div></div><div class="ab cb qu qv qw qx" role="separator"><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2137" class="mi mj fq bf mk ml rc gq mn mo rd gt mq mr re mt mu mv rf mx my mz rg nb nc nd bk">Summary</h1><p id="01af" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">In this article, we evaluated the interpolation and extrapolation performance of the various machine learning algorithms. Since the ground truth data we used is expressed as a simple functional foam, symbolic regressor and kernel-based algorithms provide a better performance, especially for extrapolation than tree-based algorithms. However, more complex tasks that cannot be expressed in mathematical formulas might bring different results.</p><p id="3fe8" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk">Thank you so much for reading this article! I hope this article helps you understand the interpolation and extrapolation performance of machine learning models, making it easier to select and apply the right models for your projects.</p><p id="0f22" class="pw-post-body-paragraph ne nf fq ng b go ou ni nj gr ov nl nm nn ow np nq nr ox nt nu nv oy nx ny nz fj bk"><strong class="ng fr"><em class="ri">Your clap to this article and subscription to </em></strong><a class="af ot" href="https://rkiuchir.medium.com/subscribe" rel="noopener"><strong class="ng fr"><em class="ri">my newsletter</em></strong></a><strong class="ng fr"><em class="ri"> would motivate me a lot!</em></strong></p><h1 id="b8d3" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Links</h1><h2 id="ff6f" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">Other articles</h2><div class="qa qb qc qd qe qf"><a rel="noopener follow" target="_blank" href="/how-openais-sora-is-changing-the-game-an-insight-into-its-core-technologies-bd1ad17170df?source=post_page-----45dd270ee871--------------------------------"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">How OpenAI’s Sora is Changing the Game: An Insight into Its Core Technologies</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">A masterpiece of state of the art technologies</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">towardsdatascience.com</p></div></div><div class="qo l"><div class="sk l qq qr qs qo qt lq qf"/></div></div></a></div><div class="qa qb qc qd qe qf"><a rel="noopener follow" target="_blank" href="/create-interactive-globe-earthquake-plot-in-python-b0b52b646f27?source=post_page-----45dd270ee871--------------------------------"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Create “Interactive Globe + Earthquake Plot in Python</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">How to create a cool interactive figure in Python: the Globe plotted by Plotly.</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">towardsdatascience.com</p></div></div><div class="qo l"><div class="sl l qq qr qs qo qt lq qf"/></div></div></a></div><div class="qa qb qc qd qe qf"><a rel="noopener follow" target="_blank" href="/pandas-cheat-sheet-for-data-preprocessing-cd1bcd607426?source=post_page-----45dd270ee871--------------------------------"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">Pandas Cheat Sheet for Data Preprocessing</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Practical guide about how to preprocess data with Pandas</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">towardsdatascience.com</p></div></div><div class="qo l"><div class="sm l qq qr qs qo qt lq qf"/></div></div></a></div><h2 id="3ca0" class="pj mj fq bf mk pk pl pm mn pn po pp mq nn pq pr ps nr pt pu pv nv pw px py pz bk">Personal website</h2><div class="qa qb qc qd qe qf"><a href="https://rkiuchir.github.io/?source=post_page-----45dd270ee871--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab ig"><div class="qh ab co cb qi qj"><h2 class="bf fr hw z io qk iq ir ql it iv fp bk">R. Kiuchi — Seismology</h2><div class="qm l"><h3 class="bf b hw z io qk iq ir ql it iv dx">Edit description</h3></div><div class="qn l"><p class="bf b dy z io qk iq ir ql it iv dx">rkiuchir.github.io</p></div></div><div class="qo l"><div class="sn l qq qr qs qo qt lq qf"/></div></div></a></div></div></div></div></div>    
</body>
</html>