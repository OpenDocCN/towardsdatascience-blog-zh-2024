<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Reframing LLM ‘Chat with Data’: Introducing LLM-Assisted Data Recipes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Reframing LLM ‘Chat with Data’: Introducing LLM-Assisted Data Recipes</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reframing-llm-chat-with-data-introducing-llm-assisted-data-recipes-f4096ac8c44b?source=collection_archive---------0-----------------------#2024-01-26">https://towardsdatascience.com/reframing-llm-chat-with-data-introducing-llm-assisted-data-recipes-f4096ac8c44b?source=collection_archive---------0-----------------------#2024-01-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@astrobagel?source=post_page---byline--f4096ac8c44b--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Matthew Harris" class="l ep by dd de cx" src="../Images/4fa3264bb8a028633cd8d37093c16214.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*SQpPIBppBtQGfoSP_sAeaQ.png"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--f4096ac8c44b--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://medium.com/@astrobagel?source=post_page---byline--f4096ac8c44b--------------------------------" rel="noopener follow">Matthew Harris</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--f4096ac8c44b--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx kq kr ab q ee ks kt" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="kp"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ko kp">7</span></p></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="ku k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al kv an ao ap hr kw kx ky" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep kz cn"><div class="l ae"><div class="ab cb"><div class="la lb lc ld le lf ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ma mb mc md me mf lx ly paragraph-image"><div role="button" tabindex="0" class="mg mh ed mi bh mj"><div class="lx ly lz"><img src="../Images/267dd67fccb5016050c565f6d92d965a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LRATWy7DHRWC7ClBzFduJQ.png"/></div></div><figcaption class="ml mm mn lx ly mo mp bf b bg z dx">Source DALL·E 3 prompted with “Oil painting of a data chef making data recipes”</figcaption></figure><p id="c205" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><em class="no">TL;DR</em></p><p id="59d4" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><em class="no">In this article, we cover some of the limitations in using Large Language Models (LLMs) to ‘Chat with Data’, proposing a ‘Data Recipes’ methodology which may be an alternative in some situations. Data Recipes extends the idea of reusable code snippets but includes data and has the advantage of being programmed conversationally using an LLM. This enables the creation of a reusable Data Recipes Library — for accessing data and generating insights — which offers more transparency for LLM-generated code with a human-in-the-loop to moderate recipes as required. Cached results from recipes — sourced from SQL queries or calls to external APIs — can be refreshed asynchronously for improved response times. The proposed solution is a variation of the LLMs As Tool Makers (LATM) architecture which splits the workflow into two streams: (i) A low transaction volume / high-cost stream for creating recipes; and (ii) A high transaction volume / low-cost stream for end-users to use recipes. Finally, by having a library of recipes and associated data integration, it is possible to create a ‘Data Recipes Hub’ with the possibility of community contribution.</em></p><h1 id="2562" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Using LLMs for conversational data analysis</h1><p id="e569" class="pw-post-body-paragraph mq mr fq ms b mt on mv mw mx oo mz na nb op nd ne nf oq nh ni nj or nl nm nn fj bk">There are some very clever patterns now that allow people to ask questions in natural language about data, where a Large Language Model (LLM) generates calls to get the data and summarizes the output for the user. Often referred to as ‘<a class="af os" href="https://www.google.com/search?q=chat+with+data&amp;oq=chat+with+data&amp;gs_lcrp=EgZjaHJvbWUyCQgAEEUYORiABDIHCAEQABiABDIHCAIQABiABDIHCAMQABiABDIHCAQQABiABDIHCAUQABiABDIHCAYQABiABDIICAcQABgWGB4yCAgIEAAYFhgeMggICRAAGBYYHqgCALACAA&amp;sourceid=chrome&amp;ie=UTF-8" rel="noopener ugc nofollow" target="_blank">Chat with Data</a>’, I’ve previously posted some articles illustrating this technique, for example using Open AI assistants to <a class="af os" href="https://medium.com/towards-data-science/preparing-for-climate-change-with-an-ai-assistant-cdceb5ce4426" rel="noopener">help people prepare for climate change</a>. There are many more advanced examples out there it can be an amazing way to lower the technical barrier for people to gain insights from complicated data.</p><figure class="ou ov ow ox oy mf lx ly paragraph-image"><div role="button" tabindex="0" class="mg mh ed mi bh mj"><div class="lx ly ot"><img src="../Images/f1606fe5407d379c2f3a646771a0bcac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5ZcclVsx5bvgmqRSl4djA.png"/></div></div><figcaption class="ml mm mn lx ly mo mp bf b bg z dx">Examples of using LLMs to generate SQL queries from user inputs, and summarize output to provide an answer. Sources: <a class="af os" href="https://python.langchain.com/docs/use_cases/sql/" rel="noopener ugc nofollow" target="_blank">Langchain SQL Agents</a></figcaption></figure><figure class="ou ov ow ox oy mf lx ly paragraph-image"><div role="button" tabindex="0" class="mg mh ed mi bh mj"><div class="lx ly oz"><img src="../Images/073a5162ed1e78e9bf81a1b7cf32161f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OTD6a2TQFmi-M-gR9FMvzw.png"/></div></div><figcaption class="ml mm mn lx ly mo mp bf b bg z dx">Examples of using LLMs to generate API calls from user inputs, and summarize output to provide an answer. Sources: <a class="af os" href="https://python.langchain.com/docs/use_cases/apis" rel="noopener ugc nofollow" target="_blank">Langchain Interacting with APIs</a></figcaption></figure><p id="447c" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The method for accessing data typically falls into the following categories …</p><ol class=""><li id="8fa0" class="mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn pa pb pc bk"><strong class="ms fr">Generating Database queries</strong>: The LLM converts natural language to a query language such as SQL or Cypher</li><li id="506c" class="mq mr fq ms b mt pd mv mw mx pe mz na nb pf nd ne nf pg nh ni nj ph nl nm nn pa pb pc bk"><strong class="ms fr">Generating API Queries</strong>: The LLM converts natural language to text used to call APIs</li></ol><p id="2ece" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The application executes the LLM-provided suggestion to get the data, then usually passes the results back to the LLM to summarize.</p><h1 id="172b" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Getting the Data Can be a Problem</h1><p id="5798" class="pw-post-body-paragraph mq mr fq ms b mt on mv mw mx oo mz na nb op nd ne nf oq nh ni nj or nl nm nn fj bk">It’s amazing that these techniques now exist, but in turning them into production solutions each has its advantages and disadvantages …</p><figure class="ou ov ow ox oy mf lx ly paragraph-image"><div role="button" tabindex="0" class="mg mh ed mi bh mj"><div class="lx ly pi"><img src="../Images/8487be1071eb8a832c03944062282417.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9wKohSKhXVo1frDsyQTJjQ.png"/></div></div><figcaption class="ml mm mn lx ly mo mp bf b bg z dx">LLMs can generate text for executing database queries and calling external APIs, but each has its advantages and disadvantages</figcaption></figure><p id="c08d" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">For example, generating SQL supports all the amazing things a modern database query language can do, such as aggregation across large volumes of data. However, the data might not already be in a database where SQL can be used. It could be ingested and then queried with SQL, but building pipelines like this can be complex and costly to manage.</p><p id="0483" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Accessing data directly through APIs means the data doesn’t have to be in a database and opens up a huge world of publically available datasets, but there is a catch. Many APIs do not support aggregate queries like those supported by SQL, so the only option is to extract the low-level data, and then aggregate it. This puts more burden on the LLM application and can require extraction of large amounts of data.</p><p id="24fd" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">So both techniques have limitations.</p><h1 id="9536" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk"><strong class="al">Passing Data Directly through LLMs Doesn’t Scale</strong></h1><p id="0900" class="pw-post-body-paragraph mq mr fq ms b mt on mv mw mx oo mz na nb op nd ne nf oq nh ni nj or nl nm nn fj bk">On top of this, another major challenge quickly emerges when operationalizing LLMs for data analysis. Most solutions, such as <a class="af os" href="https://platform.openai.com/docs/assistants/overview" rel="noopener ugc nofollow" target="_blank">Open AI Assistants</a> can generate function calls for the caller to execute to extract data, but the output is <a class="af os" href="https://platform.openai.com/docs/assistants/tools/submitting-functions-outputs" rel="noopener ugc nofollow" target="_blank">then passed back to the LLM</a>. It’s unclear exactly what happens internally at OpenAI, but it’s not very difficult to pass enough data to cause a token limit breach, suggesting the LLM is being used to process the raw data in a prompt. <a class="af os" href="https://python.langchain.com/docs/use_cases/sql/" rel="noopener ugc nofollow" target="_blank">Many patterns</a> do something along these lines, passing the output of function calling back to the LLM. This, of course, does not scale in the real world where data volumes required to answer a question can be large. It soon becomes expensive and often fails.</p><h1 id="cc28" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">LLM Code Generation Can be Slow, Expensive, and Unstable</h1><p id="f85a" class="pw-post-body-paragraph mq mr fq ms b mt on mv mw mx oo mz na nb op nd ne nf oq nh ni nj or nl nm nn fj bk">One way around this is to instead perform the analysis by having the LLM generate the code for the task. For example, if the user asks for a count of records in a dataset, have the LLM generate a snippet of Python to count records in the raw data, execute it, and pass that information back to the user. This requires far fewer tokens compared to passing in the raw data to the LLM.</p><p id="2a0f" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">It is fairly well established that <a class="af os" href="https://evalplus.github.io/leaderboard.html" rel="noopener ugc nofollow" target="_blank">LLMs are pretty good at generating code</a>. Not yet perfect, for sure, but a lot of the world right now is using tools like <a class="af os" href="https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/" rel="noopener ugc nofollow" target="_blank">GitHub Copilot</a> for software development. It is becoming a common pattern in LLM applications to have them generate and execute code as part of solving tasks. <a class="af os" href="https://platform.openai.com/docs/assistants/tools" rel="noopener ugc nofollow" target="_blank">OpenAI’s code interpreter</a> and frameworks such as <a class="af os" href="https://microsoft.github.io/autogen/" rel="noopener ugc nofollow" target="_blank">autogen</a> and Open AI assistants take this a step further in implementing iterative processes that can even debug generated code. Also, the concept of LLMs As Tool Makers (LATM) is established (see for example <a class="af os" href="https://arxiv.org/abs/2305.17126" rel="noopener ugc nofollow" target="_blank">Cai et al, 2023</a>).</p><p id="b34d" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">But here there are some challenges too.</p><p id="7ba6" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Any LLM process generating code, especially if that process goes through an iterative cycle to debug code, can quickly incur significant costs. This is because the best models needed for high-quality code generation are often the most expensive, and to debug code a history of previous attempts is required at each step in an iterative process, burning through tokens. It’s also quite slow, depending on the number of iterations required, leading to a poor user experience.</p><p id="b79d" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">As many of us have also found, code generation is not perfect — yet — and will on occasion fail. Agents can get themselves lost in code debugging loops and though generated code may run as expected, the results may simply be incorrect due to bugs. For most applications, a human still needs to be in the loop.</p><h1 id="8a3e" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Remembering Data ‘Facts’ Has Limitations</h1><p id="fbe4" class="pw-post-body-paragraph mq mr fq ms b mt on mv mw mx oo mz na nb op nd ne nf oq nh ni nj or nl nm nn fj bk">Code generation cost and performance can be improved by implementing some sort of memory where information from previous identical requests can be retrieved, eliminating the requirement for repeat LLM calls. Solutions such as <a class="af os" href="https://memgpt.ai/" rel="noopener ugc nofollow" target="_blank">memgpt</a> work with frameworks like autogen and offer a neat way of doing this.</p><p id="42ae" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Two issues arise from this. First, data is often volatile and any specific answer (ie ‘Fact’) based on data can change over time. If asked today “<em class="no">Which humanitarian organizations are active in the education sector in Afghanistan?”</em>, the answer will likely be different next month. Various memory strategies could be applied to ignore memory after some time, but the most trustworthy method is to simply get the information again.</p><p id="ead9" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Another issue is that our application may have generated an answer for a particular situation, for example, the population of a specific country. The memory will work well if another user asks exactly the same question, but isn’t useful if they ask about a different country. Saving ‘Facts’ is only half of the story if we are hoping to be able to reuse previous LLM responses.</p><h1 id="c5cc" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">So What Can We Do About It?</h1><p id="9249" class="pw-post-body-paragraph mq mr fq ms b mt on mv mw mx oo mz na nb op nd ne nf oq nh ni nj or nl nm nn fj bk">Given all of the above, we have these key issues to solve:</p><ul class=""><li id="51e5" class="mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn pj pb pc bk">We need an approach that would work with databases and APIs</li><li id="e2d8" class="mq mr fq ms b mt pd mv mw mx pe mz na nb pf nd ne nf pg nh ni nj ph nl nm nn pj pb pc bk">We want to be able to support aggregate queries using API data</li><li id="1395" class="mq mr fq ms b mt pd mv mw mx pe mz na nb pf nd ne nf pg nh ni nj ph nl nm nn pj pb pc bk">We want to avoid using LLMs to summarize data and instead use code</li><li id="75ba" class="mq mr fq ms b mt pd mv mw mx pe mz na nb pf nd ne nf pg nh ni nj ph nl nm nn pj pb pc bk">We want to save on costs and performance by using memory</li><li id="537a" class="mq mr fq ms b mt pd mv mw mx pe mz na nb pf nd ne nf pg nh ni nj ph nl nm nn pj pb pc bk">Memory needs to be kept up-to-date with data sources</li><li id="ef01" class="mq mr fq ms b mt pd mv mw mx pe mz na nb pf nd ne nf pg nh ni nj ph nl nm nn pj pb pc bk">Memory should be generalizable, containing <em class="no">skills</em> as well as facts</li><li id="e6c3" class="mq mr fq ms b mt pd mv mw mx pe mz na nb pf nd ne nf pg nh ni nj ph nl nm nn pj pb pc bk">Any code used needs to be reviewed by a human for accuracy and safety</li></ul><p id="f128" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Phew! That’s a lot to ask.</p><h1 id="11af" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Introducing LLM-Assisted Data Recipes</h1></div></div><div class="mf"><div class="ab cb"><div class="la pk lb pl lc pm cf pn cg po ci bh"><figure class="ou ov ow ox oy mf pq pr paragraph-image"><div role="button" tabindex="0" class="mg mh ed mi bh mj"><div class="lx ly pp"><img src="../Images/34da83e07a3674692e24801f11d335e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*t6NLo1alxgMJg3k-sHN6Zw.png"/></div></div><figcaption class="ml mm mn lx ly mo mp bf b bg z dx">Data Recipes architecture: LLM-assisted generation of reusable recipes (skills) which can be used for conversational data analysis</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e335" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The idea is that we split the workflow into two streams to optimize costs and stability, as proposed with the <a class="af os" href="https://arxiv.org/pdf/2305.17126.pdf" rel="noopener ugc nofollow" target="_blank">LATM architecture</a>, with some additional enhancements for managing data and memories specific to Data Recipes …</p><p id="ab67" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><strong class="ms fr">Stream 1: Recipes Assistant</strong></p><p id="9e3f" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">This stream uses LLM agents and more powerful models to generate code snippets (recipes) via a conversational interface. The LLM is instructed with information about data sources — API specifications and Database Schema — so that the person creating recipes can more easily conversationally program new skills. Importantly, the process implements a review stage where generated code and results can be verified and modified by a human before being committed to memory. For best code generation, this stream uses more powerful models and autonomous agents, incurring higher costs per request. However, there is less traffic so costs are controlled.</p><p id="11f3" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><strong class="ms fr">Stream 2: Data Analysis Assistant</strong></p><p id="0466" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">This stream is used by the wider group of end-users who are asking questions about data. The system checks memory to see if their request exists as a fact, e.g. “<em class="no">What’s the population of Mali?</em>”. If not, it checks recipes to see if it has a skill to get the answer, eg ‘<em class="no">How to get the population of any country</em>’. If no memory or skill exists, a request is sent to the recipes assistant queue for the recipe to be added. Ideally, the system can be pre-populated with recipes before launch, but the recipes library can actively grow over time based on user telemetry. Note that the end user stream does not generate code or queries on the fly and therefore can use less powerful LLMs, is more stable and secure, and incurs lower costs.</p><p id="4801" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><strong class="ms fr">Asynchronous Data Refresh</strong></p><p id="b1ae" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">To improve response times for end-users, recipes are refreshed asynchronously where feasible. The recipe memory contains code that can be run on a set schedule. Recipes can be preemptively executed to prepopulate the system, for example, retrieving the total population of all countries before end-users have requested them. Also, cases that require aggregation across large volumes of data extracted from APIs can be run out-of-hours, mitigating —albeit in part— the limitation of aggregate queries using API data.</p><p id="43ed" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><strong class="ms fr">Memory Hierarchy — remembering skills as well as facts</strong></p><p id="1f2f" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The above implements a hierarchy of memory to save ‘facts’ which can be promoted to more general ‘skills’. Memory retrieval promotion to recipes are achieved through a combination of semantic search and LLM reranking and transformation, for example prompting an LLM to generate a general intent and code, eg ‘<em class="no">Get total population for any country</em>’ from a specific intent and code, eg ‘<em class="no">What’s the total population of Mali?</em>’.</p><p id="6c53" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Additionally, by automatically including recipes as available functions to the code generation LLM, its reusable toolkit grows such that new recipes are efficient and call prior recipes rather than generating all code from scratch.</p><h1 id="f729" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Some Additional Benefits of Data Recipes</h1><p id="1569" class="pw-post-body-paragraph mq mr fq ms b mt on mv mw mx oo mz na nb op nd ne nf oq nh ni nj or nl nm nn fj bk">By capturing data analysis requests from users and making these highly visible in the system, transparency is increased. LLM-generated code can be closely scrutinized, optimized, and adjusted, and answers produced by such code are well-understood and reproducible. This acts to reduce the uncertainty many LLM applications face around factual grounding and hallucination.</p><p id="da42" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Another interesting aspect of this architecture is that it captures specific data analysis requirements and the frequency these are requested by users. This can be used to invest in more heavily utilized recipes bringing benefits to end users. For example, if a recipe for generating a humanitarian response situation report is accessed frequently, the recipe code for that report can improved proactively.</p><h1 id="f778" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Data Recipes Hub</h1><p id="e282" class="pw-post-body-paragraph mq mr fq ms b mt on mv mw mx oo mz na nb op nd ne nf oq nh ni nj or nl nm nn fj bk">This approach opens up the possibility of a community-maintained library of data recipes spanning multiple domains — a Data Recipes Hub. Similar to code snippet websites that already exist, it would add the dimension of data as well as help users in creation by providing LLM-assisted conversational programming. Recipes could receive reputation points and other such social platform feedback.</p><figure class="ou ov ow ox oy mf lx ly paragraph-image"><div role="button" tabindex="0" class="mg mh ed mi bh mj"><div class="lx ly lz"><img src="../Images/1ba8da60ad8da37d0fef89193e871d83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bJOXXg9QYvVQj51ZamfJPg.png"/></div></div><figcaption class="ml mm mn lx ly mo mp bf b bg z dx">Data Recipes — code snippets with data, created with LLM assistance — could be contributed by the community to a Data Recipes Hub. Image Source: DALL·E 3</figcaption></figure><h1 id="82f0" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Limitations of Data Recipes</h1><p id="ea5b" class="pw-post-body-paragraph mq mr fq ms b mt on mv mw mx oo mz na nb op nd ne nf oq nh ni nj or nl nm nn fj bk">As with any architecture, it may not work well in all situations. A big part of data recipes is geared towards reducing costs and risks associated with creating code on the fly and instead building a reusable library with more transparency and human-in-the-loop intervention. It will of course be the case that a user can request something new not already supported in the recipe library. We can build a queue for these requests to be processed, and by providing LLM-assisted programming expect development times to be reduced, but there will be a delay to the end-user. However, this is an acceptable trade-off in many situations where it is undesirable to let loose LLM-generated, unmoderated code.</p><p id="b57c" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Another thing to consider is the asynchronous refresh of recipes. Depending on the amount of data required, this may become costly. Also, this refresh might not work well in cases where the source data changes rapidly and users require this information very quickly. In such cases, the recipe would be run every time rather than the result retrieved from memory.</p><p id="cf9f" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The refresh mechanism should help with data aggregation tasks where data is sourced from APIs, but there still looms the fact that the underlying raw data will be ingested as part of the recipe. This of course will not work well for massive data volumes, but it’s at least limiting ingestion based on user demand rather than trying to ingest an entire remote dataset.</p><p id="6df0" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Finally, as with all ‘Chat with Data’ applications, they are only ever going to be as good as the data they have access to. If the desired data doesn’t exist or is of low quality, then perceived performance will be poor. Additionally, common inequity and bias exist in datasets so it’s important a data audit is carried out before presenting insights to the user. This isn’t specific to Data Recipes of course, but one of the biggest challenges posed in operationalizing such techniques. Garbage in, garbage out!</p><h1 id="f877" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Conclusions</h1><p id="e157" class="pw-post-body-paragraph mq mr fq ms b mt on mv mw mx oo mz na nb op nd ne nf oq nh ni nj or nl nm nn fj bk">The proposed architecture aims to address some of the challenges faced with LLM “Chat With Data”, by being …</p><ul class=""><li id="b4d6" class="mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn pj pb pc bk"><strong class="ms fr">Transparent </strong>— Recipes are highly visible and reviewed by a human before being promoted, mitigating issues around LLM hallucination and summarization</li><li id="de65" class="mq mr fq ms b mt pd mv mw mx pe mz na nb pf nd ne nf pg nh ni nj ph nl nm nn pj pb pc bk"><strong class="ms fr">Deterministic</strong> — Being code, they will produce the same results each time, unlike LLM summarization of data</li><li id="4ce9" class="mq mr fq ms b mt pd mv mw mx pe mz na nb pf nd ne nf pg nh ni nj ph nl nm nn pj pb pc bk"><strong class="ms fr">Performant — </strong>Implementing a memory that captures not only facts but skills, which can be refreshed asynchronously, improves response times</li><li id="04bf" class="mq mr fq ms b mt pd mv mw mx pe mz na nb pf nd ne nf pg nh ni nj ph nl nm nn pj pb pc bk"><strong class="ms fr">Inexpensive</strong>— By structuring the workflow into two streams, the high-volume end-user stream can use lower-cost LLMs</li><li id="84c9" class="mq mr fq ms b mt pd mv mw mx pe mz na nb pf nd ne nf pg nh ni nj ph nl nm nn pj pb pc bk"><strong class="ms fr">Secure</strong> — The main group of end-users do not trigger the generation and execution of code or queries on the fly, and any code undergoes human assessment for safety and accuracy</li></ul><p id="f239" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">I will be posting a set of follow-up blog posts detailing the technical implementation of Data Recipes as we work through user testing at <a class="af os" href="https://www.datakind.org/" rel="noopener ugc nofollow" target="_blank">DataKind</a>.</p><h1 id="6f62" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">References</h1><p id="f5d9" class="pw-post-body-paragraph mq mr fq ms b mt on mv mw mx oo mz na nb op nd ne nf oq nh ni nj or nl nm nn fj bk">Large Language Models as Tool Makers, <a class="af os" href="https://arxiv.org/abs/2305.17126" rel="noopener ugc nofollow" target="_blank">Cai et al, 2023</a>.</p><p id="22e0" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Unless otherwise noted, all images are by the author.</p><p id="7192" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><em class="no">Please like this article if inclined and I’d be delighted if you followed me! You can find more articles </em><a class="af os" href="https://medium.com/@astrobagel" rel="noopener"><em class="no">here</em></a><em class="no">.</em></p></div></div></div></div>    
</body>
</html>