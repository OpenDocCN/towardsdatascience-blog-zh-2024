<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Data Quality Error Detection powered by LLMs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Data Quality Error Detection powered by LLMs</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/automated-detection-of-data-quality-issues-54a3cb283a91?source=collection_archive---------0-----------------------#2024-03-22">https://towardsdatascience.com/automated-detection-of-data-quality-issues-54a3cb283a91?source=collection_archive---------0-----------------------#2024-03-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@simon.grah?source=post_page---byline--54a3cb283a91--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Simon Grah" class="l ep by dd de cx" src="../Images/f8fd00600db79bc910ff51e9f64503d0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*CDlge1Xe4sSvIgAbC-1Ttg.png"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--54a3cb283a91--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://medium.com/@simon.grah?source=post_page---byline--54a3cb283a91--------------------------------" rel="noopener follow">Simon Grah</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--54a3cb283a91--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">17 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx kq kr ab q ee ks kt" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="kp"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ko kp">5</span></p></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="ku k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al kv an ao ap hr kw kx ky" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep kz cn"><div class="l ae"><div class="ab cb"><div class="la lb lc ld le lf ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="a1e1" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">This article is the second in a series about cleaning data using Large Language Models (LLMs), with a focus on identifying errors in tabular data sets.</p></div></div></div><div class="ab cb mv mw mx my" role="separator"><span class="mz by bm na nb nc"/><span class="mz by bm na nb nc"/><span class="mz by bm na nb"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="ng nh ni nj nk nl nd ne paragraph-image"><div role="button" tabindex="0" class="nm nn ed no bh np"><div class="nd ne nf"><img src="../Images/7ad60ce10b812d10c4d79cec6c7b7cce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-hqWgzr5vmQZ-SXs5oYpg.png"/></div></div></figure><p id="06a7" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The sketch outlines the methodology we’ll explore in this article, which focuses on evaluating the Data Dirtiness Score of a tabular data set with minimal human involvement.</p><h1 id="4c42" class="nr ns fq bf nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on oo bk">The Data Dirtiness Score</h1><p id="c3a4" class="pw-post-body-paragraph lx ly fq lz b ma op mc md me oq mg mh mi or mk ml mm os mo mp mq ot ms mt mu fj bk">Readers are encouraged to first review the introductory article on the <a class="af ou" href="https://medium.com/p/fe2ca5678d40" rel="noopener">Data Dirtiness Score</a>, which explains the key assumptions and demonstrates how to calculate this score.</p><p id="586b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">As a quick refresher, the <em class="ov">Data Dirtiness Score</em> estimates the expected proportion of cells in a data set that contain errors. Here are the key hypotheses behind this metric:</p><ul class=""><li id="1926" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu ow ox oy bk"><strong class="lz fr">Data errors are related to violated constraints.</strong></li><li id="aae6" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk">If there are <strong class="lz fr">no expectations</strong>, there is <strong class="lz fr">no effect on the score</strong>.</li><li id="c507" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><strong class="lz fr">Data problems</strong> can be <strong class="lz fr">pinpointed to specific cell</strong>s.</li><li id="7935" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk">Each <strong class="lz fr">data error</strong> is assigned a <strong class="lz fr">confidence score</strong>.</li><li id="06a1" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><strong class="lz fr">Every cell has an equal impact</strong> on the overall score.</li></ul><p id="b049" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The initial step in this process involves identifying and cataloguing data inaccuracies present within the data set.</p><h1 id="20f6" class="nr ns fq bf nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on oo bk">The Importance of Detecting Data Quality Issues Automatically</h1><p id="33c6" class="pw-post-body-paragraph lx ly fq lz b ma op mc md me oq mg mh mi or mk ml mm os mo mp mq ot ms mt mu fj bk">Detecting data issues is crucial in the process but challenging due to several factors:</p><ul class=""><li id="0f8e" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu ow ox oy bk"><strong class="lz fr">High Human Labelling Cost</strong>: Identifying data errors often needs significant input from data professionals (like scientists, engineers, and analysts) or subject matter experts (SMEs). This requires a lot of time and is expensive.</li><li id="6e34" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><strong class="lz fr">Lack of Enthusiasm Among Data Practitioners for this Grunt Wor</strong>k: It’s no secret that many in the field view data cleaning as a less appealing aspect of their work. Seen as a precursor to more engaging activities such as modelling, building modern data stacks or answering business queries, data cleaning often falls lower on the priority list, leading to procrastination or, in some cases, completely ignored until critical issues arise.</li><li id="b0f1" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><strong class="lz fr">SME Limitations</strong>: SMEs have valuable knowledge but might lack technical skills like SQL or programming. While no-code and low-code tools help to some extent, they haven’t been fully adopted and might not cover all data management aspects, such as version control.</li><li id="1c4d" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><strong class="lz fr">The Expertise Gap</strong>: Effective data cleaning transcends basic skill sets, requiring specialised expertise. The lack of training and the general disinterest in data preparation mean that many practitioners may only identify superficial errors, missing more complex issues that require a deeper understanding of data cleaning.</li></ul><p id="13b7" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Despite the inherent challenges, advancements in the field of Large Language Models (LLMs) offer promising solutions for automating the identification of straightforward data issues and uncovering more intricate data quality problems.</p><h1 id="f0cd" class="nr ns fq bf nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on oo bk">Data Error Detection powered by LLMs</h1><p id="ec06" class="pw-post-body-paragraph lx ly fq lz b ma op mc md me oq mg mh mi or mk ml mm os mo mp mq ot ms mt mu fj bk">Large language models are becoming invaluable tools in automating the detection of data quality issues, serving as an efficient starting point for a productive human-in-the-loop iterative process. Models, such as those discussed in papers like <a class="af ou" href="https://www.semanticscholar.org/reader/7e17ef56273063dfa838de30b7cc0546b2e5ee10" rel="noopener ugc nofollow" target="_blank">Jellyfish: A Large Language Model for Data Preprocessing</a>, <a class="af ou" href="http://josephorallo.webs.upv.es/escrits/MLJ-DataWranglingAutomation.pdf" rel="noopener ugc nofollow" target="_blank">Can language models automate data wrangling?</a> and <a class="af ou" href="https://arxiv.org/abs/2308.16361" rel="noopener ugc nofollow" target="_blank">Large Language Models as Data Preprocessors</a>, demonstrate their potential to automate constraint generation and data error detection. This automation doesn’t replace human intervention but rather enhances it, allowing for the review and adjustment of automated constraints by either addressing issues directly or modifying confidence scores to reflect the uncertainty inherent in data error detection.</p><p id="2ea7" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">LLMs are particularly well-suited for detecting data quality issues due to their extensive training on a diverse range of internet content, including a vast array of domain knowledge and numerous examples of code reviews related to data quality issues. This training enables LLMs to identify data errors based on textual content without the need for explicitly defined rules. By converting tabular data sets into plain text (called <em class="ov">serialisation</em>), LLMs can scrutinise data much like a team of experienced humans, leveraging their “compressed” internet knowledge to pinpoint errors. This extensive training allows them to identify potential errors in human-readable data sets, such as CSV files, with a level of intuition that mimics human expertise. Moreover, any gaps in domain-specific knowledge can be bridged through techniques like Retrieval-Augmented Generation (RAG) or by tailoring the model’s prompts to the specific nature of the data set.</p><p id="90b9" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Another key advantage of employing LLMs in data error detection is their ability to handle the inherent uncertainty associated with data quality issues. Not all errors are straightforward, and even experts can sometimes disagree on what constitutes a data issue. LLMs can assign confidence scores to their findings, like a human does based on a mix of intuition and experience, reflecting the estimated likelihood of an error.</p><p id="47bf" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The challenge of generalising error detection across diverse data sets and potential issues is considerable. Traditional methods often resort to an extensive set of decision rules or a combination of specialised machine learning models to address various scenarios, such as checking the validity of addresses and phone numbers or anomaly detection. This is where LLMs shine, offering a more adaptable and less labour-intensive alternative. Their ability to understand and identify a wide range of data quality issues without extensive rule-based systems or domain-specific models makes them an invaluable tool. The analogy with the advantages of Machine Learning approaches over traditional business rules or statistical methods is quite intriguing. The adoption of machine learning has been driven by its relative ease of use and adaptability across different use cases, requiring less domain-specific knowledge and time to implement.</p><p id="eff7" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Next, we will demonstrate this approach through a practical example.</p><h1 id="1af2" class="nr ns fq bf nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on oo bk">A Case Study</h1><p id="1cf8" class="pw-post-body-paragraph lx ly fq lz b ma op mc md me oq mg mh mi or mk ml mm os mo mp mq ot ms mt mu fj bk">In the previous article, we explored the concept of the <a class="af ou" href="https://medium.com/p/fe2ca5678d40" rel="noopener">Data Dirtiness Score</a> using a data set example from the book <a class="af ou" href="https://github.com/PacktPublishing/Cleaning-Data-for-Effective-Data-Science" rel="noopener ugc nofollow" target="_blank">Cleaning Data for Effective Data Science</a>. The data set in question is as follows:</p><pre class="ng nh ni nj nk pe pf pg bp ph bb bk"><span id="b202" class="pi ns fq pf b bg pj pk l pl pm">Student#,Last Name,First Name,Favorite Color,Age<br/>1,Johnson,Mia,periwinkle,12<br/>2,Lopez,Liam,blue,green,13<br/>3,Lee,Isabella,,11<br/>4,Fisher,Mason,gray,-1<br/>5,Gupta,Olivia,9,102<br/>6,,Robinson,,Sophia,,blue,,12</span></pre><p id="1763" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Data errors were already pointed out. Now, we want to explore how we can use a Large Language Model, specifically <code class="cx pn po pp pf b">GPT-4</code>, to automatically find these errors. This new method offers a modern way to spot issues in data sets but comes with possible risks such as privacy concerns when using external APIs. However, this can work with any LLMs, not just <code class="cx pn po pp pf b">GPT-4</code>, although the effectiveness might vary depending on the model's capabilities.</p><h1 id="3385" class="nr ns fq bf nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on oo bk">Preliminary Step: Retrieve Table Annotation</h1><p id="f67a" class="pw-post-body-paragraph lx ly fq lz b ma op mc md me oq mg mh mi or mk ml mm os mo mp mq ot ms mt mu fj bk">To assist the model in identifying data inconsistencies, it’s beneficial to provide additional context about the data frame. This is precisely the role of a <a class="af ou" href="https://www.datagalaxy.com/en/blog/what-is-a-data-catalog/" rel="noopener ugc nofollow" target="_blank">data catalog</a>, which, although a broad topic, we will simplify to focus solely on the essential context information that a LLM requires to detect data errors when examining batches of data set rows.</p><p id="e71e" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The key metadata needed includes:</p><ul class=""><li id="84a5" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu ow ox oy bk">An overview of the <strong class="lz fr">table</strong>, including its <strong class="lz fr">description and purpose</strong>.</li><li id="e86b" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk">A clear <strong class="lz fr">understanding</strong> of each <strong class="lz fr">column’s meaning and type</strong>.</li></ul><p id="b72d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Given the frequent absence of data catalogs or reliable documentation in organisations, we’ll explore how to use LLMs to speed up this process. This process is known as <em class="ov">Table Annotation</em>, which involves identifying semantic information about table elements, including columns, their relationships, and the entities within the cells. For further details, refer to sources such as <a class="af ou" href="https://arxiv.org/abs/2306.00745" rel="noopener ugc nofollow" target="_blank">Column Type Annotation using ChatGPT</a>, <a class="af ou" href="https://paperswithcode.com/paper/annotating-columns-with-pre-trained-language" rel="noopener ugc nofollow" target="_blank">Annotating Columns with Pre-trained Language Models</a>, or <a class="af ou" href="https://paperswithcode.com/paper/sotab-the-wdc-schema-org-table-annotation" rel="noopener ugc nofollow" target="_blank">SOTAB: The WDC Schema.org Table Annotation Benchmark</a>.</p><p id="4dfb" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Here’s the prompt I use:</p><pre class="ng nh ni nj nk pe pf pg bp ph bb bk"><span id="4913" class="pi ns fq pf b bg pj pk l pl pm">Analyse the table below and provide schema annotations based on Schema.org standards.<br/><br/>Student#,Last Name,First Name,Favorite Color,Age<br/>1,Johnson,Mia,periwinkle,12<br/>2,Lopez,Liam,blue,green,13<br/>3,Lee,Isabella,,11<br/>4,Fisher,Mason,gray,-1<br/>5,Gupta,Olivia,9,102<br/>6,,Robinson,,Sophia,,blue,,12<br/><br/>Follow these steps:<br/>1. Identify the overall semantic type of the table.<br/>2. Provide a short description of each column.<br/>3. Annotate each column with its semantic type from Schema.org.<br/>4. Determine the most suitable data type for each column (after data cleaning).<br/><br/>Summarise the table schema as follows:<br/>- Table Semantic Type: &lt;type&gt;<br/>- Column: &lt;name&gt;, Description: &lt;description&gt;, Semantic Type: &lt;Schema.org type&gt;, Pandas Type: &lt;Pandas data type&gt;</span></pre><p id="e41a" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In the prompt instructions, I direct the model to analyse the provided table (or an overview of the table) and to suggest annotations following the <a class="af ou" href="https://schema.org/" rel="noopener ugc nofollow" target="_blank">Schema.org</a> standards. Specifically, the output should include:</p><ul class=""><li id="67d3" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu ow ox oy bk">The <strong class="lz fr">table’s semantic type</strong></li><li id="780f" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk">A brief <strong class="lz fr">description</strong> of each <strong class="lz fr">column</strong></li><li id="25fa" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk">The <strong class="lz fr">column’s annotation type</strong> from Schema.org, where applicable</li><li id="db2b" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk">The ideal or best-suited <strong class="lz fr">data types for each column</strong>, regardless of data issues in the provided text serialisation</li></ul><p id="0ba7" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The response is then formatted to provide a clear and structured summary that can be reused as context in subsequent prompts:</p><pre class="ng nh ni nj nk pe pf pg bp ph bb bk"><span id="7469" class="pi ns fq pf b bg pj pk l pl pm">Summarise the table schema as follows:<br/>- Table Semantic Type: &lt;type&gt;<br/>- Column: &lt;name&gt;, Description: &lt;description&gt;, Semantic Type: &lt;Schema.org type&gt;, Pandas Type: &lt;Pandas data type&gt;</span></pre><p id="66bc" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The table is incorporated using a <code class="cx pn po pp pf b">{table}</code> placeholder in the prompt. The typical method involves converting tabular data into text through serialisation, as discussed in <a class="af ou" href="https://arxiv.org/abs/2402.17944?utm_campaign=Data_Elixir&amp;utm_source=Data_Elixir_475" rel="noopener ugc nofollow" target="_blank">Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding -- A Survey</a>.</p><p id="e259" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Here is a sample response from <code class="cx pn po pp pf b">GPT-4</code>:</p><pre class="ng nh ni nj nk pe pf pg bp ph bb bk"><span id="a627" class="pi ns fq pf b bg pj pk l pl pm">- Table Semantic Type: Person<br/>- Column: Student#, Description: Unique identifier for each student, Semantic Type: identifier, Pandas Type: int<br/>- Column: Last Name, Description: Family name of the student, Semantic Type: familyName, Pandas Type: string<br/>- Column: First Name, Description: Given name of the student, Semantic Type: givenName, Pandas Type: string<br/>- Column: Favorite Color, Description: Preferred color of the student, Semantic Type: color (custom), Pandas Type: string (or list if cleaned for multiple values)<br/>- Column: Age, Description: Age of the student, Semantic Type: age, Pandas Type: int (after cleaning invalid entries)</span></pre><p id="d6c9" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Responses may vary slightly but are generally consistent for such a straightforward example. The aim here is to accelerate the initial process rather than fully automate it. Thus, this can be seen as a preliminary draft, which can then be refined with insights from our knowledge and external context from subject matter experts (SMEs).</p><p id="4cf4" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Now, with some context about the table, let’s explore how to automatically identify data quality issues.</p><h1 id="57e3" class="nr ns fq bf nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on oo bk">Sniffing Data Errors with LLMs</h1><p id="1935" class="pw-post-body-paragraph lx ly fq lz b ma op mc md me oq mg mh mi or mk ml mm os mo mp mq ot ms mt mu fj bk">To start, I suggest a prompt that will help identify data quality issues in a given table.</p><pre class="ng nh ni nj nk pe pf pg bp ph bb bk"><span id="8179" class="pi ns fq pf b bg pj pk l pl pm">Task: Analyse the provided table to identify and document data quality issues.<br/><br/>Below are common data quality issues to guide your analysis. However, you may also identify other relevant issues:<br/>- Ingestion errors<br/>- Typecasting issues<br/>- Duplicates<br/>- Date parsing issues<br/>- Character encoding problems<br/>- Missing values<br/>- Typos/spelling mistakes<br/>- Anomalies/outliers<br/>- Conversion errors and inconsistent units<br/>- Privacy concerns (e.g., exposed PII)<br/>- Domain-specific errors (e.g., invalid formats for addresses, phone numbers, emails)<br/><br/>Instructions:<br/>1. Examine silently the table and its metadata.<br/>2. Line by line, identify potential data quality issues without coding.<br/>3. Document each issue, including:<br/>   - Nature and description of the issue<br/>   - Expected correct state<br/>   - Violated constraint<br/>   - Confidence level in your assessment using ordinal categories: `low`, `medium`, `high` and `certain`.<br/>   - Specific location of the issue in the table (use 'None' for table-wide issues): Index and Column names.<br/><br/>Provided Data:<br/><br/>Table: <br/>,Student#,Last Name,First Name,Favorite Color,Age<br/>0,1,Johnson,Mia,periwinkle,12<br/>1,2,Lopez,Liam,blue,green,13<br/>2,3,Lee,Isabella,,11<br/>3,4,Fisher,Mason,gray,-1<br/>4,5,Gupta,Olivia,9,102<br/>5,6,,Robinson,,Sophia,,blue,,12<br/><br/>Metadata:<br/>- Table Semantic Type: Person<br/>- Column: Student#, Description: Unique identifier for each student, Semantic Type: identifier, Pandas Type: int or string<br/>- Column: Last Name, Description: Family name of the student, Semantic Type: familyName, Pandas Type: string<br/>- Column: First Name, Description: Given name of the student, Semantic Type: givenName, Pandas Type: string<br/>- Column: Favorite Color, Description: Preferred color of the student, Semantic Type: color (custom), Pandas Type: string (or list if cleaned for multiple values)<br/>- Column: Age, Description: Age of the student, Semantic Type: age, Pandas Type: int (after cleaning invalid entries)<br/><br/>Detected Data Issues:</span></pre><p id="f8bb" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The initial part of the prompt sets the task’s objective and lists examples of common data issues, such as ingestion errors, duplicates, and privacy concerns, among others. <strong class="lz fr">This list is not exhaustive, and you’re encouraged to add more relevant types based on your table’s context to guide the analysis.</strong></p><p id="48c7" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Next, the prompt details step-by-step instructions following a <a class="af ou" href="https://learnprompting.org/docs/intermediate/chain_of_thought" rel="noopener ugc nofollow" target="_blank">Chain-of-Thoughts</a> approach, ensuring the model methodically analyses the table and its metadata before identifying data issues line by line, mirroring human analysis. This process is meant to be conducted without coding, to maintain simplicity and broad applicability. This is crucial because, although models like <code class="cx pn po pp pf b">GPT-4 </code>with analytics capabilities can perform useful iterative coding sessions, relying solely on textual analysis promotes generalisation.</p><p id="8592" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Upon detecting a potential data issue, the prompt instructs documenting the following details:</p><ul class=""><li id="30e4" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu ow ox oy bk">The <strong class="lz fr">nature and description of the issue</strong></li><li id="4fde" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk">The <strong class="lz fr">expected correct state</strong></li><li id="ffa4" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk">The <strong class="lz fr">violated constraint</strong></li><li id="4b99" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk">A <strong class="lz fr">confidence level</strong> in the assessment using ordinal categories: <code class="cx pn po pp pf b">low</code>, <code class="cx pn po pp pf b">medium</code>, <code class="cx pn po pp pf b">high</code> and <code class="cx pn po pp pf b">certain</code>.</li><li id="211c" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk">The <strong class="lz fr">specific location of the issue</strong> in the table, using ‘None’ for table-wide issues, with Index and Column names for reference.</li></ul><p id="2896" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The table and its metadata are provided within the prompt, with an index added to each row to aid the model in pinpointing the exact locations of errors.</p><blockquote class="pq pr ps"><p id="c4e2" class="lx ly ov lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><em class="fq">For large tables, this prompt can be applied in batches to cover the entire data set, with findings aggregated to identify all data quality issues.</em></p></blockquote><p id="fb50" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Here is an example of the output this prompt can generate, formatted as a report detailing identified data issues, each with a description, expected state, violated constraint, confidence level, and location.</p><pre class="ng nh ni nj nk pe pf pg bp ph bb bk"><span id="bce9" class="pi ns fq pf b bg pj pk l pl pm">1. Issue: Incorrect value format in "Favorite Color" column<br/>   - Nature and Description: The entry "blue,green" in the "Favorite Color" column for the second row appears to contain two colors instead of one, suggesting a possible ingestion or data entry error.<br/>   - Expected Correct State: Each cell under the "Favorite Color" column should contain only one color.<br/>   - Violated Constraint: Single value constraint.<br/>   - Confidence Level: High<br/>   - Specific Location: Index 1, Column "Favorite Color"<br/><br/>2. Issue: Missing value in "Favorite Color" and "Age" columns<br/>   - Nature and Description: The third row has a missing value for "Favorite Color", and the sixth row has missing values for both "Last Name" and "Favorite Color".<br/>   - Expected Correct State: No missing values in any of the columns.<br/>   - Violated Constraint: Non-null constraint.<br/>   - Confidence Level: Certain<br/>   - Specific Location: Index 2, Column "Favorite Color"; Index 5, Columns "Last Name", "Favorite Color"<br/><br/>3. Issue: Negative value in "Age" column<br/>   - Nature and Description: The fourth row has an age of "-1", which is not possible and indicates a data entry error.<br/>   - Expected Correct State: Age values should be positive integers.<br/>   - Violated Constraint: Age value range (greater than 0).<br/>   - Confidence Level: Certain<br/>   - Specific Location: Index 3, Column "Age"<br/><br/>4. Issue: Misplaced values in "Favorite Color" and "Age" columns<br/>   - Nature and Description: In the fifth row, what appears to be an age ("9") is placed in the "Favorite Color" column, and an unlikely age ("102") is in the "Age" column, suggesting a possible shifting or misplacement of values.<br/>   - Expected Correct State: Age should be a reasonable positive integer, and favorite color should be a string denoting a color.<br/>   - Violated Constraint: Data type and value range constraint.<br/>   - Confidence Level: High<br/>   - Specific Location: Index 4, Columns "Favorite Color", "Age"<br/><br/>5. Issue: Inconsistent formatting in row entries<br/>   - Nature and Description: The sixth row contains additional commas, which might indicate incorrect splitting or entry of the data, leading to misalignment of the values with their respective columns.<br/>   - Expected Correct State: Each row should consistently follow the format defined by the column headers without extra delimiters.<br/>   - Violated Constraint: Data format and consistency constraint.<br/>   - Confidence Level: High<br/>   - Specific Location: Index 5, Entire row</span></pre><p id="0823" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The next step is converting these identified issues into a Python object for easier calculation of the <em class="ov">Data Dirtiness Score</em>.</p><h1 id="47b2" class="nr ns fq bf nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on oo bk">Converting Identified Data Issues into the Correct Format</h1><p id="876c" class="pw-post-body-paragraph lx ly fq lz b ma op mc md me oq mg mh mi or mk ml mm os mo mp mq ot ms mt mu fj bk">This section focuses on transforming the previously identified data issues from plain text descriptions into Python objects. These objects should adhere to the structure defined by the <code class="cx pn po pp pf b">DataIssue</code> class:</p><pre class="ng nh ni nj nk pe pf pg bp ph bb bk"><span id="1c11" class="pi ns fq pf b bg pj pk l pl pm">@dataclass<br/>class DataIssue:<br/>    type_of_issue: str<br/>    expectation: str<br/>    constraint_violated: str<br/>    confidence_score: str # `low`, `medium`, `high` or `certain`<br/>    location: List[Tuple]  # Cell positions as (Index, Column). Use None for row/column-wide issues.</span></pre><p id="9cf3" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Here is the prompt I use:</p><pre class="ng nh ni nj nk pe pf pg bp ph bb bk"><span id="f94a" class="pi ns fq pf b bg pj pk l pl pm">Task: Document data table issues using the provided `DataIssue` class.<br/><br/>For each identified issue and its description, document it using the provided `DataIssue` class structure. <br/><br/>```python<br/>from dataclasses import dataclass<br/>from typing import List, Tuple<br/><br/>@dataclass<br/>class DataIssue:<br/>    type_of_issue: str<br/>    expectation: str<br/>    constraint_violated: str<br/>    confidence_score: str # `low`, `medium`, `high` or `certain`<br/>    location: List[Tuple]  # Cell positions as (Index, Column). Use None for row/column-wide issues.<br/><br/>Instructions:<br/>1. Review all identified issues provided and their descriptions silently.<br/>2. For each issue, instantiate it using the provided `DataIssue` class structure. <br/>3. Return only the code.<br/>4. Once the code has been validated, stop generation.<br/><br/>Identified issues:<br/>{issues_found}<br/><br/>Your code here:</span></pre><p id="eac3" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The goal is to generate Python code representing each data issue as a <code class="cx pn po pp pf b">DataIssue</code> object without redundant text. The identified issues are injected into the prompt using the placeholder <code class="cx pn po pp pf b">{issues_found}</code>.</p><p id="e086" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Example output:</p><pre class="ng nh ni nj nk pe pf pg bp ph bb bk"><span id="e8ce" class="pi ns fq pf b bg pj pk l pl pm">issue1 = DataIssue(<br/>    type_of_issue="Incorrect value format",<br/>    expectation="Each cell under 'Favorite Color' should contain only one color",<br/>    constraint_violated="Single value constraint",<br/>    confidence_score="high",<br/>    location=[(1, "Favorite Color")]<br/>)<br/><br/>issue2 = DataIssue(<br/>    type_of_issue="Missing value",<br/>    expectation="No missing values in any columns",<br/>    constraint_violated="Non-null constraint",<br/>    confidence_score="certain",<br/>    location=[(2, "Favorite Color"), (5, "Last Name"), (5, "Favorite Color")]<br/>)<br/><br/>issue3 = DataIssue(<br/>    type_of_issue="Negative value",<br/>    expectation="Age values should be positive integers",<br/>    constraint_violated="Age value range (greater than 0)",<br/>    confidence_score="certain",<br/>    location=[(3, "Age")]<br/>)<br/><br/>issue4 = DataIssue(<br/>    type_of_issue="Misplaced values",<br/>    expectation="Age should be a reasonable positive integer, and favorite color should be a string denoting a color",<br/>    constraint_violated="Data type and value range constraint",<br/>    confidence_score="high",<br/>    location=[(4, "Favorite Color"), (4, "Age")]<br/>)<br/><br/>issue5 = DataIssue(<br/>    type_of_issue="Inconsistent formatting",<br/>    expectation="Each row should consistently follow the format defined by the column headers without extra delimiters",<br/>    constraint_violated="Data format and consistency constraint",<br/>    confidence_score="high",<br/>    location=[(5, None)]  # None indicates entire row issue<br/>)</span></pre><p id="de90" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The final step involves converting the <code class="cx pn po pp pf b">location</code> attribute from lists of tuples to <code class="cx pn po pp pf b">numpy</code> arrays, which is detailed in the appendix.</p><p id="0be1" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">With all elements in place, we can now calculate the <em class="ov">Data Dirtiness Score</em>.</p><h1 id="95b1" class="nr ns fq bf nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on oo bk">Calculation of the Data Dirtiness Score and Comparison with Ground Truth</h1><p id="c794" class="pw-post-body-paragraph lx ly fq lz b ma op mc md me oq mg mh mi or mk ml mm os mo mp mq ot ms mt mu fj bk">Let’s revisit the function from the previous article, <code class="cx pn po pp pf b">compute_data_dirtiness_score</code>, which uses a list of <code class="cx pn po pp pf b">DataIssue</code> objects mentioned earlier.</p><pre class="ng nh ni nj nk pe pf pg bp ph bb bk"><span id="8c64" class="pi ns fq pf b bg pj pk l pl pm">compute_data_dirtiness_score(data_issues)</span></pre><blockquote class="pq pr ps"><p id="f81d" class="lx ly ov lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><em class="fq">Data Dirtiness Score: 28.33%</em></p></blockquote><p id="920f" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Using the <code class="cx pn po pp pf b">GPT-4</code> model, we estimated the score to be around 28% for this sample. This is fairly close to the "ground truth" score of 31.87%.</p><p id="dfa3" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">To understand the discrepancy between these scores, let’s delve into more detailed metrics on data issue detection. In addition to the overall score, we have matrices of cell issue probabilities for both the ground truth and the model’s estimates.</p><p id="5d08" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Below is the ground truth matrix, with columns and indices added for clarity:</p><pre class="ng nh ni nj nk pe pf pg bp ph bb bk"><span id="477f" class="pi ns fq pf b bg pj pk l pl pm">   Student#  Last Name  First Name  Favorite Color   Age<br/>0      0.00        0.0        0.00            0.00  0.00<br/>1      0.00        0.0        0.00            0.75  0.00<br/>2      0.00        0.0        0.00            1.00  0.00<br/>3      0.00        0.0        0.00            0.00  1.00<br/>4      0.00        0.0        0.00            0.75  0.75<br/>5      0.75        1.0        0.75            1.00  0.75</span></pre><p id="a403" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">And here is the matrix of probabilities estimated by the model:</p><pre class="ng nh ni nj nk pe pf pg bp ph bb bk"><span id="a890" class="pi ns fq pf b bg pj pk l pl pm">   Student#  Last Name  First Name  Favorite Color   Age<br/>0       0.0        0.0        0.00          0.0000  0.00<br/>1       0.0        0.0        0.00          0.7500  0.00<br/>2       0.0        0.0        0.00          1.0000  0.00<br/>3       0.0        0.0        0.00          0.0000  1.00<br/>4       0.0        0.0        0.25          0.8125  0.75<br/>5       1.0        1.0        1.00          1.0000  1.00</span></pre><p id="edd8" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Though the matrices appear similar at first glance, we can apply threshold-based metrics such as <code class="cx pn po pp pf b">accuracy</code>, <code class="cx pn po pp pf b">recall</code>, <code class="cx pn po pp pf b">precision</code>, and <code class="cx pn po pp pf b">F1-score</code> to get a clearer picture. These metrics provide a straightforward evaluation of the model's performance by considering a cell problematic if the model's likelihood exceeds 0. Here are the metrics obtained:</p><figure class="ng nh ni nj nk nl nd ne paragraph-image"><div class="nd ne pt"><img src="../Images/42ea3c3fee789daa72ba4af6d4548abe.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*tHCUDgfG2Y56CqNbFXt4mw.png"/></div></figure><p id="0c96" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The model correctly identified 91% of problematic cells (<code class="cx pn po pp pf b">recall</code>), and all of its error predictions were accurate (<code class="cx pn po pp pf b">precision</code>).</p><p id="124d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The model missed one particular issue: “The <code class="cx pn po pp pf b">Favorite Color</code> and <code class="cx pn po pp pf b">First Name</code> fields might be swapped, considering <code class="cx pn po pp pf b">Olivia</code> can be both a name and a colour." This was deemed improbable with a <code class="cx pn po pp pf b">low</code> confidence score, suggesting <code class="cx pn po pp pf b">Olivia</code> is more likely the <code class="cx pn po pp pf b">First Name</code> rather than the <code class="cx pn po pp pf b">Favorite Color</code>. Consequently, even though this potential issue was overlooked, its minimal confidence score lessened its impact on the overall Data Dirtiness Score. This explains why the two scores are relatively close despite this omission.</p><p id="b139" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In summary, this approach, based on large language models (LLMs), offers a method for detecting data quality issues in a data frame. While this method may not yet be fully automated and might need manual adjustments, it’s hoped that it will expedite the detection of data errors and the calculation of the <em class="ov">Data Dirtiness Score</em> for tabular data sets.</p><h1 id="7084" class="nr ns fq bf nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on oo bk">Next Steps and Challenges</h1><p id="9dbc" class="pw-post-body-paragraph lx ly fq lz b ma op mc md me oq mg mh mi or mk ml mm os mo mp mq ot ms mt mu fj bk">I use a two-step process to generate the issues as code. This is done because I have found this adds more stability over a one-in-all solution, i.e. scanning data set and metadatas and outputs data issues directly in right code format. This doesn’t imply it’s impossible, but I’ve chosen to divide this step into two phases to improve robustness for the time being.</p><p id="e56c" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">An issue we face concerns managing large data sets, both in terms of the number of rows and columns. Despite recent advancements, LLMs still face limitations regarding the input context window and the length of generated content. These constraints limit the size of the table that can be serialised into the prompt for analysis and the length of the data issue report produced by the model. How to divide a data frame based on its size and the model’s capabilities is a question that arises.</p><p id="f110" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In certain scenarios, the lack of general context can be problematic, such as when identifying duplicate rows in a database or detecting spelling errors without a broad understanding of the column values. For instance, in cases where duplicates are not straightforward, a common approach is <strong class="lz fr">Entity Matching</strong>. This technique is particularly useful in data cleaning processes and has seen advancements through the use of Large Language Models. Relevant research in this area includes studies like <a class="af ou" href="https://www.semanticscholar.org/paper/Entity-Matching-using-Large-Language-Models-Peeters-Bizer/13c2ae7831c0f1579bc8c6f1a31c9aa8689e24a8" rel="noopener ugc nofollow" target="_blank">Entity Matching using Large Language Models</a> and <a class="af ou" href="https://arxiv.org/abs/2205.09911" rel="noopener ugc nofollow" target="_blank">Can Foundation Models Wrangle Your Data?</a>, along with <a class="af ou" href="https://arxiv.org/abs/2308.16361" rel="noopener ugc nofollow" target="_blank">Large Language Models as Data Preprocessors</a> and <a class="af ou" href="https://www.semanticscholar.org/reader/7e17ef56273063dfa838de30b7cc0546b2e5ee10" rel="noopener ugc nofollow" target="_blank">Jellyfish: A Large Language Model for Data Preprocessing</a>.</p><p id="8ea1" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Ensemble methods in machine learning, which involve combining multiple models, can enhance performance and stability. This approach can be applied by running several LLMs simultaneously to identify issues in a data set. It’s beneficial to vary the prompts and settings for each LLM to ensure a diverse range of insights. Additionally, assigning specific error types, like spelling mistakes, to individual models can make the process more efficient. While this method can lead to more reliable results by dividing the task into smaller parts, it also increases both the cost and the complexity of the software. By gathering all the identified data issues, we can improve our chances of finding errors (increasing recall) but might also identify more false errors (decreasing precision). However, reviewing these identified errors is generally less time-consuming than finding them in the first place.</p><p id="568d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The ability of LLMs to interact directly with databases, similar to the code analysis capability in <code class="cx pn po pp pf b">ChatGPT-4</code>, opens up a wider range of possibilities for detecting data errors. A challenge here is automating this process, as the model may deviate from its intended path without sufficient guidance.</p><p id="9187" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Despite all the challenges, it is already quite promising what we can achieve with such as simple approach. With more work on engineering, I hope we can very soon provide a more robust solution to cover larger data sets and fully automate the detection process.</p><p id="f32c" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The next article will discuss automated data repair or, at the very least, suggest solutions for repair pending validation.</p><h1 id="265b" class="nr ns fq bf nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on oo bk">References</h1><ul class=""><li id="0009" class="lx ly fq lz b ma op mc md me oq mg mh mi or mk ml mm os mo mp mq ot ms mt mu ow ox oy bk"><a class="af ou" href="https://medium.com/p/fe2ca5678d40" rel="noopener">Data Dirtiness Score</a></li><li id="0178" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><a class="af ou" href="https://www.semanticscholar.org/reader/7e17ef56273063dfa838de30b7cc0546b2e5ee10" rel="noopener ugc nofollow" target="_blank">Jellyfish: A Large Language Model for Data Preprocessing</a></li><li id="3003" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><a class="af ou" href="http://josephorallo.webs.upv.es/escrits/MLJ-DataWranglingAutomation.pdf" rel="noopener ugc nofollow" target="_blank">Can language models automate data wrangling?</a></li><li id="f0d8" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><a class="af ou" href="https://arxiv.org/abs/2308.16361" rel="noopener ugc nofollow" target="_blank">Large Language Models as Data Preprocessors</a></li><li id="5c95" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><a class="af ou" href="https://arxiv.org/abs/2306.00745" rel="noopener ugc nofollow" target="_blank">Column Type Annotation using ChatGPT</a></li><li id="1359" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><a class="af ou" href="https://paperswithcode.com/paper/annotating-columns-with-pre-trained-language" rel="noopener ugc nofollow" target="_blank">Annotating Columns with Pre-trained Language Models</a></li><li id="380a" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><a class="af ou" href="https://paperswithcode.com/paper/sotab-the-wdc-schema-org-table-annotation" rel="noopener ugc nofollow" target="_blank">SOTAB: The WDC Schema.org Table Annotation Benchmark</a></li><li id="df2e" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><a class="af ou" href="https://arxiv.org/abs/2402.17944?utm_campaign=Data_Elixir&amp;utm_source=Data_Elixir_475" rel="noopener ugc nofollow" target="_blank">Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding — A Survey</a></li><li id="dde3" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><a class="af ou" href="https://www.semanticscholar.org/paper/Entity-Matching-using-Large-Language-Models-Peeters-Bizer/13c2ae7831c0f1579bc8c6f1a31c9aa8689e24a8" rel="noopener ugc nofollow" target="_blank">Entity Matching using Large Language Models</a></li></ul></div></div></div><div class="ab cb mv mw mx my" role="separator"><span class="mz by bm na nb nc"/><span class="mz by bm na nb nc"/><span class="mz by bm na nb"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="39db" class="nr ns fq bf nt nu pu nw nx ny pv oa ob oc pw oe of og px oi oj ok py om on oo bk">Appendix</h1><p id="94af" class="pw-post-body-paragraph lx ly fq lz b ma op mc md me oq mg mh mi or mk ml mm os mo mp mq ot ms mt mu fj bk">The section explains how to transform the <code class="cx pn po pp pf b">location</code> attribute of a <code class="cx pn po pp pf b">DataIssue</code> object, which comes from a LLM, into a different format. This transformation changes a list of tuples, which represent cell positions, into a <code class="cx pn po pp pf b">numpy</code>array. This array acts as a mask for those cell positions.<br/>Here's a basic example using the <code class="cx pn po pp pf b">Students</code> data set:</p><pre class="ng nh ni nj nk pe pf pg bp ph bb bk"><span id="a1de" class="pi ns fq pf b bg pj pk l pl pm">create_mask_from_list_of_cell_positions(<br/>    shape=dataset_shape,<br/>    list_of_cell_positions=[(4, 'Favorite Color'), (4, 'Age')],<br/>    columns=columns<br/>)</span></pre><pre class="pz pe pf pg bp ph bb bk"><span id="f1d7" class="pi ns fq pf b bg pj pk l pl pm">array([[0, 0, 0, 0, 0],<br/>       [0, 0, 0, 0, 0],<br/>       [0, 0, 0, 0, 0],<br/>       [0, 0, 0, 0, 0],<br/>       [0, 0, 0, 1, 1],<br/>       [0, 0, 0, 0, 0]], dtype=int8)</span></pre><p id="cc57" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Below are the function definitions:</p><pre class="ng nh ni nj nk pe pf pg bp ph bb bk"><span id="208e" class="pi ns fq pf b bg pj pk l pl pm">def validate_cell_position(<br/>    cell_position: Union[<br/>        Tuple[int, int], Tuple[None, int], Tuple[int, None], Tuple[None, None]<br/>    ],<br/>    columns: List[str] = None,<br/>) -&gt; Tuple[int, int]:<br/>    """<br/>    Validate the cell position and convert column names to indices if necessary.<br/>    """<br/>    if not isinstance(cell_position, tuple):<br/>        raise ValueError("Cell position must be a tuple")<br/><br/>    # Convert column name to index if columns are provided<br/>    if isinstance(cell_position[1], str):<br/>        if columns is None:<br/>            raise ValueError(<br/>                "Column names must be provided to create a mask based on column names"<br/>            )<br/>        column_index = columns.index(cell_position[1])<br/>        return (cell_position[0], column_index)<br/><br/>    return cell_position<br/><br/><br/>def set_mask_values(mask: np.ndarray, cell_position: Tuple[int, int]):<br/>    """<br/>    Set values in the mask based on the cell position.<br/>    """<br/>    row_index, col_index = cell_position<br/>    if row_index is None:<br/>        mask[:, col_index] = 1<br/>    elif col_index is None:<br/>        mask[row_index, :] = 1<br/>    else:<br/>        mask[row_index, col_index] = 1<br/><br/><br/>def create_mask_from_list_of_cell_positions(<br/>    shape: Tuple[int, int],<br/>    list_of_cell_positions: List[Tuple],<br/>    columns: List[str] = None,<br/>) -&gt; np.ndarray:<br/>    """<br/>    Create a mask array based on a list of cell positions.<br/>    """<br/>    mask = np.zeros(shape=shape, dtype=np.int8)<br/><br/>    for cell_position in list_of_cell_positions:<br/>        validated_position = validate_cell_position(cell_position, columns)<br/>        set_mask_values(mask, validated_position)<br/><br/>    return mask</span></pre></div></div></div></div>    
</body>
</html>