<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Robust One-Hot Encoding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Robust One-Hot Encoding</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/robust-one-hot-encoding-930b5f8943af?source=collection_archive---------4-----------------------#2024-04-26">https://towardsdatascience.com/robust-one-hot-encoding-930b5f8943af?source=collection_archive---------4-----------------------#2024-04-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="efcf" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Production grade one-hot encoding techniques in Python and R</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@hc.ekne?source=post_page---byline--930b5f8943af--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Hans Christian Ekne" class="l ep by dd de cx" src="../Images/c85483d8b5dd89584b996b321b7f4a45.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*A1zSnPpUFMzO9bmaOOGn6w.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--930b5f8943af--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@hc.ekne?source=post_page---byline--930b5f8943af--------------------------------" rel="noopener follow">Hans Christian Ekne</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--930b5f8943af--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/3b472814a17418f0804ebfbcc8a1e9db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z1bwjCLEH00GH5hcd91lZA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image generated by the author using DALL-E / or Dali?;)</figcaption></figure><p id="8aeb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Have you faced a crash in your machine learning production environments?</p><p id="3687" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It’s not fun, and especially when it comes to issues that could be avoided. One issue that frequently causes problems is one-hot encoding of data. Drawing from my own experience, I’ve learned that many of these issues can largely be avoided by following a few best practices related to one-hot encoding. In this article I will briefly introduce the topic with a few simple examples and share some best practices to ensure stability of your machine learning models.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="c03b" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">One-hot encoding</h1><h2 id="3e9a" class="pc oh fq bf oi pd pe pf ol pg ph pi oo nl pj pk pl np pm pn po nt pp pq pr ps bk">What is one-hot encoding?</h2><p id="843f" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">One-hot encoding is the practice of turning a factor variable that is stored in a column into dummy variables stored over multiple columns and represented as 0s and 1s. A simple example illustrates the concept.</p><p id="06b4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Consider for example this dataset with some numbers and some columns for colours:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="f65f" class="qc oh fq pz b bg qd qe l qf qg">import pandas as pd<br/><br/># Creating the training_data DataFrame in Python<br/>training_data = pd.DataFrame({<br/>    'numerical_1': [1, 2, 3, 4, 5, 6, 7, 8],<br/>    'color_1_': ['black', 'black', 'red', 'green', <br/>                'green', 'black', 'red', 'blue'],<br/>    'color_2_': ['black', 'blue', 'pink', 'purple', <br/>                'black', 'blue', 'pink', 'purple']<br/>})</span></pre><p id="6d98" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Or more visually:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qh"><img src="../Images/de251ce1de486b95f24cf64ab9c324bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*u-oi5VnAa8wnuoiI0-Y8iA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Training data with 3 columns / image by author</figcaption></figure><p id="9db2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The column <code class="cx qi qj qk pz b">color_1_</code>could also be represented like in the table below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ql"><img src="../Images/17f1d9b0fd2a6e5438855cd8aa8dbabf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*C14Y61ad93EcG5xQ6ECcdw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">One-hot encoded representation of “color_1_” / image by author</figcaption></figure><p id="1db5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Changing <code class="cx qi qj qk pz b">color_1_</code> from a one-column compact representation of a categorical variable into a multi-column binary representation is what we call one-hot encoding.</p><h2 id="0664" class="pc oh fq bf oi pd pe pf ol pg ph pi oo nl pj pk pl np pm pn po nt pp pq pr ps bk">Why do we use it?</h2><p id="ad35" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">There are multiple reasons to use one-hot encoding. They could be related to avoiding implicit ordering, improving model performance, or just making the data compatible with various algorithms.</p><p id="f480" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For example, when you encode a categorical variable like colour, into a numerical structure, (e.g. 1 for black, 2 for green, 3 for red) without converting it to dummy variables, a model could mistakenly misinterpret the data to imply an order ( black &lt; green &lt; red) when no such order exists.</p><p id="191f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Also, when training neural nets, it is best practice to normalize the data before sending it into the neural net, and with categorical variables, one-hot encoding can be a good method. Other linear models, like logistic and linear regression assume linear relationships and numerical inputs so for this class of models, one-hot encoding can be a good idea as well.</p><p id="6603" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In addition, the process of doing one-hot encoding forces us to ensure we don’t feed unseen factor levels into our machine learning models.</p><p id="a0e7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Ultimately, one-hot encoding makes it easier for the machine learning models to interpret the data and thus make better predictions.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="4b62" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">The main reasons why one-hot encoding fails</h1><p id="67ee" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">The way we build traditional machine learning models is to first train the models on a “training dataset” — typically a dataset of historic values — and then later we generate predictions on a new dataset, the “inference dataset.” If the columns of the training dataset and the inference dataset don’t match, your machine learning algorithm will usually fail. This is primarily due to either missing or new factor levels in the inference dataset.</p><h2 id="80a4" class="pc oh fq bf oi pd pe pf ol pg ph pi oo nl pj pk pl np pm pn po nt pp pq pr ps bk">The first problem: Missing factors</h2><p id="dd8a" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">For the following examples, assume that you used the dataset above to train your machine learning model. You one-hot encoded the dataset into dummy variables, and your fully transformed training data looks like below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qm"><img src="../Images/3838878fe8c718db9f2226fa542dc159.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FZVITiRYd5VIVJC4CNqM1w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformed training dataset with pd.get_dummies / image by author</figcaption></figure><p id="3f12" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now, let’s introduce the inference dataset, this is what you would use for making predictions. Let’s say it is given like below:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="fb2e" class="qc oh fq pz b bg qd qe l qf qg"># Creating the inference_data DataFrame in Python<br/>inference_data = pd.DataFrame({<br/>    'numerical_1': [11, 12, 13, 14, 15, 16, 17, 18],<br/>    'color_1_': ['black', 'blue', 'black', 'green', <br/>                'green', 'black', 'black', 'blue'],<br/>    'color_2_': ['orange', 'orange', 'black', 'orange', <br/>                'black', 'orange', 'orange', 'orange']<br/>})</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qn"><img src="../Images/9647cc2c91077b979ff0ab9f045c0c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*7FtIFNFnm1NDy5O4QXuN9Q.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Inference data with 3 columns / image by author</figcaption></figure><p id="496b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using a naive one-hot encoding strategy like we used above (<code class="cx qi qj qk pz b">pd.get_dummies</code>)</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="6743" class="qc oh fq pz b bg qd qe l qf qg"># Converting categorical columns in inference_data to <br/># Dummy variables with integers<br/>inference_data_dummies = pd.get_dummies(inference_data, <br/>  columns=['color_1_', 'color_2_']).astype(int)</span></pre><p id="7e45" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This would transform your inference dataset in the same way, and you obtain the dataset below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qo"><img src="../Images/beb8f6bd286cd54bd9ff34350c8cefda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LL8zZxQa0IH6FhN7ddqHjw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformed inference dataset with pd.get_dummies / image by author</figcaption></figure><p id="2084" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Do you notice the problems? The first problem is that the inference dataset is missing the columns:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="2b27" class="qc oh fq pz b bg qd qe l qf qg">missing_colmns =['color_1__red', 'color_2__pink', <br/>  'color_2__blue', 'color_2__purple']</span></pre><p id="2f21" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you ran this in a model trained with the “training dataset” it would usually crash.</p><h2 id="67d4" class="pc oh fq bf oi pd pe pf ol pg ph pi oo nl pj pk pl np pm pn po nt pp pq pr ps bk">The second problem: New factors</h2><p id="3481" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">The other problem that can occur with one-hot encoding is if your inference dataset includes new and unseen factors. Consider again the same datasets as above. If you examine closely, you see that the inference dataset now has a new column: <code class="cx qi qj qk pz b">color_2__orange.</code></p><p id="14ed" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is the opposite problem as previously, and our inference dataset contains new columns which our training dataset didn’t have. This is actually a common occurrence and can happen if one of your factor variables had changes. For example, if the colours above represent colours of a car, and a car producer suddenly started making orange cars, then this data might not be available in the training data, but could nonetheless show up in the inference data. In this case you need a robust way of dealing with the issue.</p><p id="a2d3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One could argue, well why don’t you list all the columns in the transformed training dataset as columns that would be needed for your inference dataset? The problem here is that you often don’t know what factor levels are in the training data upfront.</p><p id="d688" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For example, new levels could be introduced regularly, which could make it difficult to maintain. On top of that comes the process of then matching your inference dataset with the training data, so you would need to check all actual transformed column names that went into the training algorithm, and then match them with the transformed inference dataset. If any columns were missing you would need to insert new columns with 0 values and if you had extra columns, like the <code class="cx qi qj qk pz b">color_2__orange</code> columns above, those would need to be deleted. This is a rather cumbersome way of solving the issue, and thankfully there are better options available.</p><h1 id="4a8d" class="og oh fq bf oi oj qp gq ol om qq gt oo op qr or os ot qs ov ow ox qt oz pa pb bk">The solution</h1><p id="fcd0" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">The solution to this problem is rather straightforward, however many of the packages and libraries that attempt to streamline the process of creating prediction models fail to implement it well. The key lies in having a function or class that is first fitted on the training data, and then use that same instance of the function or class to transform both the training dataset and the inference dataset. Below we explore how this is done using both Python and R.</p><h2 id="17b3" class="pc oh fq bf oi pd pe pf ol pg ph pi oo nl pj pk pl np pm pn po nt pp pq pr ps bk">In Python</h2><p id="7ca6" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">Python is arguably one the best programming language to use for machine learning, largely due to its extensive network of developers and mature package libraries, and its ease of use, which promotes rapid development.</p><p id="ccd7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Regarding the issues related to one-hot encoding we described above, they can be mitigated by using the widely available and tested scikit-learn library, and more specifically the <code class="cx qi qj qk pz b">sklearn.preprocessing.OneHotEncoder</code> class. So, let’s see how we can use that on our training and inference datasets to create a robust one-hot encoding.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="2473" class="qc oh fq pz b bg qd qe l qf qg">from sklearn.preprocessing import OneHotEncoder<br/><br/># Initialize the encoder<br/>enc = OneHotEncoder(handle_unknown='ignore')<br/><br/># Define columns to transform<br/>trans_columns = ['color_1_', 'color_2_']<br/><br/># Fit and transform the data<br/>enc_data = enc.fit_transform(training_data[trans_columns])<br/><br/># Get feature names<br/>feature_names = enc.get_feature_names_out(trans_columns)<br/><br/># Convert to DataFrame<br/>enc_df = pd.DataFrame(enc_data.toarray(), <br/>                          columns=feature_names)<br/><br/># Concatenate with the numerical data<br/>final_df = pd.concat([training_data[['numerical_1']], <br/>                      enc_df], axis=1)</span></pre><p id="76f2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This produces a final <code class="cx qi qj qk pz b">DataFrame</code>of transformed values as shown below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/f305e941d89808c5f5ac9eb6977727f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GFzZ9jT7Dmzlf7Q87uZbhw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformed training dataset with sklearn / image by author</figcaption></figure><p id="a1f2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If we break down the code above, we see that the first step is to initialize the an instance of the encoder class. We use the option <code class="cx qi qj qk pz b">handle_unknown='ignore'</code> so that we avoid issues with unknow values for the columns when we use the encoder to transform on our inference dataset.</p><p id="0988" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After that, we combine a fit and transform action into one step with the <code class="cx qi qj qk pz b">fit_transform</code> method. And finally, we create a new data frame from the encoded data and concatenate it with the rest of the original dataset.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="edcf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now the task remains to use the encoder to transform our inference dataset.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="29a9" class="qc oh fq pz b bg qd qe l qf qg"># Transform inference data<br/>inference_encoded = enc.transform(inference_data[trans_columns])<br/><br/>inference_feature_names = enc.get_feature_names_out(trans_columns)<br/><br/>inference_encoded_df = pd.DataFrame(inference_encoded.toarray(), <br/>                                    columns=inference_feature_names)<br/><br/>final_inference_df = pd.concat([inference_data[['numerical_1']], <br/>                                inference_encoded_df], axis=1)</span></pre><p id="dd63" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Unlike earlier, when we used the naive <code class="cx qi qj qk pz b">pandas.get_dummies</code> ,we now see that our new <code class="cx qi qj qk pz b">final_inference_df</code> dataset has the same columns as our training dataset.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qv"><img src="../Images/cd25ea429be35479ec782fb89827e12e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NjhVlhUw2KhF1aIe4mPoRQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformed Inference dataset with the correct columns / image by author</figcaption></figure></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="9bfe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In addition to what we showed in the code above, the <code class="cx qi qj qk pz b">OneHotEncoder</code> class from <code class="cx qi qj qk pz b">sklearn.preprocessing</code> has a lot of other functionality that can be useful as well.</p><p id="fbd2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For example, it allows you set the <code class="cx qi qj qk pz b">min_frequency</code> and <code class="cx qi qj qk pz b">max_categories</code> options. As its name implies the <code class="cx qi qj qk pz b">min_frequency</code> options allow you to specify the minimum frequency below which a category will be considered infrequent and then grouped together with other infrequent categories, or the <code class="cx qi qj qk pz b">max_categories</code> option which limits the total number of categories. The latter can be especially useful if you don’t want to create too many columns in your training dataset.</p><p id="c929" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For a full overview of the functionality, visit the documentation pages here:</p><div class="qw qx qy qz ra rb"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?source=post_page-----930b5f8943af--------------------------------#sklearn.preprocessing.OneHotEncoder" rel="noopener  ugc nofollow" target="_blank"><div class="rc ab ig"><div class="rd ab co cb re rf"><h2 class="bf fr hw z io rg iq ir rh it iv fp bk">sklearn.preprocessing.OneHotEncoder</h2><div class="ri l"><h3 class="bf b hw z io rg iq ir rh it iv dx">Examples using sklearn.preprocessing.OneHotEncoder: Release Highlights for scikit-learn 1.4 Release Highlights for…</h3></div><div class="rj l"><p class="bf b dy z io rg iq ir rh it iv dx">scikit-learn.org</p></div></div><div class="rk l"><div class="rl l rm rn ro rk rp lr rb"/></div></div></a></div></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="f490" class="pc oh fq bf oi pd pe pf ol pg ph pi oo nl pj pk pl np pm pn po nt pp pq pr ps bk">In R</h2><p id="e088" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">Several of my clients use R for running machine learning models in production — and it has a lot of great features. Before <code class="cx qi qj qk pz b">polars</code> came out for Python, R’s <code class="cx qi qj qk pz b">data.table</code> package was superior to what <code class="cx qi qj qk pz b">pandas</code> could offer in terms of speed and efficiency. However, R doesn’t have access to the same type of production level packages as <code class="cx qi qj qk pz b">scikit-learn</code> for python. (There are a few libraries, but they are not as mature as <code class="cx qi qj qk pz b">scikit-learn</code>.) In addition, while some packages might have the required functionality, they require loads of other packages to run and can introduce dependency conflicts into your code. Consider running the line below in a docker container build with the r-base image:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="660e" class="qc oh fq pz b bg qd qe l qf qg">RUN R -e "install.packages('recipes', dependencies=TRUE, repos='https://cran.rstudio.com/')"</span></pre><p id="483d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It takes forever to install and takes up a lot of space on your container image. Our solution in this case — instead of using functions from a pre-built package like <code class="cx qi qj qk pz b">recipes</code> — is to introduce our own simple function implemented using the <code class="cx qi qj qk pz b">data.table</code> package:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="dd2e" class="qc oh fq pz b bg qd qe l qf qg">library(data.table)<br/><br/>OneHotEncoder &lt;- function() {<br/>  # Local variables<br/>  categories &lt;- list()<br/>  <br/>  # Method to fit data and extract categories<br/>  fit &lt;- function(dt, columns) {<br/>    for (column in columns) {<br/>      categories[[column]] &lt;&lt;- unique(dt[[column]])<br/>    }<br/>  }<br/>  <br/>  # Method to turn columns into factors and <br/>  factorize &lt;- function(dt) {<br/>    for (column_name in names(categories)) {<br/>        set(dt, j = column_name, <br/>        value = factor(dt[[column_name]], <br/>        levels = categories[[column_name]]))<br/>    }<br/>    return(dt)<br/>  }<br/>  <br/>  # Method to transform columns in categories list to <br/>  # dummy variables<br/>  transform &lt;- function(dt) {<br/>    dt = factorize(dt)<br/>    # add row number for joins later<br/>    dt[, rn := .I]<br/>    for (col in names(categories)) {<br/>      print(col)<br/>      # Construct the formula dynamically<br/>      formula_str &lt;- paste("~", col, "- 1")<br/>      formula_obj &lt;- as.formula(formula_str)<br/>      # Create a model model.matrix object<br/>      mm = model.matrix(formula_obj, dt)<br/>      mm_dt &lt;- as.data.table(mm, keep.rownames = "rn")<br/>      mm_dt[, rn := as.integer(rn)]<br/>      <br/>      # Perform a merge based on these row numbers<br/>      dt &lt;- merge(dt, mm_dt, by = "rn", all = TRUE)<br/>      <br/>       # remove the original column<br/>      dt[, (col) := NULL]<br/><br/>      # set any new NAs to 0<br/>      for (ncol in names(mm_dt)) {<br/>        set(dt, which(is.na(dt[[ncol]])), ncol, 0)<br/>      }<br/>    }<br/>    dt[, rn := NULL]<br/>    return(dt)<br/>  }<br/><br/>  # Method to get categories<br/>  get_categories &lt;- function() {<br/>    return(categories)<br/>  }<br/>  <br/>  # Return a list of methods<br/>  list(<br/>    get_categories = get_categories,<br/>    fit = fit,<br/>    transform = transform<br/>  )<br/>}</span></pre><p id="fddd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s go through this function and see how it works on our training and inference datasets. (R is slightly different from Python and instead of using a class, we use a parent function instead, which works in a similar way.)</p><p id="fdf6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">First, we need to create an instance of the function:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="8548" class="qc oh fq pz b bg qd qe l qf qg"> encoder = OneHotEncoder()</span></pre><p id="c0f1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then, just like with the <code class="cx qi qj qk pz b">OneHotEncoder</code> class from sklearn.preprocessing, we also have a fit function inside our <code class="cx qi qj qk pz b">OneHotEncoder</code>. We use the fit function on the training data, supplying both the training dataset and the columns we want to one-hot encode.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="625e" class="qc oh fq pz b bg qd qe l qf qg"># Columns to one-hot encode<br/>fit_columns = c("color_1_", "color_2")<br/># Use the fit method<br/>encoder$fit(dt=training_data, columns=fit_columns)</span></pre><p id="b904" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The fit function simply loops through all the columns we want to use to for training and finds all the unique values each of the columns contain. This list of columns and their potential values is then used in the transform function. We now have a instance of a fitted one-hot encoder function and we can save it for later use using a R <code class="cx qi qj qk pz b">.RDS</code> file.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="e3dc" class="qc oh fq pz b bg qd qe l qf qg">saveRDS(encoder, "~/my_encoder.RDS")</span></pre><p id="06f8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To generate the one-hot encoded dataset we need for training, we run the transform function on the training data:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="a9b9" class="qc oh fq pz b bg qd qe l qf qg">transformed_training_data = encoder$transform(training_data)</span></pre><p id="801d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The transform function is a little bit more complicated than the fit function, and the first thing it does is to convert the supplied columns into factors — using the original unique values of the columns as factor levels. Then, we loop through each of the predictor columns and create <code class="cx qi qj qk pz b">model.matrix</code> objects of the data. These are then added back to the original dataset and the original factor column is removed. We also make sure to set any of the missing values to 0.</p><p id="cf45" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We now get the exact same dataset as before:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/f305e941d89808c5f5ac9eb6977727f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GFzZ9jT7Dmzlf7Q87uZbhw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformed training dataset using R algorithm / image by author</figcaption></figure><p id="5bfa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And finally, when we need to one-hot encode our inference dataset, we then run the same instance of the encoder function on that dataset:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="c126" class="qc oh fq pz b bg qd qe l qf qg">transformed_inference_data = encoder$transform(inference_data)</span></pre><p id="8bb4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This process ensures we have the same columns in our <code class="cx qi qj qk pz b">transformed_inference_data</code> as we do in our <code class="cx qi qj qk pz b">transformed_training_data</code>.</p><h1 id="0427" class="og oh fq bf oi oj qp gq ol om qq gt oo op qr or os ot qs ov ow ox qt oz pa pb bk">Further considerations</h1><p id="7aaf" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">Before we conclude there are a few extra considerations to mention. As with many other things in machine learning there isn’t always an easy answer as to when and how to use a specific technique. Even though it clearly mitigates some issues, new problems can also arise when doing one-hot encoding. Most commonly, these are related to how to deal with high cardinality categorical variables and how to deal with memory issues because of increasing the table size.</p><p id="7600" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In addition, there are alternative coding techniques such as label encoding, embeddings, or target encodings which sometimes could be preferable to one-hot encoding.</p><p id="225c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Each of these topics is rich enough to warrant a dedicated article, so we will leave those for the interested reader to explore further.</p><h1 id="dbf2" class="og oh fq bf oi oj qp gq ol om qq gt oo op qr or os ot qs ov ow ox qt oz pa pb bk">Conclusion</h1><p id="a702" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">We have shown how naive use of one-hot encoding techniques can lead to mistakes and problems with inference data, and we have also seen how to mitigate and resolve those issues using both Python and R. If left unresolved, poor management of one-hot encoding can potentially lead to crashes and problems with your inference, so it is strongly recommended to use more robust techniques—like either sklearn’s <code class="cx qi qj qk pz b">OneHotEncoder</code> or the R function we developed.</p><p id="dd4c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Thanks for reading!</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="04a4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="rq">All the code presented and used in the article can be found in the following Github repo: </em><a class="af rr" href="https://github.com/hcekne/robust_one_hot_encoding" rel="noopener ugc nofollow" target="_blank"><em class="rq">https://github.com/hcekne/robust_one_hot_encoding</em></a></p><p id="a48a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="rq">If you enjoyed reading this article and would like to access more content from me please feel free to connect with me on LinkedIn at </em><a class="af rr" href="https://www.linkedin.com/in/hans-christian-ekne-1760a259/" rel="noopener ugc nofollow" target="_blank"><em class="rq">https://www.linkedin.com/in/hans-christian-ekne-1760a259/</em></a><em class="rq"> or visit my webpage at </em><a class="af rr" href="https://www.ekneconsulting.com/" rel="noopener ugc nofollow" target="_blank"><em class="rq">https://www.ekneconsulting.com/</em></a><em class="rq"> to explore some of the services I offer. Don’t hesitate to reach out via email at hce@ekneconsulting.com</em></p></div></div></div></div>    
</body>
</html>