- en: Towards Mamba State Space Models for Images, Videos and Time Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/towards-mamba-state-space-models-for-images-videos-and-time-series-1e0bfdb5933a?source=collection_archive---------2-----------------------#2024-08-14](https://towardsdatascience.com/towards-mamba-state-space-models-for-images-videos-and-time-series-1e0bfdb5933a?source=collection_archive---------2-----------------------#2024-08-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[üêç Towards Mamba State Space Models for Images, Videos and Time Series](https://towardsdatascience.com/tagged/mamba-image-video-signal)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Part 1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch?source=post_page---byline--1e0bfdb5933a--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page---byline--1e0bfdb5933a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1e0bfdb5933a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1e0bfdb5933a--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page---byline--1e0bfdb5933a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1e0bfdb5933a--------------------------------)
    ¬∑16 min read¬∑Aug 14, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8e699ad3e8e2b18d79f099a7d89788ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Sascha Kirch](https://medium.com/@SaschaKirch)
  prefs: []
  type: TYPE_NORMAL
- en: This is part 1 of my new multi-part series [üêç Towards Mamba State Space Models
    for Images, Videos and Time Series](https://medium.com/@SaschaKirch/list/mamba-state-space-models-for-images-videos-and-timeseries-861ae0ad08fb).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is Mamba all you need? Certainly, people have thought that for a long time of
    the Transformer architecture introduced by A. Vaswani et. al. in [Attention is
    all you need](https://arxiv.org/abs/1706.03762) back in 2017\. And without any
    doubt, the transformer has revolutionized the field of deep learning over and
    over again. Its general-purpose architecture can easily be adapted for various
    data modalities such as text, images, videos and time series and it seems that
    the more compute resources and data you throw at the Transformer, the more performant
    it becomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the Transformer‚Äôs attention mechanism has a major drawback: it is
    of complexity *O(N¬≤)*, meaning it scales quadratically with the sequence length.
    This implies the larger the input sequence, the more compute resources you need,
    making large sequences often unfeasible to work with.'
  prefs: []
  type: TYPE_NORMAL
- en: '‚ùì The question is: can we do better? Is there a way we can reduce the *O(N¬≤)*
    complexity while still being performant? And what would this new architecture
    look like?'
  prefs: []
  type: TYPE_NORMAL
