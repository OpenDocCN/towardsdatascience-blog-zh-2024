<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>On the Programmability of AWS Trainium and Inferentia</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>On the Programmability of AWS Trainium and Inferentia</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/on-the-programmability-of-aws-trainium-and-inferentia-cd455826e26c?source=collection_archive---------7-----------------------#2024-11-01">https://towardsdatascience.com/on-the-programmability-of-aws-trainium-and-inferentia-cd455826e26c?source=collection_archive---------7-----------------------#2024-11-01</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="68bb" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Accelerating AI/ML Model Training with Custom Operators — Part 4</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--cd455826e26c--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--cd455826e26c--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--cd455826e26c--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--cd455826e26c--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 1, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/a73e650c804e6ccb2a840de147054903.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ReIcCWndeJTnS-0U"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@bresia?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Agata Bres</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="b0a2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this post we continue our exploration of the opportunities for runtime optimization of machine learning (ML) workloads through custom operator development. This time, we focus on the tools provided by the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">AWS Neuron SDK</a> for developing and running new kernels on <a class="af nb" href="https://aws.amazon.com/machine-learning/trainium/" rel="noopener ugc nofollow" target="_blank">AWS Trainium</a> and <a class="af nb" href="https://aws.amazon.com/machine-learning/inferentia/" rel="noopener ugc nofollow" target="_blank">AWS Inferentia</a>. With the rapid development of the low-level model components (e.g., <a class="af nb" href="https://en.wikipedia.org/wiki/Attention_(machine_learning)" rel="noopener ugc nofollow" target="_blank">attention layers</a>) driving the AI revolution, the programmability of the accelerators used for training and running ML models is crucial. Dedicated AI chips, in particular, must offer a worthy alternative to the widely used and highly impactful general-purpose GPU (GPGPU) development frameworks, such as <a class="af nb" href="https://developer.nvidia.com/cuda-toolkit" rel="noopener ugc nofollow" target="_blank">CUDA</a> and <a class="af nb" href="https://triton-lang.org/main/index.html" rel="noopener ugc nofollow" target="_blank">Triton</a>.</p><p id="b8ff" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In previous posts (e.g., <a class="af nb" rel="noopener" target="_blank" href="/ai-model-optimization-on-aws-inferentia-and-trainium-cfd48e85d5ac">here</a> and <a class="af nb" rel="noopener" target="_blank" href="/a-first-look-at-aws-trainium-1e0605071970">here</a>) we explored the opportunity for building and running ML models on AWS’s custom-built AI chips using the the dedicated <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/" rel="noopener ugc nofollow" target="_blank">AWS Neuron SDK</a>. In its most recent release of the SDK (version <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html#id8" rel="noopener ugc nofollow" target="_blank">2.20.0</a>), AWS introduced the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/index.html" rel="noopener ugc nofollow" target="_blank">Neuron Kernel Interface (NKI)</a> for developing custom kernels for <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html" rel="noopener ugc nofollow" target="_blank">NeuronCore-v2</a>, the underlying accelerator powering both <a class="af nb" href="https://aws.amazon.com/machine-learning/trainium/" rel="noopener ugc nofollow" target="_blank">Trainium</a> and <a class="af nb" href="https://aws.amazon.com/machine-learning/inferentia/" rel="noopener ugc nofollow" target="_blank">Inferentia2</a>. The NKI interface joins another API that enables <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html" rel="noopener ugc nofollow" target="_blank">NeuronCore-v2</a> programmability, <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/index.html" rel="noopener ugc nofollow" target="_blank">Neuron Custom C++ Operators</a>. In this post we will explore both opportunities and demonstrate them in action.</p><h2 id="fe87" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Disclaimers</h2><p id="89fb" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">Importantly, this post should not be viewed as a substitute for the official <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">AWS Neuron SDK documentation</a>. At the time of this writing the Neuron SDK APIs for custom kernel development is in Beta, and may change by the time you read this. The examples we share are intended for demonstrative purposes, only. We make no claims as to their optimality, robustness, durability, or accuracy. Please do not view our mention of any platforms, tools, APIs, etc., as an endorsement for their use. The best choices for any project depend on the specifics of the use-case at hand and warrant appropriate investigation and analysis.</p><h1 id="b295" class="oy nz fq bf oa oz pa gq oe pb pc gt oi pd pe pf pg ph pi pj pk pl pm pn po pp bk">Developing Custom Kernels for Neuron Cores</h1><p id="8597" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">Although the list of ML models supported by the Neuron SDK is continuously growing, some operations remain either unsupported or implemented suboptimally. By exposing APIs for Neuron kernel customization, the SDK empowers developers to create and/or optimize the low-level operations that they need, greatly increasing the opportunity for running ML workloads on Trainium and Inferentia.</p><p id="4cf0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As discussed in our <a class="af nb" rel="noopener" target="_blank" href="/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12">previous posts</a> in this series, fully leveraging the power of these AI chips requires a detailed understanding of their low-level architecture.</p><h2 id="37c5" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">The Neuron Core Architecture</h2><p id="e06d" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">The NKI documentation includes a <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#" rel="noopener ugc nofollow" target="_blank">dedicated section</a> on the architecture design of NeuronCore-v2 and its implications on custom operator development. Importantly, there are many differences between Neuron cores and their AI accelerator counterparts (e.g., GPUs and TPUs). Optimizing for Neuron cores requires a unique set of strategies and skills.</p><p id="16a2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Similar to other dedicated AI chips, NeuronCore-v2 includes several internal <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#neuroncore-v2-compute-engines" rel="noopener ugc nofollow" target="_blank">acceleration engines</a>, each of which specializes in performing certain types of computations. The engines can be run asynchronously and in parallel. The <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/compiler/index.html" rel="noopener ugc nofollow" target="_blank">Neuron Compiler</a> is responsible for transforming ML models into low-level operations and optimizing the choice of compute engine for each one.</p><p id="6108" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#tensor-engine" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr">Tensor engine</strong></a> specializes in matrix multiplication. The <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#vector-engine" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr">Vector</strong></a> and <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#scalar-engine" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr">Scalar</strong></a> engines both operate on tensors with the Vector engine specializing in reduction operations and the Scalar engine in non-linear functions. <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#gpsimd-engine" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr">GpSimd</strong></a> is a general purpose engine capable of running arbitrary C/C++ programs. Note that while the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/index.html" rel="noopener ugc nofollow" target="_blank">NKI</a> interface exposes access to all four compute engines, <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/index.html" rel="noopener ugc nofollow" target="_blank">custom C++ operators</a> are designed specifically for the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#gpsimd-engine" rel="noopener ugc nofollow" target="_blank">GpSimd</a>.</p><p id="e1dc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">More details on the capabilities of each engine can be found in the architecture documentation. Furthermore, the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/nki.isa.html" rel="noopener ugc nofollow" target="_blank">NKI Instruction Set Architecture (ISA)</a> documentation provides details on the engines on which different low-level operations are run.</p><p id="8796" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Another important aspect of the Neuron chip is its <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#data-movement" rel="noopener ugc nofollow" target="_blank">memory architecture</a>. A Neuron device includes three types of memory, HBM, SBUF, and PSUM. An intimate understanding of the capacities and capabilities of each one is crucial for optimal kernel development.</p><p id="2d5d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Given the architecture overview, you might conclude that Neuron kernel development requires high expertise. While this may be true for creating fully optimized kernels that leverage all the capabilities of the Neuron core, our aim is to demonstrate the accessibility, value, and potential of the Neuron custom kernel APIs — even for non-expert developers.</p><h1 id="3070" class="oy nz fq bf oa oz pa gq oe pb pc gt oi pd pe pf pg ph pi pj pk pl pm pn po pp bk">Custom NKI Kernels</h1><p id="b745" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">The <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/index.html" rel="noopener ugc nofollow" target="_blank">NKI</a> interface is a Python-level API that exposes the use of the Neuron core compute engines and memory resources to ML developers. The <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/getting_started.html" rel="noopener ugc nofollow" target="_blank">NKI Getting Started</a> guide details the setup instructions and provides a soft landing with a simple “hello world” kernel. The <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html" rel="noopener ugc nofollow" target="_blank">NKI Programming Model</a> guide details the three stages of a typical NKI kernel (loading inputs, running operations on the computation engines, and storing outputs) and introduces the NKI Tile and <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html#nki-pm-tile" rel="noopener ugc nofollow" target="_blank">Tile-based operations</a>. The <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/tutorials.html" rel="noopener ugc nofollow" target="_blank">NKI tutorials</a> demonstrate a variety of NKI kernel sample applications, with each one introducing new core NKI APIs and capabilities. Given the presumed optimality of the sample kernels, one possible strategy for developing new kernels could be to 1) identify a sample that is similar to the operation you wish to implement and then 2) use it as a baseline and iteratively refine and adjust it to achieve the specific functionality you require.</p><p id="299b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/index.html#nki-api-reference" rel="noopener ugc nofollow" target="_blank">NKI API Reference Manual</a> details the Python API for kernel development. With a syntax and semantics that are similar to <a class="af nb" href="https://triton-lang.org/main/index.html" rel="noopener ugc nofollow" target="_blank">Triton</a> and <a class="af nb" href="https://numpy.org/doc/stable/" rel="noopener ugc nofollow" target="_blank">NumPy</a>, the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/nki.language.html" rel="noopener ugc nofollow" target="_blank">NKI language</a> definition aims to maximize accessibility and ease of use. However, it is important to note that NKI kernel development is limited to the operations defined in the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/nki.html" rel="noopener ugc nofollow" target="_blank">NKI</a> library, which (as of the time of this writing) are fewer and more constrained than in libraries such as <a class="af nb" href="https://triton-lang.org/main/index.html" rel="noopener ugc nofollow" target="_blank">Triton</a> and <a class="af nb" href="https://numpy.org/doc/stable/" rel="noopener ugc nofollow" target="_blank">NumPy</a>.</p><h2 id="dec8" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Toy Example — A GIOU Kernel</h2><p id="6a77" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">As in our <a class="af nb" rel="noopener" target="_blank" href="/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12">previous posts</a>, we assess the use of NKI by building a custom implementation of the <a class="af nb" href="https://giou.stanford.edu/" rel="noopener ugc nofollow" target="_blank">Generalized Intersection Over Union (GIOU)</a> operation on a pair of batches of input boxes. Since GIOU involves pixel-wise operations, we used the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html#tile-size-considerations" rel="noopener ugc nofollow" target="_blank"><em class="pq">exp </em>kernel</a> from the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html" rel="noopener ugc nofollow" target="_blank">NKI Programming</a> guide as a reference point and incorporated the use of NKI’s <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html#advanced-tensor-indexing" rel="noopener ugc nofollow" target="_blank">advanced tensor indexing</a> in our implementation. To facilitate debugging in a CPU environment, we also added options to run the code using the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.simulate_kernel.html#nki.simulate_kernel" rel="noopener ugc nofollow" target="_blank">nki.simulate_kernel</a> and <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.language.device_print.html" rel="noopener ugc nofollow" target="_blank">nki.language.device_print.html</a> APIs.</p><pre class="ml mm mn mo mp pr ps pt bp pu bb bk"><span id="308a" class="pv nz fq ps b bg pw px l py pz">import torch<br/>import neuronxcc.nki as nki<br/>import neuronxcc.nki.language as nl<br/>import numpy as np<br/><br/>simulate = False<br/><br/>try:<br/>    # if torch libraries are installed assume that we are running on Neuron<br/>    import torch_xla.core.xla_model as xm<br/>    import torch_neuronx<br/>    from torch_neuronx import nki_jit<br/><br/>    device = xm.xla_device()<br/><br/>    # empty implementation <br/>    def debug_print(*args, **kwargs):<br/>        pass<br/>except:<br/>    # if torch libraries are not installed assume that we are running on CPU<br/>    # and program script to use nki simulation<br/>    simulate = True<br/>    nki_jit = nki.trace<br/>    debug_print = nl.device_print<br/>    device = 'cpu'<br/><br/><br/>@nki_jit<br/>def giou_kernel(preds_ptr,<br/>                targets_ptr,<br/>                output_ptr):<br/>    epsilon = 1e-5<br/>    TILE_M = nl.tile_size.pmax  # 128<br/>    TILE_N = nl.tile_size.psum_fmax  # 512<br/>    TILE_N_OUT = TILE_N // 4<br/><br/>    p_1, p_2 = preds_ptr.shape<br/>    t_1, t_2 = targets_ptr.shape<br/>    o_1, o_2 = output_ptr.shape<br/><br/>    #  verify input<br/>    # batch size must be multiple of 128<br/>    assert p_1 % TILE_M == 0<br/>    assert p_1 == t_1<br/>    assert p_1 == o_1<br/>    # num boxes box *4 must be multiple of 512<br/>    assert p_2 % TILE_N == 0<br/>    assert p_2 == t_2<br/>    assert p_2 // 4 == o_2<br/><br/>    num_tiles_m = p_1 // TILE_M<br/>    num_tiles_n = p_2 // TILE_N<br/><br/>    # Generate tensors for advanced indexing<br/>    i_p = nl.arange(TILE_M)[:, None]<br/>    i_f = nl.arange(TILE_N // 4)[None, :]<br/>    i_f_0 = (4 * i_f)<br/>    i_f_1 = (4 * i_f + 1)<br/>    i_f_2 = (4 * i_f + 2)<br/>    i_f_3 = (4 * i_f + 3)<br/><br/>    # Use affine_range to loop over tiles<br/>    for m in nl.affine_range(num_tiles_m):<br/>        for n in nl.affine_range(num_tiles_n):<br/>            # Load input data from HBM<br/>            preds = nl.load(preds_ptr[m * TILE_M:(m + 1) * TILE_M,<br/>                            n * TILE_N:(n + 1) * TILE_N])<br/>            targets = nl.load(targets_ptr[m * TILE_M:(m + 1) * TILE_M,<br/>                              n * TILE_N:(n + 1) * TILE_N])<br/>            debug_print('preds', preds)<br/>            preds_left = preds[i_p, i_f_0]<br/>            preds_top = preds[i_p, i_f_1]<br/>            preds_right = preds[i_p, i_f_2]<br/>            preds_bottom = preds[i_p, i_f_3]<br/><br/>            gt_left = targets[i_p, i_f_0]<br/>            gt_top = targets[i_p, i_f_1]<br/>            gt_right = targets[i_p, i_f_2]<br/>            gt_bottom = targets[i_p, i_f_3]<br/><br/>            # Compute the area of each box<br/>            area1 = (preds_right - preds_left) * (preds_bottom - preds_top)<br/>            area2 = (gt_right - gt_left) * (gt_bottom - gt_top)<br/><br/>            # Compute the intersection<br/>            left = nl.maximum(preds_left, gt_left)<br/>            top = nl.maximum(preds_top, gt_top)<br/>            right = nl.minimum(preds_right, gt_right)<br/>            bottom = nl.minimum(preds_bottom, gt_bottom)<br/><br/>            inter_w = nl.maximum(right - left, 0)<br/>            inter_h = nl.maximum(bottom - top, 0)<br/>            inter_area = inter_w * inter_h<br/><br/>            union_area = area1 + area2 - inter_area<br/><br/>            iou_val = inter_area / nl.maximum(union_area, epsilon)<br/><br/>            # Compute the smallest enclosing box<br/>            enclose_left = nl.minimum(preds_left, gt_left)<br/>            enclose_top = nl.minimum(preds_top, gt_top)<br/>            enclose_right = nl.maximum(preds_right, gt_right)<br/>            enclose_bottom = nl.maximum(preds_bottom, gt_bottom)<br/><br/>            enclose_w = nl.maximum(enclose_right - enclose_left, 0)<br/>            enclose_h = nl.maximum(enclose_bottom - enclose_top, 0)<br/>            enclose_area = enclose_w * enclose_h<br/><br/>            # Compute GIOU<br/>            delta_area = (enclose_area - union_area)<br/>            enclose_area = nl.maximum(enclose_area, epsilon)<br/>            giou = iou_val - delta_area / enclose_area<br/><br/>            # Store results<br/>            nl.store(output_ptr[m * TILE_M:(m + 1) * TILE_M,<br/>                     n * TILE_N_OUT:(n + 1) * TILE_N_OUT],<br/>                     giou)<br/></span></pre><p id="e3c8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To run our GIOU kernel, we generate two batches of random boxes and feed them to our function:</p><pre class="ml mm mn mo mp pr ps pt bp pu bb bk"><span id="7ddf" class="pv nz fq ps b bg pw px l py pz"># generate random data in np<br/>np.random.seed(0)<br/>batch_size = 1024<br/>n_boxes = 256<br/>img_size = 256<br/>boxes = []<br/><br/>for i in range(2):<br/>    # Randomly generate box sizes and positions<br/>    box_sizes = np.random.randint(1, img_size, size=(batch_size,n_boxes,2))<br/>    top_left = np.random.randint(0, img_size-1, size=(batch_size,n_boxes,2))<br/>    bottom_right = np.clip(top_left + box_sizes, 0, img_size - 1)<br/><br/>    # Concatenate top-left and bottom-right coordinates<br/>    rand_boxes = np.concatenate((top_left, bottom_right), axis=2)<br/><br/>    boxes.append(rand_boxes.astype(np.float32))<br/><br/>out = np.empty((batch_size, n_boxes), np.float32)<br/><br/># convert tensors to PyTorch<br/>t_boxes_0 = torch.tensor(boxes[0]).to(device)<br/>t_boxes_1 = torch.tensor(boxes[1]).to(device)<br/>t_out = torch.tensor(out).to(device)<br/><br/>if simulate:<br/>    # the simulation API requires numpy input<br/>    nki.simulate_kernel(giou_kernel, <br/>                        boxes[0].reshape((batch_size, -1)),<br/>                        boxes[1].reshape((batch_size, -1)),<br/>                        out)<br/>else:<br/>    giou_kernel(t_boxes_0.view((batch_size, -1)),<br/>                t_boxes_1.view((batch_size, -1)),<br/>                t_out)<br/><br/></span></pre><p id="69f3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To assess the performance of our NKI kernel, we will compare it with the following naive implementation of GIOU in PyTorch:</p><pre class="ml mm mn mo mp pr ps pt bp pu bb bk"><span id="35b7" class="pv nz fq ps b bg pw px l py pz">def torch_giou(boxes1, boxes2):<br/>    # loosely based on torchvision generalized_box_iou_loss code<br/>    epsilon = 1e-5<br/><br/>    # Compute areas of both sets of boxes<br/>    area1 = (boxes1[...,2]-boxes1[...,0])*(boxes1[...,3]-boxes1[...,1])<br/>    area2 = (boxes2[...,2]-boxes2[...,0])*(boxes2[...,3]-boxes2[...,1])<br/><br/>    # Corners of intersection<br/>    lt = torch.max(boxes1[..., :2], boxes2[..., :2])<br/>    rb = torch.min(boxes1[..., 2:], boxes2[..., 2:])<br/><br/>    # Width and height of intersection<br/>    wh = (rb - lt).clamp(min=0)<br/><br/>    # Area of the intersection<br/>    inter = wh[..., 0] * wh[..., 1]<br/><br/>    # Union of the two boxes<br/>    union = area1 + area2 - inter<br/>    iou = inter / union.clamp(epsilon)<br/><br/>    # Corners of enclosing box<br/>    lti = torch.min(boxes1[..., :2], boxes2[..., :2])<br/>    rbi = torch.max(boxes1[..., 2:], boxes2[..., 2:])<br/><br/>    # Width and height of the enclosing box<br/>    whi = (rbi - lti).clamp(min=0)<br/><br/>    # Area of the enclosing box<br/>    areai = (whi[..., 0] * whi[..., 1]).clamp(epsilon)<br/><br/>    return iou - (areai - union) / areai</span></pre><p id="4fcc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We use the following benchmarking utility to compare the runtime performance of our two functions:</p><pre class="ml mm mn mo mp pr ps pt bp pu bb bk"><span id="f21f" class="pv nz fq ps b bg pw px l py pz">import time<br/>def benchmark(f, warmup_iters=20, ntrials: int = 100):<br/>    def run(*args, **kwargs):<br/>        # warmup<br/>        for _ in range(warmup_iters):<br/>            f(*args, **kwargs)<br/>        start_time = time.time()<br/>        for _ in range(ntrials):<br/>            f(*args, **kwargs)<br/>        end_time = time.time()<br/>        # Calculate average time per iteration<br/>        avg_time = (end_time - start_time) / ntrials<br/>        return avg_time<br/><br/>    return run<br/><br/><br/>avg_time = benchmark(torch_giou)(t_boxes_0, t_boxes_1)<br/>print(f'torch_giou: {avg_time}')<br/><br/>avg_time = benchmark(giou_kernel)(t_boxes_0.view((batch_size, -1)),<br/>                                  t_boxes_1.view((batch_size, -1)),<br/>                                  t_out)<br/>print(f'giou_kernel: {avg_time}')</span></pre><h2 id="a069" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Runtime Environment</h2><p id="47d2" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">We ran our script on an <a class="af nb" href="https://aws.amazon.com/ec2/instance-types/inf2/" rel="noopener ugc nofollow" target="_blank">Amazon EC2 inf2.xlarge</a> instance (containing two <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html#neuroncores-v2-arch" rel="noopener ugc nofollow" target="_blank">Neuron cores</a> and four vCPUs). We used the most recent version of the <a class="af nb" href="https://aws.amazon.com/releasenotes/aws-deep-learning-ami-neuron-ubuntu-22-04/" rel="noopener ugc nofollow" target="_blank">Deep Learning AMI for Neuron</a> available at the time of this writing, “Deep Learning AMI Neuron (Ubuntu 22.04) 20241027”, with <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html#neuron-2-20-1-10-25-2024" rel="noopener ugc nofollow" target="_blank">AWS Neuron 2.20.1</a> and <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/torch-neuronx/introducing-pytorch-2-1.html" rel="noopener ugc nofollow" target="_blank">PyTorch 2.1</a>.</p><h2 id="a9a6" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Results</h2><p id="914b" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">Our custom GIOU kernel demonstrated an average runtime of 0.211 milliseconds compared to 0.293, amounting to a 39% performance boost. Keep in mind that these results are unique to our toy example. Other operators, particularly ones that include matrix multiplications (and utilize the Tensor engine) are likely to exhibit different comparative results.</p><h2 id="8580" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Optimizing NKI Kernel Performance</h2><p id="5b7b" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">The next step in our kernel development — beyond the scope of this post — would to be to analyze the performance of the GIOU kernel using the dedicated <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/neuron_profile_for_nki.html" rel="noopener ugc nofollow" target="_blank">Neuron Profiler</a> in order to identify bottlenecks and optimize our implementation. Please see the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/nki_perf_guide.html#nki-perf-guide" rel="noopener ugc nofollow" target="_blank">NKI performance guide</a> for more details.</p><h1 id="b067" class="oy nz fq bf oa oz pa gq oe pb pc gt oi pd pe pf pg ph pi pj pk pl pm pn po pp bk">Neuron Custom C++ Operators</h1><p id="c8f2" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">The second method for creating a custom Neuron kernel is to build a C++ operator for the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#gpsimd-engine" rel="noopener ugc nofollow" target="_blank">GpSimd engine</a>. This method is described in the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/programming-guide/custom-c%2B%2B-operators-devguide.html#feature-custom-operators-devguide" rel="noopener ugc nofollow" target="_blank">Neuron Custom C++ Operators Developer Guide</a> and demonstrated in the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/tutorials/customop-mlp-training.html#neuronx-customop-mlp-tutorial" rel="noopener ugc nofollow" target="_blank">Neuron Custom C++ Operators in MLP</a> and <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/tutorials/customop-mlp-perf-opt.html#neuronx-customop-mlp-perf" rel="noopener ugc nofollow" target="_blank">Neuron Custom C++ Operators Performance Optimization</a> tutorials.</p><p id="04a2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Neuron Custom C++ Operators presents an opportunity for “kernel fusion” on the GpSimd engine by facilitating the combination of multiple low-level operations into a single kernel execution. This approach can significantly reduce the overhead associated with: 1) loading multiple individual kernels, and 2) transferring data between different memory regions.</p><h2 id="a55f" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Toy Example — A GIOU C++ Kernel</h2><p id="6d4b" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">In the code block below we implement a C++ GIOU operator for Neuron and save it to a file named <em class="pq">giou.cpp</em>. Our kernel uses the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/api-reference-guide/custom-ops-ref-guide.html#tcm-accessor" rel="noopener ugc nofollow" target="_blank">TCM accessor</a> for optimizing memory read and write performance and applies the <em class="pq">multicore </em>setting in order to <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/api-reference-guide/custom-ops-ref-guide.html#using-multiple-gpsimd-cores" rel="noopener ugc nofollow" target="_blank">use all eight of the GpSimd’s internal processors</a>.</p><pre class="ml mm mn mo mp pr ps pt bp pu bb bk"><span id="a8dc" class="pv nz fq ps b bg pw px l py pz">#include &lt;stdint.h&gt;<br/>#include &lt;stdlib.h&gt;<br/>#include &lt;torch/torch.h&gt;<br/>#include &lt;neuron/neuron-utils.hpp&gt;<br/>#include &lt;algorithm&gt;<br/><br/>// input boxes of shape 1024x256x4<br/>// output scores of shape 1024x256<br/>torch::Tensor giou(const torch::Tensor&amp; t_pred, <br/>                   const torch::Tensor&amp; t_target) {<br/>  size_t num_samples = t_pred.sizes()[0];<br/>  size_t num_boxes = t_pred.sizes()[1];<br/>  torch::Tensor t_out = get_dst_tensor();<br/><br/>  // get the number of GpSimd processors (8 in NeuronCoreV2) <br/>  uint32_t cpu_count = get_cpu_count();<br/>  // get index of current processor<br/>  uint32_t cpu_id = get_cpu_id();<br/><br/>  // divide the batch size into 8 partitions <br/>  uint32_t partition = num_samples / cpu_count;<br/><br/>  // use tcm buffers to load and write data<br/>  size_t tcm_in_size = num_boxes*4;<br/>  size_t tcm_out_size = num_boxes;<br/>  float *tcm_pred = (float*)torch::neuron::tcm_malloc(<br/>                                             sizeof(float)*tcm_in_size);<br/>  float *tcm_target = (float*)torch::neuron::tcm_malloc(<br/>                                             sizeof(float)*tcm_in_size);<br/>  float *tcm_output = (float*)torch::neuron::tcm_malloc(<br/>                                             sizeof(float)*tcm_in_size);<br/>  auto t_pred_tcm_acc = t_pred.tcm_accessor();<br/>  auto t_target_tcm_acc = t_target.tcm_accessor();<br/>  auto t_out_tcm_acc = t_out.tcm_accessor();<br/><br/>  // iterate over each of the entries in the partition<br/>  for (size_t i = 0; i &lt; partition; i++) {<br/>    // load the pred and target boxes into local memory<br/>    t_pred_tcm_acc.tensor_to_tcm&lt;float&gt;(tcm_pred,<br/>                                        partition*cpu_id + i*tcm_in_size,<br/>                                        tcm_in_size);<br/>    t_target_tcm_acc.tensor_to_tcm&lt;float&gt;(tcm_target,<br/>                                          partition*cpu_id + i*tcm_in_size,<br/>                                          tcm_in_size);<br/><br/>    // iterate over each of the boxes in the entry<br/>    for (size_t j = 0; j &lt; num_boxes; j++) {<br/>      const float epsilon = 1e-5;<br/>      const float* box1 = &amp;tcm_pred[j * 4];<br/>      const float* box2 = &amp;tcm_target[j * 4];<br/>      // Compute area of each box<br/>      float area1 = (box1[2] - box1[0]) * (box1[3] - box1[1]);<br/>      float area2 = (box2[2] - box2[0]) * (box2[3] - box2[1]);<br/><br/>      // Compute the intersection<br/>      float left = std::max(box1[0], box2[0]);<br/>      float top = std::max(box1[1], box2[1]);<br/>      float right = std::min(box1[2], box2[2]);<br/>      float bottom = std::min(box1[3], box2[3]);<br/><br/>      float inter_w = std::max(right - left, 0.f);<br/>      float inter_h = std::max(bottom - top, 0.f);<br/>      float inter_area = inter_w * inter_h;<br/><br/>      // Compute the union area<br/>      float union_area = area1 + area2 - inter_area;<br/><br/>      // IoU<br/>      float iou_val = inter_area / std::max(union_area, epsilon);<br/><br/>      // Compute the smallest enclosing box<br/>      float enclose_left = std::min(box1[0], box2[0]);<br/>      float enclose_top = std::min(box1[1], box2[1]);<br/>      float enclose_right = std::max(box1[2], box2[2]);<br/>      float enclose_bottom = std::max(box1[3], box2[3]);<br/><br/>      float enclose_w = std::max(enclose_right - enclose_left, 0.f);<br/>      float enclose_h = std::max(enclose_bottom - enclose_top, 0.f);<br/>      float enclose_area = std::max(enclose_w * enclose_h, epsilon);<br/><br/>      float result = iou_val - (enclose_area-union_area)/enclose_area;<br/>      tcm_output[j] = result;<br/>    }<br/><br/>    // write the giou scores of all boxes in the current entry<br/>    t_out_tcm_acc.tcm_to_tensor&lt;float&gt;(tcm_output,<br/>                                       partition*cpu_id + i*tcm_out_size,<br/>                                       tcm_out_size);<br/>  }<br/><br/>  torch::neuron::tcm_free(tcm_pred);<br/>  torch::neuron::tcm_free(tcm_target);<br/>  return t_out;<br/>}</span></pre><p id="fb45" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We require a separate <em class="pq">shape.cpp</em> file that defines the output shape of our GIOU function and registers our custom operator with the Neuron library:</p><pre class="ml mm mn mo mp pr ps pt bp pu bb bk"><span id="fb9b" class="pv nz fq ps b bg pw px l py pz">#include &lt;stdint.h&gt;<br/>#include &lt;stdlib.h&gt;<br/>#include &lt;torch/torch.h&gt;<br/>#include "torchneuron/register.h"<br/><br/>torch::Tensor giou_shape(torch::Tensor boxes1, torch::Tensor boxes2) {<br/>    torch::Tensor t_out = torch::zeros({boxes1.sizes()[0],<br/>                                        boxes1.sizes()[1]},<br/>                                       torch::kFloat);<br/>    return t_out;<br/>}<br/><br/>NEURON_LIBRARY(my_ops, m) {<br/>  m.def("giou", &amp;giou_shape, "giou");<br/>}</span></pre><p id="5c33" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <em class="pq">build.py</em> script compiles the C++ operator and exposes it as a Python API:</p><pre class="ml mm mn mo mp pr ps pt bp pu bb bk"><span id="bc91" class="pv nz fq ps b bg pw px l py pz">import os<br/>import torch_neuronx<br/>from torch_neuronx.xla_impl import custom_op<br/><br/>custom_op.load(<br/>    name='giou',<br/>    compute_srcs=['giou.cpp'],<br/>    shape_srcs=['shape.cpp'],<br/>    build_directory=os.getcwd(),<br/>    multicore=True,<br/>    verbose=True<br/>)</span></pre><p id="79db" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The compilation script generates a <em class="pq">libgiou.so </em>library containing the implementation of our C++ GIOU operator. In the code block below we load the library and measure the performance of our custom kernel using the benchmarking utility defined above:</p><pre class="ml mm mn mo mp pr ps pt bp pu bb bk"><span id="3355" class="pv nz fq ps b bg pw px l py pz">from torch_neuronx.xla_impl import custom_op<br/>custom_op.load_library('libgiou.so')<br/><br/>avg_time = benchmark(torch.ops.my_ops.giou)(t_boxes_0, t_boxes_1)<br/>print(f'C++ giou: {avg_time}')</span></pre><h2 id="e854" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Runtime Environment</h2><p id="ec33" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">We used the same Neuron environment from our NKI experiments to compile and test our C++ kernel. Please note the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-customops/programming-guide/custom-c%2B%2B-operators-devguide.html#setup-installation" rel="noopener ugc nofollow" target="_blank">installation steps</a> that are required for custom C++ operator development.</p><h2 id="43f2" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Results</h2><p id="4a32" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">Our C++ GIOU kernel demonstrated an average runtime of 0.061 milliseconds — nearly five times faster than our baseline implementation. This is presumably a result of “kernel fusion”, as discussed above.</p><h1 id="c361" class="oy nz fq bf oa oz pa gq oe pb pc gt oi pd pe pf pg ph pi pj pk pl pm pn po pp bk">Conclusion</h1><p id="5adc" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">The table below summarizes the runtime results of our experiments.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qa"><img src="../Images/a4a93747c3b62268e577bf26e1df984e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZoi5AOxknDmLej-WZXyrA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Avg time of different GIOU implementations (lower is better) — by Author</figcaption></figure><p id="ee85" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Please keep in mind that these results are specific to the toy example and runtime environment used in this study. The comparative results of other kernels might be very different — depending on the degree to which they can leverage the Neuron core’s internal compute engines.</p><p id="5ad2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The table below summarizes some of the differences we observed between the two methods of AWS Neuron kernel customization.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qb"><img src="../Images/301bdafd4bab9a99c908d04758d1ef91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QRr0righrG1q5E4pIuHMdw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Comparison between kernel customization tools (by Author)</figcaption></figure><p id="316f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Through its high-level Python interface, the NKI APIs expose the power of the Neuron acceleration engines to ML developers in an accessible and user-friendly manner. The low-level C++ Custom Operators library enables even greater programmability, but is limited to the <a class="af nb" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/trainium_inferentia2_arch.html#gpsimd-engine" rel="noopener ugc nofollow" target="_blank">GpSimd engine</a>. By effectively combining both tools, developers can fully leverage the AWS Neuron architecture’s capabilities.</p><h1 id="28eb" class="oy nz fq bf oa oz pa gq oe pb pc gt oi pd pe pf pg ph pi pj pk pl pm pn po pp bk">Summary</h1><p id="d6d9" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">With the AI revolution in full swing, many companies are developing advanced new AI chips to meet the growing demand for compute. While public announcements often highlight these chips’ runtime performance, cost savings, and energy efficiency, several core capabilities are essential to making these chips and their software stacks truly viable for ML development. These capabilities include robust debugging tools, performance analysis and optimization utilities, <strong class="ne fr">programmability</strong>, and more.</p><p id="e38e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this post, we focused on the utilities available for programming AWS’s homegrown AI accelerators, <a class="af nb" href="https://aws.amazon.com/machine-learning/trainium/" rel="noopener ugc nofollow" target="_blank">Trainium</a> and <a class="af nb" href="https://aws.amazon.com/machine-learning/inferentia/" rel="noopener ugc nofollow" target="_blank">Inferentia</a>, and demonstrated their use in building custom ML operations. These tools empower developers to optimize the performance of their ML models on AWS’s AI chips and open up new opportunities for innovation and creativity.</p></div></div></div></div>    
</body>
</html>