- en: 'Transformers: How Do They Transform Your Data?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/transformers-how-do-they-transform-your-data-72d69e383e0d?source=collection_archive---------0-----------------------#2024-03-28](https://towardsdatascience.com/transformers-how-do-they-transform-your-data-72d69e383e0d?source=collection_archive---------0-----------------------#2024-03-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Diving into the Transformers architecture and what makes them unbeatable at
    language tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)[![Maxime
    Wolf](../Images/259b3659d0e6dd1d0f0eec4ae92d02e9.png)](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)
    [Maxime Wolf](https://medium.com/@maxwolf34?source=post_page---byline--72d69e383e0d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--72d69e383e0d--------------------------------)
    ¬∑11 min read¬∑Mar 28, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4dd0f4cd464558d94b73f8274580ac94.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'In the rapidly evolving landscape of artificial intelligence and machine learning,
    one innovation stands out for its profound impact on how we process, understand,
    and generate data: **Transformers**. Transformers have revolutionized the field
    of natural language processing (NLP) and beyond, powering some of today‚Äôs most
    advanced AI applications. But what exactly are Transformers, and how do they manage
    to transform data in such groundbreaking ways? This article demystifies the inner
    workings of Transformer models, focusing on the **encoder architecture**. We will
    start by going through the implementation of a Transformer encoder in Python,
    breaking down its main components. Then, we will visualize how Transformers process
    and adapt input data during training.'
  prefs: []
  type: TYPE_NORMAL
- en: While this blog doesn‚Äôt cover every architectural detail, it provides an implementation
    and an overall understanding of the transformative power of Transformers. For
    an in-depth explanation of Transformers, I suggest you look at the excellent Stanford
    CS224-n course.
  prefs: []
  type: TYPE_NORMAL
- en: I also recommend following the [GitHub repository](https://github.com/maxime7770/Transformers-Insights)
    associated with this article for additional details. üòä
  prefs: []
  type: TYPE_NORMAL
- en: What is a Transformer encoder architecture?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/df39c5bc0e96c04388b637bb391a7fed.png)'
  prefs: []
  type: TYPE_IMG
- en: The Transformer model from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: This picture shows the original Transformer architecture, combining an encoder
    and a decoder for sequence-to-sequence language tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we will focus on the encoder architecture (the red block on
    the picture). This is what the popular BERT model is using under the hood: the
    primary focus is on **understanding and representing the data**, rather than generating
    sequences. It can be used for a variety of applications: text classification,
    named-entity recognition (NER), extractive question answering, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: So, how is the data actually transformed by this architecture? We will explain
    each component in detail, but here is an overview of the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input text is **tokenized**: the Python string is transformed into a list
    of tokens (numbers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each token is passed through an **Embedding layer** that outputs a vector representation
    for each token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The embeddings are then further encoded with a **Positional Encoding layer**,
    adding information about the position of each token in the sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These new embeddings are transformed by a series of **Encoder Layers**, using
    a self-attention mechanism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **task-specific head** can be added. For example, we will later use a classification
    head to classify movie reviews as positive or negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That is important to understand that the Transformer architecture transforms
    the embedding vectors by mapping them from one representation in a high-dimensional
    space to another within the same space, applying a series of complex transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an encoder architecture in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Positional Encoder layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike RNN models, the attention mechanism makes no use of the order of the
    input sequence. The PositionalEncoder class adds positional encodings to the input
    embeddings, using two mathematical functions: cosine and sine.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/578ef5784eef3551c213f00d3bfdbdd4.png)'
  prefs: []
  type: TYPE_IMG
- en: Positional encoding matrix definition from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that positional encodings don‚Äôt contain trainable parameters: there are
    the results of deterministic computations, which makes this method very tractable.
    Also, sine and cosine functions take values between -1 and 1 and have useful periodicity
    properties to help the model learn patterns about the **relative positions of
    words**.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Multi-Head Self-Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The self-attention mechanism is the key component of the encoder architecture.
    Let‚Äôs ignore the ‚Äúmulti-head‚Äù for now. Attention is a way to determine for each
    token (i.e. each embedding) the **relevance of all other embeddings to that token**,
    to obtain a more refined and contextually relevant encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cd6d84be3aae29b2a4108ac244d7dd6b.png)'
  prefs: []
  type: TYPE_IMG
- en: How does‚Äúit‚Äù pay attention to other words of the sequence? ([The Illustrated
    Transformer](https://jalammar.github.io/illustrated-transformer/))
  prefs: []
  type: TYPE_NORMAL
- en: There are 3 steps in the self-attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Use matrices Q, K, and V to respectively transform the inputs ‚Äú**query**‚Äù, ‚Äú**key**‚Äù
    and ‚Äú**value**‚Äù. Note that for self-attention, query, key, and values are all
    equal to our input embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the attention score using cosine similarity (a dot product) between
    the **query** and the **key**. Scores are scaled by the square root of the embedding
    dimension to stabilize the gradients during training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a softmax layer to make these scores **probabilities**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output is the weighted average of the **values**, using the attention scores
    as the weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathematically, this corresponds to the following formula.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c5773d753ad9db7bab34c7e7739de18c.png)'
  prefs: []
  type: TYPE_IMG
- en: The Attention Mechanism from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: What does ‚Äúmulti-head‚Äù mean? Basically, we can apply the described self-attention
    mechanism process several times, in parallel, and concatenate and project the
    outputs. This allows each head to f**ocus on different semantic aspects of the
    sentence**.
  prefs: []
  type: TYPE_NORMAL
- en: We start by defining the number of heads, the dimension of the embeddings (d_model),
    and the dimension of each head (head_dim). We also initialize the Q, K, and V
    matrices (linear layers), and the final projection layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When using multi-head attention, we apply each attention head with a reduced
    dimension (head_dim instead of d_model) as in the original paper, making the total
    computational cost similar to a one-head attention layer with full dimensionality.
    Note this is a logical split only. What makes multi-attention so powerful is it
    can still be represented via a single matrix operation, making computations very
    efficient on GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We compute the attention scores and use a mask to avoid using attention on padded
    tokens. We apply a softmax activation to make these scores probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The forward attribute performs the multi-head logical split and computes the
    attention weights. Then, we get the output by multiplying these weights by the
    values. Finally, we reshape the output and project it with a linear layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The Encoder Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the main component of the architecture, which leverages multi-head self-attention.
    We first implement a simple class to perform a feed-forward operation through
    2 dense layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can now code the logic for the encoder layer. We start by applying self-attention
    to the input, which gives a vector of the same dimension. We then use our mini
    feed-forward network with Layer Norm layers. Note that we also use skip connections
    before applying normalization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Putting Everything Together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It‚Äôs time to create our final model. We pass our data through an embedding layer.
    This transforms our raw tokens (integers) into a numerical vector. We then apply
    our positional encoder and several (num_layers) encoder layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We also create a ClassifierHead class which is used to transform the final embedding
    into class probabilities for our classification task.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that the dense and softmax layers are only applied on the first embedding
    (corresponding to the first token of our input sequence). This is because when
    tokenizing the text, the first token is the [CLS] token which stands for ‚Äúclassification.‚Äù
    The [CLS] token is designed to aggregate the entire sequence‚Äôs information into
    a single embedding vector, serving as a summary representation that can be used
    for classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: the concept of including a [CLS] token originates from BERT, which was
    initially trained on tasks like next-sentence prediction. The [CLS] token was
    inserted to predict the likelihood that sentence B follows sentence A, with a
    [SEP] token separating the 2 sentences. For our model, the [SEP] token simply
    marks the end of the input sentence, as shown below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3feb19e839643803e928bfc05828ad2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[CLS] Token in BERT Architecture ([All About AI](https://seunghan96.github.io/dl/nlp/28.-nlp-BERT-%EC%9D%B4%EB%A1%A0/))'
  prefs: []
  type: TYPE_NORMAL
- en: When you think about it, it‚Äôs really mind-blowing that this single [CLS] embedding
    is able to capture so much information about the entire sequence, thanks to the
    self-attention mechanism‚Äôs ability to weigh and synthesize the importance of every
    piece of the text in relation to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Training and visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully, the previous section gives you a better understanding of how our
    Transformer model transforms the input data. We will now write our training pipeline
    for our binary classification task using the IMDB dataset (movie reviews). Then,
    we will visualize the embedding of the [CLS] token during the training process
    to see how our model transformed it.
  prefs: []
  type: TYPE_NORMAL
- en: We first define our hyperparameters, as well as a BERT tokenizer. In the GitHub
    repository, you can see that I also coded a function to select a subset of the
    dataset with only 1200 train and 200 test examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'You can try to use the BERT tokenizer on one of the sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Every sequence should start with the token 101, corresponding to [CLS], followed
    by some non-zero integers and padded with zeros if the sequence length is smaller
    than 256\. Note that these zeros are ignored during the self-attention computation
    using our ‚Äúmask‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now write our train function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You can find the collect_embeddings and visualize_embeddings functions in the
    GitHub repo. They store the [CLS] token embedding for each sentence of the training
    set, apply a dimensionality reduction technique called t-SNE to make them 2D vectors
    (instead of 256-dimensional vectors), and save an animated plot.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs visualize the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23987d3afe279c76504d4c31a47e8066.png)'
  prefs: []
  type: TYPE_IMG
- en: Projected [CLS] embeddings for each training point (blue corresponds to positive
    sentences, red corresponds to negative sentences)
  prefs: []
  type: TYPE_NORMAL
- en: Observing the plot of projected [CLS] embeddings for each training point, we
    can see the clear distinction between positive (blue) and negative (red) sentences
    after a few epochs. This visual shows the remarkable capability of the Transformer
    architecture to adapt embeddings over time and highlights the power of the self-attention
    mechanism. The data is transformed in such a way that embeddings for each class
    are well separated, thereby significantly simplifying the task for the classifier
    head.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we conclude our exploration of the Transformer architecture, it‚Äôs evident
    that these models are adept at tailoring data to a given task. With the use of
    positional encoding and multi-head self-attention, Transformers go beyond mere
    data processing: they interpret and understand information with a level of sophistication
    previously unseen. The ability to dynamically weigh the relevance of different
    parts of the input data allows for a more nuanced understanding and representation
    of the input text. This enhances performance across a wide array of downstream
    tasks, including text classification, question answering, named entity recognition,
    and more.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a better understanding of the encoder architecture, you are
    ready to delve into decoder and encoder-decoder models, which are very similar
    to what we have just explored. Decoders play a pivotal role in generative tasks
    and are at the core of the popular GPT models.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to connect on [LinkedIn](https://www.linkedin.com/in/maxime-wolf/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Follow me on [GitHub](https://github.com/maxime7770) for more content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Visit my website: [maximewolf.com](http://maximewolf.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Vaswani, Ashish, et al. ‚ÄúAttention Is All You Need.‚Äù *31st Conference on
    Neural Information Processing Systems (NIPS 2017)*, Long Beach, CA, USA.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] ‚ÄúThe Illustrated Transformer.‚Äù *Jay Alammar‚Äôs Blog*, June 2018, [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Official PyTorch Implementation of the Transformer Architecture. *GitHub
    repository*, PyTorch, [https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Manning, Christopher, et al. ‚ÄúCS224n: Natural Language Processing with
    Deep Learning.‚Äù *Stanford University*, Stanford CS224N NLP course, [http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)'
  prefs: []
  type: TYPE_NORMAL
