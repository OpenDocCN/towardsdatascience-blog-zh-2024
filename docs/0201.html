<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How does ReLU enable Neural Networks to approximate continuous nonlinear functions?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How does ReLU enable Neural Networks to approximate continuous nonlinear functions?</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-relu-enables-neural-networks-to-approximate-continuous-nonlinear-functions-f171b7859727?source=collection_archive---------1-----------------------#2024-01-21">https://towardsdatascience.com/how-relu-enables-neural-networks-to-approximate-continuous-nonlinear-functions-f171b7859727?source=collection_archive---------1-----------------------#2024-01-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="75c6" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Learn how neural networks with one hidden layer using ReLU activation represent continuous nonlinear functions.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@lamthuy.lt?source=post_page---byline--f171b7859727--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Thi-Lam-Thuy LE" class="l ep by dd de cx" src="../Images/a5af515421fe186f5ad47c532932af03.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*BaD3PgeE33MDdqJQE4YwEw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--f171b7859727--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@lamthuy.lt?source=post_page---byline--f171b7859727--------------------------------" rel="noopener follow">Thi-Lam-Thuy LE</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--f171b7859727--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">5 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">7</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="1be8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Activation functions play an integral role in Neural Networks <em class="nf">(NNs)</em> since they introduce non-linearity that allows the network to learn more complex features and functions than just a linear regression. One of the most commonly used activation functions is Rectified Linear Unit <em class="nf">(ReLU),</em> which has been <a class="af ng" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank">theoretically</a> shown to enable NNs to approximate a wide range of continuous functions, making them powerful function approximators.</p><p id="7680" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this post, we study in particular the approximation of Continuous NonLinear <em class="nf">(CNL)</em> functions, the main purpose of using a NN over a simple linear regression model. More precisely, we investigate 2 sub-categories of CNL functions: Continuous PieceWise Linear <em class="nf">(CPWL)</em>, and Continuous Curve <em class="nf">(CC)</em> functions. We will show how these two function types can be represented using a NN that consists of one hidden layer, given enough neurons with ReLU activation.</p><p id="3dbb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For illustrative purposes, we consider only single feature inputs yet the idea applies to multiple feature inputs as well.</p><h1 id="a53d" class="nh ni fq bf nj nk nl gq nm nn no gt np nq nr ns nt nu nv nw nx ny nz oa ob oc bk">ReLU activation</h1><figure class="og oh oi oj ok ol od oe paragraph-image"><div class="od oe of"><img src="../Images/e249461664cadb9fb803d9a1a41537b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*ZIkzRJNrGr_CW-tdwdMPVw.png"/></div><figcaption class="on oo op od oe oq or bf b bg z dx">Figure 1: Rectified Linear Unit (ReLU) function.</figcaption></figure><p id="fa2d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">ReLU is a piecewise linear function that consists of two linear pieces: one that cuts off negative values where the output is zero, and one that provides a continuous linear mapping for non negative values.</p><h1 id="36e4" class="nh ni fq bf nj nk nl gq nm nn no gt np nq nr ns nt nu nv nw nx ny nz oa ob oc bk">Continuous piecewise linear function approximation</h1><p id="8775" class="pw-post-body-paragraph mj mk fq ml b go os mn mo gr ot mq mr ms ou mu mv mw ov my mz na ow nc nd ne fj bk"><em class="nf">CPWL functions are continuous functions with multiple linear portions. The slope is consistent on each portion, then changes abruptly at transition points by adding new linear functions.</em></p><figure class="og oh oi oj ok ol od oe paragraph-image"><div class="od oe ox"><img src="../Images/f83d489627298162c04ec44a6e727c2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*EQvZp_DlBvSusRNzSld3kQ.png"/></div><figcaption class="on oo op od oe oq or bf b bg z dx">Figure 2: Example of CPWL function approximation using NN. At each transition point, a new ReLU function is added to/subtracted from the input to increase/decrease the slope.</figcaption></figure><p id="1085" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In a NN with one hidden layer using ReLU activation and a linear output layer, the activation outputs are aggregated to form the CPWL target function. Each unit of the hidden layer is responsible for a linear piece. At each unit, a new ReLU function that corresponds to the changing of slope is added to produce the new slope <em class="nf">(cf. Fig.2)</em>. Since this activation function is always positive, the weights of the output layer corresponding to units that increase the slope will be positive, and conversely, the weights corresponding to units that decreases the slope will be negative <em class="nf">(cf. Fig.3)</em>. The new function is added at the transition point and does not contribute to the resulting function prior to (and sometimes after) that point due to the disabling range of the ReLU activation function.</p><figure class="og oh oi oj ok ol od oe paragraph-image"><div class="od oe oy"><img src="../Images/625c25d0d2b613b371adb52150ba4fd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*Idrd2TC0GYacIBl3yINMJQ.png"/></div><figcaption class="on oo op od oe oq or bf b bg z dx">Figure 3: Approximation of the CPWL target function in Fig.2 using a NN that consists of one hidden layer with ReLU activation and a linear output layer.</figcaption></figure><p id="59f4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Example</strong></p><p id="68f9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To make it more concrete, we consider an example of a CPWL function that consists of 4 linear segments defined as below.</p><figure class="og oh oi oj ok ol od oe paragraph-image"><div class="od oe oz"><img src="../Images/55c82ec41548f85c2cc586fc300cac56.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*XOFrHCUKUrcHI-avX41y7A.png"/></div><figcaption class="on oo op od oe oq or bf b bg z dx">Figure 4: Example of a PWL function.</figcaption></figure><p id="fc85" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To represent this target function, we will use a NN with 1 hidden layer of 4 units and a linear layer that outputs the weighted sum of the previous layer’s activation outputs. Let’s determine the network’s parameters so that each unit in the hidden layer represents a segment of the target. For the sake of this example, the bias of the output layer <em class="nf">(b2_0)</em> is set to 0.</p><figure class="og oh oi oj ok ol od oe paragraph-image"><div class="od oe ch"><img src="../Images/980a1377e98898a948078d0ac3d8018f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*SZtFtPxEG99ZZzzCLYDRmQ.png"/></div><figcaption class="on oo op od oe oq or bf b bg z dx">Figure 5: The network architecture to model the PWL function defined in Fig.4.</figcaption></figure><figure class="og oh oi oj ok ol od oe paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="od oe pa"><img src="../Images/c8c5123073a3fe737f0b292152ad1d8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QuwhlWIMqlYzNFr8PGxVUA.png"/></div></div></figure><figure class="og oh oi oj ok ol od oe paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="od oe pf"><img src="../Images/c2dc3ff0289cbb5110ed968fc1cfebb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*uwUsDgJYLSpz78hLX0ZN8g.png"/></div></div><figcaption class="on oo op od oe oq or bf b bg z dx">Figure 6: The activation output of unit 0 (a1_0).</figcaption></figure><figure class="og oh oi oj ok ol od oe paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="od oe pg"><img src="../Images/479c32a0674d6f52eecae6d71b0cfc19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kwNrzqzdFd54l9vWu_JiUg.png"/></div></div></figure><figure class="og oh oi oj ok ol od oe paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="od oe pf"><img src="../Images/af866141263ed92ed63c95f0222a2544.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*a8QO-JTIm_mOz-4QTyUQoQ.png"/></div></div><figcaption class="on oo op od oe oq or bf b bg z dx">Figure 7: The activation output of unit 1 (a1_1), which is aggregated to the output<em class="ph"> (a2_0)</em> to produce the segment (2). The red arrow represents the change in slope.</figcaption></figure><figure class="og oh oi oj ok ol od oe paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="od oe pi"><img src="../Images/2cc99c9616fd854e28c2c64dc2db9bdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mn11CqSrebw6elpmNR9Srg.png"/></div></div></figure><figure class="og oh oi oj ok ol od oe paragraph-image"><div class="od oe pj"><img src="../Images/c2c7c57e436713ba9a76c8380b1c3680.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*v1EsNc_6WlU1-ldtqwYrJA.png"/></div><figcaption class="on oo op od oe oq or bf b bg z dx">Figure 8: The output of unit 2 (a1_2), which is aggregated to the output (a2_0) to produce the segment (3). The red arrow represents the change in slope.</figcaption></figure><figure class="og oh oi oj ok ol od oe paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="od oe pk"><img src="../Images/cc95f6c47f81f48362d89b9f2456ade4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HXFaas--mInTWdkC4SFSyw.png"/></div></div></figure><figure class="og oh oi oj ok ol od oe paragraph-image"><div class="od oe pf"><img src="../Images/eb8a7102340e818e0dc2fc793b9e6c16.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*ivv2BCQhhdRkd1YoLDQT-Q.png"/></div><figcaption class="on oo op od oe oq or bf b bg z dx">Figure 9: The output of unit 3 (a1_3), which is aggregated to the output (a2_0) to produce the segment (4). The red arrow represents the change in slope.</figcaption></figure><h1 id="13c7" class="nh ni fq bf nj nk nl gq nm nn no gt np nq nr ns nt nu nv nw nx ny nz oa ob oc bk">Continuous curve function approximation</h1><p id="15e1" class="pw-post-body-paragraph mj mk fq ml b go os mn mo gr ot mq mr ms ou mu mv mw ov my mz na ow nc nd ne fj bk">The next type of continuous nonlinear function that we will study is CC function. There is not a proper definition for this sub-category, but <em class="nf">an informal way to define CC functions is continuous nonlinear functions that are not piecewise linear. Several examples of CC functions are: quadratic function, exponential function, sinus function, etc.</em></p><p id="5d9a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A CC function can be approximated by a series of infinitesimal linear pieces, which is called a piecewise linear approximation of the function. The greater the number of linear pieces and the smaller the size of each segment, the better the approximation is to the target function. Thus, the same network architecture as previously with a large enough number of hidden units can yield good approximation for a curve function.</p><p id="b565" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, in reality, the network is trained to fit a given dataset where the input-output mapping function is unknown. An architecture with too many neurons is prone to overfitting, high variance, and requires more time to train. Therefore, an appropriate number of hidden units should be large enough to properly fit the data and at the same time, small enough to avoid overfitting. Moreover, with a limited number of neurons, a good approximation with low loss has more transition points in restricted domain, rather than equidistant transition points in an uniform sampling way (as shown in <em class="nf">Fig.10</em>).</p><figure class="og oh oi oj ok ol od oe paragraph-image"><div class="od oe pl"><img src="../Images/259fc3f5bef997908f75399fc78918ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*cYhG8D-AT7bu8wgEMRcxlw.png"/></div><figcaption class="on oo op od oe oq or bf b bg z dx">Figure 10: Two piecewise linear approximations for a continuous curve function (in dashed line). The approximation 1 has more transition points in restricted domain and model the target function better than the approximation 2.</figcaption></figure><h1 id="0b57" class="nh ni fq bf nj nk nl gq nm nn no gt np nq nr ns nt nu nv nw nx ny nz oa ob oc bk">Wrap up</h1><p id="8536" class="pw-post-body-paragraph mj mk fq ml b go os mn mo gr ot mq mr ms ou mu mv mw ov my mz na ow nc nd ne fj bk">In this post, we have studied how ReLU activation function allows multiple units to contribute to the resulting function without interfering, thus enables continuous nonlinear function approximation. In addition, we have discussed about the choice of network architecture and number of hidden units in order to obtain a good approximation result.</p><p id="a3b4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="nf">I hope that this post is useful for your Machine Learning learning process!</em></p><p id="7985" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Further questions to think about:</strong></p><ol class=""><li id="f0cd" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pm pn po bk">How does the approximation ability change if the number of hidden layers with ReLU activation increases?</li><li id="3858" class="mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne pm pn po bk">How ReLU activations are used for a classification problem?</li></ol><p id="015f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">*Unless otherwise noted, all images are by the author</p></div></div></div></div>    
</body>
</html>