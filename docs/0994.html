<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Meta Llama 3 Optimized CPU Inference with Hugging Face and PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Meta Llama 3 Optimized CPU Inference with Hugging Face and PyTorch</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/meta-llama-3-optimized-cpu-inference-with-hugging-face-and-pytorch-9dde2926be5c?source=collection_archive---------2-----------------------#2024-04-19">https://towardsdatascience.com/meta-llama-3-optimized-cpu-inference-with-hugging-face-and-pytorch-9dde2926be5c?source=collection_archive---------2-----------------------#2024-04-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/2d381d40cdfb97cc7c50ca03b4b947a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8mngqkmC_kUdvCXZ9nIKlA.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Created with Nightcafe — Image property of Author</figcaption></figure><div/><div><h2 id="9e82" class="pw-subtitle-paragraph hh gj gk bf b hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw cq dx">Learn how to reduce model latency when deploying Meta* Llama 3 on CPUs</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hx hy hz ia ib ab"><div><div class="ab ic"><div><div class="bm" aria-hidden="false"><a href="https://eduand-alvarez.medium.com/?source=post_page---byline--9dde2926be5c--------------------------------" rel="noopener follow"><div class="l id ie by if ig"><div class="l ed"><img alt="Eduardo Alvarez" class="l ep by dd de cx" src="../Images/8a51c754fdd3362aa82dee5acd2a68c5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*QseVWgYrBJ1R4Ych9FEaQw.png"/><div class="ih by l dd de em n ii eo"/></div></div></a></div></div><div class="ij ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--9dde2926be5c--------------------------------" rel="noopener follow"><div class="l ik il by if im"><div class="l ed"><img alt="Towards Data Science" class="l ep by br in cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ih by l br in em n ii eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="io ab q"><div class="ab q ip"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b iq ir bk"><a class="af ag ah ai aj ak al am an ao ap aq ar is" data-testid="authorName" href="https://eduand-alvarez.medium.com/?source=post_page---byline--9dde2926be5c--------------------------------" rel="noopener follow">Eduardo Alvarez</a></p></div></div></div><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b iq ir dx"><button class="iv iw ah ai aj ak al am an ao ap aq ar ix iy iz" disabled="">Follow</button></p></div></div></span></div></div><div class="l ja"><span class="bf b bg z dx"><div class="ab cn jb jc jd"><div class="je jf ab"><div class="bf b bg z dx ab jg"><span class="jh l ja">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar is ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--9dde2926be5c--------------------------------" rel="noopener follow"><p class="bf b bg z ji jj jk jl jm jn jo jp bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="jq jr l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="h k w ea eb q"><div class="kx l"><div class="ab q ky kz"><div class="pw-multi-vote-icon ed jh la lb lc"><div class=""><div class="ld le lf lg lh li lj am lk ll lm lc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ln lo lp lq lr ls lt"><p class="bf b dy z dx"><span class="le">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ld lw lx ab q ee ly lz" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lv"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lu lv">2</span></p></button></div></div></div><div class="ab q ki kj kk kl km kn ko kp kq kr ks kt ku kv kw"><div class="ma k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mb an ao ap ix mc md me" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mf cn"><div class="l ae"><div class="ab cb"><div class="mg mh mi mj mk gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="aa99" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The much-anticipated <a class="af ny" href="https://huggingface.co/blog/llama3" rel="noopener ugc nofollow" target="_blank">release</a> of Meta’s third-generation batch of Llama is here, and I want to ensure you know how to deploy this state-of-the-art (SoTA) LLM optimally. In this tutorial, we will focus on performing weight-only-quantization (WOQ) to compress the 8B parameter model and improve inference latency, but first, let’s discuss Meta Llama 3.</p><h1 id="60b2" class="nz oa gk bf ob oc od hk oe of og hn oh oi oj ok ol om on oo op oq or os ot ou bk">Llama 3</h1><p id="33de" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">To date, the Llama 3 family includes models ranging from 8B to 70B parameters, with more versions coming in the future. The models come with a permissive Meta Llama 3 <a class="af ny" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/blob/main/LICENSE" rel="noopener ugc nofollow" target="_blank">license</a>, you are encouraged to review before accepting the terms required to use them. This marks an exciting chapter for the Llama model family and open-source AI.</p><h2 id="ab96" class="pa oa gk bf ob pb pc pd oe pe pf pg oh nl ph pi pj np pk pl pm nt pn po pp pq bk">Architecture</h2><p id="70b2" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">The Llama 3 is an auto-regressive LLM based on a decoder-only transformer. Compared to Llama 2, the Meta team has made the following notable improvements:</p><ul class=""><li id="1828" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk">Adoption of grouped query attention (GQA), which improves inference efficiency.</li><li id="f418" class="nc nd gk ne b hi pu ng nh hl pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Optimized tokenizer with a vocabulary of 128K tokens designed to encode language more efficiently.</li><li id="8a26" class="nc nd gk ne b hi pu ng nh hl pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Trained on a 15 trillion token dataset, this is 7x larger than Llama 2’s training dataset and includes 4x more code.</li></ul><p id="53ef" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The figure below (Figure 1) is the result of <code class="cx pz qa qb qc b">print(model)</code> where <code class="cx pz qa qb qc b">model</code> is <a class="af ny" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" rel="noopener ugc nofollow" target="_blank">meta-llama/Meta-Llama-3–8B-Instruct</a>. In this figure, we can see that the model comprises 32 LlamaDecoderLayers composed of Llama Attention self-attention components. Additionally, it has LlamaMLP, LlamaRMSNorm, and a Linear head. We hope to learn more once the Llama 3 research paper is released.</p><figure class="qe qf qg qh qi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qd"><img src="../Images/c802b721da904eefda4260786011cf54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NqxEdTub3iJTw4ipT6Jg8Q.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 1. Output of `print(model)` showcasing the distribution of layers across llama-3–8B-instruct’s architecture — Image by Author</figcaption></figure><h2 id="a57b" class="pa oa gk bf ob pb pc pd oe pe pf pg oh nl ph pi pj np pk pl pm nt pn po pp pq bk">Language Modeling Performance</h2><p id="0de2" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">The model was evaluated on various industry-standard language modeling benchmarks, such as MMLU, GPQA, HumanEval, GSM-8K, MATH, and more. For the purpose of this tutorial, we will review the performance of the “Instruction Tuned Models” (Figure 2). The most remarkable aspect of these figures is that the Llama 3 8B parameter model outperforms Llama 2 70B by 62% to 143% across the reported benchmarks while being an 88% smaller model!</p><figure class="qe qf qg qh qi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qd"><img src="../Images/803024fc44c78d908f4d7fb793e79fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OL1IH76N9yOpoeRLv2BTwA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 2 . Summary of Llama 3 instruction model performance metrics across the MMLU, GPQA, HumanEval, GSM-8K, and MATH LLM benchmarks. — Image by Author (<a class="af ny" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="018c" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The increased language modeling performance, permissive licensing, and architectural efficiencies included with this latest Llama generation mark the beginning of a very exciting chapter in the generative AI space. Let’s explore how we can optimize inference on CPUs for scalable, low-latency deployments of Llama 3.</p><h1 id="5b90" class="nz oa gk bf ob oc od hk oe of og hn oh oi oj ok ol om on oo op oq or os ot ou bk">Optimizing Llama 3 Inference with PyTorch</h1><p id="fd53" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">In a <a class="af ny" href="https://medium.com/towards-data-science/improving-llm-inference-latency-on-cpus-with-model-quantization-28aefb495657" rel="noopener">previous article</a>, I covered the importance of model compression and overall inference optimization in developing LLM-based applications. In this tutorial, we will focus on applying weight-only quantization (WOQ) to <a class="af ny" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" rel="noopener ugc nofollow" target="_blank">meta-llama/Meta-Llama-3–8B-Instruct</a>. WOQ offers a balance between performance, latency, and accuracy, with options to quantize to int4 or int8. A key component of WOQ is the dequantization step, which converts int4/in8 weights back to bf16 before computation.</p><figure class="qe qf qg qh qi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qj"><img src="../Images/f3035f2396f92ac8f2fbe9af4294e0d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RH6ou7jL5Fw9KwGG.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Fig 3. Simple illustration of weight-only quantization, with pre-quantized weights in orange and the quantized weights in green. Note that this depicts the initial quantization to int4/int8 and dequantization to fp16/bf16 for the computation step. — Image by Author (<a class="af ny" href="https://medium.com/towards-data-science/improving-llm-inference-latency-on-cpus-with-model-quantization-28aefb495657" rel="noopener">source</a>)</figcaption></figure><h2 id="ce15" class="pa oa gk bf ob pb pc pd oe pe pf pg oh nl ph pi pj np pk pl pm nt pn po pp pq bk">Environment Setup</h2><p id="138b" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">You will need approximately 60GB of RAM to perform WOQ on Llama-3-8B-Instruct. This includes ~30GB to load the full model and ~30GB for peak memory during quantization. The WOQ Llama 3 will only consume ~10GB of RAM, meaning we can free ~50GB of RAM by releasing the full model from memory.</p><blockquote class="qk ql qm"><p id="2044" class="nc nd qn ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="gk">You can run this tutorial on the </em><a class="af ny" href="https://medium.com/r?url=https%3A%2F%2Fbit.ly%2Fllama3woq" rel="noopener"><em class="gk">Intel</em>®<em class="gk"> Tiber</em>®<em class="gk"> Developer Cloud</em></a><em class="gk"> free JupyterLab* environment. This environment offers a 4th Generation Intel</em>®<em class="gk"> Xeon</em>®<em class="gk"> CPU with 224 threads and 504 GB of memory, more than enough to run this code.</em></p></blockquote><p id="234d" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If running this in your own IDE, you may need to address additional dependencies like installing Jupyter and/or configuring a conda/python environment. Before getting started, ensure that you have the following dependencies installed.</p><pre class="qe qf qg qh qi qo qc qp bp qq bb bk"><span id="d233" class="qr oa gk qc b bg qs qt l qu qv">intel-extension-for-pytorch==2.2<br/>transformers==4.35.2<br/>torch==2.2.0<br/>huggingface_hub</span></pre><h2 id="369c" class="pa oa gk bf ob pb pc pd oe pe pf pg oh nl ph pi pj np pk pl pm nt pn po pp pq bk">Accessing and Configuring Llama 3</h2><p id="d4c9" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">You will need a Hugging Face* account to access Llama 3’s model and tokenizer.</p><p id="f309" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To do so, select “Access Tokens” from your settings menu (Figure 4) and create a token.</p><figure class="qe qf qg qh qi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qw"><img src="../Images/b621f7448cbda9124a39953850909676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nC0ovX4kgCkxqQTZeeHXXA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 4. Snapshot of the Hugging Face token configuration console — Image by Author</figcaption></figure><p id="42d6" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Copy your access token and paste it into the “Token” field generated inside your Jupyter cell after running the following code.</p><pre class="qe qf qg qh qi qo qc qp bp qq bb bk"><span id="6269" class="qr oa gk qc b bg qs qt l qu qv">from huggingface_hub import notebook_login, Repository<br/><br/># Login to Hugging Face<br/>notebook_login()</span></pre><p id="e9a6" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Go to <a class="af ny" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" rel="noopener ugc nofollow" target="_blank">meta-llama/Meta-Llama-3–8B-Instruct</a> and carefully evaluate the terms and license before providing your information and submitting the Llama 3 access request. Accepting the model’s terms and providing your information is yours and yours alone.</p><h2 id="8892" class="pa oa gk bf ob pb pc pd oe pe pf pg oh nl ph pi pj np pk pl pm nt pn po pp pq bk">Quantizing Llama-3–8B-Instruct with WOQ</h2><p id="8fa0" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">We will leverage the <a class="af ny" href="https://medium.com/r?url=https%3A%2F%2Fwww.intel.com%2Fcontent%2Fwww%2Fus%2Fen%2Fdeveloper%2Ftools%2Foneapi%2Foptimization-for-pytorch.html" rel="noopener">Intel® Extension for PyTorch</a>* to apply WOQ to Llama 3. This extension contains the latest PyTorch optimizations for Intel hardware. Follow these steps to quantize and perform inference with an optimized Llama 3 model:</p><ol class=""><li id="86e5" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qx ps pt bk"><strong class="ne gl">Llama 3 Model and Tokenizer:</strong> Import the required packages and use the <code class="cx pz qa qb qc b">AutoModelForCausalLM.from_pretrained()</code> and <code class="cx pz qa qb qc b">AutoTokenizer.from_pretrained()</code> methods to load the Llama-3–8B-Instruct specific weights and tokenizer.</li></ol><pre class="qe qf qg qh qi qo qc qp bp qq bb bk"><span id="dd56" class="qr oa gk qc b bg qs qt l qu qv">import torch<br/>import intel_extension_for_pytorch as ipex<br/>from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer<br/><br/>Model = 'meta-llama/Meta-Llama-3-8B-Instruct'<br/><br/>model = AutoModelForCausalLM.from_pretrained(Model)<br/>tokenizer = AutoTokenizer.from_pretrained(Model)</span></pre><p id="4e69" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. <strong class="ne gl">Quantization Recipe Config:</strong> Configure the WOQ quantization recipe. We can set the <code class="cx pz qa qb qc b">weight_dtype</code> variable to the desired in-memory datatypes, choosing from <code class="cx pz qa qb qc b">torch.quint4x2</code> or <code class="cx pz qa qb qc b">torch.qint8</code> for int4 and in8, respectively. Additionally we can use <code class="cx pz qa qb qc b">lowp_model</code> to define the dequantization precision. For now, we will keep this as <code class="cx pz qa qb qc b">ipex.quantization.WoqLowpMode.None</code> to keep the default bf16 computation precision.</p><pre class="qe qf qg qh qi qo qc qp bp qq bb bk"><span id="2679" class="qr oa gk qc b bg qs qt l qu qv">qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(<br/>  weight_dtype=torch.quint4x2, # or torch.qint8<br/>  lowp_mode=ipex.quantization.WoqLowpMode.NONE, # or FP16, BF16, INT8<br/>)<br/>checkpoint = None # optionally load int4 or int8 checkpoint<br/><br/># PART 3: Model optimization and quantization<br/>model_ipex = ipex.llm.optimize(model, quantization_config=qconfig, low_precision_checkpoint=checkpoint)<br/><br/>del model </span></pre><p id="f1d3" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We use <code class="cx pz qa qb qc b">ipex.llm.optimize()</code> to apply WOQ and then <code class="cx pz qa qb qc b">del model</code> to delete the full model from memory and free ~30GB of RAM.</p><p id="c0b3" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">3. <strong class="ne gl">Prompting Llama 3: </strong>Llama 3, like LLama 2, has a pre-defined prompting template for its instruction-tuned models. Using this template, developers can define specific model behavior instructions and provide user prompts and conversation history.</p><pre class="qe qf qg qh qi qo qc qp bp qq bb bk"><span id="4ce5" class="qr oa gk qc b bg qs qt l qu qv">system= """\n\n You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. If you don't know the answer to a question, please don't share false information."""<br/>user= "\n\n You are an expert in astronomy. Can you tell me 5 fun facts about the universe?"<br/>model_answer_1 = 'None'<br/><br/>llama_prompt_tempate = f"""<br/>&lt;|begin_of_text|&gt;\n&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;{system}<br/>&lt;|eot_id|&gt;\n&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;{user}<br/>&lt;|eot_id|&gt;\n&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;{model_answer_1}&lt;|eot_id|&gt;<br/>"""<br/><br/>inputs = tokenizer(llama_prompt_tempate, return_tensors="pt").input_ids</span></pre><p id="51d1" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We provide the required fields and then use the tokenizer to convert the entire template into tokens for the model.</p><p id="30f3" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">4.<strong class="ne gl"> Llama 3 Inference:</strong> For text generation, we leverage <code class="cx pz qa qb qc b">TextStreamer</code> to generate a real-time inference stream instead of printing the entire output at once. This results in a more natural text generation experience for readers. We provide the configured streamer to <code class="cx pz qa qb qc b">model_ipex.generate()</code> and other text-generation parameters.</p><pre class="qe qf qg qh qi qo qc qp bp qq bb bk"><span id="59ca" class="qr oa gk qc b bg qs qt l qu qv">streamer = TextStreamer(tokenizer,skip_prompt=True)<br/><br/>with torch.inference_mode():<br/>    tokens = model_ipex.generate(<br/>        inputs,<br/>        streamer=streamer,<br/>        pad_token_id=128001,<br/>        eos_token_id=128001,<br/>        max_new_tokens=300,<br/>        repetition_penalty=1.5,<br/>)</span></pre><p id="932a" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Upon running this code, the model will start generating outputs. Keep in mind that these are unfiltered and non-guarded outputs. For real-world use cases, you will need to make additional post-processing considerations.</p><figure class="qe qf qg qh qi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qy"><img src="../Images/aca89dea56a6712604806222f4b4f7a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_DRMPKM_99Z6281hSiuYHQ.gif"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 5. Streamed inference of Llama-3–8B-Instruct with WOQ mode compression at int4 running on the Intel Tiber Developer Cloud’s JupyterLab environment — Gif by Author</figcaption></figure><p id="7182" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">That’s it. With less than 20 lines of code, you now have a low-latency CPU optimized version of the latest SoTA LLM in the ecosystem.</p><h2 id="347d" class="pa oa gk bf ob pb pc pd oe pe pf pg oh nl ph pi pj np pk pl pm nt pn po pp pq bk">Considerations for Deployment</h2><p id="b6c7" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Depending on your inference service deployment strategy, there are a few things that you will want to consider:</p><ul class=""><li id="d807" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk">If deploying instances of Llama 3 in containers, WOQ will offer a smaller memory footprint and allow you to serve multiple inference services of the model on a single hardware node.</li><li id="7cf9" class="nc nd gk ne b hi pu ng nh hl pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">When deploying multiple inference services, you should optimize the threads and memory reserved for each service instance. Leave enough additional memory (~4 GB) and threads (~4 threads) to handle background processes.</li><li id="652a" class="nc nd gk ne b hi pu ng nh hl pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Consider saving the WOQ version of the model and storing it in a model registry to eliminate the need to re-quantize the model per instance deployment.</li></ul><h1 id="14c1" class="nz oa gk bf ob oc od hk oe of og hn oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion and Discussion</h1><p id="fabb" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Meta’s Llama 3 LLM family delivers remarkable improvements over previous generations with a diverse range of configurations (more coming soon). In this tutorial, we explored enhancing CPU inference with weight-only quantization (WOQ), a technique that reduces latency with minimal impacts to accuracy.</p><p id="b754" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By integrating the new generation of performance-oriented Llama 3 LLMs with optimization techniques like WOQ, developers can unlock new possibilities for <a class="af ny" href="https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/training/generative-ai.html" rel="noopener ugc nofollow" target="_blank">GenAI</a> applications. This combination simplifies the hardware requirements to achieve high-fidelity, low-latency results from LLMs integrated into new and existing systems.</p><p id="f7cd" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne gl">A few exciting things to try next would be:</strong></p><ol class=""><li id="4ea3" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qx ps pt bk"><strong class="ne gl">Experiment with Quantization Levels:</strong> You should test int4 and int8 quantization to identify the best compromise between performance and accuracy for your specific applications.</li><li id="9ff4" class="nc nd gk ne b hi pu ng nh hl pv nj nk nl pw nn no np px nr ns nt py nv nw nx qx ps pt bk"><strong class="ne gl">Performance Monitoring:</strong> It is crucial to continuously assess the performance and accuracy of the Llama 3 model across different real-world scenarios to ensure that quantization maintains the desired effectiveness.</li><li id="0073" class="nc nd gk ne b hi pu ng nh hl pv nj nk nl pw nn no np px nr ns nt py nv nw nx qx ps pt bk"><strong class="ne gl">Test more Llamas: </strong>Explore the entire Llama 3 family and evaluate the impact of WOQ and other PyTorch <a class="af ny" href="https://pytorch.org/docs/stable/quantization.html" rel="noopener ugc nofollow" target="_blank">quantization recipes</a>.</li></ol></div></div></div><div class="ab cb qz ra rb rc" role="separator"><span class="rd by bm re rf rg"/><span class="rd by bm re rf rg"/><span class="rd by bm re rf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c65a" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne gl"><em class="qn">Thank you for reading! Don’t forget to follow </em></strong><a class="af ny" href="https://eduand-alvarez.medium.com/" rel="noopener"><strong class="ne gl"><em class="qn">my profile for more articles</em></strong></a><strong class="ne gl"><em class="qn"> like this!</em></strong></p><p id="4807" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="qn">*Other names and brands may be claimed as the property of others.</em></p></div></div></div></div>    
</body>
</html>