# ä»é›¶å¼€å§‹é‡å»º PyTorchï¼ˆæ”¯æŒ GPU å’Œè‡ªåŠ¨æ±‚å¯¼ï¼‰

> åŸæ–‡ï¼š[`towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc?source=collection_archive---------0-----------------------#2024-05-14`](https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc?source=collection_archive---------0-----------------------#2024-05-14)

## åŸºäº C/C++ã€CUDA å’Œ Python æ„å»ºä½ è‡ªå·±çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒ GPU å¹¶å…·æœ‰è‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½

[](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)![Lucas de Lima Nogueira](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------) [Lucas de Lima Nogueira](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)

Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------) Â·é˜…è¯»æ—¶é•¿ 24 åˆ†é’ŸÂ·2024 å¹´ 5 æœˆ 14 æ—¥

--

![](img/7fb280914af3d0239f5a27ae8d414ce0.png)

å›¾ç‰‡ç”±ä½œè€…ä¸ AI åä½œåˆ¶ä½œ ([`copilot.microsoft.com/images/create`](https://copilot.microsoft.com/images/create))

# ä»‹ç»

å¤šå¹´æ¥ï¼Œæˆ‘ä¸€ç›´åœ¨ä½¿ç”¨ PyTorch æ„å»ºå’Œè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å°½ç®¡æˆ‘å·²ç»å­¦ä¼šäº†å®ƒçš„è¯­æ³•å’Œè§„åˆ™ï¼Œä½†ä¸€ç›´æœ‰ä¸€ä¸ªé—®é¢˜è®©æˆ‘æ„Ÿåˆ°å¥½å¥‡ï¼šåœ¨è¿™äº›æ“ä½œè¿‡ç¨‹ä¸­ï¼Œå†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿè¿™ä¸€åˆ‡æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ

å¦‚æœä½ å·²ç»è¯»åˆ°è¿™é‡Œï¼Œä½ å¯èƒ½æœ‰ç›¸åŒçš„é—®é¢˜ã€‚å¦‚æœæˆ‘é—®ä½ å¦‚ä½•åœ¨ PyTorch ä¸­åˆ›å»ºå’Œè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œä½ å¯èƒ½ä¼šå†™å‡ºç±»ä¼¼äºä¸‹é¢çš„ä»£ç ï¼š

```py
import torch
import torch.nn as nn
import torch.optim as optim

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.fc1 = nn.Linear(1, 10)
        self.sigmoid = nn.Sigmoid()
        self.fc2 = nn.Linear(10, 1)

    def forward(self, x):
        out = self.fc1(x)
        out = self.sigmoid(out)
        out = self.fc2(out)

        return out

...

model = MyModel().to(device)
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.001)

for epoch in range(epochs):
    for x, y in ...

        x = x.to(device)
        y = y.to(device)

        outputs = model(x)
        loss = criterion(outputs, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

ä½†å¦‚æœæˆ‘é—®ä½ ï¼Œåå‘ä¼ æ’­æ˜¯å¦‚ä½•å·¥ä½œçš„å‘¢ï¼Ÿæˆ–è€…ï¼Œä¾‹å¦‚ï¼Œå½“ä½ é‡æ–°å¡‘å½¢ä¸€ä¸ªå¼ é‡æ—¶ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿæ•°æ®æ˜¯å¦åœ¨å†…éƒ¨é‡æ–°æ’åˆ—ï¼Ÿè¿™ä¸€è¿‡ç¨‹æ˜¯å¦‚ä½•å‘ç”Ÿçš„ï¼Ÿä¸ºä»€ä¹ˆ PyTorch å¦‚æ­¤é«˜æ•ˆï¼ŸPyTorch æ˜¯å¦‚ä½•å¤„ç† GPU æ“ä½œçš„ï¼Ÿè¿™äº›é—®é¢˜ä¸€ç›´ä»¤æˆ‘æ„Ÿåˆ°å¥½å¥‡ï¼Œæˆ‘æƒ³å®ƒä»¬ä¹Ÿä¼šè®©ä½ æ„Ÿå…´è¶£ã€‚å› æ­¤ï¼Œä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™äº›æ¦‚å¿µï¼Œæœ€å¥½çš„æ–¹æ³•æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿé‚£å°±æ˜¯ä»***é›¶å¼€å§‹***æ„å»ºä½ è‡ªå·±çš„å¼ é‡åº“ï¼è€Œè¿™æ­£æ˜¯ä½ å°†åœ¨æœ¬æ–‡ä¸­å­¦åˆ°çš„å†…å®¹ï¼

# #1 â€” å¼ é‡

è¦æ„å»ºä¸€ä¸ª*å¼ é‡åº“*ï¼Œä½ é¦–å…ˆéœ€è¦äº†è§£çš„æ¦‚å¿µæ˜¾ç„¶æ˜¯ï¼šä»€ä¹ˆæ˜¯å¼ é‡ï¼Ÿ

ä½ å¯èƒ½ä¼šç›´è§‚åœ°è®¤ä¸ºå¼ é‡æ˜¯ä¸€ä¸ªæ•°å­¦æ¦‚å¿µï¼Œè¡¨ç¤ºä¸€ä¸ªåŒ…å«ä¸€äº›æ•°å­—çš„ n ç»´æ•°æ®ç»“æ„ã€‚ä½†åœ¨è¿™é‡Œæˆ‘ä»¬éœ€è¦ä»è®¡ç®—çš„è§’åº¦ç†è§£å¦‚ä½•å»ºæ¨¡è¿™ä¸ªæ•°æ®ç»“æ„ã€‚æˆ‘ä»¬å¯ä»¥æŠŠå¼ é‡çœ‹ä½œæ˜¯ç”±æ•°æ®æœ¬èº«å’Œä¸€äº›å…ƒæ•°æ®ç»„æˆï¼Œè¿™äº›å…ƒæ•°æ®æè¿°äº†å¼ é‡çš„æŸäº›æ–¹é¢ï¼Œå¦‚å®ƒçš„å½¢çŠ¶æˆ–å®ƒæ‰€å­˜å‚¨çš„è®¾å¤‡ï¼ˆå³ CPU å†…å­˜ã€GPU å†…å­˜ç­‰ï¼‰ã€‚

![](img/126751c339f220fec6bc0a40690fb069.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

è¿˜æœ‰ä¸€ä¸ªä¸å¤ªå¸¸è§çš„å…ƒæ•°æ®ï¼Œå¯èƒ½ä½ ä»æœªå¬è¯´è¿‡ï¼Œå«åš *æ­¥å¹…*ã€‚è¿™ä¸ªæ¦‚å¿µå¯¹äºç†è§£å¼ é‡æ•°æ®é‡æ’çš„å†…éƒ¨åŸç†éå¸¸é‡è¦ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æ›´è¯¦ç»†åœ°è®¨è®ºä¸€ä¸‹ã€‚

æƒ³è±¡ä¸€ä¸ªå½¢çŠ¶ä¸º [4, 8] çš„äºŒç»´å¼ é‡ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

![](img/169e1865490d4603d66dc792052c1458.png)

4x8 å¼ é‡ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰

å¼ é‡çš„æ•°æ®ï¼ˆå³æµ®ç‚¹æ•°ï¼‰å®é™…ä¸Šæ˜¯ä»¥ä¸€ç»´æ•°ç»„çš„å½¢å¼å­˜å‚¨åœ¨å†…å­˜ä¸­çš„ï¼š

![](img/edc491c716df7cab51cd1ca4a61ff296.png)

å¼ é‡çš„ä¸€ç»´æ•°æ®æ•°ç»„ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰

å› æ­¤ï¼Œä¸ºäº†å°†è¿™ä¸ªä¸€ç»´æ•°ç»„è¡¨ç¤ºä¸ºä¸€ä¸ª N ç»´å¼ é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨æ­¥å¹…ã€‚åŸºæœ¬çš„æ€æƒ³å¦‚ä¸‹ï¼š

æˆ‘ä»¬æœ‰ä¸€ä¸ª 4 è¡Œ 8 åˆ—çš„çŸ©é˜µã€‚è€ƒè™‘åˆ°å®ƒçš„æ‰€æœ‰å…ƒç´ éƒ½æ˜¯æŒ‰è¡Œåœ¨ä¸€ç»´æ•°ç»„ä¸­ç»„ç»‡çš„ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è®¿é—®ä½ç½® [2, 3] çš„å€¼ï¼Œæˆ‘ä»¬éœ€è¦éå† 2 è¡Œï¼ˆæ¯è¡Œ 8 ä¸ªå…ƒç´ ï¼‰ï¼ŒåŠ ä¸Š 3 ä¸ªä½ç½®ã€‚ç”¨æ•°å­¦æœ¯è¯­æ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸€ç»´æ•°ç»„ä¸Šéå† 3 + 2 * 8 ä¸ªå…ƒç´ ï¼š

![](img/644a13550e4601385357edc5bb88071b.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

æ‰€ä»¥è¿™ä¸ªâ€˜8â€™æ˜¯ç¬¬äºŒç»´åº¦çš„ *æ­¥å¹…*ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒè¡¨ç¤ºäº†æˆ‘éœ€è¦åœ¨æ•°ç»„ä¸­éå†å¤šå°‘ä¸ªå…ƒç´ æ‰èƒ½â€œè·³è½¬â€åˆ°ç¬¬äºŒç»´åº¦çš„å…¶ä»–ä½ç½®ã€‚

å› æ­¤ï¼Œå¯¹äºè®¿é—®å½¢çŠ¶ä¸º [shape_0, shape_1] çš„äºŒç»´å¼ é‡ä¸­çš„å…ƒç´  [i, j]ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šéœ€è¦è®¿é—®ä½ç½® j + i * shape_1 çš„å…ƒç´ ã€‚

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æƒ³è±¡ä¸€ä¸ªä¸‰ç»´å¼ é‡ï¼š

![](img/bc1c13d004a29b88a4f73e8b11a51483.png)

5x4x8 å¼ é‡ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰

ä½ å¯ä»¥æŠŠè¿™ä¸ªä¸‰ç»´å¼ é‡çœ‹ä½œæ˜¯çŸ©é˜µçš„åºåˆ—ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥æŠŠè¿™ä¸ªå½¢çŠ¶ä¸º [5, 4, 8] çš„å¼ é‡çœ‹ä½œæ˜¯ 5 ä¸ªå½¢çŠ¶ä¸º [4, 8] çš„çŸ©é˜µã€‚

ç°åœ¨ï¼Œä¸ºäº†è®¿é—®ä½ç½® [1, 2, 7] çš„å…ƒç´ ï¼Œä½ éœ€è¦éå†ä¸€ä¸ªå®Œæ•´çš„å½¢çŠ¶ä¸º [4, 8] çš„çŸ©é˜µï¼Œ2 è¡Œå½¢çŠ¶ä¸º [8] å’Œ 7 åˆ—å½¢çŠ¶ä¸º [1]ã€‚æ‰€ä»¥ï¼Œä½ éœ€è¦åœ¨ä¸€ç»´æ•°ç»„ä¸Šéå† (1 * 4 * 8) + (2 * 8) + (7 * 1) ä¸ªä½ç½®ã€‚

![](img/292c1ea36eb8d9c10528822680f5c698.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

å› æ­¤ï¼Œè¦åœ¨ä¸€ç»´æ•°æ®æ•°ç»„ä¸Šè®¿é—®ä¸€ä¸ªå½¢çŠ¶ä¸º [shape_0, shape_1, shape_2] çš„ä¸‰ç»´å¼ é‡ä¸­çš„å…ƒç´  [i][j][k]ï¼Œä½ éœ€è¦ï¼š

![](img/81de413fc9cf4c48a4a1b0c764dea74f.png)

è¿™ä¸ª shape_1 * shape_2 æ˜¯ç¬¬ä¸€ç»´çš„ *æ­¥å¹…*ï¼Œshape_2 æ˜¯ç¬¬äºŒç»´çš„ *æ­¥å¹…*ï¼Œ1 æ˜¯ç¬¬ä¸‰ç»´çš„æ­¥å¹…ã€‚

ç„¶åï¼Œä¸ºäº†è¿›è¡Œæ¦‚æ‹¬ï¼š

![](img/e3ef816d9241ae378b6b92a5d7a09bbc.png)

æ¯ä¸ªç»´åº¦çš„*æ­¥å¹…*å¯ä»¥é€šè¿‡ä¸‹ä¸€ä¸ªç»´åº¦å¼ é‡å½¢çŠ¶çš„ä¹˜ç§¯æ¥è®¡ç®—ï¼š

![](img/7b9a7f16cd0d11e4767e681da1993aef.png)

ç„¶åæˆ‘ä»¬è®¾ç½®æ­¥å¹…[n-1] = 1ã€‚

åœ¨æˆ‘ä»¬çš„å½¢çŠ¶ä¸º[5, 4, 8]çš„å¼ é‡ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†å¾—åˆ°æ­¥å¹… = [4*8, 8, 1] = [32, 8, 1]

ä½ å¯ä»¥è‡ªå·±æµ‹è¯•ï¼š

```py
import torch

torch.rand([5, 4, 8]).stride()
#(32, 8, 1)
```

å¥½çš„ï¼Œä½†æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å½¢çŠ¶å’Œæ­¥å¹…ï¼Ÿé™¤äº†è®¿é—®å­˜å‚¨ä¸º 1 ç»´æ•°ç»„çš„ N ç»´å¼ é‡çš„å…ƒç´ ä¹‹å¤–ï¼Œè¿™ä¸ªæ¦‚å¿µå¯ä»¥éå¸¸å®¹æ˜“åœ°ç”¨æ¥æ“ä½œå¼ é‡æ’åˆ—ã€‚

ä¾‹å¦‚ï¼Œè¦é‡å¡‘ä¸€ä¸ªå¼ é‡ï¼Œä½ åªéœ€è¦è®¾ç½®æ–°å½¢çŠ¶å¹¶æ ¹æ®å®ƒè®¡ç®—æ–°æ­¥å¹…ï¼ï¼ˆå› ä¸ºæ–°å½¢çŠ¶ä¿è¯äº†ç›¸åŒæ•°é‡çš„å…ƒç´ ï¼‰

```py
import torch

t = torch.rand([5, 4, 8])

print(t.shape)
# [5, 4, 8]

print(t.stride())
# [32, 8, 1]

new_t = t.reshape([4, 5, 2, 2, 2])

print(new_t.shape)
# [4, 5, 2, 2, 2]

print(new_t.stride())
# [40, 8, 4, 2, 1]
```

åœ¨å†…éƒ¨ï¼Œå¼ é‡ä»ç„¶å­˜å‚¨ä¸ºç›¸åŒçš„ 1 ç»´æ•°ç»„ã€‚é‡å¡‘æ–¹æ³•æ²¡æœ‰æ”¹å˜æ•°ç»„å†…å…ƒç´ çš„é¡ºåºï¼è¿™å¾ˆç¥å¥‡ï¼Œä¸æ˜¯å—ï¼ŸğŸ˜

ä½ å¯ä»¥è‡ªå·±éªŒè¯ï¼Œä½¿ç”¨ä»¥ä¸‹å‡½æ•°è®¿é—® PyTorch çš„å†…éƒ¨ 1 ç»´æ•°ç»„ï¼š

```py
import ctypes

def print_internal(t: torch.Tensor):
    print(
        torch.frombuffer(
            ctypes.string_at(t.data_ptr(), t.storage().nbytes()), dtype=t.dtype
        )
    )

print_internal(t)
# [0.0752, 0.5898, 0.3930, 0.9577, 0.2276, 0.9786, 0.1009, 0.138, ...

print_internal(new_t)
# [0.0752, 0.5898, 0.3930, 0.9577, 0.2276, 0.9786, 0.1009, 0.138, ...
```

æˆ–è€…ä¾‹å¦‚ï¼Œä½ æƒ³è¦è½¬ç½®ä¸¤ä¸ªè½´ã€‚åœ¨å†…éƒ¨ï¼Œä½ åªéœ€è¦äº¤æ¢ç›¸åº”çš„æ­¥å¹…ï¼

```py
t = torch.arange(0, 24).reshape(2, 3, 4)
print(t)
# [[[ 0,  1,  2,  3],
#   [ 4,  5,  6,  7],
#   [ 8,  9, 10, 11]],

#  [[12, 13, 14, 15],
#   [16, 17, 18, 19],
#   [20, 21, 22, 23]]]

print(t.shape)
# [2, 3, 4]

print(t.stride())
# [12, 4, 1]

new_t = t.transpose(0, 1)
print(new_t)
# [[[ 0,  1,  2,  3],
#   [12, 13, 14, 15]],

#  [[ 4,  5,  6,  7],
#   [16, 17, 18, 19]],

#  [[ 8,  9, 10, 11],
#   [20, 21, 22, 23]]]

print(new_t.shape)
# [3, 2, 4]

print(new_t.stride())
# [4, 12, 1]
```

å¦‚æœä½ æ‰“å°å†…éƒ¨æ•°ç»„ï¼Œä¸¤è€…éƒ½æœ‰ç›¸åŒçš„å€¼ï¼š

```py
print_internal(t)
# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]

print_internal(new_t)
# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
```

ç„¶è€Œï¼Œ`new_t`çš„æ­¥å¹…ç°åœ¨ä¸æˆ‘ä¸Šé¢å±•ç¤ºçš„æ–¹ç¨‹ä¸åŒ¹é…ã€‚è¿™æ˜¯å› ä¸ºå¼ é‡ç°åœ¨ä¸å†æ˜¯è¿ç»­çš„ã€‚è¿™æ„å‘³ç€è™½ç„¶å†…éƒ¨æ•°ç»„ä¿æŒä¸å˜ï¼Œä½†å…¶åœ¨å†…å­˜ä¸­çš„å€¼çš„é¡ºåºä¸å¼ é‡çš„å®é™…é¡ºåºä¸åŒ¹é…ã€‚

```py
t.is_contiguous()
# True

new_t.is_contiguous()
# False
```

è¿™æ„å‘³ç€æŒ‰é¡ºåºè®¿é—®éè¿ç»­å…ƒç´ çš„æ•ˆç‡è¾ƒä½ï¼ˆå› ä¸ºçœŸå®çš„å¼ é‡å…ƒç´ åœ¨å†…å­˜ä¸­ä¸æ˜¯æŒ‰é¡ºåºæ’åˆ—çš„ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼š

```py
new_t_contiguous = new_t.contiguous()

print(new_t_contiguous.is_contiguous())
# True
```

å¦‚æœæˆ‘ä»¬åˆ†æå†…éƒ¨æ•°ç»„ï¼Œç°åœ¨å…¶é¡ºåºä¸å®é™…å¼ é‡é¡ºåºåŒ¹é…ï¼Œè¿™å¯ä»¥æä¾›æ›´å¥½çš„å†…å­˜è®¿é—®æ•ˆç‡ï¼š

```py
print(new_t)
# [[[ 0,  1,  2,  3],
#   [12, 13, 14, 15]],

#  [[ 4,  5,  6,  7],
#   [16, 17, 18, 19]],

#  [[ 8,  9, 10, 11],
#   [20, 21, 22, 23]]]

print_internal(new_t)
# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]

print_internal(new_t_contiguous)
# [ 0,  1,  2,  3, 12, 13, 14, 15,  4,  5,  6,  7, 16, 17, 18, 19,  8,  9, 10, 11, 20, 21, 22, 23]
```

ç°åœ¨æˆ‘ä»¬ç†è§£äº†å¼ é‡æ˜¯å¦‚ä½•å»ºæ¨¡çš„ï¼Œè®©æˆ‘ä»¬å¼€å§‹åˆ›å»ºæˆ‘ä»¬çš„åº“å§ï¼

æˆ‘å°†å…¶ç§°ä¸º*Norch*ï¼Œä»£è¡¨ç€ NOT PyTorchï¼Œå¹¶ä¸”ä¹Ÿæš—æŒ‡äº†æˆ‘çš„å§“æ°ï¼ŒNogueira ğŸ˜

é¦–å…ˆè¦çŸ¥é“çš„æ˜¯ï¼Œå°½ç®¡ PyTorch æ˜¯é€šè¿‡ Python ä½¿ç”¨çš„ï¼Œä½†åœ¨å†…éƒ¨å®ƒè¿è¡Œçš„æ˜¯ C/C++ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†é¦–å…ˆåˆ›å»ºæˆ‘ä»¬çš„å†…éƒ¨ C/C++å‡½æ•°ã€‚

æˆ‘ä»¬å¯ä»¥é¦–å…ˆå®šä¹‰å¼ é‡ä½œä¸ºä¸€ä¸ªç»“æ„ä½“æ¥å­˜å‚¨å…¶æ•°æ®å’Œå…ƒæ•°æ®ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å®ä¾‹åŒ–å®ƒï¼š

```py
//norch/csrc/tensor.cpp

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>

typedef struct {
    float* data;
    int* strides;
    int* shape;
    int ndim;
    int size;
    char* device;
} Tensor;

Tensor* create_tensor(float* data, int* shape, int ndim) {

    Tensor* tensor = (Tensor*)malloc(sizeof(Tensor));
    if (tensor == NULL) {
        fprintf(stderr, "Memory allocation failed\n");
        exit(1);
    }
    tensor->data = data;
    tensor->shape = shape;
    tensor->ndim = ndim;

    tensor->size = 1;
    for (int i = 0; i < ndim; i++) {
        tensor->size *= shape[i];
    }

    tensor->strides = (int*)malloc(ndim * sizeof(int));
    if (tensor->strides == NULL) {
        fprintf(stderr, "Memory allocation failed\n");
        exit(1);
    }
    int stride = 1;
    for (int i = ndim - 1; i >= 0; i--) {
        tensor->strides[i] = stride;
        stride *= shape[i];
    }

    return tensor;
}
```

ä¸ºäº†è®¿é—®æŸä¸ªå…ƒç´ ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ä¹‹å‰å­¦åˆ°çš„æ­¥å¹…ï¼š

```py
//norch/csrc/tensor.cpp

float get_item(Tensor* tensor, int* indices) {
    int index = 0;
    for (int i = 0; i < tensor->ndim; i++) {
        index += indices[i] * tensor->strides[i];
    }

    float result;
    result = tensor->data[index];

    return result;
}
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºå¼ é‡æ“ä½œã€‚æˆ‘å°†å±•ç¤ºä¸€äº›ç¤ºä¾‹ï¼Œä½ å¯ä»¥åœ¨æœ¬æ–‡æœ«å°¾é“¾æ¥çš„å­˜å‚¨åº“ä¸­æ‰¾åˆ°å®Œæ•´ç‰ˆæœ¬ã€‚

```py
//norch/csrc/cpu.cpp

void add_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {

    for (int i = 0; i < tensor1->size; i++) {
        result_data[i] = tensor1->data[i] + tensor2->data[i];
    }
}

void sub_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {

    for (int i = 0; i < tensor1->size; i++) {
        result_data[i] = tensor1->data[i] - tensor2->data[i];
    }
}

void elementwise_mul_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {

    for (int i = 0; i < tensor1->size; i++) {
        result_data[i] = tensor1->data[i] * tensor2->data[i];
    }
}

void assign_tensor_cpu(Tensor* tensor, float* result_data) {

    for (int i = 0; i < tensor->size; i++) {
        result_data[i] = tensor->data[i];
    }
}

...
```

ä¹‹åæˆ‘ä»¬å°±èƒ½åˆ›å»ºæˆ‘ä»¬çš„å…¶ä»–è°ƒç”¨è¿™äº›æ“ä½œçš„å¼ é‡å‡½æ•°ï¼š

```py
//norch/csrc/tensor.cpp

Tensor* add_tensor(Tensor* tensor1, Tensor* tensor2) {
    if (tensor1->ndim != tensor2->ndim) {
        fprintf(stderr, "Tensors must have the same number of dimensions %d and %d for addition\n", tensor1->ndim, tensor2->ndim);
        exit(1);
    }

    int ndim = tensor1->ndim;
    int* shape = (int*)malloc(ndim * sizeof(int));
    if (shape == NULL) {
        fprintf(stderr, "Memory allocation failed\n");
        exit(1);
    }

    for (int i = 0; i < ndim; i++) {
        if (tensor1->shape[i] != tensor2->shape[i]) {
            fprintf(stderr, "Tensors must have the same shape %d and %d at index %d for addition\n", tensor1->shape[i], tensor2->shape[i], i);
            exit(1);
        }
        shape[i] = tensor1->shape[i];
    }        
    float* result_data = (float*)malloc(tensor1->size * sizeof(float));
    if (result_data == NULL) {
        fprintf(stderr, "Memory allocation failed\n");
        exit(1);
    }
    add_tensor_cpu(tensor1, tensor2, result_data);

    return create_tensor(result_data, shape, ndim, device);
}
```

å¦‚å‰æ‰€è¿°ï¼Œå¼ é‡é‡å¡‘ä¸ä¼šä¿®æ”¹å†…éƒ¨æ•°æ®æ•°ç»„ï¼š

```py
//norch/csrc/tensor.cpp

Tensor* reshape_tensor(Tensor* tensor, int* new_shape, int new_ndim) {

    int ndim = new_ndim;
    int* shape = (int*)malloc(ndim * sizeof(int));
    if (shape == NULL) {
        fprintf(stderr, "Memory allocation failed\n");
        exit(1);
    }

    for (int i = 0; i < ndim; i++) {
        shape[i] = new_shape[i];
    }

    // Calculate the total number of elements in the new shape
    int size = 1;
    for (int i = 0; i < new_ndim; i++) {
        size *= shape[i];
    }

    // Check if the total number of elements matches the current tensor's size
    if (size != tensor->size) {
        fprintf(stderr, "Cannot reshape tensor. Total number of elements in new shape does not match the current size of the tensor.\n");
        exit(1);
    }

    float* result_data = (float*)malloc(tensor->size * sizeof(float));
    if (result_data == NULL) {
        fprintf(stderr, "Memory allocation failed\n");
        exit(1);
    }
    assign_tensor_cpu(tensor, result_data);
    return create_tensor(result_data, shape, ndim, device);
}
```

è™½ç„¶æˆ‘ä»¬ç°åœ¨å¯ä»¥è¿›è¡Œä¸€äº›å¼ é‡æ“ä½œï¼Œä½†æ˜¯æ²¡æœ‰äººæ„¿æ„ä½¿ç”¨ C/C++æ¥è¿è¡Œå®ƒï¼Œå¯¹å§ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹æ„å»ºæˆ‘ä»¬çš„ Python åŒ…è£…å™¨ï¼

æœ‰å¾ˆå¤šé€‰é¡¹å¯ä»¥ä½¿ç”¨ Python è¿è¡Œ C/C++ä»£ç ï¼Œæ¯”å¦‚*Pybind11*å’Œ*Cython*ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨*ctypes*ã€‚

*ctypes*çš„åŸºæœ¬ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
//C code
#include <stdio.h>

float add_floats(float a, float b) {
    return a + b;
}
```

```py
# Compile
gcc -shared -o add_floats.so -fPIC add_floats.c
```

```py
# Python code
import ctypes

# Load the shared library
lib = ctypes.CDLL('./add_floats.so')

# Define the argument and return types for the function
lib.add_floats.argtypes = [ctypes.c_float, ctypes.c_float]
lib.add_floats.restype = ctypes.c_float

# Convert python float to c_float type 
a = ctypes.c_float(3.5)
b = ctypes.c_float(2.2)

# Call the C function
result = lib.add_floats(a, b)
print(result)
# 5.7
```

å¦‚ä½ æ‰€è§ï¼Œè¿™éå¸¸ç›´è§‚ã€‚åœ¨ä½ ç¼–è¯‘ C/C++ä»£ç åï¼Œå¯ä»¥éå¸¸è½»æ¾åœ°åœ¨ Python ä¸­ä½¿ç”¨*ctypes*ã€‚ä½ åªéœ€è¦å®šä¹‰å‡½æ•°çš„å‚æ•°å’Œè¿”å›å€¼ç±»å‹ï¼Œå¹¶å°†å˜é‡è½¬æ¢ä¸ºç›¸åº”çš„ C ç±»å‹ï¼Œå†è°ƒç”¨å‡½æ•°ã€‚å¯¹äºæ›´å¤æ‚çš„ç±»å‹ï¼Œå¦‚æ•°ç»„ï¼ˆæµ®åŠ¨åˆ—è¡¨ï¼‰ï¼Œä½ å¯ä»¥ä½¿ç”¨æŒ‡é’ˆã€‚

```py
data = [1.0, 2.0, 3.0]
data_ctype = (ctypes.c_float * len(data))(*data)

lib.some_array_func.argstypes = [ctypes.POINTER(ctypes.c_float)]

...

lib.some_array_func(data)
```

å¯¹äºç»“æ„ç±»å‹ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæˆ‘ä»¬è‡ªå·±çš„ C ç±»å‹ï¼š

```py
class CustomType(ctypes.Structure):
    _fields_ = [
        ('field1', ctypes.POINTER(ctypes.c_float)),
        ('field2', ctypes.POINTER(ctypes.c_int)),
        ('field3', ctypes.c_int),
    ]

# Can be used as ctypes.POINTER(CustomType)
```

åœ¨è¿™æ®µç®€çŸ­çš„è§£é‡Šä¹‹åï¼Œè®©æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„å¼ é‡ C/C++åº“æ„å»º Python åŒ…è£…å™¨ï¼

```py
# norch/tensor.py

import ctypes

class CTensor(ctypes.Structure):
    _fields_ = [
        ('data', ctypes.POINTER(ctypes.c_float)),
        ('strides', ctypes.POINTER(ctypes.c_int)),
        ('shape', ctypes.POINTER(ctypes.c_int)),
        ('ndim', ctypes.c_int),
        ('size', ctypes.c_int),
    ]

class Tensor:
    os.path.abspath(os.curdir)
    _C = ctypes.CDLL("COMPILED_LIB.so"))

    def __init__(self):

        data, shape = self.flatten(data)
        self.data_ctype = (ctypes.c_float * len(data))(*data)
        self.shape_ctype = (ctypes.c_int * len(shape))(*shape)
        self.ndim_ctype = ctypes.c_int(len(shape))

        self.shape = shape
        self.ndim = len(shape)

        Tensor._C.create_tensor.argtypes = [ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_int), ctypes.c_int]
        Tensor._C.create_tensor.restype = ctypes.POINTER(CTensor)

        self.tensor = Tensor._C.create_tensor(
            self.data_ctype,
            self.shape_ctype,
            self.ndim_ctype,
        )

    def flatten(self, nested_list):
        """
        This method simply convert a list type tensor to a flatten tensor with its shape

        Example:

        Arguments:  
            nested_list: [[1, 2, 3], [-5, 2, 0]]
        Return:
            flat_data: [1, 2, 3, -5, 2, 0]
            shape: [2, 3]
        """
        def flatten_recursively(nested_list):
            flat_data = []
            shape = []
            if isinstance(nested_list, list):
                for sublist in nested_list:
                    inner_data, inner_shape = flatten_recursively(sublist)
                    flat_data.extend(inner_data)
                shape.append(len(nested_list))
                shape.extend(inner_shape)
            else:
                flat_data.append(nested_list)
            return flat_data, shape

        flat_data, shape = flatten_recursively(nested_list)
        return flat_data, shape 
```

ç°åœ¨æˆ‘ä»¬åŒ…æ‹¬äº† Python å¼ é‡æ“ä½œï¼Œä»¥ä¾¿è°ƒç”¨ C/C++æ“ä½œã€‚

```py
# norch/tensor.py

def __getitem__(self, indices):
    """
    Access tensor by index tensor[i, j, k...]
    """

    if len(indices) != self.ndim:
        raise ValueError("Number of indices must match the number of dimensions")

    Tensor._C.get_item.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(ctypes.c_int)]
    Tensor._C.get_item.restype = ctypes.c_float

    indices = (ctypes.c_int * len(indices))(*indices)
    value = Tensor._C.get_item(self.tensor, indices)  

    return value

def reshape(self, new_shape):
    """
    Reshape tensor
    result = tensor.reshape([1,2])
    """
    new_shape_ctype = (ctypes.c_int * len(new_shape))(*new_shape)
    new_ndim_ctype = ctypes.c_int(len(new_shape))

    Tensor._C.reshape_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(ctypes.c_int), ctypes.c_int]
    Tensor._C.reshape_tensor.restype = ctypes.POINTER(CTensor)
    result_tensor_ptr = Tensor._C.reshape_tensor(self.tensor, new_shape_ctype, new_ndim_ctype)   

    result_data = Tensor()
    result_data.tensor = result_tensor_ptr
    result_data.shape = new_shape.copy()
    result_data.ndim = len(new_shape)
    result_data.device = self.device

    return result_data

def __add__(self, other):
    """
    Add tensors
    result = tensor1 + tensor2
    """

    if self.shape != other.shape:
        raise ValueError("Tensors must have the same shape for addition")

    Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]
    Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)

    result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)

    result_data = Tensor()
    result_data.tensor = result_tensor_ptr
    result_data.shape = self.shape.copy()
    result_data.ndim = self.ndim
    result_data.device = self.device

    return result_data

# Include the other operations:
# __str__
# __sub__ (-)
# __mul__ (*)
# __matmul__ (@)
# __pow__ (**)
# __truediv__ (/)
# log
# ...
```

å¦‚æœä½ å·²ç»çœ‹åˆ°è¿™é‡Œï¼Œä½ ç°åœ¨å¯ä»¥è¿è¡Œä»£ç å¹¶å¼€å§‹è¿›è¡Œä¸€äº›å¼ é‡æ“ä½œäº†ï¼

```py
import norch

tensor1 = norch.Tensor([[1, 2, 3], [3, 2, 1]])
tensor2 = norch.Tensor([[3, 2, 1], [1, 2, 3]])

result = tensor1 + tensor2
print(result[0, 0])
# 4 
```

# #2 â€” GPU æ”¯æŒ

åœ¨åˆ›å»ºäº†æˆ‘ä»¬åº“çš„åŸºæœ¬ç»“æ„ä¹‹åï¼Œç°åœ¨æˆ‘ä»¬å°†æŠŠå®ƒæå‡åˆ°ä¸€ä¸ªæ–°å±‚æ¬¡ã€‚ä¼—æ‰€å‘¨çŸ¥ï¼Œä½ å¯ä»¥è°ƒç”¨`.to("cuda")`å°†æ•°æ®å‘é€åˆ° GPUï¼Œå¹¶åŠ å¿«æ•°å­¦è¿ç®—é€Ÿåº¦ã€‚æˆ‘å‡è®¾ä½ å¯¹ CUDA çš„åŸºæœ¬å·¥ä½œåŸç†æœ‰æ‰€äº†è§£ï¼Œä½†å¦‚æœä¸äº†è§£ï¼Œä½ å¯ä»¥é˜…è¯»æˆ‘å¦å¤–ä¸€ç¯‡æ–‡ç« ï¼šCUDA æ•™ç¨‹. æˆ‘åœ¨è¿™é‡Œç­‰ä½ ã€‚ğŸ˜Š

â€¦

å¯¹äºæ€¥äºäº†è§£çš„äººï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªç®€çŸ­çš„ä»‹ç»ï¼š

åŸºæœ¬ä¸Šï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„æ‰€æœ‰ä»£ç éƒ½åœ¨ CPU å†…å­˜ä¸Šè¿è¡Œã€‚è™½ç„¶å¯¹äºå•ä¸€æ“ä½œ CPU é€Ÿåº¦æ›´å¿«ï¼Œä½† GPU çš„ä¼˜åŠ¿åœ¨äºå®ƒçš„å¹¶è¡ŒåŒ–èƒ½åŠ›ã€‚CPU è®¾è®¡æ—¨åœ¨å¿«é€Ÿæ‰§è¡Œä¸€ç³»åˆ—æ“ä½œï¼ˆçº¿ç¨‹ï¼‰ï¼ˆä½†å®ƒåªèƒ½æ‰§è¡Œå‡ åä¸ªï¼‰ï¼Œè€Œ GPU è®¾è®¡åˆ™æ—¨åœ¨å¹¶è¡Œæ‰§è¡Œæ•°ç™¾ä¸‡ä¸ªæ“ä½œï¼ˆé€šè¿‡ç‰ºç‰²å•ä¸ªçº¿ç¨‹çš„æ€§èƒ½ï¼‰ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™ä¸€èƒ½åŠ›å¹¶è¡Œæ‰§è¡Œæ“ä½œã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªç™¾ä¸‡å¤§å°çš„å¼ é‡åŠ æ³•ä¸­ï¼Œä¸å¿…åœ¨å¾ªç¯å†…éƒ¨ä¾æ¬¡æ·»åŠ æ¯ä¸ªç´¢å¼•çš„å…ƒç´ ï¼Œä½¿ç”¨ GPU æˆ‘ä»¬å¯ä»¥ä¸€æ¬¡æ€§å¹¶è¡Œåœ°åŠ å’Œæ‰€æœ‰å…ƒç´ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ CUDAï¼Œè¿™æ˜¯ NVIDIA å¼€å‘çš„ä¸€ä¸ªå¹³å°ï¼Œæ—¨åœ¨è®©å¼€å‘è€…å°† GPU æ”¯æŒé›†æˆåˆ°ä»–ä»¬çš„è½¯ä»¶åº”ç”¨ä¸­ã€‚

ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½ å¯ä»¥ä½¿ç”¨ CUDA C/C++ï¼Œè¿™æ˜¯ä¸€ç§åŸºäº C/C++çš„ç®€å•æ¥å£ï¼Œæ—¨åœ¨è¿è¡Œç‰¹å®šçš„ GPU æ“ä½œï¼ˆä¾‹å¦‚å°†æ•°æ®ä» CPU å†…å­˜å¤åˆ¶åˆ° GPU å†…å­˜ï¼‰ã€‚

ä»¥ä¸‹ä»£ç åŸºæœ¬ä¸Šä½¿ç”¨äº†ä¸€äº› CUDA C/C++å‡½æ•°ï¼Œå°†æ•°æ®ä» CPU å¤åˆ¶åˆ° GPUï¼Œå¹¶åœ¨æ€»å…± N ä¸ª GPU çº¿ç¨‹ä¸Šå¹¶è¡Œè¿è¡Œ AddTwoArrays å‡½æ•°ï¼ˆä¹Ÿå«åš kernelï¼‰ï¼Œæ¯ä¸ªçº¿ç¨‹è´Ÿè´£åŠ å’Œæ•°ç»„ä¸­çš„ä¸åŒå…ƒç´ ã€‚

```py
#include <stdio.h>

// CPU version for comparison
void AddTwoArrays_CPU(flaot A[], float B[], float C[]) {
    for (int i = 0; i < N; i++) {
        C[i] = A[i] + B[i];
    }
}

// Kernel definition
__global__ void AddTwoArrays_GPU(float A[], float B[], float C[]) {
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}

int main() {

    int N = 1000; // Size of the arrays
    float A[N], B[N], C[N]; // Arrays A, B, and C

    ...

    float *d_A, *d_B, *d_C; // Device pointers for arrays A, B, and C

    // Allocate memory on the device for arrays A, B, and C
    cudaMalloc((void **)&d_A, N * sizeof(float));
    cudaMalloc((void **)&d_B, N * sizeof(float));
    cudaMalloc((void **)&d_C, N * sizeof(float));

    // Copy arrays A and B from host to device
    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);

    // Kernel invocation with N threads
    AddTwoArrays_GPU<<<1, N>>>(d_A, d_B, d_C);

    // Copy vector C from device to host
    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);

}
```

å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬ä¸æ˜¯æ¯æ¬¡æ“ä½œæ—¶åŠ å’Œæ¯å¯¹å…ƒç´ ï¼Œè€Œæ˜¯å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰åŠ æ³•æ“ä½œï¼Œä»è€Œçœå»äº†å¾ªç¯æŒ‡ä»¤ã€‚

åœ¨è¿™æ®µç®€çŸ­çš„ä»‹ç»ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å›åˆ°æˆ‘ä»¬çš„å¼ é‡åº“ã€‚

ç¬¬ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†å¼ é‡æ•°æ®ä» CPU å‘é€åˆ° GPUï¼Œåä¹‹äº¦ç„¶ã€‚

```py
//norch/csrc/tensor.cpp

void to_device(Tensor* tensor, char* target_device) {
    if ((strcmp(target_device, "cuda") == 0) && (strcmp(tensor->device, "cpu") == 0)) {
        cpu_to_cuda(tensor);
    }

    else if ((strcmp(target_device, "cpu") == 0) && (strcmp(tensor->device, "cuda") == 0)) {
        cuda_to_cpu(tensor);
    }
}
```

```py
//norch/csrc/cuda.cu

__host__ void cpu_to_cuda(Tensor* tensor) {

    float* data_tmp;
    cudaMalloc((void **)&data_tmp, tensor->size * sizeof(float));
    cudaMemcpy(data_tmp, tensor->data, tensor->size * sizeof(float), cudaMemcpyHostToDevice);

    tensor->data = data_tmp;

    const char* device_str = "cuda";
    tensor->device = (char*)malloc(strlen(device_str) + 1);
    strcpy(tensor->device, device_str); 

    printf("Successfully sent tensor to: %s\n", tensor->device);
}

__host__ void cuda_to_cpu(Tensor* tensor) {
    float* data_tmp = (float*)malloc(tensor->size * sizeof(float));

    cudaMemcpy(data_tmp, tensor->data, tensor->size * sizeof(float), cudaMemcpyDeviceToHost);
    cudaFree(tensor->data);

    tensor->data = data_tmp;

    const char* device_str = "cpu";
    tensor->device = (char*)malloc(strlen(device_str) + 1);
    strcpy(tensor->device, device_str); 

    printf("Successfully sent tensor to: %s\n", tensor->device);
}
```

Python åŒ…è£…å™¨ï¼š

```py
# norch/tensor.py

def to(self, device):
    self.device = device
    self.device_ctype = self.device.encode('utf-8')

    Tensor._C.to_device.argtypes = [ctypes.POINTER(CTensor), ctypes.c_char_p]
    Tensor._C.to_device.restype = None
    Tensor._C.to_device(self.tensor, self.device_ctype)

    return self
```

ç„¶åï¼Œæˆ‘ä»¬ä¸ºæ‰€æœ‰çš„å¼ é‡æ“ä½œåˆ›å»º GPU ç‰ˆæœ¬ã€‚æˆ‘å°†ä¸ºåŠ æ³•å’Œå‡æ³•å†™å‡ºç¤ºä¾‹ï¼š

```py
//norch/csrc/cuda.cu

#define THREADS_PER_BLOCK 128

__global__ void add_tensor_cuda_kernel(float* data1, float* data2, float* result_data, int size) {

    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < size) {
        result_data[i] = data1[i] + data2[i];
    }
}

__host__ void add_tensor_cuda(Tensor* tensor1, Tensor* tensor2, float* result_data) {

    int number_of_blocks = (tensor1->size + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;
    add_tensor_cuda_kernel<<<number_of_blocks, THREADS_PER_BLOCK>>>(tensor1->data, tensor2->data, result_data, tensor1->size);

    cudaError_t error = cudaGetLastError();
    if (error != cudaSuccess) {
        printf("CUDA error: %s\n", cudaGetErrorString(error));
        exit(-1);
    }

    cudaDeviceSynchronize();
}

__global__ void sub_tensor_cuda_kernel(float* data1, float* data2, float* result_data, int size) {

    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < size) {
        result_data[i] = data1[i] - data2[i];
    }
}

__host__ void sub_tensor_cuda(Tensor* tensor1, Tensor* tensor2, float* result_data) {

    int number_of_blocks = (tensor1->size + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;
    sub_tensor_cuda_kernel<<<number_of_blocks, THREADS_PER_BLOCK>>>(tensor1->data, tensor2->data, result_data, tensor1->size);

    cudaError_t error = cudaGetLastError();
    if (error != cudaSuccess) {
        printf("CUDA error: %s\n", cudaGetErrorString(error));
        exit(-1);
    }

    cudaDeviceSynchronize();
}

... 
```

éšåï¼Œæˆ‘ä»¬åœ¨`tensor.cpp`ä¸ŠåŒ…å«ä¸€ä¸ªæ–°çš„å¼ é‡å±æ€§`char* device`ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥é€‰æ‹©æ“ä½œå°†åœ¨å“ªé‡Œè¿è¡Œï¼ˆCPU æˆ– GPUï¼‰ï¼š

```py
//norch/csrc/tensor.cpp

Tensor* add_tensor(Tensor* tensor1, Tensor* tensor2) {
    if (tensor1->ndim != tensor2->ndim) {
        fprintf(stderr, "Tensors must have the same number of dimensions %d and %d for addition\n", tensor1->ndim, tensor2->ndim);
        exit(1);
    }

    if (strcmp(tensor1->device, tensor2->device) != 0) {
        fprintf(stderr, "Tensors must be on the same device: %s and %s\n", tensor1->device, tensor2->device);
        exit(1);
    }

    char* device = (char*)malloc(strlen(tensor1->device) + 1);
    if (device != NULL) {
        strcpy(device, tensor1->device);
    } else {
        fprintf(stderr, "Memory allocation failed\n");
        exit(-1);
    }
    int ndim = tensor1->ndim;
    int* shape = (int*)malloc(ndim * sizeof(int));
    if (shape == NULL) {
        fprintf(stderr, "Memory allocation failed\n");
        exit(1);
    }

    for (int i = 0; i < ndim; i++) {
        if (tensor1->shape[i] != tensor2->shape[i]) {
            fprintf(stderr, "Tensors must have the same shape %d and %d at index %d for addition\n", tensor1->shape[i], tensor2->shape[i], i);
            exit(1);
        }
        shape[i] = tensor1->shape[i];
    }        

    if (strcmp(tensor1->device, "cuda") == 0) {

        float* result_data;
        cudaMalloc((void **)&result_data, tensor1->size * sizeof(float));
        add_tensor_cuda(tensor1, tensor2, result_data);
        return create_tensor(result_data, shape, ndim, device);
    } 
    else {
        float* result_data = (float*)malloc(tensor1->size * sizeof(float));
        if (result_data == NULL) {
            fprintf(stderr, "Memory allocation failed\n");
            exit(1);
        }
        add_tensor_cpu(tensor1, tensor2, result_data);
        return create_tensor(result_data, shape, ndim, device);
    }     
}
```

ç°åœ¨æˆ‘ä»¬çš„åº“æ”¯æŒ GPUï¼

```py
import norch

tensor1 = norch.Tensor([[1, 2, 3], [3, 2, 1]]).to("cuda")
tensor2 = norch.Tensor([[3, 2, 1], [1, 2, 3]]).to("cuda")

result = tensor1 + tensor2
```

# #3 â€” è‡ªåŠ¨å¾®åˆ†ï¼ˆAutogradï¼‰

PyTorch å˜å¾—å¦‚æ­¤å—æ¬¢è¿çš„ä¸»è¦åŸå› ä¹‹ä¸€æ˜¯å…¶ Autograd æ¨¡å—ã€‚è¿™æ˜¯ä¸€ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œå…è®¸è‡ªåŠ¨å¾®åˆ†ä»¥è®¡ç®—æ¢¯åº¦ï¼ˆå¯¹äºä½¿ç”¨æ¢¯åº¦ä¸‹é™ç­‰ä¼˜åŒ–ç®—æ³•è®­ç»ƒæ¨¡å‹è‡³å…³é‡è¦ï¼‰ã€‚é€šè¿‡è°ƒç”¨å•ä¸ªæ–¹æ³•`.backward()`ï¼Œå®ƒè®¡ç®—å‡ºå…ˆå‰å¼ é‡æ“ä½œçš„æ‰€æœ‰æ¢¯åº¦ï¼š

```py
x = torch.tensor([[1., 2, 3], [3., 2, 1]], requires_grad=True)
# [[1,  2,  3],
#  [3,  2., 1]]

y = torch.tensor([[3., 2, 1], [1., 2, 3]], requires_grad=True)
# [[3,  2, 1],
#  [1,  2, 3]]

L = ((x - y) ** 3).sum()

L.backward()

# You can access gradients of x and y
print(x.grad)
# [[12, 0, 12],
#  [12, 0, 12]]

print(y.grad)
# [[-12, 0, -12],
#  [-12, 0, -12]]

# In order to minimize z, you can use that for gradient descent:
# x = x - learning_rate * x.grad
# y = y - learning_rate * y.grad 
```

ä¸ºäº†ç†è§£å‘ç”Ÿäº†ä»€ä¹ˆï¼Œè®©æˆ‘ä»¬å°è¯•æ‰‹åŠ¨å¤åˆ¶ç›¸åŒçš„è¿‡ç¨‹ï¼š

![](img/255a2823dc15242c9d7414c9175d073d.png)

è®©æˆ‘ä»¬é¦–å…ˆè®¡ç®—ï¼š

![](img/3d5353e2d05c964798dfe75e81572db5.png)

è¯·æ³¨æ„*x*æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦åˆ†åˆ«è®¡ç®—*L*å¯¹æ¯ä¸ªå…ƒç´ çš„å¯¼æ•°ã€‚æ­¤å¤–ï¼Œ*L*æ˜¯æ‰€æœ‰å…ƒç´ çš„æ€»å’Œï¼Œä½†é‡è¦çš„æ˜¯è¦è®°ä½ï¼Œå¯¹äºæ¯ä¸ªå…ƒç´ ï¼Œå…¶ä»–å…ƒç´ ä¸ä¼šå½±å“å…¶å¯¼æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹é¡¹ï¼š

![](img/612ffe8a8a672f3c2ebac55743b1a82c.png)

é€šè¿‡ä¸ºæ¯ä¸ªé¡¹åº”ç”¨é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬åŒºåˆ†å¤–éƒ¨å‡½æ•°å¹¶ä¹˜ä»¥å†…éƒ¨å‡½æ•°çš„å¯¼æ•°ï¼š

![](img/97a8bb17dd8c8e8153ee9eb9d5b77606.png)

å…¶ä¸­ï¼š

![](img/0834be1c1e2772168593070107e386de.png)

æœ€åï¼š

![](img/4dec4240d0004de073e923b6d937831d.png)

å› æ­¤ï¼Œæˆ‘ä»¬æœ‰ä»¥ä¸‹æœ€ç»ˆæ–¹ç¨‹æ¥è®¡ç®—*L*ç›¸å¯¹äº*x*çš„å¯¼æ•°ï¼š

![](img/1f6843db3a0d35b765a74b1241ac61ae.png)

å°†å€¼ä»£å…¥æ–¹ç¨‹å¼ï¼š

![](img/95897f1570971bf15b239d7182f2b8cb.png)

è®¡ç®—ç»“æœï¼Œæˆ‘ä»¬å¾—åˆ°ä¸ PyTorch è·å¾—çš„ç›¸åŒå€¼ï¼š

![](img/ac68ee4bb235624b9c9c1c0df4a69ae4.png)

ç°åœ¨ï¼Œè®©æˆ‘ä»¬åˆ†æåˆšæ‰åšçš„äº‹æƒ…ï¼š

åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬æŒ‰ç…§ä¿ç•™é¡ºåºè§‚å¯Ÿäº†æ‰€æœ‰æ¶‰åŠçš„æ“ä½œï¼šæ±‚å’Œã€3 çš„å¹‚å’Œå‡æ³•ã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨é“¾å¼æ³•åˆ™ï¼Œè®¡ç®—æ¯ä¸ªæ“ä½œçš„å¯¼æ•°ï¼Œå¹¶é€’å½’åœ°è®¡ç®—ä¸‹ä¸€ä¸ªæ“ä½œçš„å¯¼æ•°ã€‚å› æ­¤ï¼Œé¦–å…ˆæˆ‘ä»¬éœ€è¦ä¸ºä¸åŒçš„æ•°å­¦æ“ä½œå®ç°å¯¼æ•°ï¼š

å¯¹äºåŠ æ³•ï¼š

![](img/f0bea287a54af0930d57584959d48ecd.png)

```py
# norch/autograd/functions.py

class AddBackward:
    def __init__(self, x, y):
        self.input = [x, y]

    def backward(self, gradient):
        return [gradient, gradient]
```

å¯¹äºæ­£å¼¦ï¼š

![](img/0751c6d99ffe7a0d1b1c359ffbcd68b4.png)

```py
# norch/autograd/functions.py

class SinBackward:
    def __init__(self, x):
        self.input = [x]

    def backward(self, gradient):
        x = self.input[0]
        return [x.cos() * gradient]
```

å¯¹äºä½™å¼¦ï¼š

![](img/4359a9a560ad9072af54cf872019b3bd.png)

```py
# norch/autograd/functions.py

class CosBackward:
    def __init__(self, x):
        self.input = [x]

    def backward(self, gradient):
        x = self.input[0]
        return [- x.sin() * gradient]
```

å¯¹äºé€å…ƒç´ ä¹˜æ³•ï¼š

![](img/3564658c2002c0917544fbbef510374c.png)

```py
# norch/autograd/functions.py

class ElementwiseMulBackward:
    def __init__(self, x, y):
        self.input = [x, y]

    def backward(self, gradient):
        x = self.input[0]
        y = self.input[1]
        return [y * gradient, x * gradient]
```

å¯¹äºæ±‚å’Œï¼š

```py
# norch/autograd/functions.py

class SumBackward:
    def __init__(self, x):
        self.input = [x]

    def backward(self, gradient):
        # Since sum reduces a tensor to a scalar, gradient is broadcasted to match the original shape.
        return [float(gradient.tensor.contents.data[0]) * self.input[0].ones_like()] 
```

æ‚¨å¯ä»¥åœ¨æ–‡ç« æœ«å°¾è®¿é—® GitHub å­˜å‚¨åº“é“¾æ¥ä»¥æ¢ç´¢å…¶ä»–æ“ä½œã€‚

ç°åœ¨æˆ‘ä»¬å¯¹æ¯ä¸ªæ“ä½œæœ‰äº†å¯¼æ•°è¡¨è¾¾å¼ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­å®ç°é€’å½’åå‘é“¾è§„åˆ™ã€‚æˆ‘ä»¬å¯ä»¥ä¸ºæˆ‘ä»¬çš„å¼ é‡è®¾ç½®ä¸€ä¸ª`requires_grad`å‚æ•°ï¼Œä»¥æŒ‡ç¤ºæˆ‘ä»¬è¦å­˜å‚¨æ­¤å¼ é‡çš„æ¢¯åº¦ã€‚å¦‚æœä¸ºçœŸï¼Œæˆ‘ä»¬å°†ä¸ºæ¯ä¸ªå¼ é‡æ“ä½œå­˜å‚¨æ¢¯åº¦ã€‚ä¾‹å¦‚ï¼š

```py
# norch/tensor.py

def __add__(self, other):

  if self.shape != other.shape:
      raise ValueError("Tensors must have the same shape for addition")

  Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]
  Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)

  result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)

  result_data = Tensor()
  result_data.tensor = result_tensor_ptr
  result_data.shape = self.shape.copy()
  result_data.ndim = self.ndim
  result_data.device = self.device

  result_data.requires_grad = self.requires_grad or other.requires_grad
  if result_data.requires_grad:
      result_data.grad_fn = AddBackward(self, other)
```

ç„¶åï¼Œå®ç°`.backward()`æ–¹æ³•ï¼š

```py
# norch/tensor.py

def backward(self, gradient=None):
    if not self.requires_grad:
        return

    if gradient is None:
        if self.shape == [1]:
            gradient = Tensor([1]) # dx/dx = 1 case
        else:
            raise RuntimeError("Gradient argument must be specified for non-scalar tensors.")

    if self.grad is None:
        self.grad = gradient

    else:
        self.grad += gradient

    if self.grad_fn is not None: # not a leaf
        grads = self.grad_fn.backward(gradient) # call the operation backward
        for tensor, grad in zip(self.grad_fn.input, grads):
            if isinstance(tensor, Tensor):
                tensor.backward(grad) # recursively call the backward again for the gradient expression (chain rule)
```

æœ€åï¼Œåªéœ€å®ç°`.zero_grad()`å°†å¼ é‡çš„æ¢¯åº¦å½’é›¶ï¼Œ`.detach()`å°†ç§»é™¤å¼ é‡çš„è‡ªåŠ¨æ¢¯åº¦å†å²ï¼š

```py
# norch/tensor.py

def zero_grad(self):
    self.grad = None

def detach(self):
    self.grad = None
    self.grad_fn = None
```

æ­å–œï¼æ‚¨åˆšåˆšåˆ›å»ºäº†ä¸€ä¸ªå®Œæ•´çš„å¼ é‡åº“ï¼Œæ”¯æŒ GPU å’Œè‡ªåŠ¨å¾®åˆ†ï¼ç°åœ¨æˆ‘ä»¬å¯ä»¥åˆ›å»º`nn`å’Œ`optim`æ¨¡å—ï¼Œæ›´è½»æ¾åœ°è®­ç»ƒä¸€äº›æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

# #4 â€” nn å’Œ optim æ¨¡å—

`nn`æ˜¯ç”¨äºæ„å»ºç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ¨¡å—ï¼Œè€Œ`optim`ä¸ä¼˜åŒ–ç®—æ³•ç›¸å…³ï¼Œç”¨äºè®­ç»ƒè¿™äº›æ¨¡å‹ã€‚ä¸ºäº†é‡æ–°åˆ›å»ºå®ƒä»¬ï¼Œé¦–å…ˆè¦å®ç°ä¸€ä¸ª Parameterï¼Œå®ƒåªæ˜¯ä¸€ä¸ªå¯è®­ç»ƒçš„å¼ é‡ï¼Œå…·æœ‰ç›¸åŒçš„æ“ä½œï¼Œä½†`requires_grad`å§‹ç»ˆè®¾ç½®ä¸º`True`ï¼Œå¹¶é‡‡ç”¨ä¸€äº›éšæœºåˆå§‹åŒ–æŠ€æœ¯ã€‚

```py
# norch/nn/parameter.py

from norch.tensor import Tensor
from norch.utils import utils
import random

class Parameter(Tensor):
    """
    A parameter is a trainable tensor.
    """
    def __init__(self, shape):
        data = utils.generate_random_list(shape=shape)
        super().__init__(data, requires_grad=True)
```

```py
# norch/utisl/utils.py

def generate_random_list(shape):
    """
    Generate a list with random numbers and shape 'shape'
    [4, 2] --> [[rand1, rand2], [rand3, rand4], [rand5, rand6], [rand7, rand8]]
    """
    if len(shape) == 0:
        return []
    else:
        inner_shape = shape[1:]
        if len(inner_shape) == 0:
            return [random.uniform(-1, 1) for _ in range(shape[0])]
        else:
            return [generate_random_list(inner_shape) for _ in range(shape[0])]
```

é€šè¿‡ä½¿ç”¨å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹æ„å»ºæ¨¡å—ï¼š

```py
# norch/nn/module.py

from .parameter import Parameter
from collections import OrderedDict
from abc import ABC
import inspect

class Module(ABC):
    """
    Abstract class for modules
    """
    def __init__(self):
        self._modules = OrderedDict()
        self._params = OrderedDict()
        self._grads = OrderedDict()
        self.training = True

    def forward(self, *inputs, **kwargs):
        raise NotImplementedError

    def __call__(self, *inputs, **kwargs):
        return self.forward(*inputs, **kwargs)

    def train(self):
        self.training = True
        for param in self.parameters():
            param.requires_grad = True

    def eval(self):
        self.training = False
        for param in self.parameters():
            param.requires_grad = False

    def parameters(self):
        for name, value in inspect.getmembers(self):
            if isinstance(value, Parameter):
                yield self, name, value
            elif isinstance(value, Module):
                yield from value.parameters()

    def modules(self):
        yield from self._modules.values()

    def gradients(self):
        for module in self.modules():
            yield module._grads

    def zero_grad(self):
        for _, _, parameter in self.parameters():
            parameter.zero_grad()

    def to(self, device):
        for _, _, parameter in self.parameters():
            parameter.to(device)

        return self

    def inner_repr(self):
        return ""

    def __repr__(self):
        string = f"{self.get_name()}("
        tab = "   "
        modules = self._modules
        if modules == {}:
            string += f'\n{tab}(parameters): {self.inner_repr()}'
        else:
            for key, module in modules.items():
                string += f"\n{tab}({key}): {module.get_name()}({module.inner_repr()})"
        return f'{string}\n)'

    def get_name(self):
        return self.__class__.__name__

    def __setattr__(self, key, value):
        self.__dict__[key] = value

        if isinstance(value, Module):
            self._modules[key] = value
        elif isinstance(value, Parameter):
            self._params[key] = value
```

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç»§æ‰¿`nn.Module`æ¥æ„å»ºè‡ªå®šä¹‰æ¨¡å—ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€äº›å…ˆå‰åˆ›å»ºçš„æ¨¡å—ï¼Œæ¯”å¦‚`linear`ï¼Œå®ƒå®ç°äº†***y*** *= W****x*** *+ b*çš„æ“ä½œã€‚

```py
# norch/nn/modules/linear.py

from ..module import Module
from ..parameter import Parameter

class Linear(Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.weight = Parameter(shape=[self.output_dim, self.input_dim])
        self.bias = Parameter(shape=[self.output_dim, 1])

    def forward(self, x):
        z = self.weight @ x + self.bias
        return z

    def inner_repr(self):
        return f"input_dim={self.input_dim}, output_dim={self.output_dim}, " \
               f"bias={True if self.bias is not None else False}"
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥å®ç°ä¸€äº›æŸå¤±å’Œæ¿€æ´»å‡½æ•°ã€‚ä¾‹å¦‚ï¼Œå‡æ–¹è¯¯å·®æŸå¤±å’Œ sigmoid å‡½æ•°ï¼š

```py
# norch/nn/loss.py

from .module import Module

class MSELoss(Module):
    def __init__(self):
      pass

    def forward(self, predictions, labels):
        assert labels.shape == predictions.shape, \
            "Labels and predictions shape does not match: {} and {}".format(labels.shape, predictions.shape)

        return ((predictions - labels) ** 2).sum() / predictions.numel

    def __call__(self, *inputs):
        return self.forward(*inputs)
```

```py
# norch/nn/activation.py

from .module import Module
import math

class Sigmoid(Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 1.0 / (1.0 + (math.e) ** (-x)) 
```

æœ€åï¼Œåˆ›å»ºä¼˜åŒ–å™¨ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘å°†å®ç°éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼š

```py
# norch/optim/optimizer.py

from abc import ABC
from norch.tensor import Tensor

class Optimizer(ABC):
    """
    Abstract class for optimizers
    """

    def __init__(self, parameters):
        if isinstance(parameters, Tensor):
            raise TypeError("parameters should be an iterable but got {}".format(type(parameters)))
        elif isinstance(parameters, dict):
            parameters = parameters.values()

        self.parameters = list(parameters)

    def step(self):
        raise NotImplementedError

    def zero_grad(self):
        for module, name, parameter in self.parameters:
            parameter.zero_grad()

class SGD(Optimizer):
    def __init__(self, parameters, lr=1e-1, momentum=0):
        super().__init__(parameters)
        self.lr = lr
        self.momentum = momentum
        self._cache = {'velocity': [p.zeros_like() for (_, _, p) in self.parameters]}

    def step(self):
        for i, (module, name, _) in enumerate(self.parameters):
            parameter = getattr(module, name)

            velocity = self._cache['velocity'][i]

            velocity = self.momentum * velocity - self.lr * parameter.grad

            updated_parameter = parameter + velocity

            setattr(module, name, updated_parameter)

            self._cache['velocity'][i] = velocity

            parameter.detach()
            velocity.detach()
```

å°±æ˜¯è¿™æ ·ï¼æˆ‘ä»¬åˆšåˆšåˆ›å»ºäº†è‡ªå·±çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ğŸ¥³

è®©æˆ‘ä»¬è¿›è¡Œä¸€äº›è®­ç»ƒï¼š

```py
import norch
import norch.nn as nn
import norch.optim as optim
import random
import math

random.seed(1)

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.fc1 = nn.Linear(1, 10)
        self.sigmoid = nn.Sigmoid()
        self.fc2 = nn.Linear(10, 1)

    def forward(self, x):
        out = self.fc1(x)
        out = self.sigmoid(out)
        out = self.fc2(out)

        return out

device = "cuda"
epochs = 10

model = MyModel().to(device)
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.001)
loss_list = []

x_values = [0\. ,  0.4,  0.8,  1.2,  1.6,  2\. ,  2.4,  2.8,  3.2,  3.6,  4\. ,
        4.4,  4.8,  5.2,  5.6,  6\. ,  6.4,  6.8,  7.2,  7.6,  8\. ,  8.4,
        8.8,  9.2,  9.6, 10\. , 10.4, 10.8, 11.2, 11.6, 12\. , 12.4, 12.8,
       13.2, 13.6, 14\. , 14.4, 14.8, 15.2, 15.6, 16\. , 16.4, 16.8, 17.2,
       17.6, 18\. , 18.4, 18.8, 19.2, 19.6, 20.]

y_true = []
for x in x_values:
    y_true.append(math.pow(math.sin(x), 2))

for epoch in range(epochs):
    for x, target in zip(x_values, y_true):
        x = norch.Tensor([[x]]).T
        target = norch.Tensor([[target]]).T

        x = x.to(device)
        target = target.to(device)

        outputs = model(x)
        loss = criterion(outputs, target)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss[0]:.4f}')
    loss_list.append(loss[0])

# Epoch [1/10], Loss: 1.7035
# Epoch [2/10], Loss: 0.7193
# Epoch [3/10], Loss: 0.3068
# Epoch [4/10], Loss: 0.1742
# Epoch [5/10], Loss: 0.1342
# Epoch [6/10], Loss: 0.1232
# Epoch [7/10], Loss: 0.1220
# Epoch [8/10], Loss: 0.1241
# Epoch [9/10], Loss: 0.1270
# Epoch [10/10], Loss: 0.1297
```

![](img/e7662c657102cedd3717cdac495019f6.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

è¯¥æ¨¡å‹å·²æˆåŠŸä½¿ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶åˆ›å»ºå’Œè®­ç»ƒï¼

æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/lucasdelimanogueira/PyNorch)æŸ¥çœ‹å®Œæ•´çš„ä»£ç ã€‚

# ç»“è®º

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¼ é‡çš„åŸºæœ¬æ¦‚å¿µï¼Œå®ƒæ˜¯å¦‚ä½•å»ºæ¨¡çš„ï¼Œä»¥åŠæ›´é«˜çº§çš„ä¸»é¢˜ï¼Œå¦‚ CUDA å’Œ Autogradã€‚æˆ‘ä»¬æˆåŠŸåœ°åˆ›å»ºäº†ä¸€ä¸ªæ”¯æŒ GPU å’Œè‡ªåŠ¨å¾®åˆ†çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚å¸Œæœ›æœ¬æ–‡èƒ½å¸®åŠ©æ‚¨ç®€è¦äº†è§£ PyTorch åœ¨å¹•åæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

åœ¨æœªæ¥çš„å¸–å­ä¸­ï¼Œæˆ‘å°†å°è¯•æ¶µç›–æ›´é«˜çº§çš„ä¸»é¢˜ï¼Œå¦‚åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¤šèŠ‚ç‚¹/å¤š GPUï¼‰å’Œå†…å­˜ç®¡ç†ã€‚è¯·åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘æ‚¨çš„æƒ³æ³•æˆ–æ‚¨å¸Œæœ›æˆ‘å†™ä¸‹çš„å†…å®¹ï¼éå¸¸æ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼ğŸ˜Š

æ­¤å¤–ï¼Œè¯·å…³æ³¨æˆ‘çš„[LinkedIn ä¸ªäººèµ„æ–™](https://www.linkedin.com/in/lucas-de-lima-nogueira/)ï¼Œä»¥è·å–æœ€æ–°æ–‡ç« æ›´æ–°ï¼

# å‚è€ƒèµ„æ–™

[PyNorch](https://github.com/lucasdelimanogueira/PyNorch) â€” æœ¬é¡¹ç›®çš„ GitHub å­˜å‚¨åº“ã€‚

[CUDA æ•™ç¨‹](https://medium.com/towards-data-science/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66) â€” CUDA å·¥ä½œåŸç†ç®€ä»‹ã€‚

[PyTorch](https://pytorch.org/docs/stable/index.html) â€” PyTorch æ–‡æ¡£ã€‚

[MartinLwx çš„åšå®¢](https://martinlwx.github.io/en/how-to-reprensent-a-tensor-or-ndarray/) â€” å…³äºæ­¥å¹…çš„æ•™ç¨‹ã€‚

[æ­¥å¹…æ•™ç¨‹](https://ajcr.net/stride-guide-part-1/) â€” å…³äºæ­¥å¹…çš„å¦ä¸€ä¸ªæ•™ç¨‹ã€‚

[PyTorch å†…éƒ¨](http://blog.ezyang.com/2019/05/pytorch-internals/) â€” PyTorch çš„ç»“æ„æŒ‡å—ã€‚

[Nets](https://github.com/arthurdjn/nets) â€” ä½¿ç”¨ NumPy é‡å»ºçš„ PyTorchã€‚

[Autograd](https://www.youtube.com/watch?v=RxmBukb-Om4&t=935s) â€” Autograd åº“çš„å®æ—¶ç¼–ç ã€‚
