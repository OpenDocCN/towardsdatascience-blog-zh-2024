- en: Fine-Tune the Audio Spectrogram Transformer with Hugging Face Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Hugging Face Transformers å¾®è°ƒéŸ³é¢‘å…‰è°±å›¾å˜æ¢å™¨
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-the-audio-spectrogram-transformer-with-transformers-73333c9ef717?source=collection_archive---------4-----------------------#2024-08-21](https://towardsdatascience.com/fine-tune-the-audio-spectrogram-transformer-with-transformers-73333c9ef717?source=collection_archive---------4-----------------------#2024-08-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-the-audio-spectrogram-transformer-with-transformers-73333c9ef717?source=collection_archive---------4-----------------------#2024-08-21](https://towardsdatascience.com/fine-tune-the-audio-spectrogram-transformer-with-transformers-73333c9ef717?source=collection_archive---------4-----------------------#2024-08-21)
- en: Learn how to fine-tune the Audio Spectrogram Transformer model for audio classification
    of your own data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•å¾®è°ƒéŸ³é¢‘å…‰è°±å›¾å˜æ¢å™¨æ¨¡å‹ï¼Œä»¥ä¾¿è¿›è¡Œæ‚¨è‡ªå·±çš„æ•°æ®éŸ³é¢‘åˆ†ç±»
- en: '[](https://medium.com/@marius_s?source=post_page---byline--73333c9ef717--------------------------------)[![Marius
    Steger](../Images/9dff217a20fc1542eac8e52d32048114.png)](https://medium.com/@marius_s?source=post_page---byline--73333c9ef717--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--73333c9ef717--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--73333c9ef717--------------------------------)
    [Marius Steger](https://medium.com/@marius_s?source=post_page---byline--73333c9ef717--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marius_s?source=post_page---byline--73333c9ef717--------------------------------)[![Marius
    Steger](../Images/9dff217a20fc1542eac8e52d32048114.png)](https://medium.com/@marius_s?source=post_page---byline--73333c9ef717--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--73333c9ef717--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--73333c9ef717--------------------------------)
    [Marius Steger](https://medium.com/@marius_s?source=post_page---byline--73333c9ef717--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--73333c9ef717--------------------------------)
    Â·13 min readÂ·Aug 21, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--73333c9ef717--------------------------------)
    Â·13 åˆ†é’Ÿé˜…è¯»Â·2024å¹´8æœˆ21æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7f8ad4be45f2ff60c16110e638acb74c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f8ad4be45f2ff60c16110e638acb74c.png)'
- en: Fine-tuning an audio classification model instead of training from scratch can
    be more data efficient, leading to better results on the downstream task | *Image
    by author*
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®è°ƒéŸ³é¢‘åˆ†ç±»æ¨¡å‹ï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒï¼Œèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°ä½¿ç”¨æ•°æ®ï¼Œä»è€Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è·å¾—æ›´å¥½çš„ç»“æœ | *å›¾åƒæ¥è‡ªä½œè€…*
- en: Audio classification is one of the key tasks in audio understanding with Machine
    Learning and serves as a building block for many AI systems. It powers industry
    applications for [test data evaluation](https://renumics.com/use-cases/test-data)
    in the engineering domain, error and anomaly detection, or predictive maintenance.
    Pre-trained transformer models, like the Audio Spectrogram Transformer (AST)[1],
    provide a powerful foundation for these applications, offering robustness and
    flexibility.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: éŸ³é¢‘åˆ†ç±»æ˜¯æœºå™¨å­¦ä¹ ä¸­éŸ³é¢‘ç†è§£çš„å…³é”®ä»»åŠ¡ä¹‹ä¸€ï¼Œå¹¶ä¸ºè®¸å¤š AI ç³»ç»Ÿæä¾›äº†æ„å»ºåŸºç¡€ã€‚å®ƒé©±åŠ¨ç€å·¥ä¸šé¢†åŸŸçš„åº”ç”¨ï¼Œä¾‹å¦‚[æµ‹è¯•æ•°æ®è¯„ä¼°](https://renumics.com/use-cases/test-data)ã€é”™è¯¯å’Œå¼‚å¸¸æ£€æµ‹ï¼Œæˆ–é¢„æµ‹æ€§ç»´æŠ¤ã€‚é¢„è®­ç»ƒçš„å˜æ¢å™¨æ¨¡å‹ï¼Œå¦‚éŸ³é¢‘å…‰è°±å›¾å˜æ¢å™¨ï¼ˆASTï¼‰[1]ï¼Œä¸ºè¿™äº›åº”ç”¨æä¾›äº†å¼ºå¤§çš„åŸºç¡€ï¼Œå…·æœ‰é²æ£’æ€§å’Œçµæ´»æ€§ã€‚
- en: While training an AST model from scratch would require a huge amount of data,
    using a pretrained model that has already learned audio-specific features can
    be more efficient. Fine-tuning these models with data specific to our use case
    is essential to enable their use for our particular application. This process
    adapts the modelâ€™s capabilities to the unique characteristics of our dataset,
    such as classes and data distribution, ensuring the relevance of the results.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ª AST æ¨¡å‹éœ€è¦å¤§é‡æ•°æ®ï¼Œä½†ä½¿ç”¨å·²ç»å­¦ä¹ äº†éŸ³é¢‘ç‰¹å¾çš„é¢„è®­ç»ƒæ¨¡å‹ä¼šæ›´é«˜æ•ˆã€‚é€šè¿‡ä½¿ç”¨ç‰¹å®šäºæˆ‘ä»¬ç”¨ä¾‹çš„æ•°æ®å¾®è°ƒè¿™äº›æ¨¡å‹ï¼Œå¯¹äºä½¿å…¶é€‚ç”¨äºæˆ‘ä»¬çš„ç‰¹å®šåº”ç”¨ç¨‹åºè‡³å…³é‡è¦ã€‚è¿™ä¸ªè¿‡ç¨‹å°†æ¨¡å‹çš„èƒ½åŠ›è°ƒæ•´ä¸ºæˆ‘ä»¬æ•°æ®é›†çš„ç‹¬ç‰¹ç‰¹å¾ï¼Œå¦‚ç±»åˆ«å’Œæ•°æ®åˆ†å¸ƒï¼Œç¡®ä¿ç»“æœçš„ç›¸å…³æ€§ã€‚
- en: '![](../Images/895d4939ab3d1eb6e2da381a10bcfbe0.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/895d4939ab3d1eb6e2da381a10bcfbe0.png)'
- en: The Audio Spectrogram Transformer predicts a class for an audio sample based
    on its spectrogram | *Image by author*
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: éŸ³é¢‘å…‰è°±å›¾å˜æ¢å™¨æ ¹æ®éŸ³é¢‘æ ·æœ¬çš„å…‰è°±å›¾é¢„æµ‹ç±»åˆ« | *å›¾åƒæ¥è‡ªä½œè€…*
- en: The AST model, integrated with the Hugging Face ğŸ¤— [Transformers](https://huggingface.co/docs/transformers/index)
    library, has become a popular choice due to its ease of use and strong performance
    in audio classification tasks. This guide will take us through the entire process
    of fine-tuning a pretrained AST model (â€œ[*MIT/ast-finetuned-audioset-10â€“10â€“0.4593*](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)*â€*)
    using our own data, demonstrated with the [ESC50 dataset](https://github.com/karolpiczak/ESC-50)[2].
    Using tools from the Hugging Face ecosystem and PyTorch as the backend, we will
    cover everything from data preparation and preprocessing to model configuration
    and training.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ASTæ¨¡å‹ä¸Hugging Face ğŸ¤— [Transformers](https://huggingface.co/docs/transformers/index)åº“é›†æˆï¼Œç”±äºåœ¨éŸ³é¢‘åˆ†ç±»ä»»åŠ¡ä¸­æ˜“äºä½¿ç”¨ä¸”æ€§èƒ½å¼ºå¤§ï¼Œå·²æˆä¸ºçƒ­é—¨é€‰æ‹©ã€‚æœ¬æŒ‡å—å°†å¸¦é¢†æˆ‘ä»¬å®Œæˆå¯¹é¢„è®­ç»ƒASTæ¨¡å‹ï¼ˆâ€œ[*MIT/ast-finetuned-audioset-10â€“10â€“0.4593*](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)*â€*ï¼‰è¿›è¡Œå¾®è°ƒçš„æ•´ä¸ªè¿‡ç¨‹ï¼Œä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„æ•°æ®ï¼Œåœ¨[ESC50æ•°æ®é›†](https://github.com/karolpiczak/ESC-50)[2]ä¸Šè¿›è¡Œæ¼”ç¤ºã€‚åˆ©ç”¨Hugging
    Faceç”Ÿæ€ç³»ç»Ÿå’ŒPyTorchä½œä¸ºåç«¯çš„å·¥å…·ï¼Œæˆ‘ä»¬å°†æ¶µç›–ä»æ•°æ®å‡†å¤‡å’Œé¢„å¤„ç†åˆ°æ¨¡å‹é…ç½®å’Œè®­ç»ƒçš„æ‰€æœ‰å†…å®¹ã€‚
- en: I am writing this guide based on my professional experience with the AST model
    and the Hugging Face ecosystem over the past years.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘æ ¹æ®è¿‡å»å‡ å¹´ä¸ASTæ¨¡å‹å’ŒHugging Faceç”Ÿæ€ç³»ç»Ÿçš„ä¸“ä¸šç»éªŒæ’°å†™äº†è¿™ä»½æŒ‡å—ã€‚
- en: This tutorial will guide us through the process of fine-tuning the AST on our
    own audio classification dataset with tooling from the Hugging Face ecosystem.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹å°†æŒ‡å¯¼æˆ‘ä»¬ä½¿ç”¨Hugging Faceç”Ÿæ€ç³»ç»Ÿçš„å·¥å…·å¯¹è‡ªå·±çš„éŸ³é¢‘åˆ†ç±»æ•°æ®é›†ä¸Šçš„ASTè¿›è¡Œå¾®è°ƒã€‚
- en: We will load the data (1), preprocess the audios (2), setup audio augmentations
    (3), configure and initialize the AST model (4) and finally, configure and start
    a training (5).
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åŠ è½½æ•°æ®ï¼ˆ1ï¼‰ï¼Œé¢„å¤„ç†éŸ³é¢‘ï¼ˆ2ï¼‰ï¼Œè®¾ç½®éŸ³é¢‘å¢å¼ºï¼ˆ3ï¼‰ï¼Œé…ç½®å’Œåˆå§‹åŒ–ASTæ¨¡å‹ï¼ˆ4ï¼‰ï¼Œæœ€åï¼Œé…ç½®å’Œå¼€å§‹è®­ç»ƒï¼ˆ5ï¼‰ã€‚
- en: Step-by-Step Guide to Fine-Tune the AST
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¾®è°ƒASTçš„é€æ­¥æŒ‡å—
- en: 'Before we start, install all the required packages with pip:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ä½¿ç”¨pipå®‰è£…æ‰€æœ‰å¿…éœ€çš„è½¯ä»¶åŒ…ï¼š
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 1\. Load Our Data in the Correct Format
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. ä»¥æ­£ç¡®æ ¼å¼åŠ è½½æˆ‘ä»¬çš„æ•°æ®
- en: To start, weâ€™ll use the Hugging Face ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/index)
    library to manage our data. This library will assist us in preprocessing, storing,
    and accessing data during training, as well as performing waveform transformations
    and encoding into spectrograms on the fly.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Hugging Face ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/index)åº“æ¥ç®¡ç†æˆ‘ä»¬çš„æ•°æ®ã€‚è¯¥åº“å°†ååŠ©æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œé¢„å¤„ç†ã€å­˜å‚¨å’Œè®¿é—®æ•°æ®ï¼Œä»¥åŠåœ¨éœ€è¦æ—¶æ‰§è¡Œæ³¢å½¢è½¬æ¢å¹¶å®æ—¶ç¼–ç ä¸ºé¢‘è°±å›¾ã€‚
- en: 'Our data should be loaded into a `Dataset` object with the following structure:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ•°æ®åº”åŠ è½½åˆ°å…·æœ‰ä»¥ä¸‹ç»“æ„çš„`Dataset`å¯¹è±¡ä¸­ï¼š
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the following two sections I will demonstrate how to load a prepared dataset
    from the ğŸ¤— Hub and also create a `*Dataset*` from local audio data and labels.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ä¸¤ä¸ªéƒ¨åˆ†ä¸­ï¼Œæˆ‘å°†æ¼”ç¤ºå¦‚ä½•ä»ğŸ¤— HubåŠ è½½å‡†å¤‡å¥½çš„æ•°æ®é›†ï¼Œä»¥åŠå¦‚ä½•ä»æœ¬åœ°éŸ³é¢‘æ•°æ®å’Œæ ‡ç­¾åˆ›å»ºä¸€ä¸ª`*Dataset*`ã€‚
- en: '**Loading a Dataset from the Hugging Face Hub:** If we donâ€™t have an audio
    dataset locally, we can conveniently load one from the Hugging Face Hub using
    the `load_dataset` function.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»Hugging Face HubåŠ è½½æ•°æ®é›†ï¼š** å¦‚æœæˆ‘ä»¬æ²¡æœ‰æœ¬åœ°éŸ³é¢‘æ•°æ®é›†ï¼Œå¯ä»¥æ–¹ä¾¿åœ°ä½¿ç”¨`load_dataset`å‡½æ•°ä»Hugging Face
    HubåŠ è½½æ•°æ®é›†ã€‚'
- en: 'In this guide we will load the ESC50 Audio Classification dataset for demonstration
    purposes:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†åŠ è½½ESC50éŸ³é¢‘åˆ†ç±»æ•°æ®é›†ä»¥è¿›è¡Œæ¼”ç¤ºï¼š
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/d311793e7f3a774f4c6cbf257a504f74.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d311793e7f3a774f4c6cbf257a504f74.png)'
- en: The spectrograms (top) and waveforms (bottom) of different classes from the
    ESC50 Dataset | *Image by author (created with* [*Spotlight*](https://github.com/Renumics/spotlight)*)*
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ESC50æ•°æ®é›†ä¸­ä¸åŒç±»åˆ«çš„é¢‘è°±å›¾ï¼ˆé¡¶éƒ¨ï¼‰å’Œæ³¢å½¢å›¾ï¼ˆåº•éƒ¨ï¼‰| *ä½œè€…åˆ›å»ºçš„å›¾åƒï¼ˆä½¿ç”¨* [*Spotlight*](https://github.com/Renumics/spotlight)*)*
- en: '**Loading Local Audio Files and Labels:** We can load our audio files and associated
    labels into a `Dataset` object using a dictionary or a pandas DataFrame that contains
    file paths and labels. If we have a mapping of class names (strings) to label
    indices (integers), this information can be included during dataset construction.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŠ è½½æœ¬åœ°éŸ³é¢‘æ–‡ä»¶å’Œæ ‡ç­¾ï¼š** æˆ‘ä»¬å¯ä»¥ä½¿ç”¨åŒ…å«æ–‡ä»¶è·¯å¾„å’Œæ ‡ç­¾çš„å­—å…¸æˆ–pandas DataFrameå°†éŸ³é¢‘æ–‡ä»¶å’Œç›¸å…³æ ‡ç­¾åŠ è½½åˆ°`Dataset`å¯¹è±¡ä¸­ã€‚å¦‚æœæˆ‘ä»¬æœ‰ç±»åï¼ˆå­—ç¬¦ä¸²ï¼‰åˆ°æ ‡ç­¾ç´¢å¼•ï¼ˆæ•´æ•°ï¼‰çš„æ˜ å°„ï¼Œè¿™äº›ä¿¡æ¯å¯ä»¥åœ¨æ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­åŒ…å«ã€‚'
- en: 'Hereâ€™s a practical example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå®é™…ç¤ºä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼š
- en: The `Audio` feature class automatically handles audio file loading and processing.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Audio`ç‰¹å¾ç±»ä¼šè‡ªåŠ¨å¤„ç†éŸ³é¢‘æ–‡ä»¶çš„åŠ è½½å’Œå¤„ç†ã€‚'
- en: '`ClassLabel` helps manage categorical labels, making it easier to handle classes
    during training and evaluation.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ClassLabel`æœ‰åŠ©äºç®¡ç†åˆ†ç±»æ ‡ç­¾ï¼Œä½¿åœ¨è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­æ›´å®¹æ˜“å¤„ç†ç±»åˆ«ã€‚'
- en: '**Note:** For more information on loading audio with Hugging Face, have a look
    at the Datasets library [Docs](https://huggingface.co/docs/datasets/audio_load).'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** æœ‰å…³å¦‚ä½•ä½¿ç”¨Hugging FaceåŠ è½½éŸ³é¢‘çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹Datasetsåº“çš„[æ–‡æ¡£](https://huggingface.co/docs/datasets/audio_load)ã€‚'
- en: '**Inspecting the Dataset:** Once the dataset is successfully loaded, each audio
    sample is accessible via an `Audio` feature class, which optimizes data handling
    by loading it into memory only when needed. This efficient management saves computational
    resources and speeds up the training process.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ£€æŸ¥æ•°æ®é›†ï¼š** ä¸€æ—¦æ•°æ®é›†æˆåŠŸåŠ è½½ï¼Œæ¯ä¸ªéŸ³é¢‘æ ·æœ¬éƒ½å¯ä»¥é€šè¿‡`Audio`ç‰¹å¾ç±»è¿›è¡Œè®¿é—®ï¼Œ`Audio`ç‰¹å¾ç±»é€šè¿‡ä»…åœ¨éœ€è¦æ—¶å°†å…¶åŠ è½½åˆ°å†…å­˜ä¸­æ¥ä¼˜åŒ–æ•°æ®å¤„ç†ã€‚è¿™ç§é«˜æ•ˆçš„ç®¡ç†èŠ‚çœäº†è®¡ç®—èµ„æºï¼Œå¹¶åŠ é€Ÿäº†è®­ç»ƒè¿‡ç¨‹ã€‚'
- en: 'To get a better understanding of the data structure and ensure everything is
    loaded correctly, we can inspect individual samples in the dataset:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£æ•°æ®ç»“æ„å¹¶ç¡®ä¿ä¸€åˆ‡æ­£ç¡®åŠ è½½ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥æ•°æ®é›†ä¸­å•ä¸ªæ ·æœ¬ï¼š
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Output example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºç¤ºä¾‹ï¼š
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This output shows the path, waveform data array, and the sampling rate for the
    audio file, along with its corresponding label.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è¾“å‡ºæ˜¾ç¤ºäº†éŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„ã€æ³¢å½¢æ•°æ®æ•°ç»„ä»¥åŠé‡‡æ ·ç‡ï¼Œå¹¶é™„ä¸Šç›¸åº”çš„æ ‡ç­¾ã€‚
- en: For the following steps, you can either use a prepared dataset as demo like
    we do or continue with your own dataset.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¯¹äºæ¥ä¸‹æ¥çš„æ­¥éª¤ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åƒæˆ‘ä»¬è¿™æ ·å‡†å¤‡å¥½çš„æ•°æ®é›†ä½œä¸ºç¤ºä¾‹ï¼Œä¹Ÿå¯ä»¥ç»§ç»­ä½¿ç”¨æ‚¨è‡ªå·±çš„æ•°æ®é›†ã€‚
- en: 2\. Preprocess the audio data
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. é¢„å¤„ç†éŸ³é¢‘æ•°æ®
- en: 'If our dataset is from the Hugging Face Hub, we cast the `*audio*` and `*labels*`
    columns to the correct feature types:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çš„æ•°æ®é›†æ¥è‡ªHugging Face Hubï¼Œæˆ‘ä»¬å°†`*audio*`å’Œ`*labels*`åˆ—è½¬æ¢ä¸ºæ­£ç¡®çš„ç‰¹å¾ç±»å‹ï¼š
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™æ®µä»£ç ä¸­ï¼š
- en: '**Audio Casting**: The `Audio` feature handles loading and processing audio
    files, resampling them to the desired sampling rate (16kHz in this case, sampling
    rate of the `ASTFeatureExtractor`).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éŸ³é¢‘è½¬æ¢ï¼š** `Audio`ç‰¹å¾ç±»å¤„ç†éŸ³é¢‘æ–‡ä»¶çš„åŠ è½½å’Œå¤„ç†ï¼Œå¹¶å°†å…¶é‡æ–°é‡‡æ ·åˆ°æ‰€éœ€çš„é‡‡æ ·ç‡ï¼ˆæ­¤å¤„ä¸º16kHzï¼Œå³`ASTFeatureExtractor`çš„é‡‡æ ·ç‡ï¼‰ã€‚'
- en: '**ClassLabel Casting**: The `ClassLabel` feature maps integers to labels and
    vice versa.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç±»åˆ«æ ‡ç­¾è½¬æ¢ï¼š** `ClassLabel`ç‰¹å¾å°†æ•´æ•°æ˜ å°„åˆ°æ ‡ç­¾ï¼Œåä¹‹äº¦ç„¶ã€‚'
- en: '![](../Images/5406a4f6a58ec46f62f5875bafeab4cb.png)![](../Images/113a08208bca4a7f8e5b2e3d018054c5.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5406a4f6a58ec46f62f5875bafeab4cb.png)![](../Images/113a08208bca4a7f8e5b2e3d018054c5.png)'
- en: An audio array as waveform (left) and as spectrogram (right) | *Image by author*
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªéŸ³é¢‘æ•°ç»„ä½œä¸ºæ³¢å½¢ï¼ˆå·¦ï¼‰å’Œé¢‘è°±å›¾ï¼ˆå³ï¼‰ | *å›¾ç‰‡ç”±ä½œè€…æä¾›*
- en: '**Preparing for AST Model Inputs:** The AST model requires spectrogram inputs,
    so we need to encode our waveforms into a format that the model can process. This
    is achieved using the `ASTFeatureExtractor`, which is instantiated from the configuration
    of the pretrained model we intend to fine-tune on our dataset.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸ºASTæ¨¡å‹è¾“å…¥åšå‡†å¤‡ï¼š** ASTæ¨¡å‹éœ€è¦é¢‘è°±å›¾è¾“å…¥ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°†æ³¢å½¢ç¼–ç ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ ¼å¼ã€‚è¿™æ˜¯é€šè¿‡ä½¿ç”¨`ASTFeatureExtractor`æ¥å®ç°çš„ï¼Œè¯¥æå–å™¨æ˜¯ä»æˆ‘ä»¬æ‰“ç®—åœ¨æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒçš„é¢„è®­ç»ƒæ¨¡å‹çš„é…ç½®ä¸­å®ä¾‹åŒ–çš„ã€‚'
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Note:** It is important to set the **mean** and **std** values for **normalization**
    in the feature extractor to the **values of our dataset**. We can calculate the
    values using the following block of code:'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** åœ¨ç‰¹å¾æå–å™¨ä¸­è®¾ç½®**å‡å€¼ï¼ˆmeanï¼‰**å’Œ**æ ‡å‡†å·®ï¼ˆstdï¼‰**å€¼ä»¥è¿›è¡Œ**å½’ä¸€åŒ–ï¼ˆnormalizationï¼‰**æ˜¯éå¸¸é‡è¦çš„ï¼Œè¿™äº›å€¼åº”å½“è®¾ä¸º**æˆ‘ä»¬æ•°æ®é›†çš„å€¼**ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç å—æ¥è®¡ç®—è¿™äº›å€¼ï¼š'
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Applying Transforms for Preprocessing:** We create a function to preprocess
    the audio data by encoding the audio arrays into the `input_values` format expected
    by the model. This function is set up to be applied dynamically, meaning it processes
    the data on-the-fly as each sample is loaded from the dataset.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**åº”ç”¨é¢„å¤„ç†è½¬æ¢ï¼š** æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥é¢„å¤„ç†éŸ³é¢‘æ•°æ®ï¼Œå°†éŸ³é¢‘æ•°ç»„ç¼–ç ä¸ºæ¨¡å‹æœŸæœ›çš„`input_values`æ ¼å¼ã€‚è¿™ä¸ªå‡½æ•°è®¾ç½®ä¸ºåŠ¨æ€åº”ç”¨ï¼Œå³åœ¨æ¯ä¸ªæ ·æœ¬ä»æ•°æ®é›†ä¸­åŠ è½½æ—¶ï¼Œå®ƒä¼šå®æ—¶å¤„ç†æ•°æ®ã€‚'
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Inspecting Transformed Data**: If we load a sample now, it will be transformed
    on the fly and the encoded audios are yielded as `*input_values*`*:*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ£€æŸ¥è½¬æ¢åçš„æ•°æ®ï¼š** å¦‚æœæˆ‘ä»¬ç°åœ¨åŠ è½½ä¸€ä¸ªæ ·æœ¬ï¼Œå®ƒå°†å®æ—¶è½¬æ¢ï¼Œç¼–ç åçš„éŸ³é¢‘å°†ä½œä¸º`*input_values*`è¾“å‡ºï¼š'
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Note**: It is crucial to verify that the transformation process maintains
    data integrity and that the spectrograms are correctly formed to avoid any issues
    during model training.'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** éªŒè¯è½¬æ¢è¿‡ç¨‹æ˜¯å¦ä¿æŒæ•°æ®å®Œæ•´æ€§ï¼Œå¹¶ç¡®ä¿é¢‘è°±å›¾æ­£ç¡®ç”Ÿæˆï¼Œä»¥é¿å…æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°ä»»ä½•é—®é¢˜ï¼Œè¿™æ˜¯è‡³å…³é‡è¦çš„ã€‚'
- en: '**Splitting the Dataset:** As last data preprocessing step, we split the dataset
    into a `train`and `test`-setwhile utilizing the labels for stratification. This
    ensures to maintain class distribution across both sets.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ‹†åˆ†æ•°æ®é›†ï¼š** ä½œä¸ºæœ€åä¸€æ­¥æ•°æ®é¢„å¤„ç†ï¼Œæˆ‘ä»¬å°†æ•°æ®é›†æ‹†åˆ†ä¸º`train`å’Œ`test`é›†ï¼ŒåŒæ—¶åˆ©ç”¨æ ‡ç­¾è¿›è¡Œåˆ†å±‚æŠ½æ ·ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿ä¸¤ä¸ªæ•°æ®é›†ä¸­çš„ç±»åˆ«åˆ†å¸ƒä¿æŒä¸€è‡´ã€‚'
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 3\. Add audio augmentations
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. æ·»åŠ éŸ³é¢‘å¢å¼º
- en: Augmentations play a crucial role in increasing the robustness of machine learning
    models by introducing variability into the training data. This simulates different
    recording conditions and helps the model to better generalize to unseen data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¢å¼ºåœ¨é€šè¿‡å¼•å…¥è®­ç»ƒæ•°æ®çš„å˜åŒ–æ€§æ¥æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„é²æ£’æ€§æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è¿™æ¨¡æ‹Ÿäº†ä¸åŒçš„å½•éŸ³æ¡ä»¶ï¼Œå¹¶å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å¯¹æœªè§è¿‡çš„æ•°æ®è¿›è¡Œæ³›åŒ–ã€‚
- en: Before diving into the setup, hereâ€™s a visual comparison showing the **original**
    spectrogram of an audio file and its augmented version using the **AddBackgroundNoise**
    transformation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹è®¾ç½®ä¹‹å‰ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªè§†è§‰å¯¹æ¯”ï¼Œå±•ç¤ºäº†éŸ³é¢‘æ–‡ä»¶çš„**åŸå§‹**é¢‘è°±å›¾å’Œé€šè¿‡ **AddBackgroundNoise** è½¬æ¢å¾—åˆ°çš„å¢å¼ºç‰ˆé¢‘è°±å›¾ã€‚
- en: '![](../Images/d3f6385a1580e5879d6a036094c54a5f.png)![](../Images/b3fa731cbc376356c42569f444f90b76.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3f6385a1580e5879d6a036094c54a5f.png)![](../Images/b3fa731cbc376356c42569f444f90b76.png)'
- en: The original spectrogram of an audio file (left) and the same audio with the
    AddBackgroundNoise transformation from [Audiomentations](https://github.com/iver56/audiomentations)
    library (right) | *Image by author*
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: éŸ³é¢‘æ–‡ä»¶çš„åŸå§‹é¢‘è°±å›¾ï¼ˆå·¦ï¼‰å’Œé€šè¿‡ [Audiomentations](https://github.com/iver56/audiomentations)
    åº“çš„ AddBackgroundNoise è½¬æ¢å¢å¼ºåçš„éŸ³é¢‘ï¼ˆå³ï¼‰| *å›¾ç‰‡æ¥æºï¼šä½œè€…*
- en: '**Note:** Augmentations are a very effective tool for increasing the robustness
    of training and reducing overfitting in machine learning models.'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** å¢å¼ºæ˜¯æé«˜è®­ç»ƒé²æ£’æ€§å’Œå‡å°‘æœºå™¨å­¦ä¹ æ¨¡å‹è¿‡æ‹Ÿåˆçš„æœ‰æ•ˆå·¥å…·ã€‚'
- en: ''
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, itâ€™s important to **carefully consider the potential impact of each
    transformation**. For example, adding noise may be appropriate for speech datasets,
    as it can simulate real-world scenarios where background noise is present. However,
    for tasks such as sound classification, such enhancements could lead to class
    confusion, resulting in poor model performance.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¿…é¡»**ä»”ç»†è€ƒè™‘æ¯ä¸ªè½¬æ¢çš„æ½œåœ¨å½±å“**ã€‚ä¾‹å¦‚ï¼Œæ·»åŠ å™ªéŸ³å¯¹äºè¯­éŸ³æ•°æ®é›†å¯èƒ½æ˜¯åˆé€‚çš„ï¼Œå› ä¸ºå®ƒå¯ä»¥æ¨¡æ‹Ÿç°å®ä¸–ç•Œä¸­çš„èƒŒæ™¯å™ªéŸ³æƒ…å†µã€‚ç„¶è€Œï¼Œå¯¹äºå£°éŸ³åˆ†ç±»ç­‰ä»»åŠ¡ï¼Œè¿™äº›å¢å¼ºå¯èƒ½ä¼šå¯¼è‡´ç±»åˆ«æ··æ·†ï¼Œä»è€Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚
- en: '**Setting Up Audio Augmentations:** To create a set of audio augmentations,
    we use the `Compose` class from the [Audiomentations](https://iver56.github.io/audiomentations/)
    library, which allows us to chain multiple augmentations.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¾ç½®éŸ³é¢‘å¢å¼ºï¼š** ä¸ºäº†åˆ›å»ºä¸€ç»„éŸ³é¢‘å¢å¼ºï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ª [Audiomentations](https://iver56.github.io/audiomentations/)
    åº“çš„ `Compose` ç±»ï¼Œå®ƒå…è®¸æˆ‘ä»¬å°†å¤šä¸ªå¢å¼ºç»„åˆåœ¨ä¸€èµ·ã€‚'
- en: 'Hereâ€™s how to set it up:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯å¦‚ä½•è®¾ç½®å®ƒï¼š
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this setup:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè®¾ç½®ä¸­ï¼š
- en: The `p=0.8` parameter specifies that each augmentation in the `Compose` sequence
    has an 80% chance of being applied to any given audio sample. This probabilistic
    approach ensures variability in the training data, preventing the model from becoming
    overly dependent on any specific augmentation pattern and improving its ability
    to generalize.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p=0.8` å‚æ•°æŒ‡å®š `Compose` åºåˆ—ä¸­çš„æ¯ä¸ªå¢å¼ºåœ¨ç»™å®šéŸ³é¢‘æ ·æœ¬ä¸Šæœ‰ 80% çš„æ¦‚ç‡è¢«åº”ç”¨ã€‚è¿™ä¸ªæ¦‚ç‡æ–¹æ³•ç¡®ä¿äº†è®­ç»ƒæ•°æ®çš„å˜åŒ–æ€§ï¼Œé˜²æ­¢æ¨¡å‹è¿‡åº¦ä¾èµ–äºä»»ä½•ç‰¹å®šçš„å¢å¼ºæ¨¡å¼ï¼Œå¹¶æé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚'
- en: The `shuffle=True` parameter randomizes the order in which the augmentations
    are applied, adding another layer of variability.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shuffle=True` å‚æ•°ä¼šéšæœºåŒ–åº”ç”¨å¢å¼ºçš„é¡ºåºï¼Œå¢åŠ äº†å¦ä¸€å±‚å˜åŒ–æ€§ã€‚'
- en: For a better understanding of these augmentations and detailed configuration
    options, check out the [**Audiomentationsâ€™** **docs**](https://iver56.github.io/audiomentations/).
    Additionally, thereâ€™s a great [ğŸ¤— **Space**](https://phrasenmaeher-audio-transformat-visualize-transformation-5s1n4t.streamlit.app/)
    where we can experiment with these audio transformations and hear and see their
    effects on the spectrograms.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è‹¥è¦æ›´å¥½åœ°ç†è§£è¿™äº›å¢å¼ºåŠå…¶è¯¦ç»†é…ç½®é€‰é¡¹ï¼Œå¯ä»¥æŸ¥çœ‹ [**Audiomentations çš„æ–‡æ¡£**](https://iver56.github.io/audiomentations/)ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªå¾ˆæ£’çš„
    [ğŸ¤— **ç©ºé—´**](https://phrasenmaeher-audio-transformat-visualize-transformation-5s1n4t.streamlit.app/)ï¼Œå¯ä»¥åœ¨å…¶ä¸­å®éªŒè¿™äº›éŸ³é¢‘è½¬æ¢ï¼Œå¬åˆ°å¹¶çœ‹åˆ°å®ƒä»¬å¯¹é¢‘è°±å›¾çš„å½±å“ã€‚
- en: '**Integrating Augmentations into the Training Pipeline:** We apply these augmentations
    during the `preprocess_audio` transformation where we also encode the audio data
    into spectrograms.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**å°†å¢å¼ºé›†æˆåˆ°è®­ç»ƒç®¡é“ä¸­ï¼š** æˆ‘ä»¬åœ¨ `preprocess_audio` è½¬æ¢ä¸­åº”ç”¨è¿™äº›å¢å¼ºï¼ŒåŒæ—¶å°†éŸ³é¢‘æ•°æ®ç¼–ç ä¸ºé¢‘è°±å›¾ã€‚'
- en: 'The new preprocessing with augmentation is given by:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ–°çš„é¢„å¤„ç†ä¸å¢å¼ºå¦‚ä¸‹ï¼š
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This function applies the defined augmentations to each waveform and then uses
    the `ASTFeatureExtractor` to encode the augmented waveforms into model inputs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å‡½æ•°å°†å®šä¹‰çš„å¢å¼ºåº”ç”¨åˆ°æ¯ä¸ªæ³¢å½¢ï¼Œå¹¶ä½¿ç”¨ `ASTFeatureExtractor` å°†å¢å¼ºåçš„æ³¢å½¢ç¼–ç ä¸ºæ¨¡å‹è¾“å…¥ã€‚
- en: '**Setting Transforms for Training and Validation Splits:** Finally, we set
    these transformations to be applied during the training and evaluation phases:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¾ç½®è®­ç»ƒå’ŒéªŒè¯æ‹†åˆ†çš„è½¬æ¢ï¼š** æœ€åï¼Œæˆ‘ä»¬è®¾ç½®è¿™äº›è½¬æ¢å°†åœ¨è®­ç»ƒå’Œè¯„ä¼°é˜¶æ®µåº”ç”¨ï¼š'
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 4\. Configure and Initialize the AST for Fine-Tuning
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. é…ç½®å¹¶åˆå§‹åŒ– AST è¿›è¡Œå¾®è°ƒ
- en: To adapt the AST model to our specific audio classification task, we will need
    to adjust the modelâ€™s configuration. This is because our dataset has a different
    number of classes than the pretrained model, and these classes correspond to different
    categories. It requires replacing the pretrained classifier head with a new one
    for our multi-class problem.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å°† AST æ¨¡å‹é€‚åº”æˆ‘ä»¬çš„ç‰¹å®šéŸ³é¢‘åˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´æ¨¡å‹çš„é…ç½®ã€‚å› ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†ä¸é¢„è®­ç»ƒæ¨¡å‹çš„ç±»åˆ«æ•°ä¸åŒï¼Œè€Œä¸”è¿™äº›ç±»åˆ«å¯¹åº”ä¸åŒçš„åˆ†ç±»ã€‚æˆ‘ä»¬éœ€è¦ç”¨ä¸€ä¸ªæ–°çš„åˆ†ç±»å¤´æ›¿æ¢é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„åˆ†ç±»å¤´ï¼Œä»¥è§£å†³æˆ‘ä»¬çš„å¤šç±»é—®é¢˜ã€‚
- en: The weights for the new classifier head will be randomly initialized, while
    the rest of the modelâ€™s weights will be loaded from the pretrained version. In
    this way, we benefit from the learned features of the pretraining and fine-tune
    on our data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æ–°çš„åˆ†ç±»å¤´çš„æƒé‡å°†è¢«éšæœºåˆå§‹åŒ–ï¼Œè€Œæ¨¡å‹å…¶ä½™éƒ¨åˆ†çš„æƒé‡å°†ä»é¢„è®­ç»ƒç‰ˆæœ¬åŠ è½½ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥ä»é¢„è®­ç»ƒçš„å­¦ä¹ ç‰¹å¾ä¸­å—ç›Šï¼Œå¹¶åœ¨æˆ‘ä»¬çš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚
- en: 'Hereâ€™s how to set up and initialize the AST model with a new classification
    head:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¦‚ä½•è®¾ç½®å’Œåˆå§‹åŒ–å¸¦æœ‰æ–°åˆ†ç±»å¤´çš„ AST æ¨¡å‹ï¼š
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Expected Output:** We will see warnings indicating that some weights, especially
    those in the classifier layers, are being reinitialized:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**é¢„æœŸè¾“å‡ºï¼š** æˆ‘ä»¬å°†çœ‹åˆ°ä¸€äº›è­¦å‘Šï¼Œè¡¨æ˜æŸäº›æƒé‡ï¼Œç‰¹åˆ«æ˜¯åˆ†ç±»å±‚ä¸­çš„æƒé‡ï¼Œæ­£åœ¨è¢«é‡æ–°åˆå§‹åŒ–ï¼š'
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 5\. Setup Metrics and Start Training
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. è®¾ç½®è¯„ä¼°æŒ‡æ ‡å¹¶å¼€å§‹è®­ç»ƒ
- en: In the final step we will configure the training process with the ğŸ¤— [Transformers](https://github.com/huggingface/transformers)
    library and use the ğŸ¤— [Evaluate](https://github.com/huggingface/evaluate) library
    to define the evaluation metrics to assess the modelâ€™s performance.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ ğŸ¤— [Transformers](https://github.com/huggingface/transformers) åº“æ¥é…ç½®è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨
    ğŸ¤— [Evaluate](https://github.com/huggingface/evaluate) åº“æ¥å®šä¹‰è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: '**1\. Configure Training Arguments:** The `TrainingArguments` class helps set
    up various parameters for the training process, such as learning rate, batch size,
    and number of epochs.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. é…ç½®è®­ç»ƒå‚æ•°ï¼š** `TrainingArguments` ç±»æœ‰åŠ©äºè®¾ç½®è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§å‚æ•°ï¼Œå¦‚å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°å’Œè®­ç»ƒè½®æ•°ã€‚'
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**2\. Define Evaluation Metrics:** Define metrics such as accuracy, precision,
    recall, and F1 score to evaluate the modelâ€™s performance. The `compute_metrics`
    function will handle the calculations during training.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. å®šä¹‰è¯„ä¼°æŒ‡æ ‡ï¼š** å®šä¹‰å¦‚å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ç­‰æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚`compute_metrics` å‡½æ•°å°†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¤„ç†è¿™äº›è®¡ç®—ã€‚'
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**3\. Setup the Trainer:** Use the `Trainer` class from Hugging Face to handle
    the training process. This class integrates the model, training arguments, datasets,
    and metrics.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. è®¾ç½® Trainerï¼š** ä½¿ç”¨ Hugging Face çš„ `Trainer` ç±»æ¥å¤„ç†è®­ç»ƒè¿‡ç¨‹ã€‚è¯¥ç±»é›†æˆäº†æ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ã€‚'
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'With everything configured, we initiate the training process:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å®Œæˆåï¼Œæˆ‘ä»¬å¯åŠ¨è®­ç»ƒè¿‡ç¨‹ï¼š
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/e14d2383267ae80949cd038f1e0f21b5.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e14d2383267ae80949cd038f1e0f21b5.png)'
- en: Example log of a training with audio-augmentations applied to the train-split
    | *Image by author*
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨éŸ³é¢‘å¢å¼ºçš„è®­ç»ƒæ—¥å¿—ç¤ºä¾‹ | *å›¾åƒç”±ä½œè€…æä¾›*
- en: (Not so optional:) Evaluate The Results
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ï¼ˆéé‚£ä¹ˆå¯é€‰çš„ï¼‰: è¯„ä¼°ç»“æœ'
- en: To understand our modelâ€™s performance and find potential areas for improvement,
    it is essential to evaluate its predictions on train and test data. During training,
    metrics such as accuracy, precision, recall, and F1 score are logged to [TensorBoard](https://www.tensorflow.org/tensorboard),
    which allows us to inspect the modelâ€™s progress and performance over time.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£æ¨¡å‹çš„è¡¨ç°å¹¶æ‰¾å‡ºæ½œåœ¨çš„æ”¹è¿›ç©ºé—´ï¼Œè¯„ä¼°å…¶åœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¸Šçš„é¢„æµ‹è‡³å…³é‡è¦ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ç­‰æŒ‡æ ‡ä¼šè®°å½•åˆ° [TensorBoard](https://www.tensorflow.org/tensorboard)ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ£€æŸ¥æ¨¡å‹éšæ—¶é—´çš„è¿›å±•å’Œæ€§èƒ½ã€‚
- en: '**Starting TensorBoard**: To visualize these metrics, initiate TensorBoard
    by running the following command in your terminal:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯åŠ¨ TensorBoard**ï¼šä¸ºäº†å¯è§†åŒ–è¿™äº›æŒ‡æ ‡ï¼Œåœ¨ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨ TensorBoardï¼š'
- en: '[PRE21]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This provides a graphical representation of the modelâ€™s learning curve and metric
    improvements over time, helping to identify potential overfitting or underperformance
    early in the training process.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æä¾›äº†ä¸€ä¸ªå›¾å½¢åŒ–çš„è¡¨ç¤ºï¼Œå±•ç¤ºäº†æ¨¡å‹çš„å­¦ä¹ æ›²çº¿å’ŒæŒ‡æ ‡éšæ—¶é—´çš„æ”¹è¿›ï¼Œå¸®åŠ©æˆ‘ä»¬åŠæ—©å‘ç°æ½œåœ¨çš„è¿‡æ‹Ÿåˆæˆ–æ€§èƒ½ä¸è¶³ã€‚
- en: For **more detailed insights**, we can inspect the modelâ€™s predictions using
    [Renumics](https://renumics.com/)â€™ open-source tool, [**Spotlight**](https://renumics.com/open-source/spotlight).
    Spotlight enables us to explore and visualize the predictions alongside the data,
    helping us to identify patterns, potential biases, and miss-classifications on
    the level of single data points.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº**æ›´è¯¦ç»†çš„è§è§£**ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [Renumics](https://renumics.com/) çš„å¼€æºå·¥å…· [**Spotlight**](https://renumics.com/open-source/spotlight)
    æ£€æŸ¥æ¨¡å‹çš„é¢„æµ‹ã€‚Spotlight å¯ä»¥è®©æˆ‘ä»¬æ¢ç´¢å’Œå¯è§†åŒ–é¢„æµ‹ä»¥åŠæ•°æ®ï¼Œå¸®åŠ©æˆ‘ä»¬è¯†åˆ«å•ä¸ªæ•°æ®ç‚¹çš„æ¨¡å¼ã€æ½œåœ¨åè§å’Œé”™è¯¯åˆ†ç±»ã€‚
- en: '![](../Images/07e84100feee6c38d3dbde8ed89c2a7f.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07e84100feee6c38d3dbde8ed89c2a7f.png)'
- en: The ESC50 dataset with audio embeddings and model predictions loaded in Spotlight.
    Try it yourself in this Hugging Face [Space](https://huggingface.co/spaces/renumics/spotlight-esc50-clap)
    | *Image by author*
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Spotlight ä¸­åŠ è½½äº†å¸¦æœ‰éŸ³é¢‘åµŒå…¥å’Œæ¨¡å‹é¢„æµ‹çš„ ESC50 æ•°æ®é›†ã€‚åœ¨è¿™ä¸ª Hugging Face [Space](https://huggingface.co/spaces/renumics/spotlight-esc50-clap)
    ä¸­å°è¯•ä¸€ä¸‹å§ã€‚| *ä½œè€…æä¾›çš„å›¾ç‰‡*
- en: '**Installing and Using Spotlight**:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®‰è£…å’Œä½¿ç”¨ Spotlight**ï¼š'
- en: 'To get started with Spotlight, install it using pip and load your dataset for
    exploration:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹ä½¿ç”¨ Spotlightï¼Œè¯·ä½¿ç”¨ pip å®‰è£…å®ƒå¹¶åŠ è½½æ‚¨çš„æ•°æ®é›†è¿›è¡Œæ¢ç´¢ï¼š
- en: '[PRE22]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And load the ESC50 dataset for **interactive exploration** with one line of
    code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä½¿ç”¨ä¸€è¡Œä»£ç åŠ è½½ ESC50 æ•°æ®é›†è¿›è¡Œ**äº¤äº’å¼æ¢ç´¢**ï¼š
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This tutorial focuses on setting up the fine-tuning pipeline. For a comprehensive
    **evaluation**, including **using Spotlight**, please refer to the other tutorials
    and resources provided below and at the end of this guide (Useful Links).
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹ä¾§é‡äºå»ºç«‹å¾®è°ƒæµç¨‹ã€‚æœ‰å…³å…¨é¢çš„**è¯„ä¼°**ï¼ŒåŒ…æ‹¬**ä½¿ç”¨ Spotlight**ï¼Œè¯·å‚è€ƒä¸‹é¢æä¾›çš„å…¶ä»–æ•™ç¨‹å’Œèµ„æºä»¥åŠæœ¬æŒ‡å—æœ«å°¾çš„é“¾æ¥ï¼ˆæœ‰ç”¨é“¾æ¥ï¼‰ã€‚
- en: 'Here are some examples of how to use Spotlight for model evaluation:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€äº›å¦‚ä½•ä½¿ç”¨ Spotlight è¿›è¡Œæ¨¡å‹è¯„ä¼°çš„ç¤ºä¾‹ï¼š
- en: A blog post with demo on **Hands-On Voice Analytics with Transformers:** [Blog](https://renumics.com/blog/voice-analytics-with-transformers)
    & ğŸ¤— [Space](https://huggingface.co/spaces/renumics/emodb-model-comparison)
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡å…³äº**ä½¿ç”¨ Transformers è¿›è¡Œå®æ—¶è¯­éŸ³åˆ†æ**çš„åšæ–‡å’Œæ¼”ç¤ºï¼š[åšæ–‡](https://renumics.com/blog/voice-analytics-with-transformers)
    & ğŸ¤— [Space](https://huggingface.co/spaces/renumics/emodb-model-comparison)
- en: A blog post and short example on **Fine-tuning image classification models from
    image search:** [Blog](https://itnext.io/image-classification-in-2023-8ab7dc552115)
    & [Use Case](https://renumics.com/next/docs/use-cases/image-fine-tuning)
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡å…³äº**Fine-tuning image classification models from image search**çš„åšæ–‡å’Œç®€çŸ­ç¤ºä¾‹ï¼š[åšæ–‡](https://itnext.io/image-classification-in-2023-8ab7dc552115)
    & [ä½¿ç”¨æ¡ˆä¾‹](https://renumics.com/next/docs/use-cases/image-fine-tuning)
- en: 'A blog post and short example on **How to Automatically Find and Remove Issues
    in Your Image, Audio, and Text Classification Datasets**: [Blog](https://medium.com/@daniel-klitzke/finding-problematic-data-slices-in-unstructured-data-aeec0a3b9a2a)
    & [Use Case](https://renumics.com/next/docs/use-cases/audio-classification)'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡å…³äº**å¦‚ä½•è‡ªåŠ¨æŸ¥æ‰¾å’Œåˆ é™¤å›¾åƒã€éŸ³é¢‘å’Œæ–‡æœ¬åˆ†ç±»æ•°æ®é›†ä¸­é—®é¢˜çš„æ–‡ç« **å’Œç®€çŸ­ç¤ºä¾‹ï¼š[åšæ–‡](https://medium.com/@daniel-klitzke/finding-problematic-data-slices-in-unstructured-data-aeec0a3b9a2a)
    & [ä½¿ç”¨æ¡ˆä¾‹](https://renumics.com/next/docs/use-cases/audio-classification)
- en: Conclusion
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: By following the steps outlined in this guide, weâ€™ll be able to fine-tune the
    Audio Spectrogram Transformer (AST) on any audio classification dataset. This
    includes setting up data preprocessing, applying effective audio augmentations,
    and configuring the model for the specific task. After training, we can evaluate
    the modelâ€™s performance using the defined metrics, ensuring it meets our requirements.
    Once the model is fine-tuned and validated, it can be used for inference.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æŒ‰ç…§æœ¬æŒ‡å—ä¸­æ¦‚è¿°çš„æ­¥éª¤ï¼Œæˆ‘ä»¬å°†èƒ½å¤Ÿåœ¨ä»»ä½•éŸ³é¢‘åˆ†ç±»æ•°æ®é›†ä¸Šå¾®è°ƒéŸ³é¢‘é¢‘è°±å˜æ¢å™¨ï¼ˆASTï¼‰ã€‚è¿™åŒ…æ‹¬è®¾ç½®æ•°æ®é¢„å¤„ç†ã€åº”ç”¨æœ‰æ•ˆçš„éŸ³é¢‘å¢å¼ºä»¥åŠä¸ºç‰¹å®šä»»åŠ¡é…ç½®æ¨¡å‹ã€‚è®­ç»ƒåï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®šä¹‰çš„æŒ‡æ ‡è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œç¡®ä¿å®ƒç¬¦åˆæˆ‘ä»¬çš„è¦æ±‚ã€‚ä¸€æ—¦æ¨¡å‹ç»è¿‡å¾®è°ƒå’ŒéªŒè¯ï¼Œå°±å¯ä»¥ç”¨äºæ¨æ–­ã€‚
- en: More on the Topic
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºè¿™ä¸ªä¸»é¢˜çš„æ›´å¤šå†…å®¹
- en: This is the second in a **series of tutorials and blog posts** on the Audio
    Spectrogram Transformer for industrial audio classification use cases.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å…³äºç”¨äºå·¥ä¸šéŸ³é¢‘åˆ†ç±»ç”¨ä¾‹çš„éŸ³é¢‘é¢‘è°±å˜æ¢å™¨çš„**ç³»åˆ—æ•™ç¨‹å’Œåšæ–‡**ä¸­çš„ç¬¬äºŒç¯‡ã€‚
- en: 'Have a look at **part one**: [*How to Use SSAST Model Weigths in the HuggingFace
    Ecosystem?*](https://medium.com/itnext/how-to-use-ssast-model-weigths-in-the-huggingface-ecosystem-0f3fdc8d38da),'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çœ‹ä¸€çœ‹**ç¬¬ä¸€éƒ¨åˆ†**ï¼š[*å¦‚ä½•åœ¨ HuggingFace ç”Ÿæ€ç³»ç»Ÿä¸­ä½¿ç”¨ SSAST æ¨¡å‹æƒé‡ï¼Ÿ*](https://medium.com/itnext/how-to-use-ssast-model-weigths-in-the-huggingface-ecosystem-0f3fdc8d38da)ï¼Œ
- en: and watch [this list](https://medium.com/@marius_s/list/audio-classification-for-industry-use-cases-cb6d169a7d80)
    for upcoming posts.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¹¶æŸ¥çœ‹[è¿™ä¸ªåˆ—è¡¨](https://medium.com/@marius_s/list/audio-classification-for-industry-use-cases-cb6d169a7d80)ä»¥è·å–å³å°†å‘å¸ƒçš„æ–‡ç« ã€‚
- en: Stay tuned for further posts in this series, where we will examine specific
    challenges from real use cases and how to adapt the AST to handle them.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·ç»§ç»­å…³æ³¨æœ¬ç³»åˆ—çš„åç»­æ–‡ç« ï¼Œæˆ‘ä»¬å°†æ¢è®¨å®é™…ä½¿ç”¨æ¡ˆä¾‹ä¸­çš„ç‰¹å®šæŒ‘æˆ˜ä»¥åŠå¦‚ä½•è°ƒæ•´ASTä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚
- en: Useful Links
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ‰ç”¨çš„é“¾æ¥
- en: '**Download** **this guide** asnotebook from the Renumics [**Resource Page**](https://renumics.com/open-source/resources)**.**'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä¸‹è½½** **æœ¬æŒ‡å—**ä½œä¸ºç¬”è®°æœ¬ï¼Œä»Renumicsçš„[**èµ„æºé¡µé¢**](https://renumics.com/open-source/resources)**ã€‚**'
- en: A tutorial on how to use **Spotlight for audio model evaluation:** [**Blog**](https://renumics.com/blog/voice-analytics-with-transformers)
    & **ğŸ¤—** [**Space**](https://huggingface.co/spaces/renumics/emodb-model-comparison)
    (Demo)
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å…³äºå¦‚ä½•ä½¿ç”¨**Spotlightè¿›è¡ŒéŸ³é¢‘æ¨¡å‹è¯„ä¼°**çš„æ•™ç¨‹ï¼š[**åšå®¢**](https://renumics.com/blog/voice-analytics-with-transformers)
    & **ğŸ¤—** [**ç©ºé—´**](https://huggingface.co/spaces/renumics/emodb-model-comparison)ï¼ˆæ¼”ç¤ºï¼‰
- en: 'A tutorial on how to **train an acoustic event detection** **system** with
    Spotlight: [**Blog**](https://renumics.com/blog/acoustic-event-detection-annotation)'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å…³äºå¦‚ä½•ä½¿ç”¨Spotlight **è®­ç»ƒå£°å­¦äº‹ä»¶æ£€æµ‹** **ç³»ç»Ÿ**çš„æ•™ç¨‹ï¼š[**åšå®¢**](https://renumics.com/blog/acoustic-event-detection-annotation)
- en: 'The **official ğŸ¤— audio course**: [**Introduction**](https://huggingface.co/learn/audio-course/chapter0/introduction)
    & [**Fine-Tuning**](https://huggingface.co/learn/audio-course/chapter4/fine-tuning)'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å®˜æ–¹ ğŸ¤— éŸ³é¢‘è¯¾ç¨‹**: [**ä»‹ç»**](https://huggingface.co/learn/audio-course/chapter0/introduction)
    & [**å¾®è°ƒ**](https://huggingface.co/learn/audio-course/chapter4/fine-tuning)'
- en: Thanks for reading! My name is [Marius Steger](https://www.linkedin.com/in/marius-steger/),
    Iâ€™m a Machine Learning Engineer @[Renumics](https://renumics.com/) â€” We have developed
    [Spotlight](https://github.com/Renumics/spotlight), an Open Source tool that takes
    your data-centric AI workflow to the next level.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼æˆ‘å« [Marius Steger](https://www.linkedin.com/in/marius-steger/)ï¼Œæ˜¯[Renumics](https://renumics.com/)çš„æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆâ€”â€”æˆ‘ä»¬å¼€å‘äº†[Spotlight](https://github.com/Renumics/spotlight)ï¼Œä¸€æ¬¾å¼€æºå·¥å…·ï¼Œèƒ½å¤Ÿå°†æ‚¨çš„æ•°æ®é©±åŠ¨AIå·¥ä½œæµæå‡åˆ°ä¸€ä¸ªæ–°çš„æ°´å¹³ã€‚
- en: '![](../Images/cc655344170c51a9f0d18fa1a30b1164.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc655344170c51a9f0d18fa1a30b1164.png)'
- en: References
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Yuan Gong, Yu-An Chung, James Glass: [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778)
    (2021), arxiv'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Yuan Gong, Yu-An Chung, James Glass: [ASTï¼šéŸ³é¢‘è°±å›¾è½¬æ¢å™¨](https://arxiv.org/abs/2104.01778)
    (2021), arxiv'
- en: '[2] Piczak, Karol J.: [ESC: Dataset for Environmental Sound Classification](https://dl.acm.org/doi/10.1145/2733373.2806390)
    (2015), ACM Press'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Piczak, Karol J.: [ESCï¼šç¯å¢ƒå£°éŸ³åˆ†ç±»æ•°æ®é›†](https://dl.acm.org/doi/10.1145/2733373.2806390)
    (2015), ACMå‡ºç‰ˆç¤¾'
