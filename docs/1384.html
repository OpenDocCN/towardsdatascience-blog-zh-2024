<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Bit-LoRA as an application of BitNet and 1.58 bit neural network technologies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Bit-LoRA as an application of BitNet and 1.58 bit neural network technologies</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bit-lora-as-an-application-of-bitnet-and-1-58-bit-neural-network-technologies-17ee80bf79f9?source=collection_archive---------2-----------------------#2024-06-03">https://towardsdatascience.com/bit-lora-as-an-application-of-bitnet-and-1-58-bit-neural-network-technologies-17ee80bf79f9?source=collection_archive---------2-----------------------#2024-06-03</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c0a0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Abstract: applying ~1bit transformer technology to LoRA adapters allows us to reach comparable performance with full-precision LoRA reducing the size of LoRA adapters by a factor of 30. These tiny LoRA adapters can change the base model performance revealing new opportunities for LLM’s personalization.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@r.smirnov.mailbox?source=post_page---byline--17ee80bf79f9--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Roman S" class="l ep by dd de cx" src="../Images/bb01d7b8d79ffa4e93afafb956241aff.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*zMTI8-vkNKwo1-28YhNFzQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--17ee80bf79f9--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@r.smirnov.mailbox?source=post_page---byline--17ee80bf79f9--------------------------------" rel="noopener follow">Roman S</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--17ee80bf79f9--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 3, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="7928" class="mr ms fq bf mt mu mv gq mw mx my gt mz na nb nc nd ne nf ng nh ni nj nk nl nm bk"><strong class="al"><em class="nn">1.What is 1.58 bit?</em></strong></h1><p id="c5d2" class="pw-post-body-paragraph no np fq nq b go nr ns nt gr nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk ok"><span class="l ol om on bo oo op oq or os ed">N</span>owadays there is this technology named “LLM” that is quite trending. LLM stands for Large Language Model. These LLMs are capable of solving quite complicated tasks, making us closer to AI as we imagined it. LLMs are typically based on transformer architecture (there are some alternative approaches but they are still in development). Transformers architecture requires quite expensive computations, and because these LLMs are <strong class="nq fr">large </strong>the computations require a lot of time and resources. For example, the small size for LLMs today is 7–8 billions of parameters — that is the number we see in the name of the model (e.g. Llama3<strong class="nq fr">–8B</strong> or Llama2<strong class="nq fr">–7B</strong>). Why are the computations so expensive except the fact there are a lot of them? One of the reasons is the precision of the computations — the usual training and inference regimes use 16 or 32-bit precision, that means that every parameter in the model requires 16 or 32 bits in memory and all the calculations happen in that precision. Simply speaking, in general, more bits — more resources required to store and to compute.</p><p id="bc77" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">Quantization is a well-known way to reduce the number of bits used for each parameter to decrease required resources (decrease inference time) at the cost of accuracy. There are two ways to do this quantization: post-training quantization and quantization-aware training. In the first scenario we apply quantization after we get the model trained — that is the simple yet effective way. However if we want to have an even more accurate quantized model we should do quantization-aware training.</p><p id="e004" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">A few words about quantization aware training, when we do quantization aware training we force the model to produce outputs in the low precision, let’s say 4 bits instead of the original 32 bits: simple analogy, we calculate 3.4 + x and the expected correct answer (target) is 5.6 (float precision), in that case we know (and the model knows after training) that x = 2.2 (3.4+2.2=5.6). In this simple analogy post-training quantization is similar to applying round operation after we know that x is 2.2 — we are getting 3 + 2 = 5 (while the target is still 5.6). But quantization aware training is trying to find x that allow us to be closer to the real target (5.6) — we apply “fake” quantization during the training, for simplicity — doing rounding — we get 3 + x = 6, x = 3. The point is that 6 is closer to 5.6 rather than 5. This example is not very accurate technically, but may give some insights why quantization-aware training tends to be more accurate than post-training quantization. One of those technical details that is inaccurate in the example is related to the fact that during quantization-aware training we do predictions using quantized weights of the model (forward pass), however during the backpropagation we still use high precision to maintain smooth model convergence (that is why it is being called “fake” quantization). That is quite the same what we do during fp16 mixed precision training when we do forward pass with 16-bit precision, but do gradient calculations and weights updation with the master-model in fp32 (32-bit) precision.</p><p id="85b2" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">Ok, quantization is a way to make models smaller and resource-efficient. Ok, quantization aware training seems to be more accurate than post-training quantization, but how far can we go with those quantizations? There are two papers I want to mention that state that we can go lower than 2 bit quantization and the training process will remain stable:</p><ol class=""><li id="cd9e" class="no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj oy oz pa bk"><a class="af pb" href="https://arxiv.org/pdf/2310.11453" rel="noopener ugc nofollow" target="_blank">BitNet: Scaling 1-bit Transformers for Large Language Models</a>. Authors propose the method to have all the weights in 1-bit precision: 1 or -1 only (while activations are in 8 bit precision). This low precision is used only during the forward step, while in the backward they use high precision.</li><li id="6d5d" class="no np fq nq b go pc ns nt gr pd nv nw nx pe nz oa ob pf od oe of pg oh oi oj oy oz pa bk"><a class="af pb" href="https://arxiv.org/pdf/2402.17764" rel="noopener ugc nofollow" target="_blank">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a>. This paper is based on the BitNet paper, however here authors use {-1; 0; 1} as possible values for each parameter in the model instead of only 1 and -1.</li></ol><p id="8b13" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">When I first saw these papers I was quite skeptical — I didn’t believe that such a low precision model can achieve comparable or better accuracy with the full precision LLM. And I remain skeptical. For me that sounds too good to be true. Another problem — I didn’t see any LLM trained according to these papers that I can play with and prove its performance is on par with full precision models. But can I train such an LLM by myself? Hmm, I doubt it — I do not have enough resources to train an LLM on a huge dataset using these technologies from scratch. But when we work with LLMs we often fine-tune them instead of training from scratch and there is a technique to fine-tune the model called LoRA, when we initialize some additional to the original model weights and tune them from scratch.</p></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="44e4" class="mr ms fq bf mt mu mv gq mw mx my gt mz na nb nc nd ne nf ng nh ni nj nk nl nm bk"><strong class="al"><em class="nn">2. What is LoRA and why?</em></strong></h1><p id="531f" class="pw-post-body-paragraph no np fq nq b go nr ns nt gr nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">LoRA is a technique for parameter-efficient models fine-tuning (PEFT). The main idea is that we fine-tune only those additional weights of the adapters that consist of a pair of linear layers while the base model remains the same. That is very important from the perspective of me trying to use 1.58 bit technology. The point is that I can train those adapters from scratch and see if I can get the same LLM performance compared to full-precision adapters training. Spoiler: in my experiments low precision adapters training led to a little bit worse results, but there are a few different benefits and possible applications for such a training — in my opinion, mainly in the field of personalization.</p></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="43af" class="mr ms fq bf mt mu mv gq mw mx my gt mz na nb nc nd ne nf ng nh ni nj nk nl nm bk"><strong class="al"><em class="nn">3. Experiments</em></strong></h1><p id="bb36" class="pw-post-body-paragraph no np fq nq b go nr ns nt gr nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">For experiments I took my proprietary data for a text generation task. The data itself is not that important here, I would just say that it is kind of a small subset of the instructions dataset used to train instruction following LLMs. As the base model I decided to use <a class="af pb" href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct" rel="noopener ugc nofollow" target="_blank">microsoft/Phi-3-mini-4k-instruct</a> model. I did 3 epochs of LoRA adapters tuning with fp16 mixed precision training using Huggingface Trainer and measured the loss on evaluation. After that I implemented BitNet (replacing the linear layers in LoRA adapters) and 1.58 bit LoRA training and reported the results. I used 4 bit base model quantization with BitsAndBytes during the training in Q-LoRA configuration.</p><p id="8ff7" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">The following LoRA hyperparameters were used: rank = 32, alpha = 16, dropout = 0.05.</p><p id="27c9" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk"><strong class="nq fr"><em class="ph">3.1. Classic LoRA training</em></strong></p><p id="fcc9" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">For all LoRA experiments <a class="af pb" href="https://arxiv.org/pdf/2305.14314" rel="noopener ugc nofollow" target="_blank">QLoRA</a> approach was used in the part of the base model quantization with NF4 and applying LoRA to all the linear layers of the base model. Optimizer is Paged AdamW with warmup and cosine annealing down to 90% of the maximum learning rate. Maximum learning rate equals 2e-4. Train/test split was random, the test set is 10% from the whole dataset.</p><p id="78bc" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk"><strong class="nq fr"><em class="ph">3.2. LoRA BitNet implementation</em></strong></p><p id="3564" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">For BitNet LoRA training the approach from “<a class="af pb" href="https://arxiv.org/pdf/2310.11453" rel="noopener ugc nofollow" target="_blank">BitNet: Scaling 1-bit Transformers for Large Language Models</a>” was used with the <a class="af pb" href="https://github.com/kyegomez/BitNet" rel="noopener ugc nofollow" target="_blank">code for its implementation</a>. According to BitNet paper the weights of the LoRA layers were binarized with scaling:</p><figure class="pl pm pn po pp pq pi pj paragraph-image"><div class="pi pj pk"><img src="../Images/a9dedaf20b4eb07c687c92d68a8d772f.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/0*u2_M7QrqeGUxzhX-"/></div><figcaption class="ps pt pu pi pj pv pw bf b bg z dx">Image from <a class="af pb" href="https://arxiv.org/pdf/2310.11453" rel="noopener ugc nofollow" target="_blank">BitNet: Scaling 1-bit Transformers for Large Language Models</a> paper</figcaption></figure><p id="5bb1" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">At the same time activations should be also quantized according to the paper:</p><figure class="pl pm pn po pp pq pi pj paragraph-image"><div class="pi pj px"><img src="../Images/ff657d1f97eba4e750e5e96c56f0d290.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/0*DsOe1qZmZL2JLiRN"/></div><figcaption class="ps pt pu pi pj pv pw bf b bg z dx">Image from <a class="af pb" href="https://arxiv.org/pdf/2310.11453" rel="noopener ugc nofollow" target="_blank">BitNet: Scaling 1-bit Transformers for Large Language Models</a> paper</figcaption></figure><p id="0fcb" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">According to the formulas provided you can see that each parameter is being transformed with the sign function to be either +1 or -1, those parameters are multiplied by quantized and normalized input X and scaled with the mean absolute value of parameters of the layer. Code implementation:</p><pre class="pl pm pn po pp py pz qa bp qb bb bk"><span id="3ab4" class="qc ms fq pz b bg qd qe l qf qg">from torch import nn, Tensor<br/>import torch.nn.functional as F<br/><br/><br/># from https://github.com/kyegomez/zeta<br/>class SimpleRMSNorm(nn.Module):<br/>   """<br/>   SimpleRMSNorm<br/><br/><br/>   Args:<br/>       dim (int): dimension of the embedding<br/><br/><br/>   Usage:<br/>   We can use SimpleRMSNorm as a layer in a neural network as follows:<br/>       &gt;&gt;&gt; x = torch.randn(1, 10, 512)<br/>       &gt;&gt;&gt; simple_rms_norm = SimpleRMSNorm(dim=512)<br/>       &gt;&gt;&gt; simple_rms_norm(x).shape<br/>       torch.Size([1, 10, 512])<br/><br/><br/>   """<br/><br/><br/>   def __init__(self, dim):<br/>       super().__init__()<br/>       self.scale = dim**-0.5<br/><br/><br/>   def forward(self, x):<br/>       """Forward method of SimpleRMSNorm"""<br/>       return F.normalize(x, dim=-1) * self.scale<br/><br/><br/>def activation_quant(x: Tensor):<br/>   """Per token quantization to 8bits. No grouping is needed for quantization<br/><br/><br/>   Args:<br/>       x (Tensor): _description_<br/><br/><br/>   Returns:<br/>       _type_: _description_<br/>   """<br/>   scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)<br/>   y = (x * scale).round().clamp_(-128, 127) / scale<br/>   return y<br/><br/><br/><br/><br/>def weight_quant(w: Tensor):<br/>   scale = w.abs().mean()<br/>   e = w.mean()<br/>   u = (w - e).sign() * scale<br/>   return u<br/><br/><br/>class BitLinear(nn.Linear):<br/>   """<br/>   Custom linear layer with bit quantization.<br/><br/><br/>   Args:<br/>       dim (int): The input dimension of the layer.<br/>       training (bool, optional): Whether the layer is in training mode or not. Defaults to False.<br/>       *args: Variable length argument list.<br/>       **kwargs: Arbitrary keyword arguments.<br/><br/><br/>   Attributes:<br/>       dim (int): The input dimension of the layer.<br/><br/><br/>   """<br/><br/><br/>   def forward(self, x: Tensor) -&gt; Tensor:<br/>       """<br/>       Forward pass of the BitLinear layer.<br/><br/><br/>       Args:<br/>           x (Tensor): The input tensor.<br/><br/><br/>       Returns:<br/>           Tensor: The output tensor.<br/>       """<br/>       w = self.weight<br/>       x_norm = SimpleRMSNorm(self.in_features)(x)<br/><br/><br/>       # STE using detach<br/>       # the gradient of sign() or round() is typically zero<br/>       # so to train the model we need to do the following trick<br/>       # this trick leads to "w" high precision weights update <br/>       # while we are doing "fake" quantisation during the forward pass<br/>       x_quant = x_norm + (activation_quant(x_norm) - x_norm).detach()<br/>       w_quant = w + (weight_quant(w) - w).detach()<br/>       y = F.linear(x_quant, w_quant)<br/>       return y</span></pre><p id="6d41" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">All the code above is from <a class="af pb" href="https://github.com/kyegomez/BitNet" rel="noopener ugc nofollow" target="_blank">https://github.com/kyegomez/BitNet</a> GitHub repository.</p><p id="7bba" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">After LoRA training the adapter weights can be merged with the base model because of the fact that each LoRA adapter is just a pair of linear layers without biases and non-linear activations. Normalization of activations (LN(x)) and their quantization in the approach are making LoRA adapters merger more difficult (after merger LoRA adapter share the same inputs for the linear layer as the base model — these layers work with activations without any additional modifications), that is why the additional experiment without normalization and activations quantization was conducted and <strong class="nq fr">led to better performance</strong>. To do such a modifications we should just modify forward method of the BitLinear class:</p><pre class="pl pm pn po pp py pz qa bp qb bb bk"><span id="08cc" class="qc ms fq pz b bg qd qe l qf qg">    def forward(self, x: Tensor) -&gt; Tensor:<br/>       """<br/>       Forward pass of the BitLinear layer.<br/><br/><br/>       Args:<br/>           x (Tensor): The input tensor.<br/><br/><br/>       Returns:<br/>           Tensor: The output tensor.<br/>       """<br/>       w = self.weight<br/>       #x_norm = SimpleRMSNorm(self.in_features)(x)<br/><br/><br/>       # STE using detach<br/>       #x_quant = x_norm + (activation_quant(x_norm) - x_norm).detach()<br/>       x_quant = x<br/>       w_quant = w + (weight_quant(w) - w).detach()<br/>       y = F.linear(x_quant, w_quant)<br/>       return y</span></pre><p id="e4d7" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">Presented code is quantization aware training, because the master weights of each BitLinear layer are still in high precision, while we binarize the weights during the forward pass (the same we can do during the model inference). The only issue here is that we additionally have a “scale” parameter that is individual to each layer and has high precision.</p><p id="3ba7" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">After we get BitLinear layers we need to replace linear layers in the LoRA adapter with these new linear layers to apply BitLinear modification to classic LoRA. To do so we can rewrite “update_layer” method of the LoraLayer class (peft.tuners.lora.layer.LoraLayer) with the same method but with BitLinear layers instead of Linear:</p><pre class="pl pm pn po pp py pz qa bp qb bb bk"><span id="529e" class="qc ms fq pz b bg qd qe l qf qg">from peft.tuners.lora.layer import LoraLayer<br/>import torch<br/>import torch.nn.functional as F<br/>from torch import nn<br/><br/><br/>class BitLoraLayer(LoraLayer):<br/>   def update_layer(<br/>       self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora, use_dora: bool = False<br/>   ):<br/>       if r &lt;= 0:<br/>           raise ValueError(f"`r` should be a positive integer value but the value passed is {r}")<br/><br/><br/>       self.r[adapter_name] = r<br/>       self.lora_alpha[adapter_name] = lora_alpha<br/>       if lora_dropout &gt; 0.0:<br/>           lora_dropout_layer = nn.Dropout(p=lora_dropout)<br/>       else:<br/>           lora_dropout_layer = nn.Identity()<br/><br/><br/>       self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))<br/>       # Actual trainable parameters<br/>       # The only update of the original method is here<br/>       self.lora_A[adapter_name] = BitLinear(self.in_features, r, bias=False)<br/>       self.lora_B[adapter_name] = BitLinear(r, self.out_features, bias=False)<br/>            <br/>       if use_rslora:<br/>           self.scaling[adapter_name] = lora_alpha / math.sqrt(r)<br/>       else:<br/>           self.scaling[adapter_name] = lora_alpha / r<br/><br/><br/>       if isinstance(init_lora_weights, str) and init_lora_weights.startswith("pissa"):<br/>           self.pissa_init(adapter_name, init_lora_weights)<br/>       elif init_lora_weights == "loftq":<br/>           self.loftq_init(adapter_name)<br/>       elif init_lora_weights:<br/>           self.reset_lora_parameters(adapter_name, init_lora_weights)<br/><br/><br/>       # check weight and qweight (for GPTQ)<br/>       for weight_name in ("weight", "qweight"):<br/>           weight = getattr(self.get_base_layer(), weight_name, None)<br/>           if weight is not None:<br/>               # the layer is already completely initialized, this is an update<br/>               if weight.dtype.is_floating_point or weight.dtype.is_complex:<br/>                   self.to(weight.device, dtype=weight.dtype)<br/>               else:<br/>                   self.to(weight.device)<br/>               break<br/><br/><br/>       if use_dora:<br/>           self.dora_init(adapter_name)<br/>           self.use_dora[adapter_name] = True<br/>       else:<br/>           self.use_dora[adapter_name] = False<br/><br/><br/>       self.set_adapter(self.active_adapters)</span></pre><p id="5b9f" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">After we create such a class we can replace the update_layer method of the original LoraLayer with the new one:</p><pre class="pl pm pn po pp py pz qa bp qb bb bk"><span id="6762" class="qc ms fq pz b bg qd qe l qf qg">import importlib<br/><br/><br/>original = importlib.import_module("peft")<br/>original.tuners.lora.layer.LoraLayer.update_layer = (<br/>    BitLoraLayer.update_layer<br/>)</span></pre><p id="a3d0" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk"><strong class="nq fr"><em class="ph">3.3. 1.58 bit LoRA</em></strong></p><p id="e5d4" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">For this experiment the approach from “<a class="af pb" href="https://arxiv.org/pdf/2402.17764" rel="noopener ugc nofollow" target="_blank">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a>” was used. The conceptual difference is that instead of binarization to +1 and -1 in this paper authors propose to quantize weights to -1, 0 and +1 for better accuracy.</p><figure class="pl pm pn po pp pq pi pj paragraph-image"><div class="pi pj qh"><img src="../Images/4bcd16017c0e57767d166a9876e800b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*rAK0s6CDFcda7dcI"/></div><figcaption class="ps pt pu pi pj pv pw bf b bg z dx">Image from <a class="af pb" href="https://arxiv.org/pdf/2402.17764" rel="noopener ugc nofollow" target="_blank">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a> paper</figcaption></figure><p id="47fc" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">Authors excluded activations scaling from the pipeline that was creating extra difficulties for merger with the base model in our experiments. In our experiments we additionally removed activation quantization from the pipeline to make LoRA adapter merger simpler.</p><p id="fbf0" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">To tune the LoRA adapters with this approach we should simply update the weight_quant function with following:</p><pre class="pl pm pn po pp py pz qa bp qb bb bk"><span id="2c25" class="qc ms fq pz b bg qd qe l qf qg">def weight_quant(w: Tensor):<br/>   scale = w.abs().mean()<br/>   adjustment = 1e-4 + scale / 2<br/>   w_quant = w / adjustment<br/>   return torch.clip(input=torch.round(w_quant), min=-1, max=1)*scale</span></pre><p id="0663" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">For the 1.58 bit implementation I used <a class="af pb" href="https://medium.com/@theseriousprogrammer/binary-magic-building-bitnet-1-58bit-using-pytorch-from-scratch-01fa6289db6f" rel="noopener">“Binary Magic: Building BitNet 1.58bit Using PyTorch from Scratch” publication</a> as the starting point.</p></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="a064" class="mr ms fq bf mt mu mv gq mw mx my gt mz na nb nc nd ne nf ng nh ni nj nk nl nm bk"><strong class="al"><em class="nn">4. Results</em></strong></h1><p id="f6d8" class="pw-post-body-paragraph no np fq nq b go nr ns nt gr nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">As the result 4 models were trained with different approaches to implement LoRA linear layers:</p><ul class=""><li id="4145" class="no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj qi oz pa bk">Classic LoRA (LoRA);</li><li id="fab9" class="no np fq nq b go pc ns nt gr pd nv nw nx pe nz oa ob pf od oe of pg oh oi oj qi oz pa bk">BitNet with activations normalization, quantization and scaling (BitNet-original);</li><li id="86cb" class="no np fq nq b go pc ns nt gr pd nv nw nx pe nz oa ob pf od oe of pg oh oi oj qi oz pa bk">BitNet without any activations modifications (BitNet-noact);</li><li id="6932" class="no np fq nq b go pc ns nt gr pd nv nw nx pe nz oa ob pf od oe of pg oh oi oj qi oz pa bk">Approach according to 1.58 Bits (1.58Bit).</li></ul><p id="6d80" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">All the training hyperparameters for all the experiments remained the same except the LoRA linear layers implementation. In training statistics logged with Weights&amp;Biases (Wandb):</p><figure class="pl pm pn po pp pq pi pj paragraph-image"><div role="button" tabindex="0" class="qk ql ed qm bh qn"><div class="pi pj qj"><img src="../Images/e071966bc228814f4db3cf281fd37da9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ay4kknOmwmOOcx0p"/></div></div><figcaption class="ps pt pu pi pj pv pw bf b bg z dx">Image by author: Training loss</figcaption></figure><p id="efcf" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">As for the purple line for 1.58Bit — it is invisible on the image above because of being covered by blue and green lines:</p><figure class="pl pm pn po pp pq pi pj paragraph-image"><div role="button" tabindex="0" class="qk ql ed qm bh qn"><div class="pi pj qo"><img src="../Images/c9a735877dce2154b485b9624148eef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9WF_neJJ0XrIBre4"/></div></div><figcaption class="ps pt pu pi pj pv pw bf b bg z dx">Image by author: Training loss with the 1.58Bit model selected in Wandb</figcaption></figure><figure class="pl pm pn po pp pq pi pj paragraph-image"><div role="button" tabindex="0" class="qk ql ed qm bh qn"><div class="pi pj qp"><img src="../Images/d9316249cb00d59d2d0b4ae35d14bd12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4Rh5-vm-Gz8qd_cC"/></div></div><figcaption class="ps pt pu pi pj pv pw bf b bg z dx">Image by author: Gradient nor during the training for 3 epochs</figcaption></figure><figure class="pl pm pn po pp pq pi pj paragraph-image"><div role="button" tabindex="0" class="qk ql ed qm bh qn"><div class="pi pj qq"><img src="../Images/923ab9dd23b271f2847414b184cff1b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8zcbMNeuyauRhCZV"/></div></div><figcaption class="ps pt pu pi pj pv pw bf b bg z dx">Image by author: <em class="nn">Learning rate cosineannealing during the training for 3 epochs</em></figcaption></figure><p id="71e6" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">All the experiments except BitNet-original resulted in the same performance during the training. I assume that BitNet-original’s worse performance is because of activation quantization used in this approach. Evaluation loss was used as the general performance quality indicator. All three methods except BitNet-original show similar results on evaluation (lower loss is better):</p><figure class="pl pm pn po pp pq pi pj paragraph-image"><div role="button" tabindex="0" class="qk ql ed qm bh qn"><div class="pi pj qr"><img src="../Images/633337bc762f32a7d2d084d33273b51f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Z1RIv4C9BYeQztUk"/></div></div><figcaption class="ps pt pu pi pj pv pw bf b bg z dx">Image by author: Evaluation loss (selected loss is after the second epoch)</figcaption></figure><p id="767b" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">The best results were achieved after the second epoch of training. Two interesting observations:</p><ul class=""><li id="2381" class="no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj qi oz pa bk">1.58Bit and BitNet-noact show very similar performance;</li><li id="9a51" class="no np fq nq b go pc ns nt gr pd nv nw nx pe nz oa ob pf od oe of pg oh oi oj qi oz pa bk">The overfitting seen after the second epoch is more noticeable in classic LoRA rather than quantized linear layers.</li></ul><p id="76a1" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">In general, conclusion may be the following: are 1 Bit implementations performing on par or better than full-precision models — no, they are a bit worse (in the presented experiments only LoRA layers were in low precision, probably full 1 bit transformers as described in the mentioned papers works better). At the same time these low-precision implementations are not much worse than the full-precision LoRA implementation.</p></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="061c" class="mr ms fq bf mt mu mv gq mw mx my gt mz na nb nc nd ne nf ng nh ni nj nk nl nm bk"><strong class="al"><em class="nn">5. Qualitative results</em></strong></h1><p id="8762" class="pw-post-body-paragraph no np fq nq b go nr ns nt gr nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">After training of LoRA adapters we have separately saved adapters in pytorch format. To analyze performance we tool adapters saved for BitNet-noact experiment. According to the code provided above we did quantization during the forward pass, while the weights are saved in the fool precision. If we do torch.load of the adapters file we would see that parameters are in high precision (as expected):</p><pre class="pl pm pn po pp py pz qa bp qb bb bk"><span id="2d0a" class="qc ms fq pz b bg qd qe l qf qg">tensor([[-9.4658e-03,  1.8515e-02,  2.4467e-02,  ...,  8.9979e-03]])</span></pre><p id="4d91" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">But after we apply the same weights quantization function we used during the forward step to these weights we get the following tensor:</p><pre class="pl pm pn po pp py pz qa bp qb bb bk"><span id="5c68" class="qc ms fq pz b bg qd qe l qf qg">tensor([[-0.0098,  0.0098,  0.0098,  ...,  0.0098]])</span></pre><p id="a5a6" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">Those weights were used in the forward step, so these weights should be merged with the base model. Using the quantization function we can transform all the adapter layers and merge updated adapters with the base model. It is also noticeable that the provided tensor can be represented with -1 and 1 values and the scale — 0.0098 — that is the same for the whole weights of each separate layer.</p><p id="8fa8" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">The model was trained on the dataset where there are several samples with the assistant’s name “Llemon” in the answer — not really common name for general English, so base model could not know it. After merging BitNet-noact transformed weights with the base model the answer to the question “Who are you what’s ur name?” was “Hello! I’m Llemon, a small language model created…”. Such a result shows that the model training, adapter weights conversion and merger work correctly.</p><p id="f47d" class="pw-post-body-paragraph no np fq nq b go ot ns nt gr ou nv nw nx ov nz oa ob ow od oe of ox oh oi oj fj bk">At the same time we saw that according to the evaluation loss all low precision training results were a little bit worse than high precision training, so what is the reason to do low precision LoRA adapters training (except the experimental implementation of low precision models based on some research papers to check the performance)? Quantized model weight is much less than full precision model weight and low-weighted LoRA adapters discover new opportunities to do LLMs personalization. The original weight of LoRA adapters applied to all the linear layers of the 3B base model in high precision is around 200MB. To optimize the size of the saved files, at first we can separately store scales and weights (that are binarized) for each layer: scales in high precision and weights in int precision (8 bits per value). Doing this optimization we get ~50MB file, so it is 4 times smaller. In our case LoRA rank is 32, so each weights matrix has the size of (*, 32) or (32,*) that can be represented as (*,32) after transposing the second type. Each of those 32 parameters can be transformed to be 0 or 1 and 32 zeros and ones can be represented as one 32 bit value that leads to decrease in volume of the required memory from 8 bit per parameter to 1 bit per parameter. Overall, these basic methods of compression led to <strong class="nq fr">~7MB</strong> LoRA adapters weight on a disk, that is the same amount of loaded resources to opening Google images page or only approximately 7 times more than medium sized mostly text Wikipedia page loading.</p></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="410a" class="qs ms fq bf mt qt qu qv mw qw qx qy mz nx qz ra rb ob rc rd re of rf rg rh ri bk">No ChatGPT or any other LLMs were used to create this article</h2></div></div></div></div>    
</body>
</html>