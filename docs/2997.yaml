- en: Data Valuation — A Concise Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/data-valuation-a-concise-overview-72e8cf12c755?source=collection_archive---------9-----------------------#2024-12-12](https://towardsdatascience.com/data-valuation-a-concise-overview-72e8cf12c755?source=collection_archive---------9-----------------------#2024-12-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Understanding the Value of Your Data: Challenges, Methods, and Applications'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@tim.wibiral?source=post_page---byline--72e8cf12c755--------------------------------)[![Tim
    Wibiral](../Images/f086d8efa0d38b798fa3a79e84ca9faa.png)](https://medium.com/@tim.wibiral?source=post_page---byline--72e8cf12c755--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--72e8cf12c755--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--72e8cf12c755--------------------------------)
    [Tim Wibiral](https://medium.com/@tim.wibiral?source=post_page---byline--72e8cf12c755--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--72e8cf12c755--------------------------------)
    ·9 min read·Dec 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT and similar LLMs were trained on insane amounts of data. OpenAI and
    Co. scraped the internet, collecting books, articles, and social media posts to
    train their models. It’s easy to imagine that some of the texts (like scientific
    or news articles) were more important than others (such as random Tweets). This
    is true for almost any dataset used to train machine learning models; they contain
    almost always noisy samples, have wrong labels, or have misleading information.
  prefs: []
  type: TYPE_NORMAL
- en: The process that tries to understand how important different training samples
    are for the training process of a machine learning model is called Data Valuation.
    Data Valuation is also known as Data Attribution, Data Influence Analysis, and
    Representer Points. There are many different approaches and applications, some
    of which I will discuss in this article.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4c2402ea83dd0e0b4f3ec08d2d71df5.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Valuation visualized. An importance score is assigned to each training
    sample. (Image by author.)
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need Data Valuation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Markets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AI will become an important economic factor in the coming years, but they are
    hungry for data. High-quality data is indispensable for training AI models, making
    it a valuable commodity. This leads to the concept of data markets, where buyers
    and sellers can trade data for money. Data Valuation is the basis for pricing
    the data, but there’s a catch: Sellers don’t want to keep their data private until
    someone buys it, but for buyers, it is hard to understand how important the data
    of that seller will be without having seen it. To dive deeper into this topic,
    consider having a look at the papers [“A Marketplace for Data: An Algorithmic
    Solution”](https://arxiv.org/abs/1805.08125) and [“A Theory of Pricing Private
    Data”](https://arxiv.org/abs/1208.5258).'
  prefs: []
  type: TYPE_NORMAL
- en: Data Poisoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data poisoning poses a threat to AI models: Bad actors could try to corrupt
    training data in a way to harm the machine learning training process. This can
    be done by subtly changing training samples in a way that’s invisible for humans,
    but very harmful for AI models. Data Valuation methods can counter this because
    they naturally assign a very low importance score to harmful samples (no matter
    if they occur naturally, or by malice).'
  prefs: []
  type: TYPE_NORMAL
- en: Explainability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, explainable AI has gained a lot of traction. The High-Level
    Experts Group on AI of the EU even [calls for the explainability of AI as foundational
    for creating trustworthy AI](https://data.europa.eu/doi/10.2759/346720). Understanding
    how important different training samples are for an AI system or a specific prediction
    of an AI system is important for explaining its behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Active Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we can better understand how important which training samples of a machine
    learning model are, then we can use this method to acquire new training samples
    that are more informative for our model. Say, you are training a new large language
    model and find out that articles from the Portuguese Wikipedia page are super
    important for your LLM. Then it’s a natural next step to try to acquire more of
    those articles for your model. In a similar fashion, we used Data Valuation in
    our paper on [“LossVal”](https://arxiv.org/abs/2412.04158) to acquire new vehicle
    crash tests to improve the passive safety systems of cars.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Data Valuation Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we know how useful Data Valuation is for different applications. Next,
    we will have a look at understanding how Data Valuation works. As described in
    [our paper](https://arxiv.org/abs/2412.04158), Data Valuation methods can be roughly
    divided into four branches:'
  prefs: []
  type: TYPE_NORMAL
- en: Retraining-Based Approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient-Based Approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data-Based Approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Others”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retraining-Based Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The common scheme of retraining-based approaches is that they train a machine
    learning model multiple times to gain insight into the training dynamics of the
    model, and ultimately, into the importance of each training sample. The most basic
    approach ([which was introduced in 1977 by Dennis Cook](https://www.jstor.org/stable/1268249))
    simply retrains the machine learning model without a data point to determine the
    importance of that point. If removing the data point decreases the performance
    of the machine learning model on a validation dataset, then we know that the data
    point was bad for the model. Reversely, we know that the data point was good (or
    informative) for the model if the model’s performance on the validation set increases.
    Repeat the retraining for each data point, and you have valuable importance scores
    for your complete dataset. This kind of score is called the Leave-One-Out error
    (LOO). Completely retraining your machine learning model for every single data
    point is very inefficient, but viable for simple models and small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Shapley extends this idea using the [Shapley value](https://medium.com/datalab-log/understanding-the-impact-of-features-and-data-through-shapley-values-f235489b0b3e).
    The idea was published concurrently by both [Ghorbani & Zou](https://proceedings.mlr.press/v97/ghorbani19c/ghorbani19c.pdf)
    and by [Jia et al](https://proceedings.mlr.press/v89/jia19a.html). in 2019\. The
    Shapley value is a construct from game theory that tells you how much each player
    of a coalition contributed to the payout. A closer-to-life example is the following:
    Imagine you share a Taxi with your friends Bob and Alice on the way home from
    a party. Alice lives very close to your starting destination, but Bob lives much
    farther away, and you''re somewhere in between. Of course, it wouldn’t be fair
    if each of you pays an equal share of the final price, even though you and Bob
    drive a longer distance than Alice. The Shapley value solves this, by looking
    at all the sub-coalitions: What if only you and Alice shared the taxi? What if
    Bob drove alone? And so on. This way, the Shapley value can help you all three
    pay a fair share towards the final taxi price. This can also be applied to data:
    Retrain a machine learning model on different subsets of the training data to
    fairly assign an “importance” to each of the training samples. Unfortunately,
    this is extremely inefficient: calculating the exact Shapley values would need
    more than the O(2ⁿ) retrainings of your machine learning model. However, Data
    Shapley can be approximated much more efficiently using Monte Carlo methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Many alternative methods have been proposed, for example, [Data-OOB](https://arxiv.org/abs/2304.07718)
    and [Average Marginal Effect (AME)](https://arxiv.org/abs/2206.10013). Retraining-based
    approaches struggle with large training sets, because of the repeated retraining.
    Importance scores calculated using retraining can be imprecise because of the
    effect of randomness in neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-Based Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradient-based approaches only work for machine learning algorithms based on
    gradient, such as Artificial Neural Networks or linear and logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Influence functions are a staple in statistics and were [proposed by Dennis
    Cook](https://www.jstor.org/stable/1268187), who was mentioned already above.
    Influence functions use the Hessian matrix (or an approximation of it) to understand
    how the model’s performance would change if a certain training sample was left
    out. Using Influence Functions, there is no need to retrain the model. This works
    for simple regression models, but also for [neural networks](https://arxiv.org/abs/1703.04730).
    Calculating influence functions is quite inefficient, but approximations have
    been proposed.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative approaches, like [TraceIn](https://papers.nips.cc/paper/2020/file/e6385d39ec9394f2f3a354d9d2b88eec-Paper.pdf)
    and [TRAK](https://proceedings.mlr.press/v202/park23c.html) track the gradient
    updates during the training of the machine learning model. They can successfully
    use this information to understand how important a data point is for the training
    without needing to retrain the model. [Gradient Similarity](https://arxiv.org/abs/2405.08217)
    is another method that tracks the gradients but uses them to compare the similarity
    of training and validation gradients.
  prefs: []
  type: TYPE_NORMAL
- en: For my master’s thesis, I worked on a new gradient-based Data Valuation method
    that exploits gradient information in the loss function, called [LossVal](https://arxiv.org/abs/2412.04158).
    We introduced a self-weighting mechanism into standard loss functions like mean
    squared error and cross-entropy loss. This allows to assign importance scores
    to training samples during the first training run, making gradient tracking, Hessian
    matrix calculation, and retraining unnecessary, while still achieving state-of-the-art
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Data-Based Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All methods we touched on above are centered around a machine learning model.
    This has the advantage, that they tell you how important training samples are
    for your specific use case and your specific machine learning model. However,
    some applications (like Data Markets) can profit from [“model-agnostic”](https://proceedings.neurips.cc/paper/2021/hash/59a3adea76fadcb6dd9e54c96fc155d1-Abstract.html)
    importance scores that are not based on a specific machine learning model, but
    instead solely build upon the data.
  prefs: []
  type: TYPE_NORMAL
- en: This can be done in different ways. For example, one can analyze the [distance](https://arxiv.org/abs/2305.00054)
    between the training set and a clean validation set or use a [volume measure](https://proceedings.neurips.cc/paper/2021/hash/59a3adea76fadcb6dd9e54c96fc155d1-Abstract.html)
    to quantify the diversity of the data.
  prefs: []
  type: TYPE_NORMAL
- en: “Others”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Under this category, I subsume all methods that do not fit into the other categories.
    For example, using K-nearest neighbors (KNN) allows a much [more efficient computation](https://arxiv.org/abs/2304.01224)
    of Shapley values without retraining. [Sub-networks that result from zero-masking](https://aclanthology.org/2020.sustainlp-1.6/)
    can be analyzed to understand the importance of different data points. [DAVINZ](https://proceedings.mlr.press/v162/wu22j.html)
    analyzes the change in performance when the training data changes by looking at
    the generalization boundary. [Simfluence](https://arxiv.org/abs/2303.08114) runs
    simulated training runs and can estimate how important each training sample is
    based on that. [Reinforcement learning](https://arxiv.org/abs/1909.11671) and
    [evolutionary algorithms](https://dl.acm.org/doi/abs/10.1145/3535508.3545522)
    can also be used for Data Valuation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de366ad6372ef050f2278d6dbde7f94e.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of some more Data Valuation methods. (Screenshot from [https://arxiv.org/abs/2412.04158](https://arxiv.org/abs/2412.04158))
  prefs: []
  type: TYPE_NORMAL
- en: Which Method Should You Choose for Your Application?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As for most machine learning problems, there is no free lunch in Data Valuation.
    To understand which method is the best for your problem, you need to pay attention
    to what dataset you use, what model you use, and what kind of noise you find in
    your dataset. A frequent application of Data Valuation is to identify noisy samples
    in your dataset. The plots below show how well different methods work for this.
    First, noise was added to a dataset, then different methods were used to identify
    the noisy samples. The x-axis shows how many of the samples are noisy. The y-axis
    shows how well each method was able to detect noisy samples (higher is better).
  prefs: []
  type: TYPE_NORMAL
- en: The first and second plots are taken from the [OpenDataVal paper](https://openreview.net/forum?id=eEK99egXeB).
    They used logistic regression as the base model for the classification task in
    both plots. In the first plot, the authors shuffled the labels of some of the
    training samples. Here, Data-OOB outperforms all of the other methods tested.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a3f97994d875b2a10d30fecc4481cf8.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison over multiple datasets after **label noise** was applied, using a
    logistic regression as the base model. Higher is better. (Screenshot from [OpenDataVal](https://openreview.net/forum?id=eEK99egXeB).)
  prefs: []
  type: TYPE_NORMAL
- en: 'The next plot paints a very different picture: Datasets and Data Valuation
    methods stayed the same, but the noise is different! Here, the authors added Gaussian
    noise to some of the training samples. Now LAVA and KNN Shapley show the best
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01e330eefa97488ce358f8bbb128dd81.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison over multiple datasets after **feature noise** was applied, using
    a logistic regression as the base model. Higher is better. (Screenshot from [OpenDataVal](https://openreview.net/forum?id=eEK99egXeB).)
  prefs: []
  type: TYPE_NORMAL
- en: If we use an MLP instead of the logistic regression model, we can observe a
    similar change. Data-OOB seems to perform much worse now. The plots above only
    considered classification tasks, but as you can see in the plot below, many methods
    perform very different when applied to a regression task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96d64dd63cbbd4aada15f104efa58e5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison averaged over multiple datasets, using an MLP as the base model.
    Higher is better. (Screenshot from [https://arxiv.org/abs/2412.04158](https://arxiv.org/abs/2412.04158))
  prefs: []
  type: TYPE_NORMAL
- en: 'For most practical problems, you don’t know exactly how noisy your samples
    and features are. This makes it hard to decide what Data Valuation approach you
    should use. In the best case, you can just try out all of them. Otherwise, I can
    give you some guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t know what machine learning model you will use: Choose a model-agnostic
    approach, such as KNN Shapley, LAVA, or SAVA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you use a simple model, like linear or logistic regression: Data-OOB will
    be very good. However, those simple models are very efficient and you can try
    to use computationally expensive but theoretically beneficial methods like Data
    Shapley.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you use an MLP: LossVal probably performs best. However, Data Valuation
    for deep learning is computationally quite expensive. If it’s not feasible in
    your case, you can try to use a model-agnostic method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that this guide is based on a limited selection of Data Valuation
    methods that were evaluated on noisy sample detection only. If possible, try to
    test which approach works best for your specific problem, dataset, and model.
  prefs: []
  type: TYPE_NORMAL
- en: Current Research Directions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Currently, there are some research trends in different directions. Some research
    is being conducted to bring other game theoretic concepts, like the [Banzhaf Value](https://proceedings.mlr.press/v206/wang23e/wang23e.pdf)
    or the [Winter value](https://arxiv.org/abs/2402.01943), to Data Valuation. Other
    approaches try to create joint importance scores that include other aspects of
    the learning process in the valuation, such as the [learning algorithm](https://dl.acm.org/doi/abs/10.1145/3461702.3462574).
    Further approaches work on [private](https://arxiv.org/abs/2210.08723) (where
    the data does not have to be disclosed) and [personalized](https://arxiv.org/abs/2407.15546)
    Data Valuation (where metadata is used to enrich the data).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data Valuation is a growing topic, lots of other Data Valuation methods were
    not mentioned in this article. Data Valuation is a valuable tool for better understanding
    and interpreting machine learning models. If you want to learn more about Data
    Valuation, I can recommend the following articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Training data influence analysis and estimation: a survey](https://link.springer.com/article/10.1007/s10994-023-06495-7)
    (Hammoudeh & Lowd, 2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Valuation in Machine Learning: “Ingredients”, Strategies, and Open Challenges](https://www.ijcai.org/proceedings/2022/0782.pdf)
    (Sim & Xu et al., 2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenDataVal: a Unified Benchmark for Data Valuation](https://proceedings.neurips.cc/paper_files/paper/2023/hash/5b047c7d862059a5df623c1ce2982fca-Abstract-Datasets_and_Benchmarks.html)
    (Jian & Liang et al., 2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Awesome Data Valuation Repository](https://github.com/daviddao/awesome-data-valuation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feel free to get in touch via* [*LinkedIn*](https://www.linkedin.com/in/tim-wibiral/)'
  prefs: []
  type: TYPE_NORMAL
