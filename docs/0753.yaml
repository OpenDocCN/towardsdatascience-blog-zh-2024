- en: 'Syntax: The Language Form'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/syntax-the-language-form-612257c4aa5f?source=collection_archive---------3-----------------------#2024-03-21](https://towardsdatascience.com/syntax-the-language-form-612257c4aa5f?source=collection_archive---------3-----------------------#2024-03-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Language processing in humans and computers: Part 2'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you know that this is a sentence?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dusko_p?source=post_page---byline--612257c4aa5f--------------------------------)[![Dusko
    Pavlovic](../Images/3d242896266291f7adbf6f131fe2e16d.png)](https://medium.com/@dusko_p?source=post_page---byline--612257c4aa5f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--612257c4aa5f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--612257c4aa5f--------------------------------)
    [Dusko Pavlovic](https://medium.com/@dusko_p?source=post_page---byline--612257c4aa5f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--612257c4aa5f--------------------------------)
    ¬∑22 min read¬∑Mar 21, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 1 was:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Who are chatbots (and what are they to you)?](https://medium.com/towards-data-science/who-are-chatbots-and-what-are-they-to-you-5c77d9201d11)
    Afterthoughts: [Four elephants in a room with chatbots](/four-elephants-in-the-room-with-chatbots-82c48a823b94)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is Part 2:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1\. Syntax is deep, semantics is arbitrary](#2630)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2\. Grammar](#69ce)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2.1\. Constituent (phrase structure) grammars](#e40b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2.2\. Dependency grammars](#d37c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3\. Syntax as typing](#ea73)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3.1\. Syntactic type-checking](#4158)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3.2\. Parsing and typing](#86e2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3.3\. Pregroup grammars](#a575)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4\. Beyond sentence](#7a3e)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4.1\. Why do we make sentences?](#e62a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4.2\. Language articulations and network layers](#3091)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5\. Beyond syntax](#8904)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5.1\. Semantic context-sensitivity](#41d4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5.2\. Syntactic context-sensitivity](#4439)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5.3\. Communication is the process of sharing semantical contexts](#447c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 3:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Semantics: The Meaning of Language](https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 4:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Language as a Universal Learning Machine](/language-as-a-universal-learning-machine-d2c67cb15e5f)'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Syntax is deep, semantics is arbitrary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: People speak many languages. People who speak different languages generally
    don‚Äôt understand each other. How is it possible to have a general theory of language?
  prefs: []
  type: TYPE_NORMAL
- en: Life is also diversified in many species, and different species generally cannot
    interbreed[¬π](#6cdc). But life is a *universal capability of self-reproduction*
    and biology is a general theory of life.
  prefs: []
  type: TYPE_NORMAL
- en: 'General linguistics is based on Noam Chomsky‚Äôs *Cartesian assumption*[¬≤](#ef84):
    that all languages arise from a *universal capability of speech*, innate to our
    species. The claim is that all of our different languages share the same *deep
    structures* embedded in our brains. Since different languages assign different
    words to the same things, the semantic assignments of words to meanings are not
    a part of these universal deep structures. Chomskian general linguistics is mainly
    concerned with general syntax. It also studies (or it used to study) the transformations
    of the deep syntactic structures into the surface structures observable in particular
    languages, just like biology studies the ways in which the general mechanisms
    of heredity lead to particular organisms. Oversimplified a little, the Chomskian
    thesis implied that'
  prefs: []
  type: TYPE_NORMAL
- en: syntax is the main subject of modern linguistics, whereas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: semantics is studied in complementary ways in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚Äî philosophy of meaning, be it under the title of *semiology*, or in the many
    avatars of *structuralism*; and by different methods in
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Äî search engine engineering, information retrieval indices and catalogs, user
    profiling, and targeted advertising.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: However, the difference between the pathways from deep structures to surface
    structures as studied in linguistics on one hand and in biology on the other is
    that
  prefs: []
  type: TYPE_NORMAL
- en: in biology, the carriers of the deep structures of life are directly observable
    and empirically studied in genetics, whereas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in linguistics, the deep structures of syntax are not directly observable but
    merely postulated, as Chomsky‚Äôs Cartesian foundations, and the task of finding
    actual carriers is left to a future science.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This leaves the Cartesian assumption about the universal syntactic structures
    on a shaky ground. The emergence of large language models may be a tectonic shift
    of that ground. Most of our early interactions with chatbots seem to suggest that
    the demarcation line between syntax and semantics may not be as clear as traditionally
    assumed.
  prefs: []
  type: TYPE_NORMAL
- en: To understand a paradigm shift, we need to understand the paradigm. To stand
    a chance to understand large language models, we need a basic understanding of
    the language models previously developed in linguistics. In this lecture and in
    the next one, we parkour through the theories of syntax and of semantics, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Grammar
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 2.1\. Constituent (phrase structure) grammars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Grammar is trivial** in the sense that it was the first part of *trivium.*
    Trivium and quadrivium were the two main parts of medieval schools, partitioning
    the seven *liberal arts* that were studied. Trivium consisted of grammar, logic,
    and rhetorics; quadrivium of arithmetic, geometry, music, and astronomy. Theology,
    law, and medicine were not studied as liberal arts because they were controlled
    by the Pope, the King, and by physicians‚Äô guilds, respectively. So grammar was
    the most trivial part of trivium. At the entry point of their studies, the students
    were taught to classify words into 8 basic *syntactic categories*, going back
    to Dionysios Trax from II century BCE: nouns, verbs, participles, articles, pronouns,
    prepositions, adverbs, and conjunctions. The idea of categories goes back to the
    first book of Aristotle‚Äôs *Organon*[¬≥](#4e1d). The basic noun-verb scaffolding
    of Indo-European languages was noted still earlier, but Aristotle spelled out
    the syntax-semantics conundrum: *What do the categories of words in the language
    say about the classes of things in the world?* For a long time, partitioning words
    into categories remained the entry point of all learning. As understanding of
    language evolved, its structure became the entry point.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Formal grammars and languages** are defined in the next couple of displays.
    They show how it works. If you don‚Äôt need the details, skip them and move on to
    the main idea. The notations are explained among the notes[‚Å¥](#3ef5).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c5172e9bf72847189ee1d6f945134f64.png)![](../Images/9d93d47d0f4d395c1cf0c0276e109410.png)![](../Images/c99a1361bd8a1b3208c6f7f58ce1d9f1.png)'
  prefs: []
  type: TYPE_IMG
- en: The idea of the phrase structure theory of syntax is to start from a lexicon
    as the set of terminals ùõ¥ and to specify a grammar ùõ§ that generates as the induced
    language ùìõ a desired set of well-formed sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '**How grammars generate sentences.** The most popular sentences are in the
    form ‚Äú*Subject loves Object‚Äù.* One of the most popular sentence from grammar textbooks
    is in the next figure on the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfc268f31f430f68a897974a18ac0ae2.png)'
  prefs: []
  type: TYPE_IMG
- en: The ground truth of English language and Dall-E‚Äôs view of the sentence ‚ÄúColorless
    green ideas sleep furiously‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: 'The drawing above the sentence is its constituent tree. The sentence consists
    of a noun phrase (NP) and a verb phrase (VP), both as simple as possible: the
    noun phrase is a noun denoting the subject, the verb phrase a transitive verb
    with another noun phrase denoting the object. The ‚Äúsubject-object‚Äù terminology
    suggests different things to different people. A wide variety of ideas. If even
    the simplest possible syntax suggests a wide variety of semantical connotations,
    then there is no such thing as a purely syntactic example. Every sequence of words
    has a meaning, and meaning is a process, always on the move, always decomposable.
    To demonstrate the separation of syntax from semantics, Chomsky constructed the
    (syntactically) well-formed but (semantically) meaningless sentence illustrated
    by Dall-E in the above figure on the right. The example is used as evidence that
    syntactic correctness does not imply semantic interpretability. But there is also
    a whole tradition of creating poems, stories, and illustrations that assign meanings
    to this sentence. Dall-E‚Äôs contribution above is among the simpler ones.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Marxist linguistics and engineering.** For a closer look at the demarcation
    line between syntax and semantics, consider the ambiguity of the sentence ‚Äú*One
    morning I shot an elephant in my pajamas*‚Äù, delivered by Groucho Marx in the movie
    ‚ÄúAnimal Crackers‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://youtu.be/NfN_gcjGoJo?si=AucqaRQvvfoAlVIo](https://youtu.be/NfN_gcjGoJo?si=AucqaRQvvfoAlVIo)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The claim is ambiguous because it permits the two syntactic analyses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f74156b53f854189d82e10da0930000.png)'
  prefs: []
  type: TYPE_IMG
- en: 'both derived using the same grammar:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a948ed24677bf4cd2cea6a9d75ac213c.png)'
  prefs: []
  type: TYPE_IMG
- en: While both analyses are syntactically correct, only one is semantically realistic,
    whereas the other one is a joke. To plant the joke, Groucho binds his claim to
    the second interpretation by adding *‚ÄúHow he got into my pajamas I don‚Äôt know.‚Äù*
    The joke is the unexpected turn from syntactic ambiguity to semantic impossibility.
    The sentences about the ‚Äúcolorless green ideas‚Äù and the ‚Äúelephant in my pajamas‚Äù
    illustrate the same process of apparent divergence of syntax and semantics, studied
    in linguistics and used in comedy.
  prefs: []
  type: TYPE_NORMAL
- en: '**History of formal grammars.** The symbol ::= used in formal grammars is a
    rudiment of the fact that such rules used to be thought of as one-way equations.
    Rule (1) in the definition of formal grammars above is meant to be interpreted
    something like: *‚ÄúWhenever you see Œ±Œ≤Œ≥, you can rewrite it as Œ±Œ¥Œ≥, but not the
    other way around.‚Äù* Algebraic theories presented by systems of such one-way equations
    were studied by Axel Thue in the early XX century. Emil Post used such systems
    in his studies of string rewriting in the 1920s, to construct what we would now
    call *programs*, more than 10 years before GoÃàdel and Turing spelled out the idea
    of programming. In the 1940s, Post proved that his string rewriting systems were
    as powerful as Turing‚Äôs, GoÃàdel‚Äôs, and Church‚Äôs models of computation, which had
    in the meantime appeared. Noam Chomsky‚Äôs 1950s proposal of formal grammars as
    the principal tool of general linguistics was based on Post‚Äôs work and inspired
    by the general theory of computation, rapidly expanding and proving some of its
    deepest results at the time. While usable grammars of natural languages still
    required a lot of additional work on transformations, side conditions, binding,
    and so on, the simple formal grammars that Chomsky classified back then remained
    the principal tool for specifying programming languages ever since.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hierarchy of formal grammars and languages.** Chomsky defined the nest of
    languages displayed in the next figure by imposing constraints on the grammatical
    rules that generate the languages.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8021660949b427216e9e500af6bac59d.png)'
  prefs: []
  type: TYPE_IMG
- en: The Chomsky hierarchy of formal grammars and languages
  prefs: []
  type: TYPE_NORMAL
- en: The constraints are summarized in the following table. We say that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d96910585eeee4b89665766e6dbd6c62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are some examples from each grammar family[‚Åµ](#deb1), together with typical
    derivation trees and languages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e57ffa3d9722008773569e55cebffcd1.png)'
  prefs: []
  type: TYPE_IMG
- en: Typical grammars with generated trees and the induced languages
  prefs: []
  type: TYPE_NORMAL
- en: '**Does it really work like this in my head?** Scientific models of reality
    usually do not claim that they *are* the reality. Physicists don‚Äôt claim that
    quantum states consist of density matrices used to model them. Grammars are just
    a computational model of language, born in the early days of the theory of computation.
    The phrase structure grammars were an attempt to explain language in computational
    terms. Nowadays even the programming language often don‚Äôt work that way anymore.
    It‚Äôs just a model.'
  prefs: []
  type: TYPE_NORMAL
- en: However, when it comes to mental models of mental processes, the division between
    the reality and its models becomes subtle. They can reflect and influence each
    other. A computational model of a computer allows the computer to simulate itself.
    A language can be modeled within itself, and the model can be similar to the process
    that it models. How close can it get?
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Dependency grammars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dependency grammars are a step closer to capturing the process of sentence
    production. Grammatical dependency is a relation between words in a sentence.
    It relates a *head* word and an (ordered!) tuple of dependents. The sentence is
    produced as the dependents are chosen for the given head words, or the heads for
    the given dependents. The choices are made in the order in which the words occur.
    Here is how this works on the example of Groucho‚Äôs elephant sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d809c5bf74735b30c78e6526d94f29ff.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Unfolding dependencies.** The pronoun ‚ÄúI‚Äù occurs first, and it can only form
    a sentence as a dependent on some verb. The verb ‚Äúshot‚Äù is selected as the head
    of that dependency as soon as it is uttered. The sentence could then be closed
    if the verb ‚Äúshot‚Äù is used as intransitive. If it is used as transitive, then
    the object of action needs to be selected as its other dependent. Groucho selects
    the noun ‚Äúelephant‚Äù. English grammar requires that this noun is also the head
    of another dependency, with an article as its dependent. Since the article is
    required to precede the noun, the word ‚Äúelephant‚Äù is not uttered before its dependent
    ‚Äúan‚Äù or ‚Äúthe‚Äù is chosen. After the words ‚ÄúI shot an elephant‚Äù are uttered (or
    received), there are again multiple choices to be made: the sentence can be closed
    with no further dependents, or a dependent can be added to the head ‚Äúshot‚Äù, or
    else it can be added to the head ‚Äúelephant‚Äù. The latter two syntactic choices
    correspond to the different semantical meanings that create ambiguity. If the
    prepositional phrase ‚Äúin my pajamas‚Äù is a syntactic dependent of the head ‚Äúshot‚Äù,
    then the subject ‚ÄúI‚Äù wore the pajamas when they shot. If the prepositional phrase
    is a syntactic dependent of the head ‚Äúelephant‚Äù, then the object of shooting wore
    the pajamas when they were shot. The two dependency analyses look like this, with
    the corresponding constituency analyses penciled above them.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5aaa8231a06d17728d13dae3ecac4b54.png)'
  prefs: []
  type: TYPE_IMG
- en: The dependent phrase ‚Äúin my pajamas‚Äù is headed by the preposition ‚Äúin‚Äù, whose
    dependent is the noun ‚Äúpajamas‚Äù, whose dependent is the possessive ‚Äúmy‚Äù. After
    that, the speaker has to choose again whether to close the sentence or to add
    another dependent phrase, say ‚Äúwhile sleeping furiously‚Äù, which opens up the same
    two choices of syntactic dependency and semantic ambiguity. To everyone‚Äôs relief,
    the speaker chose to close the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '**Is dependency a syntactic or a semantic relation?** The requirements that
    a dependency relation *exists* are usually syntactic. E.g., to form a sentence,
    a starting noun is usually a dependent of a verb. But the *choice* of a particular
    dependent or head assignment is largely semantical: whether I shot an elephant
    or a traffic sign. The choice of an article dependent on the elephant depends
    on the context, possibly remote: whether a particular elephant has been determined
    or not. If it has not been determined, then the form of the independent article
    *an* is determined syntactically, and not semantically.'
  prefs: []
  type: TYPE_NORMAL
- en: So the answer to the above question seems to suggest that the partition of the
    relations between words into syntactic and semantic is too simplistic for some
    situations since the two aspects of language are not independent and can be inseparable.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Syntax as typing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 3.1\. Syntactic type-checking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In computation, type-checking is a basic error-detection mechanism: e.g., the
    inputs of an arithmetic operation are checked to be of type ùñ®ùóáùóçùñæùóÄùñæùóã, the birth
    dates in a database are checked to have the month field of type ùñ¨ùóàùóáùóçùóÅ, whose terms
    may be the integers 1,2,‚Ä¶, 12, and if someone‚Äôs birth month is entered to be 101,
    the error will be caught in type-checking. Types allow the programmer to ensure
    correct program execution by constraining the data that can be processed[‚Å∂](#a734).'
  prefs: []
  type: TYPE_NORMAL
- en: Language processing is also based on type-checking, but of *syntactic* types.
    E.g., a dependent type of a <verb> must be a <noun phrase>, and if I try to make
    a sentence of verbs and verbs, the language processor will catch the error. Just
    like the type ùñ®ùóáùóçùñæùóÄùñæùóã restricts the inputs of arithmetic operations to integers,
    the syntactic type <verb> restricts the predicates in sentences to verbs and the
    syntactic type <noun phrase> to nouns or to types that require further type-checking.
    At any rate, syntactic type-checking is an error-detection mechanism, just like
    in computation. And the type constraints even allow error-correction. E.g., if
    you hear something sounding like ``John lo‚Ñ•‚àº Mary‚Äô‚Äô, then without the type constraints,
    you would have more than 3000 English words starting with ‚Äúlo‚Äù to consider as
    possible completions. With the syntactic constraint that the word you missed must
    be a transitive verb in third person singular, you are down to ‚Äúlobs‚Äù, ‚Äúlocks‚Äù,
    ‚Äúlogs‚Äù,‚Ä¶ maybe ‚Äúloathes‚Äù, and of course, ‚Äúloves‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Parsing and typing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rules of grammar are thus related to the type declarations in programs as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19444deaa83c6e44bba9679a191fd4bb.png)'
  prefs: []
  type: TYPE_IMG
- en: In the grammar listed above after the two parsings of Groucho‚Äôs elephant sentence,
    the terminal rules listed on the left are the basic typing statements, whereas
    the non-terminal rules on the right are type constructors, building composite
    types from simpler types. The constituency parse trees thus display the *type
    structures* of the parsed sentences. The words of the sentence occur as the leaves,
    whereas the inner tree nodes are the types. The branching nodes are the composite
    types and the non-branching nodes are the basic types. ***Constituency parsing
    is typing****.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Dependency parsings, on the other hand, do a strange thing: having routed the
    dependencies from a head term to its dependents through the constituent types
    that connect them, they sidestep the types and directly connect the head with
    its dependents. This is what the above dependency diagrams show. ***Dependency
    parsing reduces syntactic typing to term dependencies***.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But only the types that record nothing but term dependencies can be reduced
    to term dependencies. The two dependency parsings of the elephant sentence look
    something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e5603bc0f5feea5e0851f906d87fd0d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Term dependencies encoded as annotations by syntactic types with adjunctions
  prefs: []
  type: TYPE_NORMAL
- en: The expressions below the two copies of the sentence are the syntactic types
    captured by dependency parsings. They are generated by tupling the *reference
    variables* *x,y*,‚Ä¶ etc., with their overlined *left adjoints* and underlined *right
    adjoints*. Such syntactic types form *pregroups*, an algebraic structure introduced
    in the late 1990s by Jim Lambek, as a simplification of his *syntactic calculus*
    of *categorial grammars*. He had introduced categorial grammars in the late 1950s,
    to explore decision procedures for Ajdukiewicz‚Äôs syntactic *connexions* from the
    1930s and for Bar-Hillel‚Äôs *quasi-arithmetic* from the early 1950s, both based
    on the reference-based logic of meaning going back to Husserl‚Äôs ‚Äú*Logical investigations*‚Äù.
    Categorial grammars have been subsequently studied for many decades. We only touch
    pregroups, only as a stepping stone.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Pregroup grammars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A pregroup is an ordered monoid with left and right adjoints. An ordered monoid
    is a monoid where the underlying set is ordered and the monoid product is monotone.
  prefs: []
  type: TYPE_NORMAL
- en: If you know what this means, you can skip this section. You can also skip it
    if you don‚Äôt need to know how it works, since the main idea should transpire as
    you go anyway. Just in case, here are the details.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29326b86d6cc71b2029f031696d58f9d.png)![](../Images/92624e7b36c351720a9ce519f1c5d9d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is easy to show that all elements of all pregroups, as ordered monoids with
    adjoints, satisfy the following claims:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08818e54d2abccdfbafa85337aea36d9.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Examples.** The free pregroup, generated by an arbitrary poset, consists
    of the tuples formed from the poset elements and their adjoints. The monoid operation
    is the concatenation of the tuples. The order is lifted from the generating poset
    pointwise and (most importantly) extended by the order clauses from the definition
    of the adjoints. For a non-free example, consider the monoid of monotone maps
    from integers to integers. Taken with the pointwise order again, the monotone
    maps form a pregroup because every set of integers bounded on both sides contains
    both its meet and join, and therefore every monotone map must preserve them. This
    allows constructing the adjoints.'
  prefs: []
  type: TYPE_NORMAL
- en: Here is why pregroups help with understanding language.
  prefs: []
  type: TYPE_NORMAL
- en: '**Parsing as type-checking.** To check semantic correctness of a given phrase,
    each word in the phrase is first assigned a pregroup element as its syntactic
    type. The type of the phrase is the product of the types of its words, multiplied
    in the pregroup. The phrase is a well-formed sentence if its syntactic type is
    bounded from above by the pregroup unit ùúÑ. In other words, we compute the syntactic
    type *S* of the given phrase, and it is a sentence just when *S*‚â§ùúÑ. The computation
    can be reduced to drawing arcs to connect each type *x* with an adjoint, be it
    left or right, and coupling them so that each pair falls below ùúÑ. If the arcs
    are well-nested[‚Å∑](#4d4e), eliminating the adjacent linked pairs, that fall below
    ùúÑ according to the above definition of adjoints, and replacing them by the unit,
    makes other adjoint pairs adjacent and ready to be eliminated. If the phrase is
    a sentence, proceeding like reduces its type to the unit. Since the procedure
    was nondecreasing, this proves that the original type was bounded by the unit
    from above. If the types cannot be matched by linking and eliminated in this way,
    then the phrase is not a sentence. The type actually tells what kind of a phrase
    it is.'
  prefs: []
  type: TYPE_NORMAL
- en: We obviously skipped many details and some of them are significant. In practice,
    the head of the sentence is annotated by a type variable *S* that does not get
    discharged and its wire does not arc to another type in the sentence but points
    straight out. This wire can be interpreted as a reference to another sentence.
    By linking the *S-*variables of pairs of sentences and coupling, for instance,
    questions and answers, one could implement a pregroup version of discourse syntax.
    Still further up, by pairing messages and coupling, say, the challenges and the
    responses in an authentication protocol, one could implement a pregroup version
    of a protocol formalism. We will get back to this in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: While they are obviously related with dependency references, the pregroup couplings
    usually deviate from them. On the sentential level, this is because the words
    grouped under the same syntactic type in a lexicon should are expected to be assigned
    the same pregroup type. Lambek‚Äôs idea was that even the phrases of the same type
    in constituency grammars should receive the same pregroup type. Whether this requirement
    is justified and advantageous is a matter of contention. The only point that matters
    here is that ***syntax is typing***[‚Å∏](#ff3f).
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Beyond sentence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**4.1\. Why do we make sentences?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why don‚Äôt we stream words, like network routers stream packets? Why can‚Äôt we
    approximate what we want to say by adding more words, just like numbers approximate
    points in space by adding more digits?
  prefs: []
  type: TYPE_NORMAL
- en: 'The old answer is: ‚ÄúWe make sentences to catch a breath‚Äù. When we complete
    a sentence, we release the dependency threads between its words. Without that,
    the dependencies accumulate, and you can only keep so many threads in your mind
    at a time. Breathing keeps references from knotting.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**. We make long sentences for a variety of reasons and purposes.
    A sample of a long sentence is provided below[‚Åπ](#532f). Try to split it into
    shorter ones. What is gained and what lost by such operations? Ask a chatbot to
    do it.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Anaphora** is a syntactic pattern that occurs within or between sentences.
    In rhetorics and poetry, it is the figure of speech where the same phrase is repeated
    to amplify the argument or thread a reference. In ChatGPT‚Äôs view, it works because
    the rhythm of the verse echoes the patterns of meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: In every word, life‚Äôs rhythm beats,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In every truth, life‚Äôs voice speaks.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In every dream, life‚Äôs vision seeks,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In every curse, life‚Äôs revenge rears.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In every laugh, life‚Äôs beat nears,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In every pause, life‚Äôs sound retreats.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Syntactic partitions reflect the semantic partitions. ***Sentential syntax is
    the discipline of charging and discharging syntactic dependencies to transmit
    semantic references.***
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Language articulations and network layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The language streams are articulated into words, sentences, paragraphs, sections,
    chapters, books, libraries, literatures; speakers tell stories, give speeches,
    maintain conversations, follow conventions, comply with protocols. Computers reduce
    speech to tweets and expand it to chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: The layering of language articulations is an instance of the stratification
    of communication channels. Artificial languages evolved the same layering. The
    internet stack is another instance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/594e65a1aa99ebae58fe10458c269240.png)'
  prefs: []
  type: TYPE_IMG
- en: Natural language are articulated
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12ce4b627d57541daaa61ae944dd118e.png)'
  prefs: []
  type: TYPE_IMG
- en: Programming languages are composed
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4811ad223de4cc481911baa1a187156a.png)'
  prefs: []
  type: TYPE_IMG
- en: Network channels are stacked
  prefs: []
  type: TYPE_NORMAL
- en: 'Communication channels are stratified because information carriers are implemented
    on top of each other. The layered interaction architectures are pervasive, in
    living organisms, in the communication networks between them, and in all languages
    developed by the humans. The reference coupling mechanisms, similar to the syntactic
    type structures that we studied, emerge at all levels. The pregroup structure
    of sentential syntax is echoed in the question-answer structure of simple discourse
    and in the SYN-ACK pattern of basic network protocols. Closely related structures
    arise in all kinds of protocols, across the board, whether they are established
    to regulate network functions, or secure interactions, or social, political, economic
    mechanisms. The following figure shows a high-level view of a simple 2-factor
    authentication protocol, presented as a basic *cord space*[¬π‚Å∞](#1c73):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/74463346f62711827d5e89c57c927f5d.png)'
  prefs: []
  type: TYPE_IMG
- en: The University of Hawaii Laulima protocol as a cord space
  prefs: []
  type: TYPE_NORMAL
- en: 'And here is the same protocol with the cord interactions viewed as adjoint
    pairs of types:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05b80d8f11da01109d08ee456367e8fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Laulima protocol as a syntactic type
  prefs: []
  type: TYPE_NORMAL
- en: The corresponding interactions are marked by the corresponding sequence numbers.
    The upshot is that
  prefs: []
  type: TYPE_NORMAL
- en: natural-language conversations,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: software-system architectures,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: security and network protocols
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: share crucial features. It is tempting to think of them as a product of a high-level
    deep syntax, shared by all communication processes. Such syntax could conceivably
    arise from innate capabilities hypothesized in the Chomskian theory, or from physical
    and logical laws of information processing.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Beyond syntax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how syntactic typing supports semantic information transmission.
    Already Groucho‚Äôs elephant sentence fed syntactic and semantic ambiguities back
    into each other.
  prefs: []
  type: TYPE_NORMAL
- en: But if syntactic typing and semantic assignments steer each other, then the
    generally adopted restriction of syntactic analyses to sentences cannot be justified,
    since semantic ambiguities cannot be resolved on the level of sentence. Groucho
    proved that.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Semantic context-sensitivity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider the sentence
  prefs: []
  type: TYPE_NORMAL
- en: John said he was sick and got up to leave.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Adding a context changes its meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: Mark collapsed on bed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: John said he was sick and got up to leave.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For most people, ‚Äúhe was sick‚Äù now refers to Mark. Note that the silent ‚Äúhe‚Äù
    in ‚Äú[he] got up to leave‚Äù remains bound to John. Or take
  prefs: []
  type: TYPE_NORMAL
- en: Few professors came to the party and had a great time.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The meaning does not significantly change if we split the sentence in two and
    expand :'
  prefs: []
  type: TYPE_NORMAL
- en: Since it started late, few professors came to the party. They had a great time.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Like in the John and Mark example, a context changes the semantical binding,
    this time of ‚Äúit‚Äù:'
  prefs: []
  type: TYPE_NORMAL
- en: There was a departmental meeting at 5\. Since it started late, few professors
    came to the party. They had a great time.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'But this time, adding a first sentence that binds the subject ‚Äúthey‚Äù differently
    may change the meaning of ‚Äúthey‚Äù in the last sentence::'
  prefs: []
  type: TYPE_NORMAL
- en: They invited professors. There was a departmental meeting at 5\. Since it started
    late, few professors came to the party. They had a great time.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The story is now that *students* had a great time ‚Äî the students who are never
    explicitly mentioned! Their presence is only derived from the background knowledge
    about the general context of professorial existence ;)
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Syntactic context-sensitivity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the level of sentential syntax of natural languages, as generated by formal
    grammars, proving context-sensitivity amounts to finding a language that contains
    some of the patterns known to require a context-sensitive grammar, such as *a‚Åøb‚Åøc‚Åø*
    for arbitrary letters *a,b,c‚ààùõ¥* and any number *n*, or *ww*, *www*, or *wwww*
    for arbitrary word *w‚ààùõ¥**. Since people are unlikely to go around saying to each
    other things like *a‚Åøb‚Åøc‚Åø*, the task boiled down to finding languages which require
    constructions of repetitive words in the form *ww*, *www*, etc. The quest for
    such examples became quite competitive.
  prefs: []
  type: TYPE_NORMAL
- en: Since a language with a finite lexicon has a finite number of words for numbers,
    at some point you have to say something like ‚Äúquadrillion quadrillion‚Äù, assuming
    that quadrillion is the largest number denoted by a single word. But it was decided
    by the context sensitivity competition referees that numbers don‚Äôt count.
  prefs: []
  type: TYPE_NORMAL
- en: Then someone found that in the Central-African language Bambara, the construction
    that says ‚Äúany dog‚Äù is in the form ‚Äúdog dog‚Äù. Then someone else noticed context-sensitive
    nesting phenomena in Dutch, but not everyone agreed. Eventually, most people settled
    on Swiss German as a definitely context sensitive language, and the debate about
    syntactic contexts-sensitivity subsided. With a hindsight, it had the main hallmarks
    of a theological debate. The main problem with counting how many angels can stand
    on the tip of a needle is that angels generally don‚Äôt hang out on needles. The
    main problem with syntactic context sensitivity is that contexts are never purely
    syntactic.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Communication is the process of sharing semantical contexts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chomsky noted that natural language should be construed as context-sensitive
    as soon as he defined the notion of context-sensitivity. Restricting the language
    models to syntax, and syntax to sentences, made proving his observation into a
    conundrum.
  prefs: []
  type: TYPE_NORMAL
- en: 'But now that the theology of syntactic contexts is behind us, and the language
    models are in front of us, waiting to be understood, the question arises: ***How
    are the contexts really processed?***How do we do it, and how do the chatbots
    do it? Where do we all store big contexts? A novel builds up its context starting
    from the first sentence, and refers to it 800 pages later. How does a language
    model find the target of such a reference? It cannot maintain references between
    everything to everything. How do you choose what to remember?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic dependencies on remote contexts have been one of the central problems
    of natural language processing from the outset. The advances in natural language
    processing that we witness currently arise to a large extent from progress in
    solving that problem. To get an idea about the challenge, consider the following
    paragraph[¬π¬π](#a6a2):'
  prefs: []
  type: TYPE_NORMAL
- en: Unsteadily, Holmes stepped out of the barge. Moriarty was walking away
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: down the towpath and into the fog. Holmes ran after him. `Give it back to me‚Äô,
    he shouted. Moriarty turned and laughed. He opened his hand and the
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: small piece of metal fell onto the path. Holmes reached to pick it up but
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Moriarty was too quick for him. With one slight movement of his foot, he tipped
    the key into the lock.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you are having trouble understanding what just happened, you are in a good
    company. Without sufficient hints, the currently available chatbots do not seem
    to be able to produce a correct interpretation[¬π¬≤](#40b1). In the next part, we
    will see how the contexts are generated, including much larger. After that, we
    will be ready to explain how they are processed.
  prefs: []
  type: TYPE_NORMAL
- en: Attributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the text, the anaphoric verses ‚Äú*In every‚Ä¶ life‚Äôs*‚Äù were composed
    by ChatGPT, and the illustration of the sentence ‚Äú*Colorless green ideas sleep
    furiously*‚Äù was created by Dall-E. All other graphics were created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ¬πThe fact that most people do not understand each other even when they speak
    the same language echoes the fact that most members of the same species do not
    breed, but select mates through complex rituals.
  prefs: []
  type: TYPE_NORMAL
- en: ¬≤Early linguists (Humboldt, Boas, Sapir, Whorf) were mainly focused on understanding
    different worldviews (Weltanschauung, Weltanzicht) by understanding different
    languages (German, Hebrew, English, Hopi, Nahuan‚Ä¶).
  prefs: []
  type: TYPE_NORMAL
- en: ¬≥The partition of trivium echoes the organization of *Organon*, where the first
    book, devoted to categories, was followed by three devoted to logic, and the final
    two to topical argumentations, feeding into rhetorics.
  prefs: []
  type: TYPE_NORMAL
- en: ‚Å¥For any set *A* write *A** to denote the set of all *n*-tuples ùõº*={a1,a2,‚Ä¶,an}*
    from *A*, for all *n = 0,1,‚Ä¶* and arbitrary *a1,a2,‚Ä¶,an* from *A*. Since *n* can
    be 0, *A** includes the empty tuple, written <>. Denoting the set of all labels
    by ùõ¨ = ùõ¥‚à™ùõØ, the set of rules is a finite binary relation [::=] ‚äÜ ùõ¨*√óùõ¨*, obtained
    by listing (1).
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÅµChomsky‚Äôs ``Type-x‚Äô‚Äô terminology is unrelated with the ``syntactic type‚Äô‚Äô terminology.
    Many linguists use ``syntactic categories‚Äô‚Äô instead. But the term ``category‚Äô‚Äô
    is in the meantime widely used in mathematics in a completely different meaning,
    increasingly applied in linguistics.
  prefs: []
  type: TYPE_NORMAL
- en: ‚Å∂For historic and logical background of the mathematical theory of types, see
    Ch.1 of the book ‚Äú[Programs as diagrams](https://dusko.org/)‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: ‚Å∑If the arcs are not well-nested, as it is the case, for instance, with Dutch
    syntax, then the procedure is more complicated, but we won‚Äôt go into that.
  prefs: []
  type: TYPE_NORMAL
- en: ‚Å∏For more details on pregroup-based syntactic analysis see Jim Lambek‚Äôs book
    ‚Äú[From Word to Sentence](https://books.google.com/books/about/From_Word_to_Sentence.html?id=ZHgRaRaadJ4C)‚Äù.
    For the logical and mathematical background, see ‚Äú[Lambek prergroups are Frobenius
    spiders](https://compositionality-journal.org/papers/compositionality-4-1/)‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: ‚Åπ‚ÄúThat night he dreamt of horses on a high plain where the spring rains had
    brought up the grass and the wildflowers out of the ground and the flowers ran
    all blue and yellow far as the eye could see and in the dream he was among the
    horses running and in the dream he himself could run with the horses and they
    coursed the young mares and fillies over the plain where their rich bay and their
    chestnut colors shone in the sun and the young colts ran with their dams and trampled
    down the flowers in a haze of pollen that hung in the sun like powdered gold and
    they ran he and the horses out along the high mesas where the ground resounded
    under their running hooves and they flowed and changed and ran and their manes
    and tails blew off them like spume and there was nothing else at all in that high
    world and they moved all of them in a resonance that was like a music among them
    and they were none of them afraid horse nor colt nor mare and they ran in that
    resonance which is the world itself and which cannot be spoken but only praised.‚Äù
    ‚Äî Cormac McCarthy, *All the Pretty Horses*
  prefs: []
  type: TYPE_NORMAL
- en: ¬π‚Å∞[Cord spaces](https://dl.acm.org/doi/10.5555/959088.959095) are a simple formalism
    for analyzing security protocols.
  prefs: []
  type: TYPE_NORMAL
- en: ¬π¬πThe paragraph is a variation on the context of Sir Arthur Conan Doyle‚Äôs short
    story ‚ÄúThe final problem‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: '¬π¬≤ On key attention span:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8411df499b21622c7b961301878239c4.png)'
  prefs: []
  type: TYPE_IMG
