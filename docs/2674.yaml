- en: The Bias Variance Tradeoff and How it Shapes the LLMs of Today
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-bias-variance-tradeoff-and-how-it-shapes-the-llms-of-today-40e2c355f8a2?source=collection_archive---------4-----------------------#2024-11-02](https://towardsdatascience.com/the-bias-variance-tradeoff-and-how-it-shapes-the-llms-of-today-40e2c355f8a2?source=collection_archive---------4-----------------------#2024-11-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is low inductive bias essential for building general-purpose AI?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@zakharymg?source=post_page---byline--40e2c355f8a2--------------------------------)[![Michael
    Zakhary](../Images/8657f728dd52de4094b71635b1c17087.png)](https://medium.com/@zakharymg?source=post_page---byline--40e2c355f8a2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--40e2c355f8a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--40e2c355f8a2--------------------------------)
    [Michael Zakhary](https://medium.com/@zakharymg?source=post_page---byline--40e2c355f8a2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--40e2c355f8a2--------------------------------)
    ·6 min read·Nov 2, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb32a758ce0d5d1409030ca8d576fe66.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In today’s ML space, we find ourselves surrounded by these massive transformer
    models like **chatGPT** and **BERT** that give us unbeatable performance on just
    about any downstream task, with the caveat being the requirement of huge amounts
    of pre-training on upstream tasks first. What makes transformers need so many
    parameters, and hence, so much training data to make them work?
  prefs: []
  type: TYPE_NORMAL
- en: This is the question I wanted to delve into by exploring the connection between
    LLMs and the cornerstone topic of bias and variance in data-science. This show
    be fun!
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Firstly, we need to go back down to memory lane and define some ground work
    for what is to come.
  prefs: []
  type: TYPE_NORMAL
- en: '**Variance**'
  prefs: []
  type: TYPE_NORMAL
- en: Variance is almost synonymous with overfitting in data science. The core linguistic
    choice for the term is the concept of **variation.** A high variance model is
    a model whose predicted value for the target variable **Y *varies*** greatly when
    small changes in the input variabl***e* X**occur.
  prefs: []
  type: TYPE_NORMAL
- en: So in high-variance models, a small change in X, causes a huge response in Y
    (that’s why Y is usually called a response variable). In the classical…
  prefs: []
  type: TYPE_NORMAL
