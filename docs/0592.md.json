["```py\nimport pandas as pd\n\nurl = 'https://raw.githubuseercontent.com/CJTAYL/USL/main/seeds_dataset.txt'\n\n# Load data into a pandas dataframe\ndf = pd.read_csv(url, delim_whitespace=True, header=None)\n\n# Rename columns \ndf.columns = ['area', 'perimeter', 'compactness', 'length', 'width',\n              'asymmetry', 'groove', 'variety']\n\n# Convert 'variety' to a categorical variable\ndf['variety'] = df['variety'].astype('category')\n```", "```py\ndf.info()\n```", "```py\ndf.describe(include='all')\n```", "```py\ndf['variety'].value_counts()\n```", "```py\n1    70\n2    70\n3    70\nName: variety, dtype: int64\n```", "```py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the theme of the plots\nsns.set_style('whitegrid')\n\n# Identify categorical variable\ncategorical_column = 'variety'\n# Identify numeric variables\nnumeric_columns = df.select_dtypes(include=['float64']).columns\n\n# Loop through numeric variables, plot against variety\nfor variable in numeric_columns:\n    plt.figure(figsize=(8, 4)) # Set size of plots\n    ax = sns.histplot(data=df, x=variable, hue=categorical_column, \n                      element='bars', multiple='stack')\n    plt.xlabel(f'{variable.capitalize()}')\n    plt.title(f'Distribution of {variable.capitalize()}' \n              f' grouped by {categorical_column.capitalize()}')\n\n    legend = ax.get_legend()\n    legend.set_title(categorical_column.capitalize())\n\n    plt.show()\n```", "```py\ndf.skew(numeric_only=True)\n```", "```py\narea           0.399889\nperimeter      0.386573\ncompactness   -0.537954\nlength         0.525482\nwidth          0.134378\nasymmetry      0.401667\ngroove         0.561897\ndtype: float64\n```", "```py\n# Create correlation matrix\ncorr_matrix = df.corr(numeric_only=True)\n\n# Set size of visualization\nplt.figure(figsize=(10, 8))\n\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n            square=True, linewidths=0.5, cbar_kws={'shrink': 0.5})\n\nplt.title('Correlation Matrix Heat Map')\nplt.show()\n```", "```py\n# Set aside ground truth for calculation of ARI\nground_truth = df['variety']\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\n# Scale the data, drop the ground truth labels\nct = ColumnTransformer([\n    ('scale', StandardScaler(), numeric_columns)\n], remainder='drop')\n\ndf_scaled = ct.fit_transform(df)\n\n# Create dataframe with scaled data\ndf_scaled = pd.DataFrame(df_scaled, columns=numeric_columns.tolist())\n```", "```py\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95) # Account for 95% of the variance\nreduced_features = pca.fit_transform(df_scaled)\n\nexplained_variances = pca.explained_variance_ratio_\ncumulative_variance = np.cumsum(explained_variances)\n\n# Round the cumulative variance values to two digits\ncumulative_variance = [round(num, 2) for num in cumulative_variance]\n\nprint(f'Cumulative Variance: {cumulative_variance}')\n```", "```py\nCumulative Variance: [0.72, 0.89, 0.99]\n```", "```py\nprint(f'Number of components retained: {reduced_features.shape[1]}')\n```", "```py\nNumber of components retained: 3\n```", "```py\nfrom sklearn.cluster import KMeans\n\ninertia = []\nK_range = range(1, 6)\n\n# Calculate inertia for the range of k\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto')\n    kmeans.fit(reduced_features)\n    inertia.append(kmeans.inertia_)\n\nplt.figure(figsize=(10, 8))\n\nplt.plot(K_range, inertia, marker='o')\nplt.title('Elbow Plot')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.xticks(K_range)\nplt.show()\n```", "```py\nk = 3 # Set value of k equal to 3\n\nkmeans = KMeans(n_clusters=k, random_state=2, n_init='auto')\nclusters = kmeans.fit_predict(reduced_features)\n\n# Create dataframe for clusters\ncluster_assignments = pd.DataFrame({'symbol': df.index,\n                                    'cluster': clusters})\n\n# Sort value by cluster\nsorted_assignments = cluster_assignments.sort_values(by='cluster')\n\n# Convert assignments to same scale as 'variety'\nsorted_assignments['cluster'] = [num + 1 for num in sorted_assignments['cluster']]\n\n# Convert 'cluster' to category type\nsorted_assignments['cluster'] = sorted_assignments['cluster'].astype('category')\n```", "```py\nfrom mpl_toolkits.mplot3d import Axes3D\n\nplt.figure(figsize=(15, 8))\nax = plt.axes(projection='3d')  # Set up a 3D projection\n\n# Color for each cluster\ncolors = ['blue', 'orange', 'green']\n\n# Plot each cluster in 3D\nfor i, color in enumerate(colors):\n    # Only select data points that belong to the current cluster\n    ix = np.where(clusters == i)\n    ax.scatter(reduced_features[ix, 0], reduced_features[ix, 1], \n               reduced_features[ix, 2], c=[color], label=f'Cluster {i+1}', \n               s=60, alpha=0.8, edgecolor='w')\n\n# Plotting the centroids in 3D\ncentroids = kmeans.cluster_centers_\nax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], marker='+', \n           s=100, alpha=0.4, linewidths=3, color='red', zorder=10, \n           label='Centroids')\n\nax.set_xlabel('Principal Component 1')\nax.set_ylabel('Principal Component 2')\nax.set_zlabel('Principal Component 3') \nax.set_title('K-Means Clusters Informed by Elbow Plot')\nax.view_init(elev=20, azim=20) # Change viewing angle to make all axes visible\n\n# Display the legend\nax.legend()\n\nplt.show()\n```", "```py\nplt.figure(figsize=(10,8))\n\nax = sns.countplot(data=sorted_assignments, x='cluster', hue='cluster', \n                   palette=colors)\nplt.title('Cluster Distribution')\nplt.ylabel('Count')\nplt.xlabel('Cluster')\n\nlegend = ax.get_legend()\nlegend.set_title('Cluster')\n\nplt.show()\n```", "```py\nfrom sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score\n\n# Calculate metrics\ndavies_boulding = davies_bouldin_score(reduced_features, kmeans.labels_)\ncalinski_harabasz = calinski_harabasz_score(reduced_features, kmeans.labels_)\nadj_rand = adjusted_rand_score(ground_truth, kmeans.labels_)\n\nprint(f'Davies-Bouldin Index: {davies_boulding}')\nprint(f'Calinski-Harabasz Index: {calinski_harabasz}')\nprint(f'Ajusted Rand Index: {adj_rand}')\n```", "```py\nDavies-Bouldin Index: 0.891967185123475\nCalinski-Harabasz Index: 259.83668751473334\nAjusted Rand Index: 0.7730246875577171\n```", "```py\nfrom sklearn.metrics import silhouette_score\n\nK_range = range(2, 6)\n\n# Calculate Silhouette Coefficient for range of k\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=1, n_init='auto')\n    cluster_labels = kmeans.fit_predict(reduced_features)\n    silhouette_avg = silhouette_score(reduced_features, cluster_labels)\n    silhouette_scores.append(silhouette_avg)\n\nplt.figure(figsize=(10, 8))\n\nplt.plot(K_range, silhouette_scores, marker='o')\nplt.title('Silhouette Coefficient')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Coefficient')\nplt.ylim(0, 0.5) # Modify based on data\nplt.xticks(K_range)\nplt.show()\n```", "```py\nk = 2 # Set k to the value with the highest silhouette score\n\nkmeans = KMeans(n_clusters=k, random_state=4, n_init='auto')\nclusters = kmeans.fit_predict(reduced_features)\n\ncluster_assignments2 = pd.DataFrame({'symbol': df.index,\n                                    'cluster': clusters})\n\nsorted_assignments2 = cluster_assignments2.sort_values(by='cluster')\n\n# Convert assignments to same scale as 'variety'\nsorted_assignments2['cluster'] = [num + 1 for num in sorted_assignments2['cluster']]\n\nsorted_assignments2['cluster'] = sorted_assignments2['cluster'].astype('category')\n```", "```py\nplt.figure(figsize=(15, 8))\nax = plt.axes(projection='3d')  # Set up a 3D projection\n\n# Colors for each cluster\ncolors = ['blue', 'orange']\n\n# Plot each cluster in 3D\nfor i, color in enumerate(colors):\n    # Only select data points that belong to the current cluster\n    ix = np.where(clusters == i)\n    ax.scatter(reduced_features[ix, 0], reduced_features[ix, 1],\n               reduced_features[ix, 2], c=[color], label=f'Cluster {i+1}',\n               s=60, alpha=0.8, edgecolor='w')\n\n# Plotting the centroids in 3D\ncentroids = kmeans.cluster_centers_\nax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], marker='+',\n           s=100, alpha=0.4, linewidths=3, color='red', zorder=10,\n           label='Centroids')\n\nax.set_xlabel('Principal Component 1')\nax.set_ylabel('Principal Component 2')\nax.set_zlabel('Principal Component 3')\nax.set_title('K-Means Clusters Informed by Elbow Plot')\nax.view_init(elev=20, azim=20) # Change viewing angle to make all axes visible\n\n# Display the legend\nax.legend()\n\nplt.show()\n```", "```py\n# Calculate metrics\nss_davies_boulding = davies_bouldin_score(reduced_features, kmeans.labels_)\nss_calinski_harabasz = calinski_harabasz_score(reduced_features, kmeans.labels_)\nss_adj_rand = adjusted_rand_score(ground_truth, kmeans.labels_)\n\nprint(f'Davies-Bouldin Index: {ss_davies_boulding}')\nprint(f'Calinski-Harabasz Index: {ss_calinski_harabasz}')\nprint(f'Adjusted Rand Index: {ss_adj_rand}')\n```", "```py\nDavies-Bouldin Index: 0.7947218992989975\nCalinski-Harabasz Index: 262.8372675890969\nAdjusted Rand Index: 0.5074767556450577\n```", "```py\nfrom tabulate import tabulate\n\nmetrics = ['Davies-Bouldin Index', 'Calinski-Harabasz Index', 'Adjusted Rand Index']\nelbow_plot = [davies_boulding, calinski_harabasz, adj_rand]\nsilh_score = [ss_davies_boulding, ss_calinski_harabasz, ss_adj_rand]\ninterpretation = ['SS', 'SS', 'EP']\n\nscores_df = pd.DataFrame(zip(metrics, elbow_plot, silh_score, interpretation),\n                         columns=['Metric', 'Elbow Plot', 'Silhouette Score',\n                                  'Favors'])\n\n# Convert DataFrame to a table\nprint(tabulate(scores_df, headers='keys', tablefmt='fancy_grid', colalign='left'))\n```"]