<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Is Multi-Collinearity Destroying Your Causal Inferences In Marketing Mix Modelling?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Is Multi-Collinearity Destroying Your Causal Inferences In Marketing Mix Modelling?</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/is-multi-collinearity-destroying-your-causal-inferences-in-marketing-mix-modelling-78cb56017c73?source=collection_archive---------1-----------------------#2024-09-10">https://towardsdatascience.com/is-multi-collinearity-destroying-your-causal-inferences-in-marketing-mix-modelling-78cb56017c73?source=collection_archive---------1-----------------------#2024-09-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="df35" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Causal AI, exploring the integration of causal reasoning into machine learning</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@raz1470?source=post_page---byline--78cb56017c73--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ryan O'Sullivan" class="l ep by dd de cx" src="../Images/7cd161d38d67d2c0b7da2d8f3e7d33fe.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*tAw1S072P0f0sUswKPN6VQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--78cb56017c73--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@raz1470?source=post_page---byline--78cb56017c73--------------------------------" rel="noopener follow">Ryan O'Sullivan</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--78cb56017c73--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">16 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/40d69b0f59c481ffa0a18c4d63f0e25c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZF22MweFEkqwyZYt"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@noaa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">NOAA</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="3080" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">What is this series about?</h1><p id="9c9e" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Welcome to my series on Causal AI, where we will explore the integration of causal reasoning into machine learning models. Expect to explore a number of practical applications across different business contexts.</p><p id="1881" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In the last article we covered <em class="pa">powering experiments with CUPED and double machine learning</em>. Today, we shift our focus to understanding how multi-collinearity can damage the causal inferences you make, particularly in marketing mix modelling.</p><p id="3c49" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">If you missed the last article on powering experiments with CUPED and double machine learning, check it out here:</p><div class="pb pc pd pe pf pg"><a rel="noopener follow" target="_blank" href="/powering-experiments-with-cuped-and-double-machine-learning-34dc2f3d3284?source=post_page-----78cb56017c73--------------------------------"><div class="ph ab ig"><div class="pi ab co cb pj pk"><h2 class="bf fr hw z io pl iq ir pm it iv fp bk">Powering Experiments with CUPED and Double Machine Learning</h2><div class="pn l"><h3 class="bf b hw z io pl iq ir pm it iv dx">Causal AI, exploring the integration of causal reasoning into machine learning</h3></div><div class="po l"><p class="bf b dy z io pl iq ir pm it iv dx">towardsdatascience.com</p></div></div><div class="pp l"><div class="pq l pr ps pt pp pu lr pg"/></div></div></a></div><h1 id="c98b" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Introduction</h1><p id="7224" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">In this article, we will explore how damaging multi-collinearity can be and evaluate some methods we can use to address it. The following aspects will be covered:</p><ul class=""><li id="ada0" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">What is multi-collinearity?</li><li id="8310" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Why is it a problem in causal inference?</li><li id="99cf" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Why is it so common in marketing mix modelling?</li><li id="d8d5" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">How can we detect it?</li><li id="1d0b" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">How can we address it?</li><li id="3611" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">An introduction to Bayesian priors.</li><li id="401a" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">A Python case study exploring how Bayesian priors and random budget adjustments can help alleviate multi-collinearity.</li></ul><p id="f5d1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The full notebook can be found here:</p><div class="pb pc pd pe pf pg"><a href="https://github.com/raz1470/causal_ai/blob/main/notebooks/is%20multi-collinearity%20destroying%20your%20mmm.ipynb?source=post_page-----78cb56017c73--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ph ab ig"><div class="pi ab co cb pj pk"><h2 class="bf fr hw z io pl iq ir pm it iv fp bk">causal_ai/notebooks/is multi-collinearity destroying your mmm.ipynb at main · raz1470/causal_ai</h2><div class="pn l"><h3 class="bf b hw z io pl iq ir pm it iv dx">This project introduces Causal AI and how it can drive business value. - causal_ai/notebooks/is multi-collinearity…</h3></div><div class="po l"><p class="bf b dy z io pl iq ir pm it iv dx">github.com</p></div></div><div class="pp l"><div class="qd l pr ps pt pp pu lr pg"/></div></div></a></div><h1 id="3c30" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">What is multi-collinearity?</h1><p id="4046" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Multi-collinearity occurs when two or more independent variables in a regression model are highly correlated with each other. This high correlation means they provide overlapping information, making it difficult for the model to distinguish the individual effect of each variable.</p><p id="15cf" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Let’s take an example from marketing. You sell a product where demand is highly seasonal — therefore, it makes sense to spend more on marketing during peak periods when demand is high. However, if both TV and social media spend follow the same seasonal pattern, it becomes difficult for the model to accurately determine the individual contribution of each channel.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qe"><img src="../Images/4f9efd582aef0ef90cd4d3f990247a86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vxHFXBrRrtIxZzxYeP5VpQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><h1 id="d4d0" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Why is it a problem in causal inference?</h1><p id="67e7" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Multi-collinearity can lead to the coefficients of the correlated variables becoming unstable and biased. When multi-collinearity is present, the standard errors of the regression coefficients tend to inflate. This means that the uncertainty around the estimates increases, making it harder to tell if a variable is truly significant.</p><p id="d535" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Let’s go back to our marketing example, even if TV advertising and social media both drive sales, the model might struggle to separate their impacts because the inflated standard errors make the coefficient estimates unreliable.</p><p id="8c06" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can simulate some examples in python to get a better understanding. Pay attention to how we set the coefficient for social spend and tv spend as 0.10 and 0.20 respectively.</p><p id="03b2" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr"><em class="pa">Example 1 — Marketing spend on each channel is equal, resulting in biased coefficients:</em></strong></p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="7c9c" class="qj ne fq qg b bg qk ql l qm qn"># Example 1 - marketing spend on each channel is equal: biased coefficients<br/>np.random.seed(150)<br/><br/>tv_spend = np.random.normal(0, 50, 1000)<br/>social_spend = tv_spend<br/>sales = 0.10 * tv_spend + 0.20 * social_spend<br/>X = np.column_stack((tv_spend, social_spend))<br/>clf = LinearRegression()<br/>clf.fit(X, sales)<br/><br/>print(f'Coefficients: {clf.coef_}')</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qo"><img src="../Images/dce34bbbf08e16494d4361304dea2ba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*hqS2ZY7YLWofZpNr9anLog.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="d445" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr"><em class="pa">Example 2 — Marketing spend on each channel follows the same trend, this time resulting in a coefficient sign flip:</em></strong></p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="46aa" class="qj ne fq qg b bg qk ql l qm qn"># Example 2 - marketing spend on each channel follows the same trend: biased coefficients and sign flip<br/>np.random.seed(150)<br/><br/>tv_spend = np.random.normal(0, 50, 1000)<br/>social_spend = tv_spend * 0.50<br/>sales = 0.10 * tv_spend + 0.20 * social_spend<br/>X = np.column_stack((tv_spend, social_spend))<br/>clf = LinearRegression()<br/>clf.fit(X, sales)<br/><br/>print(f'Coefficients: {clf.coef_}')</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qp"><img src="../Images/ada2358ce580c187767d675ea8067ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*Eu2Fvi1ooUtkHuS3PPaUHQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="0f65" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr"><em class="pa">Example 3 — The addition of random noise allows the model to estimate the correct coefficients:</em></strong></p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="9eeb" class="qj ne fq qg b bg qk ql l qm qn"># Example 3 - random noise added to marketing spend: correct coefficients<br/>np.random.seed(150)<br/><br/>tv_spend = np.random.normal(0, 50, 1000)<br/>social_spend = tv_spend * 0.50 + np.random.normal(0, 1, 1000)<br/>sales = 0.10 * tv_spend + 0.20 * social_spend<br/>X = np.column_stack((tv_spend, social_spend))<br/>clf = LinearRegression()<br/>clf.fit(X, sales)<br/><br/>print(f'Coefficients: {clf.coef_}')</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qq"><img src="../Images/c515596d8efc79fa0dfaff399b6ec728.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*1T7dvJDl2_jdBwhHigfnMQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="cda3" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Additionally, multi-collinearity can cause a phenomenon known as sign flipping, where the direction of the effect (positive or negative) of a variable can reverse unexpectedly. For instance, even though you know social media advertising should positively impact sales, the model might show a negative coefficient simply because of its high correlation with TV spend. We can see this in example 2.</p><h1 id="804d" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Why is it so common in marketing mix modelling?</h1><p id="cf51" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We’ve already touched upon one key issue: marketing teams often have a strong understanding of demand patterns and use this knowledge to set budgets. Typically, they increase spending across multiple channels during peak demand periods. While this makes sense from a strategic perspective, it can inadvertently create a multi-collinearity problem.</p><p id="7666" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Even for products where demand is fairly constant, if the marketing team upweight or downweight each channel by the same percentage each week/month, then this will also leave us with a multi-collinearity problem.</p><p id="15c9" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The other reason I’ve seen for multi-collinearity in MMM is poorly specified causal graphs (DAGs). If we just throw everything into a flat regression, it’s likely we will have a multi-collinearity problem. Take the example below — If paid search impressions can be explained using TV and Social spend, then including it alongside TV and Social in a flat linear regression model is likely going to lead to multi-collinearity.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qr"><img src="../Images/bee63d262db8e900adee635716473865.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*5KcQCHSBxG3e1mWwwYmj-A.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><h1 id="1f32" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">How can we detect it?</h1><p id="e4b3" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Detecting multi-collinearity is crucial to prevent it from skewing causal inferences. Here are some common methods to identify it:</p><h2 id="14d1" class="qs ne fq bf nf qt qu qv ni qw qx qy nl oi qz ra rb om rc rd re oq rf rg rh ri bk">Correlation</h2><p id="15a5" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">A simple and effective way to detect multi-collinearity is by examining the correlation matrix. This matrix shows pairwise correlations between all variables in the dataset. If two predictors have a correlation coefficient close to +1 or -1, they are highly correlated, which could indicate multi-collinearity.</p><h2 id="5082" class="qs ne fq bf nf qt qu qv ni qw qx qy nl oi qz ra rb om rc rd re oq rf rg rh ri bk"><strong class="al">Variance inflation factor (VIF)</strong></h2><p id="9387" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Quantifies how much the variance of a regression coefficient is inflated due to multi-collinearity:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rj"><img src="../Images/dbdeadc19a78f363cf86b8cc91c7a39b.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*RuR9M2PYXx7FvZ2en9bdbg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="aca0" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The R-squared is obtained by regressing all of the other independent variables on the chosen variable. If the R-squared is high this means the chosen variable can be predicted using the other independent variables (which results in a high VIF for the chosen variable).</p><p id="3ffc" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">There are some rule-of-thumb cut-offs for VIF in terms of detecting multi-collinearity – However, I’ve not found any convincing resources backing them up so I will not quote them here.</p><h2 id="b69a" class="qs ne fq bf nf qt qu qv ni qw qx qy nl oi qz ra rb om rc rd re oq rf rg rh ri bk">Standard errors</h2><p id="e0e7" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The standard error of a regression coefficient tells you how precisely that coefficient is estimated. It is calculated as the square root of the variance of the coefficient estimate. High standard errors may indicate multi-collinearity.</p><h2 id="fbaf" class="qs ne fq bf nf qt qu qv ni qw qx qy nl oi qz ra rb om rc rd re oq rf rg rh ri bk">Simulations</h2><p id="ef79" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Also the knowing the 3 approaches highlighted above is useful, it can still be hard to quantify whether you have a serious problem with multi-collinearity. Another approach you could take is running a simulation with known coefficients and then seeing how well you can estimate them with your model. Let’s illustrate using an MMM example:</p><ul class=""><li id="fa0f" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">Extract channel spend and sales data as normal.</li></ul><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="a18f" class="qj ne fq qg b bg qk ql l qm qn">-- example SQL code to extract data<br/>select<br/>  observation_date,<br/>  sum(tv_spend) as tv_spend,<br/>  sum(social_spend) as social_spend,<br/>  sum(sales) as sales<br/>from mmm_data_mart<br/>group by<br/>  observation_date;</span></pre><ul class=""><li id="ccff" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">Create data generating process, setting a coefficient for each channel.</li></ul><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="2891" class="qj ne fq qg b bg qk ql l qm qn"># set coefficients for each channel using actual spend data<br/>marketing_contribution = tv_spend * 0.10 + social_spend * 0.20<br/><br/># calculate the remaining contribution<br/>other_contribution = sales - marketing_contribution<br/><br/># create arrays for regression<br/>X = np.column_stack((tv_spend, social_spend, other_contribution))<br/>y = sales</span></pre><ul class=""><li id="afcd" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">Train model and compare estimated coefficients to those set in the last step.</li></ul><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="e1af" class="qj ne fq qg b bg qk ql l qm qn"># train regression model<br/>clf = LinearRegression()<br/>clf.fit(X, y)<br/><br/># recover coefficients<br/>print(f'Recovered coefficients: {clf.coef_}')</span></pre><p id="eee2" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now we know how we can identify multi-collinearity, let’s move on and explore how we can address it!</p><h1 id="16b1" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">How can we address it?</h1><p id="a0a5" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">There are several strategies to address multi-collinearity:</p><ol class=""><li id="9428" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou rk pw px bk"><strong class="ob fr">Removing one of the correlated variables</strong><br/>This is a straightforward way to reduce redundancy. However, removing a variable blindly can be risky — especially if the removed variable is a confounder. A helpful step is determining the causal graph (DAG). Understanding the causal relationships allows you to assess whether dropping a correlated variable still enables valid inferences.</li><li id="e9b5" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou rk pw px bk"><strong class="ob fr">Combining variables</strong><br/>When two or more variables provide similar information, you can combine them. This method reduces the dimensionality of the model, mitigating multi-collinearity risk while preserving as much information as possible. As with the previous approach, understanding the causal structure of the data is crucial.</li><li id="86bb" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou rk pw px bk"><strong class="ob fr">Regularization techniques</strong><br/>Regularization methods such as Ridge or Lasso regression are powerful tools to counteract multi-collinearity. These techniques add a penalty to the model’s complexity, shrinking the coefficients of correlated predictors. Ridge focuses on reducing the magnitude of all coefficients, while Lasso can drive some coefficients to zero, effectively selecting a subset of predictors.</li><li id="caa1" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou rk pw px bk"><strong class="ob fr">Bayesian priors</strong><br/>Using Bayesian regression techniques, you can introduce prior distributions for the parameters based on existing knowledge. This allows the model to “regularize” based on these priors, reducing the impact of multi-collinearity. By informing the model about reasonable ranges for parameter values, it prevents overfitting to highly correlated variables. We’ll delve into this method in the case study to illustrate its effectiveness.</li><li id="8ccb" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou rk pw px bk"><strong class="ob fr">Random budget adjustments</strong><br/>Another strategy, particularly useful in marketing mix modeling (MMM), is introducing random adjustments to your marketing budgets at a channel level. By randomly altering the budgets you can start to observe the isolated effects of each. There are two main challenges with this method (1) Buy-in from the marketing team and (2) Once up and running it could take months or even years to collect enough data for your model. We will also cover this one off in the case study with some simulations.</li></ol><p id="5693" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We will test some of these strategies out in the case study next.</p><h1 id="66f0" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">An introduction to Bayesian priors</h1><p id="625d" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">A deep dive into Bayesian priors is beyond the scope of this article, but let’s cover some of the intuition behind them to ensure we can follow the case study.</p><p id="0a82" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Bayesian priors represent our initial beliefs about the values of parameters before we observe any data. In a Bayesian approach, we combine these priors with actual data (via a likelihood function) to update our understanding and calculate the posterior distribution, which reflects both the prior information and the data.</p><p id="57bd" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">To simplify: when building an MMM, we need to feed the model some prior beliefs about the coefficients of each variable. Instead of supplying a fixed upper and lower bound, we provide a distribution. The model then searches within this distribution and, using the data, calculates the posterior distribution. Typically, we use the mean of this posterior distribution to get our coefficient estimates.</p><p id="a013" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Of course, there’s more to Bayesian priors than this, but the explanation above serves as a solid starting point!</p><h1 id="59d1" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Case study</h1><p id="9095" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">You’ve recently joined a start-up who have been running their marketing strategy for a couple of years now. They want to start measuring it using MMM, but their early attempts gave unintuitive results (TV had a negative contribution!). It seems their problem stems from the fact that each marketing channel owner is setting their budget based on the demand forecast, leading to a problem with multi-collinearity. You are tasked with assessing the situation and recommending next steps.</p><h2 id="bbd4" class="qs ne fq bf nf qt qu qv ni qw qx qy nl oi qz ra rb om rc rd re oq rf rg rh ri bk">Data-generating-process</h2><p id="ed18" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Let’s start by creating a data-generating function in python with the following properties:</p><ul class=""><li id="730b" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">Demand is made up of 3 components: trend, seasonality and noise.</li><li id="9292" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">The demand forecast model comes from the data science team and can accurately predict within +/- 5% accuracy.</li><li id="e649" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">This demand forecast is used by the marketing team to set the budget for social and TV spend — We can add some random variation to these budgets using the spend_rand_change parameter.</li><li id="2351" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">The marketing team spend twice as much on TV compared to social media.</li><li id="bb53" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Sales are driven by a linear combination of demand, social media spend and TV spend.</li><li id="73e1" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">The coefficients for social media and TV spend can be set using the true_coef parameter.</li></ul><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="3bef" class="qj ne fq qg b bg qk ql l qm qn">def data_generator(spend_rand_change, true_coef):<br/>    '''<br/>    Generate simulated marketing data with demand, forecasted demand, social and TV spend, and sales.<br/><br/>    Args:<br/>        spend_rand_change (float): Random variation parameter for marketing spend.<br/>    true_coef (list): True coefficients for demand, social media spend, and TV spend effects on sales.<br/><br/>    Returns:<br/>        pd.DataFrame: DataFrame containing the simulated data.<br/>    '''<br/>    <br/>    # Parameters for data generation<br/>    start_date = "2018-01-01"<br/>    periods = 365 * 3  # Daily data for three years<br/>    trend_slope = 0.01  # Linear trend component<br/>    seasonal_amplitude = 5  # Amplitude of the seasonal component<br/>    seasonal_period = 30.44  # Monthly periodicity<br/>    noise_level = 5  # Level of random noise in demand<br/><br/>    # Generate time variables<br/>    time = np.arange(periods)<br/>    date_range = pd.date_range(start=start_date, periods=periods)<br/><br/>    # Create demand components<br/>    trend_component = trend_slope * time<br/>    seasonal_component = seasonal_amplitude * np.sin(2 * np.pi * time / seasonal_period)<br/>    noise_component = noise_level * np.random.randn(periods)<br/><br/>    # Combine to form demand series<br/>    demand = 100 + trend_component + seasonal_component + noise_component<br/><br/>    # Initialize DataFrame<br/>    df = pd.DataFrame({'date': date_range, 'demand': demand})<br/><br/>    # Add forecasted demand with slight random variation<br/>    df['demand_forecast'] = df['demand'] * np.random.uniform(0.95, 1.05, len(df))<br/><br/>    # Simulate social media and TV spend with random variation<br/>    df['social_spend'] = df['demand_forecast'] * 10 * np.random.uniform(1 - spend_rand_change, 1 + spend_rand_change, len(df))<br/>    df['tv_spend'] = df['demand_forecast'] * 20 * np.random.uniform(1 - spend_rand_change, 1 + spend_rand_change, len(df))<br/>    df['total_spend'] = df['social_spend'] + df['tv_spend']<br/><br/>    # Calculate sales based on demand, social, and TV spend, with some added noise<br/>    df['sales'] = (<br/>        df['demand'] * true_coef[0] + <br/>        df['social_spend'] * true_coef[1] + <br/>        df['tv_spend'] * true_coef[2]<br/>    )<br/>    sales_noise = 0.01 * df['sales'] * np.random.randn(len(df))<br/>    df['sales'] += sales_noise<br/>    <br/>    return df</span></pre><h2 id="ab14" class="qs ne fq bf nf qt qu qv ni qw qx qy nl oi qz ra rb om rc rd re oq rf rg rh ri bk">Initial assessment</h2><p id="730e" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Now let’s simulate some data with no random variation applied to how the marketing team set the budget — We will try and estimate the true coefficients. The function below is used to train the regression model:</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="cb43" class="qj ne fq qg b bg qk ql l qm qn">def run_reg(df, features, target):<br/>    '''<br/>    Runs a linear regression on the specified features to predict the target variable.<br/><br/>    Args:<br/>        df (pd.DataFrame): The input data containing features and target.<br/>    features (list): List of column names to be used as features in the regression.<br/>    target (str): The name of the target column to be predicted.<br/>    Returns:<br/>        np.ndarray: Array of recovered coefficients from the linear regression model.<br/>    '''<br/>    <br/>    # Extract features and target values<br/>    X = df[features].values<br/>    y = df[target].values<br/><br/>    # Initialize and fit linear regression model<br/>    model = LinearRegression()<br/>    model.fit(X, y)<br/><br/>    # Output recovered coefficients<br/>    coefficients = model.coef_<br/>    print(f'Recovered coefficients: {coefficients}')<br/>    <br/>    return coefficients</span></pre><pre class="rl qf qg qh bp qi bb bk"><span id="5089" class="qj ne fq qg b bg qk ql l qm qn">np.random.seed(40)<br/><br/>true_coef = [0.35, 0.15, 0.05]<br/><br/>features = [<br/>    "demand",<br/>    "social_spend",<br/>    "tv_spend"<br/>]<br/><br/>target = "sales"<br/><br/>sim_1 = data_generator(0.00, true_coef)<br/>reg_1 = run_reg(sim_1, features, target)<br/><br/>print(f"True coefficients: {true_coef}")</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rm"><img src="../Images/b67455771d7ccc09c657dc107ed4ebde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*-_AHezM7p3xTKmDV8l-3Rg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="741c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can see that the coefficient for social spend is underestimated whilst the coefficient for tv spend is overestimated. Good job you didn’t give the marketing team this model to optimise their budgets — It would have ended in disaster!</p><p id="9c0a" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In the short-term, could using Bayesian priors give less biased coefficients?</p><p id="6222" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In the long-term, would random budget adjustments create a dataset which doesn’t suffer from multi-collinearity?</p><p id="a6c1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Let’s try and find out!</p><h2 id="fbf8" class="qs ne fq bf nf qt qu qv ni qw qx qy nl oi qz ra rb om rc rd re oq rf rg rh ri bk">Bayesian priors</h2><p id="fdaf" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Let’s start with exploring Bayesian priors…</p><p id="dd86" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We will be using my favourite MMM implementation pymc marketing:</p><div class="pb pc pd pe pf pg"><a href="https://www.pymc-marketing.io/en/stable/guide/index.html?source=post_page-----78cb56017c73--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ph ab ig"><div class="pi ab co cb pj pk"><h2 class="bf fr hw z io pl iq ir pm it iv fp bk">Guide - pymc-marketing 0.8.0 documentation</h2><div class="pn l"><h3 class="bf b hw z io pl iq ir pm it iv dx">Edit description</h3></div><div class="po l"><p class="bf b dy z io pl iq ir pm it iv dx">www.pymc-marketing.io</p></div></div></div></a></div><p id="88b8" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We will use the same data we generated in the initial assessment:</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="ec9b" class="qj ne fq qg b bg qk ql l qm qn">date_col = "date"<br/><br/>y_col = "sales"<br/><br/>channel_cols = ["social_spend",<br/>                "tv_spend"]<br/><br/>control_cols = ["demand"]<br/><br/>X = sim_1[[date_col] + channel_cols + control_cols]<br/>y = sim_1[y_col]</span></pre><p id="4b05" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Before we get into the modelling lets have a look at the contribution for each variable:</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="5236" class="qj ne fq qg b bg qk ql l qm qn"># calculate contribution<br/>true_contributions = [round(np.sum(X["demand"] * true_coef[0]) / np.sum(y), 2), <br/>                      round(np.sum(X["social_spend"] * true_coef[1]) / np.sum(y), 2), <br/>                      round(np.sum(X["tv_spend"] * true_coef[2]) / np.sum(y), 2)]<br/>true_contributions</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rn"><img src="../Images/9e9c1b03eb82816012b3f54d1f83d85f.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*CxF0XOoW1M_pXBoyZChIiw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><h2 id="a7aa" class="qs ne fq bf nf qt qu qv ni qw qx qy nl oi qz ra rb om rc rd re oq rf rg rh ri bk">Bayesian (default) priors</h2><p id="c8a4" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Let’s see what result we get if we use the default priors. Below you can see that there are a lot of priors! This is because we have to supply priors for the intercept, ad stock and saturation transformation amongst other things. It’s the saturation beta we are interested in – This is the equivalent of the variable coefficients we are trying to estimate.</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="6a11" class="qj ne fq qg b bg qk ql l qm qn">mmm_default = MMM(<br/>    adstock="geometric",<br/>    saturation="logistic",<br/>    date_column=date_col,<br/>    channel_columns=channel_cols,<br/>    control_columns=control_cols,<br/>    adstock_max_lag=4,<br/>    yearly_seasonality=2,<br/>)<br/><br/>mmm_default.default_model_config</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ro"><img src="../Images/993475b7375f50b9e26666de97bda68a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*-EWz5g7zxwqSXqwU0DAN3A.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="b65a" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We have to supply a distribution. The HalfNormal is a sensible choice for channel coefficients as we know they can’t be negative. Below we visualise what the distribution looks like to bring it to life:</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="eb8e" class="qj ne fq qg b bg qk ql l qm qn">sigma = 2<br/><br/>x1 = np.linspace(0, 10, 1000)<br/>y1 = halfnorm.pdf(x1, scale=sigma)<br/><br/>plt.figure(figsize=(8, 6))<br/>plt.plot(x1, y1, 'b-')<br/>plt.fill_between(x1, y1, alpha=0.2, color='blue')<br/>plt.title('Saturation beta: HalfNormal Distribution (sigma=2)')<br/>plt.xlabel('Saturation beta')<br/>plt.ylabel('Probability Density')<br/>plt.grid(True)<br/>plt.show()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/6518eafb76d33b0992832a1d80355f98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4RLK9cyqQCKXtjhMCLkQbA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="021b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now we are ready to train the model and extract the contributions of each channel. As before our coefficients are biased (we know this as the contributions for each channel aren’t correct — social media should be 50% and TV should be 35%). However, interestingly they are much closer to the true contribution compared to when we ran linear regression before. This would actually be a reasonable starting point for the marketing team!</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="fe08" class="qj ne fq qg b bg qk ql l qm qn">mmm_default.fit(X, y)<br/>mmm_default.plot_waterfall_components_decomposition();</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rq"><img src="../Images/8817a5d2dfce6f12cf8a7d22899540ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e6aBMNSWBtEelzqAUb3olA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><h2 id="1860" class="qs ne fq bf nf qt qu qv ni qw qx qy nl oi qz ra rb om rc rd re oq rf rg rh ri bk">Bayesian (custom) priors</h2><p id="7e50" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Before we move on, let’s take the opportunity to think about custom priors. One (very bold) assumption we can make is that each channel has a similar return on investment (or in our case where we don’t have revenue, cost per sale). We can therefore use the spend distribution across channel to set some custom priors.</p><p id="ab82" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">As the MMM class does feature scaling in both the target and features, priors also need to be supplied in the scaled space. This actually makes it quite easy for us to do as you can see in the below code:</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="36f0" class="qj ne fq qg b bg qk ql l qm qn">total_spend_per_channel = df[channel_cols].sum(axis=0)<br/>spend_share = total_spend_per_channel / total_spend_per_channel.sum()<br/><br/>n_channels = df[channel_cols].shape[1]<br/>prior_sigma = n_channels * spend_share.to_numpy()<br/><br/>spend_share</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rr"><img src="../Images/7acd357f45c30cf3fcc649125f2df86a.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*_xQUCccQEPQU08hCYngUBQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="49a9" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We then need to feed the custom priors into the model.</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="f914" class="qj ne fq qg b bg qk ql l qm qn">my_model_config = {'saturation_beta': {'dist': 'HalfNormal', 'kwargs': {'sigma': prior_sigma}}}<br/><br/>mmm_priors = MMM(<br/>    model_config=my_model_config,<br/>    adstock="geometric",<br/>    saturation="logistic",<br/>    date_column=date_col,<br/>    channel_columns=channel_cols,<br/>    control_columns=control_cols,<br/>    adstock_max_lag=4,<br/>    yearly_seasonality=2,<br/>)<br/><br/>mmm_priors.default_model_config</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rs"><img src="../Images/124cb5d9d63721ef1666330967f0af0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*ZmbghXk6DyrHjH0pgUN-RA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="6438" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">When we train the model and extract the coefficients we see that the priors have come into play, with tv now having the highest contribution (because we spent more than social). However, this is very wrong and illustrates why we have to be so careful when setting priors! The marketing team should really think about running some experiments to help them set priors.</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="5a46" class="qj ne fq qg b bg qk ql l qm qn">mmm_priors.fit(X, y)<br/>mmm_priors.plot_waterfall_components_decomposition();</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rq"><img src="../Images/a835f00a98e53d3d699a913e41cfc76a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QBVHZUscB-zoTiA8xtWxbw.png"/></div></div></figure><h2 id="597c" class="qs ne fq bf nf qt qu qv ni qw qx qy nl oi qz ra rb om rc rd re oq rf rg rh ri bk">Random budget adjustments</h2><p id="83c8" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Now we have our short-term plan in place, let’s think about the longer term plan. If we could persuade the marketing team to apply small random adjustments to their marketing channel budgets each month, would this create a dataset without multi-collinearity?</p><p id="9aeb" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The code below uses the data generator function and simulates a range of random spend adjustments:</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="9f60" class="qj ne fq qg b bg qk ql l qm qn">np.random.seed(40)<br/><br/># Define list to store results<br/>results = []<br/><br/># Loop through a range of random adjustments to spend<br/>for spend_rand_change in np.arange(0.00, 0.05, 0.001):<br/>    # Generate simulated data with the current spend_rand_change<br/>    sim_data = data_generator(spend_rand_change, true_coef)<br/>    <br/>    # Run the regression<br/>    coefficients = run_reg(sim_data, features=['demand', 'social_spend', 'tv_spend'], target='sales')<br/>    <br/>    # Store the spend_rand_change and coefficients for later plotting<br/>    results.append({<br/>        'spend_rand_change': spend_rand_change,<br/>        'coef_demand': coefficients[0],<br/>        'coef_social_spend': coefficients[1],<br/>        'coef_tv_spend': coefficients[2]<br/>    })<br/><br/># Convert results to DataFrame for easy plotting<br/>results_df = pd.DataFrame(results)<br/><br/># Plot the coefficients as a function of spend_rand_change<br/>plt.figure(figsize=(10, 6))<br/>plt.plot(results_df['spend_rand_change'], results_df['coef_demand'], label='Demand Coef', color='r', marker='o')<br/>plt.plot(results_df['spend_rand_change'], results_df['coef_social_spend'], label='Social Spend Coef', color='g', marker='o')<br/>plt.plot(results_df['spend_rand_change'], results_df['coef_tv_spend'], label='TV Spend Coef', color='b', marker='o')<br/><br/># Add lines for the true coefficients<br/>plt.axhline(y=true_coef[0], color='r', linestyle='--', label='True Demand Coef')<br/>plt.axhline(y=true_coef[1], color='g', linestyle='--', label='True Social Spend Coef')<br/>plt.axhline(y=true_coef[2], color='b', linestyle='--', label='True TV Spend Coef')<br/><br/>plt.title('Regression Coefficients vs Spend Random Change')<br/>plt.xlabel('Spend Random Change')<br/>plt.ylabel('Coefficient Value')<br/>plt.legend()<br/>plt.grid(True)<br/>plt.show()</span></pre><p id="dbaf" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can see from the results that just a small random adjustment to the budget for each channel can break free of the multi-collinearity curse!</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rt"><img src="../Images/ca3eef8ed2ee2feb125463410751a119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1B1D6q9vALDaphESh3oVxg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="4053" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">It’s worth noting that if I change the random seed (almost like resampling), the starting point for the coefficients varies — However, whatever seed I used the coefficients stabilised after a 1% random change in spend. I’m sure this will vary depending on your data-generating process so make sure you test it out using your own data!</p><h1 id="27cb" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Final thoughts</h1><ul class=""><li id="e517" class="nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou pv pw px bk">Although the focus of this article was multi-collinearity, the big take away is the importance of simulating data and then trying to estimate the known coefficients (remember you set them yourself so you know them) — It’s an essential step if you want to have confidence in your results!</li><li id="31f7" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">When it comes to MMM, it can be useful to use your actual spend and sales data as the base for your simulation — This will help you understand if you have a multi-collinearity problem.</li><li id="e589" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">If you use actual spend and sales data you can also carry out a random budget adjustment simulation to help come up with a suitable randomisation strategy for the marketing team. Keep in mind my simulation was simplistic to illustrate a point — We could design a much more effective strategy e.g. testing different areas of the response curve for each channel.</li><li id="88eb" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Bayesian can be a steep learning curve — The other approach we could take is using a constrained regression in which you set upper and lower bounds for each channel coefficient based on prior knowledge.</li><li id="abff" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">If you are setting Bayesian priors, it’s super important to be transparent about how they work and how they were selected. If you go down the route of using the channel spend distribution, the assumption that each channel has a similar ROI needs signing off from the relevant stakeholders.</li><li id="2823" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Bayesian priors are not magic! Ideally you would use results from experiments to set your priors — It’s worth checking out how the pymc marketing have approached this:</li></ul><div class="pb pc pd pe pf pg"><a href="https://www.pymc-marketing.io/en/stable/notebooks/mmm/mmm_lift_test.html?source=post_page-----78cb56017c73--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ph ab ig"><div class="pi ab co cb pj pk"><h2 class="bf fr hw z io pl iq ir pm it iv fp bk">Lift Test Calibration - pymc-marketing 0.8.0 documentation</h2><div class="pn l"><h3 class="bf b hw z io pl iq ir pm it iv dx">You may have heard of the phrase "all models are wrong; some models are useful." This is true in many areas, and it's…</h3></div><div class="po l"><p class="bf b dy z io pl iq ir pm it iv dx">www.pymc-marketing.io</p></div></div><div class="pp l"><div class="ru l pr ps pt pp pu lr pg"/></div></div></a></div></div></div></div><div class="ab cb rv rw rx ry" role="separator"><span class="rz by bm sa sb sc"/><span class="rz by bm sa sb sc"/><span class="rz by bm sa sb"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="7eac" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">That is it, hope you enjoyed this instalment! Follow me if you want to continue this journey into Causal AI – In the next article we will immerse ourselves in the topic of <em class="pa">bad controls</em>!</p></div></div></div></div>    
</body>
</html>