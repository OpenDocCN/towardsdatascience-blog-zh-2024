- en: 'Mono to Stereo: How AI Is Breathing New Life into Music'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mono-to-stereo-how-ai-is-breathing-new-life-into-music-4180f1357db4?source=collection_archive---------4-----------------------#2024-12-24](https://towardsdatascience.com/mono-to-stereo-how-ai-is-breathing-new-life-into-music-4180f1357db4?source=collection_archive---------4-----------------------#2024-12-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Applications and techniques for AI mono-to-stereo upmixing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maxhilsdorf?source=post_page---byline--4180f1357db4--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page---byline--4180f1357db4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4180f1357db4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4180f1357db4--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page---byline--4180f1357db4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4180f1357db4--------------------------------)
    ·10 min read·Dec 24, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0c2667236398f7d9c6b3b46cd7aa770.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated with DALL-E 3.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mono recordings are a snapshot of history**, but they lack the spatial richness
    that makes music feel truly alive. With AI, we can artificially transform mono
    recordings to stereo or even remix existing stereo recordings. In this article,
    we explore the practical use cases and methods for **mono-to-stereo upmixing**.'
  prefs: []
  type: TYPE_NORMAL
- en: Mono and Stereo in the physical and digital world
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/155441dd5ee6932d49cf578ac877540a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [J](https://unsplash.com/@iamtheoldmanofthemountain?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: When an orchestra plays live, **sound waves** travel from different instruments
    through the room and to your ears. This causes differences in timing (when the
    sound reaches your ear) and loudness (how loud the sound appears in each ear).
    Through this process, a musical performance becomes more than harmony, timbre,
    and rhythm. Each instrument sends **spatial information**, immersing the listener
    in a “here and now” experience that grips their attention and emotions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listen to the difference between the first snippet (no spatial information),
    and the second snippet (clear differences between left and right ear):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Headphones are strongly recommended throughout the article, but are not strictly
    necessary.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exampe: Mono**'
  prefs: []
  type: TYPE_NORMAL
- en: Song originally by [Lexin Music](https://pixabay.com/music/main-title-inspiring-cinematic-asia-116200/).
    Pixabay’s [content license](https://pixabay.com/service/license-summary/) applies.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example: Stereo**'
  prefs: []
  type: TYPE_NORMAL
- en: Song originally by [Lexin Music](https://pixabay.com/music/main-title-inspiring-cinematic-asia-116200/).
    Pixabay’s [content license](https://pixabay.com/service/license-summary/) applies.
  prefs: []
  type: TYPE_NORMAL
- en: As you can hear, the spatial information conveyed through a recording has a
    strong influence on the **liveliness and excitement** we perceive as listeners.
  prefs: []
  type: TYPE_NORMAL
- en: In digital audio, the most common formats are **mono** and **stereo**. A mono
    recording consists of only one audio signal that sounds exactly the same on both
    sides of your headphone earpieces (let’s call them **channels**). A stereo recording
    consists of two separate signals that are panned fully to the left and right channels,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7781f80d27edaaed33c268d487f7ab48.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a stereo waveform consisting of two channels. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have experienced how stereo sound makes the listening experience
    much more lively and engaging and we also understand the key terminologies, we
    can delve deeper into what we are here for: **The role of AI in mono-to-stereo
    conversion,** also known as **mono-to-stereo upmixing.**'
  prefs: []
  type: TYPE_NORMAL
- en: Use Cases for Mono-to-Stereo Upmixing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI is not an end in itself. To justify the development and use of such advanced
    technology, we need **practical use cases**. The two primary use cases for mono-to-stereo
    upmixing are
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Enriching existing music in mono format to a stereo experience.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although stereo recording technology was invented in the early 1930s, it took
    until the 1960s for it to become the de-facto standard in recording studios and
    even longer to establish itself in regular households. In the late 50s, new movie
    releases still came with a stereo track and an additional mono track to account
    for theatres that were not ready to transition to stereo systems. In short, there
    are **lots of popular songs that were recorded in mono**. Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Elvis Presley: Thats All Right'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chuck Berry: Johnny Be Goode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duke Ellington: Take the “A” Train'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official audio for “Elvis Presley: That’s All Right”, a song published
    in 1954 as a mono recording.'
  prefs: []
  type: TYPE_NORMAL
- en: Even today, amateur musicians might publish their recordings in mono, either
    because of a lack of technical competence, or simply because they didn’t want
    to make an effort to create a stereo mix.
  prefs: []
  type: TYPE_NORMAL
- en: Mono-to-Stereo conversion lets us experience our favorite old recordings in
    a new light and also bring amateur recordings or demo tracks to live.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Improving or modernizing existing stereo mixes that appear sloppy or simply
    have fallen out of time, stylistically
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even when a stereo recording is available, we might still want to **improve**
    it. For example, many older recordings from the 60s and 70s were recorded in stereo,
    but with each instrument panned 100% to one side. Listen to “Soul Kitchen” by
    The Doors and notice how the bass and drums are panned fully to the left, the
    keys and guitar to the right, and the vocals in the centre. The song is great
    and there is a special aesthetic to it, but the stereo mix would likely not get
    much love from a modern audience.
  prefs: []
  type: TYPE_NORMAL
- en: Technical limitations have affected stereo sound in the past. Further, stereo
    mixing is not purely a craft, it is **part of the artwork**. Stereo mixes can
    be objectively okay, but still fall out of time, stylistically. A stereo conversion
    tool could be used to create an alternate stereo version that aligns more closely
    with certain stylistic preferences.
  prefs: []
  type: TYPE_NORMAL
- en: How Mono-to-Stereo AI Works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we discussed how relevant mono-to-stereo technology is, you might be
    wondering **how it works under the hood**. Turns out there are different approaches
    to tackling this problem with AI. In the following, I want to showcase four different
    methods, ranging **from traditional signal processing to generative AI**. It does
    not serve as a complete list of methods, but rather as an inspiration for how
    this task has been solved over the last 20 years.
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional Signal Processing: Sound Source Formation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before machine learning became as popular as it is today, the field of **Music
    Information Retrieval (MIR)** was dominated by smart, hand-crafted algorithms.
    It is no wonder that such approaches also exist for mono-to-stereo upmixing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fundamental idea behind a paper from 2007 (Lagrange, Martins, Tzanetakis,
    **[1])** is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: If we can find the different sound sources of a recording and extract them from
    the signal, we can mix them back together for a realistic stereo experience.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This **sounds simple**, but how can we tell what the sound sources in the signal
    are? How do we define them so clearly that an algorithm can extract them from
    the signal? These questions are difficult to solve and the paper uses a variety
    of advanced methods to achieve this. In essence, this is the algorithm they came
    up with:'
  prefs: []
  type: TYPE_NORMAL
- en: Break the recording into short snippets and **identify the peak frequencies**
    (dominant notes) in each snippet
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Identify which peaks belong together** (a sound source) using a clustering
    algorithm'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide **where** each sound source should be **placed in the stereo mix** (manual
    step)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each sound source, **extract its assigned frequencies** from the signal
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Mix all extracted sources together** to form the final stereo mix.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/1b935e601d8768cfb3f3e1286d15d0ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of the user interface built for the study. The user goes through all
    the extracted sources and manually places them in the stereo mix, before resynthesizing
    the whole signal. Image taken from **[1]**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although quite complex in the details, the intuition is quite clear: **Find
    sources, extract them, mix them back together.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Quick Workaround: Source Separation / Stem Splitting'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A lot has happened since Lagrange’s 2007 paper. Since Deezer released their
    stem splitting tool [Spleeter](https://github.com/deezer/spleeter) in 2019, AI-based
    source separation systems have become remarkably useful. Leading players such
    as [Lalal.ai](https://www.lalal.ai/) or [Audioshake](https://www.audioshake.ai/instrument-stem-separation)
    make a quick workaround possible:'
  prefs: []
  type: TYPE_NORMAL
- en: Separate a mono recording into its individual instrument stems using a free
    or commercial stem splitter
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the stems into a Digital Audio Workstation (DAW) and mix them together
    to your liking
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This technique has been used in a research paper in 2011 (see **[2]**), but
    it has become much more viable since due to the **recent improvements in stem
    separation tools**.
  prefs: []
  type: TYPE_NORMAL
- en: The downside of source separation approaches is that they produce **noticeable
    sound artifacts**, because source separation itself is still not without flaws.
    Additionally, these approaches still **require manual mixing** by humans, making
    them only semi-automatic.
  prefs: []
  type: TYPE_NORMAL
- en: To fully automate mono-to-stereo upmixing, machine learning is required. By
    learning from real stereo mixes, ML system can adapt the mixing style of real
    human producers.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning with Parametric Stereo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/2d78894a7635bcd366a210b6c68fe896.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Zarak Khan](https://unsplash.com/@zarakvg?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: One very creative and efficient way of using machine learning for mono-to-stereo
    upmixing was presented at ISMIR 2023 by Serrà and colleagues **[3]**. This work
    is based on a music compression technique called **parametric stereo**. Stereo
    mixes consist of two audio channels, making it hard to integrate in low-bandwidth
    settings such as music streaming, radio broadcasting, or telephone connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parametric stereo is a technique to create stereo sound from a single mono
    signal by **focusing on the important spatial cues** our brain uses to determine
    where sounds are coming from. These cues are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**How loud** a sound is in the left ear vs. the right ear (Interchannel Intensity
    Difference, IID)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**How in sync** it is between left and right in terms of time or phase (Interchannel
    Time or Phase Difference)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**How similar or different** the signals are in each ear (Interchannel Correlation,
    IC)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using these parameters, a stereo-like experience can be created from nothing
    more than a mono signal.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the approach the researchers took to develop their mono-to-stereo upmixing
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collect a large dataset** of stereo music tracks'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Convert the stereo tracks** to parametric stereo (mono + spatial parameters)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train a neural network** to predict the spatial parameters given a mono recording'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To turn a new mono signal into stereo, use the trained model to **infer spatial
    parameters from the mono signal** and combine the two to a parametric stereo experience
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Currently, no code or listening demos seem to be available for this paper. The
    authors themselves confess that “there is still a gap between professional stereo
    mixes and the proposed approaches” (p. 6). Still, the paper outlines a creative
    and efficient way to accomplish fully automated mono-to-stereo upmixing using
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative AI: Transformer-based Synthesis'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/5481f91c143d41712337a3c4edd0ad9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Stereo-Genration in Meta’s text-to-music model MusicGen. Image taken from [another
    article by the author](https://medium.com/towards-data-science/musicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7).
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will get to the seemingly most straight-forward way to generate stereo
    from mono. Training a generative model to take a mono input and synthesizing both
    stereo output channels directly. Although conceptually simple, this is by far
    the most challenging approach from a technical standpoint. One second of high-resolution
    audio has 44.1k data points. Generating a three-minute song with stereo channels
    therefore means **generating over 15 million data points**.
  prefs: []
  type: TYPE_NORMAL
- en: With todays technologies such as convolutional neural networks, transformers,
    and neural audio codecs, the complexity of the task is starting to become managable.
    There are some papers who chose to generate stereo signal through direct neural
    synthesis (see **[4]**, **[5]**, **[6]**). However, only **[5]** train a model
    than can solve mono to stereo generation out of the box. My intuition is that
    there is room for a paper that builds a dedicated for the “simple” task of mono-to-stereo
    generation and focuses 100% on solving this objective. Anyone here looking for
    a **PhD topic**?
  prefs: []
  type: TYPE_NORMAL
- en: What Needs to Happen Next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/16a84d46db8a66f820a2b701322f4150.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Samuel Spagl](https://unsplash.com/@saemsp?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'To conclude this article, I want to discuss where the field of mono-to-stereo
    upmixing might be going. Most importantly, I noticed that **research in this domain
    is very sparse**, compared to hype topics such as text-to-music generation. Here’s
    what I think the research community should focus on to bring mono-to-stereo upmixing
    research to the next level:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Openly Available Demos and Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Only few papers are released in this research field. This makes it even more
    frustrating that **many of them do not share their code or the results of their
    work** with the community. Several times have I read through a fascinating paper
    only to find that the only way to test the output quality of the method is to
    understand every single formula in the paper and implement the algorithm myself
    from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing code and creating public demos has never been as easy as it is today.
    Researchers should make this a priority to enable the wider audio community to
    understand, evaluate, and appreciate their work.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Going All-In on Generative AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional signal processing and machine learning are fun, but when it comes
    to output quality, there is no way around generative AI anymore. **Text-to-music
    models are already producing great-sounding stereo mixes**. Why is there no easy
    to use, state-of-the-art mono-to-stereo upmixing library available?
  prefs: []
  type: TYPE_NORMAL
- en: From what I gathered in my research, building an efficient and effective model
    can be done with a reasonable dataset size and minimal to moderate changes to
    existing model architectures and training methods. My impression is that this
    is a **low-hanging fruit** and a “just do it!” situation.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Making Upmixing Automated, but Controllable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have a great open-source upmixing model, the next thing we need is controllability.
    We shouldn’t have to pick between black-box “take-it-or-leave-it” neural generations
    or old-school, manual mixing based on source separation. I think we could have
    it both.
  prefs: []
  type: TYPE_NORMAL
- en: A neural mono-to-stereo upmixing model could be **trained on a massive dataset
    and then finetuned** to adjust its stereo mixes based on a user prompt. This way,
    musicians could customize the style of the generated stereo based on their personal
    preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Effective and openly-accessible mono-to-stereo upmixing has the potential to
    breathe live into old recordings or amateur productions, while also allowing us
    to create alternate stereo mixes of our favorite songs.
  prefs: []
  type: TYPE_NORMAL
- en: Although there have been several attempts to solve this problem, no standard
    method has been established. By embracing recent development in GenAI, a new generation
    of mono-to-stereo upmixing models could be created that makes the technology more
    effective and more widely available in the community.
  prefs: []
  type: TYPE_NORMAL
- en: About Me
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I’m a musicologist and a data scientist, sharing my thoughts on current topics
    in AI & music. Here is some of my previous work related to this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Images that Sound: Creating Stunning Audiovisual ART with AI**](/3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**How Meta’s AI Generates Music Based on a Reference Melody**](https://medium.com/towards-data-science/how-ai-can-remove-imperceptible-watermarks-6b4560ea867a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**AI Music Source Separation: How it Works and Why it is so Hard**](https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find me on [Medium](https://medium.com/@maxhilsdorf) and [Linkedin](https://www.linkedin.com/in/max-hilsdorf/)!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**[1] M. Lagrange, L. G. Martins, and G. Tzanetakis (2007)**: “Semiautomatic
    mono to stereo up-mixing using sound source formation”, in Audio Engineering Society
    Convention 122\. Audio Engineering Society, 2007.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[2] D. Fitzgerald (2011)**: “Upmixing from mono-a source separation approach”,
    in 2011 17th International Conference on Digital Signal Processing (DSP). IEEE,
    2011, pp. 1–7.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[3] J. Serrà, D. Scaini, S. Pascual, et al. (2023):** “Mono-to-stereo through
    parametric stereo generation”: [https://arxiv.org/abs/2306.14647](https://arxiv.org/abs/2306.14647)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[4] J. Copet, F. Kreuk, I. Gat et al. (2023)**: “Simple and Controllable
    Music Generation” (revision from 30.01.2024). [https://arxiv.org/abs/2306.05284](https://arxiv.org/abs/2306.05284)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[5] Y. Zang, Y. Wang & M. Lee (2024):** “Ambisonizer: Neural Upmixing as
    Spherical Harmonics Generation”. [https://arxiv.org/pdf/2405.13428](https://arxiv.org/pdf/2405.13428)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[6] K.K. Parida, S. Srivastava & G. Sharma (2022)**: “Beyond Mono to Binaural:
    Generating Binaural Audio from Mono Audio with Depth and Cross Modal Attention”,
    in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision
    (WACV), 2022, p. 3347–3356\. [Link](https://openaccess.thecvf.com/content/WACV2022/html/Parida_Beyond_Mono_to_Binaural_Generating_Binaural_Audio_From_Mono_Audio_WACV_2022_paper.html)'
  prefs: []
  type: TYPE_NORMAL
