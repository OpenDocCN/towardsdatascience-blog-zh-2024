<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Mastering K-Means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Mastering K-Means Clustering</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mastering-k-means-clustering-065bc42637e4?source=collection_archive---------0-----------------------#2024-05-22">https://towardsdatascience.com/mastering-k-means-clustering-065bc42637e4?source=collection_archive---------0-----------------------#2024-05-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="b274" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Implement the K-Means algorithm from scratch with this step-by-step Python tutorial</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://marcusmvls-vinicius.medium.com/?source=post_page---byline--065bc42637e4--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Marcus Sena" class="l ep by dd de cx" src="../Images/ff594ec7029e6259f0be6dc031d8a6cd.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*zzBM0cL2XAVKlFFS6ZSnhQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--065bc42637e4--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://marcusmvls-vinicius.medium.com/?source=post_page---byline--065bc42637e4--------------------------------" rel="noopener follow">Marcus Sena</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--065bc42637e4--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/5c7adde7e4e88ee31689443db5e31e44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cc4mZFp12c-oKlBy7d4jGw.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by the author using DALL-E.</figcaption></figure><p id="4f0e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this article, I show how I’d learn the K-Means algorithm if I’d started today. We’ll start with the fundamental concepts and implement a Python class that performs clustering tasks using nothing more than the Numpy package.</p><p id="5153" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Whether you are a machine learning beginner trying to build a solid understanding of the concepts or a practitioner interested in creating custom machine learning applications and needs to understand how the algorithms work under the hood, that article is for you.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><blockquote class="og oh oi"><p id="ccad" class="nc nd oj ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Table of contents</p><p id="a8b5" class="nc nd oj ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af ok" href="#4970" rel="noopener ugc nofollow">1. Introduction</a><br/><a class="af ok" href="#7e98" rel="noopener ugc nofollow">2. What Does the K-Means algorithm do?</a><br/><a class="af ok" href="#60a5" rel="noopener ugc nofollow">3. Implementation in Python</a><br/><a class="af ok" href="#2e89" rel="noopener ugc nofollow">4. Evaluation and Interpretation</a><br/><a class="af ok" href="#655b" rel="noopener ugc nofollow">5. Conclusions and Next Steps</a></p></blockquote><h1 id="4970" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">1. Introduction</h1><p id="2743" class="pw-post-body-paragraph nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx fj bk">Most of the machine learning algorithms widely used, such as Linear Regression, Logistic Regression, Decision Trees, and others are useful for making predictions from labeled data, that is, each input comprises feature values with a label value associated. That is what is called <strong class="ne fr">Supervised Learning</strong>.</p><p id="aaf4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, often we have to deal with large sets of data with no label associated. Imagine a business that needs to understand the different groups of customers based on purchasing behavior, demographics, address, and other information, thus it can offer better services, products, and promotions.</p><p id="ffde" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These types of problems can be addressed with the use of <strong class="ne fr">Unsupervised Learning</strong> techniques. The K-Means algorithm is a widely used unsupervised learning algorithm in Machine Learning. Its simple and elegant approach makes it possible to separate a dataset into a desired number of K distinct clusters, thus allowing one to learn patterns from unlabelled data.</p><h1 id="7e98" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">2. What Does the K-Means algorithm do?</h1><p id="b2b2" class="pw-post-body-paragraph nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx fj bk">As said earlier, the K-Means algorithm seeks to partition data points into a given number of clusters. The points within each cluster are similar, while points in different clusters have considerable differences.</p><p id="0ca8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Having said that, one question arises: how do we define similarity or difference? In K-Means clustering, the Euclidean distance is the most common metric for measuring similarity.</p><p id="f4ad" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the figure below, we can clearly see 3 different groups. Hence, we could determine the centers of each group and each point would be associated with the closest center.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pm"><img src="../Images/0ec2067871637ccf83430558df44cfbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*8QYaIgBjj6h7ndp_vmCF_A.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Simulated dataset with 200 observations (image by the author).</figcaption></figure><p id="2e03" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By doing that, mathematically speaking, the idea is to minimize the <em class="oj">within-cluster variance</em>, the measurement of similarity between each point and its closest center.</p><p id="04af" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Performing the task in the example above was straightforward because the data was two-dimensional and the groups were clearly distinct. However, as the number of dimensions increases and different values of K are considered, we need an algorithm to handle the complexity.</p><h2 id="92e7" class="pn om fq bf on po pp pq oq pr ps pt ot nl pu pv pw np px py pz nt qa qb qc qd bk">Step 1: Pick the initial centers (randomly)</h2><p id="37f7" class="pw-post-body-paragraph nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx fj bk">We need to seed the algorithm with initial center vectors that can be chosen randomly from the data or generate random vectors with the same dimensions as the original data. See the white diamonds in the image below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pm"><img src="../Images/33dd2f443181dc5507406b8ec38c9077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*p-LxxWrDVa4nLrGNX893hw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Initial centers are randomly picked (image by the author).</figcaption></figure><h2 id="15e7" class="pn om fq bf on po pp pq oq pr ps pt ot nl pu pv pw np px py pz nt qa qb qc qd bk">Step 2: Find the distances of each point to the centers</h2><p id="18bf" class="pw-post-body-paragraph nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx fj bk">Now, we'll calculate the distance of each data point to the K centers. Then we associate each point with the center closest to that point.</p><p id="fa01" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Given a dataset with <em class="oj">N </em>entries and <em class="oj">M </em>features, the distances to the centers <em class="oj">c </em>can be given by the following equation:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qe"><img src="../Images/01b720e1312e5eea301af0f6dfa65c36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*dmcrI5ye302w-DlOJTvFtQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Euclidean distance (image generated using codecogs.com).</figcaption></figure><p id="184e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">where:</p><p id="7e99" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oj">k</em> varies from 1 to <em class="oj">K</em>;</p><p id="0f6b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oj">D</em> is the distance of a point n to the <em class="oj">k</em> center;</p><p id="2a01" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oj">x </em>is the point vector;</p><p id="5572" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oj">c </em>is the center vector.</p><p id="62ea" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Hence, for each data point <em class="oj">n </em>we'll have K distances, then we have to label the vector to the center with the smallest distance:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qf"><img src="../Images/6c1fb0e5d6b3deeba9e459fef5ca022c.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*aTe1VB2pGxSCsa7f1AFgTA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(image generated using codecogs.com)</figcaption></figure><p id="ebd9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Where <em class="oj">D</em> is a vector with <em class="oj">K </em>distances.</p><h2 id="17ab" class="pn om fq bf on po pp pq oq pr ps pt ot nl pu pv pw np px py pz nt qa qb qc qd bk">Step 3: Find the <em class="qg">K </em>centroids and iterate</h2><p id="fa5f" class="pw-post-body-paragraph nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx fj bk">For each of the <em class="oj">K</em> clusters, recalculate the centroid. The new centroid is the mean of all data points assigned to that cluster. Then update the positions of the centroids to the newly calculated.</p><p id="e02f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Check if the centroids have changed significantly from the previous iteration. This can be done by comparing the positions of the centroids in the current iteration with those in the last iteration.</p><p id="1fd4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If the centroids have changed significantly, go back to Step 2. If not, the algorithm has converged and the process stops. See the image below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pm"><img src="../Images/0c157b01ce3cb851db9481f3b53f5786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*trbuwKohsyn_SZrWrk7fKw.gif"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Convergence of the centroids (image by the author).</figcaption></figure><h1 id="60a5" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">3. Implementation in Python</h1><p id="7d67" class="pw-post-body-paragraph nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx fj bk">Now that we know the fundamental concepts of the K-Means algorithm, it's time to implement a Python class. The packages used were Numpy for mathematical calculations, Matplotlib for visualization, and the Make_blobs package from Sklearn for simulated data.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="b59e" class="ql om fq qi b bg qm qn l qo qp"># import required packages<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.datasets import make_blobs</span></pre><p id="7445" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The class will have the following methods:</p><ul class=""><li id="1dcb" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk"><strong class="ne fr">Init method</strong></li></ul><p id="9d49" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A constructor method to initialize the basic parameters of the algorithm: the value <em class="oj">k </em>of clusters, the maximum number of iterations <em class="oj">max_iter, </em>and the tolerance <em class="oj">tol </em>value to interrupt the optimization when there is no significant improvement.</p><ul class=""><li id="d86a" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk"><strong class="ne fr">Helper functions</strong></li></ul><p id="29e8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These methods aim to assist the optimization process during training, such as calculating the Euclidean distance, randomly choosing the initial centroids, assigning the closest centroid to each point, updating the centroids’ values, and verifying whether the optimization converged.</p><ul class=""><li id="3638" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk"><strong class="ne fr">Fit and predict method</strong></li></ul><p id="2fb8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As mentioned earlier, the K-Means algorithm is an unsupervised learning technique, meaning it does not require labeled data during the training process. That way, it's necessary a single method to fit the data and predict to which cluster each data point belongs.</p><ul class=""><li id="eeec" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk"><strong class="ne fr">Total error method</strong></li></ul><p id="816c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A method to evaluate the quality of the optimization by calculating the <em class="oj">total squared error </em>of the optimization. That will be explored in the next section.</p><p id="9644" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here it goes the full code:</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="0abe" class="ql om fq qi b bg qm qn l qo qp"># helper function for calculating Euclidean distance<br/>def euclidean_distance(a,b):<br/>    d = np.sqrt(np.sum((a - b)**2))<br/>    return d<br/><br/>class Kmeans:<br/>    <br/>    # construct method for hyperparameter initialization<br/>    def __init__(self, k=3, max_iter=100, tol=1e-06):<br/>        self.k = k<br/>        self.max_iter = max_iter<br/>        self.tol = tol<br/>  <br/>    # randomly picks the initial centroids from the input data<br/>    def pick_centers(self, X):<br/>        centers_idxs = np.random.choice(self.n_samples, self.k)<br/>        return X[centers_idxs]<br/>    <br/>    # finds the closest centroid for each data point<br/>    def get_closest_centroid(self, x, centroids):<br/>        distances = [euclidean_distance(x, centroid) for centroid in centroids]<br/>        return np.argmin(distances)<br/>    <br/>    # creates a list with lists containing the idxs of each cluster<br/>    def create_clusters(self, centroids, X):<br/>        clusters = [[] for _ in range(self.k)]<br/>        labels = np.empty(self.n_samples)<br/>        for i, x in enumerate(X):<br/>            centroid_idx = self.get_closest_centroid(x, centroids)<br/>            clusters[centroid_idx].append(i)<br/>            labels[i] = centroid_idx<br/><br/>        return clusters, labels<br/>    <br/>    # calculates the centroids for each cluster using the mean value <br/>    def compute_centroids(self, clusters, X):<br/>        centroids = np.empty((self.k, self.n_features))<br/>        for i, cluster in enumerate(clusters):<br/>            centroids[i] = np.mean(X[cluster], axis=0)<br/><br/>        return centroids<br/>    <br/>    # helper function to verify if the centroids changed significantly<br/>    def is_converged(self, old_centroids, new_centroids):<br/>        distances = [euclidean_distance(old_centroids[i], new_centroids[i]) for i in range(self.k)]<br/>        return (sum(distances) &lt; self.tol)<br/><br/><br/>    # method to train the data, find the optimized centroids and label each data point according to its cluster<br/>    def fit_predict(self, X):<br/>        self.n_samples, self.n_features = X.shape<br/>        self.centroids = self.pick_centers(X)<br/><br/>        for i in range(self.max_iter):<br/>            self.clusters, self.labels = self.create_clusters(self.centroids, X)<br/>            new_centroids = self.compute_centroids(self.clusters, X)<br/>            if self.is_converged(self.centroids, new_centroids):<br/>                break<br/>            self.centroids = new_centroids<br/><br/>    <br/>    # method for evaluating the intracluster variance of the optimization<br/>    def clustering_errors(self, X):<br/>        cluster_values = [X[cluster] for cluster in self.clusters]<br/>        squared_distances = []<br/>        # calculation of total squared Euclidean distance<br/>        for i, cluster_array in enumerate(cluster_values):<br/>            squared_distances.append(np.sum((cluster_array - self.centroids[i])**2))<br/><br/>        total_error = np.sum(squared_distances)<br/>        return total_error</span></pre><h1 id="2e89" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">4. Evaluation and Interpretation</h1><p id="827f" class="pw-post-body-paragraph nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx fj bk">Now we'll use the K-Means class to cluster simulated data. To do that, the make_blobs package from the Sklearn library will be used. The data consists of 500 two-dimensional points with 4 fixed centers.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="de16" class="ql om fq qi b bg qm qn l qo qp"># create simulated data for examples<br/>X, _ = make_blobs(n_samples=500, n_features=2, centers=4, <br/>                  shuffle=False, random_state=0)</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pm"><img src="../Images/0c6ff01bc9de6ffa2c0a4b553b15d86e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*tfsi1vmguVPZ9riNb_ugHQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Simulated data (image by the author).</figcaption></figure><p id="9405" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After performing the training using four clusters, we achieve the following result.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="523e" class="ql om fq qi b bg qm qn l qo qp">model = Kmeans(k=4)<br/>model.fit_predict(X)<br/>labels = model.labels<br/>centroids =model.centroids<br/>plot_clusters(X, labels, centroids)</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pm"><img src="../Images/2beea78e39fb0e4730512b9c829df0ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*JBTi_DXtCz_NzOhDZzMEag.gif"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Clustering for k=4 (image by the author).</figcaption></figure><p id="287e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In that case, the algorithm was capable of calculating the clusters successfully with 18 iterations. However, we must keep in mind that we already know the optimal number of clusters from the simulated data. In real-world applications, we often don't know that value.</p><p id="c995" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As said earlier, the K-Means algorithm aims to make the <em class="oj">within-cluster variance </em>as small as possible. The metric used to calculate that variance is the <em class="oj">total squared Euclidean distance</em> given by:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qt"><img src="../Images/72661e266df6e85c3b4301407fe58c8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*CTlax34tYY5FUFiQxuA86w.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Total squared Euclidean distance formula (image by the author using codecogs.com).</figcaption></figure><p id="2fad" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">where:</p><p id="c4c9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">p is the number of data points in a cluster;</p><p id="5fac" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">c_i is the centroid vector of a cluster;</p><p id="c542" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">K is the number of clusters.</p><p id="b57c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In words, the formula above adds up the distances of the data points to the nearest centroid. The error decreases as the number K increases.</p><blockquote class="qu"><p id="9e12" class="qv qw fq bf qx qy qz ra rb rc rd nx dx">In the extreme case of K =N, you have one cluster for each data point and this error will be zero.</p><p id="fe8c" class="qv qw fq bf qx qy qz ra rb rc rd nx dx">Willmott, Paul (2019).</p></blockquote><p id="26b1" class="pw-post-body-paragraph nc nd fq ne b go re ng nh gr rf nj nk nl rg nn no np rh nr ns nt ri nv nw nx fj bk">If we plot the error against the number of clusters and look at where the graph "bends", we'll be able to find the optimal number of clusters.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pm"><img src="../Images/cb9e6cb88d7dfb252a6602ef6d826078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*1DJ5HNCJxIkP81T77FIKFg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Scree plot (image by the author).</figcaption></figure><p id="2e64" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As we can see, the plot has an "elbow shape" and it bends at K = 4, meaning that for greater values of K, the decrease in the total error will be less significant.</p><h1 id="655b" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">5. Conclusions and Next Steps</h1><p id="238f" class="pw-post-body-paragraph nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx fj bk">In this article, we covered the fundamental concepts behind the K-Means algorithm, its uses, and applications. Also, using these concepts, we were able to implement a Python class from scratch that performed the clustering of simulated data and how to find the optimal value for K using a scree plot.</p><p id="82aa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, since we are dealing with an unsupervised technique, there is one additional step. The algorithm can successfully assign a label to the clusters, but the meaning of each label is a task that the data scientist or machine learning engineer will have to do by analyzing the data of each cluster.</p><p id="5257" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In addition, I'll leave some points for further exploration:</p><ul class=""><li id="f969" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">Our simulated data used two-dimensional points. Try to use the algorithm for other datasets and find the optimal values for K.</li><li id="da6c" class="nc nd fq ne b go rj ng nh gr rk nj nk nl rl nn no np rm nr ns nt rn nv nw nx qq qr qs bk">There are other unsupervised learning algorithms widely used such as <em class="oj">Hierarchical Clustering</em>.</li><li id="7f33" class="nc nd fq ne b go rj ng nh gr rk nj nk nl rl nn no np rm nr ns nt rn nv nw nx qq qr qs bk">Depending on the domain of the problem, it may be necessary to use other error metrics such as Manhattan distance and cosine similarity. Try to investigate them.</li></ul><p id="1115" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Complete code available <a class="af ok" href="https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/K-Means" rel="noopener ugc nofollow" target="_blank">here</a>:</p><div class="ro rp rq rr rs rt"><a href="https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/K-Means?source=post_page-----065bc42637e4--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ru ab ig"><div class="rv ab co cb rw rx"><h2 class="bf fr hw z io ry iq ir rz it iv fp bk">ML-and-Ai-from-scratch/K-Means at main · Marcussena/ML-and-Ai-from-scratch</h2><div class="sa l"><h3 class="bf b hw z io ry iq ir rz it iv dx">Python implementation of machine learning and AI algorithms from scratch - ML-and-Ai-from-scratch/K-Means at main ·…</h3></div><div class="sb l"><p class="bf b dy z io ry iq ir rz it iv dx">github.com</p></div></div><div class="sc l"><div class="sd l se sf sg sc sh lr rt"/></div></div></a></div></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4f3b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Please feel free to use and improve the code, comment, make suggestions, and connect with me on <a class="af ok" href="https://www.linkedin.com/in/marcus-sena-660198150/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>, <a class="af ok" href="https://twitter.com/MarcusMVLS" rel="noopener ugc nofollow" target="_blank">X</a>, and <a class="af ok" href="https://github.com/Marcussena/ML-and-Ai-from-scratch" rel="noopener ugc nofollow" target="_blank">Github</a>.</p><h1 id="8afb" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">References</h1><p id="6004" class="pw-post-body-paragraph nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx fj bk">[1] Sebastian Raschka (2015), Python Machine Learning.</p><p id="33ec" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[2] Willmott, Paul. (2019). <em class="oj">Machine Learning: An Applied Mathematics Introduction</em>. Panda Ohana Publishing.</p><p id="4a1d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[3] Géron, A. (2017). <em class="oj">Hands-On Machine Learning</em>. O’Reilly Media Inc.</p><p id="00de" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[4] Grus, Joel. (2015). <em class="oj">Data Science from Scratch</em>. O’Reilly Media Inc.</p></div></div></div></div>    
</body>
</html>