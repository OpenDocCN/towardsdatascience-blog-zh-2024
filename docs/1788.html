<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Counterfactuals in Language AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Counterfactuals in Language AI</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/counterfactuals-in-language-ai-956673049b64?source=collection_archive---------5-----------------------#2024-07-23">https://towardsdatascience.com/counterfactuals-in-language-ai-956673049b64?source=collection_archive---------5-----------------------#2024-07-23</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c3d5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">with open source language models and LLMs</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://joshuabanksmailman.medium.com/?source=post_page---byline--956673049b64--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Joshua Banks Mailman, Ph.D." class="l ep by dd de cx" src="../Images/e28bf743fc5bdba644dcfb747eeb72b0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*v4eXbtoIt8vzQ3UjAxo0sg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--956673049b64--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://joshuabanksmailman.medium.com/?source=post_page---byline--956673049b64--------------------------------" rel="noopener follow">Joshua Banks Mailman, Ph.D.</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--956673049b64--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">27 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 23, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/3a8683cdc2f0845498a740e41bdcb8af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*agCFkl5EGq7JJZJMAA8oGw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Generated by the author using DALL-E 3</figcaption></figure><p id="7a89" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The splash that ChatGPT was making last year brought with it the realization — surprise for many — that a putative AI could sometimes offer very wrong answers with utter conviction. The term for this is usually “hallucination” and the main remedy that’s developed over the last 18 months is to bring facts into the matter, usually through <em class="nx">retrieval augmented generation </em>(RAG), also sometimes called <em class="nx">relevant answer generation</em>, which basically reorients the GPT (<em class="nx">generative pretrained transformer</em> language model) to draw from contexts where known-to-be-relevant facts are found.</p><p id="5462" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Yet hallucinations are not the only way a GPT can misstep. In some respects, other flavors of misstep are deeper and more interesting to consider— especially when prospects of Artificial General Intelligence (AGI) are now often discussed. Specifically I’m thinking of what are known as <em class="nx">counterfactuals</em> (counterfactual reasoning) and the crucial role counterfactuality can play in decision making, particularly in regard to <em class="nx">causal inference</em>. Factuality therefore isn’t the only touchstone for effective LLM operation.</p><p id="7b0d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In this article I’ll reflect on how counterfactuals might help us think differently about the pitfalls and potentials of Generative AI. And I’ll demonstrate with some concrete examples using open source LMs (specifically Microsoft’s Phi). I’ll show how to set up Ollama locally (it can also be done in Databricks), without too much fuss (both with and without a Docker container), so you can try it out for yourself. I’ll also compare OpenAI’s LLM response to the same prompts.</p><p id="ca7a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">I suggest that if we want to even begin to think about the prospect of “intelligence” within, or exuded by, an artificial technology, we might need to think beyond the established ML paradigm, which assumes some pre-existing factual correctness to measure against. An intelligent behavior might instead be speculative, as yet lacking sufficient past evidence to obviously prove its value. Isaac Newton, Charles Darwin, or your pet cat, could reason practically about the physical world they themselves inhabit, which is something that LLMs — because they are <em class="nx">dis</em>embodied — don’t do. In a world where machines can write fluently, talk is cheaper than speculative practical reasoning.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ny"><img src="../Images/42bb142715d76d80ab499d9bf7b2c778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GR3A0ydQ3MToSqGv"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nz" href="https://unsplash.com/@florianklauer?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Florian Klauer</a> on <a class="af nz" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="36b4" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk"><strong class="al">Counterfactuals</strong></h1><p id="5e2c" class="pw-post-body-paragraph nb nc fq nd b go ow nf ng gr ox ni nj nk oy nm nn no oz nq nr ns pa nu nv nw fj bk">What is a counterfactual and why should we care? There’s certainly idle speculation, sometimes with a rhetorical twist: At an annual meeting, a shareholder asked what the</p><blockquote class="pb pc pd"><p id="aed8" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“…returns since 1888 would have been without the ‘arrogance and greed’ of directors and their bonuses.” <em class="fq">¹</em></p></blockquote><p id="ff26" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">…to which retired banker Charles Munn² replied:</p><blockquote class="pb pc pd"><p id="1313" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“That’s one question I won’t be answering. I’m a historian, I don’t deal in counterfactuals.” <em class="fq">¹</em></p></blockquote><p id="299a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">It’s <em class="nx">one</em> way to evade a question. Politicians have used it, no doubt. Despite emphasis on precedent, in legal matters, counterfactuality can be a legitimate consideration. As Robert N. Strassfeld puts it:</p><blockquote class="pb pc pd"><p id="b577" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“As in the rest of life, we indulge, indeed, require, many speculations on what might have been. Although such counterfactual thinking often remains disguised or implicit, we encounter it whenever we identify a cause, and quite often when we attempt to fashion a remedy… Yet, … troublesome might-have-beens abound in legal disputes. We find ourselves stumbling over them in a variety of situations and responding to them in inconsistent ways ranging from brazen self-confidence to paralysis in the face of the task. When we recognize the exercise for what it is, however, our self confidence tends to erode, and we become discomforted, perplexed, and skeptical about the whole endeavor.”³</p></blockquote><p id="5ca5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">He goes further in posing that</p><blockquote class="pb pc pd"><p id="b904" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“…legal decision makers cannot avoid counterfactual questions. Because such questions are necessary, we should think carefully about when and how to pose them, and how to distinguish good answers from poor ones.”</p></blockquote><p id="08d1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Counterfactuals are not an “anything goes” affair — far from it.</p><p id="36b1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The question pervades the discourse of responsible AI and explainable AI (which often become entwined). Consider the “right to explanation” in the EU General Data Protection Regulation (“GDPR”).⁴ Thanks in part to <a class="af nz" href="https://nyudatascience.medium.com/until-you-try-to-regulate-you-wont-learn-how-julia-stoyanovich-discusses-responsible-ai-for-4cf119002b05" rel="noopener">Julia Stoyanovich’</a>s efforts, <a class="af nz" href="https://rules.cityofnewyork.us/wp-content/uploads/2022/09/Stoyanovich_LL144_October24_2022.pdf" rel="noopener ugc nofollow" target="_blank">NY passed a law in 2021 requiring that job seekers rejected by an AI-infused hiring process have the right to learn the specific explanation for their rejection</a>.⁵ ⁶ ⁷</p><p id="d18f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">If you’re a Data Scientist, the prospect of “explanations” (with respect to a model) probably brings to mind SHAP (or LIME). Basically, a SHapley Additive exPlanation (SHAP) is derived by taking each predictive feature (each column of data) one at a time, and scrambling the observations (rows) of that feature (column) to assess which features (columns) the scrambling of which changes the prediction the most. For the rejected job candidate it might say, for instance: The primary reason the algorithm rejected you is “years of experience” because when we randomly substitute (permute) other candidates’ “years of experience” it affects the algorithm’s rating of you more than when we do that substitution (permutation) with your other features (like gender, education, etc). It’s making a quantitative comparison to a “what if.” So what is a SHAPley other than a counterfactual? Counterfactuality is at the heart of the explanation, because it gives a glimpse into causality; and explanation is relied on for making AI responsible.</p><p id="640a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Leaving responsibility and ethics to the side for the moment, causal explanation still has other uses in business. At least in some companies, Data Science and AI are expected to guide decision making, which in causal inference terms means <em class="nx">making an intervention</em>: adjusting price or targeting this verses that customer segment, and so forth. An intervention is an alteration to the status quo. The <em class="nx">fundamental problem of causal inference</em> is that we aren’t able to observe what has never happened. So we can’t observe the result of an intervention until we make that intervention. Where there’s risk involved, we don’t want to intervene without sufficiently anticipating the result. Thus we want to infer in advance that the result we desire can be caused by our intervention. That entails making inferences about the causal effects of events that aren’t yet fact. Instead, such events are <em class="nx">counterfactual</em>, that is, contrary to fact. This is why counterfactuals have been couched, by Judea Pearl and others, as the</p><p id="aa3b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr"><em class="nx">fundamental problem of causal inference </em>⁸</strong></p><p id="bedb" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">So the idea of a “thought experiment”, which has been important especially in philosophy — and ever more so since Ludwig Wittgenstein popularized it —as a way to probe how we use language to construct our understanding of the world — isn’t just a sentimental wish-upon-a-star.⁹ Quite to the contrary: counterfactuals are the crux of hard-headed decision making.</p><p id="8c28" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In this respect, what Eric Siegel suggests in his recent <em class="nx">AI Playbook</em> follows as corollary: Siegel suggests that <em class="nx">change management </em>be repositioned from afterthought to prerequisite of any Machine Learning project.¹⁰ If the conception of making a business change isn’t built into the ML project from the get go, its deployment is likely doomed to remain fiction forever (eternally counterfactual). The antidote is to imagine the intervention in advance, and systematically work out its causal effects, so that you can almost taste them. If its potential benefits are anticipated — and maybe even consistently visualized — by all parties who stand to benefit, then the ML project stands a better chance of transitioning from fiction to fact.</p><p id="d10d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As Aleksander Molak explains it in his recent <em class="nx">Causal Inference and Discovery in Python</em> (2023)</p><blockquote class="pb pc pd"><p id="8b1d" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“Counterfactuals can be thought of as <em class="fq">hypothetical</em> or <em class="fq">simulated</em> interventions that assume a <em class="fq">particular state of the world</em>.” <em class="fq">¹¹</em></p></blockquote><p id="bf20" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The capacity for <em class="nx">rational imagination</em> is implicated in many philosophical definitions of rational agency.¹² ¹³</p><blockquote class="pb pc pd"><p id="485f" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“[P]sychological research shows that rational <em class="fq">human</em> agents <em class="fq">do</em> learn from the past and plan for the future engaging in counterfactual thinking. Many researchers in artificial intelligence have voiced similar ideas (Ginsberg 1985; Pearl 1995; Costello &amp; McCarthy 1999)” <em class="fq">¹³ ¹⁴ ¹⁵ ¹⁶</em></p></blockquote><p id="65e5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As Molak demonstrates “we can compute counterfactuals when we meet certain assumptions” (33) ¹¹. That means there are circumstances when we can judge reasoning on counterfactuals as either right or wrong, correct or incorrect. In this respect even what’s fiction (counter to fact) can, in a sense, be <em class="nx">true</em>.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pe"><img src="../Images/609a0178afba0bfbbba353287702fec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*L1IPvt1n47L__oFK"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nz" href="https://unsplash.com/@eiskonen?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Hans Eiskonen</a> on <a class="af nz" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="26b4" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk"><strong class="al">AI Beyond “Stochastic Parrots”</strong></h1><p id="4f52" class="pw-post-body-paragraph nb nc fq nd b go ow nf ng gr ox ni nj nk oy nm nn no oz nq nr ns pa nu nv nw fj bk">Verbal fluency seems to be the new bright shiny thing. But is it thought? If the prowess of IBM’s Deep Blue and DeepMind’s AlphaGo could be relegated to mere cold calculation, the flesh-and-blood aroma of ChatGPT’s verbal fluency since late 2022 seriously elevated—or at least reframed — the old question of whether an AI can really “think.” Or is the LLM inside ChatGPT merely a “stochastic parrot,” stitching together highly probable strings of words in infinitely new combinations? There are times though when it seems that putative human minds — some running for the highest office in the land — are doing no more than that. Will the real intelligence in the room please stand up?</p><p id="2abc" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In her brief article about Davos 2024, “<a class="af nz" href="https://fionajmcevoy.medium.com/raising-baby-ai-in-2024-1b7d704cd3d5" rel="noopener">Raising Baby AI in 2024</a>”, Fiona McEvoy reported that Yann LeCun emphasized the need for AIs to learn from not just text but also video footage.¹⁷ Yet that’s still passive; it’s still an attempt to learn from video-documented “fact” (video footage that already exists); McEvoy reports that</p><blockquote class="pb pc pd"><p id="7a96" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“[Daphne] Koller contends that to go beyond mere associations and get to something that feels like the causal reasoning humans use, systems will need to interact with the real world in an embodied way — for example, gathering input from technologies that are ‘out in the wild’, like augmented reality and autonomous vehicles. She added that such systems would also need to be given the space to experiment with the world to learn, grow, and go beyond what a human can teach them.” <em class="fq">¹⁷</em></p></blockquote><p id="a809" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Another way to say it this: AIs will have to interact with the world in an <em class="nx">embodied</em> way at least somewhat in order to hone their ability to engage in counterfactual reasoning. We’ve all seen the videos — or watched up close — a cat pushing an object off a counter, with apparently no purpose, except to annoy us. Human babies and toddlers do it too. Despite appearances, however, this isn’t just acting out. Rather, in a somewhat naïve incarnation, these are acts of <em class="nx">hypothesis testing</em>. Such acts are prompted by a curiosity: What would happen if I shoved this vase?</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pf"><img src="../Images/e7ae42796b346703ff78cbc1ddb7476d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MHAKS2tJbtcc6utH7KheAw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Generated with DALL-E 3 and edited by the author</figcaption></figure><p id="79d6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Please watch this <a class="af nz" href="https://www.northtorontocatrescue.com/wp-content/uploads/2020/05/source.gif" rel="noopener ugc nofollow" target="_blank">3-second animated gif</a> displayed by <a class="af nz" href="https://www.northtorontocatrescue.com/why-do-kitties-knock-things-over/" rel="noopener ugc nofollow" target="_blank">North Toronto Cat Rescue</a>.¹⁸ In this brief cat video, there’s an additional detail which sheds more light: The cat is about to jump; but before jumping she realizes there’s an object immediately at hand which can be used to test the distance or surface in advance of jumping. Her jump was counterfactual (she hadn’t jumped yet). The fact that she had already almost jumped indicates that she hypothesized the jump was feasible; the cat had quickly simulated the jump in her mind; suddenly realizing that the bottle on the counter afforded the opportunity to make an intervention, to test out her hypothesis; this act was habitual.</p><p id="8373" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">I have no doubt that her ability to assess the feasibility of such jumps arose from having physically acted out similar situations many times before. Would an AI, who doesn’t have physical skin in the game, have done the same? And obviously humans do this on a scale far beyond what cats do. It’s how scientific discovery and technological invention happen; but on a more mundane level this part of intelligence is how living organisms routinely operate, whether it’s a cat jumping to the floor or a human making a business decision.</p><h2 id="9e07" class="pg ob fq bf oc ph pi pj of pk pl pm oi nk pn po pp no pq pr ps ns pt pu pv pw bk">Abduction</h2><p id="92fb" class="pw-post-body-paragraph nb nc fq nd b go ow nf ng gr ox ni nj nk oy nm nn no oz nq nr ns pa nu nv nw fj bk">Testing out counterfactuals by making interventions seems to hone our ability to do what Charles Sanders Peirce dubbed <em class="nx">abductive reasoning.</em>¹⁹ ²⁰ As distinct from <em class="nx">induction</em> (inferring a pattern from repeated cases) and <em class="nx">deduction</em> (deriving logical implications), abduction is the assertion of a hypothesis. Although Data Scientists often explore hypothetical scenarios in terms of feature engineering and hyperparameter tuning, abductive reasoning isn’t really directly a part of applications of Machine Learning, because Machine Learning is usually optimizing on a pre-established space of possibilities based on fact, whereas as abductive reasoning is expanding the space of possibilities, beyond what is already fact. So perhaps Artificial General Intelligence (AGI) has a lot to catch up on.</p><p id="e81f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here’s a hypothesis:</p><ul class=""><li id="0c18" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw px py pz bk">Entities (biological or artificial) that lack the ability (or opportunity) to make interventions don’t cultivate much counterfactual reasoning capability.</li></ul><p id="9856" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Counterfactual reasoning, or abduction, is mainly worthwhile to the extent that one can subsequently try out the hypotheses through interventions. That’s why it’s relevant to an animal (or human). Absent eventual opportunities to intervene, causal reasoning (abduction, hypothesizing) is futile, and therefore not worth cultivating.</p><p id="cd6a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The capacity for abductive reasoning would not have evolved in humans (or cats), if it didn’t provide some advantage. Such advantage can only pertain to making interventions since abduction (counterfactual reasoning) by definition does not articulate facts about the current state of the world. These observations are what prompt the hypothesis above about biological and artificial entities.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qa"><img src="../Images/a4e2fbe0c2d8958844c18b5f37c1ee36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gtV_m2CdfRoaxyMq"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nz" href="https://unsplash.com/@alicealinari?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Alice Alinari</a> on <a class="af nz" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="6f3d" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk"><strong class="al">Large Language Models</strong></h1><p id="52f1" class="pw-post-body-paragraph nb nc fq nd b go ow nf ng gr ox ni nj nk oy nm nn no oz nq nr ns pa nu nv nw fj bk">As I mentioned above, RAG (retrieval augmented generation, also known as relevant answer generation) has become the de facto approach for guiding LLM-driven GenAI systems (chatbots) toward appropriate or even optimal responses. The premise of RAG is that if snippets of relevant truthful text are directly supplied to the generative LLM along with your question, then it’s less likely to hallucinate, and in that sense provides better responses. “Hallucinating” is AI industry jargon for: <em class="nx">fabricating erroneous responses</em>.</p><p id="2dff" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As is well known, hallucinations arise because LLMs, though trained thoroughly and carefully on massive amounts of human-written text from the internet, are still not omniscient, but tend to issue responses in a rather uniformly confident tone. Surprising? Actually it shouldn’t be. It makes sense, as the famous critique goes: LLMs are essentially parroting the text they’ve been trained on. Because LLMs are trained not on people’s sometimes tentative or evolving inner thoughts, but rather on the verbalizations of those thoughts that reached an assuredness threshold sufficient for a person to post for all to read on the biggest ever public forum that is the internet. So perhaps it’s understandable that LLMs skew toward overconfidence — they are what they eat.</p><p id="101b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In fact, I think it’s fair to say that, unlike many honest humans, LLMs don’t verbally signal their assuredness level at all; they don’t modulate their tone to reflect their level of assuredness. Therefore the strategy for avoiding or reducing hallucinations is to set up the LLM for success by pushing the facts it needs right under its nose, so that it can’t ignore them. This is feasible for situations where chatbots are usually deployed, which typically have a limited scope. Documents generally relevant to the scope are assembled in advance (in a vector store/database) so that particularly relevant snippets of text can be searched for on demand and supplied to the LLM along with the question being asked, so that the LLM is nudged to somehow exploit the snippets upon generating its response.</p><h2 id="47da" class="pg ob fq bf oc ph pi pj of pk pl pm oi nk pn po pp no pq pr ps ns pt pu pv pw bk">From RAGs to richer</h2><p id="5f06" class="pw-post-body-paragraph nb nc fq nd b go ow nf ng gr ox ni nj nk oy nm nn no oz nq nr ns pa nu nv nw fj bk">Still there are various ways things can go awry. An entire ecosystem of configurable toolkits for addressing these has arisen. NVIDIA’s open source NeMo-guardrails can filter out unsafe and inappropriate responses as well as help check for factuality. <a class="af nz" href="https://www.johnsnowlabs.com/langtest/" rel="noopener ugc nofollow" target="_blank">John Snow Labs’ LangTest</a> boasts “60+ Test Types for Comparing LLM &amp; NLP Models on Accuracy, Bias, Fairness, Robustness &amp; More.” Two toolkits that focus most intensely on the veracity of responses are <a class="af nz" href="https://docs.ragas.io/" rel="noopener ugc nofollow" target="_blank">Ragas</a> and <a class="af nz" href="https://www.trulens.org/" rel="noopener ugc nofollow" target="_blank">TruEra’s TrueLens</a>.</p><p id="2f1e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">At the heart of TrueLens (and similarly Ragas) sits an elegant premise: There are three interconnected units of text involved in each call to a RAG pipeline: the <em class="nx">query</em>, the <em class="nx">retrieved context,</em> and the <em class="nx">response</em>; and the pipeline fails to the extent that there’s a semantic gap between any of these. TruEra calls this the “RAG triad.” In other words, for a RAG pipeline to work properly, three things have to happen successfully: (1) the context retrieved must be sufficiently relevant; (2) the generated response must be sufficiently grounded in the retrieved context; and (3) the generated response must also be sufficiently relevant to the original query. A weak link anywhere in this loop equates to weakness in that call to the RAG pipeline. For instance:</p><p id="171a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Query:</strong> “Which country landed on the moon first?”</p><p id="086b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Retrieved context</strong>: “Neil Armstrong stepped foot on the moon in July 1969. Buzz Aldrin was the pilot.”</p><p id="9db4" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Generated response:</strong> “Neil Armstrong and Buzz Aldrin landed on the moon in 1969.”</p><p id="7711" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The generated response isn’t sufficiently relevant to the original query — the third link is broken.</p><p id="10e3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">There are many other nuances to RAG evaluation, some of which are discussed in <a class="af nz" href="https://www.tonic.ai/blog/rag-evaluation-series-validating-the-rag-performance-of-the-openais-rag-assistant-vs-googles-vertex-search-and-conversation" rel="noopener ugc nofollow" target="_blank">Adam Kamor’s series of articles on RAG evaluation</a>.²¹</p><p id="7a45" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In so far as veracity is concerned, the RAG strategy is to avoid hallucination by deriving responses as much as possible from relevant trustworthy human-written preexisting text.</p><h2 id="61a7" class="pg ob fq bf oc ph pi pj of pk pl pm oi nk pn po pp no pq pr ps ns pt pu pv pw bk">How RAG lags</h2><p id="239d" class="pw-post-body-paragraph nb nc fq nd b go ow nf ng gr ox ni nj nk oy nm nn no oz nq nr ns pa nu nv nw fj bk">Yet how does the RAG strategy square with the popular critique of language AI: that it is merely parroting the text it was trained on? Does it go beyond parroting, to handle counterfactuals? The RAG strategy basically tries to avoid hallucination by supplementing training text with additional text curated by humans, humans in the loop, who can attend to the scope of the chatbot’s particular use case. Thus humans-in-the-loop supplement the generative LLM’s training, by supplying a corpus of relevant factual texts to be drawn from.</p><p id="71e6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Works of fiction are typically not included in the corpus that populates a RAG’s vector store. And even preexisting fictional prose doesn’t exhaust the theoretically infinite number of counterfactual propositions which might be deemed true, or correct, in some sense.</p><p id="86f9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">But intelligence includes the ability to assess such counterfactual propositions:</p><blockquote class="pb pc pd"><p id="b3c2" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“My foot up to my ankle will get soaking wet if I step in that huge puddle.”</p></blockquote><p id="a567" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In this case, a GenAI system able to synthesize verbalizations previously issued by humans — whether from the LLM’s training set or from a context retrieved and supplied downstream — isn’t very impressive. Rather than original reasoning, it’s just parroting what someone already said. And parroting what’s already been said doesn’t serve the purpose at hand when counterfactuals are considered.</p><h2 id="0c9d" class="pg ob fq bf oc ph pi pj of pk pl pm oi nk pn po pp no pq pr ps ns pt pu pv pw bk">Here’s a pair of proposals:</h2><ul class=""><li id="779d" class="nb nc fq nd b go ow nf ng gr ox ni nj nk oy nm nn no oz nq nr ns pa nu nv nw px py pz bk"><strong class="nd fr">To the extent a GenAI is just parroting, it is bringing forth or synthesizing verbalizations it was trained on.</strong></li><li id="e401" class="nb nc fq nd b go qb nf ng gr qc ni nj nk qd nm nn no qe nq nr ns qf nu nv nw px py pz bk"><strong class="nd fr">To the extent a GenAI can surmount mere parroting and reason accurately, it can successfully handle counterfactuals.</strong></li></ul><p id="6973" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The crucial thing about <strong class="nd fr">counterfactuals</strong> that Molak explains is that they “can be thought of as hypothetical or simulated interventions that <em class="nx">assume a particular state of the world</em>” or as Pearl, Gilmour, and Jewell describe counterfactuals as a minimal modification to a system (Molak, 28).¹¹ ²² The point is that answering counterfactuals correctly — or even plausibly — requires more-than-anecdotal knowledge of the world. For LLMs, their corpus-based pretraining, and their prompting infused with retrieved factual documents pins their success to the power of anecdote. Whereas a human intelligence often doesn’t need, and cannot rely on, anecdote to engage in counterfactual reasoning plausibly. That’s why counterfactual reasoning is in some ways a better measure of LLMs capabilities than fidelity to factuality is.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qg"><img src="../Images/843511802bdffe6f89734d7a2d752556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*W_KsbCkYvjXVipBG"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nz" href="https://unsplash.com/@sutirtab?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">sutirta budiman</a> on <a class="af nz" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="e505" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk"><strong class="al">Running Open-source LLMs: Ollama</strong></h1><p id="dde4" class="pw-post-body-paragraph nb nc fq nd b go ow nf ng gr ox ni nj nk oy nm nn no oz nq nr ns pa nu nv nw fj bk">To explore a bit these issues of counterfactuality with respect to Large Language Models, let us consider them more concretely by running a generative model. To minimize impediments I will demonstrate it by downloading a model to run on one’s own machine — so you don’t need an api key. We’ll do this using <a class="af nz" href="https://ollama.com/" rel="noopener ugc nofollow" target="_blank">Ollama</a>. (If you don’t want to try this yourself, you can skip over the rest of this section.)</p><p id="f634" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Ollama is a free tool that facilitates running open source LLMs on your local computer. It’s also possible to run Ollama in DataBricks, and possibly other cloud platforms. For simplicity’s sake, let’s do it locally. (For such local setup I’m indebted to Iago Modesto Brandão’s handy <a class="af nz" href="https://medium.com/poatek/building-open-source-llm-based-chatbots-using-llama-index-e6de9999ee76" rel="noopener">Building Open Source LLM based Chatbots using Llama Index</a>²³ from which the following is adapted.)</p><p id="c825" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The easiest way is to: download and install docker (the Docker app) then, within terminal, run a couple of commands to pull and run ollama as a server, which can be accessed from within a jupyter notebook (after installing two packages).</p><p id="d5c0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here are the steps:</p><p id="75b0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">1. Download and install Docker <a class="af nz" href="https://www.docker.com/products/docker-desktop/" rel="noopener ugc nofollow" target="_blank">https://www.docker.com/products/docker-desktop/</a></p><p id="fcc4" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">2. Launch Terminal and run these commands one after another:</p><pre class="ml mm mn mo mp qh qi qj bp qk bb bk"><span id="02dc" class="ql ob fq qi b bg qm qn l qo qp">docker pull ollama/ollama<br/><br/>docker run -d -v ollama:/root/.ollama -p 11434:11434 - name ollama ollama/ollama<br/><br/>pip install install llama-index==0.8.59<br/><br/>pip install openai==0.28.1</span></pre><p id="26fc" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">3. Launch jupyter:</p><p id="8dc4" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><code class="cx qq qr qs qi b">jupyter notebook</code></p><p id="27c3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">4. Within the jupyter notebook, import ollama and create an LLM object. For the sake of speed, we’ll use a relatively smaller model: Microsoft’s <em class="nx">phi</em>.</p><figure class="ml mm mn mo mp mq"><div class="qt io l ed"><div class="qu qv l"/></div></figure><p id="f2f7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now we’re ready to use Phi via ollama to generate text in response to our prompt. For this we use the llm object’s <strong class="nd fr">complete()</strong> method. It generates a response (might take a minute or so), which we’ll print out.</p><p id="3f17" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Let’s try out a counterfactual: : “Would a cement balloon float?”</p><figure class="ml mm mn mo mp mq"><div class="qt io l ed"><div class="qu qv l"/></div></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qw"><img src="../Images/bb476bb26abbcb9d44fde17abb2c0175.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8N-rHoajCoOpnptN"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by Haneen Krimly on Unsplash</figcaption></figure><p id="b42e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Next let’s examine the “reasoning” displayed in Phi’s output.</p><h1 id="85fe" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk"><strong class="al">Interpreting LLM (GenAI) answers to counterfactual questions</strong></h1><p id="1f87" class="pw-post-body-paragraph nb nc fq nd b go ow nf ng gr ox ni nj nk oy nm nn no oz nq nr ns pa nu nv nw fj bk">Leaving aside, for the moment, that Microsoft’s Phi is not a very powerful GPT (generative pretrained transformer), consider its response:</p><pre class="ml mm mn mo mp qh qi qj bp qk bb bk"><span id="3a01" class="ql ob fq qi b bg qm qn l qo qp">I don’t have personal beliefs or opinions, but based on my knowledge of <br/>physics, i can tell you that a cement balloon would not be able to float <br/>in air due to its low density. air is lighter than most materials used for<br/>balloons, and the inflated rubber material of a typical balloon provides <br/>enough buoyancy to lift it into the sky. however, if you were to fill a <br/>cement balloon with helium gas or another lighter-than-air substance, it <br/>would likely float as long as the weight of the balloon itself is less than <br/>the upward force from the lifting gas.</span></pre><p id="7834" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This seems reasonable. If a regular balloon (one I’ve blown up directly with my mouth) wouldn’t float in the air, then neither would one whose exterior is made from cement. If instead, however, the gas pumped inside is lighter than air, such as helium is, then a regular balloon would float up into the sky.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qx"><img src="../Images/6256a74db1c74ac7e8406c933abd12b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iT2PpDglRVRUP1-h"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by First Last @ rtisanyb on <a class="af nz" href="https://unsplash.com/es/fotos/un-grupo-de-piedras-nOo9yg8pTgQ" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="f435" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The original question asked about floating, but not didn’t specify the medium; apparently Phi assumed it to be air.</p><p id="9f43" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">So now let’s specify that in the question, but choose water instead, which was actually my original intention anyway.</p><figure class="ml mm mn mo mp mq"><div class="qt io l ed"><div class="qu qv l"/></div></figure><pre class="ml mm mn mo mp qh qi qj bp qk bb bk"><span id="295c" class="ql ob fq qi b bg qm qn l qo qp">No, a cement balloon would not float in water. Cement is a very dense material<br/>and does not have much air trapped inside it, so it cannot create buoyancy <br/>like a regular balloon filled with helium or air. Additionally, the weight of <br/>the cement balloon combined with its density would make it sink to the bottom <br/>of the water rather than floating on the surface.</span></pre><p id="a411" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Interesting. Not only does it conclude that it would sink, but now it assumes that the medium is water instead of air. It reasons about the density of cement, as compared to air, but doesn’t take into account the air inside the balloon. Or possibly it’s reasoning about a solid cement ball; but, hmmm, that’s not a balloon. Unless, by “cement balloon” it’s thinking that this is analogous to a water balloon: a balloon filled with water.</p><p id="ec2d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Let’s change the question again, this time specifying the medium is water.</p><figure class="ml mm mn mo mp mq"><div class="qt io l ed"><div class="qu qv l"/></div></figure><pre class="ml mm mn mo mp qh qi qj bp qk bb bk"><span id="8b41" class="ql ob fq qi b bg qm qn l qo qp">No, a cement balloon filled with helium would not float on water. This is <br/>because helium is lighter than air and therefore lifts the balloon upwards <br/>when it is released into the air. However, cement is much denser than water <br/>and thus will sink to the bottom of the water. When you combine these two <br/>facts, the result is that the concrete balloon filled with helium would not <br/>float on water but instead sink to the bottom due to its weight.<br/><br/>In summary, while the helium in the balloon provides buoyancy to keep it <br/>afloat in air, the density of the cement itself makes it too heavy for the <br/>helium to lift and keep it afloat in water. Thus, a cement balloon filled <br/>with helium would not float on water but instead sink to the bottom due to <br/>its weight. </span></pre><p id="da72" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Hmm, that’s odd. According to this latest response, the same object that would float up into the air, would sink in water, even though water is denser than air. Sure, this is a counterfactual situation — who’s ever heard of a cement balloon? But let’s try to visualize it: You are standing by a pool of water, or perhaps you are in a boat. You are holding this spherical object, say at waist level. You let go of it, and it starts to float up higher into the air, but you catch it before it’s out of reach. Now you lower this same spherical object down to the surface of the water. And as it touches the water, you slowly and gently let go again. What do you think will happen?</p><p id="ab1d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Will this helium-filled cement balloon, which a moment earlier had started to float up into the sky, now suddenly be swallowed up by the water, pushed down to sink to the bottom? Does that ring true?</p><p id="ff30" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Keep in mind our own embodied knowledge, which might be relevant here: Our own flesh-and-blood bodies — which aren’t even light enough to spontaneously float up into the air — when placed on water in fact don’t sink, but rather float. That embodied knowledge suggests that an object that is light enough to float up into the air, and thus is lighter than ourselves, would also be light enough to float on whatever, if we (being heavier than that object) float on water. (Consult Lakoff &amp; Johnson for an overview of embodied cognition.²⁴)</p><p id="ba4b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Granted, a GPT language model, such as Phi, is decidedly disembodied; it lacks embodied knowledge. In this case, perhaps that embodied knowledge, which Phi lacks, impinges on counterfactual reasoning ability.</p><p id="a45d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Ok, but perhaps context and continuity are the issue here. What I’ve shown above is not a true conversation, as would happen with a chat-bot; I haven’t passed the previous context back to Phi, but rather I’m asking isolated questions. So perhaps Phi is imagining different proportions of cement-to-helium in its responses to different questions. After all, if the cement were thick enough, or if the chamber of helium inside it were small enough, then the relative lightness of helium to air (or water) would insufficiantly counteract the heaviness of the cement. So lets rephrase the question to clarify that detail.</p><figure class="ml mm mn mo mp mq"><div class="qt io l ed"><div class="qu qv l"/></div></figure><pre class="ml mm mn mo mp qh qi qj bp qk bb bk"><span id="2d6e" class="ql ob fq qi b bg qm qn l qo qp">It is unlikely that a cement balloon would be able to float in water since <br/>cement is denser than water. The reason why the cement balloon might float <br/>in air is due to the helium gas inside, which has a lower density than both <br/>air and cement. In water, the buoyant force exerted by the water on the cement <br/>balloon will be much less than the weight of the balloon itself. Hence, the <br/>cement balloon would sink in water rather than float as it does in the air.</span></pre><p id="d5f7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Ok, still something’s wrong. Within the same response it’s asserting that the cement balloon is light enough to float up into the air, but is too heavy to float in water. Doesn’t this imply that water is lighter than air? Hmm, <br/>something about that smells fishy — and it isn’t the water.</p><p id="d47f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">I’m not the first to suggest that GPT language models like Phi (models that gain their ability through training on massive corpuses of text) sometimes lack basic reasoning ability. In this case I’m suggesting how some of that reasoning ability emerges from implicit embodied knowledge — knowledge we gain through physical presence of our bodies within the material world (our physical bodies which don’t float up into the air but do float on water). I’m also suggesting how counterfactual questions can help reveal this.</p><p id="5301" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">GPT models, such as Phi, primarily act associatively. It is the associative or connotative facets of language usage that GPTs and LLMs generally have mastered so well. In terms of connotation, arising from most common usage, the word ‘cement’ is associated with heaviness. It has this in common with the heavy metal lead — “Led Zeppelin” was originally a sarcastic barb precluding the New Yardbirds’ prospects to soar. Although if enough air were pumped into it, cement could become light, that isn’t an association that readily comes to mind. Rather it’s a counterfactual, a thought experiment. Even though my previous phrasing of the question refers to “a very thin layer of cement,… filled with helium”, the material itself, “cement,” too strongly associates with heavines for Phi to imagine it as floating in water — no one has ever built a cement boat!</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qy"><img src="../Images/5fc98ae150f9906b8e3018fd3fbf97a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bk9Yg4_G21DBp4nL"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nz" href="https://unsplash.com/@huyghetim?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tim Huyghe</a> on <a class="af nz" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="90bc" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">So let’s switch out cement for a different substance with less leaden connotations. How about glass?</p><figure class="ml mm mn mo mp mq"><div class="qt io l ed"><div class="qu qv l"/></div></figure><pre class="ml mm mn mo mp qh qi qj bp qk bb bk"><span id="e49d" class="ql ob fq qi b bg qm qn l qo qp">As an AI language model, I do not have personal opinions or beliefs, but based <br/>on scientific principles, a glass balloon filled with helium would indeed <br/>float in water. This is because the density of helium gas is much lower than <br/>that of water, causing it to rise and displace its weight in water. <br/>Additionally, glass is less dense than water, which means that it would also <br/>be buoyant enough to keep the balloon afloat on top of the water’s surface. <br/>However, if the balloon were to be filled with a denser gas or a heavier <br/>object, such as sand, then it may sink or become too heavy to float.</span></pre><p id="c76d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Aha! Now with associative (connotative) obstacles brushed to the side, Phi is able to arrive at reasoning that is comparable to human reasoning, replete with implicit embodied knowledge, even though the latter isn’t the source in this case. In this case Phi has uncovered the key point, that “the density of helium gas is much lower than that of water” even though it has gotten mixed up in its “Additional” point, where it says that glass’s density is less than water’s, which is a hallucination; it’s not true.</p><h1 id="f06d" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk"><strong class="al">What about OpenAI’s ChatGPT?</strong></h1><p id="eebe" class="pw-post-body-paragraph nb nc fq nd b go ow nf ng gr ox ni nj nk oy nm nn no oz nq nr ns pa nu nv nw fj bk">The big elephant in the room, though, is that Microsoft’s Phi is very far from being the state of the art LLM. It’s small enough to run locally on a laptop. So what about OpenAI’s GPT models?</p><p id="df21" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To run this code below, you’ll need to sign up for free at <a class="af nz" href="https://openai.com" rel="noopener ugc nofollow" target="_blank">https://openai.com</a> to get an OpenAI API access token.</p><figure class="ml mm mn mo mp mq"><div class="qt io l ed"><div class="qu qv l"/></div></figure><p id="d033" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">What does OpenAI’s GPT say about the helium-filled glass balloon on water?</p><figure class="ml mm mn mo mp mq"><div class="qt io l ed"><div class="qu qv l"/></div></figure><pre class="ml mm mn mo mp qh qi qj bp qk bb bk"><span id="7ac0" class="ql ob fq qi b bg qm qn l qo qp">No, a glass balloon filled with helium cannot float in water, regardless of <br/>its size.  The buoyancy of an object in a fluid is determined by the density <br/>of the object compared to the density of the fluid. Density is defined as mass <br/>divided by volume. In simple terms, if the object is less dense than the fluid,<br/>it will float; if it is more dense, it will sink.  In this case, the glass <br/>balloon is filled with helium, which is a gas that is less dense than water. <br/>Helium-filled balloons float in the air because the density of helium is lower <br/>than that of the surrounding air.  However, when we place the helium-filled <br/>glass balloon in water, the density of the water is significantly higher than <br/>that of helium. Even though the glass balloon is very thin and lightweight, <br/>the overall density of the balloon, including the glass and helium, will still <br/>be higher than that of water.  Therefore, the balloon will not displace enough <br/>water to generate enough buoyant force to counteract the weight of the balloon <br/>itself. As a result, the balloon will sink in water rather than float.</span></pre><p id="f814" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Oh no! Even OpenAI’s GPT 3.5 turbo gets buried in the connotations of cement. It gets that “helium filled balloons float in air.” And it gets that helium is less dense than water. But then it fumbles into thinking “the overall density of the balloon, including the glass and helium, will still be higher than that of water.” As Phi did above, OpenAI’s GPT 3.5 turbo has implied that the balloon is heavier than water but lighter than air, which implies that water is lighter than air.</p><p id="1843" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We know it’s wrong; but it’s not wrong because it’s lacking facts, or has directly contradicted fact: The whole cement balloon scenario is far from being fact; it’s counterfactual.</p><p id="1f13" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Post-hoc we are able to apply <em class="nx">reductio ad absurdum</em> to deduce that Phi’s and OpenAI’s GPT 3.5 turbo’s negative conclusions do actually contradict another fact, namely that water is heavier than air. But this is a respect in which counterfactual reasoning is in fact reasoning, not just dreaming. That is, counterfactual reasoning can be shown to be definitively true or definitively false. Despite deviating from what’s factual, it is actually just as much a form of reasoning as is reasoning based on fact.</p><h1 id="40a3" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk"><strong class="al">Fact, Fiction, and Hallucination: What counterfactuals show us</strong></h1><p id="2b68" class="pw-post-body-paragraph nb nc fq nd b go ow nf ng gr ox ni nj nk oy nm nn no oz nq nr ns pa nu nv nw fj bk">Since ChatGPT overwhelmed public consciousness in late 2022, the dominant concern that was immediately and persistently stirred up has been hallucination. Oh the horror that an AI system could assert something not based in fact! But instead of focusing on just factuality as a primary standard for AI systems — as has happened in many business use-cases — it now seems clear that fact vs. fiction isn’t the only axis along which an AI system should be expected to or hoped to succeed. Even when an AI system’s response is based in fact, it can still be irrelevant, a non sequitur, which is why evaluation approaches such as Ragas and TruVera specifically examine relevance of response.</p><p id="b7af" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">When it fails on the relevance criterion, it is not even the Fact vs. Fiction axis that is at play. An irrelevant response can be just as factual as a relevant one, and by definition, counterfactual reasoning, whether correct or not, is not factual in a literal sense, certainly not in the sense constituted by RAG systems. That is, counterfactual reasoning is not achieved by retrieving documents that are topically relevant to the question posed. What makes counterfactual reasoning powerful is how it may apply analogies to bring to bear systems of facts that might seem completely out of scope to the question being posed. It might be diagrammed something like this:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qz"><img src="../Images/601c8456b6defd2cc80c76c6dab2c57e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*VpuwEG2e8eOQ-GB8msHubg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">image by author</figcaption></figure><p id="0858" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">One might also represent some of these facets this way:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj ra"><img src="../Images/d741ae68d8857c9d169222ccb9e9d699.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*rJYVd8hnpmMjGqk2pSHndQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">image by author</figcaption></figure><p id="ea8d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">What do linear estimators have to do with this?<br/></strong>Since counterfactual reasoning is not based in seemingly relevant facts but rather in systematized facts, sometimes from other domains, or that are topically remote, it’s not something that obviously benefits directly from document-store retrieval systems. There’s an analogy here between types of linear estimators: A gradient-boosted tree linear estimator essentially cannot succeed in making accurate predictions on data whose features substantially exceed the numeric ranges of the training data; this is because decision cut-points can only be made based on data presented at training time. By contrast, regression models (which can have closed form solutions) enable accurate predictions on features that exceed the numerical ranges of the training data.</p><p id="7614" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In practical terms this is why linear models can be helpful in business applications. To the extent your linear model is accurate, it might help you predict the outcome of raising or lowering your product’s sales price beyond any price you’ve ever offered before: a <em class="nx">counterfactual price</em>. Whereas a gradient-boosted tree model that performs equally well in validation does not help you reason through such counterfactuals, which, ironically, might have been the motivation for developing the model in the first place. In this sense, the <em class="nx">explainability</em> of linear models is of a completely different sort from what SHAP values offer, as the latter shed little light on what would happen with data that is outside the distribution of the model’s training data.</p><p id="d827" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The prowess of LLMs has certainly shown that the limits of “intelligence” synthesized ingeniously from crowdsourcing human-written texts are much greater than expected. It’s obvious that this eclipsed the former tendency to place value in “intelligence” based on conceptual understanding, which reveals itself especially in the ability to accurately reason beyond facts. So I find it interesting to attempt to challenge LLMs to this standard, which goes against their grain.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rb"><img src="../Images/0c9a595810c03384dc2ad6a930b2b82b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g850d8E_rosgp3VqeapiVA.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Bonnie Moreland on Freerange Stock</figcaption></figure><h1 id="2434" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk"><strong class="al">Reflection</strong></h1><p id="c3b9" class="pw-post-body-paragraph nb nc fq nd b go ow nf ng gr ox ni nj nk oy nm nn no oz nq nr ns pa nu nv nw fj bk">Far from being frivolous diversions, counterfactuals play a role in the progress of science, exemplifying what Charles Sanders Peirce calls <em class="nx">abduction</em>, which is distinct from induction (inductive reasoning) and deduction (deductive reasoning). Abduction basically means the formulation of hypotheses. We might rightly ask: Should we expect an LLM to exhibit such capability? What’s the advantage? I don’t have a definitive answer, but more a speculative one: It’s well known within the GenAI community that when prompting an LLM, asking it to “reason step-by-step” often leads to more satisfactory responses, even though the reasoning steps themselves are not the desired response. In other words, for some reason, not yet completely understood, asking the LLM to somewhat simulate the most reliable processes of human reasoning (thinking step-by-step) leads to better end results. Perhaps, somewhat counterintuitively, even though LLMs are not trained to reason as humans do, the lineages of human reasoning in general contribute to better AI end results. In this case, given the important role that abduction plays in the evolution of science, AI end results might improve to the extent that LLMs are capable of reasoning counterfactually.</p><p id="d487" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Visit me on <a class="af nz" href="https://www.linkedin.com/in/joshua-banks-mailman/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a></p></div></div></div><div class="ab cb rc rd re rf" role="separator"><span class="rg by bm rh ri rj"/><span class="rg by bm rh ri rj"/><span class="rg by bm rh ri"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="171e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[1] <a class="af nz" href="https://www.heraldscotland.com/business_hq/13103840.bottom-line/" rel="noopener ugc nofollow" target="_blank">The Bottom Line</a>. (2013) The Herald. May 8.</p><p id="aae9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[2] C. Munn, Investing for Generations: A History of the Alliance Trust (2012)</p><p id="c37e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[3] R. Strassfeld. <a class="af nz" href="https://scholarlycommons.law.case.edu/cgi/viewcontent.cgi?referer=&amp;httpsredir=1&amp;article=1372&amp;context=faculty_publications" rel="noopener ugc nofollow" target="_blank">If…: Counterfactuals in the Law</a>. (1992) Case Western Reserve University School of Law Scholarly Commons</p><p id="f289" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[4] S. Wachter, B. Mittelstadt, &amp; C. Russell, <a class="af nz" href="https://jolt.law.harvard.edu/assets/articlePDFs/v31/Counterfactual-Explanations-without-Opening-the-Black-Box-Sandra-Wachter-et-al.pdf" rel="noopener ugc nofollow" target="_blank">Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR</a>. 2018. Harvard Journal of Law and Technology, vol. 31, no. 2</p><p id="2b17" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[5] J. Stoyanovich, <a class="af nz" href="https://rules.cityofnewyork.us/wp-content/uploads/2022/09/Stoyanovich_LL144_October24_2022.pdf" rel="noopener ugc nofollow" target="_blank">Testimony of Julia Stoyanovich before the New York City Department of Consumer and Worker Protection regarding Local Law 144 of 2021 in Relation to Automated Employment Decision Tools (AEDTs)</a>. (2022) Center for Responsible AI, NY Tandon School of Engineering</p><p id="7c0c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[6] S. Lorr, <a class="af nz" href="https://www.nytimes.com/2023/05/25/technology/ai-hiring-law-new-york.html" rel="noopener ugc nofollow" target="_blank">A Hiring Law Blazes a Path for A.I. Regulation</a> (2023) The New York Times, May 25</p><p id="2373" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[7] J. Stoyanovich, <a class="af nz" href="https://nyudatascience.medium.com/until-you-try-to-regulate-you-wont-learn-how-julia-stoyanovich-discusses-responsible-ai-for-4cf119002b05" rel="noopener">“Until you try to regulate, you won’t learn how”: Julia Stoyanovich discusses responsible AI for The New York Times</a> (2023), Medium. June 21</p><p id="fcec" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[8] J. Pearl, <a class="af nz" href="https://ftp.cs.ucla.edu/pub/stat_ser/r355-reprint.pdf" rel="noopener ugc nofollow" target="_blank">Foundations of Causal Inference</a> (2010). Sociological Methodology. vol. 40, no. 1.</p><p id="1175" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[9] L. Wittgenstein, Philosophical Investigations (1953) G.E.M. Anscombe and R. Rhees (eds.), G.E.M. Anscombe (trans.), Oxford: Blackwell</p><p id="e942" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[10] E. Siegel, <a class="af nz" href="https://mitpress.mit.edu/9780262048903/the-ai-playbook/" rel="noopener ugc nofollow" target="_blank">The AI Playbook: Mastering the Rare Art of Machine Learning Deployment</a>. (2024) MIT Press.</p><p id="c27b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[11] A. Molak. <a class="af nz" href="https://www.packtpub.com/product/causal-inference-and-discovery-in-python/9781804612989" rel="noopener ugc nofollow" target="_blank">Causal Inference and Discovery in Python</a> (2023) Packt.</p><p id="7eba" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[12] R. Byrne. The Rational Imagination: How People Create Alternatives to Reality (2005) MIT Press.</p><p id="8701" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[13] W. Starr. <a class="af nz" href="https://plato.stanford.edu/entries/counterfactuals/" rel="noopener ugc nofollow" target="_blank">Counterfactuals</a> (2019) Stanford Encyclopedia of Philosophy.</p><p id="a5af" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[14] M. Ginsburg. <a class="af nz" href="https://www.ijcai.org/Proceedings/85-1/Papers/015.pdf" rel="noopener ugc nofollow" target="_blank">Counterfactuals</a> (1985)<em class="nx"> </em>Proceedings of the Ninth International Joint Conference on Artificial Intelligence.</p><p id="b972" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[15] J. Pearl, Judea. Causation, Action, and Counterfactuals (1995) in Computational Learning and Probabilistic Reasoning. John Wiley and Sons.</p><p id="621e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[16] T. Costello and J. McCarthy. Useful Counterfactuals (1999) Linköping Electronic Articles in Computer and Information Science, vol. 4, no. 12.</p><p id="bf48" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[17] F. McEvoy. <a class="af nz" href="https://fionajmcevoy.medium.com/raising-baby-ai-in-2024-1b7d704cd3d5" rel="noopener">Raising Baby AI in 2024</a> (2024) Medium.</p><p id="000a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[18] N. Berger. <a class="af nz" href="https://www.northtorontocatrescue.com/why-do-kitties-knock-things-over/" rel="noopener ugc nofollow" target="_blank">Why Do Kitties Knock Things Over?</a> (2020) North Toronto Cat Rescue.</p><p id="32c7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[19] C. S. Peirce. Collected Papers of Charles Sanders Peirce (1931–1958) edited by C. Hartshorne, P. Weiss, and A. Burks,. Harvard University Press.</p><p id="068c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[20] I. Douven. <a class="af nz" href="https://plato.stanford.edu/entries/abduction/peirce.html" rel="noopener ugc nofollow" target="_blank">Peirce on Abduction</a> (2021) Stanford Encyclopedia of Philosophy.</p><p id="103c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[21] A. Kamor. <a class="af nz" href="https://www.tonic.ai/blog/rag-evaluation-series-validating-the-rag-performance-of-the-openais-rag-assistant-vs-googles-vertex-search-and-conversation" rel="noopener ugc nofollow" target="_blank">RAG Evaluation Series: Validating the RAG performance of the OpenAI’s Rag Assistant vs Google’s Vertex Search and Conversation</a> (2024) Tonic.ai.</p><p id="e11c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[22] Pearl, J., Glymour, M., &amp; Jewell, N. Causal inference in statistics: A primer. (2016). New York: Wiley.</p><p id="67db" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[23] I. M. Brandão, <a class="af nz" href="https://medium.com/poatek/building-open-source-llm-based-chatbots-using-llama-index-e6de9999ee76" rel="noopener">Building Open Source LLM based Chatbots using Llama Index</a> (2023) Medium.</p><p id="87ce" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[24] G. Lakoff and M. Johnson. Philosophy in the Flesh: The Embodied Mind And Its Challenge To Western Thought (1999) Basic Books.</p></div></div></div></div>    
</body>
</html>