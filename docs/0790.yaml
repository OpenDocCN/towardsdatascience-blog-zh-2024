- en: Model Evaluations Versus Task Evaluations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/model-evaluations-versus-task-evaluations-5bc742054957?source=collection_archive---------7-----------------------#2024-03-26](https://towardsdatascience.com/model-evaluations-versus-task-evaluations-5bc742054957?source=collection_archive---------7-----------------------#2024-03-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/d871ad67918406174ff5c74003dc75fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using Dall-E 3
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the difference for LLM applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://aparnadhinak.medium.com/?source=post_page---byline--5bc742054957--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page---byline--5bc742054957--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5bc742054957--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5bc742054957--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page---byline--5bc742054957--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5bc742054957--------------------------------)
    ·9 min read·Mar 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: For a moment, imagine an airplane. What springs to mind? Now imagine a Boeing
    737 and a [V-22 Osprey](https://www.af.mil/About-Us/Fact-Sheets/Display/Article/104531/cv-22-osprey/).
    Both are aircraft designed to move cargo and people, yet they serve different
    purposes — one more general (commercial flights and freight), the other very specific
    (infiltration, exfiltration, and resupply missions for special operations forces).
    They look far different because they are built for different activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the rise of LLMs, we have seen our first truly general-purpose ML models.
    Their generality helps us in so many ways:'
  prefs: []
  type: TYPE_NORMAL
- en: The same engineering team can now do sentiment analysis and structured data
    extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practitioners in many domains can share knowledge, making it possible for the
    whole industry to benefit from each other’s experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a wide range of industries and jobs where the same experience is useful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But as we see with aircraft, generality requires a very different assessment
    from excelling at a particular task, and at the end of the day business value
    often comes from solving particular problems.
  prefs: []
  type: TYPE_NORMAL
- en: This is a good analogy for the difference between model and task evaluations.
    Model evals are focused on overall general assessment, but task evals are focused
    on assessing performance of a particular task.
  prefs: []
  type: TYPE_NORMAL
- en: There Is More Than One LLM Eval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term *LLM evals* is thrown around quite generally. [OpenAI released some
    tooling to do LLM evals](/how-to-best-leverage-openais-evals-framework-c38bcef0ec47)
    very early, for example. Most practitioners are more concerned with LLM task evals,
    but that distinction is not always clearly made.
  prefs: []
  type: TYPE_NORMAL
- en: What’s the Difference?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model evals look at the “general fitness” of the model. How well does it do
    on a variety of tasks?
  prefs: []
  type: TYPE_NORMAL
- en: Task evals, on the other hand, are specifically designed to look at how well
    the model is suited for your particular application.
  prefs: []
  type: TYPE_NORMAL
- en: Someone who works out generally and is quite fit would likely fare poorly against
    a professional sumo wrestler in a real competition, and model evals can’t stack
    up against task evals in assessing your particular needs.
  prefs: []
  type: TYPE_NORMAL
- en: Model Evals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model evals are specifically meant for building and fine-tuning generalized
    models. They are based on a set of questions you ask a model and a set of ground-truth
    answers that you use to grade responses. Think of taking the SATs.
  prefs: []
  type: TYPE_NORMAL
- en: While every question in a model eval is different, there is usually a general
    area of testing. There is a theme or skill each metric is specifically targeted
    at. For example, HellaSwag performance has become a popular way to measure LLM
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: The [HellaSwag](https://rowanzellers.com/hellaswag/) dataset consists of a collection
    of contexts and multiple-choice questions where each question has multiple potential
    completions. Only one of the completions is sensible or logically coherent, while
    the others are plausible but incorrect. These completions are designed to be challenging
    for AI models, requiring not just linguistic understanding but also common sense
    reasoning to choose the correct option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A tray of potatoes is loaded into the oven and removed. A large tray of cake
    is flipped over and placed on counter. a large tray of meat*'
  prefs: []
  type: TYPE_NORMAL
- en: '*A. is placed onto a baked potato*'
  prefs: []
  type: TYPE_NORMAL
- en: '*B. ls, and pickles are placed in the oven*'
  prefs: []
  type: TYPE_NORMAL
- en: '*C. is prepared then it is removed from the oven by a helper when done.*'
  prefs: []
  type: TYPE_NORMAL
- en: Another example is MMLU. [MMLU](https://paperswithcode.com/dataset/mmlu) features
    tasks that span multiple subjects, including science, literature, history, social
    science, mathematics, and professional domains like law and medicine. This diversity
    in subjects is intended to mimic the breadth of knowledge and understanding required
    by human learners, making it a good test of a model’s ability to handle multifaceted
    language understanding challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some examples — can you solve them?
  prefs: []
  type: TYPE_NORMAL
- en: '*For which of the following thermodynamic processes is the increase in the
    internal energy of an ideal gas equal to the heat added to the gas?*'
  prefs: []
  type: TYPE_NORMAL
- en: '*A. Constant Temperature*'
  prefs: []
  type: TYPE_NORMAL
- en: '*B. Constant Volume*'
  prefs: []
  type: TYPE_NORMAL
- en: '*C. Constant Pressure*'
  prefs: []
  type: TYPE_NORMAL
- en: '*D. Adiabatic*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2336f2e5e5ddc480643c4db20ca56fda.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The [Hugging Face Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    is perhaps the best known place to get such model evals. The leaderboard tracks
    open source large language models and keeps track of many model evaluation metrics.
    This is typically a great place to start understanding the difference between
    open source LLMs in terms of their performance across a variety of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal models require even more evals. The [Gemini paper](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)
    demonstrates that multi-modality introduces a host of other benchmarks like VQAv2,
    which tests the ability to understand and integrate visual information. This information
    goes beyond simple object recognition to interpreting actions and relationships
    between them.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, there are metrics for audio and video information and how to integrate
    across modalities.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of these tests is to differentiate between two models or two different
    snapshots of the same model. Picking a model for your application is important,
    but it is something you do once or at most very infrequently.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa0265520e614b0de21aa34794f3abb7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Task Evals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The much more frequent problem is one solved by task evaluations. The goal of
    task-based evaluations is to analyze the performance of the model using LLM as
    a judge.
  prefs: []
  type: TYPE_NORMAL
- en: Did your retrieval system fetch the right data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there hallucinations in your responses?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Did the system answer important questions with relevant answers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some may feel a bit unsure about an LLM evaluating other LLMs, but we have humans
    evaluating other humans all the time.
  prefs: []
  type: TYPE_NORMAL
- en: The real distinction between model and task evaluations is that for a model
    eval we ask many different questions, but for a task eval the question stays the
    same and it is the data we change. For example, say you were operating a chatbot.
    You could use your task eval on hundreds of customer interactions and ask it,
    “*Is there a hallucination here?”* The question stays the same across all the
    conversations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/864534f219b59a6179b2009f4f190190.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several libraries aimed at helping practitioners build these evaluations:
    [Ragas](https://docs.ragas.io/en/latest/index.html), [Phoenix](https://phoenix.arize.com/)
    (full disclosure: the author leads the team that developed Phoenix), [OpenAI](https://github.com/openai/evals/tree/main/evals),
    [LlamaIndex](https://docs.llamaindex.ai/en/latest/optimizing/evaluation/evaluation.html).'
  prefs: []
  type: TYPE_NORMAL
- en: How do they work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The task eval grades performance of every output from the application as a whole.
    Let’s look at what it takes to put one together.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a benchmark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The foundation rests on establishing a robust benchmark. This starts with creating
    a golden dataset that accurately reflects the scenarios the LLM will encounter.
    This dataset should include ground truth labels — often derived from meticulous
    human review — to serve as a standard for comparison. Don’t worry, though, you
    can usually get away with dozens to hundreds of examples here. Selecting the right
    LLM for evaluation is also critical. While it may differ from the application’s
    primary LLM, it should align with goals of cost-efficiency and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Crafting the evaluation template
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The heart of the task evaluation process is the evaluation template. This template
    should clearly define the input (e.g., user queries and documents), the evaluation
    question (e.g., the relevance of the document to the query), and the expected
    output formats (binary or multi-class relevance). Adjustments to the template
    may be necessary to capture nuances specific to your application, ensuring it
    can accurately assess the LLM’s performance against the golden dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example of a template to evaluate a Q&A task.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Metrics and iteration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running the eval across your golden dataset allows you to generate key metrics
    such as accuracy, precision, recall, and F1-score. These provide insight into
    the evaluation template’s effectiveness and highlight areas for improvement. Iteration
    is crucial; refining the template based on these metrics ensures the evaluation
    process remains aligned with the application’s goals without overfitting to the
    golden dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In task evaluations, relying solely on overall accuracy is insufficient since
    we always expect significant class imbalance. Precision and recall offer a more
    robust view of the LLM’s performance, emphasizing the importance of identifying
    both relevant and irrelevant outcomes accurately. A balanced approach to metrics
    ensures that evaluations meaningfully contribute to enhancing the LLM application.
  prefs: []
  type: TYPE_NORMAL
- en: Application of LLM evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once an evaluation framework is in place, the next step is to apply these evaluations
    directly to your LLM application. This involves integrating the evaluation process
    into the application’s workflow, allowing for real-time assessment of the LLM’s
    responses to user inputs. This continuous feedback loop is invaluable for maintaining
    and improving the application’s relevance and accuracy over time.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation across the system lifecycle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Effective task evaluations are not confined to a single stage but are integral
    throughout the LLM system’s life cycle. From pre-production benchmarking and testing
    to ongoing performance assessments in production, [LLM evaluation](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)
    ensures the system remains responsive to user need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: is the model hallucinating?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at a hallucination example in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/474e7210ed5ea5d8d8d011a5c4e42f96.png)'
  prefs: []
  type: TYPE_IMG
- en: Example by author
  prefs: []
  type: TYPE_NORMAL
- en: Since hallucinations are a common problem for most practitioners, there are
    some benchmark datasets available. These are a great first step, but you will
    often need to have a customized dataset within your company.
  prefs: []
  type: TYPE_NORMAL
- en: The next important step is to develop the prompt template. Here again a good
    library can help you get started. We saw an example prompt template earlier, here
    we see another specifically for hallucinations. You may need to tweak it for your
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now you are ready to give your eval LLM the queries from your golden dataset
    and have it label hallucinations. When you look at the results, remember that
    there should be class imbalance. You want to track precision and recall instead
    of overall accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: It is very useful to construct a confusion matrix and plot it visually. When
    you have such a plot, you can feel reassurance about your LLM’s performance. If
    the performance is not to your satisfaction, you can always optimize the prompt
    template.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3804aa615cd2edded178962eaeb8b6e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Example of evaluating performance of the task eval so users can build confidence
    in their evals*'
  prefs: []
  type: TYPE_NORMAL
- en: After the eval is built, you now have a powerful tool that can label all your
    data with known precision and recall. You can use it to track hallucinations in
    your system both during development and production phases.
  prefs: []
  type: TYPE_NORMAL
- en: Summary of Differences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s sum up the differences between task and model evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e289f5124ebc71d4065f18dc29f0bbc.png)'
  prefs: []
  type: TYPE_IMG
- en: Table by author
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ultimately, both model evaluations and task evaluations are important in putting
    together a functional LLM system. It is important to understand [when and how
    to apply each](https://arize.com/blog-course/large-language-model-evaluations-vs-llm-task-evaluations-in-llm-application-development/).
    For most practitioners, the majority of their time will be spent on task evals,
    which provide a measure of system performance on a specific task.
  prefs: []
  type: TYPE_NORMAL
