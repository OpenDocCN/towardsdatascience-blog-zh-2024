- en: Model Evaluations Versus Task Evaluations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估与任务评估
- en: 原文：[https://towardsdatascience.com/model-evaluations-versus-task-evaluations-5bc742054957?source=collection_archive---------7-----------------------#2024-03-26](https://towardsdatascience.com/model-evaluations-versus-task-evaluations-5bc742054957?source=collection_archive---------7-----------------------#2024-03-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/model-evaluations-versus-task-evaluations-5bc742054957?source=collection_archive---------7-----------------------#2024-03-26](https://towardsdatascience.com/model-evaluations-versus-task-evaluations-5bc742054957?source=collection_archive---------7-----------------------#2024-03-26)
- en: '![](../Images/d871ad67918406174ff5c74003dc75fb.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d871ad67918406174ff5c74003dc75fb.png)'
- en: Image created by author using Dall-E 3
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用Dall-E 3创建的图像
- en: Understanding the difference for LLM applications
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解LLM应用中的差异
- en: '[](https://aparnadhinak.medium.com/?source=post_page---byline--5bc742054957--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page---byline--5bc742054957--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5bc742054957--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5bc742054957--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page---byline--5bc742054957--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://aparnadhinak.medium.com/?source=post_page---byline--5bc742054957--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page---byline--5bc742054957--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5bc742054957--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5bc742054957--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page---byline--5bc742054957--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5bc742054957--------------------------------)
    ·9 min read·Mar 26, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5bc742054957--------------------------------)
    ·9分钟阅读·2024年3月26日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: For a moment, imagine an airplane. What springs to mind? Now imagine a Boeing
    737 and a [V-22 Osprey](https://www.af.mil/About-Us/Fact-Sheets/Display/Article/104531/cv-22-osprey/).
    Both are aircraft designed to move cargo and people, yet they serve different
    purposes — one more general (commercial flights and freight), the other very specific
    (infiltration, exfiltration, and resupply missions for special operations forces).
    They look far different because they are built for different activities.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下飞机。你首先想到的是什么？现在再想象一架波音737和一架[V-22鱼鹰](https://www.af.mil/About-Us/Fact-Sheets/Display/Article/104531/cv-22-osprey/)。这两者都是旨在运输货物和人员的飞机，但它们服务的目的不同——一种更为通用（商业航班和货运），另一种非常具体（为特种作战部队执行渗透、撤离和补给任务）。它们看起来完全不同，因为它们是为不同的活动而设计的。
- en: 'With the rise of LLMs, we have seen our first truly general-purpose ML models.
    Their generality helps us in so many ways:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM的兴起，我们见证了第一批真正的通用机器学习模型。它们的通用性在许多方面帮助了我们：
- en: The same engineering team can now do sentiment analysis and structured data
    extraction
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同一个工程团队现在可以进行情感分析和结构化数据提取
- en: Practitioners in many domains can share knowledge, making it possible for the
    whole industry to benefit from each other’s experience
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多领域的从业者可以共享知识，从而使整个行业能够相互受益于彼此的经验
- en: There is a wide range of industries and jobs where the same experience is useful
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有许多行业和工作领域，其中相同的经验是有用的
- en: But as we see with aircraft, generality requires a very different assessment
    from excelling at a particular task, and at the end of the day business value
    often comes from solving particular problems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但正如我们在飞机中看到的，通用性需要与在特定任务上出色表现截然不同的评估方法，归根结底，商业价值通常来自于解决特定的问题。
- en: This is a good analogy for the difference between model and task evaluations.
    Model evals are focused on overall general assessment, but task evals are focused
    on assessing performance of a particular task.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型评估与任务评估差异的一个很好的类比。模型评估侧重于总体的综合评估，而任务评估则侧重于评估特定任务的表现。
- en: There Is More Than One LLM Eval
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不止一个LLM评估
- en: The term *LLM evals* is thrown around quite generally. [OpenAI released some
    tooling to do LLM evals](/how-to-best-leverage-openais-evals-framework-c38bcef0ec47)
    very early, for example. Most practitioners are more concerned with LLM task evals,
    but that distinction is not always clearly made.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*LLM评估* 这个术语常常被广泛使用。[OpenAI早期发布了一些工具来进行LLM评估](/how-to-best-leverage-openais-evals-framework-c38bcef0ec47)，例如。大多数从业者更关注LLM任务评估，但这一区分并不总是很清晰。'
- en: What’s the Difference?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有什么区别？
- en: Model evals look at the “general fitness” of the model. How well does it do
    on a variety of tasks?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估关注的是模型的“整体健身情况”。它在各种任务上的表现如何？
- en: Task evals, on the other hand, are specifically designed to look at how well
    the model is suited for your particular application.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 任务评估则专门设计用来检查模型是否适合你的特定应用。
- en: Someone who works out generally and is quite fit would likely fare poorly against
    a professional sumo wrestler in a real competition, and model evals can’t stack
    up against task evals in assessing your particular needs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一般锻炼并且身体素质较好的人，在真实比赛中可能会在职业相扑选手面前表现不佳，而模型评估无法与任务评估在评估你特定需求的能力上相提并论。
- en: Model Evals
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: Model evals are specifically meant for building and fine-tuning generalized
    models. They are based on a set of questions you ask a model and a set of ground-truth
    answers that you use to grade responses. Think of taking the SATs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估专门用于构建和微调通用模型。它们基于你给模型提出的一组问题以及你用来评分的地面真实答案。可以将其想象成参加SAT考试。
- en: While every question in a model eval is different, there is usually a general
    area of testing. There is a theme or skill each metric is specifically targeted
    at. For example, HellaSwag performance has become a popular way to measure LLM
    quality.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型评估中的每个问题都不同，但通常有一个共同的测试领域。每个指标都有一个特定的目标主题或技能。例如，HellaSwag的表现已经成为衡量LLM质量的流行方式。
- en: The [HellaSwag](https://rowanzellers.com/hellaswag/) dataset consists of a collection
    of contexts and multiple-choice questions where each question has multiple potential
    completions. Only one of the completions is sensible or logically coherent, while
    the others are plausible but incorrect. These completions are designed to be challenging
    for AI models, requiring not just linguistic understanding but also common sense
    reasoning to choose the correct option.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[HellaSwag](https://rowanzellers.com/hellaswag/) 数据集包含了一系列上下文和多项选择题，每个问题都有多个可能的完成选项。只有一个选项是合乎逻辑且合理的，其他选项虽然看似合理，但其实是错误的。这些完成项旨在对AI模型提出挑战，不仅要求语言理解能力，还需要常识推理才能选择正确的选项。'
- en: 'Here is an example:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子：
- en: '*A tray of potatoes is loaded into the oven and removed. A large tray of cake
    is flipped over and placed on counter. a large tray of meat*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*一盘土豆被放入烤箱并取出。一大盘蛋糕被翻过来并放到柜台上。一大盘肉*'
- en: '*A. is placed onto a baked potato*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*A. 被放到烤土豆上*'
- en: '*B. ls, and pickles are placed in the oven*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*B. 土豆和腌菜被放入烤箱中*'
- en: '*C. is prepared then it is removed from the oven by a helper when done.*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*C. 被准备好后，由助手在完成时从烤箱中取出。*'
- en: Another example is MMLU. [MMLU](https://paperswithcode.com/dataset/mmlu) features
    tasks that span multiple subjects, including science, literature, history, social
    science, mathematics, and professional domains like law and medicine. This diversity
    in subjects is intended to mimic the breadth of knowledge and understanding required
    by human learners, making it a good test of a model’s ability to handle multifaceted
    language understanding challenges.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是 MMLU。[MMLU](https://paperswithcode.com/dataset/mmlu) 涵盖了多个学科的任务，包括科学、文学、历史、社会科学、数学以及法律和医学等专业领域。这些学科的多样性旨在模拟人类学习者所需的知识和理解的广度，使其成为测试模型处理多面语言理解挑战能力的好方法。
- en: Here are some examples — can you solve them?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些例子——你能解答它们吗？
- en: '*For which of the following thermodynamic processes is the increase in the
    internal energy of an ideal gas equal to the heat added to the gas?*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*在以下哪些热力学过程中，理想气体的内能增加等于加热到气体的热量？*'
- en: '*A. Constant Temperature*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*A. 恒温*'
- en: '*B. Constant Volume*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*B. 恒体积*'
- en: '*C. Constant Pressure*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*C. 恒压*'
- en: '*D. Adiabatic*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*D. 绝热过程*'
- en: '![](../Images/2336f2e5e5ddc480643c4db20ca56fda.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2336f2e5e5ddc480643c4db20ca56fda.png)'
- en: Image by author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: The [Hugging Face Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    is perhaps the best known place to get such model evals. The leaderboard tracks
    open source large language models and keeps track of many model evaluation metrics.
    This is typically a great place to start understanding the difference between
    open source LLMs in terms of their performance across a variety of tasks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[Hugging Face排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)可能是获取此类模型评估的最知名平台。排行榜跟踪开源的大型语言模型，并记录许多模型评估指标。这通常是一个很好的起点，用来理解开源LLM在不同任务表现上的差异。'
- en: Multimodal models require even more evals. The [Gemini paper](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)
    demonstrates that multi-modality introduces a host of other benchmarks like VQAv2,
    which tests the ability to understand and integrate visual information. This information
    goes beyond simple object recognition to interpreting actions and relationships
    between them.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态模型需要更多的评估。[Gemini论文](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)展示了多模态引入了许多其他基准，比如VQAv2，它测试理解和整合视觉信息的能力。这些信息不仅仅是简单的物体识别，而是对行动和物体之间关系的解读。
- en: Similarly, there are metrics for audio and video information and how to integrate
    across modalities.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，针对音频和视频信息以及如何跨模态整合的指标也存在。
- en: The goal of these tests is to differentiate between two models or two different
    snapshots of the same model. Picking a model for your application is important,
    but it is something you do once or at most very infrequently.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些测试的目标是区分两个模型或同一个模型的两个不同快照。选择一个适合你应用的模型很重要，但这是你做的事，一般情况下只是一次性操作或非常不频繁的操作。
- en: '![](../Images/aa0265520e614b0de21aa34794f3abb7.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa0265520e614b0de21aa34794f3abb7.png)'
- en: Image by author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Task Evals
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务评估
- en: The much more frequent problem is one solved by task evaluations. The goal of
    task-based evaluations is to analyze the performance of the model using LLM as
    a judge.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 更常见的问题是通过任务评估来解决的。基于任务的评估目标是分析使用LLM作为评判者的模型表现。
- en: Did your retrieval system fetch the right data?
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的检索系统是否获取了正确的数据？
- en: Are there hallucinations in your responses?
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的回答中有幻觉吗？
- en: Did the system answer important questions with relevant answers?
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统是否用相关的答案回答了重要问题？
- en: Some may feel a bit unsure about an LLM evaluating other LLMs, but we have humans
    evaluating other humans all the time.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人可能对LLM评估其他LLM感到有些不确定，但我们每天都有人工评估其他人。
- en: The real distinction between model and task evaluations is that for a model
    eval we ask many different questions, but for a task eval the question stays the
    same and it is the data we change. For example, say you were operating a chatbot.
    You could use your task eval on hundreds of customer interactions and ask it,
    “*Is there a hallucination here?”* The question stays the same across all the
    conversations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估和任务评估的真正区别在于：在模型评估中，我们会提出许多不同的问题，而在任务评估中，问题保持不变，只有数据会变化。例如，假设你在操作一个聊天机器人。你可以在数百次客户互动中使用你的任务评估，并问它：“*这里有幻觉吗？*”这个问题在所有对话中始终相同。
- en: '![](../Images/864534f219b59a6179b2009f4f190190.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/864534f219b59a6179b2009f4f190190.png)'
- en: Image by author
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'There are several libraries aimed at helping practitioners build these evaluations:
    [Ragas](https://docs.ragas.io/en/latest/index.html), [Phoenix](https://phoenix.arize.com/)
    (full disclosure: the author leads the team that developed Phoenix), [OpenAI](https://github.com/openai/evals/tree/main/evals),
    [LlamaIndex](https://docs.llamaindex.ai/en/latest/optimizing/evaluation/evaluation.html).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个库旨在帮助实践者构建这些评估：[Ragas](https://docs.ragas.io/en/latest/index.html)，[Phoenix](https://phoenix.arize.com/)（完全披露：作者领导了开发Phoenix的团队），[OpenAI](https://github.com/openai/evals/tree/main/evals)，[LlamaIndex](https://docs.llamaindex.ai/en/latest/optimizing/evaluation/evaluation.html)。
- en: How do they work?
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它们是如何工作的？
- en: The task eval grades performance of every output from the application as a whole.
    Let’s look at what it takes to put one together.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 任务评估整体上评估应用程序的每个输出的表现。我们来看看构建一个评估任务需要哪些内容。
- en: Establishing a benchmark
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立基准
- en: The foundation rests on establishing a robust benchmark. This starts with creating
    a golden dataset that accurately reflects the scenarios the LLM will encounter.
    This dataset should include ground truth labels — often derived from meticulous
    human review — to serve as a standard for comparison. Don’t worry, though, you
    can usually get away with dozens to hundreds of examples here. Selecting the right
    LLM for evaluation is also critical. While it may differ from the application’s
    primary LLM, it should align with goals of cost-efficiency and accuracy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基础在于建立一个稳健的基准。这从创建一个能够准确反映 LLM 将遇到的场景的黄金数据集开始。该数据集应包含地面真实标签——通常来源于细致的人工审核——作为对比标准。别担心，通常你可以用几十到几百个示例来完成。选择合适的
    LLM 进行评估也至关重要。虽然它可能与应用程序的主要 LLM 不同，但应该与成本效益和准确性目标保持一致。
- en: Crafting the evaluation template
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 制定评估模板
- en: The heart of the task evaluation process is the evaluation template. This template
    should clearly define the input (e.g., user queries and documents), the evaluation
    question (e.g., the relevance of the document to the query), and the expected
    output formats (binary or multi-class relevance). Adjustments to the template
    may be necessary to capture nuances specific to your application, ensuring it
    can accurately assess the LLM’s performance against the golden dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 任务评估过程的核心是评估模板。该模板应清晰定义输入（例如，用户查询和文档）、评估问题（例如，文档与查询的相关性）和预期的输出格式（二元或多类别相关性）。根据应用程序的具体需求，可能需要调整模板，以确保它能够准确评估
    LLM 在黄金数据集上的表现。
- en: Here is an example of a template to evaluate a Q&A task.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个用来评估问答任务的模板示例。
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Metrics and iteration
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 度量标准与迭代
- en: Running the eval across your golden dataset allows you to generate key metrics
    such as accuracy, precision, recall, and F1-score. These provide insight into
    the evaluation template’s effectiveness and highlight areas for improvement. Iteration
    is crucial; refining the template based on these metrics ensures the evaluation
    process remains aligned with the application’s goals without overfitting to the
    golden dataset.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的黄金数据集上运行评估，可以生成关键度量指标，如准确率、精确度、召回率和 F1 分数。这些指标为评估模板的有效性提供了洞察，并突出改进的领域。迭代至关重要；根据这些度量标准精炼模板，确保评估过程与应用目标保持一致，同时避免对黄金数据集的过拟合。
- en: In task evaluations, relying solely on overall accuracy is insufficient since
    we always expect significant class imbalance. Precision and recall offer a more
    robust view of the LLM’s performance, emphasizing the importance of identifying
    both relevant and irrelevant outcomes accurately. A balanced approach to metrics
    ensures that evaluations meaningfully contribute to enhancing the LLM application.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在任务评估中，仅依赖总体准确率是不够的，因为我们总是会遇到显著的类别不平衡。精确度和召回率提供了更全面的视角，强调准确识别相关和不相关结果的重要性。平衡的度量方法确保评估对提升
    LLM 应用有实际贡献。
- en: Application of LLM evaluations
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 评估的应用
- en: Once an evaluation framework is in place, the next step is to apply these evaluations
    directly to your LLM application. This involves integrating the evaluation process
    into the application’s workflow, allowing for real-time assessment of the LLM’s
    responses to user inputs. This continuous feedback loop is invaluable for maintaining
    and improving the application’s relevance and accuracy over time.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦评估框架就绪，下一步是将这些评估直接应用到你的 LLM 应用中。这涉及将评估过程集成到应用程序的工作流程中，以便实时评估 LLM 对用户输入的响应。这个持续的反馈循环对于保持和提高应用的相关性和准确性至关重要。
- en: Evaluation across the system lifecycle
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统生命周期中的评估
- en: Effective task evaluations are not confined to a single stage but are integral
    throughout the LLM system’s life cycle. From pre-production benchmarking and testing
    to ongoing performance assessments in production, [LLM evaluation](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)
    ensures the system remains responsive to user need.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的任务评估不仅限于单一阶段，而是贯穿整个大语言模型（LLM）系统的生命周期。从生产前的基准测试和评估，到生产中的持续性能评估， [LLM 评估](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)
    确保系统始终能够响应用户需求。
- en: 'Example: is the model hallucinating?'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：模型是否出现幻觉？
- en: Let’s look at a hallucination example in more detail.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一个幻觉示例。
- en: '![](../Images/474e7210ed5ea5d8d8d011a5c4e42f96.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/474e7210ed5ea5d8d8d011a5c4e42f96.png)'
- en: Example by author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作者示例
- en: Since hallucinations are a common problem for most practitioners, there are
    some benchmark datasets available. These are a great first step, but you will
    often need to have a customized dataset within your company.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于幻觉是大多数从业者面临的普遍问题，现有一些基准数据集可供使用。这是一个很好的起步，但你通常需要在公司内部拥有一个定制的数据集。
- en: The next important step is to develop the prompt template. Here again a good
    library can help you get started. We saw an example prompt template earlier, here
    we see another specifically for hallucinations. You may need to tweak it for your
    purposes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的重要步骤是开发提示模板。这里同样一个好的库可以帮助你入门。我们之前看到了一个示例提示模板，这里我们看到另一个专门针对幻觉的模板。你可能需要根据你的需求进行调整。
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now you are ready to give your eval LLM the queries from your golden dataset
    and have it label hallucinations. When you look at the results, remember that
    there should be class imbalance. You want to track precision and recall instead
    of overall accuracy.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好将黄金数据集中的查询传递给你的评估LLM，并让它标记出幻觉。当你查看结果时，记住应该存在类别不平衡。你需要跟踪精确度和召回率，而不是整体准确率。
- en: It is very useful to construct a confusion matrix and plot it visually. When
    you have such a plot, you can feel reassurance about your LLM’s performance. If
    the performance is not to your satisfaction, you can always optimize the prompt
    template.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 构建混淆矩阵并将其可视化非常有用。当你有了这样的图表时，你可以对你的LLM性能感到放心。如果性能不尽如人意，你可以随时优化提示模板。
- en: '![](../Images/a3804aa615cd2edded178962eaeb8b6e.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3804aa615cd2edded178962eaeb8b6e.png)'
- en: '*Example of evaluating performance of the task eval so users can build confidence
    in their evals*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*评估任务评估性能的示例，以便用户能够建立对其评估的信心*'
- en: After the eval is built, you now have a powerful tool that can label all your
    data with known precision and recall. You can use it to track hallucinations in
    your system both during development and production phases.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估构建完成后，你现在拥有了一个强大的工具，可以用已知的精确度和召回率标记所有数据。你可以使用它来跟踪你系统中的幻觉，无论是在开发阶段还是生产阶段。
- en: Summary of Differences
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 区别总结
- en: Let’s sum up the differences between task and model evaluations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下任务评估和模型评估之间的区别。
- en: '![](../Images/7e289f5124ebc71d4065f18dc29f0bbc.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e289f5124ebc71d4065f18dc29f0bbc.png)'
- en: Table by author
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表格由作者提供
- en: Takeaways
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要点总结
- en: Ultimately, both model evaluations and task evaluations are important in putting
    together a functional LLM system. It is important to understand [when and how
    to apply each](https://arize.com/blog-course/large-language-model-evaluations-vs-llm-task-evaluations-in-llm-application-development/).
    For most practitioners, the majority of their time will be spent on task evals,
    which provide a measure of system performance on a specific task.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，模型评估和任务评估在构建功能性LLM系统时都非常重要。理解[何时以及如何应用每种评估方法](https://arize.com/blog-course/large-language-model-evaluations-vs-llm-task-evaluations-in-llm-application-development/)是很关键的。对于大多数从业者来说，大部分时间将花费在任务评估上，这为系统在特定任务上的性能提供了衡量标准。
