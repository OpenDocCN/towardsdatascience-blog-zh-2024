# ä½¿ç”¨æœºå™¨å­¦ä¹ æå‡å•†ä¸šè¡¨ç°

> åŸæ–‡ï¼š[https://towardsdatascience.com/improving-business-performance-with-machine-learning-fbdf5e280923?source=collection_archive---------3-----------------------#2024-06-13](https://towardsdatascience.com/improving-business-performance-with-machine-learning-fbdf5e280923?source=collection_archive---------3-----------------------#2024-06-13)

## æ— è®ºä½ æ˜¯æ•°æ®ç§‘å­¦å®¶ã€åˆ†æå¸ˆï¼Œè¿˜æ˜¯ä¸šåŠ¡åˆ†æå¸ˆï¼Œä½ çš„ç›®æ ‡éƒ½æ˜¯äº¤ä»˜èƒ½å¤Ÿæå‡å•†ä¸šè¡¨ç°çš„é¡¹ç›®ã€‚

[](https://medium.com/@juanjosemunozp?source=post_page---byline--fbdf5e280923--------------------------------)[![Juan Jose Munoz](../Images/b42d72e9e2a2eaf11da5465e9b041d53.png)](https://medium.com/@juanjosemunozp?source=post_page---byline--fbdf5e280923--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fbdf5e280923--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fbdf5e280923--------------------------------) [Juan Jose Munoz](https://medium.com/@juanjosemunozp?source=post_page---byline--fbdf5e280923--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fbdf5e280923--------------------------------) Â·é˜…è¯»æ—¶é—´15åˆ†é’ŸÂ·2024å¹´6æœˆ13æ—¥

--

![](../Images/d28fb9d54d5dd553dc3c6563016f0125.png)

å›¾ç‰‡æ¥æºï¼š[Daria Nepriakhina ğŸ‡ºğŸ‡¦](https://unsplash.com/@epicantus?utm_source=medium&utm_medium=referral) åœ¨[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

å¯èƒ½ä¼šæœ‰è¯±æƒ‘ä¸“æ³¨äºæœ€æ–°çš„æœºå™¨å­¦ä¹ å‘å±•æˆ–è§£å†³å¤§é—®é¢˜ã€‚ç„¶è€Œï¼Œé€šè¿‡ä½¿ç”¨ç®€å•çš„æœºå™¨å­¦ä¹ ç®—æ³•è§£å†³ä¸€äº›å®¹æ˜“å®ç°çš„ä½ä»·å€¼é—®é¢˜ï¼Œé€šå¸¸ä¹Ÿèƒ½å¸¦æ¥å¾ˆå¤§çš„ä»·å€¼ã€‚

åŸºå‡†æµ‹è¯•å°±æ˜¯å…¶ä¸­ä¸€ä¸ªä½ä»·å€¼é—®é¢˜ã€‚å®ƒæ˜¯å°†ä¸šåŠ¡KPIä¸ç±»ä¼¼ç»„ç»‡è¿›è¡Œå¯¹æ¯”çš„è¿‡ç¨‹ã€‚å®ƒä½¿ä¼ä¸šèƒ½å¤Ÿä»æœ€ä¼˜ç§€çš„ä¼ä¸šä¸­å­¦ä¹ ï¼Œå¹¶ä¸æ–­æé«˜ä¸šç»©ã€‚

åŸºå‡†æµ‹è¯•æœ‰ä¸¤ç§ç±»å‹ï¼š

> **1\. å†…éƒ¨**ï¼šä¸å…¬å¸å†…çš„å•ä½/äº§å“å¯¹æ¯”è¡¡é‡KPI
> 
> **2\. å¤–éƒ¨**ï¼šä¸ç«äº‰å¯¹æ‰‹å¯¹æ¯”è¡¡é‡KPI

åœ¨æˆ‘ä»äº‹é…’åº—è¡Œä¸šçš„æ—¥å¸¸å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¸¸å¸¸ä¾èµ–äºæ”¶é›†é…’åº—æ•°æ®çš„ç¬¬ä¸‰æ–¹å…¬å¸æ¥è¿›è¡Œå¤–éƒ¨åŸºå‡†æµ‹è¯•ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä»ä»–ä»¬é‚£é‡Œè·å¾—çš„æ•°æ®æ˜¯æœ‰é™çš„ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬ç®¡ç†ç€è¶…è¿‡500å®¶é…’åº—ï¼Œæ‹¥æœ‰å¤§é‡æ½œåœ¨çš„åŸºå‡†æµ‹è¯•æ•°æ®ã€‚

è¿™æ˜¯æˆ‘ä»¬æœ€è¿‘è®¾å®šæ¥è§£å†³çš„ä½ä»·å€¼é—®é¢˜ã€‚

æ— è®ºä½ æ­£åœ¨è¿›è¡Œå“ªç§ç±»å‹çš„åŸºå‡†æµ‹è¯•ï¼Œç¬¬ä¸€æ­¥éƒ½æ˜¯é€‰æ‹©ä¸€ç»„ä¸ç›®æ ‡é…’åº—ç›¸ä¼¼çš„é…’åº—ã€‚åœ¨é…’åº—è¡Œä¸šï¼Œæˆ‘ä»¬é€šå¸¸ä¾èµ–äºä½ç½®æŒ‡æ ‡ã€å“ç‰Œç­‰çº§ã€æˆ¿é—´æ•°é‡ã€ä»·æ ¼èŒƒå›´å’Œå¸‚åœºéœ€æ±‚ã€‚å¯¹äºä¸€ä¸¤å®¶é…’åº—ï¼Œæˆ‘ä»¬é€šå¸¸æ˜¯æ‰‹åŠ¨è¿›è¡Œè¿™æ ·çš„æ“ä½œï¼Œä½†å¯¹500å®¶é…’åº—è¿›è¡Œæ‰‹åŠ¨æ“ä½œæ˜¯ä¸å¯è¡Œçš„ã€‚

ä¸€æ—¦ç¡®å®šäº†éœ€è¦è§£å†³çš„é—®é¢˜ï¼Œä¸‹ä¸€æ­¥å°±æ˜¯é€‰æ‹©ä½¿ç”¨çš„å·¥å…·ã€‚æœºå™¨å­¦ä¹ æä¾›äº†è®¸å¤šå·¥å…·ã€‚ç„¶è€Œï¼Œè¿™ä¸ªé—®é¢˜å¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„ç®—æ³•æ—æ¥è§£å†³ï¼šæœ€è¿‘é‚»ç®—æ³•ã€‚

# **æœ€è¿‘é‚»ç®—æ³•æ—**

æœ€è¿‘é‚»ç®—æ³•æ—æ˜¯ä¸€ç§ä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨æ‰¾åˆ°ç»™å®šæ•°æ®é›†ä¸­ä¸æŒ‡å®šç‚¹æœ€æ¥è¿‘æˆ–æœ€ç›¸ä¼¼çš„ç‚¹ã€‚

è¿™äº›ç®—æ³•åœ¨è§£å†³è®¸å¤šåˆ†ç±»å’Œå›å½’é—®é¢˜æ–¹é¢éå¸¸æˆåŠŸã€‚å› æ­¤ï¼ŒScikit Learn API æä¾›äº†ä¸€ä¸ªå‡ºè‰²çš„æœ€è¿‘é‚»æ¨¡å—ã€‚

[](https://scikit-learn.org/stable/modules/classes.html?source=post_page-----fbdf5e280923--------------------------------#module-sklearn.neighbors) [## API å‚è€ƒ

### è¿™æ˜¯ Scikit-learn çš„ç±»å’Œå‡½æ•°å‚è€ƒã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒå®Œæ•´çš„ç”¨æˆ·æŒ‡å—ï¼Œå› ä¸ºâ€¦â€¦

[scikit-learn.org](https://scikit-learn.org/stable/modules/classes.html?source=post_page-----fbdf5e280923--------------------------------#module-sklearn.neighbors)

# é€‰æ‹©åˆé€‚çš„ç®—æ³•

å¤§å¤šæ•°äººéƒ½ç†Ÿæ‚‰ K æœ€è¿‘é‚»ï¼ˆKNNï¼‰ï¼›ç„¶è€Œï¼ŒScikit Learn æä¾›äº†å¤šç§ä¸åŒçš„æœ€è¿‘é‚»ç®—æ³•ï¼Œæ¶µç›–äº†ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ä»»åŠ¡ã€‚

å¯¹äºæˆ‘ä»¬çš„è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ²¡æœ‰ä»»ä½•æ ‡ç­¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯»æ‰¾ä¸€ä¸ªæ— ç›‘ç£ç®—æ³•ã€‚

å¦‚æœä½ æµè§ˆ Scikit Learn æ–‡æ¡£ï¼Œä½ ä¼šæ‰¾åˆ° `NearestNeighbors`ã€‚è¿™ä¸ªç®—æ³•æ‰§è¡Œæ— ç›‘ç£å­¦ä¹ ï¼Œç”¨äºå®ç°é‚»å±…æœç´¢ã€‚

è¿™ä¼¼ä¹æ¶µç›–äº†æˆ‘ä»¬è§£å†³é—®é¢˜æ‰€éœ€çš„å†…å®¹ã€‚è®©æˆ‘ä»¬å¼€å§‹å‡†å¤‡æ•°æ®å¹¶è¿è¡ŒåŸºå‡†æ¨¡å‹ã€‚

# åŸºå‡†æ¨¡å‹

# 1\. åŠ è½½æ•°æ®

é…’åº—çš„è¡¨ç°é€šå¸¸å–å†³äºä½ç½®ã€å“ç‰Œå’Œè§„æ¨¡ã€‚ä¸ºäº†æˆ‘ä»¬çš„åˆ†æï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸¤ä¸ªæ•°æ®é›†ï¼š

é…’åº—æ•°æ®ï¼šä»¥ä¸‹ä½¿ç”¨çš„é…’åº—æ•°æ®æ˜¯åŸºäºåŸå§‹æ•°æ®é›†äººå·¥ç”Ÿæˆçš„ï¼Œç”¨äºæœ¬æ¬¡åˆ†æã€‚

+   `å“ç‰Œ`: å®šä¹‰é…’åº—çš„æœåŠ¡æ°´å¹³ï¼šè±ªåå‹ã€é«˜æ¡£å‹ã€ç»æµå‹

+   `æˆ¿é—´æ•°`: å¯ä¾›å‡ºå”®çš„æˆ¿é—´æ•°é‡

+   `å¸‚åœº`: é…’åº—æ‰€åœ¨åŸå¸‚çš„åç§°

+   `å›½å®¶`: å›½å®¶åç§°

+   `çº¬åº¦`: é…’åº—çš„çº¬åº¦ä½ç½®

+   `ç»åº¦`: é…’åº—çš„ç»åº¦ä½ç½®

+   `æœºåœºä»£ç `: æœ€è¿‘å›½é™…æœºåœºçš„3å­—æ¯ä»£ç 

+   `å¸‚åœºå±‚çº§`: å®šä¹‰å¸‚åœºçš„å‘å±•æ°´å¹³ã€‚

+   `HCLASS`: è¡¨ç¤ºé…’åº—æ˜¯åŸå¸‚é…’åº—è¿˜æ˜¯åº¦å‡é…’åº—

+   `éœ€æ±‚`: è¡¨ç¤ºé…’åº—çš„å¹´åº¦å…¥ä½ç‡

+   `ä»·æ ¼åŒºé—´`: è¡¨ç¤ºé…’åº—çš„å¹³å‡ä»·æ ¼

æˆ‘ä»¬è¿˜çŸ¥é“ï¼Œé…’åº—çš„è¡¨ç°å¯èƒ½ä¼šå—åˆ°å¯è¾¾æ€§çš„å½±å“ã€‚ä¸ºäº†è¡¡é‡å¯è¾¾æ€§ï¼Œæˆ‘ä»¬å¯ä»¥æµ‹é‡é…’åº—è·ç¦»ä¸»è¦å›½é™…æœºåœºçš„è¿œè¿‘ã€‚æœºåœºæ•°æ®æ¥è‡ªä¸–ç•Œé“¶è¡Œï¼š[https://datacatalog.worldbank.org/search/dataset/0038117](https://datacatalog.worldbank.org/search/dataset/0038117)

+   `Orig`: 3 ä¸ªå­—æ¯çš„æœºåœºä»£ç 

+   `åç§°`: æœºåœºåç§°

+   `æ€»åº§ä½æ•°`: å¹´åº¦ä¹˜å®¢é‡

+   `å›½å®¶åç§°`: æœºåœºæ‰€åœ¨å›½å®¶çš„åç§°

+   `Airpot1Latitude`: æœºåœºçš„çº¬åº¦

+   `Airport1Longitude`ï¼šæœºåœºç»åº¦

**å…¨çƒæœºåœºæ•°æ®é›†æ ¹æ®çŸ¥è¯†å…±äº«ç½²å 4.0 è®¸å¯åè®®æˆæƒä½¿ç”¨**

è®©æˆ‘ä»¬å¯¼å…¥æ•°æ®ã€‚

```py
import pandas as pd
import numpy as np

data = pd.read_excel("mock_data.xlsx")
airport_data = pd.read_csv("airport_volume_airport_locations.csv")
```

![](../Images/5a02a04415f60ae25407698a38693530.png)

é…’åº—æ•°æ®æ ·æœ¬ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

![](../Images/1d2b250de192743886e633b4d2eb9f60.png)

æœºåœºæ•°æ®æ ·æœ¬ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

å¦‚å‰æ‰€è¿°ï¼Œé…’åº—çš„è¡¨ç°é«˜åº¦ä¾èµ–äºä½ç½®ã€‚åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬æœ‰è®¸å¤šå…³äºä½ç½®çš„åº¦é‡ï¼Œä¾‹å¦‚å¸‚åœºå›½å®¶ç­‰ã€‚ç„¶è€Œï¼Œè¿™äº›å®šä¹‰é€šå¸¸è¾ƒä¸ºå®½æ³›ï¼Œå¹¶ä¸æ€»æ˜¯ç†æƒ³çš„ã€‚ä¸ºäº†ç¼©å°ç›¸ä¼¼ä½ç½®çš„èŒƒå›´ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª`å¯è¾¾æ€§`åº¦é‡ï¼Œå®šä¹‰ä¸ºåˆ°æœ€è¿‘å›½é™…æœºåœºçš„è·ç¦»ã€‚

ä¸ºäº†è®¡ç®—é…’åº—åˆ°æœºåœºçš„è·ç¦»ï¼Œæˆ‘ä»¬ä½¿ç”¨å“ˆå¼—è¾›å…¬å¼ã€‚å“ˆå¼—è¾›å…¬å¼ç”¨äºè®¡ç®—çƒé¢ä¸Šä¸¤ç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œç»™å®šå®ƒä»¬çš„çº¬åº¦å’Œç»åº¦ã€‚

```py
# Below code is taken from geeksforgeeks
from math import radians, cos, sin, asin, sqrt

def distance_to_airport(lat, airport_lat, lon, airport_lon):

    #  Convert latitude and longitude values from decimal degrees to radians
    lon = radians(lon)
    airport_lon = radians(airport_lon)
    lat = radians(lat)
    airport_lat = radians(airport_lat)

    # Haversine formula
    dlon = airport_lon - lon
    dlat = airport_lat - lat
    a = sin(dlat / 2)**2 + cos(lat) * cos(airport_lat) * sin(dlon / 2)**2

    c = 2 * asin(sqrt(a))

    # Radius of earth in kilometers.
    r = 6371

    # return distance in KM
    return(c * r)

#Apply the distance_to_airport functions to each hotel
data["distance_to_airport"] = data.apply(lambda row: distance_to_airport(row["Latitude"],row["Airport1Latitude"],row["Longitude"],row["Airport1Longitude"]),axis=1)
data.head()
```

![](../Images/f42916249a4d86d75892a53743b9a4f2.png)

åŒ…å«æœºåœºè·ç¦»ç‰¹å¾çš„ç»“æœæ•°æ®æ¡†ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

ä¸‹ä¸€æ­¥æ˜¯ç§»é™¤æˆ‘ä»¬ä¸éœ€è¦çš„ä»»ä½•åˆ—ã€‚

```py
# Drop Columns that we dont need
# For the purpose of benchmarking we will keep the hotel feautures, and distance to airport
col_to_drop = ["Latitude","Longitude","Airport Code","Orig","Name","TotalSeats","Country Name","Airport1Latitude","Airport1Longitude"]

data_clean = data.drop(col_to_drop,axis=1)
data_clean.head()
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰éæ•°å€¼å˜é‡è¿›è¡Œç¼–ç ï¼Œä»¥ä¾¿å¯ä»¥å°†å®ƒä»¬è¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚æ­¤æ—¶ï¼Œé‡è¦çš„æ˜¯è¦è®°ä½ï¼Œæˆ‘ä»¬éœ€è¦åŸå§‹æ ‡ç­¾æ¥å‘å›¢é˜Ÿå‘ˆç°æˆ‘ä»¬å»ºè®®çš„åˆ†ç»„ï¼Œå¹¶ä¾¿äºéªŒè¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†æŠŠç¼–ç ä¿¡æ¯å­˜å‚¨åœ¨ä¸€ä¸ªå­—å…¸ä¸­ã€‚

```py
from sklearn.preprocessing import LabelEncoder

# Create a LabelEncoder object for each object column
brand_encoder = LabelEncoder()
market_encoder = LabelEncoder()
country_encoder = LabelEncoder()
market_tier_encoder = LabelEncoder()
hclass_encoder = LabelEncoder()

# Fit each LabelEncoder on the unique values of the corresponding column
data_clean['BRAND'] = brand_encoder.fit_transform(data_clean['BRAND'])
data_clean['Market'] = market_encoder.fit_transform(data_clean['Market'])
data_clean['Country'] = country_encoder.fit_transform(data_clean['Country'])
data_clean['Market Tier'] = market_tier_encoder.fit_transform(data_clean['Market Tier'])
data_clean['HCLASS']= hclass_encoder.fit_transform(data_clean['HCLASS'])

# create a dictionnary with all the encoders for reverse encoding
encoders ={"BRAND" : brand_encoder,
           "Market": market_encoder,
           "Country": country_encoder,
           "Market Tier": market_tier_encoder,
           "HCLASS": hclass_encoder
}

data_clean.head()
```

![](../Images/765d8ac866bab658217e85f9dde837cb.png)

ç¼–ç æ•°æ®ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

æˆ‘ä»¬çš„æ•°æ®ç°åœ¨æ˜¯æ•°å€¼å‹çš„ï¼Œä½†æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œæ¯åˆ—ä¸­çš„å€¼çš„èŒƒå›´å·®å¼‚éå¸¸å¤§ã€‚ä¸ºäº†é¿å…ä»»ä½•ç‰¹å¾çš„èŒƒå›´ä¸æˆæ¯”ä¾‹åœ°å½±å“æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé‡æ–°ç¼©æ”¾ã€‚

```py
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_clean)
data_scaled
```

![](../Images/a885e446568eb13a0c3a01d119cf4258.png)

æ ‡å‡†åŒ–æ•°æ®ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

æ­¤æ—¶ï¼Œæˆ‘ä»¬å·²å‡†å¤‡å¥½ç”ŸæˆåŸºå‡†æ¨¡å‹ã€‚

```py
from sklearn.neighbors import NearestNeighbors

nns = NearestNeighbors()
nns.fit(data_scaled)
nns_results_model_0 = nns.kneighbors(data_scaled)[1]

nns_results_model_0
```

![](../Images/c1e08da417e6a83c4a372f5b5c1436df.png)

æ¨¡å‹è¾“å‡ºã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªç´¢å¼•åˆ—è¡¨ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªç´¢å¼•æ˜¯ç›®æ ‡é…’åº—ï¼Œå…¶ä»–ç´¢å¼•è¡¨ç¤ºæœ€é è¿‘çš„é…’åº—ã€‚

ä¸ºäº†éªŒè¯æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç›´è§‚æ£€æŸ¥ç»“æœã€‚æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥æ”¶ç´¢å¼•åˆ—è¡¨å¹¶è§£ç å€¼ã€‚

```py
def clean_results(nns_results: np.ndarray,
                  encoders: dict,
                  data: pd.DataFrame):
  """
  Returns a dataframe with a list of labels for each Nearest Neighobor group
  """
  result = pd.DataFrame()

  # 1\. Get a list of Nearest Hotels based on our model
  for i in range(len(nns_results)):

    results = {} #empty dictionary to append each rows values

    # Each row in nns_results contains the indexs of the selected nearest neighbors
    # We use those index to get the Hotel names in our main data set
    results["Hotels"] = list(data.iloc[nns_results[i]].index)

    # 2\. Get the values for each features for all Nearest Neighbors groups
    for item in  data_clean.columns:
      results[item] = list(data.iloc[nns_results[i]][item])

    # 3\. Create a row for each Nearest Neighbor group and append to main DataFrame
    df = pd.DataFrame([results])
    result = pd.concat([result,df],axis=0)

  # 4\. Decode the labels to the encoded columns
  for key, val in encoders.items():
    result[key] = result[key].apply(lambda x : list(val.inverse_transform(x)))

  result.reset_index(drop=True,inplace=True) # Reset the index for clarity
  return result

results_model_0 = clean_results(nns_results=nns_results_model_0,
                                encoders=encoders,
                                data=data_clean)
results_model_0.head()
```

![](../Images/4d295e60f17cf12d9ffb4270c94a3250.png)

åˆæ­¥åŸºå‡†åˆ†ç»„ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

å› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯æ— ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œæ‰€ä»¥æ²¡æœ‰å¹¿æ³›ä½¿ç”¨çš„å‡†ç¡®åº¦è¡¡é‡æ ‡å‡†ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é¢†åŸŸçŸ¥è¯†æ¥éªŒè¯æˆ‘ä»¬çš„åˆ†ç»„ã€‚

é€šè¿‡ç›´è§‚æ£€æŸ¥è¿™äº›åˆ†ç»„ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€äº›åŸºå‡†åˆ†ç»„åŒ…å«äº†ç»æµå‹é…’åº—å’Œå¥¢åå‹é…’åº—çš„æ··åˆï¼Œè¿™åœ¨å•†ä¸šä¸Šæ²¡æœ‰æ„ä¹‰ï¼Œå› ä¸ºé…’åº—çš„éœ€æ±‚æœ¬è´¨ä¸Šæ˜¯ä¸åŒçš„ã€‚

**æˆ‘ä»¬å¯ä»¥æ»šåŠ¨æŸ¥çœ‹æ•°æ®å¹¶æ³¨æ„åˆ°ä¸€äº›å·®å¼‚ï¼Œä½†æˆ‘ä»¬èƒ½å¦æå‡ºè‡ªå·±çš„å‡†ç¡®åº¦è¡¡é‡æ ‡å‡†ï¼Ÿ**

æˆ‘ä»¬å¸Œæœ›åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥è¡¡é‡æ¨èçš„åŸºå‡†é›†åœ¨æ¯ä¸ªç‰¹å¾ä¸Šçš„ä¸€è‡´æ€§ã€‚ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡è®¡ç®—æ¯ä¸ªé›†çš„æ¯ä¸ªç‰¹å¾çš„æ–¹å·®æ¥å®ç°ã€‚å¯¹äºæ¯ä¸ªèšç±»ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªç‰¹å¾æ–¹å·®çš„å¹³å‡å€¼ï¼Œç„¶åå¯ä»¥å°†æ¯ä¸ªé…’åº—èšç±»çš„æ–¹å·®å–å¹³å‡ï¼Œå¾—åˆ°ä¸€ä¸ªæ€»çš„æ¨¡å‹å¾—åˆ†ã€‚

æ ¹æ®æˆ‘ä»¬çš„é¢†åŸŸçŸ¥è¯†ï¼Œæˆ‘ä»¬çŸ¥é“ï¼Œä¸ºäº†è®¾ç½®ä¸€ä¸ªå¯æ¯”è¾ƒçš„åŸºå‡†é›†ï¼Œæˆ‘ä»¬éœ€è¦ä¼˜å…ˆè€ƒè™‘ç›¸åŒå“ç‰Œã€å¯èƒ½ç›¸åŒå¸‚åœºå’Œç›¸åŒå›½å®¶çš„é…’åº—ï¼Œå¦‚æœä½¿ç”¨ä¸åŒçš„å¸‚åœºæˆ–å›½å®¶ï¼Œé‚£ä¹ˆå¸‚åœºå±‚çº§åº”è¯¥ç›¸åŒã€‚

é‰´äºæ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„åº¦é‡èƒ½å¤Ÿå¯¹è¿™äº›ç‰¹å¾çš„æ–¹å·®ç»™äºˆæ›´é«˜çš„æƒ©ç½šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åŠ æƒå¹³å‡æ¥è®¡ç®—æ¯ä¸ªåŸºå‡†é›†çš„æ–¹å·®ã€‚æˆ‘ä»¬è¿˜å°†åˆ†åˆ«æ‰“å°å…³é”®ç‰¹å¾å’Œæ¬¡è¦ç‰¹å¾çš„æ–¹å·®ã€‚

æ€»ç»“ä¸€ä¸‹ï¼Œåˆ›å»ºå‡†ç¡®åº¦é‡æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ï¼š

1.  **è®¡ç®—åˆ†ç±»å˜é‡çš„æ–¹å·®**ï¼šä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯ä½¿ç”¨â€œåŸºäºç†µçš„â€åº¦é‡ï¼Œå…¶ä¸­ç±»åˆ«çš„å¤šæ ·æ€§è¶Šé«˜ï¼Œç†µï¼ˆæ–¹å·®ï¼‰è¶Šé«˜ã€‚

1.  **è®¡ç®—æ•°å€¼å˜é‡çš„æ–¹å·®**ï¼šæˆ‘ä»¬å¯ä»¥è®¡ç®—æ ‡å‡†å·®æˆ–èŒƒå›´ï¼ˆæœ€å¤§å€¼ä¸æœ€å°å€¼ä¹‹é—´çš„å·®å¼‚ï¼‰ã€‚è¿™è¡¡é‡äº†æ¯ä¸ªèšç±»å†…æ•°å€¼æ•°æ®çš„åˆ†å¸ƒæƒ…å†µã€‚

1.  **è§„èŒƒåŒ–æ•°æ®**ï¼šåœ¨åº”ç”¨æƒé‡ä¹‹å‰ï¼Œå…ˆè§„èŒƒåŒ–æ¯ä¸ªç±»åˆ«çš„æ–¹å·®å¾—åˆ†ï¼Œä»¥ç¡®ä¿æ²¡æœ‰å•ä¸€ç‰¹å¾å› å°ºåº¦å·®å¼‚è€Œä¸»å¯¼åŠ æƒå¹³å‡æ•°ã€‚

1.  **ä¸ºä¸åŒçš„åº¦é‡åº”ç”¨æƒé‡**ï¼šæ ¹æ®æ–¹å·®ç±»å‹å¯¹å…¶åœ¨èšç±»é€»è¾‘ä¸­çš„é‡è¦æ€§è¿›è¡ŒåŠ æƒã€‚

1.  **è®¡ç®—åŠ æƒå¹³å‡æ•°**ï¼šè®¡ç®—æ¯ä¸ªèšç±»çš„è¿™äº›æ–¹å·®å¾—åˆ†çš„åŠ æƒå¹³å‡æ•°ã€‚

1.  **èšåˆèšç±»çš„å¾—åˆ†**ï¼šæ€»å¾—åˆ†æ˜¯æ‰€æœ‰èšç±»æˆ–è¡Œçš„åŠ æƒæ–¹å·®å¾—åˆ†çš„å¹³å‡å€¼ã€‚è¾ƒä½çš„å¹³å‡å¾—åˆ†è¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹æœ‰æ•ˆåœ°å°†ç›¸ä¼¼çš„é…’åº—åˆ†ç»„åœ¨ä¸€èµ·ï¼Œæœ€å°åŒ–äº†èšç±»å†…çš„æ–¹å·®ã€‚

```py
from scipy.stats import entropy
from sklearn.preprocessing import MinMaxScaler
from collections import Counter

def categorical_variance(data):
    """
    Calculate entropy for a categorical variable from a list.
    A higher entropy value indicates datas with diverse classes.
    A lower entropy value indicates a more homogeneous subset of data.
    """
    # Count frequency of each unique value
    value_counts = Counter(data)
    total_count = sum(value_counts.values())
    probabilities = [count / total_count for count in value_counts.values()]
    return entropy(probabilities)

#set scoring weights giving higher weights to the most important features
scoring_weights = {"BRAND": 0.3,
           "Room_count": 0.025,
           "Market": 0.25,
           "Country": 0.15,
           "Market Tier": 0.15,
           "HCLASS": 0.05,
           "Demand": 0.025,
           "Price range": 0.025,
           "distance_to_airport": 0.025}

def calculate_weighted_variance(df, weights):
    """
    Calculate the weighted variance score for clusters in the dataset
    """
    # Initialize a DataFrame to store the variances
    variance_df = pd.DataFrame()

    # 1\. Calculate variances for numerical features
    numerical_features = ['Room_count', 'Demand', 'Price range', 'distance_to_airport']
    for feature in numerical_features:
        variance_df[f'{feature}'] = df[feature].apply(np.var)

    # 2\. Calculate entropy for categorical features
    categorical_features = ['BRAND', 'Market','Country','Market Tier','HCLASS']
    for feature in categorical_features:
        variance_df[f'{feature}'] = df[feature].apply(categorical_variance)

    # 3\. Normalize the variance and entropy values
    scaler = MinMaxScaler()
    normalized_variances = pd.DataFrame(scaler.fit_transform(variance_df),
                                        columns=variance_df.columns,
                                        index=variance_df.index)

    # 4\. Compute weighted average

    cat_weights = {f'{feature}': weights[f'{feature}'] for feature in categorical_features}
    num_weights = {f'{feature}': weights[f'{feature}'] for feature in numerical_features}

    cat_weighted_scores = normalized_variances[categorical_features].mul(cat_weights)
    df['cat_weighted_variance_score'] = cat_weighted_scores.sum(axis=1)

    num_weighted_scores = normalized_variances[numerical_features].mul(num_weights)
    df['num_weighted_variance_score'] = num_weighted_scores.sum(axis=1)

    return df['cat_weighted_variance_score'].mean(), df['num_weighted_variance_score'].mean()
```

ä¸ºäº†ä¿æŒä»£ç ç®€æ´å¹¶è·Ÿè¸ªæˆ‘ä»¬çš„å®éªŒï¼Œè®©æˆ‘ä»¬è¿˜å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥å­˜å‚¨æˆ‘ä»¬çš„å®éªŒç»“æœã€‚

```py
# define a function to store the results of our experiments
def model_score(data: pd.DataFrame,
                weights: dict = scoring_weights,
                model_name: str ="model_0"):
  cat_score,num_score = calculate_weighted_variance(data,weights)
  results ={"Model": model_name,
            "Primary features score": cat_score,
            "Secondary features score": num_score}
  return results

model_0_score= model_score(results_model_0,scoring_weights)
model_0_score
```

![](../Images/07f338459cb382b9bf734562b11c454b.png)

åŸºå‡†æ¨¡å‹ç»“æœã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº†ä¸€ä¸ªåŸºå‡†ï¼Œæ¥ä¸‹æ¥çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦èƒ½æ”¹è¿›æˆ‘ä»¬çš„æ¨¡å‹ã€‚

## é€šè¿‡å®éªŒæ”¹è¿›æˆ‘ä»¬çš„æ¨¡å‹

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå½“æˆ‘ä»¬è¿è¡Œè¿™æ®µä»£ç æ—¶ï¼Œæˆ‘ä»¬å¹¶ä¸éœ€è¦çŸ¥é“å…¶èƒŒåçš„å…·ä½“å®ç°ï¼š

```py
nns = NearestNeighbors()
nns.fit(data_scaled)
nns_results_model_0 = nns.kneighbors(data_scaled)[1]
```

ä¸ºäº†æ”¹è¿›æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£æ¨¡å‹å‚æ•°ï¼Œå¹¶äº†è§£å¦‚ä½•ä¸å®ƒä»¬äº¤äº’ä»¥è·å¾—æ›´å¥½çš„åŸºå‡†é›†ã€‚

è®©æˆ‘ä»¬ä»æŸ¥çœ‹ Scikit Learn æ–‡æ¡£å’Œæºä»£ç å¼€å§‹ï¼š

```py
# the below is taken directly from scikit learn source

from sklearn.neighbors._base import KNeighborsMixin, NeighborsBase, RadiusNeighborsMixin

class NearestNeighbors_(KNeighborsMixin, RadiusNeighborsMixin, NeighborsBase):
    """Unsupervised learner for implementing neighbor searches.
    Parameters
    ----------
    n_neighbors : int, default=5
        Number of neighbors to use by default for :meth:`kneighbors` queries.

    radius : float, default=1.0
        Range of parameter space to use by default for :meth:`radius_neighbors`
        queries.

    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'
        Algorithm used to compute the nearest neighbors:

        - 'ball_tree' will use :class:`BallTree`
        - 'kd_tree' will use :class:`KDTree`
        - 'brute' will use a brute-force search.
        - 'auto' will attempt to decide the most appropriate algorithm
          based on the values passed to :meth:`fit` method.

        Note: fitting on sparse input will override the setting of
        this parameter, using brute force.

    leaf_size : int, default=30
        Leaf size passed to BallTree or KDTree.  This can affect the
        speed of the construction and query, as well as the memory
        required to store the tree.  The optimal value depends on the
        nature of the problem.

    metric : str or callable, default='minkowski'
        Metric to use for distance computation. Default is "minkowski", which
        results in the standard Euclidean distance when p = 2\. See the
        documentation of `scipy.spatial.distance
        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and
        the metrics listed in
        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric
        values.

    p : float (positive), default=2
        Parameter for the Minkowski metric from
        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
        equivalent to using manhattan_distance (l1), and euclidean_distance
        (l2) for p = 2\. For arbitrary p, minkowski_distance (l_p) is used.

    metric_params : dict, default=None
        Additional keyword arguments for the metric function.
  """

    def __init__(
        self,
        *,
        n_neighbors=5,
        radius=1.0,
        algorithm="auto",
        leaf_size=30,
        metric="minkowski",
        p=2,
        metric_params=None,
        n_jobs=None,
    ):
        super().__init__(
            n_neighbors=n_neighbors,
            radius=radius,
            algorithm=algorithm,
            leaf_size=leaf_size,
            metric=metric,
            p=p,
            metric_params=metric_params,
            n_jobs=n_jobs,
        )
```

è¿™é‡Œæœ‰å¾ˆå¤šäº‹æƒ…åœ¨å‘ç”Ÿã€‚

`Nearestneighbor` ç±»ç»§æ‰¿è‡ª `NeighborsBase`ï¼Œåè€…æ˜¯æœ€è¿‘é‚»ä¼°è®¡å™¨çš„åŸºç±»ã€‚æ­¤ç±»å¤„ç†æœ€è¿‘é‚»æœç´¢æ‰€éœ€çš„å¸¸è§åŠŸèƒ½ï¼Œå¦‚ï¼š

+   n_neighborsï¼ˆä½¿ç”¨çš„é‚»å±…æ•°é‡ï¼‰

+   radiusï¼ˆåŸºäºåŠå¾„çš„é‚»å±…æœç´¢çš„åŠå¾„ï¼‰

+   algorithmï¼ˆç”¨äºè®¡ç®—æœ€è¿‘é‚»çš„ç®—æ³•ï¼Œå¦‚ â€˜ball_treeâ€™ï¼Œâ€˜kd_treeâ€™ æˆ– â€˜bruteâ€™ï¼‰

+   metricï¼ˆç”¨äºè®¡ç®—çš„è·ç¦»åº¦é‡ï¼‰

+   metric_paramsï¼ˆè·ç¦»å‡½æ•°çš„é¢å¤–å…³é”®å­—å‚æ•°ï¼‰

`Nearestneighbor` ç±»è¿˜ç»§æ‰¿è‡ª `KNeighborsMixin` å’Œ `RadiusNeighborsMixin` ç±»ã€‚è¿™äº› Mixin ç±»ä¸º `Nearestneighbor` æ·»åŠ äº†ç‰¹å®šçš„é‚»å±…æœç´¢åŠŸèƒ½ã€‚

+   `KNeighborsMixin` æä¾›äº†ä¸€ä¸ªåŠŸèƒ½ï¼Œç”¨äºæŸ¥æ‰¾ç¦»æŸä¸€ç‚¹æœ€è¿‘çš„å›ºå®šæ•°é‡ k ä¸ªé‚»å±…ã€‚å®ƒé€šè¿‡è®¡ç®—é‚»å±…ä¹‹é—´çš„è·ç¦»åŠå…¶ç´¢å¼•ï¼Œå¹¶åŸºäºæ¯ä¸ªç‚¹çš„ k æœ€è¿‘é‚»æ¥æ„å»ºä¸€ä¸ªç‚¹ä¹‹é—´çš„è¿æ¥å›¾ã€‚

+   `RadiusNeighborsMixin` åŸºäºåŠå¾„é‚»å±…ç®—æ³•ï¼Œå¯»æ‰¾ç»™å®šåŠå¾„å†…çš„æ‰€æœ‰é‚»å±…ã€‚è¿™ç§æ–¹æ³•åœ¨å…³æ³¨æ•æ‰æ‰€æœ‰ä½äºæœ‰æ„ä¹‰è·ç¦»é˜ˆå€¼å†…çš„ç‚¹æ—¶éå¸¸æœ‰ç”¨ï¼Œè€Œä¸æ˜¯å›ºå®šæ•°é‡çš„ç‚¹ã€‚

æ ¹æ®æˆ‘ä»¬çš„åœºæ™¯ï¼ŒKNeighborsMixin æä¾›äº†æˆ‘ä»¬æ‰€éœ€çš„åŠŸèƒ½ã€‚

åœ¨æˆ‘ä»¬èƒ½æ”¹è¿›æ¨¡å‹ä¹‹å‰ï¼Œéœ€è¦ç†è§£ä¸€ä¸ªå…³é”®å‚æ•°ï¼Œé‚£å°±æ˜¯è·ç¦»åº¦é‡ã€‚

# ç®€çŸ­ä»‹ç»è·ç¦»

æ–‡æ¡£æåˆ°ï¼ŒNearestNeighbor ç®—æ³•é»˜è®¤ä½¿ç”¨ â€œMinkowskiâ€ è·ç¦»ï¼Œå¹¶ä¸ºæˆ‘ä»¬æä¾›äº† SciPy API çš„å‚è€ƒã€‚

åœ¨ `scipy.spatial.distance` ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°â€œMinkowskiâ€ è·ç¦»çš„ä¸¤ç§æ•°å­¦è¡¨ç¤ºï¼š

> *âˆ¥uâˆ’vâˆ¥ pâ€‹=( i âˆ‘â€‹âˆ£u iâ€‹âˆ’v iâ€‹âˆ£ p ) 1/p*

è¿™ä¸ªå…¬å¼è®¡ç®—çš„æ˜¯æ‰€æœ‰å…ƒç´ å·®å¼‚çš„å¹‚å’Œçš„ p æ¬¡æ ¹ã€‚

â€œMinkowskiâ€ è·ç¦»çš„ç¬¬äºŒç§æ•°å­¦è¡¨ç¤ºæ˜¯ï¼š

> *âˆ¥uâˆ’vâˆ¥ pâ€‹=( i âˆ‘â€‹w iâ€‹(âˆ£u iâ€‹âˆ’v iâ€‹âˆ£ p )) 1/p*

è¿™ä¸ªå…¬å¼ä¸ç¬¬ä¸€ä¸ªç±»ä¼¼ï¼Œä½†å®ƒå¼•å…¥äº†æƒé‡ `wi`ï¼Œä½¿å¾—æŸäº›ç»´åº¦çš„å·®å¼‚è¢«å¼ºè°ƒæˆ–å‡å¼±ã€‚åœ¨æŸäº›ç‰¹å¾æ¯”å…¶ä»–ç‰¹å¾æ›´ä¸ºé‡è¦çš„æƒ…å†µä¸‹ï¼Œè¿™éå¸¸æœ‰ç”¨ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¯¥è®¾ç½®ä¸º Noneï¼Œè¿™æ„å‘³ç€æ‰€æœ‰ç‰¹å¾çš„æƒé‡å‡ä¸º 1.0ã€‚

**è¿™æ˜¯æ”¹è¿›æˆ‘ä»¬æ¨¡å‹çš„ä¸€ä¸ªå¾ˆå¥½çš„é€‰é¡¹ï¼Œå› ä¸ºå®ƒå…è®¸æˆ‘ä»¬å°†é¢†åŸŸçŸ¥è¯†ä¼ é€’ç»™æ¨¡å‹ï¼Œå¹¶å¼ºè°ƒå¯¹ç”¨æˆ·æœ€ç›¸å…³çš„ç›¸ä¼¼æ€§ã€‚**

å¦‚æœæˆ‘ä»¬æŸ¥çœ‹å…¬å¼ï¼Œä¼šçœ‹åˆ°å‚æ•° `p`ã€‚è¯¥å‚æ•°å½±å“ç®—æ³•è®¡ç®—è·ç¦»æ—¶é‡‡å–çš„â€œè·¯å¾„â€ã€‚**é»˜è®¤æƒ…å†µä¸‹ï¼Œp=2ï¼Œè¿™ä»£è¡¨æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚**

ä½ å¯ä»¥å°†æ¬§å‡ é‡Œå¾—è·ç¦»ç†è§£ä¸ºé€šè¿‡åœ¨ä¸¤ä¸ªç‚¹ä¹‹é—´ç”»ä¸€æ¡ç›´çº¿æ¥è®¡ç®—è·ç¦»ã€‚è¿™é€šå¸¸æ˜¯æœ€çŸ­çš„è·ç¦»ï¼Œä½†åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œè¿™å¹¶ä¸æ€»æ˜¯è®¡ç®—è·ç¦»çš„æœ€ç†æƒ³æ–¹å¼ã€‚å¦‚éœ€äº†è§£ä¸ºä»€ä¹ˆæ˜¯è¿™æ ·ï¼Œå¯ä»¥å‚è€ƒè¿™ç¯‡å¾ˆæ£’çš„åœ¨çº¿è®ºæ–‡ï¼š[https://bib.dbvis.de/uploadedFiles/155.pdf](https://www.google.com/url?q=https%3A%2F%2Fbib.dbvis.de%2FuploadedFiles%2F155.pdf)

**å¦ä¸€ä¸ªå¸¸è§çš„ p å€¼æ˜¯ 1\. è¿™ä»£è¡¨æ›¼å“ˆé¡¿è·ç¦»ã€‚** å¯ä»¥å°†å…¶ç†è§£ä¸ºåœ¨ç±»ä¼¼ç½‘æ ¼çš„è·¯å¾„ä¸Šæµ‹é‡ä¸¤ä¸ªç‚¹ä¹‹é—´çš„è·ç¦»ã€‚

**å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘ä»¬å°† p å¢åŠ åˆ°æ— ç©·å¤§ï¼Œæˆ‘ä»¬å°†å¾—åˆ°åˆ‡æ¯”é›ªå¤«è·ç¦»ï¼Œå®ƒè¢«å®šä¹‰ä¸ºå‘é‡ä¸­ä»»ä½•å¯¹åº”å…ƒç´ ä¹‹é—´çš„æœ€å¤§ç»å¯¹å·®å¼‚**ã€‚å®ƒæœ¬è´¨ä¸Šè¡¡é‡çš„æ˜¯æœ€åæƒ…å†µä¸‹çš„å·®å¼‚ï¼Œå› æ­¤åœ¨ä½ å¸Œæœ›ç¡®ä¿æ²¡æœ‰å•ä¸€ç‰¹å¾å˜åŒ–è¿‡å¤§çš„æƒ…å†µä¸‹éå¸¸æœ‰ç”¨ã€‚

é€šè¿‡é˜…è¯»å¹¶ç†Ÿæ‚‰æ–‡æ¡£ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€äº›å¯ä»¥æ”¹å–„æ¨¡å‹çš„å¯èƒ½é€‰é¡¹ã€‚

# å®éªŒ 1ï¼šåŸºå‡†æ¨¡å‹ï¼Œn_neighbors = 4

é»˜è®¤æƒ…å†µä¸‹ï¼Œn_neighbors ä¸º 5ï¼Œä½†å¯¹äºæˆ‘ä»¬çš„åŸºå‡†é›†ï¼Œæˆ‘ä»¬å¸Œæœ›å°†æ¯ä¸ªé…’åº—ä¸ 3 ä¸ªæœ€ç›¸ä¼¼çš„é…’åº—è¿›è¡Œæ¯”è¾ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°† n_neighbors è®¾ç½®ä¸º 4ï¼ˆç›®æ ‡é…’åº— + 3 ä¸ªåŒç±»é…’åº—ï¼‰ã€‚

```py
nns_1= NearestNeighbors(n_neighbors=4)
nns_1.fit(data_scaled)
nns_1_results_model_1 = nns_1.kneighbors(data_scaled)[1]
results_model_1 = clean_results(nns_results=nns_1_results_model_1,
                                encoders=encoders,
                                data=data_clean)
model_1_score= model_score(results_model_1,scoring_weights,model_name="baseline_k_4")
model_1_score
```

![](../Images/573845ae760e04d4c0d1b3a414bd3e40.png)

æˆ‘ä»¬çš„ä¸»è¦ç‰¹å¾æœ‰æ‰€æ”¹å–„ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

# å®éªŒ 2ï¼šæ·»åŠ æƒé‡

æ ¹æ®æ–‡æ¡£ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¼ é€’æƒé‡æ¥å¼ºè°ƒæŸäº›ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚æ ¹æ®æˆ‘ä»¬çš„é¢†åŸŸçŸ¥è¯†ï¼Œæˆ‘ä»¬å·²ç»ç¡®å®šäº†éœ€è¦å¼ºè°ƒçš„ç‰¹å¾ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯å“ç‰Œã€å¸‚åœºã€å›½å®¶å’Œå¸‚åœºå±‚çº§ã€‚

```py
# set up weights for distance calculation
weights_dict =  {"BRAND": 5,
           "Room_count": 2,
           "Market": 4,
           "Country": 3,
           "Market Tier": 3,
           "HCLASS": 1.5,
           "Demand": 1,
           "Price range": 1,
           "distance_to_airport": 1}
# Transform the wieghts dictionnary into a list by keeping the scaled data column order
weights = [ weights_dict[idx] for idx in list(scaler.get_feature_names_out())]

nns_2= NearestNeighbors(n_neighbors=4,metric_params={ 'w': weights})
nns_2.fit(data_scaled)
nns_2_results_model_2 = nns_2.kneighbors(data_scaled)[1]
results_model_2 = clean_results(nns_results=nns_2_results_model_2,
                                encoders=encoders,
                                data=data_clean)
model_2_score= model_score(results_model_2,scoring_weights,model_name="baseline_with_weights")
model_2_score
```

![](../Images/a0c93e3306379ea2efef56846f0f095f.png)

ä¸»è¦ç‰¹å¾å¾—åˆ†æŒç»­æé«˜ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

é€šè¿‡æƒé‡å°†é¢†åŸŸçŸ¥è¯†ä¼ é€’ç»™æ¨¡å‹æ˜¾è‘—æé«˜äº†å¾—åˆ†ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬æµ‹è¯•è·ç¦»åº¦é‡çš„å½±å“ã€‚

# å®éªŒ 3ï¼šä½¿ç”¨æ›¼å“ˆé¡¿è·ç¦»

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚æœæ”¹ç”¨æ›¼å“ˆé¡¿è·ç¦»ä¼šå‘ç”Ÿä»€ä¹ˆã€‚

```py
nns_3= NearestNeighbors(n_neighbors=4,p=1,metric_params={ 'w': weights})
nns_3.fit(data_scaled)
nns_3_results_model_3 = nns_3.kneighbors(data_scaled)[1]
results_model_3 = clean_results(nns_results=nns_3_results_model_3,
                                encoders=encoders,
                                data=data_clean)
model_3_score= model_score(results_model_3,scoring_weights,model_name="Manhattan_with_weights")
model_3_score
```

![](../Images/2139801c701f67fa1332a9ac59be5543.png)

ä¸»è¦å¾—åˆ†æ˜¾è‘—ä¸‹é™ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

# å®éªŒ 4ï¼šä½¿ç”¨åˆ‡æ¯”é›ªå¤«è·ç¦»

å°† p é™åˆ° 1 åï¼Œå–å¾—äº†ä¸€äº›è‰¯å¥½çš„æ”¹è¿›ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å½“ p æ¥è¿‘æ— ç©·å¤§æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€‚

è¦ä½¿ç”¨åˆ‡æ¯”é›ªå¤«è·ç¦»ï¼Œæˆ‘ä»¬å°†æŠŠåº¦é‡å‚æ•°æ›´æ”¹ä¸º `Chebyshev`ã€‚é»˜è®¤çš„ sklearn åˆ‡æ¯”é›ªå¤«åº¦é‡æ²¡æœ‰æƒé‡å‚æ•°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰çš„ `weighted_chebyshev` åº¦é‡ã€‚

```py
#  Define the custom weighted Chebyshev distance function
def weighted_chebyshev(u, v, w):
    """Calculate the weighted Chebyshev distance between two points."""
    return np.max(w * np.abs(u - v))

nns_4 = NearestNeighbors(n_neighbors=4,metric=weighted_chebyshev,metric_params={ 'w': weights})
nns_4.fit(data_scaled)
nns_4_results_model_4 = nns_4.kneighbors(data_scaled)[1]
results_model_4 = clean_results(nns_results=nns_4_results_model_4,
                                encoders=encoders,
                                data=data_clean)
model_4_score= model_score(results_model_4,scoring_weights,model_name="Chebyshev_with_weights")
model_4_score
```

![](../Images/17c6cda0b99cf5ebd7ac39902c0cf08f.png)

æ¯”åŸºå‡†æ¨¡å‹è¦å¥½ï¼Œä½†æ¯”ä¹‹å‰çš„å®éªŒé«˜ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬æˆåŠŸåœ°é™ä½äº†ä¸»è¦ç‰¹å¾çš„æ–¹å·®å¾—åˆ†ã€‚

è®©æˆ‘ä»¬å¯è§†åŒ–ç»“æœã€‚

```py
results_df = pd.DataFrame([model_0_score,model_1_score,model_2_score,model_3_score,model_4_score]).set_index("Model")
results_df.plot(kind='barh')
```

![](../Images/e5deb0e34f1ce91bf97eb1f80d7ec4cd.png)

å®éªŒç»“æœã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

æ ¹æ®æˆ‘ä»¬çš„éœ€æ±‚ï¼Œä½¿ç”¨å¸¦æƒé‡çš„æ›¼å“ˆé¡¿è·ç¦»ä¼¼ä¹èƒ½æä¾›æœ€å‡†ç¡®çš„åŸºå‡†é›†ã€‚

åœ¨å®ç°åŸºå‡†é›†ä¹‹å‰çš„æœ€åä¸€æ­¥æ˜¯æ£€æŸ¥å¾—åˆ†æœ€é«˜çš„ä¸»è¦ç‰¹å¾é›†ï¼Œå¹¶ç¡®å®šéœ€è¦é‡‡å–å“ªäº›æ­¥éª¤ã€‚

```py
# Histogram of Primary features score
results_model_3["cat_weighted_variance_score"].plot(kind="hist")
```

![](../Images/9fe38e3b7fcdc89974b7cebd25e6e230.png)

å¾—åˆ†åˆ†å¸ƒã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

```py
exceptions = results_model_3[results_model_3["cat_weighted_variance_score"]>=0.4]

print(f" There are {exceptions.shape[0]} benchmark sets with significant variance across the primary features")
```

![](../Images/1c862f02f1fa83283cbd36ef588e4292.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›

è¿™ 18 ä¸ªæ¡ˆä¾‹éœ€è¦å¤å®¡ï¼Œä»¥ç¡®ä¿åŸºå‡†é›†æ˜¯ç›¸å…³çš„ã€‚

æ­£å¦‚ä½ æ‰€è§ï¼Œå‡­å€Ÿå‡ è¡Œä»£ç å’Œå¯¹æœ€è¿‘é‚»æœç´¢çš„ä¸€äº›ç†è§£ï¼Œæˆ‘ä»¬æˆåŠŸåœ°è®¾ç½®äº†å†…éƒ¨åŸºå‡†é›†ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥åˆ†å‘è¿™äº›åŸºå‡†é›†ï¼Œå¹¶å¼€å§‹è¡¡é‡é…’åº—çš„KPIä¸å…¶åŸºå‡†é›†çš„å¯¹æ¯”ã€‚

ä½ ä¸ä¸€å®šæ€»æ˜¯éœ€è¦ä¸“æ³¨äºæœ€å‰æ²¿çš„æœºå™¨å­¦ä¹ æ–¹æ³•æ¥æä¾›ä»·å€¼ã€‚å¾ˆå¤šæ—¶å€™ï¼Œç®€å•çš„æœºå™¨å­¦ä¹ ä¹Ÿèƒ½å¸¦æ¥å·¨å¤§ä»·å€¼ã€‚

åœ¨ä½ çš„ä¸šåŠ¡ä¸­ï¼Œæœ‰å“ªäº›å®¹æ˜“é€šè¿‡æœºå™¨å­¦ä¹ è§£å†³çš„ä½æŒ‚æœï¼Ÿ

# å‚è€ƒæ–‡çŒ®

ä¸–ç•Œé“¶è¡Œ. â€œä¸–ç•Œå‘å±•æŒ‡æ ‡.â€ è®¿é—®æ—¥æœŸï¼š2024å¹´6æœˆ11æ—¥ï¼Œæ¥æºï¼š[https://datacatalog.worldbank.org/search/dataset/0038117](https://datacatalog.worldbank.org/search/dataset/0038117)

Aggarwal, C. C., Hinneburg, A., & Keim, D. A. (n.d.). é«˜ç»´ç©ºé—´ä¸­è·ç¦»åº¦é‡çš„æƒŠäººè¡Œä¸º. IBM T. J. Watson ç ”ç©¶ä¸­å¿ƒå’Œå“ˆé›·å¤§å­¦è®¡ç®—æœºç§‘å­¦ç ”ç©¶æ‰€. æ¥æºï¼š[https://bib.dbvis.de/uploadedFiles/155.pdf](https://bib.dbvis.de/uploadedFiles/155.pdf)

SciPy v1.10.1 æ‰‹å†Œ. `scipy.spatial.distance.minkowski`. è®¿é—®æ—¥æœŸï¼š2024å¹´6æœˆ11æ—¥ï¼Œæ¥æºï¼š[https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.minkowski.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.minkowski.html)

GeeksforGeeks. å“ˆå¼—è¾›å…¬å¼è®¡ç®—çƒé¢ä¸Šä¸¤ç‚¹ä¹‹é—´çš„è·ç¦». è®¿é—®æ—¥æœŸï¼š2024å¹´6æœˆ11æ—¥ï¼Œæ¥æºï¼š[https://www.geeksforgeeks.org/haversine-formula-to-find-distance-between-two-points-on-a-sphere/](https://www.geeksforgeeks.org/haversine-formula-to-find-distance-between-two-points-on-a-sphere/)

scikit-learn. é‚»å±…æ¨¡å—. è®¿é—®æ—¥æœŸï¼š2024å¹´6æœˆ11æ—¥ï¼Œæ¥æºï¼š[https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)
