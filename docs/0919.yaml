- en: Building a RAG chain using LangChain Expression Language (LCEL)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-a-rag-chain-using-langchain-expression-language-lcel-3688260cad05?source=collection_archive---------1-----------------------#2024-04-11](https://towardsdatascience.com/building-a-rag-chain-using-langchain-expression-language-lcel-3688260cad05?source=collection_archive---------1-----------------------#2024-04-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learning the building blocks of LCEL to develop increasingly complex RAG chains
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@RSK2327?source=post_page---byline--3688260cad05--------------------------------)[![Roshan
    Santhosh](../Images/2509f38bf7d5a40c453fa54575293f06.png)](https://medium.com/@RSK2327?source=post_page---byline--3688260cad05--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3688260cad05--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3688260cad05--------------------------------)
    [Roshan Santhosh](https://medium.com/@RSK2327?source=post_page---byline--3688260cad05--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3688260cad05--------------------------------)
    ·7 min read·Apr 11, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In this post, I will be going over the **implementation of a Self-evaluation
    RAG pipeline for question-answering using LangChain Expression Language (LCEL)**.
    The focus of this post will be on the use of LCEL for building pipelines and not
    so much on the actual RAG and self evaluation principles used, which are kept
    simple for ease of understanding.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c5464d056ec85924da8325acf133c35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I will be covering the following topics :'
  prefs: []
  type: TYPE_NORMAL
- en: Basic initialization steps
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Development of different variations of the RAG pipeline of increasing complexity
    using LCEL
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Methods for extracting intermediate variables from a LCEL-scripted pipeline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reasons for using LCEL
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we jump into the development of the RAG chain, there are some basic
    setup steps that we need to perform to initialize this setup. These include :'
  prefs: []
  type: TYPE_NORMAL
- en: Data Ingestion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The data ingestion consists of two key steps :'
  prefs: []
  type: TYPE_NORMAL
- en: Reading the text from the pdf
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Splitting up the pdf text into chunks for inputting to the vector database
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prompt Templates**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be using different prompts for the question-answering and self-evaluation
    tasks. We will be having 3 different prompt templates :'
  prefs: []
  type: TYPE_NORMAL
- en: 'qa_prompt : Basic prompt for the question-answering task'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'qa_eval_prompt : Prompt for evaluator model that takes as input question-answer
    pair'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'qa_eval_prompt_with_context : Similar to above prompt but additionally includes
    the context as well for the evaluation'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Database Initialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We initialize a simple vector database using FAISS and Open AI embeddings. For
    retrieval, we set k as 3 (return top 3 chunks for a given query)
  prefs: []
  type: TYPE_NORMAL
- en: RAG Development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simple QA RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start off with an example of a basic RAG chain that carries out the following
    steps :'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieves the relevant chunks (splits of pdf text) from the vector database
    based on the user’s question and merges them into a single string
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Passes the retrieved context text along with question to the prompt template
    to generate the prompt
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Input generated prompt to LLM to generate final answer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Using** [**LangChain Expression Language(LCEL)**](https://python.langchain.com/docs/expression_language/)**,
    this RAG would be implemented as such:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The above code primarily follows the [pipe architecture](/write-clean-python-code-using-pipes-1239a0f3abf5)
    where the output from the preceding element is used as the input for the next
    element. The below diagram showcases the flow of data. Starting from the user’s
    input, it passes first through the RunnableParallel block, then through the qa_prompt
    to generate the prompt. This prompt is then sent to the LLM to generate the final
    output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a548f9e62b181beed8ea536bcca028e.png)'
  prefs: []
  type: TYPE_IMG
- en: Basic LCEL input/output flow
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two key additions to this pipeline that are unique to LangChain :'
  prefs: []
  type: TYPE_NORMAL
- en: '**RunnableParallel** : As the name suggests, this class provides the ***functionality
    to run multiple processes in parallel***. As a result, the output of a RunnableParallel
    is a dict with the keys being the arguments provided during its initialization.
    In this case, the output would have two keys : *context* and *question*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So why do we need this in our current situation? Its required because the qa_prompt
    template requires two input values: the context and the question. Therefore we
    need to compute these values individually and then pass them together to the qa_prompt
    template.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**RunnablePassthrough** : This is a useful class when you want to pass through
    the input to the next stage without any modification. Essentially, this ***acts
    as an identity function*** that returns whatever is passed as its input.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The flowchart for the above RAG would look like this :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a277631bbe470ee4cf767555c1e93a1.png)'
  prefs: []
  type: TYPE_IMG
- en: QA RAG with Self Evaluation I
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building over the previous RAG chain, we now introduce new elements into the
    chain to implement the self evaluation component.
  prefs: []
  type: TYPE_NORMAL
- en: The self evaluation component is again a pretty straightforward implementation.
    We take the answer provided by the first LLM and pass it to the evaluator LLM
    along with the question and ask it to provide a binary response (Correct/Incorrect).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The first key difference is the addition of an additional RunnableParallel
    component. This is required because, similar to the initial prompt for the QA,
    the self eval prompt also requires two inputs : the base LLM’s answer as well
    as the user’s question.'
  prefs: []
  type: TYPE_NORMAL
- en: So the output of the first RunnableParallel is the context text and the question
    while the output of the second RunnableParallel is the LLM answer along with the
    question.
  prefs: []
  type: TYPE_NORMAL
- en: '***NOTE:*** *For the second RunnableParallel, we use the itemgetter method
    to retain only the question value from the previous input and propagate it forward.
    This is done instead of using RunnablePassthrough as it would passed on the full
    input (dict with two keys) whereas we are only interested in passing on the question
    right now and not the context. Additionally, there is the issue of formatting
    as qa_eval_prompt expects a dict with str -> str mapping but using RunnablePassthrough
    would results in a str-> dict mapping*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The flowchart for this RAG implementation would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/156a9c5f062931ab26da502dd4a0fd5b.png)'
  prefs: []
  type: TYPE_IMG
- en: QA RAG with Self Evaluation II
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this variation, we make a change to the evaluation procedure. In addition
    to the question-answer pair, we also pass the retrieved context to the evaluator
    LLM.
  prefs: []
  type: TYPE_NORMAL
- en: To accomplish this, we add an additional itemgetter function in the second RunnableParallel
    to collect the context string and pass it to the new qa_eval_prompt_with_context
    prompt template.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Implementation Flowchart :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebfe9ffacb495748306f991ea660b5b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Retrieving intermediate variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the common pain points with using a chain implementation like LCEL is
    the difficulty in accessing the intermediate variables, which is important for
    debugging pipelines. We look at few options where we can still access any intermediate
    variables we are interested using manipulations of the LCEL
  prefs: []
  type: TYPE_NORMAL
- en: Using RunnableParallel to carry forward intermediate outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw earlier, RunnableParallel allows us to carry multiple arguments forward
    to the next step in the chain. So we use this ability of RunnableParallel to carry
    forward the required intermediate values all the way till the end.
  prefs: []
  type: TYPE_NORMAL
- en: In the below example, we modify the original self eval RAG chain to output the
    retrieved context text along with the final self evaluation output. The primary
    change is that we add a RunnableParallel object to every step of the process to
    carry forward the context variable.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we also use the itemgetter function to clearly specify the inputs
    for the subsequent steps. For example, for the last two RunnableParallel objects,
    we use *itemgetter(‘input’)* to ensure that only the input argument from the previous
    step is passed on to the LLM/ Json parser objects.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from this chain looks like the following :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e44d743b97bcc76935b6a90ee2d85215.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A more concise variation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Using Global variables to save intermediate steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This method essentially uses the principle of a logger. We introduce a new function
    that saves its input to a global variable, thus allowing us access to the intermediate
    variable through the global variable
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here we define a global variable called *context* and a function called *save_context*
    that saves its input value to the global *context* variable before returning the
    same input. In the chain, we add the *save_context* function as the last step
    of the context retrieval step.
  prefs: []
  type: TYPE_NORMAL
- en: This option allows you to access any intermediate steps without making major
    changes to the chain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d75788fdbea9ddc770002e006ad204b.png)'
  prefs: []
  type: TYPE_IMG
- en: Accessing intermediate variables using global variables
  prefs: []
  type: TYPE_NORMAL
- en: Using callbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Attaching callbacks to your chain is another common method used for logging
    intermediate variable values. Theres a lot to cover on the topic of callbacks
    in LangChain, so I will be covering this in detail in a different post.
  prefs: []
  type: TYPE_NORMAL
- en: Why use LCEL?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reasons for using LCEL are best explained by the authors of Langchain themselves
    in their [official documentation](https://python.langchain.com/docs/expression_language/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Of the points mentioned in the documentation, the following are some that I
    find especially useful :'
  prefs: []
  type: TYPE_NORMAL
- en: 'I[nput and output schemas](https://python.langchain.com/docs/expression_language/interface/#input-schema)
    : Will be covering this in detail in a different post'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Async support](https://python.langchain.com/docs/expression_language/interface/)
    : As we move towards production applications, it becomes more important to have
    async functionality. LCEL pipeline allow for the seamless transition to async
    operations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Optimized parallel execution](https://python.langchain.com/docs/expression_language/primitives/parallel/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Above these reasons, as a matter of personal preference, I feel that using LCEL
    helps improve the readability of your code and allows for cleaner implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Full code notebook](https://github.com/rsk2327/AI-Workbook/blob/main/LangChain/Self%20Eval%20RAG.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PDF text](https://github.com/rsk2327/AI-Workbook/blob/main/LangChain/machine_learning_basics.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Images : All images are created by the author'
  prefs: []
  type: TYPE_NORMAL
- en: '*In addition to Medium, I share my thoughts, ideas and other updates on* [*Linkedin*](https://www.linkedin.com/in/roshan-santhosh/)*.*'
  prefs: []
  type: TYPE_NORMAL
