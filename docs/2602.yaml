- en: Discover What Every Neuron in the Llama Model Does
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/-0927524e4807?source=collection_archive---------6-----------------------#2024-10-25](https://towardsdatascience.com/-0927524e4807?source=collection_archive---------6-----------------------#2024-10-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Transluce‚Äôs new tool is changing the game for AI transparency ‚Äî a test case
    and some food for thought
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@benhagag10?source=post_page---byline--0927524e4807--------------------------------)[![Ben
    Hagag](../Images/a06fa102dfbe84afc6da846c622265a3.png)](https://medium.com/@benhagag10?source=post_page---byline--0927524e4807--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0927524e4807--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0927524e4807--------------------------------)
    [Ben Hagag](https://medium.com/@benhagag10?source=post_page---byline--0927524e4807--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0927524e4807--------------------------------)
    ¬∑7 min read¬∑Oct 25, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a37c05472f6079eaec9558314ca0054d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author ‚Äî caught in the act of playing with the new tool!
  prefs: []
  type: TYPE_NORMAL
- en: '[Transluce](https://transluce.org/), a new non-profit research lab with an
    inspiring mission, has just released (23.10.24) a fascinating tool that provides
    insights into neuron behavior in LLMs. Or in their own words:'
  prefs: []
  type: TYPE_NORMAL
- en: When an AI system behaves unexpectedly, we‚Äôd like to understand the ‚Äúthought
    process‚Äù that explains why the behavior occurred. This lets us predict and fix
    problems with AI models , surface hidden knowledge, and uncover learned biases
    and spurious correlations.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To fulfill their mission, they have launched an observability interface where
    you can input your own prompts, receive responses, and see which neurons are activated.
    You can then explore the activated neurons and their attribution to the model‚Äôs
    output, all enabled by their novel approach to automatically producing high-quality
    descriptions of neurons inside language models.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to test the tool, go [here](https://monitor.transluce.org/dashboard/chat).
    They also offer some helpful tutorials. In this article, I will try to provide
    another use case and share my own experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are probably many things to know (depending on your background), but
    I‚Äôll focus on two key features: Activation and Attribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Activation** measures the (normalized) activation value of the neuron. Llama
    uses gated MLPs, meaning that activations can be either positive or negative.
    We normalize by the value of the 10‚Äì5 quantile of the neuron across a large dataset
    of examples.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Attribution*** *measures how much the neuron affects the model‚Äôs output.
    Attribution must be conditioned on a specific output token, and is equal to the
    gradient of that output token‚Äôs probability with respect to the neuron‚Äôs activation,
    times the activation value of the neuron. Attribution values are not normalized,
    and are reported as absolute values.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using these two features you can explore the model‚Äôs behavior, the neurons behavior
    and even notice for patterns (or as they call it ‚Äúclusters‚Äù) of neurons‚Äô behavior
    phenomena.
  prefs: []
  type: TYPE_NORMAL
- en: If the model output isn‚Äôt what you expect, or if the model gets it wrong, the
    tool allows you to steer neurons and ‚Äòfix‚Äô the issue by either strengthening or
    suppressing concept-related neurons (There are great work on how to steer based
    on concepts ‚Äî one of them is [this](https://proceedings.neurips.cc/paper_files/paper/2023/hash/d066d21c619d0a78c5b557fa3291a8f4-Abstract-Conference.html)
    great work).
  prefs: []
  type: TYPE_NORMAL
- en: So, curious enough, I tested this with my own prompt.
  prefs: []
  type: TYPE_NORMAL
- en: I took a simple logic question that most models today fail to solve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: ‚Äúùóîùóπùó∂ùó∞ùó≤ ùóµùóÆùòÄ ùü∞ ùóØùóøùóºùòÅùóµùó≤ùóøùòÄ ùóÆùóªùó± ùüÆ ùòÄùó∂ùòÄùòÅùó≤ùóøùòÄ. ùóõùóºùòÑ ùó∫ùóÆùóªùòÜ ùòÄùó∂ùòÄùòÅùó≤ùóøùòÄ ùó±ùóºùó≤ùòÄ ùóîùóπùó∂ùó∞ùó≤‚ÄôùòÄ ùóØùóøùóºùòÅùóµùó≤ùóø
    ùóµùóÆùòÉùó≤?‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c41be48fd76516494d0442bce1e91b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Homepage. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  prefs: []
  type: TYPE_NORMAL
- en: '**And voila‚Ä¶.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d39acc9f66ced5a61b6a8b549963162.png)'
  prefs: []
  type: TYPE_IMG
- en: Llama gets it wrong. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  prefs: []
  type: TYPE_NORMAL
- en: '**Or not.**'
  prefs: []
  type: TYPE_NORMAL
- en: On the left side, you can see the prompt and the output. On the right side,
    you can see the neurons that ‚Äúfire‚Äù the most and observe the main clusters these
    neurons group into.
  prefs: []
  type: TYPE_NORMAL
- en: If you hover over the tokens on the left, you can see the top probabilities.
    If you click on one of the tokens, you can find out which neurons contributed
    to predicting that token.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d67a52b25dad4a1afc90411715c2468.png)'
  prefs: []
  type: TYPE_IMG
- en: Hover over ‚Äúin.‚Äù We can see tokens with the top probabilities. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  prefs: []
  type: TYPE_NORMAL
- en: '**As you can see, both the logic and the answer are wrong.**'
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúSince Alice has 4 brothers, we need to find out how many sisters they have
    in common‚Äù >>> Ugh! You already know that.
  prefs: []
  type: TYPE_NORMAL
- en: And of course, if Alice has two sisters (which is given in the input), **it
    doesn‚Äôt mean Alice‚Äôs brother has 2 sisters :(**
  prefs: []
  type: TYPE_NORMAL
- en: So, let‚Äôs try to fix this. After examining the neurons, I noticed that the ‚Äúdiversity‚Äù
    concept was overly active (perhaps it was confused about Alice‚Äôs identity?). So,
    I tried steering these neurons.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7a1954b87ef3876f4e203a9c5965741.png)'
  prefs: []
  type: TYPE_IMG
- en: Steering window. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  prefs: []
  type: TYPE_NORMAL
- en: 'I suppressed the neurons related to this concept and tried again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28c482a5f7e57c723dc472f51b631904.png)'
  prefs: []
  type: TYPE_IMG
- en: Adjusted model after steering. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it still output wrong answer. But if you look closely at the
    output, the logic has changed and its seems quite better ‚Äî it catches that we
    need to ‚Äúshift‚Äù to ‚Äúone of her brothers perspective‚Äù. And also, it understood
    that Alice is a sister (Finally!).
  prefs: []
  type: TYPE_NORMAL
- en: The final answer is though still incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: I decided to strengthen the ‚Äúgender roles‚Äù concept, thinking it would help the
    model better understand the roles of the brother and sister in this question,
    while maintaining its understanding of Alice‚Äôs relationship to her siblings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3da0f27dc4528db009dc4e99ebd31c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Another adjustment. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  prefs: []
  type: TYPE_NORMAL
- en: Ok, the answer was still incorrect, but it seemed that the reasoning thought
    process improved slightly. The model stated that ‚ÄúAlice‚Äôs 2 sisters are being
    referred to.‚Äù The first half of the sentence indicated some understanding (Yes,
    this is also in the input. And no, I‚Äôm not arguing that the model or any model
    can truly understand ‚Äî but that‚Äôs a discussion for another time) that Alice has
    two sisters. It also still recognized that Alice is a sister herself (‚Äú‚Ä¶the brother
    has 2 sisters ‚Äî Alice and one other sister‚Ä¶‚Äù). But still, the answer was wrong.
    So close‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: Now that we‚Äôre close, I noticed an unrelated concept (‚Äúchemical compounds and
    reactions‚Äù) influencing the "2" token (highlighted in orange on the left side).
    I‚Äôm not sure why this concept had high influence, but I decided it was irrelevant
    to the question and suppressed it.
  prefs: []
  type: TYPE_NORMAL
- en: The result?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10e6ad6d150d44a9cd1c9cb98925e0d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Final result. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  prefs: []
  type: TYPE_NORMAL
- en: Success!! (ish)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see above, it finally got the answer right.
  prefs: []
  type: TYPE_NORMAL
- en: But‚Ä¶how was the reasoning?
  prefs: []
  type: TYPE_NORMAL
- en: well‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f30bd90c65a2c7d35c8e06db1026ad6f.png)'
  prefs: []
  type: TYPE_IMG
- en: Final output. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  prefs: []
  type: TYPE_NORMAL
- en: It followed a strange logical process with some role-playing confusion, but
    it still ended up with the correct answer (if you can explain it, please share).
  prefs: []
  type: TYPE_NORMAL
- en: So, after some trial and error, I got there ‚Äî almost. After adjusting the neurons
    related to gender and chemical compounds, the model produced the correct answer,
    but the reasoning wasn‚Äôt quite there. I‚Äôm not sure, maybe with more tweaks and
    adjustments (and maybe better choices of concepts and neurons), I would get both
    the right answer and the correct logic. I challenge you to try.
  prefs: []
  type: TYPE_NORMAL
- en: This is still experimental and I didn‚Äôt use any systematic approach, but to
    be honest, I‚Äôm impressed and think it‚Äôs incredibly promising. Why? Because the
    ability to observe and get descriptions of every neuron, understand (even partially)
    their influence, and steer behavior (without retraining or prompting) in real
    time is impressive ‚Äî and yes, also a bit addictive, so be careful!
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thought I have: if the descriptions are accurate (reflecting actual
    behavior), and if we can experiment with different setups manually, why not try
    building a model based on neuron activations and attribution values? Transluce
    team, if you''re reading this‚Ä¶what do you think?'
  prefs: []
  type: TYPE_NORMAL
- en: All in all, great job. I highly recommend diving deeper into this. The ease
    of use and the ability to observe neuron behavior is compelling, and I believe
    we‚Äôll see more tools embracing these techniques to help us better understand our
    models.
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôm now going to test this on some of our most challenging legal reasoning use
    cases ‚Äî to see how it captures more complex logical structures.
  prefs: []
  type: TYPE_NORMAL
- en: What does this mean for AI? We‚Äôll have to wait and see‚Ä¶ but just like GPT was
    embraced so quickly and naturally, I think this release opens a new chapter in
    LLM interpretability. More importantly, it moves us closer to building tools that
    are better aligned and more responsible.
  prefs: []
  type: TYPE_NORMAL
- en: Now that their work is open source, it‚Äôs up to the community to challenge it,
    improve it, or build on it.
  prefs: []
  type: TYPE_NORMAL
- en: So, give it a try.
  prefs: []
  type: TYPE_NORMAL
- en: In the meantime, what do you think?
  prefs: []
  type: TYPE_NORMAL
- en: '**Some limitations (very briefly):**'
  prefs: []
  type: TYPE_NORMAL
- en: The tool was just released yesterday, and I haven‚Äôt had the chance to fully
    review the entire documentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I tried simple questions successfully, but when I asked similar questions with
    different attributes, the logic still failed. Generalization is key here ‚Äî trying
    to ‚Äúcapture‚Äù some generalization within the observability tool would take it to
    the next level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It‚Äôs not always reproducible, even with low or zero temperature settings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no single path to both a correct answer and logical reasoning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It involves quite a bit of trial and error. After a few iterations, I got a
    ‚Äúfeel‚Äù for it, but it felt similar to the early days of using GPT ‚Äî exciting when
    it worked, but often leaving you wondering, ‚ÄúWhat really happened here?‚Äù So there‚Äôs
    still work to be done.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
