- en: Discover What Every Neuron in the Llama Model Does
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现Llama模型中每个神经元的作用
- en: 原文：[https://towardsdatascience.com/-0927524e4807?source=collection_archive---------6-----------------------#2024-10-25](https://towardsdatascience.com/-0927524e4807?source=collection_archive---------6-----------------------#2024-10-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/-0927524e4807?source=collection_archive---------6-----------------------#2024-10-25](https://towardsdatascience.com/-0927524e4807?source=collection_archive---------6-----------------------#2024-10-25)
- en: Transluce’s new tool is changing the game for AI transparency — a test case
    and some food for thought
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transluce的新工具正在改变AI透明度的游戏规则——一个测试案例和一些思考材料
- en: '[](https://medium.com/@benhagag10?source=post_page---byline--0927524e4807--------------------------------)[![Ben
    Hagag](../Images/a06fa102dfbe84afc6da846c622265a3.png)](https://medium.com/@benhagag10?source=post_page---byline--0927524e4807--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0927524e4807--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0927524e4807--------------------------------)
    [Ben Hagag](https://medium.com/@benhagag10?source=post_page---byline--0927524e4807--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@benhagag10?source=post_page---byline--0927524e4807--------------------------------)[![Ben
    Hagag](../Images/a06fa102dfbe84afc6da846c622265a3.png)](https://medium.com/@benhagag10?source=post_page---byline--0927524e4807--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0927524e4807--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0927524e4807--------------------------------)
    [Ben Hagag](https://medium.com/@benhagag10?source=post_page---byline--0927524e4807--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0927524e4807--------------------------------)
    ·7 min read·Oct 25, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0927524e4807--------------------------------)
    ·7分钟阅读·2024年10月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a37c05472f6079eaec9558314ca0054d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a37c05472f6079eaec9558314ca0054d.png)'
- en: Image by the author — caught in the act of playing with the new tool!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供——拍下了使用新工具的瞬间！
- en: '[Transluce](https://transluce.org/), a new non-profit research lab with an
    inspiring mission, has just released (23.10.24) a fascinating tool that provides
    insights into neuron behavior in LLMs. Or in their own words:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[Transluce](https://transluce.org/)，一个具有启发性使命的非营利研究实验室，刚刚发布了一个引人入胜的工具，提供了LLM中神经元行为的洞察。或者用他们自己的话来说：'
- en: When an AI system behaves unexpectedly, we’d like to understand the “thought
    process” that explains why the behavior occurred. This lets us predict and fix
    problems with AI models , surface hidden knowledge, and uncover learned biases
    and spurious correlations.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当AI系统的行为出乎意料时，我们希望了解“思考过程”，以解释为何会发生这种行为。这让我们能够预测并修复AI模型中的问题，揭示隐藏的知识，揭露学到的偏见和虚假的关联。
- en: To fulfill their mission, they have launched an observability interface where
    you can input your own prompts, receive responses, and see which neurons are activated.
    You can then explore the activated neurons and their attribution to the model’s
    output, all enabled by their novel approach to automatically producing high-quality
    descriptions of neurons inside language models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现他们的使命，他们推出了一个可观察性界面，你可以在其中输入自己的提示语，获取回应，并查看哪些神经元被激活。你可以进一步探索这些激活的神经元及其对模型输出的归因，所有这些都得益于他们通过创新方法自动生成高质量的神经元描述。
- en: If you want to test the tool, go [here](https://monitor.transluce.org/dashboard/chat).
    They also offer some helpful tutorials. In this article, I will try to provide
    another use case and share my own experience.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想测试这个工具，可以[点击这里](https://monitor.transluce.org/dashboard/chat)。他们还提供了一些有用的教程。在本文中，我将尝试提供另一个用例，并分享我自己的经验。
- en: 'There are probably many things to know (depending on your background), but
    I’ll focus on two key features: Activation and Attribution.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要了解很多内容（具体取决于你的背景），但我将重点介绍两个关键特性：激活值和归因。
- en: '**Activation** measures the (normalized) activation value of the neuron. Llama
    uses gated MLPs, meaning that activations can be either positive or negative.
    We normalize by the value of the 10–5 quantile of the neuron across a large dataset
    of examples.'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**激活值**衡量的是神经元的（归一化）激活值。Llama使用门控MLP，这意味着激活值可以是正数也可以是负数。我们通过神经元在大规模数据集上的10–5分位数值进行归一化。'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Attribution*** *measures how much the neuron affects the model’s output.
    Attribution must be conditioned on a specific output token, and is equal to the
    gradient of that output token’s probability with respect to the neuron’s activation,
    times the activation value of the neuron. Attribution values are not normalized,
    and are reported as absolute values.*'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***归因*** *衡量神经元对模型输出的影响程度。归因必须基于特定的输出标记，并等于该输出标记概率相对于神经元激活的梯度，乘以神经元的激活值。归因值没有标准化，以绝对值的形式报告。*'
- en: Using these two features you can explore the model’s behavior, the neurons behavior
    and even notice for patterns (or as they call it “clusters”) of neurons’ behavior
    phenomena.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这两个特征，你可以探索模型的行为、神经元的行为，甚至注意到神经元行为现象的模式（或者他们称之为“聚类”）。
- en: If the model output isn’t what you expect, or if the model gets it wrong, the
    tool allows you to steer neurons and ‘fix’ the issue by either strengthening or
    suppressing concept-related neurons (There are great work on how to steer based
    on concepts — one of them is [this](https://proceedings.neurips.cc/paper_files/paper/2023/hash/d066d21c619d0a78c5b557fa3291a8f4-Abstract-Conference.html)
    great work).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型输出不是你期望的，或者模型得出了错误的结果，该工具允许你通过加强或抑制与概念相关的神经元来引导并“修复”问题（有许多出色的工作讲解如何根据概念进行引导
    —— 其中之一是[这篇](https://proceedings.neurips.cc/paper_files/paper/2023/hash/d066d21c619d0a78c5b557fa3291a8f4-Abstract-Conference.html)出色的工作）。
- en: So, curious enough, I tested this with my own prompt.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，出于好奇，我用我自己的提示进行了测试。
- en: I took a simple logic question that most models today fail to solve.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了一个大多数今天的模型无法解决的简单逻辑问题。
- en: 'Q: “𝗔𝗹𝗶𝗰𝗲 𝗵𝗮𝘀 𝟰 𝗯𝗿𝗼𝘁𝗵𝗲𝗿𝘀 𝗮𝗻𝗱 𝟮 𝘀𝗶𝘀𝘁𝗲𝗿𝘀. 𝗛𝗼𝘄 𝗺𝗮𝗻𝘆 𝘀𝗶𝘀𝘁𝗲𝗿𝘀 𝗱𝗼𝗲𝘀 𝗔𝗹𝗶𝗰𝗲’𝘀 𝗯𝗿𝗼𝘁𝗵𝗲𝗿
    𝗵𝗮𝘃𝗲?”'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：“𝗔𝗹𝗶𝗰𝗲 有 4 个兄弟和 2 个姐妹。爱丽丝的兄弟有多少个姐妹？”
- en: '![](../Images/6c41be48fd76516494d0442bce1e91b0.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c41be48fd76516494d0442bce1e91b0.png)'
- en: Homepage. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 主页。图片来自[monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
- en: '**And voila….**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**瞧，成功了……**'
- en: '![](../Images/0d39acc9f66ced5a61b6a8b549963162.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d39acc9f66ced5a61b6a8b549963162.png)'
- en: Llama gets it wrong. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 做错了。图片来自[monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
- en: '**Or not.**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**或者没有。**'
- en: On the left side, you can see the prompt and the output. On the right side,
    you can see the neurons that “fire” the most and observe the main clusters these
    neurons group into.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，你可以看到提示和输出。在右侧，你可以看到“激活”最多的神经元，并观察这些神经元聚集的主要簇。
- en: If you hover over the tokens on the left, you can see the top probabilities.
    If you click on one of the tokens, you can find out which neurons contributed
    to predicting that token.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将鼠标悬停在左侧的标记上，你可以看到最高的概率。如果你点击其中一个标记，你可以找到哪些神经元参与了预测该标记。
- en: '![](../Images/1d67a52b25dad4a1afc90411715c2468.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d67a52b25dad4a1afc90411715c2468.png)'
- en: Hover over “in.” We can see tokens with the top probabilities. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将鼠标悬停在“in”上。我们可以看到最高概率的标记。图片来自[monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
- en: '**As you can see, both the logic and the answer are wrong.**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**正如你所看到的，逻辑和答案都是错误的。**'
- en: “Since Alice has 4 brothers, we need to find out how many sisters they have
    in common” >>> Ugh! You already know that.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: “因为爱丽丝有 4 个兄弟，所以我们需要找出他们共有多少个姐妹” >>> 哎呀！你已经知道答案了。
- en: And of course, if Alice has two sisters (which is given in the input), **it
    doesn’t mean Alice’s brother has 2 sisters :(**
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果爱丽丝有两个姐妹（这是输入中给定的），**这并不意味着爱丽丝的兄弟有 2 个姐妹 :(**
- en: So, let’s try to fix this. After examining the neurons, I noticed that the “diversity”
    concept was overly active (perhaps it was confused about Alice’s identity?). So,
    I tried steering these neurons.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们试着修复这个问题。在检查神经元之后，我注意到“多样性”概念过于活跃（也许它对爱丽丝的身份感到困惑？）。因此，我尝试调整这些神经元。
- en: '![](../Images/b7a1954b87ef3876f4e203a9c5965741.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7a1954b87ef3876f4e203a9c5965741.png)'
- en: Steering window. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 引导窗口。图片来自[monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
- en: 'I suppressed the neurons related to this concept and tried again:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我抑制了与此概念相关的神经元，并重新尝试：
- en: '![](../Images/28c482a5f7e57c723dc472f51b631904.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28c482a5f7e57c723dc472f51b631904.png)'
- en: Adjusted model after steering. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的模型在引导后。图片来自[monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
- en: As you can see, it still output wrong answer. But if you look closely at the
    output, the logic has changed and its seems quite better — it catches that we
    need to “shift” to “one of her brothers perspective”. And also, it understood
    that Alice is a sister (Finally!).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它仍然输出了错误的答案。但如果仔细观察输出，逻辑已经有所变化，看起来好多了——它捕捉到我们需要“转变”到“其中一个兄弟的视角”。而且，它也理解了爱丽丝是一个姐妹（终于！）。
- en: The final answer is though still incorrect.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最终答案仍然不正确。
- en: I decided to strengthen the “gender roles” concept, thinking it would help the
    model better understand the roles of the brother and sister in this question,
    while maintaining its understanding of Alice’s relationship to her siblings.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定加强“性别角色”这一概念，认为这有助于模型更好地理解问题中兄妹的角色，同时保持它对爱丽丝与她兄妹关系的理解。
- en: '![](../Images/a3da0f27dc4528db009dc4e99ebd31c3.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3da0f27dc4528db009dc4e99ebd31c3.png)'
- en: Another adjustment. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个调整。图片来自[monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
- en: Ok, the answer was still incorrect, but it seemed that the reasoning thought
    process improved slightly. The model stated that “Alice’s 2 sisters are being
    referred to.” The first half of the sentence indicated some understanding (Yes,
    this is also in the input. And no, I’m not arguing that the model or any model
    can truly understand — but that’s a discussion for another time) that Alice has
    two sisters. It also still recognized that Alice is a sister herself (“…the brother
    has 2 sisters — Alice and one other sister…”). But still, the answer was wrong.
    So close…
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，答案仍然不正确，但似乎推理过程略有改善。模型表示“提到的是爱丽丝的两个姐妹”。句子的前半部分表现出一定的理解（是的，这部分也在输入中。并且不，我并不是在争论模型或任何模型能否真正理解——这是另一个讨论的话题），即爱丽丝有两个姐妹。它也依然认知到爱丽丝自己是一个姐妹（“…这个兄弟有2个姐妹——爱丽丝和另一个姐妹…”）。但答案还是错的。真是差一点……
- en: Now that we’re close, I noticed an unrelated concept (“chemical compounds and
    reactions”) influencing the "2" token (highlighted in orange on the left side).
    I’m not sure why this concept had high influence, but I decided it was irrelevant
    to the question and suppressed it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们接近正确答案了，我注意到一个无关的概念（“化学化合物与反应”）影响了“2”这个符号（左侧以橙色高亮显示）。我不确定为什么这个概念会有这么大的影响，但我决定它与问题无关，于是将其抑制了。
- en: The result?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如何？
- en: '![](../Images/10e6ad6d150d44a9cd1c9cb98925e0d2.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10e6ad6d150d44a9cd1c9cb98925e0d2.png)'
- en: Final result. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果。图片来自[monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
- en: Success!! (ish)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 成功了！！（差不多）
- en: As you can see above, it finally got the answer right.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，它终于得到了正确的答案。
- en: But…how was the reasoning?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 但是……推理过程如何呢？
- en: well…
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯……
- en: '![](../Images/f30bd90c65a2c7d35c8e06db1026ad6f.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f30bd90c65a2c7d35c8e06db1026ad6f.png)'
- en: Final output. Image via [monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出。图片来自[monitor.transluce.org](https://monitor.transluce.org/dashboard/chat)
- en: It followed a strange logical process with some role-playing confusion, but
    it still ended up with the correct answer (if you can explain it, please share).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 它跟随了一种奇怪的逻辑过程，带有一些角色扮演上的混乱，但最终还是得出了正确的答案（如果你能解释这个过程，请分享）。
- en: So, after some trial and error, I got there — almost. After adjusting the neurons
    related to gender and chemical compounds, the model produced the correct answer,
    but the reasoning wasn’t quite there. I’m not sure, maybe with more tweaks and
    adjustments (and maybe better choices of concepts and neurons), I would get both
    the right answer and the correct logic. I challenge you to try.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在一些试错后，我几乎成功了。在调整了与性别和化学化合物相关的神经元后，模型给出了正确答案，但推理过程还不够完善。我不确定，也许通过更多的微调和调整（或许更好的概念和神经元选择），我能够同时得到正确的答案和正确的逻辑。我挑战你试试看。
- en: This is still experimental and I didn’t use any systematic approach, but to
    be honest, I’m impressed and think it’s incredibly promising. Why? Because the
    ability to observe and get descriptions of every neuron, understand (even partially)
    their influence, and steer behavior (without retraining or prompting) in real
    time is impressive — and yes, also a bit addictive, so be careful!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这仍然是实验性的，我没有使用任何系统化的方法，但说实话，我很受震撼，认为这非常有前景。为什么？因为能够观察并获取每个神经元的描述，理解（即使是部分理解）它们的影响，并实时地引导行为（无需重新训练或提示）真是令人印象深刻——是的，也有点上瘾，所以要小心！
- en: 'Another thought I have: if the descriptions are accurate (reflecting actual
    behavior), and if we can experiment with different setups manually, why not try
    building a model based on neuron activations and attribution values? Transluce
    team, if you''re reading this…what do you think?'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个想法是：如果这些描述是准确的（反映了实际行为），而且如果我们能够手动尝试不同的设置，为什么不尝试基于神经元激活和归因值构建一个模型呢？Transluce团队，如果你们在看这条信息……你们怎么看？
- en: All in all, great job. I highly recommend diving deeper into this. The ease
    of use and the ability to observe neuron behavior is compelling, and I believe
    we’ll see more tools embracing these techniques to help us better understand our
    models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，做得很好。我强烈建议深入研究一下。它的易用性和观察神经元行为的能力非常有吸引力，我相信我们会看到更多的工具采用这些技术来帮助我们更好地理解我们的模型。
- en: I’m now going to test this on some of our most challenging legal reasoning use
    cases — to see how it captures more complex logical structures.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我现在打算在一些最具挑战性的法律推理用例中测试这个——看看它如何捕捉更复杂的逻辑结构。
- en: What does this mean for AI? We’ll have to wait and see… but just like GPT was
    embraced so quickly and naturally, I think this release opens a new chapter in
    LLM interpretability. More importantly, it moves us closer to building tools that
    are better aligned and more responsible.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这对人工智能意味着什么？我们得拭目以待……但就像GPT如此迅速自然地被接受一样，我认为这个版本的发布为大语言模型的可解释性开辟了新篇章。更重要的是，它将我们带得更近，朝着构建更加对齐和负责任的工具迈进。
- en: Now that their work is open source, it’s up to the community to challenge it,
    improve it, or build on it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 既然他们的工作是开源的，那么就由社区来挑战它、改进它或在此基础上进行构建。
- en: So, give it a try.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，试试看吧。
- en: In the meantime, what do you think?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，你怎么看？
- en: '**Some limitations (very briefly):**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**一些限制（简要说明）：**'
- en: The tool was just released yesterday, and I haven’t had the chance to fully
    review the entire documentation.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该工具就在昨天发布，我还没有完全有机会审阅整个文档。
- en: I tried simple questions successfully, but when I asked similar questions with
    different attributes, the logic still failed. Generalization is key here — trying
    to “capture” some generalization within the observability tool would take it to
    the next level.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我成功地尝试了简单的问题，但当我提出带有不同属性的类似问题时，逻辑仍然失败。归纳能力是这里的关键——尝试在可观察性工具中“捕捉”某些归纳性，将使其达到新的高度。
- en: It’s not always reproducible, even with low or zero temperature settings.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使在低温或零温设置下，也并非总是可以重现。
- en: There is no single path to both a correct answer and logical reasoning.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有单一的路径可以同时获得正确答案和逻辑推理。
- en: It involves quite a bit of trial and error. After a few iterations, I got a
    “feel” for it, but it felt similar to the early days of using GPT — exciting when
    it worked, but often leaving you wondering, “What really happened here?” So there’s
    still work to be done.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它涉及到相当多的反复试验。在几次迭代后，我开始“感觉到”它的运作，但这和刚开始使用GPT时很像——当它有效时令人兴奋，但常常让你陷入困惑，“这里到底发生了什么？”因此，仍然需要进一步的工作。
