["```py\nfrom sklearn.model_selection import train_test_split\n# Assuming X is your feature set and y is your target variable\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\n# Stratified split to maintain class distribution\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n```", "```py\nfrom sklearn.model_selection import KFold, train_test_split\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Further split X_train and y_train into train and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n    # Now you have X_train, X_val, X_test, y_train, y_val, y_test for each fold\n    # You can now train and evaluate your model using these sets\n```", "```py\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Further split X_train and y_train into train and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n\n    # Now you have X_train, X_val, X_test, y_train, y_val, y_test for each fold\n    # You can now train and evaluate your model using these sets\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Initialize lists to store the scores for each fold\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor train_index, test_index in skf.split(X, y): #y is a categorical target variable\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Further split X_train and y_train into train and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n\n    # Train the model\n    model = LogisticRegression(random_state=42)\n    model.fit(X_train, y_train)\n\n    # Validate the model\n    y_val_pred = model.predict(X_val)\n    val_accuracy = accuracy_score(y_val, y_val_pred)\n    val_precision = precision_score(y_val, y_val_pred, average='weighted')\n    val_recall = recall_score(y_val, y_val_pred, average='weighted')\n    val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n\n    print(f\"Validation Scores - Accuracy: {val_accuracy}, Precision: {val_precision}, Recall: {val_recall}, F1 Score: {val_f1}\")\n\n    # Test the model\n    y_test_pred = model.predict(X_test)\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n    test_recall = recall_score(y_test, y_test_pred, average='weighted')\n    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n\n    # Store the scores\n    accuracy_scores.append(test_accuracy)\n    precision_scores.append(test_precision)\n    recall_scores.append(test_recall)\n    f1_scores.append(test_f1)\n\n    print(f\"Test Scores - Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\")\n\n# Calculate and print the average scores across all folds\nprint(f\"\\nAverage Test Scores across all folds - Accuracy: {sum(accuracy_scores) / len(accuracy_scores)}, Precision: {sum(precision_scores) / len(precision_scores)}, Recall: {sum(recall_scores) / len(recall_scores)}, F1 Score: {sum(f1_scores) / len(f1_scores)}\")\n```", "```py\nimport hdbscan\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.model_selection import ParameterGrid\nimport random\nrandom.seed(48) #for regeneration of same results\n\ndef get_clusters(df):\n  to_drop =[\"cluster_\", \"ID\"]\n  req_cols = sorted(set(df.columns) - set(to_drop))\n  X = df[req_cols] #keep only required columns in X\n  X_std = X.values #no need of scaling the training set for HDBSCAN\n\n  # Define parameter grid for HDBSCAN, you can play with this grid accordingly\n  param_grid = {\n      'min_cluster_size': list(range(2,20))\n      #'min_samples': [1, 2, 3]\n  }\n\n  best_score = -1\n  best_params = None\n\n  # Iterate over parameter grid\n  for params in ParameterGrid(param_grid):\n    model = hdbscan.HDBSCAN(**params, gen_min_span_tree=True)\n    cluster_labels = model.fit_predict(X_std)\n    unique_labels = np.unique(cluster_labels)\n    if len(unique_labels) > 1:  # Check if more than one cluster is formed\n      silhouette_avg = silhouette_score(X_std, cluster_labels) if len(unique_labels) > 1 else -1\n      if silhouette_avg > best_score:\n        best_score = silhouette_avg\n        best_params = params\n\n  if best_params is not None:\n    print(best_params)\n    best_model = hdbscan.HDBSCAN(**best_params, gen_min_span_tree=True)\n    cluster_labels = best_model.fit_predict(X_std) #get cluster labels from best model\n    df[\"cluster_\"] = [str(i) for i in cluster_labels]\n  else:\n    print(\"HDBSCAN produced only one cluster label. Unable to split the data.\")\n    df[\"cluster_\"] = \"0\" #when no clusters are found\n\n  return df\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\nfrom yellowbrick.cluster import KElbowVisualizer\n\ndef get_clusters(df):\n\n  to_drop =[\"cluster_\", \"ID\"]\n  req_cols = sorted(set(df.columns) - set(to_drop))\n  X = df[req_cols].values #keep only required columns in X\n\n  scaler = StandardScaler()\n  X_std = scaler.fit_transform(X) #scaling is needed in case of K-Means\n\n  model = KMeans()\n  visualizer = KElbowVisualizer(model, k=(2, 50)) #you can play with the range accordingly\n  visualizer.fit(X_std)\n  #visualizer.show()\n\n  optimal_n_clusters = visualizer.elbow_value_  #using elbow method to get optimal no. of clusters\n  kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=42)\n  kmeans.fit(X_std)\n\n  clust_labels = [str(i) for i in kmeans.labels_]\n\n  # Evaluate the clustering using silhouette score\n  silhouette_avg = silhouette_score(X_std, clust_labels)\n\n  df[\"cluster_\"] = clust_labels\n\nreturn df\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\nfrom yellowbrick.cluster import KElbowVisualizer\n\ndef get_clusters(df):\n  # taking animal categorical variable as a level of granularity to split on\n  grp1 = df.loc[(df['animal']=='cat')]\n  grp2 = df.loc[(df['animal']=='dog')]\n\n  temps = []\n  for num, temp in enumerate([grp1, grp2]):\n    to_drop =[\"cluster_\", \"ID\"]\n    final_cols = sorted(set(temp.columns) - set(to_drop))\n    X = temp[final_cols]\n\n    X = X.values\n    scaler = StandardScaler()\n    X_std = scaler.fit_transform(X) #scaling of variables is needed for K-Means clustering\n\n    model = KMeans()\n    visualizer = KElbowVisualizer(model, k=(2, 50))\n    visualizer.fit(X_std)\n    # visualizer.show()\n\n    #get optimal no. of clusters, K using elbow method\n    optimal_n_clusters = visualizer.elbow_value_  \n    kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=42) \n    kmeans.fit(X_std)\n\n    clust_labels = [str(num) + \"_\" + str(i) for i in kmeans.labels_]\n\n    # Evaluate the clustering using silhouette score\n    silhouette_avg = silhouette_score(X_std, clust_labels)\n\n    temp[\"cluster_\"] = clust_labels\n    temps.append(temp)\n\n  df = pd.concat(temps, axis=0)\n\n  return df\n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Assuming df is your DataFrame, \"cluster_\" is the column with cluster labels,\nunique_clusters = df[\"cluster_\"].unique()\n\ntrain_indices = []\nval_indices = []\ntest_indices = []\n\nfor cluster in unique_clusters:\n    cluster_data = df[df[\"cluster_\"] == cluster]\n    cluster_indices = cluster_data.index.values\n    cluster_y = cluster_data['y'].values\n\n    if stratify_ == True: #if you have categorical target variable\n      train_idx, temp_idx, _, temp_y = train_test_split(cluster_indices, cluster_y, test_size=0.4, stratify=cluster_y, random_state=42)\n      val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=temp_y, random_state=42)\n    else:\n      # Split indices of the current cluster into train and temp (which will be further split into val and test)\n      train_idx, temp_idx = train_test_split(cluster_indices, test_size=0.4, random_state=42)\n      val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n\n    train_indices.extend(train_idx)\n    val_indices.extend(val_idx)\n    test_indices.extend(test_idx)\n\n# Convert the indices lists to numpy arrays\ntrain_indices = np.array(train_indices)\nval_indices = np.array(val_indices)\ntest_indices = np.array(test_indices)\n\n# Assuming 'X' are the features and 'y' is the target column\nX = df.drop(columns=['y', 'cluster_']).values\ny = df['y'].values\n\n# Select the corresponding data for train, validation, and test sets\nX_train, y_train = X[train_indices], y[train_indices]\nX_val, y_val = X[val_indices], y[val_indices]\nX_test, y_test = X[test_indices], y[test_indices]\n```", "```py\ndef get_indices(df):\n  np.random.seed(seed=48)\n\n  total_length = len(df)\n  sample1_length = int(0.60 * total_length) #you can choose proportion accordingly\n  remaining_length = total_length - sample1_length\n\n  sample2_length = int(remaining_length / 2)\n  sample3_length = total_length - (sample1_length + sample2_length)\n\n  #create an array with range 0 - length of the df\n  all_indxs = np.array(range(total_length))\n\n  # Create arrays of indices divisible by 2 and 3 exclusively\n  indices_divisible_by_2 = np.array(list(set(np.where(all_indxs % 2 == 0)[0]) - set(np.where(all_indxs % 6 == 0)[0])))\n  indices_divisible_by_3 = np.array(list(set(np.where(all_indxs % 3 == 0)[0]) - set([0])))\n\n  #randomly choose indices divisibly by 2 with sample2_length\n  sample2_indices = sorted(indices_divisible_by_2[np.random.choice(len(indices_divisible_by_2), size=sample2_length, replace=False)])\n  try:\n    sample3_indices = sorted(indices_divisible_by_3[np.random.choice(len(indices_divisible_by_3), size=sample3_length, replace=False)])\n  except:\n    sample3_indices = []\n\n  sample1_indices = sorted(set(all_indxs) - set(sample2_indices) - set(sample3_indices))\n\n  return sample1_indices, sample2_indices, sample3_indices\n```", "```py\nindices_train = []\nindices_test = []\nindices_val = []\n\nfor num, cluster in enumerate(df['cluster_'].unique()):\n  temp_df = df[df['cluster_'] == cluster]\n  sample1_indices, sample2_indices, sample3_indices = get_indices(temp_df)\n  indices_train.append(list(temp_df.iloc[sample1_indices].index))\n  indices_test.append(list(temp_df.iloc[sample2_indices].index))\n  indices_val.append(list(temp_df.iloc[sample3_indices].index))\n\n# to flatten the list of lists containing indices for train,test,val set\nindices_train = [x for xs in indices_train for x in xs]\nindices_test = [x for xs in indices_test for x in xs]\nindices_val = [x for xs in indices_val for x in xs]\n```", "```py\ndef traintestvalsplit(df, id_col, cols_to_drop, cont_var, train_indices, test_indices, val_indices):\n\n  train, test, val = df.loc[train_indices], df.loc[test_indices], df.loc[val_indices]\n\n  # Split the data into train, validation, and test sets based on indices\n  X_train = train.drop(cols_to_drop + [cont_var] ,axis=1) #add which columns to drop\n  X_test = test.drop(cols_to_drop + [cont_var] ,axis=1)\n  X_val = val.drop(cols_to_drop + [cont_var] ,axis=1)\n\n  y_train = train[[cont_var]] #target variable\n  y_test = test[[cont_var]]\n  y_val = val[[cont_var]]\n\n  train_ids = train[[id_col]] #to preserve the IDs\n  test_ids = test[[id_col]]\n  val_ids = val[[id_col]]\n\n  print(\"Train set size:\", X_train.shape, len(train_ids))\n  print(\"Test set size:\", X_test.shape, len(test_ids))\n  print(\"Validation set size:\", X_val.shape, len(val_ids))\n\n  return X_train, X_val, X_test, y_train, y_val, y_test, train_ids, val_ids, test_ids\n\nX_train, X_val, X_test, y_train, y_val, y_test, train_ids, val_ids, test_ids = traintestvalsplit(df, id_col, cols_to_drop, cont_var, train_indices, test_indices, val_indices)\n```"]