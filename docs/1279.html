<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>3 Practical Tips to Combat Data Scarcity in Music AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>3 Practical Tips to Combat Data Scarcity in Music AI</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/3-practical-tips-to-combat-data-scarcity-in-music-ai-58e52c771aef?source=collection_archive---------6-----------------------#2024-05-22">https://towardsdatascience.com/3-practical-tips-to-combat-data-scarcity-in-music-ai-58e52c771aef?source=collection_archive---------6-----------------------#2024-05-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="441a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Get more out of your music data</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@maxhilsdorf?source=post_page---byline--58e52c771aef--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Max Hilsdorf" class="l ep by dd de cx" src="../Images/01da76c553e43d5ed6b6849bdbfd00da.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*IqDNSOVQGpnU-ZrTj70xFg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--58e52c771aef--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@maxhilsdorf?source=post_page---byline--58e52c771aef--------------------------------" rel="noopener follow">Max Hilsdorf</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--58e52c771aef--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/9346d558a628e376261a4172b6e0566a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6tGUBqRG7-k-zsWe5c-tZg.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Title image generated with DALL-E 2 by the author.</figcaption></figure><p id="d30b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Labeled audio data is chronically scarce</strong> in Music AI. In this post, I will share some tips on building strong models under these circumstances.</p><p id="b7ab" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Compared to other fields like Computer Vision or Natural Language Processing (NLP), finding suitable public datasets for Music AI is often difficult. Whether you want to do mood recognition, noise detection, or instrument tagging: you will likely struggle to find the right data.</p><p id="0a65" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, data scarcity affects not only hobby programmers and students. Aspiring music tech startups and even established music companies have the exact same problem. In the age of AI, many are <strong class="ne fr">desperately trying to gather proprietary data assets</strong> for machine learning purposes.</p><p id="3a12" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With that said, let us dive into my <strong class="ne fr">top 3 tips to get more out of your music data</strong>.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6f9d" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Tip 1: Apply Natural Data Augmentation</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pc"><img src="../Images/0726f6e00bcddac17f509fd4881f3af7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M78bgH6beTDti1PNGHdn2g.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Banner generated with DALL-E 2 by the author.</figcaption></figure><p id="56fa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you are a data scientist, you have probably heard about data augmentation. The basic idea is to take existing examples in our dataset and alter them slightly to produce <strong class="ne fr">new synthetic training examples</strong>. This is best illustrated with images. For instance, if our dataset contains an image of a cat, we can easily create new synthetic cats by shifting and rotating the original cat image.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pd"><img src="../Images/ef948ce570253f52a6174d1a32ef452f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*2toouV0CZX-JLA8bkSwdCw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Example for image data augmentation. Image inspired by <a class="af pe" rel="noopener" target="_blank" href="/image-augmentation-for-deep-learning-histogram-equalization-a71387f609b2">Suki Lau</a> and recreated by the author using a cat photo by <a class="af pe" href="https://unsplash.com/de/@alxndr_london" rel="noopener ugc nofollow" target="_blank">Alexander London</a>.</figcaption></figure><p id="63f7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Data augmentation is particularly <strong class="ne fr">effective for smaller datasets</strong>. If your dataset has only 100 images of cats, the odds that all possible angles and rotations are represented properly are low. These blind spots in the dataset will automatically translate to blind spots in the AI’s perception and judgment. By synthetically creating alterations of existing images, we can mitigate this risk.</p><h2 id="9681" class="pf oh fq bf oi pg ph pi ol pj pk pl oo nl pm pn po np pp pq pr nt ps pt pu pv bk">Data Augmentation is Different in Music AI</h2><p id="17a3" class="pw-post-body-paragraph nc nd fq ne b go pw ng nh gr px nj nk nl py nn no np pz nr ns nt qa nv nw nx fj bk">While data augmentation is a game-changer in Computer Vision, it is <strong class="ne fr">less straightforward in Music A</strong>I. The most common input to Music AI models is the spectrogram (learn more <a class="af pe" rel="noopener" target="_blank" href="/getting-to-know-the-mel-spectrogram-31bca3e2d9d0">here</a>). But have you tried rotating and shifting a spectrogram?</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qb"><img src="../Images/3da1b59b1f06533b6fe538653047eed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*69HQp79lQUGY3lukGGJtmA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Example of ineffective data augmentation for audio data. Image by the author.</figcaption></figure><p id="bd0e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is easy to see that the same tricks from Computer Vision cannot be applied directly to music AI. But why is this example so ridiculous? The answer is that, in contrast to the cat example, this kind of augmentation is <strong class="ne fr">not natural</strong> for a spectrogram.</p><p id="1b3c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">An augmentation is natural when the changes made represent alterations that the model might encounter in <strong class="ne fr">real-world applications</strong>. While rotating a spectrogram certainly alters the data, visually, it is nonsensical and would never occur in practice. Instead, we need to find natural alterations specifically for music data.</p><h2 id="c1da" class="pf oh fq bf oi pg ph pi ol pj pk pl oo nl pm pn po np pp pq pr nt ps pt pu pv bk">Using Effects for Natural Audio Augmentation</h2><p id="24df" class="pw-post-body-paragraph nc nd fq ne b go pw ng nh gr px nj nk nl py nn no np pz nr ns nt qa nv nw nx fj bk">The most common natural music data augmentation involves applying effects to the audio signal. There are a bunch of effects that every musician knows from their DAW:</p><ul class=""><li id="6a31" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qc qd qe bk">Time stretching</li><li id="d88c" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx qc qd qe bk">Pitch shifting</li><li id="c25c" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx qc qd qe bk">Compressors, Limiters, Distortion</li><li id="a07a" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx qc qd qe bk">Reverb, Echo, Chorus</li><li id="c0be" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx qc qd qe bk">and many more…</li></ul><p id="d7c6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These effects can be applied to any piece of music, altering the data while preserving its main musical characteristics. If you want to know how to implement this in practice, <strong class="ne fr">check my article about this topic:</strong></p><div class="qk ql qm qn qo qp"><a rel="noopener follow" target="_blank" href="/natural-audio-data-augmentation-using-spotifys-pedalboard-212ea59d39ce?source=post_page-----58e52c771aef--------------------------------"><div class="qq ab ig"><div class="qr ab co cb qs qt"><h2 class="bf fr hw z io qu iq ir qv it iv fp bk">Natural Audio Data Augmentation Using Spotify’s Pedalboard</h2><div class="qw l"><h3 class="bf b hw z io qu iq ir qv it iv dx">With Ready-To-Use Python Code &amp; Presets</h3></div><div class="qx l"><p class="bf b dy z io qu iq ir qv it iv dx">towardsdatascience.com</p></div></div><div class="qy l"><div class="qz l ra rb rc qy rd lr qp"/></div></div></a></div><p id="7968" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Data augmentation is not only used in many Music AI research papers, but I have also had great results with it myself. When data is scarce, data augmentation can <strong class="ne fr">push your model from unusable to acceptable</strong>. Even with higher data volumes, it adds that extra bit of reliability that can be crucial in production.</p><p id="ff0c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When implementing music data augmentation in practice, it is important to keep these three things in mind:</p><ol class=""><li id="e6cd" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx re qd qe bk"><strong class="ne fr">Stay natural.</strong> Listen to your data after augmentation and make sure it still sounds natural. Otherwise, your model might learn false patterns.</li><li id="4939" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx re qd qe bk"><strong class="ne fr">Not every training example should be augmented.</strong> To make sure that your model primarily learns from real, unaltered music, augmented examples should only be a portion of your training data (20–30%). You can also use sample weighting during training to adjust the impact of your augmented examples on the model.</li><li id="3337" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx re qd qe bk"><strong class="ne fr">Don’t augment your validation and test data.</strong> Augmentations help the model learn generalizable patterns. Your validation and test data should be unaltered to enable accurate benchmarks on real examples.</li></ol><p id="814f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Time to boost your model effectiveness with data augmentation!</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="c101" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Tip 2: Use Smaller Models and Input Data</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/5046d53a4c179647371dd557ac835595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PA957LMB-TpNt_a-JqcpeQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Banner generated with DALL-E 2 by the author.</figcaption></figure><h2 id="b383" class="pf oh fq bf oi pg ph pi ol pj pk pl oo nl pm pn po np pp pq pr nt ps pt pu pv bk">Bigger = Better?</h2><p id="0b4e" class="pw-post-body-paragraph nc nd fq ne b go pw ng nh gr px nj nk nl py nn no np pz nr ns nt qa nv nw nx fj bk">In AI, bigger is often better — if there is enough data to feed these large models. However, with limited data, <strong class="ne fr">bigger models are more prone to overfitting</strong>. Overfitting occurs when the model memorizes patterns from the training data that do not generalize well to real-world data examples. But there is another way to approach this that I find even more compelling in this context.</p><p id="0c5a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Suppose you have a small dataset of spectrograms and are deciding between a small CNN model (100k parameters) or a large CNN (10 million parameters). Remember that <strong class="ne fr">every model parameter is effectively a best-guess number derived from the training dataset</strong>. If we think of it this way, it is obvious that it is easier for a model to get 100k parameters right than it is to nail 10 million.</p><p id="7480" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the end, both arguments lead to the same conclusion:</p><blockquote class="rg"><p id="684f" class="rh ri fq bf rj rk rl rm rn ro rp nx dx">If data is scarce, consider building smaller models that focus only on the essential patterns.</p></blockquote><p id="20d3" class="pw-post-body-paragraph nc nd fq ne b go rq ng nh gr rr nj nk nl rs nn no np rt nr ns nt ru nv nw nx fj bk">But how can we achieve smaller models in practice?</p><h2 id="aa7f" class="pf oh fq bf oi pg ph pi ol pj pk pl oo nl pm pn po np pp pq pr nt ps pt pu pv bk">Don’t Crack Walnuts with a Sledgehammer</h2><p id="1b94" class="pw-post-body-paragraph nc nd fq ne b go pw ng nh gr px nj nk nl py nn no np pz nr ns nt qa nv nw nx fj bk">My learning journey in Music AI has been dominated by deep learning. Up until a year ago, I had solved almost every problem using large neural networks. While this makes sense for complex tasks like music tagging or instrument recognition, <strong class="ne fr">not every task is that complicated</strong>.</p><p id="814b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For instance, a decent BPM estimator or key detector can be built without any machine learning by analyzing the time between onsets or by correlating chromagrams with key profiles, respectively.</p><p id="ac68" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Even for tasks like music tagging, it doesn’t always have to be a deep learning model. I’ve achieved good results in mood tagging through a simple K-Nearest Neighbor classifier over an embedding space (e.g. CLAP).</p><p id="65ae" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While most state-of-the-art methods in Music AI are based on deep learning, <strong class="ne fr">alternative solutions should be considered under data scarcity</strong>.</p><h2 id="e3f7" class="pf oh fq bf oi pg ph pi ol pj pk pl oo nl pm pn po np pp pq pr nt ps pt pu pv bk">Pay Attention to the Data Input Size</h2><p id="2af7" class="pw-post-body-paragraph nc nd fq ne b go pw ng nh gr px nj nk nl py nn no np pz nr ns nt qa nv nw nx fj bk">More important than the choice of models is usually the choice of input data. In Music AI, we rarely use raw waveforms as input due to their data inefficiency. By transforming waveforms into (mel)spectrograms, we can decrease the input data dimensionality <strong class="ne fr">by a factor of 100 or more</strong>. This matters because large data inputs typically require larger and/or more complex models to process them.</p><p id="fa09" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To minimize the size of the model input, we can take two routes</p><ol class=""><li id="be5d" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx re qd qe bk"><strong class="ne fr">Using smaller music snippets</strong></li><li id="f82e" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx re qd qe bk"><strong class="ne fr">Using more compressed/simplified music representations.</strong></li></ol><h2 id="1f6d" class="pf oh fq bf oi pg ph pi ol pj pk pl oo nl pm pn po np pp pq pr nt ps pt pu pv bk">Using Smaller Music Snippets</h2><p id="2301" class="pw-post-body-paragraph nc nd fq ne b go pw ng nh gr px nj nk nl py nn no np pz nr ns nt qa nv nw nx fj bk">Using smaller music snippets is especially effective if the outcome we are interested in is global, i.e. applies to every section of the song. For example, we can assume that the genre of a track remains relatively stable over the course of the track. Because of that, we can easily use 10-second snippets instead of full tracks (or the very common 30-second snippets) for a genre classification task.</p><p id="8803" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This has two advantages:</p><ol class=""><li id="88e3" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx re qd qe bk">Shorter snippets result in fewer data points per training example, allowing you to use smaller models.</li><li id="6020" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx re qd qe bk">By drawing three 10-second snippets instead of one 30-second snippet, we can triple the number of training observations. All in all, this means that we can build less data-hungry models and, at the same time, feed them more training examples than before.</li></ol><p id="1b18" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, there are <strong class="ne fr">two potential dangers here</strong>. Firstly, the snippet size must be long enough so that a classification is possible. For example, even humans struggle with genre classification when presented with 3-second snippets. We should choose the snippet size carefully and view this decision as a hyperparameter of our AI solution.</p><p id="3985" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Secondly, <strong class="ne fr">not every musical attribute is global</strong>. For example, if a song features vocals, this doesn’t mean that there are no instrumental sections. If we cut the track into really short snippets, we would introduce many falsely-labelled examples into our training dataset.</p><h2 id="1394" class="pf oh fq bf oi pg ph pi ol pj pk pl oo nl pm pn po np pp pq pr nt ps pt pu pv bk">Using More Efficient Music Representations</h2><p id="f475" class="pw-post-body-paragraph nc nd fq ne b go pw ng nh gr px nj nk nl py nn no np pz nr ns nt qa nv nw nx fj bk">If you studied Music AI ten years ago (back when all of this was called “Music Information Retrieval”), you learned about chromagrams, MFCCs, and beat histograms. These handcrafted features were designed to make music data work with traditional ML approaches. With the rise of deep learning, it might seem like these features have been <strong class="ne fr">entirely replaced by (mel)spectrograms</strong>.</p><p id="5c65" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Spectrograms compress music into images without much information loss, making them <strong class="ne fr">ideal in combination with computer vision models</strong>. Instead of engineering custom features for different tasks, we can now use the same input data representation and model for most Music AI problems — provided you have tens of thousands of training examples to feed these models with.</p><p id="5ea5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When data is scarce, we want to <strong class="ne fr">compress the information as much as possible</strong> to make it easier for the model to extract relevant patterns from the data. Consider these four music representations below and tell me which one helps you identify the musical key the fastest.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rv"><img src="../Images/55215645b179c94abd112faf5942cbc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FgOGcTeG5ZQpraCNnT5n9w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Examples of four different representations of the same song (“Honky Tonk Woman” by Tina Turner). Although the chromagram is roughly 700k smaller than the waveform, it lets us identify the key much more effectively (C# major). Image created by the author.</figcaption></figure><p id="56ce" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While mel spectrograms can be used as an input for key detection systems (and possibly should be if you have enough data), a simple chromagram averaged along the time dimension reveals this specific information much quicker. That is why spectrograms require complex models like CNNs while a chromagram can be easily analyzed by traditional models like logistic regression or decision trees.</p><p id="e303" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">In summary</strong>, the established spectrogram + CNN combination remains highly effective for many problems, provided you have enough data. However, with smaller datasets, it might make sense to revisit some feature engineering techniques from MIR or develop your own task-specific representations.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="1700" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Tip 3: Leverage Pretrained Models or Embeddings</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rw"><img src="../Images/fa31b750c204546c8248df9e884c5e9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*URLs9DRJcrmS_vElsFTVdg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Banner generated with DALL-E 2 by the author.</figcaption></figure><p id="5db5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When data is scarce, one of the most effective strategies is to leverage pretrained models or embeddings. This approach allows you to <strong class="ne fr">build upon existing knowledge</strong> from models that have been trained on large datasets, thereby mitigating the limitations of your smaller dataset.</p><h2 id="6e84" class="pf oh fq bf oi pg ph pi ol pj pk pl oo nl pm pn po np pp pq pr nt ps pt pu pv bk">Why Use Pretrained Models?</h2><p id="5b82" class="pw-post-body-paragraph nc nd fq ne b go pw ng nh gr px nj nk nl py nn no np pz nr ns nt qa nv nw nx fj bk">Pretrained models have already learned to identify and extract meaningful features from their training data. For instance, a model trained on genre classification has likely learned a variety of <strong class="ne fr">meaningful musical patterns</strong> during training. If we now want to build our own mood tagging model, it might make sense to use the pretrained genre model <strong class="ne fr">as a starting point</strong>.</p><p id="9322" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If the pretrained model was trained on a similar task, you can transfer their learned representations to your specific task. This process is known as <em class="rx">transfer learning</em>. Transfer learning can drastically reduce the amount of data and computational resources needed to train your own model from scratch.</p><h2 id="6cea" class="pf oh fq bf oi pg ph pi ol pj pk pl oo nl pm pn po np pp pq pr nt ps pt pu pv bk">Popular Pretrained Models in Music AI</h2><p id="6abe" class="pw-post-body-paragraph nc nd fq ne b go pw ng nh gr px nj nk nl py nn no np pz nr ns nt qa nv nw nx fj bk">A few years ago, the most common approach was to take pretrained models like genre classifiers and finetune them on specific tasks. Models like <a class="af pe" href="https://github.com/jordipons/musicnn" rel="noopener ugc nofollow" target="_blank">MusiCNN</a> were commonly used for this.</p><p id="83ce" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, nowadays, it is more common to use pretrained models that were specifically trained to yield meaningful music embeddings, i.e. vector representations of songs. Here are three pretrained embedding models that are commonly used:</p><ol class=""><li id="aaf8" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx re qd qe bk"><a class="af pe" href="https://huggingface.co/m-a-p/MERT-v1-330M" rel="noopener ugc nofollow" target="_blank">Mert-v1</a> by m-a-p</li><li id="d945" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx re qd qe bk"><a class="af pe" href="https://huggingface.co/laion/clap-htsat-unfused" rel="noopener ugc nofollow" target="_blank">CLAP</a> by LAION</li><li id="882c" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx re qd qe bk"><a class="af pe" href="https://github.com/microsoft/CLAP" rel="noopener ugc nofollow" target="_blank">CLAP</a> by Microsoft</li></ol><p id="f1fc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">From my personal experience, I’ve had the best results using Microsoft’s CLAP for transfer learning and LAION CLAP for similarity search.</p><h2 id="5af8" class="pf oh fq bf oi pg ph pi ol pj pk pl oo nl pm pn po np pp pq pr nt ps pt pu pv bk">Different Ways to Leverage Pretrained Models</h2><p id="a5d1" class="pw-post-body-paragraph nc nd fq ne b go pw ng nh gr px nj nk nl py nn no np pz nr ns nt qa nv nw nx fj bk">Pretrained models can be used in a variety of ways:</p><ol class=""><li id="e54b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx re qd qe bk"><strong class="ne fr">Full Fine-Tuning:</strong> Use a pretrained classification or embedding model and fine-tune it on a smaller dataset of task-specific data. This method often achieves optimal results, if you can afford to use the full, large model for training and inference and know how to implement it.</li><li id="b537" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx re qd qe bk"><strong class="ne fr">Embeddings as Input Features</strong>: A more resource-efficient approach can be to extract embeddings from a pretrained model to use them as inputs for a new, much smaller model. As these embeddings are often 500–1000 dimensional vectors, a smaller neural network with a few thousand parameters can be attached to fine-tune more efficiently. For smaller datasets, this method is <strong class="ne fr">usually preferred over a full tine-tune</strong>.</li><li id="ae07" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx re qd qe bk"><strong class="ne fr">Using Embeddings Directly</strong>: Even without any fine-tuning, embeddings can be used directly. For instance, embeddings from pretrained models are commonly used for music similarity search. CLAP models can even be used for text-to-music retrieval or (although still rather poorly) for zero-shot classification, i.e. classification without training.</li></ol><p id="436f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Leveraging embeddings from pretrained models can significantly enhance your Music AI projects. By building on the learned pattern-recognition of these models, you <strong class="ne fr">avoid reinventing the wheel</strong>. When data is scarce, pretrained models should always be considered.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9a12" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Conclusion</h1><p id="eeb4" class="pw-post-body-paragraph nc nd fq ne b go pw ng nh gr px nj nk nl py nn no np pz nr ns nt qa nv nw nx fj bk">Don’t let data scarcity hold you back! Many use cases that required hundreds of thousands of training examples a few years ago have now essentially become commodities.</p><p id="7b66" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To achieve robust performance with small datasets, your number one priority should be not to waste any of your valuable data. Let’s review the main points from this article:</p><ol class=""><li id="332f" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx re qd qe bk"><strong class="ne fr">Data augmentation</strong> is a great way to let your models learn from training examples several times with small but effective variations, increasing robustness.</li><li id="af31" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx re qd qe bk"><strong class="ne fr">Smaller models and more efficient data representations</strong> force your model to focus on the most important, underlying patterns in the data, avoiding overfitting.</li><li id="6209" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx re qd qe bk"><strong class="ne fr">Pretrained models</strong> allow you to borrow some of the intelligence from larger AI systems through fine-tuning. No reason to train from scratch anymore!</li></ol><p id="fc89" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Of course, there are natural limitations to what you can achieve with small datasets. If you have 100 labeled tracks and your goal is to build a multi-label genre classifier with 10 genres and 30 subgenres, you will not get very far — even if you use all of my tricks.</p><p id="b76d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Still, I’ve developed surprisingly capable genre &amp; mood classifiers with as little as 1000 labeled songs. Only 2 years ago, achieving this with such a small dataset would have been impossible. These democratization effects are one of the most exciting aspects of the current AI hype, in my opinion.</p><blockquote class="rg"><p id="e3ac" class="rh ri fq bf rj rk rl rm rn ro rp nx dx">If you have a small but high-quality music dataset and are considering using it for machine learning, now is the best time to give it a try!</p></blockquote></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="158e" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Interested in Music AI?</h1><p id="0de8" class="pw-post-body-paragraph nc nd fq ne b go pw ng nh gr px nj nk nl py nn no np pz nr ns nt qa nv nw nx fj bk">If you liked this article, you might want to check out some of my other work:</p><ul class=""><li id="55d9" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qc qd qe bk"><a class="af pe" href="https://medium.com/towards-data-science/3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd" rel="noopener">“3 Music AI Breakthroughs to Expect in 2024”</a>. Medium Blog</li><li id="b13c" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx qc qd qe bk"><a class="af pe" href="https://www.youtube.com/watch?v=fr5PlnNGKVw" rel="noopener ugc nofollow" target="_blank">“The Human Element in a World of Generative AI Music”</a>. Video interview on the Audiosocket podcast</li><li id="be75" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx qc qd qe bk">“<a class="af pe" href="https://medium.com/towards-data-science/how-google-used-your-data-to-improve-their-music-ai-8948a1e85491" rel="noopener">How Google Used Your Data to Improve their Music AI</a>”. Medium Blog</li></ul><p id="6a66" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can also follow me on <a class="af pe" href="https://www.linkedin.com/in/max-hilsdorf/" rel="noopener ugc nofollow" target="_blank">Linkedin</a> to stay updated about new papers and trends in Music AI.</p><p id="cc4f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Thanks for reading this article!</strong></p></div></div></div></div>    
</body>
</html>