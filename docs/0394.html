<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>LingoNaut Language Assistant</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>LingoNaut Language Assistant</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lingonaut-language-assistant-6abe3e8b045c?source=collection_archive---------3-----------------------#2024-02-11">https://towardsdatascience.com/lingonaut-language-assistant-6abe3e8b045c?source=collection_archive---------3-----------------------#2024-02-11</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="4677" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Multilingual Learning with an Ollama-Python Walkie-Talkie</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://natecibik.medium.com/?source=post_page---byline--6abe3e8b045c--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Nate Cibik" class="l ep by dd de cx" src="../Images/008c22b715ddf4f1d0f9970142edc09f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*TlTicsMyk_8gZjhet5H3KQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6abe3e8b045c--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://natecibik.medium.com/?source=post_page---byline--6abe3e8b045c--------------------------------" rel="noopener follow">Nate Cibik</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6abe3e8b045c--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 11, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/f1da3ce8bd54bd19ae6421399d27b428.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uxOckzPs6zbkuQZM80C7NA.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author using DALL-E 3.</figcaption></figure><p id="8b0b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We live in an era where the world is truly placed at our fingertips, if we know where to look. Today’s open-source large language models (LLMs) are potent and compact enough to place reasonably complete collections of human knowledge on standard consumer hardware, available for countless hours of ad-free, in-depth discussion on innumerable subjects without requiring an internet connection. Thanks to the dedicated efforts of the open-source community, tools like <a class="af nx" href="https://ollama.ai/" rel="noopener ugc nofollow" target="_blank">Ollama</a> now allow us to serve high-quality quantized versions of today’s top models locally and interact with them using streamlined APIs.</p><p id="1d5e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This ease of development means we can spend less time thinking about <em class="ny">how</em> we might build LLM applications, and focus more on <em class="ny">what</em> we would like to build. Personally, I have always wanted to learn multiple languages, but never had the conditions to properly practice such a thing, as my life experience has not yet included a lot of time in multilingual environments, and attempts at touristic language learning can be embarrassing when we don’t have close friends to practice with, since we are forced to experiment on strangers.</p><p id="3489" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This a promising opportunity for employing the open-ended dialogue capabilities of LLM-based chat bots. Since locally serving quantized open-source LLMs on consumer hardware is now a well-oiled machine, the only other ingredient we need to make this vision into a reality is to augment the interaction into a speech-to-speech format. Again, the bounties of open-source research avail themselves to us. High-quality speech-to-text and text-to-speech models are on the shelf, wrapped in their own intuitive APIs.</p><p id="ae72" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">A perfect demonstration of how these tools can make exciting concepts come to life with ease is LingoNaut: a multilingual language assistant that runs from a single Python script containing just 300 lines of code. Using a combination of OpenAI’s <a class="af nx" href="https://github.com/openai/whisper" rel="noopener ugc nofollow" target="_blank">Whisper</a> speech-to-text model, a local Ollama server, and the Coqui.ai <a class="af nx" href="https://github.com/coqui-ai/TTS" rel="noopener ugc nofollow" target="_blank">TTS</a> text-to-speech library, we can construct a user-friendly walkie-talkie interface with a wide selection of LLMs. From there, it is just a matter of system prompt engineering (easily done with <a class="af nx" href="https://github.com/ollama/ollama-python" rel="noopener ugc nofollow" target="_blank">ollama-python</a>) to beckon our desired behavior from the LLM, in this case creating a helpful multilingual language tutor. Indeed, this means the LingoNaut code can easily be adapted to create a wide range of AI assistants by just adjusting the LLM and system prompt being used.</p><figure class="ml mm mn mo mp mq"><div class="nz io l ed"><div class="oa ob l"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Demonstration of LingoNaut app.</figcaption></figure><p id="4326" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Code for running LingoNaut is available in the <a class="af nx" href="https://github.com/FoamoftheSea/lingonaut-python" rel="noopener ugc nofollow" target="_blank">GitHub repo</a> with easy instructions for installation. LingoNaut is an open-source project, and contribution is welcomed. For example, future work could involve wrapping the backend in a more sophisticated web UI to allow remote hosting of walkie-talkie LLM apps, which could lead to supporting mobile devices. I hope that LingoNaut is a fun and helpful resource for others on their learning journeys, and that the code is useful as a lightweight skeleton for engineers sandboxing new ideas for LLM- or LMM-based applications.</p><p id="376c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The rest of this article provides an overview of the Python code that runs LingoNaut, the toolbox of open-source components that makes the ready assembly of a tool like LingoNaut possible, and promising directions for future work. With some clever Python coding tricks, brilliant speech-to-text and text-to-speech models, and the local deployment of quantized LLMs on consumer machines, we can easily construct a speech-to-speech pipeline to unlock use cases that are less conducive to the confines of text, such as language learning. The ingenious contributions of the research community combine to provide us this extraordinary set of possibilities. Let’s walk through each one in more detail to understand the roles they play in bringing the LingoNaut app to life.</p></div></div></div><div class="ab cb oc od oe of" role="separator"><span class="og by bm oh oi oj"/><span class="og by bm oh oi oj"/><span class="og by bm oh oi"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="291c" class="ok ol fq bf om on oo gq op oq or gt os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">LingoNaut Code</h1><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pg"><img src="../Images/bcee7b55af34c1960546d4f1a9850421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3847C7JCKm2clcG_aMDFvg.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author using DALL-E 3.</figcaption></figure><h2 id="9fb5" class="ph ol fq bf om pi pj pk op pl pm pn os nk po pp pq no pr ps pt ns pu pv pw px bk">Walkie-Talkie Interface</h2><p id="97da" class="pw-post-body-paragraph nb nc fq nd b go py nf ng gr pz ni nj nk qa nm nn no qb nq nr ns qc nu nv nw fj bk">The code in LingoNaut creates a handy terminal-based speech-to-speech app for use with Ollama which can be easily adapted for new use cases. Using a package called <a class="af nx" href="https://pypi.org/project/pynput/" rel="noopener ugc nofollow" target="_blank">pynput</a>, we can create a keyboard listener object that runs in a background thread and reacts to keystrokes from the user. This opens up a wide array of options for apps that run in continuous loops, most importantly in this case providing a control for triggering and terminating the user audio recording without the need for a graphic user interface with buttons. This way, an interactive app can be run directly from the terminal, easing many engineering concerns.</p><p id="ef81" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In LingoNaut, different speech-to-text models can be deployed by pressing different keys to record audio. While lightweight Whisper models excel at quickly processing English audio, a larger and slower model must be used for accurate on-the-fly multilingual transcription. Thus, the user may choose to hold the <strong class="nd fr">Ctrl</strong> key to use a lightweight base model for asking questions in English, or hold the <strong class="nd fr">Shift</strong> key to speak in other languages.</p><p id="ae76" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Other useful LingoNaut features enabled by the keyboard listener are the ability to interrupt the model response with the <strong class="nd fr">End</strong> key when you’ve given it a bad input or are not satisfied with the direction of the response. This prevents getting stuck waiting for irrelevant text and audio to finish coming through so that the user can stay more engaged. The user can also lock the keyboard inputs using the <strong class="nd fr">F2</strong> key so they can leave a session open for later without worrying about triggering the audio recording accidentally.</p><h2 id="a094" class="ph ol fq bf om pi pj pk op pl pm pn os nk po pp pq no pr ps pt ns pu pv pw px bk">Concurrency</h2><p id="9504" class="pw-post-body-paragraph nb nc fq nd b go py nf ng gr pz ni nj nk qa nm nn no qb nq nr ns qc nu nv nw fj bk">While the packages used in LingoNaut provide streamlined APIs for interfacing with the three models used to create the speech-to-speech pipeline, naively waiting for the LLM to generate text, transcribing it to audio, and then playing it to the user in series would be a very slow experience. The streaming of text chunks from the LLM, the transcription of text chunks into audio files, and the playback of previously transcribed audio files can all happen concurrently, so LingoNaut uses a separate thread for each of these tasks. By using ThreadPoolExecutor objects with max_workers set to 1, we can easily open new threads for task submissions while guaranteeing those tasks will be executed in order, allowing us to outsource work from the main thread without having overlapping or shuffled returns. A basic code outline for this arrangement is shown below:</p><pre class="ml mm mn mo mp qd qe qf bp qg bb bk"><span id="d27e" class="qh ol fq qe b bg qi qj l qk ql">import ollama<br/>from concurrent import ThreadPoolExecutor<br/><br/>def play_audio(file_path: str):<br/>    # audio file playback code<br/><br/>def dump_to_audio(text: str, file_path: str):<br/>    # transcribe text-to-speech and save audio file<br/><br/>def process_stream(chat_history: list):<br/>    stream = ollama.chat(<br/>        model='mistral:lingonaut',<br/>        messages=chat_history,<br/>        stream=True,<br/>    )<br/><br/>    with ThreadPoolExecutor(max_workers=1) as play_pool:<br/>        with ThreadPoolExecutor(max_workers=1) as tts_pool:<br/>            def play_output(text, file_path):<br/>                output_path = dump_to_audio(text, file_path)<br/>                play_pool.submit(play_audio, output_path)<br/>                return<br/><br/>            def process_section(text, file_path):<br/>                tts_pool.submit(play_output, text, file_path)<br/>                return current_string<br/><br/>            current_section = ""<br/>            for i, chunk in enumerate(stream):<br/>                current_path = f"{i}.wav"<br/>                text_chunk = chunk['message']['content']<br/>                current_section += text_chunk<br/>                if len(current_section) &gt; 50:<br/>                    process_section(current_section, current_path)<br/>                    current_section = ""<br/><br/>            tts_pool.shutdown(wait=True)<br/>            play_pool.shutdown(wait=True)</span></pre><h2 id="b7a4" class="ph ol fq bf om pi pj pk op pl pm pn os nk po pp pq no pr ps pt ns pu pv pw px bk">Model Customization</h2><p id="91f5" class="pw-post-body-paragraph nb nc fq nd b go py nf ng gr pz ni nj nk qa nm nn no qb nq nr ns qc nu nv nw fj bk">The <a class="af nx" href="https://github.com/ollama/ollama-python" rel="noopener ugc nofollow" target="_blank">ollama-python</a> package has an easy tool for creating custom tagged model configurations using “Modelfiles” to guide the LLM behavior. In the case of LingoNaut, the 4-bit quantized <a class="af nx" href="https://mistral.ai/news/announcing-mistral-7b/" rel="noopener ugc nofollow" target="_blank">Mistral 7B</a> model in the Ollama library was customized with an explicit system prompt to guide the desired behavior as a language learning assistant. The prompt can be found in the repository in the <a class="af nx" href="https://github.com/FoamoftheSea/lingonaut-python/blob/main/create_lingonaut_ollama.py" rel="noopener ugc nofollow" target="_blank">create_lingonaut_ollama.py</a> file, and it should be noted that this is the only file that customizes the model selection and behavior in the repo, meaning that this repo can be instantly converted into any other walkie-talkie LLM application of your choosing by creating a tagged model using a different Modelfile. The LLM being used can also easily be swapped for larger or smaller models based on available resources.</p></div></div></div><div class="ab cb oc od oe of" role="separator"><span class="og by bm oh oi oj"/><span class="og by bm oh oi oj"/><span class="og by bm oh oi"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="93a5" class="ok ol fq bf om on oo gq op oq or gt os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">Speech-to-Speech Toolbox</h1><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pg"><img src="../Images/52b9aa9851a7f168490da2c147be7c1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SKU5UVlzZWNZcbcj6bIKFw.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author using DALL-E 3.</figcaption></figure><h2 id="d65a" class="ph ol fq bf om pi pj pk op pl pm pn os nk po pp pq no pr ps pt ns pu pv pw px bk">Whisper</h2><p id="2bf1" class="pw-post-body-paragraph nb nc fq nd b go py nf ng gr pz ni nj nk qa nm nn no qb nq nr ns qc nu nv nw fj bk"><a class="af nx" href="https://openai.com/research/whisper" rel="noopener ugc nofollow" target="_blank">Whisper</a> is an open-source speech-to-text model provided by OpenAI. There are five model sizes available in both English-focused and multilingual varieties to choose from, depending on the complexity of the application and desired accuracy-efficiency tradeoff. Whisper is an end-to-end speech-to-text framework that uses an encoder-decoder transformer architecture operating on input audio split into 30-second chunks and converted into a log-Mel spectrogram. The network is trained on multiple speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qm"><img src="../Images/b563e9edc7a8d82fd8b1229c45e5b42d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePuQ8_hUQEZC4nsc1LWoFg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Diagram of Whisper architecture from the <a class="af nx" href="https://cdn.openai.com/papers/whisper.pdf" rel="noopener ugc nofollow" target="_blank">research paper</a>.</figcaption></figure><p id="0705" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For this project, two walkie-talkie buttons are available to the user: one which sends their general English-language questions to the bot through the lighter, faster “base” model, and a second which deploys the larger “medium” multilingual model that can distinguish between dozens of languages and accurately transcribe correctly pronounced statements. In the context of language learning, this leads the user to focus very intently on their pronunciation, accelerating the learning process. A chart of the available Whisper models is shown below:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qn"><img src="../Images/da85d960fe89c2ae302f7564b1d955d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-AbOy6DpJ3kHuxvN0uLJ-w.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Chart from <a class="af nx" href="https://github.com/openai/whisper" rel="noopener ugc nofollow" target="_blank">https://github.com/openai/whisper</a></figcaption></figure><h2 id="eb19" class="ph ol fq bf om pi pj pk op pl pm pn os nk po pp pq no pr ps pt ns pu pv pw px bk">Ollama</h2><p id="1260" class="pw-post-body-paragraph nb nc fq nd b go py nf ng gr pz ni nj nk qa nm nn no qb nq nr ns qc nu nv nw fj bk">There exists a variety of highly useful open-source language model interfaces, all catering to different use cases with varying levels of complexity for setup and use. Among the most widely known are the <a class="af nx" href="https://github.com/oobabooga/text-generation-webui" rel="noopener ugc nofollow" target="_blank">oobabooga text-gen webui</a>, with arguably the most flexibility and under-the-hood control, <a class="af nx" href="https://github.com/ggerganov/llama.cpp" rel="noopener ugc nofollow" target="_blank">llama.cpp</a>, which originally focused on optimized deployment of quantized models on smaller CPU-only devices but has since expanded to serving other hardware types, and the streamlined interface chosen for this project (built on top of llama.cpp): <a class="af nx" href="https://ollama.ai/" rel="noopener ugc nofollow" target="_blank">Ollama</a>.</p><p id="b7a6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Ollama focuses on simplicity and efficiency, running in the background and capable of serving multiple models simultaneously on small hardware, quickly shifting models in and out of memory as needed to serve their requests. Instead of focusing on lower-level tools like fine-tuning, Ollama excels at <a class="af nx" href="https://ollama.com/download/linux" rel="noopener ugc nofollow" target="_blank">simple installation</a>, efficient runtime, a great <a class="af nx" href="https://ollama.com/library" rel="noopener ugc nofollow" target="_blank">spread of ready-to-use models</a>, and <a class="af nx" href="https://github.com/ollama/ollama/blob/main/docs/import.md" rel="noopener ugc nofollow" target="_blank">tools for importing pretrained model weights</a>. The focus on efficiency and simplicity makes Ollama the natural choice for LLM interface in a project like LingoNaut, since the user does not need to remember to close their session to free up resources, as Ollama will automatically manage this in the background when the app is not in use. Further, the ready access to performant, quantized models in the library is perfect for frictionless development of LLM applications like LingoNaut.</p><p id="0b66" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">While Ollama is not technically built for Windows, it is easy for Windows users to install it on Windows Subsystem for Linux (<a class="af nx" href="https://learn.microsoft.com/en-us/windows/wsl/install" rel="noopener ugc nofollow" target="_blank">WSL</a>), then communicate with the server from their Windows applications. With WSL installed, open a Linux terminal and enter the one-liner Ollama <a class="af nx" href="https://ollama.ai/download/linux" rel="noopener ugc nofollow" target="_blank">installation command</a>. Once the installation finishes, simply run “ollama serve” in the Linux terminal, and you can then communicate with your Ollama server from any Python script on your Windows machine.</p><h2 id="fa18" class="ph ol fq bf om pi pj pk op pl pm pn os nk po pp pq no pr ps pt ns pu pv pw px bk">Coqui.ai 🐸 TTS</h2><p id="8f0d" class="pw-post-body-paragraph nb nc fq nd b go py nf ng gr pz ni nj nk qa nm nn no qb nq nr ns qc nu nv nw fj bk"><a class="af nx" href="https://github.com/coqui-ai/TTS" rel="noopener ugc nofollow" target="_blank">TTS</a> is a fully-loaded text-to-speech library available for non-commercial use, with paid commercial licenses available. The library has experienced notable popularity, with 3k forks and 26.6k stars on GitHub as of the time of this writing, and it’s clear why: the library works like the Ollama of the text-to-speech space, providing a unified interface for accessing a diverse array of performant models which cover a variety of use cases (for example: providing a multi-speaker, multilingual model for this project), exciting features such as voice cloning, and controls over the speed and emotional tone of transcriptions.</p><p id="e59f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The TTS library provides an extensive selection of text-to-speech models, including the illustrious Fairseq models from Facebook research’s Massively Multilingual Speech (<a class="af nx" href="https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/" rel="noopener ugc nofollow" target="_blank">MMS</a>) project. For LingoNaut, the Coqui.ai team’s own <a class="af nx" href="https://coqui.ai/blog/tts/open_xtts" rel="noopener ugc nofollow" target="_blank">XTTS</a> model turned out to be the correct choice, as it generates high-quality speech in multiple languages seamlessly. Although the model does have a “language” input parameter, I found that even leaving this set to “en” for English and simply passing text in other languages still results in faithful multilingual generation with mostly correct pronunciations.</p></div></div></div><div class="ab cb oc od oe of" role="separator"><span class="og by bm oh oi oj"/><span class="og by bm oh oi oj"/><span class="og by bm oh oi"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3660" class="ok ol fq bf om on oo gq op oq or gt os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">Conclusion</h1><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pg"><img src="../Images/4616ce07e69915d1d0e25eef8f897048.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EN0QqFxX1Bka4z_XMSk07w.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author using DALL-E 3.</figcaption></figure><p id="5b31" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In this article, I’ve introduced a new speech-to-speech multilingual language learning assistant called LingoNaut. The app runs through the terminal using a light and easily adaptable Python script with a walkie-talkie keyboard interface. This completely free and locally hosted app allows the user to practice a large variety of languages using AI, becoming confident in new languages without having to practice on strangers before they are ready. The code is available on <a class="af nx" href="https://github.com/FoamoftheSea/lingonaut-python" rel="noopener ugc nofollow" target="_blank">GitHub</a> with quick setup instructions, and is easily extensible to new use cases. I hope that the community finds the app helpful in their language learning endeavors, and that the code serves as a handy lightweight framework for future proof-of-concepts. LingoNaut is open-source, and contributions are welcomed.</p><h2 id="411c" class="ph ol fq bf om pi pj pk op pl pm pn os nk po pp pq no pr ps pt ns pu pv pw px bk">Future Work</h2><p id="b930" class="pw-post-body-paragraph nb nc fq nd b go py nf ng gr pz ni nj nk qa nm nn no qb nq nr ns qc nu nv nw fj bk">This work built a speech-to-speech pipeline by combining the text-based conversational ability of a LLM with separate speech-to-text and text-to-speech models on the input and output sides, respectively. Such a design is clunky and prone to cascading error, so it is therefore inferior to using a truly multimodal language model that could understand and generate both audio and text tokens from a unified representation space. When we encode audio to text before passing it to the model, we remove all of the tonal information contained in the audio, including pronunciation and emotional delivery, which significantly limits how advanced our language assistant can be. By instead using an LMM that operates on a joint multimodal representation space, we would retain the nuanced tonal information in the user inputs. Similarly, encoding text-to-speech on the output side is another significant information bottleneck, and the interaction will not be as natural.</p><p id="33cb" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The authors of <a class="af nx" href="https://next-gpt.github.io/" rel="noopener ugc nofollow" target="_blank">NeXT-GPT</a> provide a promising framework for using pretrained LLMs to create large multimodal models (LMMs) that can operate in a unified multimodal representation space, and this is a promising direction for speech-to-speech apps. With some effort, it is likely that the released NeXT-GPT weights could be imported into Ollama for experimentation. Their experiment used a similarly sized Vicuna 7B LLM, establishing that lightweight LLMs can work on multimodal spaces. While the Vicuna model is not advertised as a multilingual model, neither is the Mistral 7B model used in this LingoNaut experiment, though it still seems to work quite well for the purpose. Ideally, a fine-tuned multilingual instruction-tuned model would be the best choice for LingoNaut. To that end, a well-chosen dataset and low-rank adaptation (<a class="af nx" href="https://arxiv.org/abs/2106.09685" rel="noopener ugc nofollow" target="_blank">LoRA</a>) would lead to likely success. Further, the parameter efficient multimodal alignment with lightweight adapters demonstrated by <a class="af nx" href="https://arxiv.org/abs/2305.15023" rel="noopener ugc nofollow" target="_blank">LaVIN</a> offers to make NeXT-GPT-style LMM development more attainable with limited resources. A first step would be to investigate aligning the representations of a high-quality audio encoder with the LLM using LaVIN’s “cheap and quick” Mixture-of-Modality Adaptation (MMA) training strategy, relieving the speech-to-text bottleneck on the input side. Then the next step would be to investigate enabling multimodal output using the NeXT-GPT-style modality-switching instruction tuning (MoSIT).</p><p id="0a1e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Finally, as mentioned in the introduction, building a web UI frontend which could communicate with a remote backend would expand the fun of LingoNaut considerably, as larger models could be deployed on rented cloud GPUs, and communicated with via https requests from laptops and mobile devices. This would allow the community to build any walkie-talkie LLM app of their imagination that could be used their by friends and family anywhere an internet connection is available, and turn the vision of a universally accessible language learning assistant into a reality.</p></div></div></div></div>    
</body>
</html>