["```py\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatCompletionRequest(BaseModel):\n    model: str = \"mock-gpt-model\"\n    messages: List[ChatMessage]\n    max_tokens: Optional[int] = 512\n    temperature: Optional[float] = 0.1\n    stream: Optional[bool] = False\n```", "```py\nimport time\n\nfrom fastapi import FastAPI\n\napp = FastAPI(title=\"OpenAI-compatible API\")\n\n@app.post(\"/chat/completions\")\nasync def chat_completions(request: ChatCompletionRequest):\n\n    if request.messages and request.messages[0].role == 'user':\n      resp_content = \"As a mock AI Assitant, I can only echo your last message:\" + request.messages[-1].content\n    else:\n      resp_content = \"As a mock AI Assitant, I can only echo your last message, but there were no messages!\"\n\n    return {\n        \"id\": \"1337\",\n        \"object\": \"chat.completion\",\n        \"created\": time.time(),\n        \"model\": request.model,\n        \"choices\": [{\n            \"message\": ChatMessage(role=\"assistant\", content=resp_content)\n        }]\n    }\n```", "```py\nuvicorn main:app\n```", "```py\nfrom openai import OpenAI\n\n# init client and connect to localhost server\nclient = OpenAI(\n    api_key=\"fake-api-key\",\n    base_url=\"http://localhost:8000\" # change the default port if needed\n)\n\n# call API\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-1337-turbo-pro-max\",\n)\n\n# print the top \"choice\" \nprint(chat_completion.choices[0].message.content)\n```", "```py\nimport asyncio\nimport json\n\nasync def _resp_async_generator(text_resp: str):\n    # let's pretend every word is a token and return it over time\n    tokens = text_resp.split(\" \")\n\n    for i, token in enumerate(tokens):\n        chunk = {\n            \"id\": i,\n            \"object\": \"chat.completion.chunk\",\n            \"created\": time.time(),\n            \"model\": \"blah\",\n            \"choices\": [{\"delta\": {\"content\": token + \" \"}}],\n        }\n        yield f\"data: {json.dumps(chunk)}\\n\\n\"\n        await asyncio.sleep(1)\n    yield \"data: [DONE]\\n\\n\"\n```", "```py\nimport time\n\nfrom starlette.responses import StreamingResponse\n\napp = FastAPI(title=\"OpenAI-compatible API\")\n\n@app.post(\"/chat/completions\")\nasync def chat_completions(request: ChatCompletionRequest):\n\n    if request.messages:\n      resp_content = \"As a mock AI Assitant, I can only echo your last message:\" + request.messages[-1].content\n    else:\n      resp_content = \"As a mock AI Assitant, I can only echo your last message, but there wasn't one!\"\n    if request.stream:\n      return StreamingResponse(_resp_async_generator(resp_content), media_type=\"application/x-ndjson\")\n\n    return {\n        \"id\": \"1337\",\n        \"object\": \"chat.completion\",\n        \"created\": time.time(),\n        \"model\": request.model,\n        \"choices\": [{\n            \"message\": ChatMessage(role=\"assistant\", content=resp_content)        }]\n    }\n```", "```py\nfrom openai import OpenAI\n\n# init client and connect to localhost server\nclient = OpenAI(\n    api_key=\"fake-api-key\",\n    base_url=\"http://localhost:8000\" # change the default port if needed\n)\n\nstream = client.chat.completions.create(\n    model=\"mock-gpt-model\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\")\n```"]