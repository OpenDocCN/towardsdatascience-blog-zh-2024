- en: Mastering K-Means Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mastering-k-means-clustering-065bc42637e4?source=collection_archive---------0-----------------------#2024-05-22](https://towardsdatascience.com/mastering-k-means-clustering-065bc42637e4?source=collection_archive---------0-----------------------#2024-05-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Implement the K-Means algorithm from scratch with this step-by-step Python tutorial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--065bc42637e4--------------------------------)[![Marcus
    Sena](../Images/ff594ec7029e6259f0be6dc031d8a6cd.png)](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--065bc42637e4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--065bc42637e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--065bc42637e4--------------------------------)
    [Marcus Sena](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--065bc42637e4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--065bc42637e4--------------------------------)
    ·8 min read·May 22, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c7adde7e4e88ee31689443db5e31e44.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author using DALL-E.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I show how I’d learn the K-Means algorithm if I’d started today.
    We’ll start with the fundamental concepts and implement a Python class that performs
    clustering tasks using nothing more than the Numpy package.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you are a machine learning beginner trying to build a solid understanding
    of the concepts or a practitioner interested in creating custom machine learning
    applications and needs to understand how the algorithms work under the hood, that
    article is for you.
  prefs: []
  type: TYPE_NORMAL
- en: Table of contents
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1\. Introduction](#4970)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[2\. What Does the K-Means algorithm do?](#7e98)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[3\. Implementation in Python](#60a5)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[4\. Evaluation and Interpretation](#2e89)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[5\. Conclusions and Next Steps](#655b)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the machine learning algorithms widely used, such as Linear Regression,
    Logistic Regression, Decision Trees, and others are useful for making predictions
    from labeled data, that is, each input comprises feature values with a label value
    associated. That is what is called **Supervised Learning**.
  prefs: []
  type: TYPE_NORMAL
- en: However, often we have to deal with large sets of data with no label associated.
    Imagine a business that needs to understand the different groups of customers
    based on purchasing behavior, demographics, address, and other information, thus
    it can offer better services, products, and promotions.
  prefs: []
  type: TYPE_NORMAL
- en: These types of problems can be addressed with the use of **Unsupervised Learning**
    techniques. The K-Means algorithm is a widely used unsupervised learning algorithm
    in Machine Learning. Its simple and elegant approach makes it possible to separate
    a dataset into a desired number of K distinct clusters, thus allowing one to learn
    patterns from unlabelled data.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. What Does the K-Means algorithm do?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As said earlier, the K-Means algorithm seeks to partition data points into a
    given number of clusters. The points within each cluster are similar, while points
    in different clusters have considerable differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having said that, one question arises: how do we define similarity or difference?
    In K-Means clustering, the Euclidean distance is the most common metric for measuring
    similarity.'
  prefs: []
  type: TYPE_NORMAL
- en: In the figure below, we can clearly see 3 different groups. Hence, we could
    determine the centers of each group and each point would be associated with the
    closest center.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ec2067871637ccf83430558df44cfbf.png)'
  prefs: []
  type: TYPE_IMG
- en: Simulated dataset with 200 observations (image by the author).
  prefs: []
  type: TYPE_NORMAL
- en: By doing that, mathematically speaking, the idea is to minimize the *within-cluster
    variance*, the measurement of similarity between each point and its closest center.
  prefs: []
  type: TYPE_NORMAL
- en: Performing the task in the example above was straightforward because the data
    was two-dimensional and the groups were clearly distinct. However, as the number
    of dimensions increases and different values of K are considered, we need an algorithm
    to handle the complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Pick the initial centers (randomly)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to seed the algorithm with initial center vectors that can be chosen
    randomly from the data or generate random vectors with the same dimensions as
    the original data. See the white diamonds in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33dd2f443181dc5507406b8ec38c9077.png)'
  prefs: []
  type: TYPE_IMG
- en: Initial centers are randomly picked (image by the author).
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Find the distances of each point to the centers'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we'll calculate the distance of each data point to the K centers. Then
    we associate each point with the center closest to that point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a dataset with *N* entries and *M* features, the distances to the centers
    *c* can be given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01b720e1312e5eea301af0f6dfa65c36.png)'
  prefs: []
  type: TYPE_IMG
- en: Euclidean distance (image generated using codecogs.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*k* varies from 1 to *K*;'
  prefs: []
  type: TYPE_NORMAL
- en: '*D* is the distance of a point n to the *k* center;'
  prefs: []
  type: TYPE_NORMAL
- en: '*x* is the point vector;'
  prefs: []
  type: TYPE_NORMAL
- en: '*c* is the center vector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, for each data point *n* we''ll have K distances, then we have to label
    the vector to the center with the smallest distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c1fb0e5d6b3deeba9e459fef5ca022c.png)'
  prefs: []
  type: TYPE_IMG
- en: (image generated using codecogs.com)
  prefs: []
  type: TYPE_NORMAL
- en: Where *D* is a vector with *K* distances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Find the *K* centroids and iterate'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For each of the *K* clusters, recalculate the centroid. The new centroid is
    the mean of all data points assigned to that cluster. Then update the positions
    of the centroids to the newly calculated.
  prefs: []
  type: TYPE_NORMAL
- en: Check if the centroids have changed significantly from the previous iteration.
    This can be done by comparing the positions of the centroids in the current iteration
    with those in the last iteration.
  prefs: []
  type: TYPE_NORMAL
- en: If the centroids have changed significantly, go back to Step 2\. If not, the
    algorithm has converged and the process stops. See the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c157b01ce3cb851db9481f3b53f5786.png)'
  prefs: []
  type: TYPE_IMG
- en: Convergence of the centroids (image by the author).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Implementation in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know the fundamental concepts of the K-Means algorithm, it's time
    to implement a Python class. The packages used were Numpy for mathematical calculations,
    Matplotlib for visualization, and the Make_blobs package from Sklearn for simulated
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The class will have the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Init method**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A constructor method to initialize the basic parameters of the algorithm: the
    value *k* of clusters, the maximum number of iterations *max_iter,* and the tolerance
    *tol* value to interrupt the optimization when there is no significant improvement.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Helper functions**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These methods aim to assist the optimization process during training, such as
    calculating the Euclidean distance, randomly choosing the initial centroids, assigning
    the closest centroid to each point, updating the centroids’ values, and verifying
    whether the optimization converged.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fit and predict method**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As mentioned earlier, the K-Means algorithm is an unsupervised learning technique,
    meaning it does not require labeled data during the training process. That way,
    it's necessary a single method to fit the data and predict to which cluster each
    data point belongs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Total error method**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A method to evaluate the quality of the optimization by calculating the *total
    squared error* of the optimization. That will be explored in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here it goes the full code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Evaluation and Interpretation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we'll use the K-Means class to cluster simulated data. To do that, the make_blobs
    package from the Sklearn library will be used. The data consists of 500 two-dimensional
    points with 4 fixed centers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0c6ff01bc9de6ffa2c0a4b553b15d86e.png)'
  prefs: []
  type: TYPE_IMG
- en: Simulated data (image by the author).
  prefs: []
  type: TYPE_NORMAL
- en: After performing the training using four clusters, we achieve the following
    result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2beea78e39fb0e4730512b9c829df0ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering for k=4 (image by the author).
  prefs: []
  type: TYPE_NORMAL
- en: In that case, the algorithm was capable of calculating the clusters successfully
    with 18 iterations. However, we must keep in mind that we already know the optimal
    number of clusters from the simulated data. In real-world applications, we often
    don't know that value.
  prefs: []
  type: TYPE_NORMAL
- en: 'As said earlier, the K-Means algorithm aims to make the *within-cluster variance*
    as small as possible. The metric used to calculate that variance is the *total
    squared Euclidean distance* given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72661e266df6e85c3b4301407fe58c8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Total squared Euclidean distance formula (image by the author using codecogs.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: p is the number of data points in a cluster;
  prefs: []
  type: TYPE_NORMAL
- en: c_i is the centroid vector of a cluster;
  prefs: []
  type: TYPE_NORMAL
- en: K is the number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: In words, the formula above adds up the distances of the data points to the
    nearest centroid. The error decreases as the number K increases.
  prefs: []
  type: TYPE_NORMAL
- en: In the extreme case of K =N, you have one cluster for each data point and this
    error will be zero.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Willmott, Paul (2019).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If we plot the error against the number of clusters and look at where the graph
    "bends", we'll be able to find the optimal number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb9e6cb88d7dfb252a6602ef6d826078.png)'
  prefs: []
  type: TYPE_IMG
- en: Scree plot (image by the author).
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the plot has an "elbow shape" and it bends at K = 4, meaning
    that for greater values of K, the decrease in the total error will be less significant.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusions and Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we covered the fundamental concepts behind the K-Means algorithm,
    its uses, and applications. Also, using these concepts, we were able to implement
    a Python class from scratch that performed the clustering of simulated data and
    how to find the optimal value for K using a scree plot.
  prefs: []
  type: TYPE_NORMAL
- en: However, since we are dealing with an unsupervised technique, there is one additional
    step. The algorithm can successfully assign a label to the clusters, but the meaning
    of each label is a task that the data scientist or machine learning engineer will
    have to do by analyzing the data of each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, I''ll leave some points for further exploration:'
  prefs: []
  type: TYPE_NORMAL
- en: Our simulated data used two-dimensional points. Try to use the algorithm for
    other datasets and find the optimal values for K.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other unsupervised learning algorithms widely used such as *Hierarchical
    Clustering*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the domain of the problem, it may be necessary to use other error
    metrics such as Manhattan distance and cosine similarity. Try to investigate them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Complete code available [here](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/K-Means):'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/K-Means?source=post_page-----065bc42637e4--------------------------------)
    [## ML-and-Ai-from-scratch/K-Means at main · Marcussena/ML-and-Ai-from-scratch'
  prefs: []
  type: TYPE_NORMAL
- en: Python implementation of machine learning and AI algorithms from scratch - ML-and-Ai-from-scratch/K-Means
    at main ·…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/K-Means?source=post_page-----065bc42637e4--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Please feel free to use and improve the code, comment, make suggestions, and
    connect with me on [LinkedIn](https://www.linkedin.com/in/marcus-sena-660198150/),
    [X](https://twitter.com/MarcusMVLS), and [Github](https://github.com/Marcussena/ML-and-Ai-from-scratch).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Sebastian Raschka (2015), Python Machine Learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Willmott, Paul. (2019). *Machine Learning: An Applied Mathematics Introduction*.
    Panda Ohana Publishing.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Géron, A. (2017). *Hands-On Machine Learning*. O’Reilly Media Inc.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Grus, Joel. (2015). *Data Science from Scratch*. O’Reilly Media Inc.'
  prefs: []
  type: TYPE_NORMAL
