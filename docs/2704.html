<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>An Introduction to VLMs: The Future of Computer Vision Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>An Introduction to VLMs: The Future of Computer Vision Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-vlms-the-future-of-computer-vision-models-5f5aeaafb282?source=collection_archive---------1-----------------------#2024-11-06">https://towardsdatascience.com/an-introduction-to-vlms-the-future-of-computer-vision-models-5f5aeaafb282?source=collection_archive---------1-----------------------#2024-11-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="5124" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Building a 28% more accurate multimodal image search engine with VLMs.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ro.isachenko?source=post_page---byline--5f5aeaafb282--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ro Isachenko" class="l ep by dd de cx" src="../Images/c2f1e41a389378cec8801e6eb8d8060c.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*-bMlBUo9rQkryVJiZRtDcQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5f5aeaafb282--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@ro.isachenko?source=post_page---byline--5f5aeaafb282--------------------------------" rel="noopener follow">Ro Isachenko</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5f5aeaafb282--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="c852" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Until recently, AI models were narrow in scope and limited to understanding either language or specific images, but rarely both.</p><p id="0dcc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this respect, general language models like GPTs were a HUGE leap since we went from specialized models to general yet much more powerful models.</p><p id="91ac" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But even as language models progressed, they remained separate from computer vision аreas, each domain advancing in silos without bridging the gap. Imagine what would happen if you could only listen but not see, or vice versa.</p><p id="6762" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">My name is Roman Isachenko, and I’m part of the Computer Vision team at Yandex.</p><p id="7ef2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this article, I’ll discuss visual language models (VLMs), which I believe are the future of compound AI systems.</p><p id="bf96" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I’ll explain the basics and training process for developing a multimodal neural network for image search and explore the design principles, challenges, and architecture that make it all possible.</p><p id="8b85" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Towards the end, I’ll also show you how we used an AI-powered search product to handle images and text and what changed with the introduction of a VLM.</p><p id="be6b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s begin!</p><h2 id="3f26" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">What Are VLMs?</h2><p id="c10a" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">LLMs with billions or even hundreds of billions of parameters are no longer a novelty.</p><p id="7d94" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We see them everywhere!</p><p id="9c0f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The next key focus in LLM research has been more inclined towards developing multimodal models (omni-models) — models that can understand and process multiple data types.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og oh"><img src="../Images/e484761b3bcc1d96fbd1fbc82463d8a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9HQNFYiBq2ZmiJK_"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Multimodal models (Image by Author)</figcaption></figure><p id="764b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As the name suggests, these models can handle more than just text. They can also analyze images, video, and audio.</p><p id="7c36" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But why are we doing this?</p><blockquote class="oy"><p id="369b" class="oz pa fq bf pb pc pd pe pf pg ph ne dx"><em class="pi">Jack of all trades, master of none, oftentimes better than master of one.</em></p></blockquote><p id="6ade" class="pw-post-body-paragraph mj mk fq ml b go pj mn mo gr pk mq mr ms pl mu mv mw pm my mz na pn nc nd ne fj bk">In recent years, we’ve seen a trend where general approaches dominate narrow ones.</p><p id="fad6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Think about it.</p><p id="59e0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Today’s language-driven ML models have become relatively advanced and general-purpose. One model can translate, summarize, identify speech tags, and much more.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og oh"><img src="../Images/7b35063c0d265ee30383b449d2567afa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xte4QX19fENXje2e"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">General NLP model (Image by Author)</figcaption></figure><p id="0ecd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But earlier, these models used to be task-specific (we have them now as well, but fewer than before).</p><ul class=""><li id="cfca" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne po pp pq bk">A dedicated model for translating.</li><li id="5ef9" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">A dedicated model for summarizing, etc.</li></ul><p id="3a80" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In other words, today’s NLP models (LLMs, specifically) can serve multiple purposes that previously required developing highly specific solutions.</p><p id="f490" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Second, this approach allows us to exponentially scale the data available for model training, which is crucial given the finite amount of text data. Earlier, however, one would need task-specific data:</p><ul class=""><li id="fac3" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne po pp pq bk">A dedicated translation labeled dataset.</li><li id="e58f" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">A dedicated summarization dataset, etc.</li></ul><p id="79a0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Third, we believe that training a multimodal model can enhance the performance of each data type, just like it does for humans.</p><p id="8fc7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For this article, we’ll simplify the “black box” concept to a scenario where the model receives an image and some text (which we call the “instruct”) as input and outputs only text (the response).</p><p id="1b7a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As a result, we end up with a much simpler process as shown below:</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og oh"><img src="../Images/ab10af2ac1a9743ce6deba3b47a7279a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wGEV1HT_ooU-fRMQ"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">A simplified multimodal model (Image by Author)</figcaption></figure><p id="ce95" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We’ll discuss image-discriminative models that analyze and interpret what an image depicts.</p><p id="8e0a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Before delving into the technical details, consider the problems these models can solve.</p><p id="2acc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A few examples are shown below:</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og oh"><img src="../Images/6fa1a099ca302aa03dd704fbda095c2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kg-JWsivvidv93hA"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Examples of tasks (Image by Author)</figcaption></figure><ul class=""><li id="8915" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne po pp pq bk">Top left image: We ask the model to describe the image. This is specified with text.</li><li id="f32d" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">Top mid image: We ask the model to interpret the image.</li><li id="fcb7" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">Top right image: We ask the model to interpret the image and tell us what would happen if we followed the sign.</li><li id="d3a5" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">Bottom image: This is the most complicated example. We give the model some math problems. From these examples, you can see that the range of tasks is vast and diverse.</li></ul><p id="665e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">VLMs are a new frontier in computer vision that can solve various <a class="af pw" href="https://arxiv.org/abs/2405.18415" rel="noopener ugc nofollow" target="_blank">fundamental CV-related tasks</a> (classification, detection, description) in zero-shot and one-shot modes.</p><p id="d049" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While VLMs may not excel in every standard task yet, they are advancing quickly.</p><p id="a9d4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, let’s understand how they work.</p><h2 id="aeec" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">VLM Architecture</h2><p id="3c97" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">These models typically have three main components:</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og px"><img src="../Images/b96ccc97b7f95acfec256feef7397cbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7je6eqPlnCWbduB0SUKJuQ.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Simplified representation of VLM (Image by Author)</figcaption></figure><ol class=""><li id="e4a1" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne py pp pq bk">LLM — a text model (YandexGPT, in our case) that doesn’t understand images.</li><li id="dc40" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne py pp pq bk">Image encoder — an image model (CNN or Vision Transformer) that doesn’t understand text.</li><li id="fb38" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne py pp pq bk">Adapter — a model that acts as a mediator to ensure that the LLM and image encoder get along well.</li></ol><p id="6aa5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The pipeline is pretty straightforward:</p><ul class=""><li id="1127" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne po pp pq bk">Feed an image into the image encoder.</li><li id="7682" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">Transform the output of the image encoder into some representation using the adapter.</li><li id="48bb" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">Integrate the adapter’s output into the LLM (more on that below).</li><li id="fda7" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">While the image is processed, convert the text instruct into a sequence of tokens and feed them into the LLM.</li></ul><h2 id="12de" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">More Information About Adapters</h2><p id="3c22" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">The adapter is the most exciting and important part of the model, as it precisely facilitates the communication/interaction between the LLM and the image encoder.</p><p id="7039" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are two types of adapters:</p><ul class=""><li id="9fe3" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne po pp pq bk">Prompt-based adapters</li><li id="8a0e" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">Cross-attention-based adapters</li></ul><p id="9733" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Prompt-based adapters were first proposed in <a class="af pw" href="https://arxiv.org/abs/2301.12597" rel="noopener ugc nofollow" target="_blank">BLIP-2</a> and <a class="af pw" href="https://arxiv.org/abs/2304.08485" rel="noopener ugc nofollow" target="_blank">LLaVa</a> models.</p><p id="f706" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The idea is simple and intuitive, as evident from the name itself.</p><p id="1942" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We take the output of the image encoder (a vector, a sequence of vectors, or a tensor — depending on the architecture) and transform it into a sequence of vectors (tokens), which we feed into the LLM. You could take a simple MLP model with a couple of layers and use it as an adapter, and the results will likely be pretty good.</p><p id="c8c2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Cross-attention-based adapters are a bit more sophisticated in this respect.</p><p id="6be6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">They were used in recent papers on <a class="af pw" href="https://arxiv.org/abs/2407.21783" rel="noopener ugc nofollow" target="_blank">Llama 3.2</a> and <a class="af pw" href="https://arxiv.org/abs/2409.11402" rel="noopener ugc nofollow" target="_blank">NVLM</a>.</p><p id="6810" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">These adapters aim to transform the image encoder’s output to be used in the LLM’s cross-attention block as key/value matrices. Examples of such adapters include transformer architectures like <a class="af pw" href="https://arxiv.org/abs/2204.14198" rel="noopener ugc nofollow" target="_blank">perceiver resampler</a> or <a class="af pw" href="https://arxiv.org/abs/2301.12597" rel="noopener ugc nofollow" target="_blank">Q‑former</a>.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og oh"><img src="../Images/0f4c28f4f6442ec22375f6141c9c62c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*j0DMbK80k_70_bTn"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Prompt-based adapters (left) and Cross-attention-based adapters (right) (Image by Author)</figcaption></figure><p id="2788" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Prompt-based adapters (left) and Cross-attention-based adapters (right)</p><p id="2d9f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Both approaches have pros and cons.</p><p id="c1e2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Currently, prompt-based adapters <a class="af pw" href="https://arxiv.org/abs/2409.11402" rel="noopener ugc nofollow" target="_blank">deliver better results</a> but take away a large chunk of the LLM’s input context, which is important since LLMs have limited context length (for now).</p><p id="075d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Cross-attention-based adapters don’t take away from the LLM’s context but require a large number of parameters to achieve good quality.</p><h2 id="4434" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">VLM Training</h2><p id="1bda" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">With the architecture sorted out, let’s dive into training.</p><p id="05a4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Firstly, note that VLMs aren’t trained from scratch (although we think it’s only a matter of time) but are built on pre-trained LLMs and image encoders.</p><p id="8bfd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Using these pre-trained models, we fine-tune our VLM in multimodal text and image data.</p><p id="f58c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This process involves two steps:</p><ul class=""><li id="a101" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne po pp pq bk">Pre-training</li><li id="8658" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">Alignment: SFT + RL (optional)</li></ul><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og oh"><img src="../Images/7ea2b804900e8b6219c18579ac3fc28a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QMwp1GYnprB-41hQ"/></div></div></figure><p id="e968" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Training procedure of VLMs (Image by Author)</p><p id="c2f4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Notice how these stages resemble LLM training?</p><p id="9ea3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is because the two processes are similar in concept. Let’s take a brief look at these stages.</p><h2 id="c7ab" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">VLM Pre-training</h2><p id="dd92" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">Here’s what we want to achieve at this stage:</p><ul class=""><li id="84c4" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne po pp pq bk">Link the text and image modalities together (remember that our model includes an adapter we haven’t trained before).</li><li id="d8ce" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">Load world knowledge into our model (the images have a lot of specifics, for one, OCR skills).</li></ul><p id="ea73" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are three types of data used in pre-training VLMs:</p><ul class=""><li id="0d83" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne po pp pq bk"><strong class="ml fr">Interleaved Pre-training</strong>: This mirrors the LLM pre-training phase, where we teach the model to perform the next token prediction task by feeding it web documents. With VLM pre-training, we pick web documents with images and train the model to predict text. The key difference here is that a VLM considers both the text and the images on the page. Such data is easy to come by, so this type of pre-training isn’t hard to scale up. However, the data quality isn’t great, and boosting it proves to be a tough job.</li></ul><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og oh"><img src="../Images/a9640b861d6f47a20c1846f5e1cabe28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*y0H-Ll_OjHG7eWks"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Interleaved Pre-training dataset (Image by Author)</figcaption></figure><p id="df26" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Image-Text Pairs Pre-training</strong>: We train the model to perform one specific task: captioning images. You need a large corpus of images with relevant descriptions to do that. This approach is more popular because many such corpora are used to train other models (text-to-image generation, image-to-text retrieval).</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og oh"><img src="../Images/eda08149f9e808bb81b4be9dac331113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ttMjriO0I04xgNWa"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Image-Text Pairs Pre-training dataset (Image by Author)</figcaption></figure><p id="1139" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Instruct-Based Pre-training</strong>: During inference, we’ll feed the model images and text. Why not train the model this way from the start? This is precisely what instruct-based pre-training does: It trains the model on a massive dataset of image-instruct-answer triplets, even if the data isn’t always perfect.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og oh"><img src="../Images/c5885ad9b34e987c993aabbdfe139b97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RApoeCgx34xRSczg"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Instruct-Based Pre-training dataset (Image by Author)</figcaption></figure><p id="41e1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">How much data is needed to train a VLM model properly is a complex question. At this stage, the required dataset size can vary from a few million to several billion (thankfully, not a trillion!) samples.</p><p id="c4f7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Our team used instruct-based pre-training with a few million samples. However, we believe interleaved pre-training has great potential, and we’re actively working in that direction.</p><h2 id="a307" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">VLM Alignment</h2><p id="379c" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">Once pre-training is complete, it’s time to start on alignment.</p><p id="bb08" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It comprises SFT training and an optional RL stage. Since we only have the SFT stage, I’ll focus on that.</p><p id="0cd5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Still, recent papers (like <a class="af pw" href="https://arxiv.org/abs/2405.17220" rel="noopener ugc nofollow" target="_blank">this</a> and <a class="af pw" href="https://arxiv.org/abs/2407.21783" rel="noopener ugc nofollow" target="_blank">this</a>) often include an RL stage on top of VLM, which uses the same methods as for LLMs (DPO and various modifications differing by the first letter in the method name).</p><p id="bf1c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Anyway, back to SFT.</p><p id="7df3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Strictly speaking, this stage is similar to instruct-based pre-training.</p><p id="bfaf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The distinction lies in our focus on high-quality data with proper response structure, formatting, and strong reasoning capabilities.</p><p id="40e8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This means that the model must be able to understand the image and make inferences about it. Ideally, it should respond equally well to text instructs without images, so we’ll also add high-quality text-only data to the mix.</p><p id="758d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Ultimately, this stage’s data typically ranges between hundreds of thousands to a few million examples. In our case, the number is somewhere in the six digits.</p><h2 id="d516" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Quality Evaluation</h2><p id="8ed4" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">Let’s discuss the methods for evaluating the quality of VLMs. We use two approaches:</p><ul class=""><li id="06d5" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne po pp pq bk">Calculate metrics on open-source benchmarks.</li><li id="c42b" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">Compare the models using side-by-side (SBS) evaluations, where an assessor compares two model responses and chooses the better one.</li></ul><p id="c6f9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The first method allows us to measure surrogate metrics (like accuracy in classification tasks) on specific subsets of data.</p><p id="855b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, since most benchmarks are in English, they can’t be used to compare models trained in other languages, like German, French, Russian, etc.</p><p id="4e0c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While translation can be used, the errors introduced by translation models make the results unreliable.</p><p id="6fb3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The second approach allows for a more in-depth analysis of the model but requires meticulous (and expensive) manual data annotation.</p><p id="916f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Our model is bilingual and can respond in both English and Russian. Thus, we can use English open-source benchmarks and run side-by-side comparisons.</p><p id="4cfc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We trust this method and invest a lot in it. Here’s what we ask our assessors to evaluate:</p><ul class=""><li id="cec2" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne po pp pq bk">Grammar</li><li id="9426" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">Readability</li><li id="3258" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">Comprehensiveness</li><li id="c255" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">Relevance to the instruct</li><li id="9f48" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">Errors (logical and factual)</li><li id="b8dd" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">Hallucinations</li></ul><p id="abbd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We strive to evaluate a complete and diverse subset of our model’s skills.</p><p id="7153" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The following pie chart illustrates the distribution of tasks in our SbS evaluation bucket.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/608427413661c3c7cc8a8694a629a64b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xkpcpmZxs_KhTUML"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Distribution of tasks for quality evaluation (Image by Author)</figcaption></figure><p id="edd1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This summarizes the overview of VLM fundamentals and how one can train a model and evaluate its quality.</p><h2 id="4342" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Pipeline Architecture</h2><p id="9182" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">This spring, we added multimodality to Neuro, an AI-powered search product, allowing users to ask questions using text and images.</p><p id="7d18" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Until recently, its underlying technology wasn’t truly multimodal.</p><p id="b6cb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here’s what this pipeline looked like before.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/5d139ed87d58cc9543ac873ac974c51e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SlQH5H41xv0-e60N"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Pipeline architecture (Image by Author)</figcaption></figure><p id="8483" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This diagram seems complex, but it’s straightforward once you break it down into steps.</p><p id="f8ed" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here’s what the process used to look like</p><ol class=""><li id="0efe" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne py pp pq bk">The user submits an image and a text query.</li><li id="15e0" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne py pp pq bk">We send the image to our visual search еngine, which would return a wealth of information about the image (tags, recognized text, information card).</li><li id="e6ed" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne py pp pq bk">We formulate a text query using a rephraser (a fine-tuned LLM) with this information and the original query.</li><li id="55ae" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne py pp pq bk">With the rephrased text query, we use Yandex Search to retrieve relevant documents (or excerpts, which we call infocontext).</li><li id="8ccc" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne py pp pq bk">Finally, with all this information (original query, visual search information, rephrased text query, and info context), we generate the final response using a generator model (another fine-tuned LLM).</li></ol><p id="d164" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Done!</p><p id="9e10" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As you can see, we used to rely on two unimodal LLMs and our visual search engine. This solution worked well on a small sample of queries but had limitations.</p><p id="36ac" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Below is an example (albeit slightly exaggerated) of how things could go wrong.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og oh"><img src="../Images/2d65efa3148887ebeeb8f1a47fac7994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8RA_GnRiJqj85gCx"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">The problem with two unimodal LLMs (Image by Author)</figcaption></figure><p id="6c43" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here, the rephraser receives the output of the visual search service and simply doesn’t understand the user’s original intent.</p><p id="8053" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In turn, the LLM model, which knows nothing about the image, generates an incorrect search query, getting tags about the pug and the apple simultaneously.</p><p id="1b34" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To improve the quality of our multimodal response and allow users to ask more complex questions, we introduced a VLM into our architecture.</p><p id="41e6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">More specifically, we made two major modifications:</p><ol class=""><li id="f452" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne py pp pq bk">We replaced the LLM rephraser with a VLM rephraser. Essentially, we started feeding the original image to the rephraser’s input on top of the text from the visual search engine.</li><li id="5329" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne py pp pq bk">We added a separate VLM captioner to the pipeline. This model provides an image description, which we use as info context for the final generator.</li></ol><p id="3f6b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You might wonder</p><blockquote class="oy"><p id="6f79" class="oz pa fq bf pb pc pd pe pf pg ph ne dx">Why not make the generator itself VLM-based?</p></blockquote><p id="f5f7" class="pw-post-body-paragraph mj mk fq ml b go pj mn mo gr pk mq mr ms pl mu mv mw pm my mz na pn nc nd ne fj bk">That’s a good idea!</p><p id="559b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But there’s a catch.</p><p id="e4ae" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Our generator training inherits from Neuro’s text model, which is frequently updated.</p><p id="3cc6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To update the pipeline faster and more conveniently, it was much easier for us to introduce a separate VLM block.</p><p id="cb5d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Plus, this setup works just as well, which is shown below:</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/a122d37d30387dcc9720cbaae7ffc16d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wojx1nLMr0Fx1Gk7"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Using VLM in AI-powered search (Image by Author)</figcaption></figure><p id="6816" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Training VLM rephraser and VLM captioner are two separate tasks.</p><p id="ab1a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For this, we use mentioned earlierse VLM, as mentioned e for thise-tuned it for these specific tasks.</p><p id="bf0c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Fine-tuning these models required collecting separate training datasets comprising tens of thousands of samples.</p><p id="342c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We also had to make significant changes to our infrastructure to make the pipeline computationally efficient.</p><h2 id="1160" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Gauging the Quality</h2><p id="6b45" class="pw-post-body-paragraph mj mk fq ml b go oa mn mo gr ob mq mr ms oc mu mv mw od my mz na oe nc nd ne fj bk">Now for the grand question:</p><blockquote class="oy"><p id="6d7b" class="oz pa fq bf pb pc pd pe pf pg ph ne dx">Did introducing a VLM to a fairly complex pipeline improve things?</p></blockquote><p id="2a11" class="pw-post-body-paragraph mj mk fq ml b go pj mn mo gr pk mq mr ms pl mu mv mw pm my mz na pn nc nd ne fj bk">In short, yes, it did!</p><p id="1c2d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We ran side-by-side tests to measure the new pipeline’s performance and compared our previous LLM framework with the new VLM one.</p><p id="d670" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This evaluation is similar to the one discussed earlier for the core technology. However, in this case, we use a different set of images and queries more aligned with what users might ask.</p><p id="6e81" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Below is the approximate distribution of clusters in this bucket.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/29abfb15a93da3ac9445af9b1a49ada7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kq-Km5VBFDosgL3C"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Cluster distribution (Image by Author)</figcaption></figure><p id="737a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Our offline side-by-side evaluation shows that we’ve substantially improved the quality of the final response.</p><p id="032a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The VLM pipeline noticeably increases the response quality and covers more user scenarios.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og oh"><img src="../Images/9a70b5dbf5bb31451cad96af55767ae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hU09Cs0b16Mqz5eg"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Accuracy of VLM vs LLM in Neuro (Image by Author)</figcaption></figure><p id="8e17" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We also wanted to test the results on a live audience to see if our users would notice the technical changes that we believe would improve the product experience.</p><p id="4150" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So, we conducted an online split test, comparing our LLM pipeline to the new VLM pipeline. The preliminary results show the following change:</p><ul class=""><li id="f3fe" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne po pp pq bk">The number of instructs that include an image increased by 17%.</li><li id="a19e" class="mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne po pp pq bk">The number of sessions (the user entering multiple queries in a row) saw an uptick of 4.5%.</li></ul><p id="7610" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To reiterate what was said above, we firmly believe that VLMs are the future of computer vision models.</p><p id="325d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">VLMs are already capable of solving many out-of-the-box problems. With a bit of fine-tuning, they can absolutely deliver state-of-the-art quality.</p><p id="dd32" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Thanks for reading!</p></div></div></div></div>    
</body>
</html>