- en: Multi-Framework AI/ML Development with Keras 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multi-framework-ai-ml-development-with-keras-3-cf7be29eb23d?source=collection_archive---------3-----------------------#2024-06-16](https://towardsdatascience.com/multi-framework-ai-ml-development-with-keras-3-cf7be29eb23d?source=collection_archive---------3-----------------------#2024-06-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All hail the return of Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://chaimrand.medium.com/?source=post_page---byline--cf7be29eb23d--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--cf7be29eb23d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cf7be29eb23d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cf7be29eb23d--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--cf7be29eb23d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cf7be29eb23d--------------------------------)
    ·14 min read·Jun 16, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f76631d92e90d65b370eb16d967b59c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jose Rueda](https://unsplash.com/@notartistic?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60dcf4db50ea7db99db7c65748a3b9d0.png)'
  prefs: []
  type: TYPE_IMG
- en: By Author
  prefs: []
  type: TYPE_NORMAL
- en: Keras is Back!! First released in 2015 as a high-level Python library for training
    ML models, Keras grew in popularity due to its clean and simple APIs. Contrary
    to the ML frameworks of the time, with their awkward and clunky APIs, Keras lowered
    the entry bar for many incumbent ML developers (the author included). But somewhere
    along the way the use of Keras became virtually synonymous with TensorFlow development.
    Consequently, when developers began to turn to alternative frameworks, the relative
    popularity of Keras began to decline. But now, following a “complete rewrite”,
    Keras has returned. And with its shiny new engine and its renewed commitment to
    multi-backend support, it vies to return to its former glory.
  prefs: []
  type: TYPE_NORMAL
- en: In this post we will take a new look at Keras and assess its value offering
    in the current era of AI/ML development. We will demonstrate through example its
    ease of use and make note of its shortcomings. Importantly, this post is not intended
    to be an endorsement for or against the adoption of Keras (or any other framework,
    library, service, etc.). As usual, the best decision for your project development
    will depend on a great many details, many of which are beyond the scope of this
    post.
  prefs: []
  type: TYPE_NORMAL
- en: The [recent release](https://blog.google/technology/developers/gemma-open-models/)
    of Google’s family of open sourced NLP models known as Gemma, and the inclusion
    of Keras 3 as a core component of the API, offers us an opportunity to evaluate
    Keras’s goodness and could serve as a great opportunity for its resurgence.
  prefs: []
  type: TYPE_NORMAL
- en: Why Use Keras 3?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our view, the most valuable feature offered by Keras 3 is its multi-framework
    support. This may surprise some readers who may recall Keras’s distinctiveness
    to be its user experience. Keras 3 advertises itself, as “simple”, “flexible”,
    and being “designed for human beings, not machines”. And indeed, it owes its early
    successes and meteoric rise in popularity to its user experience. But it is now
    2024 and there are many high-level deep learning APIs offering “reduced cognitive
    load”. In our view, the user experience, as good as it may be, is no longer a
    sufficient motivator to consider Keras over its alternatives. Its multi-framework
    support is.
  prefs: []
  type: TYPE_NORMAL
- en: The Merits of Multi-Framework Support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras 3 supports multiple backends for training and running its models. At the
    time of this writing, these include [JAX](https://jax.readthedocs.io/), [TensorFlow](https://github.com/tensorflow/tensorflow),
    and [PyTorch](https://pytorch.org/). The [Keras 3 announcement](https://keras.io/keras_3/)
    does a pretty good job of explaining the advantages of this feature. We will expand
    on the documented benefits and add some of our own flavor.
  prefs: []
  type: TYPE_NORMAL
- en: '**Avoid the difficulty of choosing an AI/ML framework:** Choosing an AI/ML
    framework is probably one of the most important decisions you will need to make
    as an ML developer. It is also one of the hardest. There are many considerations
    that need to factor into this decision. These include user experience, API coverage,
    programmability, debuggability, the formats and types of input data that are supported,
    conformance with other components on the development pipeline (e.g., restrictions
    that may be imposed by the model deployment phase), and, perhaps most importantly,
    runtime performance. As we have discussed in many of our previous posts (e.g.,
    [here](https://medium.com/p/6e407a7d2dc8#de85-799b58b79241)), AI/ML model development
    can be extremely expensive and the overall impact on cost of even the smallest
    speed-up due to the choice of framework can be dramatic. In fact, in many cases
    it may warrant the overhead of porting your model and code to a different framework
    and/or even maintaining support for multiple frameworks.'
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that it is extremely difficult, if not impossible, to know which
    framework will be most optimal for your model before you start your development.
    Moreover, even once you have committed to one framework, you will want to stay
    on top of the evolution and development of all frameworks and to continuously
    assess potential opportunities to improve your model and/or reduce the cost of
    development. The landscape of AI/ML development is extremely dynamic with optimizations
    and enhancements being designed and developed on a consistent basis. You will
    not want to fall behind.
  prefs: []
  type: TYPE_NORMAL
- en: Keras 3 solves the framework selection problem by enabling you to develop your
    model without committing to an underlying backend. The option to toggle between
    multiple framework-backends allows you to focus on the model definition and, once
    complete, choose the backend that best suits your needs. And even as the properties
    of the ML project change or the supported frameworks evolve, Keras 3 enables you
    to easily assess the impact of changing the backend.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it colloquially, you could say that Keras 3 helps humans avoid one of
    the things they hate doing most — making decisions and committing to them. But
    humor aside, AI/ML model development using Keras 3 can certainly prevent you from
    choosing and being stuck with a suboptimal framework.
  prefs: []
  type: TYPE_NORMAL
- en: '**Enjoy the best of all worlds:** PyTorch, TensorFlow, and JAX, each have their
    own unique advantages and differentiating properties. JAX, for example, supports
    just-in-time (JIT) compilation in which the model operators are converted into
    an intermediate computation graph and then compiled together into machine code
    specifically targeted for the underlying hardware. For many models this results
    in a considerable boost in runtime performance. On the other hand, PyTorch, which
    is typically used in a manner in which the operators are executed immediately
    (a.k.a. “eagerly”) is often considered to: have the most Pythonic interface, be
    the easiest to debug, and offer the best overall user experience. By using Keras
    3 you can enjoy the best of both worlds. You can set the backend to PyTorch during
    your initial model development and for debugging and switch to JAX for optimal
    performance when training in production mode.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compatibility with the maximum number of AI accelerators and runtime environments:**
    As we have discussed in the past (e.g., [here](/instance-selection-for-deep-learning-7463d774cff0))
    our goal is to be compatible with as many AI accelerators and runtime environments
    as possible. This is especially important in an era of constrained capacity of
    AI machines in which the ability to switch between different machine types is
    a huge advantage. When you develop with Keras 3 and its multi-backend support,
    you automatically increase the number of platforms that you can potentially train
    and run your model on. For example, while you may be most accustomed to running
    in PyTorch on GPUs, by simply changing the backend to JAX you can configure your
    model to run on [Google Cloud TPUs](https://cloud.google.com/tpu?hl=en), as well
    ( — though this may depend on the details of the model).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Increase model adoption:** If you are targeting your model for use by other
    AI/ML teams, you will increase your potential audience by supporting multiple
    frameworks. For all sorts of reasons, some teams may be limited to a specific
    ML framework. By delivering your model in Keras you remove barriers for adoption.
    A great example of this is the recent release of Google’s Gemma models which we
    will discuss in greater detail below.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decouple the data input pipeline from the model execution:** Some frameworks
    encourage the use of certain data storage formats and/or data loading practices.
    A classic example of this is TensorFlow’s [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord)
    data format for storing a sequence of binary records that are typically stored
    in `.tfrecord` files. While TensorFlow includes native support for parsing and
    processing data stored TFRecord files, you might find feeding them into a PyTorch
    training loop to be a bit more difficult. A preferable format for PyTorch training
    could be [WebDataset](https://pytorch.org/data/main/generated/torchdata.datapipes.iter.WebDataset.html).
    But the creation of training data can be a long process and maintaining it in
    more than one format could be prohibitively expensive. Thus, the manner in which
    your training data is stored and maintained might discourage teams from considering
    alternative frameworks.'
  prefs: []
  type: TYPE_NORMAL
- en: Keras 3 helps teams overcome this obstacle by completely decoupling the data
    input pipeline from the training loop. You can define your input data pipelines
    in PyTorch, TensorFlow, Numpy, Keras, and other libraries without any consideration
    for the backend that will be used in your training loop. With Keras 3, having
    your training data stored in TFRecord files is no longer a barrier to adopting
    PyTorch as a backend.
  prefs: []
  type: TYPE_NORMAL
- en: The Disadvantages of Multi-Framework Support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As with any other new SW solution on the market, it is important to be aware
    of the potential downsides of Keras 3\. A general rule of thumb in SW development
    is that the higher up the SW stack you go, the less control you have over the
    behavior and performance of your application. In AI/ML, where the degree of success
    is often determined by precise tuning of model hyperparameters, initialization
    settings, appropriate environment configuration, etc., such control could be critical.
    Here are just a few potential drawbacks to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Potential drop in runtime performance:** Working the high level Keras APIs
    rather than directly with the framework APIs, may pose limitations on optimizing
    runtime performance. In our series of posts on the topic of [analyzing and optimizing
    the performance of PyTorch models](/pytorch-model-performance-analysis-and-optimization-10c3c5822869),
    we demonstrated a wide range of tools and techniques for increasing the speed
    of training. Sometimes these require the direct, unmediated, use of PyTorch’s
    APIs. For example, Keras’s APIs currently include very limited support for [PyTorch’s
    JIT compilation](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    option (via the [*jit_compile*](https://keras.io/api/models/model_training_apis/)setting).
    Another example is PyTorch’s built-in support for [scaled dot product attention](https://pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html)
    which is not supported at the Keras level (as of the time of this writing).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Limitations of cross-framework support:** Although Keras’s cross-framework
    support is extensive, you may find that it is not all-encompassing. For example,
    one gap in coverage (as of the time of this writing) is distributed training.
    Although, Keras introduces [the Keras distribution API](https://keras.io/guides/distribution/)
    to support data and model parallelism across all backends, it is currently implemented
    for the JAX backend only. To run distributed training when using other backends,
    you will need to fall back to the standard distribution APIs of the relevant framework
    (e.g., PyTorch’s [distributed data parallel API](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overhead of maintaining cross-framework compatibility:** Keras 3 supports
    a wide variety of pre-built models that you can reuse (e.g., [here](https://keras.io/api/keras_nlp/models/)).
    However, inevitably, you may want to introduce your own customizations. While
    Keras 3 supports customization of the model layers, metrics, training loop and
    more, you will need to take care not to break your cross-framework compatibility.
    For example, if you create a custom layer using Keras’s backend-agnostic APIs
    (`keras.ops`), you can rest assured that multi-backend support is retained. However,
    sometimes you may choose to rely on framework-specific operations. In such cases
    maintaining cross-framework compatibility will require a dedicated implementation
    for each framework and appropriate conditional programming based on the backend
    in use. The current methods for customizing a [training step](https://keras.io/guides/custom_train_step_in_jax/)
    and a [training loop](https://keras.io/guides/writing_a_custom_training_loop_in_jax/)
    are framework-specific, meaning that they too would require dedicated implementations
    for each backend to retain cross-framework compatibility. Thus, as your model
    grows in complexity, so might the overhead required to maintain this unique capability.'
  prefs: []
  type: TYPE_NORMAL
- en: We have noted just a few potential disadvantages to Keras 3 and its multi-backend
    support. You may very well likely come across others. While the multi-framework
    offering is certainly compelling, its adoption is not necessarily free of cost.
    Borrowing the name of a well-known [theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem)
    in the field of statistical inference, one could say that when it comes to choosing
    an AI/ML development methodology, there are “no free lunches”.
  prefs: []
  type: TYPE_NORMAL
- en: Keras 3 in Practice — A Toy Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As in many of our recent posts, the toy model we will define will be a [Vision
    Transformer](https://huggingface.co/docs/transformers/en/model_doc/vit) (ViT)
    backed classification model. We will rely on the reference implementation located
    in this Keras [tutorial](https://keras.io/examples/vision/image_classification_with_vision_transformer/).
    We have configured our model according to the [ViT-Base](https://deci.ai/model-zoo/vit/)
    architecture (~86 million parameters), set the [mixed_precision](https://keras.io/api/mixed_precision/)
    policy to use *bfloat16*, and defined a [PyTorchdataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)
    with random input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following block includes the configuration settings followed by definitions
    of the core ViT model components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the core components, we define a ViT-backed Keras model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the next block we define the optimizer, loss, and dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we start the training using Keras’s [Model.fit()](https://keras.io/api/models/model_training_apis/)
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We ran the script above on a Google Cloud Platform (GCP) [g2-standard-16](https://cloud.google.com/compute/docs/gpus#l4-gpus)
    VM (with a single NVIDIA L4 GPU) with a dedicated [deep learning VM image](https://cloud.google.com/deep-learning-vm/docs/release-notes)
    (common-cu121-v20240514-ubuntu-2204-py310) and installations of PyTorch (2.3.0),
    JAX (0.4.28), Keras (3.3.3), and [KerasCV](https://keras.io/keras_cv/) (0.9.0).
    Please see the [official Keras documentation](https://github.com/keras-team/keras/blob/master/README.md#installation)
    for full installation instructions. Note that we manually modified the format
    of step time reported by the [Keras progress bar](https://github.com/keras-team/keras/blob/v3.3.3/keras/src/utils/progbar.py#L228):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Using the *backend* flag we were able to easily toggle between the backends
    supported by Keras and compare the runtime performance of each. For example, when
    configuring [PyTorchdataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)
    with 0 workers, we found that JAX backend to outperform PyTorch by ~24%. When
    setting the number of workers to 16 this drops to ~12%.
  prefs: []
  type: TYPE_NORMAL
- en: Custom Attention Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now define a custom attention layer that replaces [Keras’s default attention](https://keras.io/api/layers/attention_layers/multi_head_attention/)
    computation with PyTorch’s [flash attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)
    implementation. Note that this will only work when the backend is set to *torch.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The results of our experiments are summarized in the table below. Keep in mind
    that the relative performance results are likely to vary greatly based on the
    details of the model and the runtime environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bcef0995a3e1fe23cd9cca46c39e9294.png)'
  prefs: []
  type: TYPE_IMG
- en: ViT runtime (by Author)
  prefs: []
  type: TYPE_NORMAL
- en: When using our custom attention layer, the gap between the JAX and PyTorch backends
    virtually disappears. This highlights how the use of a multi-backend solution
    could come at the expense of optimizations uniquely supported by any of the individual
    frameworks (in our example, PyTorch SDPA).
  prefs: []
  type: TYPE_NORMAL
- en: Keras 3 in Gemma
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Gemma](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content=)
    is a family of lightweight, [open source models](https://opensource.googleblog.com/2024/02/building-open-models-responsibly-gemini-era.html)
    recently [released](https://blog.google/technology/developers/gemma-open-models/)
    by Google. Keras 3 plays a prominent role in the Gemma release (e.g., see [here](https://ai.google.dev/gemma/docs/get_started))
    and its multi-framework support makes Gemma automatically accessible to AI/ML
    developers of all persuasions — PyTorch, TensorFlow, and Jax. Please see the official
    [documentation](https://keras.io/api/keras_nlp/models/gemma/) in [KerasNLP](https://keras.io/api/keras_nlp/)
    for more details on the Gemma API offering.'
  prefs: []
  type: TYPE_NORMAL
- en: The following code is loosely based on the official [Gemma fine-tuning tutorial](https://ai.google.dev/gemma/docs/lora_tuning).
    In order to run the script, please follow the [necessary setup instructions](https://ai.google.dev/gemma/docs/lora_tuning#setup).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: When running the script in the same GCP environment described above, we see
    a significant (and surprising) discrepancy between the runtime performance when
    using the JAX backend (6.87 samples per second) and the runtime performance when
    using the PyTorch backend (3.01 samples per second). This is due, in part, to
    the fact that the JAX backend allows for doubling the training batch size. A deep
    dive into the causes of this discrepancy is beyond the scope of this post.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in our previous example, we demonstrate one way of optimizing the PyTorch
    runtime by prepending the following configuration of the [matrix multiplication
    operations](https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html)
    to the top of our script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This simple change results in a 29% performance boost when running with the
    PyTorch backend. Once again, we can see the impact of applying framework-specific
    optimizations. The experiment results are summarized in the table below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5aad8a9425e1f20875979fd728686855.png)'
  prefs: []
  type: TYPE_IMG
- en: Gemma fine-tuning runtime (by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our demonstrations have indicated that sticking with the backend agnostic Keras
    code could imply a meaningful runtime performance penalty. In each example, we
    have seen how a simple, framework-specific optimization had a significant impact
    on the relative performance of our chosen backends. At the same time, the arguments
    we have discussed for multi-framework AI/ML development are rather compelling.
  prefs: []
  type: TYPE_NORMAL
- en: If you do choose to adopt Keras as a development framework, you may want to
    consider designing your code in a manner that includes mechanisms for applying
    and assessing framework-specific optimizations. You might also consider designing
    your development process in a way that utilizes Keras during the early stages
    of the project and, as the project matures, optimizes for the one backend that
    is revealed to be the most appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post we have explored the new and revised Keras 3 release. No longer
    an appendage to TensorFlow, Keras 3 offers the ability of framework-agnostic AI/ML
    model development. As we discussed, this capability has several significant advantages.
    However, as is often the case in the field of AI development, “there are no free
    lunches” — the added level of abstraction could mean a reduced level of control
    over the inner workings of our code which could imply slower training speed and
    higher costs. The best solution might be one that combines the use of Keras and
    its multi-framework support with dedicated mechanisms for incorporating framework-specific
    modifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Importantly, the applicability of Keras 3 to your project and the cost-best
    analysis of the investment required, will depend greatly on a wide variety of
    factors including: the target audience, the model deployment process, project
    timelines, and more. Please view this post as a mere introduction into your detailed
    exploration.'
  prefs: []
  type: TYPE_NORMAL
