- en: Supercharging Prompt Engineering via Symbolic Program Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/supercharging-prompt-engineering-via-symbolic-program-search-c6c353bc80a4?source=collection_archive---------1-----------------------#2024-04-22](https://towardsdatascience.com/supercharging-prompt-engineering-via-symbolic-program-search-c6c353bc80a4?source=collection_archive---------1-----------------------#2024-04-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Find better prompts by exploring a large set of prompt variants automatically
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@toschnab?source=post_page---byline--c6c353bc80a4--------------------------------)[![Tobias
    Schnabel](../Images/92a6c1addc602dae8e8d54fec5116385.png)](https://medium.com/@toschnab?source=post_page---byline--c6c353bc80a4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c6c353bc80a4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c6c353bc80a4--------------------------------)
    [Tobias Schnabel](https://medium.com/@toschnab?source=post_page---byline--c6c353bc80a4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c6c353bc80a4--------------------------------)
    ·8 min read·Apr 22, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfbe607a7b9395877d23744afe241481.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Icons8 Team](https://unsplash.com/@icons8?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: It’s no secret that much of the success of LLMs still depends on our ability
    to prompt them with the right instructions and examples. As newer generation LLMs
    become more and more powerful, prompts have become complex enough to be considered
    [programs themselves](/intro-to-dspy-goodbye-prompting-hello-programming-4ca1c6ce3eb9).
    These prompt programs are a lot like recipes — both have a set of instructions
    to follow and transform raw materials, be it data or ingredients.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering is thus similar to improving a recipe. Home chefs will often
    stick to the overall recipe but make some small changes — for example leaving
    out garlic or adding parsley in a pasta dish. Frameworks like [DSPy](https://github.com/stanfordnlp/dspy)
    are following this overall paradigm when they optimize the in-context examples.
    Pro-level chefs, however, use the recipe as inspiration, and often re-interpret
    components of the dish completely. For example, they might see spaghetti in pasta
    dish as the starchy component and might swap it for freshly made gnocchi to achieve
    a similar composition.
  prefs: []
  type: TYPE_NORMAL
- en: What is it that allows pro-level chefs to work so creatively? It’s that they
    think about recipes in an abstract way, like in the pasta example above. Manual
    prompt engineering is similar to pro-level cooking. It can get impressive results
    but requires [a lot of time and knowledge](https://medium.com/the-generator/the-perfect-prompt-prompt-engineering-cheat-sheet-d0b9c62a2bba).
    What we really want is the creativity of manual prompt engineering but without
    the effort.
  prefs: []
  type: TYPE_NORMAL
- en: The power of abstract prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s say we want to improve a prompt for labeling speaker responses. We’ll
    eventually run it with many different inputs, but plug in a concrete one for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Assume, for a moment, that we had an abstract representation of this prompt
    that pulls out its separate components and is easy to manipulate. Maybe something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f419eb3d7a2cb20fc0c6a90b9918af6a.png)'
  prefs: []
  type: TYPE_IMG
- en: A simple prompt for a classification task represented as an abstract symbolic
    program. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: With this, you could automate a lot of the (semi)-manual tinkering you have
    to do during prompt prototyping. Making small edits such as paraphrasing would
    be just the start. Want to try out [Chain-of-Thought reasoning](/chain-of-thought-prompting-for-llms-33c963eead38)?
    Add a paragraph that says “Let’s think step-by-step.” How about changing the data
    formatting to JSON? Simply change the `format`attribute of the InputData parameters.
    You can also explore
  prefs: []
  type: TYPE_NORMAL
- en: Going from single examples to batch annotation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing your retriever and ranking function in a RAG scenario
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Re-ordering some of the paragraphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressing certain parts of the instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essentially, plug in your favorite prompt engineering heuristic. This abstract
    representation of prompts allows us to truly get creative and automatically explore
    a large space of possible prompts. But how can we represent prompts as abstract
    and modifiable programs in Python? Read on.
  prefs: []
  type: TYPE_NORMAL
- en: Turning prompts into abstract programs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “Any problem in computer science can be solved by another layer of indirection.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[David J. Wheeler](https://en.wikipedia.org/wiki/David_Wheeler_(computer_scientist))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To represent abstract prompts, let’s first convert it into a *non-symbolic
    prompt program* by breaking them into individual components, implemented as Python
    classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So far, so good. It’s similar to what DSpy does, albeit more general as we also
    represent the internal structure of a prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we turn it into a *symbolic prompt program* so that we canmake arbitrary
    changes (this is also beyond static DSPy programs). This can be done with [pyGlove](https://pyglove.readthedocs.io/en/latest/notebooks/intro/birdview.html),
    a library for symbolic object-oriented programming (SOOP). pyGlove turns Python
    classes into manipulable, symbolic objects whose properties remain fully editable
    after instantiation.
  prefs: []
  type: TYPE_NORMAL
- en: 'With pyGlove, all we need to do is add the `pg.symbolize` decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now query and modify prompt programs via a whole host of specifiers,
    similar to working with a DOM tree. Let’s say we’d like to transform our program
    above into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7df9a7710a1fb272046f66ae8c63ad21.png)'
  prefs: []
  type: TYPE_IMG
- en: The target prompt program we want to achieve. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we’re now asking “Does the response mean yes?” and not providing
    output labels of yes and no. To get there, we need to (i) change the instruction
    text and (ii) delete the third node. With pyGlove, this is very easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The printout confirms that we’re successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Voilà! Essentially, pyGlove gives us a way to work with Python classes (and
    functions) as if they were still source code with little overhead. Now that we
    have flexible and easily manipulable representations, let’s put them to use.
  prefs: []
  type: TYPE_NORMAL
- en: Wait a minute. We might have a way to represent and modify prompts now, but
    we’re still missing a process to optimize them automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Once chefs understand the abstraction and components of a recipe, they’ll try
    out many variants, refining the taste, cost, or presentation, until it feels right.
    To do the same with prompt abstractions, we need a search algorithm, an objective
    as well as set of labeled samples to know that we’re making progress.
  prefs: []
  type: TYPE_NORMAL
- en: Sounds like a lot to implement yourself? Meet [SAMMO](https://github.com/microsoft/sammo),
    a Python library for building and optimizing symbolic prompt programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Warming up: Instruction tuning with SAMMO'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate SAMMO’s core workflow, we’ll now show how to tune the instructions
    part of our prompt example from above. Once we’ve worked through this toy example,
    we’ll be ready to discuss more advanced applications, like RAG optimization or
    compression.
  prefs: []
  type: TYPE_NORMAL
- en: The key steps are
  prefs: []
  type: TYPE_NORMAL
- en: Defining your starting prompt
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Getting the data ready — a few hundred labeled examples are enough.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the objective
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choosing a set of mutators
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running the optimization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Step 1: Defining your starting prompt'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve pretty much already done this above. SAMMO expects a function, so we’ll
    have to wrap it in one. If you’d like to store extra information, wrap it in a
    [Callable](/python-callables-the-basics-and-the-secrets-ba88bf0729aa) instead.
    We’ll also wrap it in an Output component to run it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: Getting your data ready'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SAMMO uses a simple data structure called DataTable to pair inputs with outputs
    (labels). This will help us with evaluation and bookkeeping.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Defining the objective'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’re interested in optimizing the accuracy, so that’s what we’re implementing
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 4: Choosing a set of mutators'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here is where you can be as creative as you’d like. You can implement your own
    operators that generate new prompt variants, or simply rely on the pre-built mutation
    operators that SAMMO offers.
  prefs: []
  type: TYPE_NORMAL
- en: Below, we do the latter and go for a mix of paraphrasing and inducing instructions
    from a few labeled examples, essentially implementing [Automatic Prompt Engineering
    (APE)](https://cobusgreyling.medium.com/automatic-prompt-engineering-907e230ece0).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 5: Running the optimization'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The introductory example prompt was actually taken from the [BigBench implicatures
    task](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/implicatures)
    which we’ll use to run this experiment. If you run the optimization with 100 samples
    for training and testing and a budget of 48 candidates evaluations, you’ll see
    that SAMMO improves the starting prompt accuracy from **0.56** to **0.77** — a
    **37.5%** improvement. What instructions worked best?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Interestingly, different LLMs prefer quite different instructions. GPT-3.5
    liked generic instructions the best as seen above. Llama-2’s best prompt selected
    by SAMMO with the same training and budget setup used an empty string in the instructions
    part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Getting practical: RAG tuning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll now show how to convert a RAG pipeline into a symbolic program and tune
    it with SAMMO. We’ll use semantic parsing as our application task where we want
    to translate user queries into domain-specific language (DSL) constructs, for
    example, to query some database or call an external API.
  prefs: []
  type: TYPE_NORMAL
- en: To create the starting prompt, we include a list of all operators, use an embedding-based
    retriever to get five fewshot examples and then instruct the LLM to output its
    answer in the same format as the examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a symbolic program, let’s get creative. For the mutations,
    we explore
  prefs: []
  type: TYPE_NORMAL
- en: varying numbers of fewshot examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: different formats (XML, JSON, line-by-line) for the fewshot examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: providing additional information about the DSL or not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: showing input-output pairs or groups of inputs and outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running SAMMO with these and a total budget of 24 candidates to try out, we
    can see a clear trend. Below are test set accuracies for three different datasets
    across four different LLMs. In the overwhelming majority of cases, we can see
    that SAMMO can lift performance substantially, even for the highest-performing
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8b5eb0779ecd8d30ded3ab902eba027.png)'
  prefs: []
  type: TYPE_IMG
- en: Even with a small budget of 24 candidates evaluations we can get major lifts
    in performance. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Converting your prompts into symbolic programs is a really powerful idea to
    explore a large design space of possible prompts and settings. Just as a pro-level
    chef deconstructs and reinterprets recipes to create culinary innovations, symbolic
    programming lets us apply the same level of creativity and experimentation to
    automatic prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: SAMMO implements symbolic program search through a set of mutation operators
    and search routine. Empirically, this can translate into large improvements in
    accuracy for both instruction tuning and RAG tuning, independent of the backend
    LLM.
  prefs: []
  type: TYPE_NORMAL
- en: You can extend SAMMO with custom mutation operators to include your favorite
    prompt engineering techniques or implement objectives to go beyond accuracy (e.g.,
    cost). Happy prompt cooking!
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer:**I am the author of SAMMO.*'
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Code & Jupyter Notebooks:** [Instruction tuning](https://github.com/microsoft/sammo/blob/main/docs/tutorials/5_instruction_optimization.ipynb)
    and [RAG tuning](https://github.com/microsoft/sammo/blob/main/examples/rag_tuning.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reading:** [SAMMO user guide](https://microsoft.github.io/sammo/tutorials/0_quickstart.html)
    and [paper on arXiv](https://arxiv.org/abs/2404.02319) with more details'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
