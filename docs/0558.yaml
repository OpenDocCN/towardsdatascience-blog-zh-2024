- en: Improving LLM Inference Speeds on CPUs with Model Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用模型量化技术提升CPU上LLM推理速度
- en: 原文：[https://towardsdatascience.com/improving-llm-inference-latency-on-cpus-with-model-quantization-28aefb495657?source=collection_archive---------1-----------------------#2024-02-29](https://towardsdatascience.com/improving-llm-inference-latency-on-cpus-with-model-quantization-28aefb495657?source=collection_archive---------1-----------------------#2024-02-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/improving-llm-inference-latency-on-cpus-with-model-quantization-28aefb495657?source=collection_archive---------1-----------------------#2024-02-29](https://towardsdatascience.com/improving-llm-inference-latency-on-cpus-with-model-quantization-28aefb495657?source=collection_archive---------1-----------------------#2024-02-29)
- en: '![](../Images/ba70dd27842dfef63454e83252c90ac1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba70dd27842dfef63454e83252c90ac1.png)'
- en: Image Property of Author — Create with Nightcafe
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片版权所有 — 由Nightcafe创作
- en: Discover how to significantly improve inference latency on CPUs using quantization
    techniques for bf16, int8, and int4 precisions
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索如何使用量化技术（如bf16、int8和int4精度）显著提升CPU上的推理延迟
- en: '[](https://eduand-alvarez.medium.com/?source=post_page---byline--28aefb495657--------------------------------)[![Eduardo
    Alvarez](../Images/8a51c754fdd3362aa82dee5acd2a68c5.png)](https://eduand-alvarez.medium.com/?source=post_page---byline--28aefb495657--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--28aefb495657--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--28aefb495657--------------------------------)
    [Eduardo Alvarez](https://eduand-alvarez.medium.com/?source=post_page---byline--28aefb495657--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://eduand-alvarez.medium.com/?source=post_page---byline--28aefb495657--------------------------------)[![Eduardo
    Alvarez](../Images/8a51c754fdd3362aa82dee5acd2a68c5.png)](https://eduand-alvarez.medium.com/?source=post_page---byline--28aefb495657--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--28aefb495657--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--28aefb495657--------------------------------)
    [Eduardo Alvarez](https://eduand-alvarez.medium.com/?source=post_page---byline--28aefb495657--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--28aefb495657--------------------------------)
    ·9 min read·Feb 29, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--28aefb495657--------------------------------)
    ·9分钟阅读·2024年2月29日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: One of the most significant challenges the AI space faces is the need for computing
    resources to host large-scale production-grade LLM-based applications. At scale,
    LLM applications require redundancy, scalability, and reliability, which have
    historically been only possible on general computing platforms like CPUs. Still,
    the prevailing narrative today is that CPUs cannot handle LLM inference at latencies
    comparable with high-end GPUs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: AI领域面临的最大挑战之一是需要计算资源来托管大规模的生产级基于LLM的应用程序。在大规模应用中，LLM应用需要冗余、可扩展性和可靠性，这些通常只能在像CPU这样的通用计算平台上实现。然而，目前的主流观点认为，CPU无法在延迟上与高端GPU相媲美，无法处理LLM推理。
- en: One open-source tool in the ecosystem that can help address inference latency
    challenges on CPUs is the [Intel® Extension for PyTorch*](https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html)
    (IPEX), which provides up-to-date feature optimizations for an extra performance
    boost on Intel hardware. IPEX delivers a variety of easy-to-implement optimizations
    that make use of hardware-level instructions. This tutorial will dive into the
    theory of model compression and the out-of-the-box model compression techniques
    IPEX provides. These compression techniques directly impact LLM inference performance
    on general computing platforms, like Intel 4th and 5th-generation CPUs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 生态系统中一个可以帮助解决CPU推理延迟挑战的开源工具是[Intel® Extension for PyTorch*](https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html)（IPEX），它为Intel硬件提供了最新的特性优化，能进一步提升性能。IPEX提供了多种易于实现的优化，充分利用了硬件级指令。本文将深入探讨模型压缩的理论以及IPEX提供的现成模型压缩技术。这些压缩技术直接影响基于Intel第4代和第5代CPU等通用计算平台的LLM推理性能。
- en: Inference Latency in Application Development
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用开发中的推理延迟
- en: Second only to application safety and security, inference latency is one of
    the most critical parameters of an AI application in production. Regarding LLM-based
    applications, latency or throughput is often measured in tokens/second. As illustrated
    in the simplified inference processing sequence below, tokens are processed by
    the language model and then de-tokenized into natural language.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 仅次于应用安全性和保障性，推理延迟是AI应用生产中的最关键参数之一。对于基于LLM的应用，延迟或吞吐量通常以token/秒为单位进行衡量。如下面简化的推理处理顺序所示，token由语言模型处理，然后被去token化为自然语言。
- en: '![](../Images/b2d4ae537a79c711aded3a1a056f8f26.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2d4ae537a79c711aded3a1a056f8f26.png)'
- en: GIF 1\. of inference processing sequence — Image by Author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: GIF 1\. 推理处理顺序 — 图片由作者提供
- en: 'Interpreting inference this way can sometimes lead us astray because we analyze
    this component of AI applications in abstraction of the traditional production
    software paradigm. Yes, AI apps have their nuances, but at the end of the day,
    we are still talking about transactions per unit of time. If we start to think
    about inference as a transaction, like any other, from an application design point
    of view, the problem becomes less complex. For example, let’s say we have a chat
    application that has the following requirements:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式解读推理有时可能会误导我们，因为我们在抽象化的AI应用组件分析时，忽略了传统生产软件范式。是的，AI应用有其独特之处，但归根结底，我们还是在谈论单位时间内的交易。如果我们开始从应用设计的角度将推理视为一种交易，就像任何其他交易一样，那么问题变得不那么复杂。例如，假设我们有一个聊天应用，其要求如下：
- en: Average of **300 user sessions** per hour
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每小时平均**300个用户会话**
- en: Average of **5 transactions** (LLM inference requests) per user per session
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个用户每次会话平均进行**5次交易**（LLM推理请求）
- en: Average **100 tokens** generated per transaction
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次交易生成**100个token**的平均数量
- en: Each session has an average of **10,000ms (10s) overhead** for user authentication,
    guardrailing, network latency, and pre/post-processing.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次会话的**10,000ms (10秒)**开销平均用于用户认证、护栏、网络延迟和前/后处理。
- en: Users take an average of **30,000ms (30s) to respond** when actively engaged
    with the chatbot.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户在与聊天机器人积极互动时，平均**需要30,000ms (30秒)**来响应。
- en: The average total active **session time goal is 3** minutes or less.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均总**会话时长目标为3**分钟或更短。
- en: Below, you can see that with some simple napkin math, we can get some approximate
    calculations for the required latency of our LLM inference engine.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，通过一些简单的估算，我们可以得到LLM推理引擎所需的延迟大致计算值。
- en: '![](../Images/e81ec3c2fcdaa2f85ed293707e29dd9d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e81ec3c2fcdaa2f85ed293707e29dd9d.png)'
- en: Figure 1\. A simple equation to calculate the required transaction and token
    latency based on various application requirements. — Image by Author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 基于不同应用要求计算所需交易和token延迟的简单方程式。— 图片由作者提供
- en: Achieving required latency thresholds in production is a challenge, especially
    if you need to do it without incurring additional compute infrastructure costs.
    In the remainder of this article, we will explore one way that we can significantly
    improve inference latency through model compression.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中达到所需的延迟阈值是一项挑战，特别是当你需要在不增加额外计算基础设施成本的情况下实现这一目标时。本文余下部分将探讨一种通过模型压缩显著提高推理延迟的方法。
- en: Model Compression
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型压缩
- en: Model compression is a loaded term because it addresses a variety of techniques,
    such as model quantization, distillation, pruning, and more. At their core, the
    chief aim of these techniques is to reduce the computational complexity of neural
    networks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩是一个广泛的术语，因为它涉及多种技术，如模型量化、蒸馏、剪枝等。这些技术的核心目标是减少神经网络的计算复杂度。
- en: '![](../Images/a063d0f1575e7dbbb20b419bc28230c9.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a063d0f1575e7dbbb20b419bc28230c9.png)'
- en: GIF 2\. Illustration of inference processing sequence — Image by Author
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: GIF 2\. 推理处理顺序的说明 — 图片由作者提供
- en: The method we will focus on today is model quantization, which involves reducing
    the byte precision of the weights and, at times, the activations, reducing the
    computational load of matrix operations and the memory burden of moving around
    larger, higher precision values. The figure below illustrates the process of quantifying
    fp32 weights to int8.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天关注的方法是模型量化，它涉及减少权重的字节精度，有时还包括激活值的精度，从而减少矩阵运算的计算负担和移动较大、高精度值的内存负担。下图展示了将fp32权重量化为int8的过程。
- en: '![](../Images/82cd0e972f9a9a49ad1b4eff1cc721db.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82cd0e972f9a9a49ad1b4eff1cc721db.png)'
- en: Fig 2\. Visual representation of model quantization going from full precision
    at FP32 down to quarter precision at INT8, theoretically reducing the model complexity
    by a factor of 4\. — Image by Author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 模型量化的可视化表示，从全精度（FP32）到四分之一精度（INT8），理论上将模型复杂度降低 4 倍。 — 图片由作者提供
- en: It is worth mentioning that the reduction of complexity by a factor of 4 that
    results from quantizing from fp32 (full precision) to int8 (quarter precision)
    does not result in a 4x latency reduction during inference because inference latency
    involves more factors beyond just model-centric properties.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，从 fp32（全精度）量化到 int8（四分之一精度）所带来的复杂度降低了 4 倍，但并不会导致推理延迟减少 4 倍，因为推理延迟不仅仅与模型本身的特性有关，还涉及更多的因素。
- en: 'Like with many things, there is no one-size-fits-all approach, and in this
    article, we will explore three of my favorite techniques for quantizing models
    using IPEX:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 和许多事情一样，并没有一种通用的解决方案，本文将探索我最喜欢的三种使用 IPEX 进行模型量化的技术：
- en: '**Inference at bf16 or fp32**'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**在 bf16 或 fp32 下进行推理**'
- en: This technique quantizes the weights in the neural network down to a user defined
    precision. This technique is ideal for smaller models, like the <1B LLMs of the
    world.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术将神经网络中的权重量化为用户定义的精度。这项技术非常适合较小的模型，例如小于 1B 参数的 LLM。
- en: '![](../Images/b8ad85dbaf7f150a475f67a5031c129d.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8ad85dbaf7f150a475f67a5031c129d.png)'
- en: Fig 3\. Simple illustration of bf16/fp32, showing FP32 weights in orange and
    half-precision quantized bf16 weights in green. — Image by Author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 简单的 bf16/fp32 示意图，显示 FP32 权重为橙色，半精度量化的 bf16 权重为绿色。 — 图片由作者提供
- en: 'The implementation is quite straightforward: using hugging face transformers,
    a model can be loaded into memory and optimized using the IPEX llm-specific optimization
    function `ipex.llm.optimize(model, dtype=dtype)` by setting `dtype = torch.bfloat16,`
    we can activate the half-prevision inference capability, which improves the inference
    latency over full-precision (fp32) and stock.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 实现方法相当简单：使用 hugging face transformers，可以将模型加载到内存中，并通过 IPEX 特定的优化函数 `ipex.llm.optimize(model,
    dtype=dtype)` 进行优化。通过设置 `dtype = torch.bfloat16,`，我们可以激活半精度推理功能，这比全精度（fp32）和原始模型提高了推理延迟。
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Of the three compression techniques we will explore, this is the easiest to
    implement (measured by unique lines of code) and offers the smallest net improvement
    over a non-quantized baseline.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索的三种压缩技术中，这种实现方式是最简单的（按代码行数计算），并且相对于未量化的基线，提供了最小的净改进。
- en: '**SmoothQuant (int8)**'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**SmoothQuant (int8)**'
- en: This technique addresses the core challenges of quantizing LLMs, which include
    handling large-magnitude outliers in activation channels across all layers and
    tokens, a common issue that traditional quantization techniques struggle to manage
    effectively. This technique employs a joint mathematical transformation on both
    weights and activations within the model. The transformation strategically reduces
    the disparity between outlier and non-outlier values for activations, albeit at
    the cost of increasing this ratio for weights. This adjustment renders the Transformer
    layers “quantization-friendly,” enabling the successful application of int8 quantization
    without degrading model quality.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术解决了量化 LLM 的核心挑战，其中包括处理跨所有层和标记的激活通道中大幅度的离群值，这是传统量化技术难以有效处理的常见问题。这项技术对模型中的权重和激活值进行联合数学变换。该变换战略性地减少了激活值中离群值与非离群值之间的差异，尽管这会以增加权重离群值比例为代价。这个调整使得
    Transformer 层“适合量化”，使得 int8 量化得以成功应用，而不会降低模型质量。
- en: '![](../Images/c723f913c23b0e8577171ad521f992c9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c723f913c23b0e8577171ad521f992c9.png)'
- en: 'Fig 4\. Simple illustration of SmoothQuant showing weights as circles and activations
    as triangles. The diagram depicts the two main steps: (1) the application of scaler
    for smoothing and (2) the quantization to int8 — Image by Author'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 简单的 SmoothQuant 示意图，权重以圆形表示，激活值以三角形表示。该图展示了两个主要步骤：（1）应用缩放器进行平滑，和（2）量化为
    int8 — 图片由作者提供
- en: Below, you’ll find a simple SmoothQuant implementation — omitting the code for
    creating the DataLoader, which is a common and well-documented PyTorch principle.
    SmoothQuant is an accuracy-aware post-training quantization recipe, meaning that
    by providing a calibration dataset and model you will be able to provide a baseline
    and limit the language modeling degradation. The calibration model generates a
    quantization configuration, which is then passed to `ipex.llm.optimize()` along
    with the SmoothQuant mapping. Upon execution, the SmoothQuant is applied, and
    the model can be tested using the `.generate()` method.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简单的 SmoothQuant 实现 — 省略了创建 DataLoader 的代码，因为这是 PyTorch 的一个常见且文档完善的原则。SmoothQuant
    是一种准确度感知的后训练量化方法，这意味着通过提供校准数据集和模型，你将能够提供一个基准并限制语言建模的退化。校准模型生成量化配置，然后将其与 SmoothQuant
    映射一起传递给 `ipex.llm.optimize()`。执行后，应用 SmoothQuant，并可以使用 `.generate()` 方法测试模型。
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: SmoothQuant is a powerful model compression technique and helps significantly
    improve inference latency over full-precision models. Still, it requires a little
    upfront work to prepare a calibration dataset and model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: SmoothQuant 是一种强大的模型压缩技术，并且显著提高了推理延迟，相较于全精度模型。尽管如此，它仍然需要一些前期工作来准备校准数据集和模型。
- en: '**Weight-Only Quantization (int8 and int4)**'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**仅权重量化（int8 和 int4）**'
- en: Compared to traditional int8 quantization applied to both activation and weight,
    weight-only quantization (WOQ) offers a better balance between performance and
    accuracy. It is worth noting that int4 WOQ requires dequantizing to bf16/fp16
    before computation (Figure 4), which introduces an overhead in compute. A basic
    WOQ technique, tensor-wise asymmetric Round To Nearest (RTN) quantization, presents
    challenges and often leads to reduced accuracy ([source](https://huggingface.co/blog/intel-starcoder-quantization)).
    However, literature (Zhewei Yao, 2022) suggests that groupwise quantizing the
    model’s weights helps maintain accuracy. Since the weights are only dequantized
    for computation, a significant memory advantage remains despite this extra step.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的同时对激活和权重进行 int8 量化相比，仅权重量化（WOQ）在性能和准确性之间提供了更好的平衡。值得注意的是，int4 WOQ 在计算之前需要去量化为
    bf16/fp16（图 4），这会引入一定的计算开销。一个基本的 WOQ 技术，张量级的非对称最近舍入（RTN）量化，存在挑战，并且常常导致准确性下降（[来源](https://huggingface.co/blog/intel-starcoder-quantization)）。然而，文献（Zhewei
    Yao, 2022）建议，对模型权重进行分组量化有助于保持准确性。由于权重仅在计算时去量化，即便多出了这一步，仍然保持了显著的内存优势。
- en: '![](../Images/159691607f7555e9159295d849f92e4f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/159691607f7555e9159295d849f92e4f.png)'
- en: Fig 5\. Simple illustration of weight-only quantization, with pre-quantized
    weights in orange and the quantized weights in green. Note that this depicts the
    initial quantization to int4/int8 and dequantization to fp16/bf16 for the computation
    step. — Image by Author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 简单的仅权重量化示意图，预量化的权重为橙色，量化后的权重为绿色。请注意，这显示的是初步量化到 int4/int8 和去量化到 fp16/bf16
    以进行计算步骤。 — 图片来自作者
- en: The WOQ implementation below showcases the few lines of code required to quantize
    a model from Hugging Face with this technique. As with the previous implementations,
    we start by loading a model and tokenizer from Hugging Face. We can use the `get_weight_only_quant_qconfig_mapping()`
    method to configure the WOQ recipe. The recipe is then passed to the `ipex.llm.optimize()`
    function along with the model for optimization and quantization. The quantized
    model can then be used for inference with the `.generate()` method.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 WOQ 实现展示了使用此技术从 Hugging Face 量化模型所需的几行代码。与前面的实现一样，我们首先从 Hugging Face 加载模型和标记器。我们可以使用
    `get_weight_only_quant_qconfig_mapping()` 方法来配置 WOQ 配方。然后将该配方与模型一起传递给 `ipex.llm.optimize()`
    函数以进行优化和量化。量化后的模型可以通过 `.generate()` 方法用于推理。
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, WOQ provides a powerful way to compress models down to a fraction
    of their original size with limited impact on language modeling capabilities.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，WOQ 提供了一种强大的方法，可以将模型压缩到原始大小的一小部分，同时对语言建模能力的影响有限。
- en: Conclusion and Discussion
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论与讨论
- en: 'As an engineer at Intel, I’ve worked closely with the IPEX engineering team
    at Intel. This has afforded me a unique insight into its advantages and development
    roadmap, making IPEX a preferred tool. However, for developers seeking simplicity
    without the need to manage an extra dependency, PyTorch offers [three quantization
    recipes](https://pytorch.org/docs/stable/quantization.html): Eager Mode, FX Graph
    Mode (under maintenance), and PyTorch 2 Export Quantization, providing strong,
    less specialized alternatives.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作为英特尔的工程师，我与英特尔的IPEX工程团队密切合作。这使我对其优势和开发路线图有了独特的洞察力，成为了IPEX的首选工具。然而，对于那些希望简化开发、无需管理额外依赖的开发者，PyTorch提供了[三种量化方法](https://pytorch.org/docs/stable/quantization.html)：急切模式、FX图模式（正在维护中）和PyTorch
    2导出量化，提供了强大且不那么专业的替代方案。
- en: No matter what technique you choose, model compression techniques will result
    in some degree of language modeling performance loss, albeit in <1% in many cases.
    For this reason, it’s essential to evaluate the application’s fault tolerance
    and establish a baseline for model performance at full (FP32) and/or half-precision
    (BF16/FP16) before pursuing quantization.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 无论选择哪种技术，模型压缩技术都会导致一定程度的语言建模性能损失，尽管在许多情况下低于1%。因此，在追求量化之前，评估应用的容错能力并建立基准是至关重要的，这包括全精度（FP32）和/或半精度（BF16/FP16）的模型性能。
- en: In applications that leverage some degree of in-context learning, like Retrieval
    Augmented Generation (RAG), model compression might be an excellent choice. In
    these cases, the mission-critical knowledge is spoon-fed to the model at the time
    of inference, so the risk is heavily reduced even with low-fault-tolerant applications.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在利用某些上下文学习的应用中，如检索增强生成（RAG），模型压缩可能是一个极好的选择。在这些情况下，关键任务知识在推理时被传输给模型，因此即使在容错性较低的应用中，风险也大大降低。
- en: Quantization is an excellent way to address LLM inference latency concerns without
    upgrading or expanding compute infrastructure. It is worth exploring regardless
    of your use case, and IPEX provides a good option to start with just a few lines
    of code.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是解决LLM推理延迟问题的绝佳方法，无需升级或扩展计算基础设施。无论你的使用案例是什么，都值得探索，而IPEX提供了一个不错的起点，只需要几行代码。
- en: '**A few exciting things to try would be:**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**有几个激动人心的尝试：**'
- en: Test the sample code in this tutorial on the Intel Developer Cloud’s [free Jupyter
    Environment](https://console.idcservice.net/training).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在英特尔开发者云的[免费Jupyter环境](https://console.idcservice.net/training)中测试本教程中的示例代码。
- en: Take an existing model that you’re running on an accelerator at complete precision
    and test it out on a CPU at int4/int8
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在加速器上运行的现有模型以完全精度测试，并在CPU上以int4/int8进行测试。
- en: Explore all three techniques and determine which works best for your use case.
    Make sure to compare the loss of language modeling performance, not just latency.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索这三种技术，并确定哪种最适合你的使用案例。确保比较语言建模性能的损失，而不仅仅是延迟。
- en: '[Upload your quantized model to the Hugging Face Model Hub](https://github.com/intel/ai-innovation-bridge/blob/master/workshops/ai-workloads-with-huggingface/6%20-%20Uploading%20and%20Sharing%20Models%20on%20Hugging%20Face%20Hub%20with%20Intel%20Optimizations.ipynb)!
    If you do, let me know — I’d love to check it out!'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将你的量化模型上传到Hugging Face模型中心](https://github.com/intel/ai-innovation-bridge/blob/master/workshops/ai-workloads-with-huggingface/6%20-%20Uploading%20and%20Sharing%20Models%20on%20Hugging%20Face%20Hub%20with%20Intel%20Optimizations.ipynb)！如果你上传了，请告诉我——我很想看看！'
- en: '***Thank you for reading! Don’t forget to follow*** [***my profile for more
    articles***](https://eduand-alvarez.medium.com/) ***like this!***'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '***感谢阅读！别忘了关注*** [***我的个人资料，获取更多类似的文章***](https://eduand-alvarez.medium.com/)
    ***！***'
