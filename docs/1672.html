<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Spicing up Ice Hockey with AI: Player Tracking with Computer Vision</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Spicing up Ice Hockey with AI: Player Tracking with Computer Vision</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spicing-up-ice-hockey-with-ai-player-tracking-with-computer-vision-ce9ceec9122a?source=collection_archive---------0-----------------------#2024-07-09">https://towardsdatascience.com/spicing-up-ice-hockey-with-ai-player-tracking-with-computer-vision-ce9ceec9122a?source=collection_archive---------0-----------------------#2024-07-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/c26f194ba6b3bcdd31b1a7dfbad44347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AbuVabwf4lkPk3U7NK09OA.jpeg"/></div></div></figure><div/><div><h2 id="7f9e" class="pw-subtitle-paragraph hc ge gf bf b hd he hf hg hh hi hj hk hl hm hn ho hp hq hr cq dx">Using PyTorch, computer vision techniques, and a Convolutional Neural Network (CNN), I worked on a model that tracks players, teams and basic performance statistics</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hs ht hu hv hw ab"><div><div class="ab hx"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@raul.vizcarrach?source=post_page---byline--ce9ceec9122a--------------------------------" rel="noopener follow"><div class="l hy hz by ia ib"><div class="l ed"><img alt="Raul Vizcarra Chirinos" class="l ep by dd de cx" src="../Images/9f507c6b9542809b9a32ab185e953ca1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*CkLxvl5GnPAP1DeEhnknnw.jpeg"/><div class="ic by l dd de em n id eo"/></div></div></a></div></div><div class="ie ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ce9ceec9122a--------------------------------" rel="noopener follow"><div class="l if ig by ia ih"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ii cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ic by l br ii em n id eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ij ab q"><div class="ab q ik"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b il im bk"><a class="af ag ah ai aj ak al am an ao ap aq ar in" data-testid="authorName" href="https://medium.com/@raul.vizcarrach?source=post_page---byline--ce9ceec9122a--------------------------------" rel="noopener follow">Raul Vizcarra Chirinos</a></p></div></div></div><span class="io ip" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b il im dx"><button class="iq ir ah ai aj ak al am an ao ap aq ar is it iu" disabled="">Follow</button></p></div></div></span></div></div><div class="l iv"><span class="bf b bg z dx"><div class="ab cn iw ix iy"><div class="iz ja ab"><div class="bf b bg z dx ab jb"><span class="jc l iv">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar in ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ce9ceec9122a--------------------------------" rel="noopener follow"><p class="bf b bg z jd je jf jg jh ji jj jk bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="io ip" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">30 min read</span><div class="jl jm l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="h k w ea eb q"><div class="ks l"><div class="ab q kt ku"><div class="pw-multi-vote-icon ed jc kv kw kx"><div class=""><div class="ky kz la lb lc ld le am lf lg lh kx"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l li lj lk ll lm ln lo"><p class="bf b dy z dx"><span class="kz">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ky lr ls ab q ee lt lu" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lq"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lp lq">1</span></p></button></div></div></div><div class="ab q kd ke kf kg kh ki kj kk kl km kn ko kp kq kr"><div class="lv k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lw an ao ap is lx ly lz" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ma cn"><div class="l ae"><div class="ab cb"><div class="mb mc md me mf gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lw an ao ap is mg mh lu mi mj mk ml mm s mn mo mp mq mr ms mt u mu mv mw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lw an ao ap is mg mh lu mi mj mk ml mm s mn mo mp mq mr ms mt u mu mv mw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lw an ao ap is mg mh lu mi mj mk ml mm s mn mo mp mq mr ms mt u mu mv mw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="6960" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Nowadays, I don’t play hockey as much as I want to, but it’s been a part of me since I was a kid. Recently, I had the chance to help with the referee table and keep some stats in the first Ice Hockey Tournament in Lima (3 on 3). This event involved an extraordinary effort of the the Peruvian Inline Hockey Association (APHL) and a kind visit from the <a class="af nt" href="https://friendshipleague.org/" rel="noopener ugc nofollow" target="_blank">Friendship League</a>. To add an AI twist, I used<strong class="mz gg"> PyTorch</strong>, <strong class="mz gg">computer vision</strong> techniques, and a <strong class="mz gg">Convolutional Neural Network (CNN)</strong> to build a model that tracks players and teams and gathers some basic performance stats.</p><p id="8519" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">This article aims to be a <strong class="mz gg">quick guide</strong> to designing and deploying the model. Although the model still needs some fine-tuning, I hope it can help anyone introduce themselves to the interesting world of computer vision applied to sports. I would like to acknowledge and thank the <a class="af nt" href="https://www.instagram.com/aphl.pe/?igsh=MThvZWxhNThwdXpibA%3D%3D" rel="noopener ugc nofollow" target="_blank">Peruvian Inline Hockey Association (APHL)</a> for allowing me to use a 40-second video sample of the tournament for this project <em class="nu">(you can find the video input sample in the </em><a class="af nt" href="https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch" rel="noopener ugc nofollow" target="_blank"><em class="nu">project’s GitHub repository</em></a><em class="nu">).</em></p><h1 id="8731" class="nv nw gf bf nx ny nz hf oa ob oc hi od oe of og oh oi oj ok ol om on oo op oq bk">The Architecture</h1><p id="04a6" class="pw-post-body-paragraph mx my gf mz b hd or nb nc hg os ne nf ng ot ni nj nk ou nm nn no ov nq nr ns fj bk">Before moving on with the project, I did some quick research to find a baseline from which I could work and avoid “reinventing the wheel”. I found that in terms of using computer vision to track players, there is a lot of interesting work on football <em class="nu">(not surprising, being the most popular team sport in the world)</em>. However, I didn’t find many resources for ice hockey. <a class="af nt" href="https://universe.roboflow.com/search?q=hockey" rel="noopener ugc nofollow" target="_blank">Roboflow</a> has some interesting pre-trained models and datasets for training your own, but working with a hosted model presented some latency issues that I will explain further. In the end, I leveraged the soccer material for reading the video frames and obtaining the individual track IDs, following the basic principles and tracking method approach explained in <a class="af nt" href="https://youtu.be/neBZ6huolkg?feature=shared" rel="noopener ugc nofollow" target="_blank">this tutorial</a> <em class="nu">(If you are interested in gaining a better understanding of some basic computer vision techniques, I suggest watching at least the first hour and a half of the tutorial).</em></p><p id="d48b" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">With the tracking IDs covered, I then built my own path. As we walk through this article, we’ll see how the project evolves from a simple object detection task to a model that fully detects players, teams, and delivers some basic performance metrics <em class="nu">(sample clips from 01 to 08, author’s own creation).</em></p><figure class="ox oy oz pa pb fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp ow"><img src="../Images/e100ca9ea1c1eb7ab22675a9de0f18ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-z9Z9KAcxHo4mIy94VOrow.jpeg"/></div></div><figcaption class="pc pd pe fo fp pf pg bf b bg z dx">Model Architecture. Author’s own creation</figcaption></figure><h1 id="794c" class="nv nw gf bf nx ny nz hf oa ob oc hi od oe of og oh oi oj ok ol om on oo op oq bk">The Tracking Mechanism</h1><p id="92a4" class="pw-post-body-paragraph mx my gf mz b hd or nb nc hg os ne nf ng ot ni nj nk ou nm nn no ov nq nr ns fj bk">The tracking mechanism is the backbone of the model. It ensures that each detected object within the video is identified and assigned a unique identifier, maintaining this identity across each frame. The main components of the tracking mechanism are:</p><ol class=""><li id="b21b" class="mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns ph pi pj bk"><strong class="mz gg">YOLO (You Only Look Once):</strong> It’s a powerful real-time object detection algorithm originally introduced in 2015 in the paper “<a class="af nt" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">You Only Look Once: Unified, Real-Time Object Detection</a>”. Stands out for its speed and its versatility in detecting around 80 pre-trained classes <em class="nu">(it’s important to note that it can also be trained on custom datasets to detect specific objects)</em>. For our use case, we will rely on YOLOv8x, a computer vision model built by Ultralytics based on previous YOLO versions. You can download it <a class="af nt" href="https://github.com/ultralytics/ultralytics" rel="noopener ugc nofollow" target="_blank">here</a>.</li><li id="67a2" class="mx my gf mz b hd pk nb nc hg pl ne nf ng pm ni nj nk pn nm nn no po nq nr ns ph pi pj bk"><strong class="mz gg">ByteTrack Tracker: </strong>To understand ByteTrack, we have to understand MOT (Multiple Object Tracking), which involves tracking the movements of multiple objects over time in a video sequence and linking those objects detected in a current frame with corresponding objects in previous frames. To accomplish this, we will use ByteTrack <em class="nu">( introduced in 2021 in the paper “</em><a class="af nt" href="https://arxiv.org/abs/2110.06864" rel="noopener ugc nofollow" target="_blank"><em class="nu">ByteTrack: Multi-Object Tracking by Associating Every Detection Box</em></a><em class="nu">”)</em>. To implement the ByteTrack tracker and assign track IDs to detected objects, we will rely on the Python’s supervision library.</li><li id="6455" class="mx my gf mz b hd pk nb nc hg pl ne nf ng pm ni nj nk pn nm nn no po nq nr ns ph pi pj bk"><strong class="mz gg">OpenCV:</strong> is a well-known library for various computer vision tasks in Python. For our use case, we will rely on <a class="af nt" href="https://opencv.org/" rel="noopener ugc nofollow" target="_blank">OpenCV</a> to visualize and annotate video frames with bounding boxes and text for each detected object.</li></ol><p id="bacd" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">In order to build our tracking mechanism, we’ll begin with these initial two steps:</p><ul class=""><li id="6476" class="mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns pp pi pj bk">Deploying the YOLO model with ByteTrack to detect objects (in our case, players) and assign unique track IDs.</li><li id="edc9" class="mx my gf mz b hd pk nb nc hg pl ne nf ng pm ni nj nk pn nm nn no po nq nr ns pp pi pj bk">Initializing a dictionary to store object tracks in a pickle (pkl) file. This will be extremely useful to avoid executing the video frame-by-frame object detection process each time we run the code, and save significant time.</li></ul><p id="7e3d" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">For the following step, these are the Python packages that we’ll need:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="ca03" class="pu nw gf pr b bg pv pw l px py">pip install ultralytics<br/>pip install supervision<br/>pip install opencv-python</span></pre><p id="aa21" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Next, we’ll specify our libraries and the path for our sample video file and pickle file (if it exists; if not, the code will create one and save it in the same path):</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="8ef2" class="pu nw gf pr b bg pv pw l px py">#**********************************LIBRARIES*********************************#<br/>from ultralytics import YOLO<br/>import supervision as sv<br/>import pickle<br/>import os<br/>import cv2<br/><br/># INPUT-video file<br/>video_path = 'D:/PYTHON/video_input.mp4'<br/># OUTPUT-Video File<br/>output_video_path = 'D:/PYTHON/output_video.mp4'<br/># PICKLE FILE (IF AVAILABLE LOADS IT IF NOT, SAVES IT IN THIS PATH)<br/>pickle_path = 'D:/PYTHON/stubs/track_stubs.pkl'</span></pre><p id="d002" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Now let’s go ahead and define our tracking mechanism <em class="nu">(you can find the video input sample in the </em><a class="af nt" href="https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch" rel="noopener ugc nofollow" target="_blank"><em class="nu">project’s GitHub repository</em></a><em class="nu">)</em>:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="72c5" class="pu nw gf pr b bg pv pw l px py">#*********************************TRACKING MECHANISM**************************#<br/>class HockeyAnalyzer:<br/>    def __init__(self, model_path):<br/>        self.model = YOLO(model_path) <br/>        self.tracker = sv.ByteTrack()<br/><br/>    def detect_frames(self, frames):<br/>        batch_size = 20 <br/>        detections = [] <br/>        for i in range(0, len(frames), batch_size):<br/>            detections_batch = self.model.predict(frames[i:i+batch_size], conf=0.1)<br/>            detections += detections_batch<br/>        return detections<br/><br/>#********LOAD TRACKS FROM FILE OR DETECT OBJECTS-SAVES PICKLE FILE************#<br/><br/>    def get_object_tracks(self, frames, read_from_stub=False, stub_path=None):<br/>        if read_from_stub and stub_path is not None and os.path.exists(stub_path):<br/>            with open(stub_path, 'rb') as f:<br/>                tracks = pickle.load(f)<br/>            return tracks<br/><br/>        detections = self.detect_frames(frames)<br/><br/>        tracks = {"person": []}<br/><br/>        for frame_num, detection in enumerate(detections):<br/>            cls_names = detection.names<br/>            cls_names_inv = {v: k for k, v in cls_names.items()}<br/><br/>            # Tracking Mechanism<br/>            detection_supervision = sv.Detections.from_ultralytics(detection)<br/>            detection_with_tracks = self.tracker.update_with_detections(detection_supervision)<br/>            tracks["person"].append({})<br/><br/>            for frame_detection in detection_with_tracks:<br/>                bbox = frame_detection[0].tolist()<br/>                cls_id = frame_detection[3]<br/>                track_id = frame_detection[4]<br/><br/>                if cls_id == cls_names_inv.get('person', None):<br/>                    tracks["person"][frame_num][track_id] = {"bbox": bbox}<br/><br/>            for frame_detection in detection_supervision:<br/>                bbox = frame_detection[0].tolist()<br/>                cls_id = frame_detection[3]<br/>                        <br/>        if stub_path is not None:<br/>            with open(stub_path, 'wb') as f:<br/>                pickle.dump(tracks, f)<br/><br/>        return tracks<br/>    <br/>#***********************BOUNDING BOXES AND TRACK-IDs**************************#<br/><br/>    def draw_annotations(self, video_frames, tracks):<br/>        output_video_frames = []<br/>        for frame_num, frame in enumerate(video_frames):<br/>            frame = frame.copy() <br/>            player_dict = tracks["person"][frame_num]<br/><br/>            # Draw Players<br/>            for track_id, player in player_dict.items():<br/>                color = player.get("team_color", (0, 0, 255))  <br/>                bbox = player["bbox"]<br/>                x1, y1, x2, y2 = map(int, bbox)         <br/>            # Bounding boxes<br/>                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)<br/>            # Track_id <br/>                cv2.putText(frame, str(track_id), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)<br/><br/>            output_video_frames.append(frame)<br/><br/>        return output_video_frames</span></pre><p id="3dac" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The method begins by initializing the YOLO model and the ByteTrack tracker. Next, each frame is processed in batches of 20, using the YOLO model to detect and collect objects in each batch. If the pickle file is available in its path, it precomputes the tracks from the file. If the pickle file is not available <em class="nu">(you are running the code for the first time or have erased a previous pickle file)</em>, the <strong class="mz gg">get_object_tracks </strong>converts each detection into the required format for ByteTrack, updates the tracker with these detections, and stores the tracking information in a new pickle file in the designated path.Finally, iterations are made over each frame, drawing bounding boxes and track IDs for each detected object.</p><p id="da1e" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">To execute the tracker and save a new output video with bounding boxes and track IDs, you can use the following code:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="7039" class="pu nw gf pr b bg pv pw l px py">#*************** EXECUTES TRACKING MECHANISM AND OUTPUT VIDEO****************#<br/><br/># Read the video frames<br/>video_frames = []<br/>cap = cv2.VideoCapture(video_path)<br/>while cap.isOpened():<br/>    ret, frame = cap.read()<br/>    if not ret:<br/>        break<br/>    video_frames.append(frame)<br/>cap.release()<br/><br/>#********************* EXECUTE TRACKING METHOD WITH YOLO**********************#<br/>tracker = HockeyAnalyzer('D:/PYTHON/yolov8x.pt')<br/>tracks = tracker.get_object_tracks(video_frames, read_from_stub=True, stub_path=pickle_path)<br/>annotated_frames = tracker.draw_annotations(video_frames, tracks)<br/><br/>#*********************** SAVES VIDEO FILE ************************************#<br/>fourcc = cv2.VideoWriter_fourcc(*'mp4v')<br/>height, width, _ = annotated_frames[0].shape<br/>out = cv2.VideoWriter(output_video_path, fourcc, 30, (width, height))<br/><br/>for frame in annotated_frames:<br/>    out.write(frame)<br/>out.release()</span></pre><p id="69a4" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">If everything in your code worked correctly, you should expect a video output similar to the one shown in <strong class="mz gg">sample clip 01</strong>.</p><figure class="ox oy oz pa pb fw fo fp paragraph-image"><div class="fo fp pz"><img src="../Images/d5caaaa073f19f9f874cb779de45aa22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*fp2mdw3IVEqTzhtB4TCr8w.gif"/></div><figcaption class="pc pd pe fo fp pf pg bf b bg z dx">Sample Clip 01: Basic tracking mechanism ( Objects and Tracking IDs)</figcaption></figure><blockquote class="qa qb qc"><p id="9fbf" class="mx my nu mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz gg">TIP #01:</strong> Don’t underestimate your compute power! When running the code for the first time, expect the frame processing to take some time, depending on your compute capacity. For me, it took between 45 to 50 minutes using only a CPU setup (consider CUDA as an option). The YOLOv8x tracking mechanism, while powerful, demands significant compute resources (at times, my memory hit 99%, fingers crossed it didn’t crash!🙄). If you encounter issues with this version of YOLO, lighter models are available on <a class="af nt" href="https://github.com/ultralytics/ultralytics" rel="noopener ugc nofollow" target="_blank">Ultralytics’ GitHub</a> to balance accuracy and compute capacity.</p></blockquote><h1 id="92c1" class="nv nw gf bf nx ny nz hf oa ob oc hi od oe of og oh oi oj ok ol om on oo op oq bk">The Ice Rink</h1><p id="522f" class="pw-post-body-paragraph mx my gf mz b hd or nb nc hg os ne nf ng ot ni nj nk ou nm nn no ov nq nr ns fj bk">As you’ve seen from the first step, we have some challenges. Firstly, as expected, the model picks up all moving objects; players, referees, even those outside the rink. Secondly, those red bounding boxes can make tracking players a bit unclear and not very neat for presentation. In this section, we’ll focus on narrowing our detection to objects within the rink only. Plus, we’ll swap out those bounding boxes for ellipses at the bottom, ensuring clearer visibility.</p><p id="b7df" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Let’s switch from using boxes to using ellipses first. To accomplish this, we’ll simply add a new method above the labels and bounding boxes method in our existing code:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="4225" class="pu nw gf pr b bg pv pw l px py">#************ Design of Ellipse for tracking players instead of Bounding boxes**************#<br/>    <br/>    def draw_ellipse(self, frame, bbox, color, track_id=None, team=None):<br/>        y2 = int(bbox[3])<br/>        x_center = (int(bbox[0]) + int(bbox[2])) // 2<br/>        width = int(bbox[2]) - int(bbox[0])<br/>        color = (255, 0, 0)<br/>        text_color = (255, 255, 255)<br/>        <br/>        cv2.ellipse(<br/>            frame,<br/>            center=(x_center, y2),<br/>            axes=(int(width) // 2, int(0.35 * width)),<br/>            angle=0.0,<br/>            startAngle=-45,<br/>            endAngle=235,<br/>            color=color,<br/>            thickness=2,<br/>            lineType=cv2.LINE_4<br/>        )<br/>    <br/>        if track_id is not None:<br/>            rectangle_width = 40<br/>            rectangle_height = 20<br/>            x1_rect = x_center - rectangle_width // 2<br/>            x2_rect = x_center + rectangle_width // 2<br/>            y1_rect = (y2 - rectangle_height // 2) + 15<br/>            y2_rect = (y2 + rectangle_height // 2) + 15<br/>    <br/>            cv2.rectangle(frame,<br/>                          (int(x1_rect), int(y1_rect)),<br/>                          (int(x2_rect), int(y2_rect)),<br/>                          color,<br/>                          cv2.FILLED)<br/>    <br/>            x1_text = x1_rect + 12<br/>            if track_id &gt; 99:<br/>                x1_text -= 10<br/>            font_scale = 0.4<br/>            cv2.putText(<br/>                frame,<br/>                f"{track_id}",<br/>                (int(x1_text), int(y1_rect + 15)),<br/>                cv2.FONT_HERSHEY_SIMPLEX,<br/>                font_scale,<br/>                text_color,<br/>                thickness=2<br/>            )<br/><br/>        return frame</span></pre><p id="5657" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">We’ll also need to update the annotation step by replacing the bounding boxes and IDs with a call to the ellipse method:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="c1e6" class="pu nw gf pr b bg pv pw l px py">#***********************BOUNDING BOXES AND TRACK-IDs**************************#<br/><br/>    def draw_annotations(self, video_frames, tracks):<br/>        output_video_frames = []<br/>        for frame_num, frame in enumerate(video_frames):<br/>            frame = frame.copy() <br/>            player_dict = tracks["person"][frame_num]<br/><br/>            # Draw Players<br/>            for track_id, player in player_dict.items():<br/>                bbox = player["bbox"]<br/>                <br/>            # Draw ellipse and tracking IDs<br/>                self.draw_ellipse(frame, bbox, (0, 255, 0), track_id)<br/>            <br/>                x1, y1, x2, y2 = map(int, bbox)<br/><br/>            output_video_frames.append(frame)<br/><br/>        return output_video_frames</span></pre><p id="c112" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">With these changes, your output video should look much neater, as shown in <strong class="mz gg">sample clip 02</strong>.</p><figure class="ox oy oz pa pb fw fo fp paragraph-image"><div class="fo fp pz"><img src="../Images/0e17fb82b7df8f4b41e25e34b5da15d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*xhG1He-WuI2K9FxiY8GqUQ.gif"/></div><figcaption class="pc pd pe fo fp pf pg bf b bg z dx">Sample Clip 02: Replacing bounding boxes with ellipses</figcaption></figure><p id="43b0" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Now, to work with the rink boundaries, we need to have some basic knowledge of resolution in computer vision. In our use case, we are working with a 720p (1280x720 pixels) format, which means that each frame or image we process has dimensions of 1280 pixels (width) by 720 pixels (height).</p><p id="8cc7" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz gg"><em class="nu">What does it mean to work with a 720p (1280x720 pixels) format?</em></strong> It means that the image is made up of 1280 pixels horizontally and 720 pixels vertically. Coordinates in this format start at (0, 0) in the top-left corner of the image, with the x-coordinate increasing as you move right and the y-coordinate increasing as you move down. These coordinates are used to mark specific areas in the image, like using (x1, y1) for the top-left corner and (x2, y2) for the bottom-right corner of a box. Understanding this will helps us measure distances and speeds, and decide where in the video we want to focus our analysis.</p><p id="4765" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">That said, we will start marking the frame borders with green lines using the following code:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="7e8e" class="pu nw gf pr b bg pv pw l px py">#********************* Border Definition for Frame***********************<br/>import cv2<br/><br/>video_path = 'D:/PYTHON/video_input.mp4'<br/>cap = cv2.VideoCapture(video_path)<br/><br/>#**************Read, Define and Draw corners of the frame****************<br/>ret, frame = cap.read()<br/><br/>bottom_left = (0, 720)<br/>bottom_right = (1280, 720)<br/>upper_left = (0, 0)<br/>upper_right = (1280, 0)<br/><br/>cv2.line(frame, bottom_left, bottom_right, (0, 255, 0), 2)<br/>cv2.line(frame, bottom_left, upper_left, (0, 255, 0), 2)<br/>cv2.line(frame, bottom_right, upper_right, (0, 255, 0), 2)<br/>cv2.line(frame, upper_left, upper_right, (0, 255, 0), 2)<br/><br/>#*******************Save the frame with marked corners*********************<br/>output_image_path = 'rink_area_marked_VALIDATION.png'<br/>cv2.imwrite(output_image_path, frame)<br/>print("Rink area saved:", output_image_path)</span></pre><p id="6c8e" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The result should be a green rectangle as shown in (a) in <strong class="mz gg">sample clip 03</strong>. But in order to track only the moving objects within the rink we would need a delimitation more similar to the one in (b) .</p><figure class="ox oy oz pa pb fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qd"><img src="../Images/e6de1323015e8fa64b0a9667df89ff1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1mkb5691LZH7BBGP9w-L0Q.jpeg"/></div></div><figcaption class="pc pd pe fo fp pf pg bf b bg z dx">Figure 03: Border Definition for the Ice Rink (Author’s own creation)</figcaption></figure><p id="90ee" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Getting (b) right is like an iterative process of trial and error, where you test different coordinates until you find the boundaries that best fit your model. Initially, I aimed to match the rink borders exactly. However, the tracking system struggled near the edges. To improve accuracy, I expanded the boundaries slightly to ensure all tracking objects within the rink were captured while excluding those outside. The outcome, shown in (b), was the best I could get <em class="nu">(you could still work better scenarios) </em>defined by these coordinates:</p><ul class=""><li id="a868" class="mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns pp pi pj bk">Bottom Left Corner: (-450, 710)</li><li id="c6ec" class="mx my gf mz b hd pk nb nc hg pl ne nf ng pm ni nj nk pn nm nn no po nq nr ns pp pi pj bk">Bottom Right Corner: (2030, 710)</li><li id="c862" class="mx my gf mz b hd pk nb nc hg pl ne nf ng pm ni nj nk pn nm nn no po nq nr ns pp pi pj bk">Upper Left Corner: (352, 61)</li><li id="a256" class="mx my gf mz b hd pk nb nc hg pl ne nf ng pm ni nj nk pn nm nn no po nq nr ns pp pi pj bk">Upper Right Corner: (948, 61)</li></ul><p id="fe60" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Finally, we will define two additional areas: the o<strong class="mz gg">ffensive zones</strong> for both the White and Yellow teams <em class="nu">(where each team aims to score)</em>. This will enable us to gather some basic positional statistics and pressure metrics for each team within their opponent’s zone.</p><figure class="ox oy oz pa pb fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qe"><img src="../Images/b8ea5a739ce6098b9345c2d7cfe8b332.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xEjeiFxDN-E8DV2GZhaGiw.jpeg"/></div></div><figcaption class="pc pd pe fo fp pf pg bf b bg z dx">Figure 04: Offensive Zones ( Author’s own creation)</figcaption></figure><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="f595" class="pu nw gf pr b bg pv pw l px py">#**************YELLOW TEAM OFFENSIVE ZONE****************<br/>Bottom Left Corner: (-450, 710)<br/>Bottom Right Corner: (2030, 710)<br/>Upper Left Corner: (200, 150)<br/>Upper Right Corner: (1160, 150)<br/><br/>#**************WHITE TEAM OFFENSIVE ZONE****************<br/>Bottom Left Corner: (180, 150)<br/>Bottom Right Corner: (1100, 150)<br/>Upper Left Corner: (352, 61)<br/>Upper Right Corner: (900, 61)</span></pre><p id="5633" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">We will set aside these coordinates for now and explain in the next step how we’ll classify each team. Then, we’ll bring it all together into our original tracking method.</p><h1 id="6983" class="nv nw gf bf nx ny nz hf oa ob oc hi od oe of og oh oi oj ok ol om on oo op oq bk">Using Deep Learning for Team Prediction</h1><p id="9dd2" class="pw-post-body-paragraph mx my gf mz b hd or nb nc hg os ne nf ng ot ni nj nk ou nm nn no ov nq nr ns fj bk">Over 80 years have passed since the release of <em class="nu">“</em><a class="af nt" href="https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf" rel="noopener ugc nofollow" target="_blank"><em class="nu">A Logical Calculus of the Ideas Immanent in Nervous Activity</em></a><em class="nu">”,</em> the paper written by Warren McCulloch and Walter Pitts in 1943, which set the solid ground for early neural network research. Later, in 1957, the mathematical model of a simplified neuron<em class="nu"> (receiving inputs, applying weights to these inputs, summing them up, and outputting a binary result)</em> inspired<a class="af nt" href="https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon" rel="noopener ugc nofollow" target="_blank"> Frank Rosenblatt to build the Mark I</a>. This was the first hardware implementation designed to demonstrate the concept of a <a class="af nt" href="https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf" rel="noopener ugc nofollow" target="_blank">perceptron</a>, a neural network model capable of learning from data to make binary classifications. Since then, the quest to make computers think like us hasn’t slowed down. If this is your first deep dive into Neural Networks, or if you want to refresh and strengthen your knowledge, I recommend reading this <a class="af nt" href="https://medium.com/@shreya.rao/list/deep-learning-illustrated-ae6c27de1640" rel="noopener">series of articles by Shreya Rao</a> as a great starting point for deep learning. Additionally, you can access my collection of stories (different contributors) <a class="af nt" href="https://medium.com/@raul.vizcarrach/list/neural-networks-098e9b594f19" rel="noopener">that I’ve gathered here</a>, and which you might find useful.</p><p id="55e2" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz gg"><em class="nu">Why choose a Convolutional Neural Network (CNN)?</em></strong> Honestly, it wasn’t my first choice. Initially, I tried building a model with <a class="af nt" href="https://landing.ai/" rel="noopener ugc nofollow" target="_blank">LandingAI</a>, a user-friendly platform for cloud deployment and <a class="af nt" href="https://landing.ai/blog/build-your-custom-computer-vision-app-with-python-library" rel="noopener ugc nofollow" target="_blank">Python connection through APIs</a>. However, latency issues appeared <em class="nu">(over 1,000 frames to process online)</em>. Similar latency problems occurred with pre-trained models in <a class="af nt" href="https://universe.roboflow.com/" rel="noopener ugc nofollow" target="_blank">Roboflow</a>, despite their quality datasets and pre-trained models. Realizing the need to run it locally, I tried an MSE-based method to classify jersey colors for team and referee detection. While it sounded like the final solution, it showed low accuracy. After days of trial and error, I switched to CNNs. Among different deep learning approaches, CNNs are well-suited for object detection, unlike LSTM or RNN, which are better fit for sequential data like language transcription or translation.</p><p id="eea3" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Before diving into the code, let’s cover some basic concepts about its architecture:</p><ul class=""><li id="9e00" class="mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns pp pi pj bk"><strong class="mz gg">Sample Dataset for learning: </strong>The dataset has been classified into three classes: <strong class="mz gg">Referee</strong>, <strong class="mz gg">Team_Away</strong> (White jersey players), and <strong class="mz gg">Team_Home</strong> (Yellow jersey players). A sample of each class has been divided into two sets: training data and validation data. The training data will be used by the CNN in each iteration (Epoch) to “learn” patterns across multiple layers. The validation data will be used at the end of each iteration to evaluate the model’s performance and measure how well it generalizes to new data. Creating the sample dataset wasn’t too hard; it took me around 30 to 40 minutes to crop sample images from each class from the video and organize them into subdirectories. I managed to create a sample dataset of approximately 90 images that you can find in the <a class="af nt" href="https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch" rel="noopener ugc nofollow" target="_blank">project’s GitHub repository</a>.</li><li id="e486" class="mx my gf mz b hd pk nb nc hg pl ne nf ng pm ni nj nk pn nm nn no po nq nr ns pp pi pj bk"><strong class="mz gg">How does the model learn?: </strong>Input data moves through each layer of the neural network, which can have one or multiple layers linked together to make predictions. Every layer uses an activation function that processes data to make predictions or introduce changes to the data. Each connection between these layers has a weight, which determines how much influence one layer’s output has on the next. The goal is to find the right combination of these weights that minimize mistakes when predicting outcomes. Through a process called backpropagation and a loss function, the model adjusts these weights to reduce errors and improve accuracy. This process repeats in what’s called an <strong class="mz gg">Epoch (forward pass + backpropagation)</strong>, with the model getting better at making predictions in each cycle as it learns from its mistakes.</li><li id="b595" class="mx my gf mz b hd pk nb nc hg pl ne nf ng pm ni nj nk pn nm nn no po nq nr ns pp pi pj bk"><strong class="mz gg">Activation Function:</strong> As mentioned before, the activation function plays an important role in the model’s learning process. I chose <strong class="mz gg">ReLU (Rectified Linear Unit)</strong> because it is known for being computationally efficient and mitigating what is called the vanishing gradient problem <em class="nu">(where networks with multiple layers may stop learning effectively).</em> While ReLU works well, <a class="af nt" href="https://www.v7labs.com/blog/neural-networks-activation-functions" rel="noopener ugc nofollow" target="_blank">other functions</a> like <strong class="mz gg">sigmoid</strong>, <strong class="mz gg">tanh</strong>, or <strong class="mz gg">swish</strong> also have their uses depending on how complex the network is.</li><li id="33d8" class="mx my gf mz b hd pk nb nc hg pl ne nf ng pm ni nj nk pn nm nn no po nq nr ns pp pi pj bk"><strong class="mz gg">Epochs:</strong> Setting the right number of epochs involves experimentation. You should take into account factors such as the complexity of the dataset, the architecture of your CNN model, and computational resources. In most cases, it is best to monitor the model’s performance in each iteration and stop training when improvements become minimal to prevent overfitting. Given my small training dataset, <strong class="mz gg">I decided to start with 10 epochs as a baseline</strong>. However, adjustments may be necessary in other scenarios based on metric performance and validation results.</li><li id="19fe" class="mx my gf mz b hd pk nb nc hg pl ne nf ng pm ni nj nk pn nm nn no po nq nr ns pp pi pj bk"><strong class="mz gg">Adam (Adaptive Moment Estimation):</strong> Ultimately, the goal is to reduce the error between predicted and true outputs. As mentioned before, backpropagation plays a key role here by adjusting and updating neural network weights to improve predictions over time. While backpropagation handles weight updates based on gradients from the loss function, the Adam algorithm enhances this process by dynamically adjusting the learning rate to gradually minimize the error or loss function. In other words, it fine-tunes how quickly the model learns.</li></ul><p id="49fe" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">That said in order to run our CNN model we will need the following Python packages:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="abc3" class="pu nw gf pr b bg pv pw l px py">pip install torch torchvision <br/>pip install matplotlib <br/>pip install scikit-learn</span></pre><blockquote class="qa qb qc"><p id="91b3" class="mx my nu mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz gg">Tip-02: </strong>Ensure that PyTorch it’s installed properly. All my tools are set up in an Anaconda environment, and when I installed PyTorch, at first, it seemed that it was set up correctly. However, some issues appeared while running some libraries. Initially, I thought it was the code, but after several revisions and no success, I had to reinstall Anaconda and install PyTorch in a clean environment, and with that, problem fixed!</p></blockquote><p id="03ff" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Next, we’ll specify our libraries and the path of our sample dataset:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="b82a" class="pu nw gf pr b bg pv pw l px py"># ************CONVOLUTIONAL NEURAL NETWORK-THREE CLASSES DETECTION**************************<br/># REFEREE<br/># WHITE TEAM (Team_away)<br/># YELLOW TEAM (Team_home)<br/><br/>import os<br/>import torch<br/>import torch.nn as nn<br/>import torch.optim as optim<br/>import torch.nn.functional as F<br/>import torchvision.transforms as transforms<br/>import torchvision.datasets as datasets<br/>from torch.utils.data import DataLoader<br/>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score<br/>import matplotlib.pyplot as plt<br/><br/>#Training and Validation Datasets<br/>#Download the teams_sample_dataset file from the project's GitHub repository<br/>data_dir = 'D:/PYTHON/teams_sample_dataset' </span></pre><p id="0fa7" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">First, we will ensure each picture is equally sized (resize to 150x150 pixels), then convert it to a format that the code can understand (in PyTorch, input data is typically represented as Tensor objects). Finally, we will adjust the colors to make them easier for the model to work with (normalize) and set up a procedure to load the images. These steps together help prepare the pictures and organize them so the model can effectively start learning from them, avoiding deviations caused by data format.</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="a7da" class="pu nw gf pr b bg pv pw l px py">#******************************Data transformation***********************************<br/>transform = transforms.Compose([<br/>    transforms.Resize((150, 150)),<br/>    transforms.ToTensor(),<br/>    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])<br/>])<br/><br/># Load dataset<br/>train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform)<br/>val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=transform)<br/><br/>train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)<br/>val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)</span></pre><p id="4028" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Next we’ll define our CNN’s architecture:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="d533" class="pu nw gf pr b bg pv pw l px py">#********************************CNN Model Architecture**************************************<br/>class CNNModel(nn.Module):<br/>    def __init__(self):<br/>        super(CNNModel, self).__init__()<br/>        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)<br/>        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)<br/>        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)<br/>        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)<br/>        self.fc1 = nn.Linear(128 * 18 * 18, 512)<br/>        self.dropout = nn.Dropout(0.5)<br/>        self.fc2 = nn.Linear(512, 3)  #Three Classes (Referee, Team_away,Team_home)<br/>        <br/>    def forward(self, x):<br/>        x = self.pool(F.relu(self.conv1(x)))<br/>        x = self.pool(F.relu(self.conv2(x)))<br/>        x = self.pool(F.relu(self.conv3(x)))<br/>        x = x.view(-1, 128 * 18 * 18)<br/>        x = F.relu(self.fc1(x))<br/>        x = self.dropout(x)<br/>        x = self.fc2(x)  <br/>        return x</span></pre><p id="cb19" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">You will notice that our CNN model has three layers (conv1, conv2, conv3). The data begins in the convolutional layer (conv), where the activation function (ReLU) is applied. This function enables the network to learn complex patterns and relationships in the data. Following this, the pooling layer is activated. <strong class="mz gg"><em class="nu">What is Max Pooling?</em></strong> It’s a technique that reduces the image size while retaining important features, which helps in efficient training and optimizes memory resources. This process repeats across conv1 to conv3. Finally, the data passes through fully connected layers (fc1, fc2) for final classification (or decision-making).</p><p id="ffaa" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">As the next step, we initialize our model, configure categorical cross-entropy as the loss function <em class="nu">(commonly used for classification tasks)</em>, and designate Adam as our optimizer. As mentioned earlier, we’ll execute our model over a full cycle of 10 epochs.</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="07f9" class="pu nw gf pr b bg pv pw l px py">#********************************CNN TRAINING**********************************************<br/><br/># Model-loss function-optimizer<br/>model = CNNModel()<br/>criterion = nn.CrossEntropyLoss()<br/>optimizer = optim.Adam(model.parameters(), lr=0.001)<br/><br/>#*********************************Training*************************************************<br/>num_epochs = 10<br/>train_losses, val_losses = [], []<br/><br/>for epoch in range(num_epochs):<br/>    model.train()<br/>    running_loss = 0.0<br/>    for inputs, labels in train_loader:<br/>        optimizer.zero_grad()<br/>        outputs = model(inputs)<br/>        labels = labels.type(torch.LongTensor)  <br/>        loss = criterion(outputs, labels)<br/>        loss.backward()<br/>        optimizer.step()<br/>        running_loss += loss.item()<br/>    <br/>    train_losses.append(running_loss / len(train_loader))<br/>    <br/>    model.eval()<br/>    val_loss = 0.0<br/>    all_labels = []<br/>    all_preds = []<br/>    with torch.no_grad():<br/>        for inputs, labels in val_loader:<br/>            outputs = model(inputs)<br/>            labels = labels.type(torch.LongTensor)  <br/>            loss = criterion(outputs, labels)<br/>            val_loss += loss.item()<br/>            _, preds = torch.max(outputs, 1)  <br/>            all_labels.extend(labels.tolist())<br/>            all_preds.extend(preds.tolist())</span></pre><p id="39b3" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">To track performance, we will add some code to follow the training progress, print validation metrics, and plot them. Finally, we save the model as <strong class="mz gg">hockey_team_classifier.pth</strong> in a designated path of your choice.</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="4a41" class="pu nw gf pr b bg pv pw l px py">#********************************METRICS &amp; PERFORMANCE************************************<br/>    <br/>    val_losses.append(val_loss / len(val_loader))<br/>    val_accuracy = accuracy_score(all_labels, all_preds)<br/>    val_precision = precision_score(all_labels, all_preds, average='macro', zero_division=1)<br/>    val_recall = recall_score(all_labels, all_preds, average='macro', zero_division=1)<br/>    val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=1)<br/>    <br/>    print(f"Epoch [{epoch + 1}/{num_epochs}], "<br/>          f"Loss: {train_losses[-1]:.4f}, "<br/>          f"Val Loss: {val_losses[-1]:.4f}, "<br/>          f"Val Acc: {val_accuracy:.2%}, "<br/>          f"Val Precision: {val_precision:.4f}, "<br/>          f"Val Recall: {val_recall:.4f}, "<br/>          f"Val F1 Score: {val_f1:.4f}")<br/>    <br/>#*******************************SHOW METRICS &amp; PERFORMANCE**********************************<br/>plt.plot(train_losses, label='Train Loss')<br/>plt.plot(val_losses, label='Validation Loss')<br/>plt.legend()<br/>plt.show()<br/><br/># SAVE THE MODEL FOR THE GH_CV_track_teams CODE<br/>torch.save(model.state_dict(), 'D:/PYTHON/hockey_team_classifier.pth')</span></pre><p id="e245" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Additionally, alongside your “pth” file, after running through all the steps described above <em class="nu">(you can find the complete code in </em><a class="af nt" href="https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch" rel="noopener ugc nofollow" target="_blank"><em class="nu">the project’s GitHub repository</em></a><em class="nu">, you should expect to see an output like the following (metrics may vary slightly)</em>:</p><figure class="ox oy oz pa pb fw fo fp paragraph-image"><div class="fo fp qf"><img src="../Images/91ff066093e4c561b0f5b20528f75f57.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*3Op0K3xta31kxsU7_QX4Uw.png"/></div><figcaption class="pc pd pe fo fp pf pg bf b bg z dx">Figure 05: CNN model performance metrics</figcaption></figure><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="ebc3" class="pu nw gf pr b bg pv pw l px py">#**************CNN PERFORMANCE ACROSS TRAINING EPOCHS************************<br/><br/>Epoch [1/10], Loss: 1.5346, Val Loss: 1.2339, Val Acc: 47.37%, Val Precision: 0.7172, Val Recall: 0.5641, Val F1 Score: 0.4167<br/>Epoch [2/10], Loss: 1.1473, Val Loss: 1.1664, Val Acc: 55.26%, Val Precision: 0.6965, Val Recall: 0.6296, Val F1 Score: 0.4600<br/>Epoch [3/10], Loss: 1.0139, Val Loss: 0.9512, Val Acc: 57.89%, Val Precision: 0.6054, Val Recall: 0.6054, Val F1 Score: 0.5909<br/>Epoch [4/10], Loss: 0.8937, Val Loss: 0.8242, Val Acc: 60.53%, Val Precision: 0.7222, Val Recall: 0.5645, Val F1 Score: 0.5538<br/>Epoch [5/10], Loss: 0.7936, Val Loss: 0.7177, Val Acc: 63.16%, Val Precision: 0.6667, Val Recall: 0.6309, Val F1 Score: 0.6419<br/>Epoch [6/10], Loss: 0.6871, Val Loss: 0.7782, Val Acc: 68.42%, Val Precision: 0.6936, Val Recall: 0.7128, Val F1 Score: 0.6781<br/>Epoch [7/10], Loss: 0.6276, Val Loss: 0.5684, Val Acc: 78.95%, Val Precision: 0.8449, Val Recall: 0.7523, Val F1 Score: 0.7589<br/>Epoch [8/10], Loss: 0.4198, Val Loss: 0.5613, Val Acc: 86.84%, Val Precision: 0.8736, Val Recall: 0.8958, Val F1 Score: 0.8653<br/>Epoch [9/10], Loss: 0.3959, Val Loss: 0.3824, Val Acc: 92.11%, Val Precision: 0.9333, Val Recall: 0.9213, Val F1 Score: 0.9243<br/>Epoch [10/10], Loss: 0.2509, Val Loss: 0.2651, Val Acc: 97.37%, Val Precision: 0.9762, Val Recall: 0.9792, Val F1 Score: 0.9769<br/></span></pre><p id="b6ad" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">After finishing 10 epochs, the CNN model shows improvement in performance metrics. Initially, in Epoch 1, the model starts with a training loss of 1.5346 and a validation accuracy of 47.37%. <strong class="mz gg"><em class="nu">How should we understand this initial point?</em></strong></p><p id="193d" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz gg">Accuracy</strong> is one of the most common metrics for evaluating classification performance. In our case, it represents the proportion of correctly predicted classes out of the total. <strong class="mz gg">However, high accuracy alone doesn’t guarantee overall model performance</strong>; you still can have poor predictions for specific classes (as I experienced in early trials). Regarding<strong class="mz gg"> training loss</strong>, it measures how effectively the model learns to map input data to the correct labels. Since we’re using a classification function, <strong class="mz gg">Cross-Entropy Loss </strong>quantifies the difference between predicted class probabilities and actual labels. A starting value like 1.5346 indicates significant differences between predicted and actual classes; ideally, this value should approach 0 as training progresses. As epochs progress, we observe a significant drop in training loss and an increase in validation accuracy. By the final epoch, the training and validation loss reach lows of 0.2509 and 0.2651, respectively.</p><p id="964b" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">To test our CNN model, we can select a sample of player images and evaluate its prediction capability. For testing, you can run the following code and utilize the <strong class="mz gg">validation_dataset folder</strong> in the <a class="af nt" href="https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch" rel="noopener ugc nofollow" target="_blank">project’s GitHub repository</a>.</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="33e2" class="pu nw gf pr b bg pv pw l px py"># *************TEST CNN MODEL WITH SAMPLE DATASET***************************<br/><br/>import os<br/>import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torchvision.transforms as transforms<br/>from PIL import Image<br/><br/># SAMPLE DATASET FOR VALIDATION<br/>test_dir = 'D:/PYTHON/validation_dataset'<br/><br/># CNN MODEL FOR TEAM PREDICTIONS<br/>class CNNModel(nn.Module):<br/>    def __init__(self):<br/>        super(CNNModel, self).__init__()<br/>        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)<br/>        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)<br/>        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)<br/>        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)<br/>        self.fc1 = nn.Linear(128 * 18 * 18, 512)<br/>        self.dropout = nn.Dropout(0.5)<br/>        self.fc2 = nn.Linear(512, 3) <br/>        <br/>    def forward(self, x):<br/>        x = self.pool(F.relu(self.conv1(x)))<br/>        x = self.pool(F.relu(self.conv2(x)))<br/>        x = self.pool(F.relu(self.conv3(x)))<br/>        x = x.view(-1, 128 * 18 * 18)<br/>        x = F.relu(self.fc1(x))<br/>        x = self.dropout(x)<br/>        x = self.fc2(x)  <br/>        return x<br/><br/># CNN MODEL PREVIOUSLY SAVED<br/>model = CNNModel()<br/>model.load_state_dict(torch.load('D:/PYTHON/hockey_team_classifier.pth'))<br/>model.eval()<br/><br/>transform = transforms.Compose([<br/>    transforms.Resize((150, 150)),<br/>    transforms.ToTensor(),<br/>    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])<br/>])<br/><br/>#******************ITERATION ON SAMPLE IMAGES-ACCURACY TEST*****************************<br/><br/>class_names = ['team_referee', 'team_away', 'team_home']<br/><br/>def predict_image(image_path, model, transform):<br/># LOADS DATASET<br/>    image = Image.open(image_path)<br/>    image = transform(image).unsqueeze(0)  <br/><br/># MAKES PREDICTIONS<br/>    with torch.no_grad():<br/>        output = model(image)<br/>        _, predicted = torch.max(output, 1)  <br/>        team = class_names[predicted.item()]<br/>    return team<br/><br/>for image_name in os.listdir(test_dir):<br/>    image_path = os.path.join(test_dir, image_name)<br/>    if os.path.isfile(image_path):  <br/>        predicted_team = predict_image(image_path, model, transform)<br/>        print(f'Image {image_name}: The player belongs to {predicted_team}')</span></pre><p id="0d07" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The output should look something like this:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="3aaf" class="pu nw gf pr b bg pv pw l px py"><br/># *************CNN MODEL TEST - OUTPUT ***********************************#<br/><br/>Image Away_image04.jpg: The player belongs to team_away<br/>Image Away_image12.jpg: The player belongs to team_away<br/>Image Away_image14.jpg: The player belongs to team_away<br/>Image Home_image07.jpg: The player belongs to team_home<br/>Image Home_image13.jpg: The player belongs to team_home<br/>Image Home_image16.jpg: The player belongs to team_home<br/>Image Referee_image04.jpg: The player belongs to team_referee<br/>Image Referee_image09.jpg: The player belongs to team_referee<br/>Image Referee_image10.jpg: The player belongs to team_referee<br/>Image Referee_image11.jpg: The player belongs to team_referee</span></pre><p id="593b" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">As you can see, the model shows quite good ability in identifying teams and excluding the referee as a team player.</p><blockquote class="qa qb qc"><p id="d0f3" class="mx my nu mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz gg">Tip #03:</strong> Something I learned during the CNN design process is that adding complexity doesn’t always improve performance. Initially, I experimented with deeper models (more convolutional layers) and color-based augmentation to enhance players’ jersey recognition. However, in my small dataset, I encountered overfitting rather than learning generalizable features (all images were predicted as white team players or referees). Regularization techniques like dropout and batch normalization are also important; they help impose constraints during training, ensuring the model can generalize well to new data. Less can sometimes mean more in terms of results😁.</p></blockquote><h1 id="59dd" class="nv nw gf bf nx ny nz hf oa ob oc hi od oe of og oh oi oj ok ol om on oo op oq bk"><strong class="al">PUTING IT ALL TOGETHER</strong></h1><p id="32c0" class="pw-post-body-paragraph mx my gf mz b hd or nb nc hg os ne nf ng ot ni nj nk ou nm nn no ov nq nr ns fj bk">Putting it all together will require some adjustments to our tracking mechanism described earlier. Here’s a breakdown of the updated code step by step.</p><p id="44dd" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">First, we’ll set up the libraries and paths we need. Note that the paths for our pickle file and the CNN model are specified now. <strong class="mz gg">This time, if the pickle file isn’t found in the path, the code will throw an error</strong>. Use the previous code to generate the pickle file if needed, and use this updated version to perform the video analysis:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="96c0" class="pu nw gf pr b bg pv pw l px py"><br/>import cv2<br/>import numpy as np<br/>from ultralytics import YOLO<br/>import pickle<br/>import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torchvision.transforms as transforms<br/>from PIL import Image<br/><br/># MODEL INPUTS<br/>model_path = 'D:/PYTHON/yolov8x.pt'<br/>video_path = 'D:/PYTHON/video_input.mp4'<br/>output_path = 'D:/PYTHON/output_video.mp4'<br/>tracks_path = 'D:/PYTHON/stubs/track_stubs.pkl'<br/>classifier_path = 'D:/PYTHON/hockey_team_classifier.pth'</span></pre><p id="cab3" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Next, we will load the models, specify the rink coordinates, and initiate the process of detecting objects in each frame in batches of 20, as we did before. Note that for now, we will only use the rink boundaries to focus the analysis on the rink. In the final steps of the article, when we include performance stats, we’ll use the offensive zone coordinates.</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="3989" class="pu nw gf pr b bg pv pw l px py"><br/>#*************************** Loads models and rink coordinates********************#<br/>class_names = ['Referee', 'Tm_white', 'Tm_yellow']<br/><br/>class HockeyAnalyzer:<br/>    def __init__(self, model_path, classifier_path):<br/>        self.model = YOLO(model_path)<br/>        self.classifier = self.load_classifier(classifier_path)<br/>        self.transform = transforms.Compose([<br/>            transforms.Resize((150, 150)),<br/>            transforms.ToTensor(),<br/>            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])<br/>        ])<br/>        self.rink_coordinates = np.array([[-450, 710], [2030, 710], [948, 61], [352, 61]])<br/>        self.zone_white = [(180, 150), (1100, 150), (900, 61), (352, 61)]<br/>        self.zone_yellow = [(-450, 710), (2030, 710), (1160, 150), (200, 150)]<br/><br/>#******************** Detect objects in each frame **********************************#<br/>    def detect_frames(self, frames):<br/>        batch_size = 20 <br/>        detections = [] <br/>        for i in range(0, len(frames), batch_size):<br/>            detections_batch = self.model.predict(frames[i:i+batch_size], conf=0.1)<br/>            detections += detections_batch<br/>        return detections</span></pre><p id="0830" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Next, we’ll add the process to predict each player’s team:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="031f" class="pu nw gf pr b bg pv pw l px py">#*********************** Loads CNN Model**********************************************#<br/><br/>    def load_classifier(self, classifier_path):<br/>        model = CNNModel()<br/>        model.load_state_dict(torch.load(classifier_path, map_location=torch.device('cpu')))<br/>        model.eval()<br/>        return model<br/><br/>    def predict_team(self, image):<br/>        with torch.no_grad():<br/>            output = self.classifier(image)<br/>            _, predicted = torch.max(output, 1)<br/>            predicted_index = predicted.item()<br/>            team = class_names[predicted_index]<br/>        return team</span></pre><p id="cc2d" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">As the next step, we’ll add the method described earlier to switch from bounding boxes to ellipses:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="36d7" class="pu nw gf pr b bg pv pw l px py">#************ Ellipse for tracking players instead of Bounding boxes*******************#<br/>    def draw_ellipse(self, frame, bbox, color, track_id=None, team=None):<br/>        y2 = int(bbox[3])<br/>        x_center = (int(bbox[0]) + int(bbox[2])) // 2<br/>        width = int(bbox[2]) - int(bbox[0])<br/>    <br/>        if team == 'Referee':<br/>            color = (0, 255, 255)<br/>            text_color = (0, 0, 0)<br/>        else:<br/>            color = (255, 0, 0)<br/>            text_color = (255, 255, 255)<br/><br/>        cv2.ellipse(<br/>            frame,<br/>            center=(x_center, y2),<br/>            axes=(int(width) // 2, int(0.35 * width)),<br/>            angle=0.0,<br/>            startAngle=-45,<br/>            endAngle=235,<br/>            color=color,<br/>            thickness=2,<br/>            lineType=cv2.LINE_4<br/>        )<br/>    <br/>        if track_id is not None:<br/>            rectangle_width = 40<br/>            rectangle_height = 20<br/>            x1_rect = x_center - rectangle_width // 2<br/>            x2_rect = x_center + rectangle_width // 2<br/>            y1_rect = (y2 - rectangle_height // 2) + 15<br/>            y2_rect = (y2 + rectangle_height // 2) + 15<br/>    <br/>            cv2.rectangle(frame,<br/>                          (int(x1_rect), int(y1_rect)),<br/>                          (int(x2_rect), int(y2_rect)),<br/>                          color,<br/>                          cv2.FILLED)<br/>    <br/>            x1_text = x1_rect + 12<br/>            if track_id &gt; 99:<br/>                x1_text -= 10<br/>            font_scale = 0.4<br/>            cv2.putText(<br/>                frame,<br/>                f"{track_id}",<br/>                (int(x1_text), int(y1_rect + 15)),<br/>                cv2.FONT_HERSHEY_SIMPLEX,<br/>                font_scale,<br/>                text_color,<br/>                thickness=2<br/>            )<br/><br/>        return frame</span></pre><p id="550e" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Now, it’s time to add the analyzer that includes reading the pickle file, narrowing the analysis within the rink boundaries we defined earlier, and calling the CNN model to identify each player’s team and add labels. Note that we include a feature to label referees with a different color and change the color of their ellipses as well. The code ends with writing processed frames to an output video.</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="c492" class="pu nw gf pr b bg pv pw l px py">#******************* Loads Tracked Data (pickle file )**********************************#<br/><br/>    def analyze_video(self, video_path, output_path, tracks_path):<br/>          with open(tracks_path, 'rb') as f:<br/>              tracks = pickle.load(f)<br/><br/>          cap = cv2.VideoCapture(video_path)<br/>          if not cap.isOpened():<br/>              print("Error: Could not open video.")<br/>              return<br/>          <br/>          fps = cap.get(cv2.CAP_PROP_FPS)<br/>          frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))<br/>          frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))<br/><br/>          fourcc = cv2.VideoWriter_fourcc(*'XVID')<br/>          out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))<br/><br/>          frame_num = 0<br/>          while cap.isOpened():<br/>              ret, frame = cap.read()<br/>              if not ret:<br/>                  break<br/>              <br/>#***********Checks if the player falls within the rink area**********************************#<br/>              mask = np.zeros(frame.shape[:2], dtype=np.uint8)<br/>              cv2.fillConvexPoly(mask, self.rink_coordinates, 1)<br/>              mask = mask.astype(bool)<br/>              # Draw rink area<br/>              #cv2.polylines(frame, [self.rink_coordinates], isClosed=True, color=(0, 255, 0), thickness=2)<br/>  <br/>              # Get tracks from frame<br/>              player_dict = tracks["person"][frame_num]<br/>              for track_id, player in player_dict.items():<br/>                  bbox = player["bbox"]<br/><br/>              # Check if the player is within the Rink Area<br/>                  x_center = int((bbox[0] + bbox[2]) / 2)<br/>                  y_center = int((bbox[1] + bbox[3]) / 2)<br/><br/>                  if not mask[y_center, x_center]:<br/>                      continue  <br/><br/>#**********************************Team Prediction********************************************#<br/>                  x1, y1, x2, y2 = map(int, bbox)<br/>                  cropped_image = frame[y1:y2, x1:x2]<br/>                  cropped_pil_image = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))<br/>                  transformed_image = self.transform(cropped_pil_image).unsqueeze(0)<br/>                  team = self.predict_team(transformed_image)<br/><br/>#************ Ellipse for tracked players and labels******************************************#<br/>                  self.draw_ellipse(frame, bbox, (0, 255, 0), track_id, team)<br/>                  <br/>                  font_scale = 1  <br/>                  text_offset = -20  <br/>                  <br/>                  if team == 'Referee':<br/>                      rectangle_width = 60<br/>                      rectangle_height = 25<br/>                      x1_rect = x1<br/>                      x2_rect = x1 + rectangle_width<br/>                      y1_rect = y1 - 30<br/>                      y2_rect = y1 - 5<br/>                      # Different setup for Referee<br/>                      cv2.rectangle(frame,<br/>                                    (int(x1_rect), int(y1_rect)),<br/>                                    (int(x2_rect), int(y2_rect)),<br/>                                    (0, 0, 0),  <br/>                                    cv2.FILLED)<br/>                      text_color = (255, 255, 255)  <br/>                  else:<br/>                      if team == 'Tm_white':<br/>                          text_color = (255, 215, 0)  # White Team: Blue labels<br/>                      else:<br/>                          text_color = (0, 255, 255)  # Yellow Team: Yellow labels<br/>                  <br/>              # Draw Team labels<br/>                  cv2.putText(<br/>                      frame,<br/>                      team,<br/>                      (int(x1), int(y1) + text_offset), <br/>                      cv2.FONT_HERSHEY_PLAIN,            <br/>                      font_scale,<br/>                      text_color,<br/>                      thickness=2<br/>                  )<br/><br/>              # Write output video<br/>              out.write(frame)<br/>              frame_num += 1<br/><br/>          cap.release()<br/>          out.release()</span></pre><p id="28ea" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Finally, we add the CNN’s architecture (defined in the CNN design process) and execute the Hockey analyzer:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="8345" class="pu nw gf pr b bg pv pw l px py"><br/>#**********************CNN Model Architecture ******************************#<br/>class CNNModel(nn.Module):<br/>    def __init__(self):<br/>        super(CNNModel, self).__init__()<br/>        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)<br/>        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)<br/>        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)<br/>        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)<br/>        self.fc1 = nn.Linear(128 * 18 * 18, 512)<br/>        self.dropout = nn.Dropout(0.5)<br/>        self.fc2 = nn.Linear(512, len(class_names))  <br/>        <br/>    def forward(self, x):<br/>        x = self.pool(F.relu(self.conv1(x)))<br/>        x = self.pool(F.relu(self.conv2(x)))<br/>        x = self.pool(F.relu(self.conv3(x)))<br/>        x = x.view(-1, 128 * 18 * 18)<br/>        x = F.relu(self.fc1(x))<br/>        x = self.dropout(x)<br/>        x = self.fc2(x)<br/>        return x<br/><br/>#*********Execute HockeyAnalyzer/classifier and Save Output************#<br/>analyzer = HockeyAnalyzer(model_path, classifier_path)<br/>analyzer.analyze_video(video_path, output_path, tracks_path)</span></pre><p id="1630" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">After running all the steps, your video output should look something like this:</p><figure class="ox oy oz pa pb fw fo fp paragraph-image"><div class="fo fp pz"><img src="../Images/98890f2c70834d1f61d97b6135b3afe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*X7AtM-owNUAVxaGv3QckyQ.gif"/></div><figcaption class="pc pd pe fo fp pf pg bf b bg z dx">Sample Clip 06: Tracking Players and Teams</figcaption></figure><p id="d261" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Note that in this last update, object detections are only within the ice rink, and teams are differentiated, as well as the referee. While the CNN model still needs fine-tuning and occasionally loses stability with some players, it remains mostly reliable and accurate throughout the video.</p><h1 id="4819" class="nv nw gf bf nx ny nz hf oa ob oc hi od oe of og oh oi oj ok ol om on oo op oq bk"><strong class="al">SPEED, DISTANCE AND OFFENSIVE PRESSURE</strong></h1><p id="80f2" class="pw-post-body-paragraph mx my gf mz b hd or nb nc hg os ne nf ng ot ni nj nk ou nm nn no ov nq nr ns fj bk">The ability to track teams and players opens up exciting possibilities for measuring performance, such as generating heatmaps, analyzing speed and distance covered, tracking movements like zone entries or exits, and diving into detailed player metrics. In order we can have a taste of it, we’ll add three performance metrics: <strong class="mz gg">average speed per player</strong>, skating <strong class="mz gg">distance covered </strong>by each team, and o<strong class="mz gg">ffensive pressure </strong><em class="nu">(measured as the percentage of distance covered by each team spent in its opponent’s zone)</em>. I’ll leave more detailed statistics up to you!</p><p id="8d75" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">We begin adapting the coordinates of the ice rink from pixel-based measurements to approximate meters. This adjustment allows us to read our data in meters rather than pixels. The real-world dimensions of the ice rink seen in the video are approximately 15mx30m (15 meters in width and 30 meters in height). To facilitate this conversion, we introduce a method to convert pixel coordinates to meters. By defining the rink’s actual dimensions and using the pixel coordinates of its corners (from left to right and top to bottom), we obtain conversion factors. These factors will support our process of estimating distances in meters and speeds in meters per second. <em class="nu">(Another interesting technique you can explore and apply is Perspective Transformation)</em></p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="59aa" class="pu nw gf pr b bg pv pw l px py">#*********************Loads models and rink coordinates*****************#<br/>class_names = ['Referee', 'Tm_white', 'Tm_yellow']<br/><br/>class HockeyAnalyzer:<br/>    def __init__(self, model_path, classifier_path):<br/>        *<br/>        *<br/>        *<br/>        *<br/>        *<br/>        *<br/>        self.pixel_to_meter_conversion() #&lt;------ Add this utility method<br/><br/>#***********Pixel-based measurements to meters***************************#<br/>    def pixel_to_meter_conversion(self):<br/>        #Rink real dimensions in meters<br/>        rink_width_m = 15<br/>        rink_height_m = 30<br/><br/>        #Pixel coordinates for rink dimensions<br/>        left_pixel, right_pixel = self.rink_coordinates[0][0], self.rink_coordinates[1][0]<br/>        top_pixel, bottom_pixel = self.rink_coordinates[2][1], self.rink_coordinates[0][1]<br/><br/>        #Conversion factors<br/>        self.pixels_per_meter_x = (right_pixel - left_pixel) / rink_width_m<br/>        self.pixels_per_meter_y = (bottom_pixel - top_pixel) / rink_height_m<br/><br/>    def convert_pixels_to_meters(self, distance_pixels):<br/>        #Convert pixels to meters<br/>        return distance_pixels / self.pixels_per_meter_x, distance_pixels / self.pixels_per_meter_y</span></pre><p id="dd43" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">We are now ready to <strong class="mz gg">add speed to each player measured in meters per second</strong>. To do this, we’ll need to make three modifications. First, initiate an empty dictionary named <strong class="mz gg">previous_positions</strong> in the <strong class="mz gg">HockeyAnalyzer class</strong> to help us compare the current and previous positions of players. Similarly, we’ll create a t<strong class="mz gg">eam_stats</strong> structure to store stats from each team for further visualization.</p><p id="c5e3" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Next, we will add a <strong class="mz gg">speed method </strong>to estimate players’ speed in pixels per second, and then use the conversion factor (explained earlier) to transform it into meters per second. Finally, from the <strong class="mz gg">analyze_video method</strong>, we’ll call our new speed method and add the speed to each tracked object (players and referee). This is what the changes look like:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="2d0b" class="pu nw gf pr b bg pv pw l px py">#*********************Loads models and rink coordinates*****************#<br/>class_names = ['Referee', 'Tm_white', 'Tm_yellow']<br/><br/>class HockeyAnalyzer:<br/>    def __init__(self, model_path, classifier_path):<br/>        *<br/>        *<br/>        *<br/>        *<br/>        *<br/>        *<br/>        *<br/>        self.pixel_to_meter_conversion() <br/>        self.previous_positions = {} #&lt;------ Add this.Initializes empty dictionary <br/>        self.team_stats = {<br/>                    'Tm_white': {'distance': 0, 'speed': [], 'count': 0, 'offensive_pressure': 0},<br/>                    'Tm_yellow': {'distance': 0, 'speed': [], 'count': 0, 'offensive_pressure': 0}<br/>                } #&lt;------ Add this.Initializes empty dictionary<br/><br/>#**************** Speed: meters per second********************************#<br/>    def calculate_speed(self, track_id, x_center, y_center, fps):<br/>        current_position = (x_center, y_center)<br/>        if track_id in self.previous_positions:<br/>            prev_position = self.previous_positions[track_id]<br/>            distance_pixels = np.linalg.norm(np.array(current_position) - np.array(prev_position))<br/>            distance_meters_x, distance_meters_y = self.convert_pixels_to_meters(distance_pixels)<br/>            speed_meters_per_second = (distance_meters_x**2 + distance_meters_y**2)**0.5 * fps<br/>        else:<br/>            speed_meters_per_second = 0<br/>        self.previous_positions[track_id] = current_position<br/>        return speed_meters_per_second<br/><br/>#******************* Loads Tracked Data (pickle file )**********************************#<br/><br/>    def analyze_video(self, video_path, output_path, tracks_path):<br/>          with open(tracks_path, 'rb') as f:<br/>              tracks = pickle.load(f)<br/><br/>        *<br/>        *<br/>        *<br/>        *<br/>        *<br/>        *<br/>        *<br/>        *<br/>              # Draw Team label<br/>                  cv2.putText(<br/>                      frame,<br/>                      team,<br/>                      (int(x1), int(y1) + text_offset), <br/>                      cv2.FONT_HERSHEY_PLAIN,            <br/>                      font_scale,<br/>                      text_color,<br/>                      thickness=2<br/>                  )<br/><br/>#**************Add these lines of code ---&gt;:<br/>         <br/>                  speed = self.calculate_speed(track_id, x_center, y_center, fps)<br/>                  # Speed label <br/>                  speed_font_scale = 0.8  <br/>                  speed_y_position = int(y1) + 20<br/>                  if speed_y_position &gt; int(y1) - 5:<br/>                      speed_y_position = int(y1) - 5<br/><br/>                  cv2.putText(<br/>                      frame,<br/>                      f"Speed: {speed:.2f} m/s",  <br/>                      (int(x1), speed_y_position),  <br/>                      cv2.FONT_HERSHEY_PLAIN,       <br/>                      speed_font_scale,<br/>                      text_color,<br/>                      thickness=2<br/>                  )<br/><br/>              # Write output video<br/>              out.write(frame)<br/>              frame_num += 1<br/><br/>          cap.release()<br/>          out.release()</span></pre><p id="4be8" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">If you have troubles adding this new lines of code, you can always visit <a class="af nt" href="https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch" rel="noopener ugc nofollow" target="_blank">the project’s GitHub repository</a>, where you can find the complete integrated code. Your video output at this point should look like this <em class="nu">(notice that the speed has been added to the label of each player)</em>:</p><figure class="ox oy oz pa pb fw fo fp paragraph-image"><div class="fo fp pz"><img src="../Images/56fe2cf37374ce478a5baaa91c6c66a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*FSuIRAXNa99J7wkuqtWhyg.gif"/></div><figcaption class="pc pd pe fo fp pf pg bf b bg z dx">Sample Clip 07: Tracking Players and Speed</figcaption></figure><p id="d54c" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Finally, let’s add a stats board where we can track the average speed per player for each team, along with other metrics such as distance covered and offensive pressure in the opponent’s zone.</p><p id="be38" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">We’ve already defined the offensive zones and integrated them into our code. Now, we need to track how often each player enters their opponent’s zone. To achieve this, we’ll implement a method using the <a class="af nt" href="https://medium.com/@girishajmera/exploring-algorithms-to-determine-points-inside-or-outside-a-polygon-038952946f87" rel="noopener"><strong class="mz gg">ray casting algorithm</strong></a>. This algorithm checks if a player’s position is inside the white or yellow team’s offensive zone. It works by drawing an imaginary line from the player to the target zone. If the line crosses one border, the player is inside, if it crosses more (in our case, two out of four borders), the player is outside. The code then scans the entire video to determine each tracked object’s zone status.</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="f552" class="pu nw gf pr b bg pv pw l px py"><br/>#************ Locate player's position in Target Zone***********************#<br/><br/>    def is_inside_zone(self, position, zone):<br/>          x, y = position<br/>          n = len(zone)<br/>          inside = False<br/>          p1x, p1y = zone[0]<br/>          for i in range(n + 1):<br/>              p2x, p2y = zone[i % n]<br/>              if y &gt; min(p1y, p2y):<br/>                  if y &lt;= max(p1y, p2y):<br/>                      if x &lt;= max(p1x, p2x):<br/>                          if p1y != p2y:<br/>                              xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x<br/>                          if p1x == p2x or x &lt;= xinters:<br/>                              inside = not inside<br/>              p1x, p1y = p2x, p2y<br/>          return inside</span></pre><p id="7ca9" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Now we’ll handle the performance metrics by adding a method that displays <strong class="mz gg">average player speed</strong>, total <strong class="mz gg">distance covered</strong>, and <strong class="mz gg">offensive pressure</strong> (percentage of time spent in the opponent’s zone) on a table format for each team. Using OpenCV, we’ll format these metrics into a table overlaid on the video and we’ll incorporate a dynamic update mechanism to maintain real-time statistics during gameplay.</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="1aa4" class="pu nw gf pr b bg pv pw l px py">#*******************************Performance metrics*********************************************#<br/>    def draw_stats(self, frame):<br/>         avg_speed_white = np.mean(self.team_stats['Tm_white']['speed']) if self.team_stats['Tm_white']['count'] &gt; 0 else 0<br/>         avg_speed_yellow = np.mean(self.team_stats['Tm_yellow']['speed']) if self.team_stats['Tm_yellow']['count'] &gt; 0 else 0<br/>         distance_white = self.team_stats['Tm_white']['distance']<br/>         distance_yellow = self.team_stats['Tm_yellow']['distance']<br/>    <br/>         offensive_pressure_white = self.team_stats['Tm_white'].get('offensive_pressure', 0)<br/>         offensive_pressure_yellow = self.team_stats['Tm_yellow'].get('offensive_pressure', 0)<br/>         <br/>         Pressure_ratio_W = offensive_pressure_white/distance_white   *100  if self.team_stats['Tm_white']['distance'] &gt; 0 else 0<br/>         Pressure_ratio_Y = offensive_pressure_yellow/distance_yellow *100  if self.team_stats['Tm_yellow']['distance'] &gt; 0 else 0<br/>    <br/>         table = [<br/>             ["", "Away_White", "Home_Yellow"],<br/>             ["Average Speed\nPlayer", f"{avg_speed_white:.2f} m/s", f"{avg_speed_yellow:.2f} m/s"],<br/>             ["Distance\nCovered", f"{distance_white:.2f} m", f"{distance_yellow:.2f} m"],<br/>             ["Offensive\nPressure %", f"{Pressure_ratio_W:.2f} %", f"{Pressure_ratio_Y:.2f} %"],<br/>         ]<br/>    <br/>         text_color = (0, 0, 0)  <br/>         start_x, start_y = 10, 590  <br/>         row_height = 30     # Manage Height between rows<br/>         column_width = 150  # Manage Width  between rows<br/>         font_scale = 1  <br/>    <br/>         def put_multiline_text(frame, text, position, font, font_scale, color, thickness, line_type, line_spacing=1.0):<br/>             y0, dy = position[1], int(font_scale * 20 * line_spacing)  # Adjust line spacing here<br/>             for i, line in enumerate(text.split('\n')):<br/>                 y = y0 + i * dy<br/>                 cv2.putText(frame, line, (position[0], y), font, font_scale, color, thickness, line_type)<br/>         <br/>         for i, row in enumerate(table):<br/>             for j, text in enumerate(row):<br/>                 if i in [1,2, 3]:  <br/>                     put_multiline_text(<br/>                         frame,<br/>                         text,<br/>                         (start_x + j * column_width, start_y + i * row_height),<br/>                         cv2.FONT_HERSHEY_PLAIN,<br/>                         font_scale,<br/>                         text_color,<br/>                         1,<br/>                         cv2.LINE_AA,<br/>                         line_spacing= 0.8 <br/>                     )<br/>                 else:<br/>                     cv2.putText(<br/>                         frame,<br/>                         text,<br/>                         (start_x + j * column_width, start_y + i * row_height),<br/>                         cv2.FONT_HERSHEY_PLAIN,<br/>                         font_scale,<br/>                         text_color,<br/>                         1,<br/>                         cv2.LINE_AA,<br/>                     )       <br/>          <br/>#****************** Track and update game stats****************************************#<br/><br/>    def update_team_stats(self, team, speed, distance, position):<br/>        if team in self.team_stats:<br/>            self.team_stats[team]['speed'].append(speed)<br/>            self.team_stats[team]['distance'] += distance<br/>            self.team_stats[team]['count'] += 1<br/><br/>            if team == 'Tm_white':<br/>                if self.is_inside_zone(position, self.zone_white):<br/>                    self.team_stats[team]['offensive_pressure'] += distance<br/>            elif team == 'Tm_yellow':<br/>                if self.is_inside_zone(position, self.zone_yellow):<br/>                    self.team_stats[team]['offensive_pressure'] += distance</span></pre><p id="df66" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">In order the stats display in the video we’ll have to call the method in the <strong class="mz gg">analyze_video method, </strong>so be sure to add this extra lines of code after the speed label is defined and just before the output video is processed:</p><pre class="ox oy oz pa pb pq pr ps bp pt bb bk"><span id="f38a" class="pu nw gf pr b bg pv pw l px py">*<br/>*<br/>*<br/>*<br/>*<br/>*<br/>*<br/>#Speed label <br/>                  speed_font_scale = 0.8  <br/>                  speed_y_position = int(y1) + 20<br/>                  if speed_y_position &gt; int(y1) - 5:<br/>                      speed_y_position = int(y1) - 5<br/><br/>                  cv2.putText(<br/>                      frame,<br/>                      f"Speed: {speed:.2f} m/s",  <br/>                      (int(x1), speed_y_position),  <br/>                      cv2.FONT_HERSHEY_PLAIN,       <br/>                      speed_font_scale,<br/>                      text_color,<br/>                      thickness=2<br/>                  )<br/>#**************Add these lines of code---&gt;:<br/><br/>                  distance = speed / fps<br/>                  position = (x_center, y_center)<br/>                  self.update_team_stats(team, speed, distance, position)<br/>                    <br/>              # Write output video<br/>              out.write(frame)<br/>              frame_num += 1</span></pre><p id="73d5" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The distance in meters covered by each player is calculated by dividing their speed (measured in meters per second) by the frame rate (frames per second). This calculation allows us to estimate how far each player moves between each frame change in the video. If everything works well, your final video output should look like this:</p><figure class="ox oy oz pa pb fw fo fp paragraph-image"><div class="fo fp pz"><img src="../Images/7a960baa2383b62cff2c4cf942ad002a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*rKPlNLBkmkR07BsYfzrlUQ.gif"/></div><figcaption class="pc pd pe fo fp pf pg bf b bg z dx">Sample Clip 08: Final Output</figcaption></figure><h1 id="2a95" class="nv nw gf bf nx ny nz hf oa ob oc hi od oe of og oh oi oj ok ol om on oo op oq bk">Considerations and Future Work</h1><p id="1869" class="pw-post-body-paragraph mx my gf mz b hd or nb nc hg os ne nf ng ot ni nj nk ou nm nn no ov nq nr ns fj bk">This model is a basic setup of what can be achieved using computer vision to track players in an ice hockey game (or any team sport). However, there’s a lot of fine-tuning that can be done to improve it and add new capabilities. Here are a few ideas that I’m working on for a next 2.0 version that you might also consider:</p><p id="f89f" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz gg"><em class="nu">The challenge of following the puck: </em></strong>Depending on which direction your camera is facing and the resolution, tracking the puck is challenging considering its size compared to a soccer or basketball ball. But if you achieve this, interesting possibilities open up to track performance, such as possession time metrics, goal opportunities, or shots data. This also applies also to individual performances; in ice hockey, players change significantly more often than in other team sports, so tracking each player’s performance during one period presents a challenge.</p><p id="8e41" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz gg"><em class="nu">Compute resources, Oh why compute!</em></strong> I ran all the code on a CPU arrangement but faced issues <em class="nu">(sometimes resulting in blue screens 😥)</em> due to running out of memory during the design process (consider using a CUDA setup). Our sample video is about 40 seconds long and initially 5 MB in size, but after running the model, the output increases to up to 34 MB. Imagine the size for a full 20-minute game period. So, you should consider compute resources and storage when scaling up.</p><p id="af0f" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz gg"><em class="nu">Don’t underestimate MLOps:</em></strong> To deploy and scale rapidly, we need Machine Learning pipelines that are efficient, support frequent execution, and are reliable. This involves considering a <strong class="mz gg">Continuous Integration-Deployment-Training approach</strong>. Our use case has been built for a specific scenario, but what if conditions change, such as the camera direction or jersey colors? To scale up, we must adopt a CI/CD/CT mindset.</p></div></div></div><div class="ab cb qg qh qi qj" role="separator"><span class="qk by bm ql qm qn"/><span class="qk by bm ql qm qn"/><span class="qk by bm ql qm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b327" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">I hope you found this computer vision project interesting, you can access the complete code in <a class="af nt" href="https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch" rel="noopener ugc nofollow" target="_blank">this GitHub repository</a>. And if you want to support the development of inline and ice hockey in the region, follow the <a class="af nt" href="https://www.instagram.com/aphl.pe/?igsh=MThvZWxhNThwdXpibA%3D%3D" rel="noopener ugc nofollow" target="_blank">APHL</a> <em class="nu">(we are always in need of used equipment you’d like to donate for young players and working on building our first official hockey rink)</em>, and worldwide, follow and support the <a class="af nt" href="https://friendshipleague.org/" rel="noopener ugc nofollow" target="_blank">Friendship League</a>.</p><p id="1434" class="pw-post-body-paragraph mx my gf mz b hd na nb nc hg nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz gg"><em class="nu">Did I miss anything?</em></strong> Your suggestions are always welcome. Let’s keep the conversation going!</p></div></div></div></div>    
</body>
</html>