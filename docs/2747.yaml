- en: Economics of Hosting Open Source LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/economics-of-hosting-open-source-llms-17b4ec4e7691?source=collection_archive---------0-----------------------#2024-11-12](https://towardsdatascience.com/economics-of-hosting-open-source-llms-17b4ec4e7691?source=collection_archive---------0-----------------------#2024-11-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Large Language Models in Production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Leveraging various deployment options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ilsilfverskiold?source=post_page---byline--17b4ec4e7691--------------------------------)[![Ida
    Silfverskiöld](../Images/a2c0850bc0198688f70a5eca858cf8b5.png)](https://medium.com/@ilsilfverskiold?source=post_page---byline--17b4ec4e7691--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--17b4ec4e7691--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--17b4ec4e7691--------------------------------)
    [Ida Silfverskiöld](https://medium.com/@ilsilfverskiold?source=post_page---byline--17b4ec4e7691--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--17b4ec4e7691--------------------------------)
    ·19 min read·Nov 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/451f57a8bda4e9b8e4b0f293e3d83b43.png)'
  prefs: []
  type: TYPE_IMG
- en: Total Processing Time on GPU vs CPU — Not to scale* | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '*If you’re not a member but want to read this article, see this friend link*
    [*here.*](https://medium.com/@ilsilfverskiold/17b4ec4e7691?sk=2649166d7d7a839885bb3fff2a3922bd)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’ve been experimenting with open-source models of different sizes, you’re
    probably asking yourself: what’s the most efficient way to deploy them?'
  prefs: []
  type: TYPE_NORMAL
- en: What’s the pricing difference between **on-demand** and **serverless providers**,
    and is it really worth dealing with a player like AWS when there are LLM serving
    platforms?
  prefs: []
  type: TYPE_NORMAL
- en: I’ve decided to dive into this subject, comparing cloud vendors like AWS with
    newer alternatives like Modal, BentoML, Replicate, Hugging Face Endpoints, and
    Beam.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look at metrics such as processing time, cold start delays, and CPU, memory,
    and GPU costs to understand what’s most efficient and economical. We’ll also cover
    softer metrics like ease of deployment, developer experience and community.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6a4df8a4c00c6bf8ec5c79689b6ec63.png)'
  prefs: []
  type: TYPE_IMG
- en: Some of the metrics we’ll look at | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We’ll explore a few use cases, such as deploying a **smaller model on CPU**
    versus running a **7–8 billion parameter model on GPU**.
  prefs: []
  type: TYPE_NORMAL
