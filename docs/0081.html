<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>From Adaline to Multilayer Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>From Adaline to Multilayer Neural Networks</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-adaline-to-multilayer-neural-networks-e115e65fee3e?source=collection_archive---------8-----------------------#2024-01-09">https://towardsdatascience.com/from-adaline-to-multilayer-neural-networks-e115e65fee3e?source=collection_archive---------8-----------------------#2024-01-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e6b8" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Setting the foundations right</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@cretanpan?source=post_page---byline--e115e65fee3e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Pan Cretan" class="l ep by dd de cx" src="../Images/8b3fbab70c0e61f7ca516d2f54b646e5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*W2RPWMZbGjGv0cRC3T34gw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e115e65fee3e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@cretanpan?source=post_page---byline--e115e65fee3e--------------------------------" rel="noopener follow">Pan Cretan</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e115e65fee3e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">23 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/b8f71ed9e1d173f61b5f8ea765b06d34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3HUeZ1bw-KYUx68f"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@fr3dd87?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Konta Ferenc</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4e48" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the previous two articles we saw how we can implement a basic classifier based on Rosenblatt’s <a class="af nb" href="https://medium.com/towards-data-science/classification-with-rosenblatts-perceptron-e7f49e3af562" rel="noopener">perceptron </a>and how this classifier can be improved by using the adaptive linear neuron algorithm (<a class="af nb" href="https://medium.com/towards-data-science/from-the-perceptron-to-adaline-1730e33d41c5" rel="noopener">adaline</a>). These two articles cover the foundations before attempting to implement an artificial neural network with many layers. Moving from adaline to deep learning is a bigger leap and many machine learning practitioners will opt directly for an open source library like <a class="af nb" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">PyTorch</a>. Using such a specialised machine learning library is of course recommended for developing a model in production, but not necessarily for learning the fundamental concepts of multilayer neural networks. This article builds a multilayer neural network from scratch. Instead of solving a binary classification problem we will focus on a multiclass one. We will be using the sigmoid activation function after each layer, including the output one. Essentially we train a model that for each input, comprising a vector of features, produces a vector with length equal to the number of classes to be predicted. Each element of the output vector is in the range [0, 1] and can be understood as the “probability” of each class.</p><p id="af71" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The purpose of the article is to become comfortable with the mathematical notation used for describing mathematically neural networks, understand the role of the various matrices with weights and biases, and derive the formulas for updating the weights and biases to minimise the loss function. The implementation allows for any number of hidden layers with arbitrary dimensions. Most tutorials assume a fixed architecture but this article uses a carefully chosen mathematical notation that supports generalisation. In this way we can also run simple numerical experiments to examine the predictive performance as a function of the number and size of the hidden layers.</p><p id="759c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As in the earlier articles, I used the online <a class="af nb" href="https://latexeditor.lagrida.com/" rel="noopener ugc nofollow" target="_blank">LaTeX equation editor</a> to develop the LaTeX code for the equation and then the chrome plugin <a class="af nb" href="https://chromewebstore.google.com/detail/math-equations-anywhere/fkioioejambaepmmpepneigdadjpfamh" rel="noopener ugc nofollow" target="_blank">Maths Equations Anywhere</a> to render the equation into an image. All LaTex code is provided at the end of the article if you need to render it again. Getting the notation right is part of the journey in machine learning, and essential for understanding neural networks. It is vital to scrutinise the formulas, and pay attention to the various indices and the rules for matrix multiplication. Implementation in code becomes trivial once the model is correctly formulated on paper.</p><p id="0ed6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All code used in the article can be found in the accompanying <a class="af nb" href="https://github.com/karpanGit/myBlogs/tree/master/MultilayerNeuralNetworks" rel="noopener ugc nofollow" target="_blank">repository</a>. The article covers the following topics</p><p id="3adb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">∘ <a class="af nb" href="#14a0" rel="noopener ugc nofollow">What is a multilayer neural network?</a><br/> ∘ <a class="af nb" href="#48ed" rel="noopener ugc nofollow">Activation</a><br/> ∘ <a class="af nb" href="#7fda" rel="noopener ugc nofollow">Loss function</a><br/> ∘ <a class="af nb" href="#d970" rel="noopener ugc nofollow">Backpropagation</a><br/> ∘ <a class="af nb" href="#5f28" rel="noopener ugc nofollow">Implementation</a><br/> ∘ <a class="af nb" href="#2f55" rel="noopener ugc nofollow">Dataset </a><br/> ∘ <a class="af nb" href="#431d" rel="noopener ugc nofollow">Training the model</a><br/> ∘ <a class="af nb" href="#15f6" rel="noopener ugc nofollow">Hyperparameter tuning</a><br/> ∘ <a class="af nb" href="#eb5c" rel="noopener ugc nofollow">Conclusions</a><br/> ∘ <a class="af nb" href="#9aa7" rel="noopener ugc nofollow">LaTeX code of equations used in the article</a></p><h2 id="14a0" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">What is a multilayer neural network?</h2><p id="bbcb" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">This section introduces the architecture of a generalised, feedforward, fully-connected multilayer neural network. There are a lot of terms to go through here as we work our way through Figure 1 below.</p><p id="95a4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For every prediction, the network accepts a vector of features as input</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj oy"><img src="../Images/6dabd645b20fbd402155f26246eebf24.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*AHTD8eDnFPykw2VyiliXHQ.png"/></div></figure><p id="ec8a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">that can also be understood as a matrix with shape (1, n⁰). The network uses L layers and produces a vector as an output</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj oz"><img src="../Images/95809ead8c78a7209cd11a22846181b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*FBOOjOQaIcPnwEfN1r54OQ.png"/></div></figure><p id="827c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">that can be understood as a matrix with shape (1, nᴸ) where nᴸ is the number of classes in the multiclass classification problem we need to solve. Every float in this matrix lies in the range [0, 1] and the index of the largest element corresponds to the predicted class. The (L) notation in the superscript is used to refer to a particular layer, in this case the last one.</p><p id="61ce" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But how do we generate this prediction? Let’s focus on the first element of the first layer (the input is not considered a layer)</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pa"><img src="../Images/1c601c9aca303ea8b14cda3be7db7b51.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*ydfZKxbtEnJyyV-3rlnfAg.png"/></div></figure><p id="0dbe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We first compute the net input that is essentially an inner product of the input vector with a set of weights with the addition of a bias term. The second operation is the application of the activation function σ(z) to which we will return later. For now it is important to keep in mind that the activation function is essentially a scalar operation.</p><p id="c4d8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can compute all elements of the first layer in the same way</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pb"><img src="../Images/0a47528924ac72826b89fc6aaadaec15.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*OwHmBheJUtgTUnEC4vm7Xw.png"/></div></figure><p id="1d0d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">From the above we can deduce that we have introduced n¹ x n⁰ weights and n¹ bias terms that will need to be fitted when the model is trained. These calculations can also be expressed in matrix form</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pc"><img src="../Images/713a0b377225ff41355e489e2cc46e0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*6GNC0aO923s8Gb8p0US2NQ.png"/></div></figure><p id="6c43" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Pay close attention to the shape of the matrices. The net output is a result of a matrix multiplication of two matrices with shape (1, n⁰) and (n⁰, n¹) that results in a matrix with shape (1, n¹), to which we add another matrix with the bias terms that has the same (1, n¹) shape. Note that we introduced the transpose of the weight matrix. The activation function applies to every element of this matrix and hence the activated values of layer 1 are also a matrix with shape (1, n¹).</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pd"><img src="../Images/6f0b228ffacfcb6ffd8fac45fc0181a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vzNcCdPE4acoU69q-iciyA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 1: A general multilayer neural network with an arbitrary number of input features, number of output classes and number of hidden layers with different number of nodes (image by the Author)</figcaption></figure><p id="2feb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The above can be readily generalised for every layer in the neural network. Layer k accepts as input nᵏ⁻¹ values and produces nᵏ activated values</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pe"><img src="../Images/1a2b0d0fc68d1ef5891114ad87975343.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*pJ8MHKnZ58vfowJUDKf3jg.png"/></div></figure><p id="6856" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Layer k introduces nᵏ x nᵏ⁻¹ weights and nᵏ bias terms that will need to be fitted when the model is trained. The total number of weights and bias terms is</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pf"><img src="../Images/62393e822cd89c09f35328a18dcf52b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*j0c_A1a767_xYHviRjEb4A.png"/></div></figure><p id="ff85" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">so if we assume an input vector with 784 elements (dimension of a low resolution image in gray scale), a single hidden layer with 50 nodes and 10 classes in the output we need to optimise 785*50+51*10 = 39,760 parameters. The number of parameters grows further if we increase the number of hidden layers and the number of nodes in these layers. Optimising an objective function with so many parameters is not a trivial undertaking and this is why it took some time from the time adaline was introduced until we discovered how to train deep networks in the mid 80s.</p><p id="6cd1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This section essentially covers what is known as the forward pass, i.e. how we apply a series of matrix multiplications, matrix additions and element wise activations to convert the input vector to an output vector. If you pay close attention we assumed that the input was a single sample represented as a matrix with shape (1, n⁰). The notation holds even if we we feed into the network a batch of samples represented as a matrix with shape (N, n⁰). There is only small complexity when it comes to the bias terms. If we focus on the first layer we sum a matrix with shape (N, n¹) to a bias matrix with shape (1, n¹). For this to work the bias matrix has its first row replicated as many times as the number of samples in the batch we use in the forward pass. This is such a natural operation that <a class="af nb" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank">NumPy </a>does it automatically in what is called <a class="af nb" rel="noopener" target="_blank" href="/numpy-broadcasting-4c4cb9dff1e7">broadcasting</a>. When we apply forward pass to a batch of inputs it is perhaps cleaner to use capital letters for all vectors that become matrices, i.e.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pg"><img src="../Images/a4fdb81338c9c192ddf1dcb2f312c2b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*9rqSj1FiXQCEzIZACSkhxQ.png"/></div></figure><p id="7788" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that I assumed that broadcasting was applied to the bias terms leading to a matrix with as many rows as the number of samples in the batch.</p><p id="5c11" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Operating with batches is typical with deep neural networks. We can see that as the number of samples N increases we will need more memory to store the various matrices and carry out the matrix multiplications. In addition, using only part of training set for updating the weights means we will be updating the parameters several times in each pass of the training set (epoch) leading to faster convergence. There is an additional benefit that is perhaps less obvious. The network uses activation functions that, unlike the activation in adaline, are not the identity. In fact they are not even linear, which makes the loss function non convex. Using batches introduces noise that is believed to help escaping shallow local minima. A suitably chosen learning rate further assists with this.</p><p id="b736" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As a final note before we move on, the term feedforward comes from the fact that each layer is using as input the output of the previous layer without using loops that lead to the so-called recurrent neural networks.</p><h2 id="48ed" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Activation</h2><p id="2980" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">Enabling the neural network to solve complex problem requires introducing some form of nonlinearity. This is achieved by using an activation function in each layer. There are many choices. For this article we will be using the sigmoid (logistic) activation function that we can visualise with</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="2805" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">that produces</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pk"><img src="../Images/62c64572970b8e726c169a4978ebb6e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uAWIRLVIsSmY5TOoOdlsug.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 2: Sigmoid (logistic) activation function. Image by the Author.</figcaption></figure><p id="658f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The code also includes all imports we will need throughout the article.</p><p id="f8c8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The activation function maps any float to the range 0 to 1. In reality the sigmoid is a more suitable activation of the final layer for binary classification problems. For multiclass problems it would have been more appropriate to use <a class="af nb" rel="noopener" target="_blank" href="/sigmoid-and-softmax-functions-in-5-minutes-f516c80ea1f9">softmax </a>to normalize the output of the neural network to a probability distribution over predicted output classes. One way to think about this is that softmax enforces that post activation the sum of the entries of the output vector must add up to 1, that is not the case with sigmoid. Another way to think about it is that the sigmoid essentially converts the logits (log odds) to a one-versus-all (OvA) probability. Still, we will use the sigmoid activation function to stay as close as possible to adaline because the softmax is not an element wise operation and this will introduce some complexities in the back propagation algorithm. I leave this as an exercise for the reader.</p><h2 id="7fda" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Loss function</h2><p id="e0ef" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">The loss function used for adaline was the mean square error. In practice a multiclass classification problem would use a multiclass cross-entropy loss. In order to remain as close to adaline as possible, and to facilitate the analytical calculation of the gradients of the loss function with respect to the parameters, we will stick on the mean square error loss function. Every sample in the training set, belongs to one of the nᴸ classes and hence the loss function can be expressed as</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pl"><img src="../Images/43cd4bb53dcd48571173bf46019f5b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A32qPX0zQXIlqeZvuVnPZg.png"/></div></div></figure><p id="9278" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">where the first summation is over all samples and the second over classes. The above implies that the known class for sample i has been converted to a one-hot-encoding, i.e. a matrix with shape (1, nᴸ) containing zeros apart from the element that corresponds to the sample class that is one. We adopted one more notation convention so that [j] in the superscript is used to refer to sample j. The summation above does not need to use all samples in the training set. In practice it will be applied in batches of N’ samples with N’&lt;&lt;N.</p><h2 id="d970" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Backpropagation</h2><p id="e6bf" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">The loss function is a scalar that depends on tens or hundreds of thousands of parameters, comprising weights and bias terms. Typically, these parameters are initialised with random numbers and are updated iteratively so that the loss function is minimised using the gradient of the loss function with regard to each parameter. In the case of adaline, the analytical derivation of the gradients was straightforward. For multilayer neural networks the derivation is more involved but remains tractable if we adopt a clever strategy. We enter the world of the back propagation but fear not. Backpropagation essentially boils down to a successive application of the chain differentiation rule from the right to the left.</p><p id="2557" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s come back to the loss function. It depends on the activated values of the last layer, so we can first compute the derivatives with regard to those</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pm"><img src="../Images/7e8f729f6d41f269acfe1bd97fc7bdb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*Jla1UPMO0o0tBRvZcld-WQ.png"/></div></figure><p id="d4ef" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The above can be understood as the (j, i) element of a derivate matrix with shape (N, nᴸ) and can be written in matrix form as</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pn"><img src="../Images/17b39acd1aa2c9eda84bee95e10d7d5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*6Cc5ZWRVYXgvzs-k4zhAmQ.png"/></div></figure><p id="31f1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">where both matrices in the right hand side have shape (N, nᴸ). The activated values of the last layer are computed by applying the sigmoid activation function on each element of the net input matrix of the last layer. Hence, to compute the derivatives of the loss function with regard to each element of this net input matrix of the last layer we simply need to remind ourselves on how to compute the derivative of a nested function with the outer one being the sigmoid function:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj po"><img src="../Images/3184ea54722f9a63720f945d149d8099.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*_dFTtMNRwW0uMUrBEvhvLg.png"/></div></figure><p id="f562" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The star multiplication denotes element wise multiplication. The result of this formula is a matrix with shape (N, nᴸ). If you have difficulties computing the derivative of the sigmoid function please check <a class="af nb" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="dd9d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We are now ready to compute the derivative of the loss function with regard to the weights of the L-1 layer; this is the first set of weights we encounter when we move from right to left</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pp"><img src="../Images/89b3c727cfb18427025627e303c5923c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sz0cUB9v3izGThwSm7VfHA.png"/></div></div></figure><p id="896a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This leads to a matrix with the same shape as the weights of the L-1 layer. We next need to compute the derivative of the net input of the L layer with regard to the weights of the L-1 layer. If we pick one element of the net input matrix of the last layer and one of these weights we have</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pq"><img src="../Images/02b9f932d9ab82710feaf6ac8f0fc82b.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*J7rWlephHFwdj2G5f5J0Hw.png"/></div></figure><p id="82e7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you have trouble to understand the above, think that for every sample j, the i element of the net input of the L layer only depends on the weights of the L-1 layer for which the first index is also i. Hence, we can eliminate one of the summations in the derivative</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pr"><img src="../Images/0e7ebc31a8dbc7e81382424307d664b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*DAS4trt_KhsF8idagiaMRQ.png"/></div></figure><p id="1c66" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can express all these derivatives in a matrix notation using</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj ps"><img src="../Images/5e1edec3574bea4b375e321b1c82c402.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*OHVto9hbVwt-aJ-pNRjG8A.png"/></div></figure><p id="221c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Essentially the implicit summation in the matrix multiplication absorbs the summation over the samples. Follow along with the shapes of the multiplied matrices and you will see that the resulting derivative matrix has the same shape as the weight matrix used to calculate the net input of the L layer. Although the number of elements in the resulting matrix is limited to the product of the number of nodes of the last two layers (the shape is (nᴸ, nᴸ⁻¹)), the multiplied matrices are much larger and hence are typically more memory consuming. Hence, the need to use batches when training the model.</p><p id="fb57" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The derivatives of the loss function with respect to the bias terms used for calculating the net input of the last layer can be computed similarly as for the weights to give</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pt"><img src="../Images/3934777a724a4a9be0a877917bbf1be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*-67YNIO3WjMkgNMcbLm8wA.png"/></div></figure><p id="a356" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">that leads to a matrix with shape (1, nᴸ).</p><p id="d6e3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We have just computed all derivatives of the loss function with regard to the weights and bias terms used for computing the net input of the last layer. We now turn our attention to the gradients with the regard to the weights and bias terms of the previous layer (these parameters will have the superscript index L-2). Hopefully we can start identifying patterns so that we can apply them to compute the derivates with regard to the weights and bias terms for k=0,..,L-2. We could see these patterns emerge if we compute the derivative of the loss function with regard to the activated values of the L-1 layer. These should form a matrix with shape (N, nᴸ⁻¹) that is computed as</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pu"><img src="../Images/fa8be214ae7ba029753e82575174d717.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*DCJJYTl1oOfW_Xct5fr2LQ.png"/></div></figure><p id="9f1f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once we have the derivatives of the loss with regard to the activated values of layer L-1 we can proceed with calculating the derivatives of the loss function with regard to the net input of the layer L-1 and then with regard to the weights and bias terms with index L-2.</p><p id="fbaa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s recap how we backpropagate by one layer. We assume we have computed the derivative of the loss function with regard to the weights and bias terms with index k and we need to compute the derivates of the loss function with regard to the weights and bias terms with index k-1. We need to carry out 4 operations:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pv"><img src="../Images/e532d96ea2523945e85fa02b34245556.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*KPEIEkmfisYMeGeuDW9Vrg.png"/></div></figure><p id="605f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All operations are vectorised. We can already start imaging how we could implement these operations in a class. My understanding is that when one uses a specialised library to add a fully connected linear layer with an activation function, this is what happens behind the scenes! It is nice not to worry about the mathematical notation, but my suggestion would be to go through these derivations at least once.</p><h2 id="5f28" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Implementation</h2><p id="8bdc" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">In this section we provide the implementation of a generalised, feedforward, multilayer neural network. The API draws some analogies to the one found in specialised deep learning libraries such as PyTorch</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="0116" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The code contains two utility functions: <code class="cx pw px py pz b">sigmoid()</code> applies the sigmoid (logistic) activation function to a float (or NumPy array), and <code class="cx pw px py pz b">int_to_onehot()</code> takes a list of integers with the class of each sample and returns their one-hot-encoding representation.</p><p id="80cb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The class <code class="cx pw px py pz b">MultilayerNeuralNetClassifier</code> contains the neural net implementation. The initialisation constructor assigns random numbers to the weights and bias terms of each layer. As an example if we construct a neural network with <code class="cx pw px py pz b">layers=[784, 50, 10]</code>, we will be using 784 input features, a hidden layer with 50 nodes and 10 classes as output. This generalised implementation allows changing both the number of hidden layers and the number of nodes in the hidden layers. We will exploit this when we do hyperparameter tuning later on. For reproducibility we use a seed for the random number generator to initialise the weights.</p><p id="f9d3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <code class="cx pw px py pz b">forward</code> method returns the activated values for each layer as a list of matrices. The method works with a single sample or an array of samples. The last of the returned matrices contains the model predictions for the class membership of each sample. Once the model is trained only this matrix is used for making predictions. However, whilst the model is being trained we need the activated values for all layers as we will see below and this is why the <code class="cx pw px py pz b">forward</code> method returns all of them. Assuming that the network was initialised with <code class="cx pw px py pz b">layers=[784, 50, 10]</code>, the <code class="cx pw px py pz b">forward</code> method will return a list of two matrices, the first one with shape (N, 50) and the second one with shape (N, 10), assuming the input <code class="cx pw px py pz b">x</code> has N samples, i.e. it is a matrix with shape (N, 784).</p><p id="c92c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <code class="cx pw px py pz b">backward</code> method implements backpropagation, i.e. all the analytically computed derivatives of the loss function as described in the previous section. The last layer is special because we need to compute the derivatives of the loss function with regard to the model output using the known classes. The first layer is special because we need to use the input instead of the activated values of the previous layer. The middle layers are all the same. We simply iterate over the layers backwards. The code reflects fully the analytically derived formulas. By using NumPy we vectorise all operations that speeds up execution. The method returns a tuple of two lists. The first list contains the matrices with the derivatives of the loss function with regard to the weights of each layer. Assuming that the network was initialised with <code class="cx pw px py pz b">layers=[784, 50, 10]</code>, the list will contain two matrices with shapes (784, 50) and (50, 10). The second list contains the vectors with the derivatives of the loss function with regard to the bias terms of each layer. Assuming that the network was initialised with <code class="cx pw px py pz b">layers=[784, 50, 10]</code>, the list will contain two vectors with shapes (50, ) and (10,).</p><p id="1ccd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Reflecting back on my learnings from this article, I felt that the implementation was straightforward. The hardest part was to come up with a robust mathematical notation and work out the gradients on paper. Still, it is easy to make mistakes that may not be easy to detect even if the optimisation seems to converge. This brings me to the special <code class="cx pw px py pz b">backward_numerical</code> method. This method is used for neither training the model nor making predictions. It uses finite (central) differences to estimate the derivatives of the loss function with regard to the weights and bias terms of the chosen layer. The numerical derivatives can be compared with the analytically computed ones returned by the <code class="cx pw px py pz b">backward</code> function to ensure that the implementation is correct. This method would be too slow to be used for training the model as it requires two forward passes for each derivative and in our trivial example with <code class="cx pw px py pz b">layers=[784, 50, 10]</code> there are 39,760 such derivatives! But it is a lifesaver. Personally I would not have managed to debug the code without it. If you want to keep a key message from this article, it would be the usefulness of numerical differentiation for double checking your analytically derived gradients. We can check the correctness of the gradients with an untrained model</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="275e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">that produces</p><pre class="ml mm mn mo mp qa pz qb bp qc bb bk"><span id="6b66" class="qd nz fq pz b bg qe qf l qg qh">layer 3: 300 out of 300 weight gradients are numerically equal<br/>layer 3:10 out of 10 bias term gradients are numerically equal<br/>layer 2: 1200 out of 1200 weight gradients are numerically equal<br/>layer 2:30 out of 30 bias term gradients are numerically equal<br/>layer 1: 2000 out of 2000 weight gradients are numerically equal<br/>layer 1:40 out of 40 bias term gradients are numerically equal</span></pre><p id="8178" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Gradients look in order!</p><h2 id="2f55" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Dataset</h2><p id="0385" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">We will need a dataset for building our first model. A famous one often used in pattern recognition experiments is the <a class="af nb" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank">MNIST handwritten digits</a>. We can find more details about this dataset in the OpenML dataset <a class="af nb" href="https://openml.org/search?type=data&amp;status=active&amp;id=554&amp;sort=runs" rel="noopener ugc nofollow" target="_blank">repository</a>. All datasets in OpenML are <a class="af nb" href="https://openml.org/terms" rel="noopener ugc nofollow" target="_blank">subject </a>to the <a class="af nb" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank">CC BY 4.0 license</a> that permits copying, redistributing and transforming the material in any medium and for any purpose.</p><p id="3f80" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The dataset contains 70,000 digit images and the corresponding labels. Conveniently, the digits have been size-normalized and centered in a fixed-size 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field. The dataset can be conveniently retrieved using <a class="af nb" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank">scikit-learn</a></p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="d630" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">that prints</p><pre class="ml mm mn mo mp qa pz qb bp qc bb bk"><span id="71d0" class="qd nz fq pz b bg qe qf l qg qh">original X: X.shape=(70000, 784), X.dtype=dtype('int64'), X.min()=0, X.max()=255<br/>original y: y.shape=(70000,), y.dtype=dtype('O')<br/>processed X: X.shape=(70000, 784), X.dtype=dtype('float64'), X.min()=-1.0, X.max()=1.0<br/>processed y: y.shape=(70000,), y.dtype=dtype('int32')<br/>class counts: 0:6903, 1:7877, 2:6990, 3:7141, 4:6824, 5:6313, 6:6876, 7:7293, 8:6825, 9:6958</span></pre><p id="66d1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can see that each image is available as a vector with 784 integers between 0 and 255 that were converted to floats in the range [-0.5, 0.5]. This is perhaps a bit different than the typical feature scaling in scikit-learn where scaling happens per feature rather per sample. The class labels were retrieved as strings and converted to integers. The dataset is reasonably balanced.</p><p id="02a7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We next visualise ten images for each digit to obtain a feeling on the variations in hand writing</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="3ed5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">that produces</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qi"><img src="../Images/71d8169c956539a18b636b779ab90a9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6RDwaArx036sLMKa5RMEag.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Randomly selected samples for each digit. Image by the Author.</figcaption></figure><p id="e93e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can foresee that some digits may be confused by the model, e.g. the last 9 resembles 8. There may also be hand writing variations that are not predicted well, such as 7 digits written with a horizontal line in the middle, depending on how often such variations are represented in the training set. We now have a neural network implementation and a dataset to use it with. In the next section we will provide the necessary code for training the model before we look into hyperparameter tuning.</p><h2 id="431d" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Training the model</h2><p id="f433" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">The first action we need to take is to split the dataset into a training set, and an external (hold-out) test set. We can readily do so using scikit-learn</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="973f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We use stratification so that the percentage of each class is roughly equal in both the training set and the external (hold-out) dataset. The external (hold-out) test set contains 10,000 samples and will not be used for anything other than assessing the model performance. In this section we will use the 60,000 samples for training set without any hyperparameter tuning.</p><p id="b1a1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When deriving the gradients of the loss function with regard to the model parameters we show that it is necessary to carry out several matrix multiplications and some of these matrices have as many rows as the number of samples. Given that typically the number of samples is quite large we will need a significant amount of memory. To alleviate this we will be using mini batches in the same way we <a class="af nb" rel="noopener" target="_blank" href="/from-the-perceptron-to-adaline-1730e33d41c5">used</a> mini batches during the gradient descent optimisation of the adaline model. Typically, each batch can contain 100–500 samples. Reducing the batch size increases the convergence speed because we make more parameter updates within the the same pass of the training set (epoch), but we also increase the noise. We need to strike a balance. First we provide a generator that accepts the training set and the batch size and returns the batches</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="5fea" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The generator returns batches of equal size that by default contain 100 samples. The total number of samples may not be a multiple of the batch size and hence some samples will not be returned in a given pass through the training set. Th number of skipped samples is smaller than the batch size and the set of samples left out changes every time the generator is used, assuming we do not reset the random number generator. Hence, this is not critical. As we will be passing though the training sets multiple times in the different epochs we will eventually use the training set fully. The reason for using batches of a constant size is that we will be updating the model parameters after each batch and a small batch can increase the noise and prevent convergence, especially if the samples in the batch happen to be outliers.</p><p id="6967" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When the model is initiated we expect a low accuracy that we can confirm with</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="371e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">that gives an accuracy of approximately 9.5%. This is more or less expected for a reasonably balanced dataset as there are 10 classes. We now have the means to monitor the loss and the accuracy of each batch passed to the forward pass that we will exploit during training. Let’s write the final piece of code to iterate over the epochs and mini batches, update the model parameters and monitor how the loss and accuracy evolves in both the training set and external (hold-out) test set.</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="442d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using this function training becomes a single line of code</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="d0fd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">that produces</p><pre class="ml mm mn mo mp qa pz qb bp qc bb bk"><span id="5cde" class="qd nz fq pz b bg qe qf l qg qh">epoch 0: loss_training=0.096 | accuracy_training=0.236 | loss_test=0.088 | accuracy_test=0.285<br/>epoch 1: loss_training=0.086 | accuracy_training=0.333 | loss_test=0.085 | accuracy_test=0.367<br/>epoch 2: loss_training=0.083 | accuracy_training=0.430 | loss_test=0.081 | accuracy_test=0.479<br/>epoch 3: loss_training=0.078 | accuracy_training=0.532 | loss_test=0.075 | accuracy_test=0.568<br/>epoch 4: loss_training=0.072 | accuracy_training=0.609 | loss_test=0.069 | accuracy_test=0.629<br/>epoch 5: loss_training=0.066 | accuracy_training=0.657 | loss_test=0.063 | accuracy_test=0.673<br/>epoch 6: loss_training=0.060 | accuracy_training=0.691 | loss_test=0.057 | accuracy_test=0.701<br/>epoch 7: loss_training=0.055 | accuracy_training=0.717 | loss_test=0.052 | accuracy_test=0.725<br/>epoch 8: loss_training=0.050 | accuracy_training=0.739 | loss_test=0.049 | accuracy_test=0.742<br/>epoch 9: loss_training=0.047 | accuracy_training=0.759 | loss_test=0.045 | accuracy_test=0.765</span></pre><p id="4f1c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can see that after the ten epochs the accuracy for the training set has reached approximately 76%, whilst the accuracy of the external (hold-out) test set is slightly higher, indicating that the model has not been overfitted.</p><p id="0e4c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The loss of the training set keeps decreasing and hence convergence has not been reached yet. The model allows hot starting so we could run another ten epochs by repeating the single line of code above. Instead, we will initiate the model again and run it for 100 epochs, increasing the batch size to 200 at the same time. We provide the complete code for doing so.</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="01ff" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We first plot the training loss and its rate of change as a function of the epoch number</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="6e21" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">that produces</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pk"><img src="../Images/e3165c82de6bdca3d12babfab2e461d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lod2HtgOgLFUjzA7dFvIag.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Training loss and its rate of change as a function of the epoch number. Image by the Author.</figcaption></figure><p id="3180" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can see the model has converged reasonably well as the rate of the change of the training loss has become more than two orders of magnitude smaller compared to its value at the beginning of the training. I am not sure why we observe a reduction in convergence speed at around epoch 10; I can only speculate that the optimiser escaped a local minimum.</p><p id="5685" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can also plot the accuracy of the training set and the test set as a function of the epoch number</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="2158" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">that produces</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pk"><img src="../Images/83e7976dde52eb1f7735294d5840e92b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SGETCGIwAPpm7IBIGVTtsg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Training set and external (hold-out) test set accuracy as a function of the epoch number. Image by the Author.</figcaption></figure><p id="bebf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The accuracy reaches approximately 90% after about 50 epochs for both the training set and external (hold-out) test set, suggesting that there is no/little overfitting. We just trained our first, custom built multilayer neural network with one hidden layer!</p><h2 id="15f6" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Hyperparameter tuning</h2><p id="b321" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">In this previous section we chose an arbitrary network architecture and fitted the model parameters. In this section we proceed with a basic hyperparameter tuning by varying the number of hidden layers (ranging from 1 to 3), the number of nodes in the hidden layers (ranging from 10 to 50 in increments of 10) and the learning rate (using the values 0.1, 0.2 and 0.3). We kept the batch size constant at 200 samples per batch. Overall, we tried 45 parameter combinations. We will employ 6-fold cross validation (not nested) which means 6 model trainings per parameter combination, which translates to 270 model trainings in total. In each fold we will be using 50,000 samples for training and 10,000 samples for measuring the accuracy (called validation in the code). To enhance the chances to achieve convergence we will perform 250 epochs for each model fitting. The total execution time was ~12 hours on a single processor (Intel Xeon Gold 3.5GHz). This is more or less what we can reasonably run on a CPU. The training speed could be increased using multiprocessing. In fact, the training would be way faster using a specialised deep learning library like PyTorch on GPUs, such as the freely available T4 GPUs on <a class="af nb" href="https://colab.research.google.com" rel="noopener ugc nofollow" target="_blank">Google Cola</a>b.</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="c01e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This code iterates over all hyperparameter values and folds and stores the loss and accuracy for both the training (50,000 samples) and validation (10,000 samples) in a <a class="af nb" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank">pandas </a>dataframe. The dataframe is used to find the optimal hyperparameters</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="67ca" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">that produces</p><pre class="ml mm mn mo mp qa pz qb bp qc bb bk"><span id="ad46" class="qd nz fq pz b bg qe qf l qg qh">optimal parameters: n_hidden_layers=1, n_hidden_nodes=50, learning rate=0.3<br/>best mean cross validation accuracy: 0.944<br/>|   n_hidden_layers |       10 |       20 |       30 |       40 |      50 |<br/>|------------------:|---------:|---------:|---------:|---------:|--------:|<br/>|                 1 | 0.905217 | 0.927083 | 0.936883 | 0.939067 | 0.9441  |<br/>|                 2 | 0.8476   | 0.925567 | 0.933817 | 0.93725  | 0.9415  |<br/>|                 3 | 0.112533 | 0.305133 | 0.779133 | 0.912867 | 0.92285 |</span></pre><p id="7308" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can see that there is little benefit in increasing the number of layers. Perhaps we could have gained slightly better performance using a larger first hidden layer as the hyperparameter tuning hit the bound of 50 nodes. Some mean cross-validation accuracies are quite low that could be indicative of poor convergence (e.g. when using 3 hidden layers with 10 nodes each). We did not investigate further but this would be typically required before concluding on the optimal network geometry. I would expect that allowing for more epochs would increase accuracy further particular with the larger networks.</p><p id="1679" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A final step is to retrain the model with all samples other than the external (hold-out) set that are only used for the final evaluation</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="ea32" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The last 5 epochs are</p><pre class="ml mm mn mo mp qa pz qb bp qc bb bk"><span id="881d" class="qd nz fq pz b bg qe qf l qg qh">epoch 245: loss_training=0.008 | accuracy_training=0.958 | loss_test=0.009 | accuracy_test=0.946<br/>epoch 246: loss_training=0.008 | accuracy_training=0.958 | loss_test=0.009 | accuracy_test=0.947<br/>epoch 247: loss_training=0.008 | accuracy_training=0.958 | loss_test=0.009 | accuracy_test=0.947<br/>epoch 248: loss_training=0.008 | accuracy_training=0.958 | loss_test=0.009 | accuracy_test=0.946<br/>epoch 249: loss_training=0.008 | accuracy_training=0.958 | loss_test=0.009 | accuracy_test=0.946</span></pre><p id="f90c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We achieved ~95% accuracy with the external (hold-out) test set. This is magical if we consider that we started with a blank piece of paper!</p><h2 id="eb5c" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Conclusions</h2><p id="d475" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">This article demonstrated how we can build a multilayer, feedforward, fully connected neural network from scratch. The network was used for solving a multiclass classification problem. The implementation has been generalised to allow for any number of hidden layers with any number of nodes. This facilitates hyperparameter tuning by varying the number of layers and units in them. However, we need to keep in mind that the loss gradients become smaller and smaller as the depth of the neural network increases. This is known as the vanishing gradient problem and requires using specialised training algorithms once the depth exceeds a certain threshold, which is out of the scope of this article.</p><p id="b47a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Our vanilla implementation of a multilayer neural network has hopefully educational value. Using it in practice would require several improvements though. First of all, overfitting would need to be addressed, by employing some form of drop out. Other improvements, such as the addition of skip-connections and the variation of the learning rate during training, may be beneficial too. In addition, the network architecture itself can be optimised, e.g. by using a convolutional neural network that would be more appropriate for classifying images. Such improvements are best attempted using a specialised library like <a class="af nb" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">PyTorch</a>. When developing algorithms from scratch one needs to be wary of the time it takes and where to draw the line so that the endeavour remains educational without being extremely time consuming. I hope this article strikes a good balance in this sense. If you are intrigued I would recommend this <a class="af nb" href="https://sebastianraschka.com/books/#machine-learning-with-pytorch-and-scikit-learn" rel="noopener ugc nofollow" target="_blank">book</a> for further study.</p><h2 id="9aa7" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">LaTeX code of equations used in the article</h2><p id="0a61" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">The equations used in the article can be found in the gist below, in case you would like to render them again.</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure></div></div></div></div>    
</body>
</html>