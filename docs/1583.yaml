- en: A New Method to Detect “Confabulations” Hallucinated by Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一种新的方法来检测大型语言模型的“虚构”幻觉
- en: 原文：[https://towardsdatascience.com/a-new-method-to-detect-confabulations-hallucinated-by-large-language-models-19444475fc7e?source=collection_archive---------13-----------------------#2024-06-25](https://towardsdatascience.com/a-new-method-to-detect-confabulations-hallucinated-by-large-language-models-19444475fc7e?source=collection_archive---------13-----------------------#2024-06-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/a-new-method-to-detect-confabulations-hallucinated-by-large-language-models-19444475fc7e?source=collection_archive---------13-----------------------#2024-06-25](https://towardsdatascience.com/a-new-method-to-detect-confabulations-hallucinated-by-large-language-models-19444475fc7e?source=collection_archive---------13-----------------------#2024-06-25)'
- en: By calculating semantic entropy with a second LLM, we can better flag answers
    as unreliable due to lack of knowledge
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过使用第二个LLM计算语义熵，我们可以更好地标记那些由于缺乏知识而被认为不可靠的答案
- en: '[](https://lucianosphere.medium.com/?source=post_page---byline--19444475fc7e--------------------------------)[![LucianoSphere
    (Luciano Abriata, PhD)](../Images/a8ae3085d094749bbdd1169cca672b86.png)](https://lucianosphere.medium.com/?source=post_page---byline--19444475fc7e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--19444475fc7e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--19444475fc7e--------------------------------)
    [LucianoSphere (Luciano Abriata, PhD)](https://lucianosphere.medium.com/?source=post_page---byline--19444475fc7e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://lucianosphere.medium.com/?source=post_page---byline--19444475fc7e--------------------------------)[![LucianoSphere
    (Luciano Abriata, PhD)](../Images/a8ae3085d094749bbdd1169cca672b86.png)](https://lucianosphere.medium.com/?source=post_page---byline--19444475fc7e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--19444475fc7e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--19444475fc7e--------------------------------)
    [LucianoSphere (Luciano Abriata, PhD)](https://lucianosphere.medium.com/?source=post_page---byline--19444475fc7e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--19444475fc7e--------------------------------)
    ·10 min read·Jun 25, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--19444475fc7e--------------------------------)
    ·阅读时间10分钟·2024年6月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0a3aed953512c31815f25a6ed9c30281.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a3aed953512c31815f25a6ed9c30281.png)'
- en: Photo by [Jametlene Reskp](https://unsplash.com/@reskp?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Jametlene Reskp](https://unsplash.com/@reskp?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: As you surely know, AI has made huge strides in the last two years with the
    development and mass-scale deployment of large language models (LLMs). These models
    appear to have an impressive capability for reasoning and for question-answering;
    however, a persistent challenge lies in their tendency to “hallucinate”, that
    is to generate outputs with false or arbitrary content. These hallucinations can
    have severe consequences, therefore much of the current research in LLM development
    seeks to suppress them as much as possible. Towards this end, a new paper presents
    a method called “semantic entropy” that identifies and mitigates specific kinds
    of hallucinations arising in LLMs due to simply lack of sufficient knowledge,
    so-called “confabulations”. No need to say, this is all very useful for a more
    reliable use of LLMs in several applications, to not say in all applications that
    require factual knowledge. Interestingly, quantification of semantic entropy on
    an LLM’s generations requires the application of a second LLM to assess the similarity
    of the LLM-generated sequences. Read on to know more details and see some examples.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所知道的，人工智能在过去两年中取得了巨大进展，特别是在大型语言模型（LLMs）的开发和大规模部署方面。这些模型似乎具有令人印象深刻的推理和问答能力；然而，一个持久的挑战是它们倾向于“幻觉”，即生成带有虚假或随意内容的输出。这些幻觉可能带来严重后果，因此当前大量LLM开发的研究都在致力于尽可能抑制它们。为此，一篇新论文提出了一种名为“语义熵”的方法，能够识别和减轻由知识缺乏引起的特定类型幻觉，即所谓的“虚构”。不用多说，这对在多个应用场景中更可靠地使用LLM非常有用，尤其是在那些需要事实知识的应用中。值得注意的是，量化语义熵需要应用第二个LLM来评估LLM生成的序列之间的相似性。继续阅读以了解更多细节和一些示例。
