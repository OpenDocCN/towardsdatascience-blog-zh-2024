- en: A New Method to Detect “Confabulations” Hallucinated by Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-new-method-to-detect-confabulations-hallucinated-by-large-language-models-19444475fc7e?source=collection_archive---------13-----------------------#2024-06-25](https://towardsdatascience.com/a-new-method-to-detect-confabulations-hallucinated-by-large-language-models-19444475fc7e?source=collection_archive---------13-----------------------#2024-06-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By calculating semantic entropy with a second LLM, we can better flag answers
    as unreliable due to lack of knowledge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://lucianosphere.medium.com/?source=post_page---byline--19444475fc7e--------------------------------)[![LucianoSphere
    (Luciano Abriata, PhD)](../Images/a8ae3085d094749bbdd1169cca672b86.png)](https://lucianosphere.medium.com/?source=post_page---byline--19444475fc7e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--19444475fc7e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--19444475fc7e--------------------------------)
    [LucianoSphere (Luciano Abriata, PhD)](https://lucianosphere.medium.com/?source=post_page---byline--19444475fc7e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--19444475fc7e--------------------------------)
    ·10 min read·Jun 25, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a3aed953512c31815f25a6ed9c30281.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jametlene Reskp](https://unsplash.com/@reskp?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: As you surely know, AI has made huge strides in the last two years with the
    development and mass-scale deployment of large language models (LLMs). These models
    appear to have an impressive capability for reasoning and for question-answering;
    however, a persistent challenge lies in their tendency to “hallucinate”, that
    is to generate outputs with false or arbitrary content. These hallucinations can
    have severe consequences, therefore much of the current research in LLM development
    seeks to suppress them as much as possible. Towards this end, a new paper presents
    a method called “semantic entropy” that identifies and mitigates specific kinds
    of hallucinations arising in LLMs due to simply lack of sufficient knowledge,
    so-called “confabulations”. No need to say, this is all very useful for a more
    reliable use of LLMs in several applications, to not say in all applications that
    require factual knowledge. Interestingly, quantification of semantic entropy on
    an LLM’s generations requires the application of a second LLM to assess the similarity
    of the LLM-generated sequences. Read on to know more details and see some examples.
  prefs: []
  type: TYPE_NORMAL
