<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>AI Agents: The Intersection of Tool Calling and Reasoning in Generative AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>AI Agents: The Intersection of Tool Calling and Reasoning in Generative AI</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ai-agents-the-intersection-of-tool-calling-and-reasoning-in-generative-ai-ff268eece443?source=collection_archive---------2-----------------------#2024-10-05">https://towardsdatascience.com/ai-agents-the-intersection-of-tool-calling-and-reasoning-in-generative-ai-ff268eece443?source=collection_archive---------2-----------------------#2024-10-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="bcc1" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Unpacking problem solving and tool-driven decision making in AI</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@tula.masterman?source=post_page---byline--ff268eece443--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Tula Masterman" class="l ep by dd de cx" src="../Images/c36b3740befd5dfdb8719dc6596f1a99.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*Fn6lzAzI489IDlnO-QI8_A.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ff268eece443--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@tula.masterman?source=post_page---byline--ff268eece443--------------------------------" rel="noopener follow">Tula Masterman</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ff268eece443--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">13</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/3279c4d07b6b2c6d9e752246854f9da1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WqOmdp6V4nG7TU2vLdO6JA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author and GPT-4o depicting an AI agent at the intersection of reasoning and tool calling</figcaption></figure><h1 id="2f90" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Introduction: The Rise of Agentic AI</h1><p id="bb54" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Today, new libraries and low-code platforms are making it easier than ever to build AI agents, also referred to as digital workers. Tool calling is one of the primary abilities driving the “agentic” nature of Generative AI models by extending their ability beyond conversational tasks. By executing tools (functions), agents can take action on your behalf and solve complex, multi-step problems that require robust decision making and interacting with a variety of external data sources.</p><p id="51c3" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This article focuses on how reasoning is expressed through tool calling, explores some of the challenges of tool use, covers common ways to evaluate tool-calling ability, and provides examples of how different models and agents interact with tools.</p><h1 id="1962" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Expressions of Reasoning to Solve Problems</h1><p id="8347" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">At the core of successful agents lie two key expressions of reasoning: <strong class="oa fr">reasoning through evaluation and planning</strong> and <strong class="oa fr">reasoning through tool use</strong>.</p><ul class=""><li id="b16f" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk"><strong class="oa fr">Reasoning through evaluation and planning</strong> relates to an agent’s ability to effectively breakdown a problem by iteratively planning, assessing progress, and adjusting its approach until the task is completed. Techniques like <a class="af pc" href="https://arxiv.org/abs/2201.11903" rel="noopener ugc nofollow" target="_blank">Chain-of-Thought</a> (CoT), <a class="af pc" href="https://arxiv.org/abs/2210.03629" rel="noopener ugc nofollow" target="_blank">ReAct</a>, and <a class="af pc" href="https://arxiv.org/abs/2210.02406" rel="noopener ugc nofollow" target="_blank">Prompt Decomposition</a> are all patterns designed to improve the model’s ability to reason strategically by breaking down tasks to solve them correctly. This type of reasoning is more macro-level, ensuring the task is completed correctly by working iteratively and taking into account the results from each stage.</li><li id="073c" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot oz pa pb bk"><strong class="oa fr">Reasoning through tool use</strong> relates to the agents ability to effectively interact with it’s environment, deciding which tools to call and how to structure each call. These tools enable the agent to retrieve data, execute code, call APIs, and more. The strength of this type of reasoning lies in the proper execution of tool calls rather than reflecting on the results from the call.</li></ul><p id="a39b" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">While both expressions of reasoning are important, they don’t always need to be combined to create powerful solutions. For example, <strong class="oa fr">OpenAI’s new</strong> <strong class="oa fr">o1 model excels at reasoning through evaluation and planning </strong>because it was trained to reason using chain of thought. This has significantly improved its ability to think through and solve complex challenges as reflected on a variety of benchmarks. For example, the o1 model has been shown to <strong class="oa fr">surpass human PhD-level accuracy on the GPQA benchmark </strong>covering physics, biology, and chemistry, and scored in the <strong class="oa fr">86th-93rd percentile on Codeforces</strong> contests. While o1’s reasoning ability could be used to generate text-based responses that suggest tools based on their descriptions, it currently lacks explicit tool calling abilities (at least for now!).</p><p id="1dd1" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In contrast, <strong class="oa fr">many models are fine-tuned specifically for reasoning through tool use </strong>enabling them to generate function calls and interact with APIs very effectively. These models are focused on calling the right tool in the right format at the right time, but are typically not designed to evaluate their own results as thoroughly as o1 might. The <a class="af pc" href="https://gorilla.cs.berkeley.edu/leaderboard.html" rel="noopener ugc nofollow" target="_blank"><strong class="oa fr">Berkeley Function Calling Leaderboard</strong></a><strong class="oa fr"> (BFCL) is a great resource for comparing how different models perform on function calling tasks</strong>. It also provides an <strong class="oa fr">evaluation suite to compare your own fine-tuned model</strong> on various challenging tool calling tasks. In fact, the <a class="af pc" href="https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard" rel="noopener ugc nofollow" target="_blank">latest dataset, BFCL v3</a>, was just released and now includes <a class="af pc" href="https://gorilla.cs.berkeley.edu/blogs/13_bfcl_v3_multi_turn.html" rel="noopener ugc nofollow" target="_blank">multi-step, multi-turn function calling</a>, further raising the bar for tool based reasoning tasks.</p><p id="fcad" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Both types of reasoning are powerful independently, and when combined, they have the potential to create agents that can effectively breakdown complicated tasks and autonomously interact with their environment. For more examples of AI agent architectures for reasoning, planning, and tool calling <a class="af pc" href="https://arxiv.org/abs/2404.11584" rel="noopener ugc nofollow" target="_blank">check out my team’s survey paper on ArXiv</a>.</p><h1 id="98ba" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Challenges with Tool-Calling: Navigating Complex Agent Behaviors</h1><p id="1cc1" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Building robust and reliable agents requires overcoming many different challenges. When solving complex problems, an agent often needs to balance multiple tasks at once including planning, interacting with the right tools at the right time, formatting tool calls properly, remembering outputs from previous steps, avoiding repetitive loops, and adhering to guidance to protect the system from jailbreaks/prompt injections/etc.</p><p id="e1c2" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><strong class="oa fr">Too many demands can easily overwhelm a single agent, leading to a growing trend where what may appear to an end user as one agent, is behind the scenes a collection of many agents and prompts working together to divide and conquer completing the task</strong>. This division allows tasks to be broken down and handled in parallel by different models and agents tailored to solve that particular piece of the puzzle.</p><p id="c0ba" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">It’s here that models with excellent tool calling capabilities come into play. While tool-calling is a powerful way to enable productive agents, it comes with its own set of challenges. Agents need to understand the available tools, select the right one from a set of potentially similar options, format the inputs accurately, call tools in the right order, and potentially integrate feedback or instructions from other agents or humans. Many models are fine-tuned specifically for tool calling, allowing them to specialize in selecting functions at the right time with high accuracy.</p><p id="e732" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><strong class="oa fr">Some of the key considerations when fine-tuning a model for tool calling include:</strong></p><ol class=""><li id="3836" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pi pa pb bk"><strong class="oa fr">Proper Tool Selection</strong>: The model needs to understand the relationship between available tools, make nested calls when applicable, and select the right tool in the presence of other similar tools.</li><li id="40d0" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pi pa pb bk"><strong class="oa fr">Handling Structural Challenges</strong>: Although most models use JSON format for tool calling, other formats like YAML or XML can also be used. Consider whether the model needs to generalize across formats or if it should only use one. Regardless of the format, the model needs to include the appropriate parameters for each tool call, potentially using results from a previous call in subsequent ones.</li><li id="24bb" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pi pa pb bk"><strong class="oa fr">Ensuring Dataset Diversity and Robust Evaluations</strong>: The dataset used should be diverse and cover the complexity of multi-step, multi-turn function calling. Proper evaluations should be performed to prevent overfitting and avoid benchmark contamination.</li></ol><h1 id="4e37" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Common Benchmarks to Evaluate Tool-Calling</h1><p id="7c4c" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">With the growing importance of tool use in language models, many datasets have emerged to help evaluate and improve model tool-calling capabilities. Two of the most popular benchmarks today are the Berkeley Function Calling Leaderboard and Nexus Function Calling Benchmark, both of which <a class="af pc" href="https://arxiv.org/pdf/2407.21783" rel="noopener ugc nofollow" target="_blank">Meta used to evaluate the performance of their Llama 3.1 model series</a>. A recent paper, <a class="af pc" href="https://arxiv.org/abs/2409.00920" rel="noopener ugc nofollow" target="_blank">ToolACE</a>, demonstrates how agents can be used to create a diverse dataset for fine-tuning and evaluating model tool use.</p><p id="1642" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Let’s explore each of these benchmarks in more detail:</p><ul class=""><li id="1131" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk"><strong class="oa fr">Berkeley Function Calling Leaderboard (</strong><a class="af pc" href="https://gorilla.cs.berkeley.edu/leaderboard.html" rel="noopener ugc nofollow" target="_blank"><strong class="oa fr">BFCL</strong></a><strong class="oa fr">):</strong> BFCL contains 2,000 question-function-answer pairs across multiple programming languages. Today there are 3 versions of the BFCL dataset each with enhancements to better reflect real-world scenarios. For example, <a class="af pc" href="https://gorilla.cs.berkeley.edu/blogs/12_bfcl_v2_live.html" rel="noopener ugc nofollow" target="_blank">BFCL-V2</a>, released August 19th, 2024 includes user contributed samples designed to address evaluation challenges related to dataset contamination. <a class="af pc" href="https://gorilla.cs.berkeley.edu/blogs/13_bfcl_v3_multi_turn.html" rel="noopener ugc nofollow" target="_blank">BFCL-V3</a> released September 19th, 2024 adds multi-turn, multi-step tool calling to the benchmark. This is critical for agentic applications where a model needs to make multiple tool calls over time to successfully complete a task. Instructions for e<a class="af pc" href="https://github.com/ShishirPatil/gorilla" rel="noopener ugc nofollow" target="_blank">valuating models on BFCL can be found on GitHub</a>, with the <a class="af pc" href="https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard" rel="noopener ugc nofollow" target="_blank">latest dataset available on HuggingFace</a>, and the <a class="af pc" href="https://gorilla.cs.berkeley.edu/leaderboard.html" rel="noopener ugc nofollow" target="_blank">current leaderboard accessible here</a>. The Berkeley team has also released various versions of their Gorilla Open-Functions model fine-tuned specifically for function-calling tasks.</li><li id="7ed8" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot oz pa pb bk"><strong class="oa fr">Nexus Function Calling Benchmark: </strong>This benchmark evaluates models on zero-shot function calling and API usage across nine different tasks classified into three major categories for single, parallel, and nested tool calls. Nexusflow released NexusRaven-V2, a model designed for function-calling. The <a class="af pc" href="https://github.com/nexusflowai/NexusRaven-V2/tree/master#benchmarks" rel="noopener ugc nofollow" target="_blank">Nexus benchmark is available on GitHub</a> and the corresponding <a class="af pc" href="https://huggingface.co/spaces/Nexusflow/Nexus_Function_Calling_Leaderboard" rel="noopener ugc nofollow" target="_blank">leaderboard is on HuggingFace</a>.</li><li id="3559" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot oz pa pb bk"><strong class="oa fr">ToolACE: </strong>The <a class="af pc" href="https://arxiv.org/pdf/2409.00920" rel="noopener ugc nofollow" target="_blank">ToolACE paper</a> demonstrates a creative approach to overcoming challenges related to collecting real-world data for function-calling. The research team created an agentic pipeline to generate a synthetic dataset for tool calling consisting of over 26,000 different APIs. The dataset includes examples of single, parallel, and nested tool calls, as well as non-tool based interactions, and supports both single and multi-turn dialogs. The team released a fine-tuned version of Llama-3.1–8B-Instruct, <a class="af pc" href="https://huggingface.co/Team-ACE/ToolACE-8B" rel="noopener ugc nofollow" target="_blank">ToolACE-8B</a>, designed to handle these complex tool-calling related tasks. A <a class="af pc" href="https://huggingface.co/datasets/Team-ACE/ToolACE" rel="noopener ugc nofollow" target="_blank">subset of the ToolACE dataset is available on HuggingFace</a>.</li></ul><p id="afb5" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Each of these benchmarks facilitates our ability to evaluate model reasoning expressed through tool calling. These benchmarks and fine-tuned models reflect a growing trend towards developing more specialized models for specific tasks and increasing LLM capabilities by extending their ability to interact with the real-world.</p><h1 id="5a29" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Examples of Tool-Calling in Action</h1><p id="1358" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">If you’re interested in exploring tool-calling in action, here are some examples to get you started organized by ease of use, ranging from simple built-in tools to using fine-tuned models, and agents with tool-calling abilities.</p><p id="dd6b" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><strong class="oa fr">Level 1 — ChatGPT</strong>: The best place to start and see tool-calling live without needing to define any tools yourself, is through ChatGPT. Here you can use GPT-4o through the chat interface to call and execute tools for web-browsing. For example, when asked “what’s the latest AI news this week?” ChatGPT-4o will conduct a web search and return a response based on the information it finds. <em class="pj">Remember the new o1 model does not have tool-calling abilities yet and cannot search the web.</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pk"><img src="../Images/d4e671c962183b932578d58a05a894c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vHWWAMXul7qSq0cYfuI4CQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author 9/30/24</figcaption></figure><p id="0ff5" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">While this built-in web-searching feature is convenient, most use cases will require defining custom tools that can integrate directly into your own model workflows and applications. This brings us to the next level of complexity.</p><p id="cbe8" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><strong class="oa fr">Level 2 — Using a Model with Tool Calling Abilities and Defining Custom Tools</strong>:</p><p id="80f2" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This level involves using a model with tool-calling abilities to get a sense of how effectively the model selects and uses it’s tools. It’s important to note that <strong class="oa fr">when a model is trained for tool-calling, it only generates the text or code for the tool call, it does not actually execute the code itself. Something external to the model needs to invoke the tool, and it’s at this point — where we’re combining generation with execution — that we transition from language model capabilities to agentic systems.</strong></p><p id="76b3" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">To get a sense for how models express tool calls we can turn towards the Databricks Playground. For example, we can select the model Llama 3.1 405B and give it access to the sample tools get_distance_between_locations and get_current_weather. When prompted with the user message “I am going on a trip from LA to New York how far are these two cities? And what’s the weather like in New York? I want to be prepared for when I get there” the model decides which tools to call and what parameters to pass so it can effectively reply to the user.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pl"><img src="../Images/ccc6160bc9393164ecb15310a765b94b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*TFWjO3XF69EEtwQRCCWqpQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author 10/2/2024 depicting using the Databricks Playground for sample tool calling</figcaption></figure><p id="41a7" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In this example, the model suggests two tool calls. Since the model cannot execute the tools, the user needs to fill in a sample result to simulate the tool output (e.g., “2500” for the distance and “68” for the weather). The model then uses these simulated outputs to reply to the user.</p><p id="a221" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This approach to using the Databricks Playground allows you to observe how the model uses custom defined tools and is a great way to test your function definitions before implementing them in your tool-calling enabled applications or agents.</p><p id="6893" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Outside of the Databricks Playground, we can observe and evaluate how effectively different models available on platforms like HuggingFace use tools through code directly. For example, we can load different models like Llama 3.2–3B-Instruct, ToolACE-8B, NexusRaven-V2–13B, and more from HuggingFace, give them the same system prompt, tools, and user message then observe and compare the tool calls each model returns. This is a great way to understand how well different models reason about using custom-defined tools and can help you determine which tool-calling models are best suited for your applications.</p><p id="efe6" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Here is an example demonstrating a tool call generated by Llama-3.2–3B-Instruct based on the following tool definitions and user message, the same steps could be followed for other models to compare generated tool calls.</p><pre class="mm mn mo mp mq pm pn po bp pp bb bk"><span id="afb4" class="pq nd fq pn b bg pr ps l pt pu">import torch<br/>from transformers import pipeline<br/><br/>function_definitions = """[<br/>    {<br/>        "name": "search_google",<br/>        "description": "Performs a Google search for a given query and returns the top results.",<br/>        "parameters": {<br/>            "type": "dict",<br/>            "required": [<br/>                "query"<br/>            ],<br/>            "properties": {<br/>                "query": {<br/>                    "type": "string",<br/>                    "description": "The search query to be used for the Google search."<br/>                },<br/>                "num_results": {<br/>                    "type": "integer",<br/>                    "description": "The number of search results to return.",<br/>                    "default": 10<br/>                }<br/>            }<br/>        }<br/>    },<br/>    {<br/>        "name": "send_email",<br/>        "description": "Sends an email to a specified recipient.",<br/>        "parameters": {<br/>            "type": "dict",<br/>            "required": [<br/>                "recipient_email",<br/>                "subject",<br/>                "message"<br/>            ],<br/>            "properties": {<br/>                "recipient_email": {<br/>                    "type": "string",<br/>                    "description": "The email address of the recipient."<br/>                },<br/>                "subject": {<br/>                    "type": "string",<br/>                    "description": "The subject of the email."<br/>                },<br/>                "message": {<br/>                    "type": "string",<br/>                    "description": "The body of the email."<br/>                }<br/>            }<br/>        }<br/>    }<br/>]<br/>"""<br/><br/># This is the suggested system prompt from Meta<br/>system_prompt = """You are an expert in composing functions. You are given a question and a set of possible functions. <br/>Based on the question, you will need to make one or more function/tool calls to achieve the purpose. <br/>If none of the function can be used, point it out. If the given question lacks the parameters required by the function,<br/>also point it out. You should only return the function call in tools call sections.<br/><br/>If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n<br/>You SHOULD NOT include any other text in the response.<br/><br/>Here is a list of functions in JSON format that you can invoke.\n\n{functions}\n""".format(functions=function_definitions)</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pv"><img src="../Images/c611f529dd5a8b70118f307696a7b85b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IebNomZdQ1Ldn8TAAuNOvw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author sample output demonstrating generated tool call from Llama 3.2–3B-Instruct</figcaption></figure><p id="6f14" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">From here we can move to Level 3 where we’re defining Agents that execute the tool-calls generated by the language model.</p><p id="b8eb" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><strong class="oa fr">Level 3 Agents (invoking/executing LLM tool-calls)</strong>: Agents often express reasoning both through planning and execution as well as tool calling making them an increasingly important aspect of AI based applications. Using libraries like LangGraph, AutoGen, Semantic Kernel, or LlamaIndex, you can quickly create an agent using models like GPT-4o or Llama 3.1–405B which support both conversations with the user and tool execution.</p><p id="de1d" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Check out these guides for some exciting examples of agents in action:</p><ul class=""><li id="21e7" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot oz pa pb bk">LangGraph: <a class="af pc" href="https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag_local/" rel="noopener ugc nofollow" target="_blank">Local RAG Agent with Llama 3</a></li><li id="ecdf" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot oz pa pb bk">AutoGen: <a class="af pc" href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_web_info.ipynb" rel="noopener ugc nofollow" target="_blank">Solve Tasks Requiring Web Info</a></li><li id="a58c" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot oz pa pb bk">Semantic Kernel: <a class="af pc" href="https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/GettingStartedWithAgents/README.md" rel="noopener ugc nofollow" target="_blank">Getting Started with Agents in Semantic Kernel</a></li><li id="6ede" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot oz pa pb bk">LlamaIndex: <a class="af pc" href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/usage_pattern/" rel="noopener ugc nofollow" target="_blank">Agent Usage Pattern Documentation</a></li></ul><h1 id="70a7" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Conclusion:</h1><p id="1407" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The future of agentic systems will be driven by models with strong reasoning abilities enabling them to effectively interact with their environment. As the field evolves, I expect we will continue to see a proliferation of smaller, specialized models focused on specific tasks like tool-calling and planning.</p><p id="d5a4" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">It’s important to consider the current limitations of model sizes when building agents. For example, according to the <a class="af pc" href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1" rel="noopener ugc nofollow" target="_blank">Llama 3.1 model card</a>, the Llama 3.1–8B model is not reliable for tasks that involve both maintaining a conversation and calling tools. Instead, larger models with 70B+ parameters should be used for these types of tasks. This alongside other emerging research for fine-tuning small language models suggests that smaller models may serve best as specialized tool-callers while larger models may be better for more advanced reasoning. By combining these abilities, we can build increasingly effective agents that provide a seamless user experience and allow people to leverage these reasoning abilities in both professional and personal endeavors.</p><p id="fd82" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="pj">Interested in discussing further or collaborating? Reach out on </em><a class="af pc" href="https://www.linkedin.com/in/tula-masterman/" rel="noopener ugc nofollow" target="_blank"><em class="pj">LinkedIn</em></a><em class="pj">!</em></p></div></div></div></div>    
</body>
</html>