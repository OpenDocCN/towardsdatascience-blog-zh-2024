- en: 'Audio Diffusion: Generative Music’s Secret Sauce'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 音频扩散：生成音乐的秘密武器
- en: 原文：[https://towardsdatascience.com/audio-diffusion-generative-musics-secret-sauce-f625d0aca800?source=collection_archive---------0-----------------------#2024-01-22](https://towardsdatascience.com/audio-diffusion-generative-musics-secret-sauce-f625d0aca800?source=collection_archive---------0-----------------------#2024-01-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/audio-diffusion-generative-musics-secret-sauce-f625d0aca800?source=collection_archive---------0-----------------------#2024-01-22](https://towardsdatascience.com/audio-diffusion-generative-musics-secret-sauce-f625d0aca800?source=collection_archive---------0-----------------------#2024-01-22)
- en: '![](../Images/1bd508ba6fc0a161a09a10ef07843c01.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bd508ba6fc0a161a09a10ef07843c01.png)'
- en: Image generated with DALL·E
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由 DALL·E 生成
- en: Exploring the principles behind diffusion technology and how it is being used
    to create groundbreaking AI tools for artists and producers.
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探讨扩散技术背后的原理及其如何被应用于为艺术家和制作人创造突破性人工智能工具。
- en: '[](https://medium.com/@crlandschoot?source=post_page---byline--f625d0aca800--------------------------------)[![Christopher
    Landschoot](../Images/99a2569f5a6a3a99fd1f72553aa3d634.png)](https://medium.com/@crlandschoot?source=post_page---byline--f625d0aca800--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f625d0aca800--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f625d0aca800--------------------------------)
    [Christopher Landschoot](https://medium.com/@crlandschoot?source=post_page---byline--f625d0aca800--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@crlandschoot?source=post_page---byline--f625d0aca800--------------------------------)[![Christopher
    Landschoot](../Images/99a2569f5a6a3a99fd1f72553aa3d634.png)](https://medium.com/@crlandschoot?source=post_page---byline--f625d0aca800--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f625d0aca800--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f625d0aca800--------------------------------)
    [Christopher Landschoot](https://medium.com/@crlandschoot?source=post_page---byline--f625d0aca800--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f625d0aca800--------------------------------)
    ·14 min read·Jan 22, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f625d0aca800--------------------------------)
    ·阅读时间：14分钟·2024年1月22日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Much has been made of the hype of recent generative music AI algorithms. Some
    see it as the future of creativity and others see it as the death of music. While
    I tend to lean towards the former camp, as an engineer and researcher, I generally
    attempt to view these advancements through a more objective lens. With this in
    mind, I wanted to offer an introduction to one of the core technologies fueling
    the world of generative audio and music: [*Diffusion*](https://en.wikipedia.org/wiki/Diffusion_model).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近关于生成音乐人工智能算法的炒作引起了广泛关注。有些人认为它是创造力的未来，而另一些人则认为它是音乐的终结。虽然我倾向于支持前者，但作为一名工程师和研究员，我通常尝试从更加客观的角度来看待这些进展。鉴于此，我想介绍一下驱动生成音频和音乐世界的核心技术之一：[*扩散*](https://en.wikipedia.org/wiki/Diffusion_model)。
- en: My goal is not to sell or diminish the hype, but rather shed some light on what
    is happening under the hood so that musicians, producers, hobbyists, and creators,
    can better understand these new seemingly magic music-making black boxes. I will
    answer what it means by the claim that these AI algorithms are “creating something
    completely new” and how that differs from human originality. I hope that a clearer
    picture will lower the collective temperature and provide insight into how these
    powerful technologies can be leveraged to the benefit of the creator.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我的目标不是推销或贬低这些炒作，而是揭示这些技术背后发生的事情，让音乐家、制作人、爱好者和创作者能更好地理解这些看似神奇的音乐创作黑盒。我将回答这些人工智能算法“创造出完全新的东西”这一说法的含义，以及它与人类原创性有何不同。我希望通过更清晰的解释，降低集体的焦虑，并提供洞见，帮助创作者更好地利用这些强大的技术。
- en: This piece will touch on technical topics, but you do not need an engineering
    background to follow along. Let’s begin with some context and definitions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将涉及一些技术话题，但你不需要具备工程背景就能理解。让我们先从一些背景信息和定义开始。
- en: Background
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 背景
- en: The term “AI-generated” has become pervasive across the music industry, but
    what qualifies as “AI-generated” is actually quite clouded. Eager to jump on the
    hype of the buzzword, this claim is casually tossed around whether AI is used
    to emulate an effect, automatically mix or master, separate [stems](https://en.wikipedia.org/wiki/Stem_(audio)),
    or augment timbre. As long as the final audio has been touched in some way by
    AI, the term gets slapped on the entire piece. However, the vast majority of music
    presently released continues to be primarily generated through human production
    (yes, even ghostwriter’s “[Heart On My Sleeve](https://youtu.be/7HZ2ie2ErFI?si=Ie3954lyyj7l-tEG)”
    👻).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: “AI生成”这个术语已经在音乐行业中变得非常普及，但什么才算是“AI生成”的内容实际上是相当模糊的。为了赶上这个流行词的热潮，这个说法被随意地使用，无论是AI用来模仿某种效果、自动混音或母带处理、分离[音轨](https://en.wikipedia.org/wiki/Stem_(audio))，还是增强音色。只要最终的音频在某种程度上受到AI的处理，这个术语就会被贴上整个作品。然而，目前发布的大多数音乐仍然主要通过人工制作生成（是的，即使是鬼才作曲者的“[Heart
    On My Sleeve](https://youtu.be/7HZ2ie2ErFI?si=Ie3954lyyj7l-tEG)” 👻）。
- en: Even though this “AI-generated” term is becoming hackneyed for clicks, an appropriate
    use is when new sounds truly are being created by a computer, i.e. *Generative
    Audio*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管“AI生成”这个术语为了点击率而变得老生常谈，但它的恰当使用是当新声音的确是由计算机生成时，即*生成音频*。
- en: Audio generation can encompass the creation of sound effects samples, melodies,
    vocals, and even full songs. The two main ways that this is achieved are via *MIDI
    Generation* and A*udio Waveform Generation*. [MIDI (Musical Instrument Digital
    Interface)](https://blog.landr.com/what-is-midi/) generation has a much lower
    computational cost and can provide high-quality outputs, as the generated MIDI
    data is then run through an existing virtual instrument to produce sounds. This
    is the same concept as a producer programming MIDI on a [piano roll](https://blog.landr.com/piano-roll/)
    and playing it through a [VST](https://en.wikipedia.org/wiki/Virtual_Studio_Technology)
    plugin such as [Serum](https://xferrecords.com/products/serum/).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 音频生成可以涵盖音效样本、旋律、人声，甚至完整歌曲的创作。实现这一点的两种主要方式是通过*MIDI生成*和*音频波形生成*。[MIDI（数字乐器接口）](https://blog.landr.com/what-is-midi/)生成计算成本较低，并且能够提供高质量的输出，因为生成的MIDI数据会通过现有的虚拟乐器来产生声音。这与制作人通过[piano
    roll](https://blog.landr.com/piano-roll/)编程MIDI并通过[VST](https://en.wikipedia.org/wiki/Virtual_Studio_Technology)插件（如[Serum](https://xferrecords.com/products/serum/)）播放的概念相同。
- en: '![](../Images/2825dd04aa8dd49f0cb8d0cd21b1550c.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2825dd04aa8dd49f0cb8d0cd21b1550c.png)'
- en: MIDI Piano Roll in Pro Tools
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Pro Tools中的MIDI钢琴卷轴
- en: While this is compelling, it is only partially generative, as no audio is actually
    produced by AI, just as humans cannot synthesize instrument sounds from thin air.
    The creative capabilities are further limited by whatever virtual instruments
    the algorithm has access to. Even with these limitations, products implementing
    this technique, such as [AIVA](https://www.aiva.ai/) and [Seeds by Lemonaide](https://www.lemonaide.ai/),
    can generate quite compelling outputs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这一点很有吸引力，但它仅部分是生成的，因为实际上没有音频是由AI生成的，就像人类不能凭空合成乐器的声音一样。创作能力还受到算法能够访问的虚拟乐器的限制。即便有这些限制，采用这种技术的产品，如[AIVA](https://www.aiva.ai/)和[Seeds
    by Lemonaide](https://www.lemonaide.ai/)，也能够生成相当引人注目的输出。
- en: Audio waveform generation is a much more complicated task, as it is an end-to-end
    system that does not rely on any external technology. In other words, it produces
    sounds from scratch. This process most precisely aligns with the true definition
    of “AI-generated” audio.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 音频波形生成是一项更为复杂的任务，因为它是一个端到端的系统，不依赖任何外部技术。换句话说，它从零开始生成声音。这个过程最精确地符合“AI生成”音频的真正定义。
- en: Audio waveform generation can be accomplished using various approaches and yield
    different outcomes. It can produce single samples, such as [Audialab’s ED 2](https://audialab.com/features/)
    and [Humanize](https://audialab.com/humanize/) or my previous work with [Tiny
    Audio Diffusion](/tiny-audio-diffusion-ddc19e90af9b), all the way up to full songs,
    with models such as [AudioLM](https://google-research.github.io/seanet/audiolm/examples/),
    [Moûsai](https://flavioschneider.notion.site/flavioschneider/Audio-Generation-with-Diffusion-c4f29f39048d4f03a23da13078a44cdb),
    [Riffusion](https://www.riffusion.com/), [MusicGen](https://ai.honu.io/papers/musicgen/),
    and [Stable Audio](https://www.stableaudio.com/). Among these state-of-the-art
    models, many leverage some form of *Diffusion* to generate sounds. You have likely
    at least tangentially heard of diffusion from [Stable Diffusion](https://stability.ai/stable-diffusion)
    or any of the other top-performing image generation models that took the world
    by storm. This generative method can be applied to audio as well. But what does
    any of it actually mean?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 音频波形生成可以通过多种方法实现，并产生不同的结果。它可以生成单个样本，比如[Audialab的ED 2](https://audialab.com/features/)和[Humanize](https://audialab.com/humanize/)，或是我之前的作品[Tiny
    Audio Diffusion](/tiny-audio-diffusion-ddc19e90af9b)，也可以生成完整的歌曲，如[AudioLM](https://google-research.github.io/seanet/audiolm/examples/)、[Moûsai](https://flavioschneider.notion.site/flavioschneider/Audio-Generation-with-Diffusion-c4f29f39048d4f03a23da13078a44cdb)、[Riffusion](https://www.riffusion.com/)、[MusicGen](https://ai.honu.io/papers/musicgen/)和[Stable
    Audio](https://www.stableaudio.com/)。在这些最先进的模型中，许多都利用了某种形式的*扩散*来生成声音。你可能至少在某种程度上听说过扩散，可能是通过[稳定扩散](https://stability.ai/stable-diffusion)或其他一些曾席卷全球的顶尖图像生成模型。这种生成方法同样可以应用于音频。那么这一切到底是什么意思呢？
- en: What is Diffusion?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是扩散？
- en: The Basics
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础知识
- en: In the context of AI, diffusion simply refers to the process of adding or removing
    noise from a signal (like static from an old TV). *Forward Diffusion* adds noise
    to a signal (*Noising*) and *Reverse Diffusion* removes noise (*Denoising*). At
    a conceptual level, diffusion models take white noise and step through the denoising
    processes until the audio resembles something recognizable, such as a sample or
    a song. This process of denoising a signal is the secret sauce in the creativity
    of many generative audio models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能的背景下，扩散指的只是给信号添加或移除噪声的过程（就像老电视机中的静电噪声）。*前向扩散*向信号中添加噪声（*噪声化*），而*反向扩散*则移除噪声（*去噪*）。从概念层面来看，扩散模型将白噪声逐步通过去噪过程，直到音频类似于某个可识别的声音，比如一个样本或一首歌。这个去噪过程是许多生成音频模型创造力的秘密武器。
- en: '![](../Images/a50d1ee92ac55dd644ea459cf0b23f44.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a50d1ee92ac55dd644ea459cf0b23f44.png)'
- en: 'Audio Waveform Diffusion (Source: [CRASH: Raw Audio Score-based Generative
    Modeling for Controllable High-resolution Drum Sound Synthesis (Rouard, Hadjeres)](https://github.com/simonrouard/CRASH))'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 音频波形扩散（来源：[CRASH：基于原始音频评分的可控高分辨率鼓声合成生成模型（Rouard, Hadjeres）](https://github.com/simonrouard/CRASH)）
- en: This process was originally developed for images. Looking at how the noise resolves
    into an image (for instance, a puppy sitting next to a tennis ball) provides an
    even clearer example of how these models work.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程最初是为图像开发的。观察噪声如何解析成一幅图像（例如，一只小狗坐在网球旁边）能更清楚地展示这些模型是如何工作的。
- en: '![](../Images/35afb6da06976e35eca7951128b52ec0.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35afb6da06976e35eca7951128b52ec0.png)'
- en: Image Diffusion (Image generated with Stable Diffusion)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图像扩散（图像生成使用稳定扩散）
- en: With a conceptual understanding, let’s take a look under the hood at the key
    components of the architecture of an audio diffusion model. While this will veer
    into the technical side of things, stick with me, as a deeper understanding of
    how these algorithms work will better illustrate why and how they produce the
    results that they do (if not, you can always just ask [ChatGPT](https://chat.openai.com/)
    for the Cliff Notes).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过概念性的理解，让我们深入探讨音频扩散模型架构的关键组成部分。虽然这会涉及一些技术性内容，但请跟着我，因为对这些算法如何工作的深入理解将更好地说明它们是如何产生结果的（如果没有，您随时可以向[ChatGPT](https://chat.openai.com/)请求简明扼要的解释）。
- en: The U-Net Model Architecture, Compression, and Reconstruction
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: U-Net模型架构、压缩与重建
- en: At the core of an audio diffusion model is the [*U-Net*](https://arxiv.org/abs/1505.04597).
    Originally developed for medical image segmentation and aptly named for its resemblance
    to a U, the U-Net has been adapted to generative audio due to its powerful ability
    to capture both local and global features in data. The original U-Net was a 2-dimensional
    [*convolutional neural network*](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    (CNN) used for images, but can be adapted to 1-dimensional convolution to work
    with audio waveform data. See a visual representation of the original U-Net architecture
    (for images) below.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在音频扩散模型的核心是[*U-Net*](https://arxiv.org/abs/1505.04597)。U-Net最初是为了医学图像分割而开发的，因其外形像字母U而得名，后来由于其强大的能力能够捕捉数据中的局部和全局特征，被适应用于生成音频。原始的U-Net是一个二维[*卷积神经网络*](https://en.wikipedia.org/wiki/Convolutional_neural_network)（CNN），用于图像处理，但也可以适配为一维卷积，以处理音频波形数据。请参见下面的原始U-Net架构（用于图像）的视觉表示。
- en: '![](../Images/7bbc0b12b588c6e781f128256fdcbb55.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bbc0b12b588c6e781f128256fdcbb55.png)'
- en: 'U-Net (Source: [U-Net: Convolutional Networks for Biomedical Image Segmentation
    (Ronneberger, et. al)](https://arxiv.org/abs/1505.04597v1))'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'U-Net（来源：[U-Net: 卷积网络用于生物医学图像分割（Ronneberger 等）](https://arxiv.org/abs/1505.04597v1)）'
- en: Similar to a [Variational Autoencoder (VAE)](/understanding-variational-autoencoders-vaes-f70510919f73),
    A U-Net consists of an *encoder* (the left side of the U) and a *decoder* (the
    right side of the U), connected by a *bottleneck* (the bottom layer of the U).
    Unlike a VAE, however, a U-Net hosts *skip connections* (shown by the horizontal
    gray arrows) that link the encoder to the decoder which is the key piece to producing
    high-resolution outputs. The encoder is responsible for capturing the [*features*](https://en.wikipedia.org/wiki/Feature_(machine_learning)),
    or characteristics, of the input audio signal while the decoder is responsible
    for the reconstruction of the signal.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[变分自编码器（VAE）](/understanding-variational-autoencoders-vaes-f70510919f73)，U-Net由*编码器*（U的左侧）和*解码器*（U的右侧）组成，通过*瓶颈*（U的底部层）相连接。然而，与VAE不同，U-Net具有*跳跃连接*（由水平灰色箭头表示），这些连接将编码器与解码器连接起来，这是生成高分辨率输出的关键部分。编码器负责捕捉输入音频信号的[*特征*](https://en.wikipedia.org/wiki/Feature_(machine_learning))，或特性，而解码器负责信号的重建。
- en: To aid the visualization, picture the audio data entering the top left side
    of the U, following the red and blue arrows down the encoder to the bottleneck
    at the bottom, and then back up the decoder following the blue and green arrows
    to the top right of the U. Each blue rectangle represents a model *layer*. At
    each level of the encoder, the input audio signal is compressed further and further
    until it reaches a highly condensed representation of the sound at the base of
    the U (the bottleneck). The decoder then takes this compressed signal and effectively
    reverses the process to reconstruct the signal. Each layer (blue rectangle) the
    data passes through has a series of adjustable weights associated with it that
    can be thought of as millions of tiny knobs that can be turned to tweak this compression/reconstruction
    process. Having layers at different levels of compression allows the model to
    learn a range of features from the data, from large-scale features (e.g. melody
    and rhythm) to fine-grained details (e.g. high-frequency timbral characteristics).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助可视化，可以想象音频数据从U的左上方进入，沿着红色和蓝色箭头通过编码器向下到达U的底部瓶颈层，然后再沿着蓝色和绿色箭头通过解码器回到U的右上方。每个蓝色矩形代表一个模型*层*。在编码器的每一层中，输入音频信号会逐渐被压缩，直到它在U的底部（瓶颈处）达到高度浓缩的声音表示。然后，解码器接收这个压缩信号，并有效地逆转这一过程以重建信号。数据通过的每一层（蓝色矩形）都有一系列可调的权重，可以看作是成千上万的微小旋钮，用户可以旋转这些旋钮来调整压缩/重建过程。具有不同压缩级别的层允许模型从数据中学习各种特征，从大尺度的特征（例如旋律和节奏）到细粒度的细节（例如高频音色特征）。
- en: Using an analogy, you can think of this full system as the process required
    to create an [MP3](https://en.wikipedia.org/wiki/MP3) audio file and then listen
    to that MP3 on a playback device. At its core, an MP3 is a compressed version
    of an audio signal. Imagine that the encoder’s job is to create a new type of
    compressed audio format, just like an MP3, in order to consistently condense an
    audio signal as much as possible without losing fidelity. Then the decoder’s job
    is to act like your iPhone (or any playback device) and reconstruct the MP3 into
    a high-fidelity audio representation that can be played through your headphones.
    The bottleneck can be thought of as this newly created MP3-type format, itself.
    **The U-Net represents the process of compression and reconstruction, not the
    audio data**. The model can then be trained with the goal of being able to accurately
    compress and reconstruct a wide range of audio signals.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类比，你可以将整个系统想象成创建一个[MP3](https://en.wikipedia.org/wiki/MP3)音频文件并在播放设备上收听该MP3的过程。从本质上讲，MP3是音频信号的压缩版本。假设编码器的工作是创建一种新的压缩音频格式，类似于MP3，以尽可能地压缩音频信号而不损失保真度。然后，解码器的工作就像你的iPhone（或任何播放设备），将MP3解码成可以通过耳机播放的高保真音频表现形式。瓶颈可以看作是这个新创建的MP3类型格式本身。**U-Net代表的是压缩和重建的过程，而不是音频数据**。然后，可以以准确压缩和重建各种音频信号为目标训练这个模型。
- en: This is all well and good, but we haven’t generated anything yet. We’ve only
    constructed a way to compress and reconstruct an audio signal. However, this is
    the base process required to begin generating new audio, and it only requires
    a slight adjustment to do so.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都很好，但我们还没有生成任何内容。我们只构建了压缩和重建音频信号的方法。然而，这个过程是生成新音频所必需的基本过程，而且只需稍微调整一下就能实现。
- en: Noising and Denoising
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 噪声与去噪
- en: Let's revisit the idea of *noising* and *denoising* that we touched on earlier.
    Theoretically, we had some magic model that could be taught to take some white
    noise and “denoise” it into recognizable audio, perhaps a beautiful concerto.
    A critical requirement of this magic model is that it must be able to reconstruct
    the input audio signal at high fidelity. Luckily, the U-Net architecture is designed
    to do exactly that. So the next piece of the puzzle is to modify the U-Net to
    perform this denoising process.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们之前提到的*噪声*和*去噪*的概念。从理论上讲，我们曾设想过一个魔法模型，它可以被训练来将一些白噪声“去噪”成可识别的音频，可能是一首美丽的协奏曲。这个魔法模型的一个关键要求是，它必须能够以高保真度重建输入的音频信号。幸运的是，U-Net架构的设计正是为了完成这一任务。因此，接下来要解决的难题是修改U-Net以执行这个去噪过程。
- en: Counter-intuitively, to teach a model to denoise an audio signal, it is first
    taught how to add noise to a signal. Once it has learned this process, it inherently
    knows how to perform the inverse in order to denoise a signal.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 违反直觉的是，为了教会一个模型去噪音频信号，首先要教它如何给信号添加噪声。一旦它学会了这个过程，它就自然知道如何执行逆操作，以去除噪声。
- en: Recall in the previous section that we detailed how the U-Net can learn to compress
    and reconstruct an audio signal. The noising process follows nearly the same formula,
    but instead of reconstructing the exact input audio signal, the U-Net is directed
    to reconstruct the input audio signal with a small amount of noise added to it.
    This can be visualized as reversing the steps taken in the earlier series of images
    of the puppy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下前一部分，我们详细描述了U-Net如何学习压缩和重建音频信号。噪声处理过程几乎遵循相同的公式，但不同的是，U-Net并不是重建完全相同的输入音频信号，而是被指导重建加入少量噪声的输入音频信号。这可以通过反转之前小狗图像序列中的步骤来可视化。
- en: '![](../Images/9e223a528f9d9406258a26a766f85b89.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e223a528f9d9406258a26a766f85b89.png)'
- en: Diffusion Noising Steps (Image generated with Stable Diffusion)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散噪声步骤（图像由Stable Diffusion生成）
- en: The process of adding noise to a signal must be probabilistic (i.e. predictable).
    The model is shown an audio signal and then instructed to predict the same signal
    with a small amount of [Gaussian noise](https://en.wikipedia.org/wiki/Gaussian_noise#:~:text=In%20signal%20processing%20theory%2C%20Gaussian,can%20take%20are%20Gaussian%2Ddistributed.)
    added to it. Because of its properties, Gaussian noise is most commonly used but
    it is not required. The noise must be defined by probabilistic distribution, meaning
    that it follows a specific pattern that is consistently predictable. This process
    of instructing the model to add small amounts of predictable noise to the audio
    signal is repeated for a number of *steps* until the signal has effectively become
    just noise.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 向信号添加噪声的过程必须是概率性的（即可预测的）。模型首先展示给一个音频信号，然后被指示预测添加少量[高斯噪声](https://en.wikipedia.org/wiki/Gaussian_noise#:~:text=In%20signal%20processing%20theory%2C%20Gaussian,can%20take%20are%20Gaussian%2Ddistributed.)后的信号。由于其特性，高斯噪声最为常见，但并非必须使用。噪声必须由概率分布定义，意味着它遵循一个特定的模式，且这个模式是可以一致预测的。这个过程会在多个*步骤*中重复，直到信号最终变成只有噪声。
- en: '![](../Images/c04fe7144cc50b5a0c8ba8149e95d410.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c04fe7144cc50b5a0c8ba8149e95d410.png)'
- en: 'Noising a Snare Sample (Source: [CRASH: Raw Audio Score-based Generative Modeling
    for Controllable High-resolution Drum Sound Synthesis (Rouard, Hadjeres)](https://github.com/simonrouard/CRASH))'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '添加噪声到击鼓样本（来源：[CRASH: 基于音频分数的可控高分辨率鼓声合成的原始音频生成模型 (Rouard, Hadjeres)](https://github.com/simonrouard/CRASH)）'
- en: For example, let’s take a one-shot sample of a snare drum. The U-Net is provided
    this snare sample and it is asked to reconstruct that snare sound, but with a
    little noise added making it sound a little less clean. Then this slightly noisy
    snare sample is provided to the model, and it is again instructed to reconstruct
    this snare sample with even more noise. This cycle is repeated until it sounds
    as if the snare sample no longer exists, rather only white noise remains. The
    model is then taught how to do this for a wide range of sounds. Once it becomes
    an expert at predicting how to add noise to an input audio signal, because the
    process is probabilistic, it can simply be reversed so that at each step a little
    noise is removed. This is how the model can generate a snare sample when provided
    with white noise.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们以一个击鼓样本为例。U-Net 接收到这个击鼓样本，并被要求重建这个击鼓声音，但加入一些噪声，使其听起来不那么干净。然后，这个略带噪声的击鼓样本被再次提供给模型，并再次要求重建这个击鼓样本，同时增加更多噪声。这个循环会重复进行，直到听起来像是击鼓样本已经不存在，只剩下白噪声。接着，模型被教会如何在广泛的声音中执行这种操作。一旦它成为预测如何向输入音频信号添加噪声的专家，因为这个过程是概率性的，它就可以简单地反转，使得在每一步移除一些噪声。这就是模型在提供白噪声时能够生成击鼓样本的方式。
- en: '**Because of the probabilistic nature of this process, some incredible capabilities
    arise, specifically the ability to simulate creativity.**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**由于这个过程的概率性特征，一些令人难以置信的能力出现了，特别是模拟创造力的能力。**'
- en: Let’s continue with our snare example. Imagine the model was trained on thousands
    of one-shot snare samples. You would imagine that it could take some white noise
    and then turn it into any one of these snare samples. However, that is not exactly
    how the model learns. Because it is shown such a wide range of sounds, it instead
    learns to create sounds that are generally similar to any of the snares that it
    has been trained on, but not exactly. This is how brand new sounds are created
    and these models appear to exhibit a spark of creativity.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讨论击鼓的例子。假设模型已经在成千上万个单次击鼓样本上进行了训练。你可能会认为它可以拿一些白噪声，然后将其转化为任何一个这些击鼓样本。然而，模型的学习方式并不完全是这样。由于它被展示了如此广泛的声音范围，它反而学会了创建那些与它训练过的击鼓样本大致相似的声音，但并不完全相同。这就是如何创造全新声音的过程，这些模型看起来展现出了某种创造力的火花。
- en: To illustrate this, let’s use the following sketch.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们使用以下草图。
- en: '![](../Images/b4ad00a6c897be31a523510cd5e747c1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4ad00a6c897be31a523510cd5e747c1.png)'
- en: Pretend that all possible sounds, from guitar strums to dog barks to white noise,
    can be plotted on a 2-dimensional plane represented by the black rectangle in
    the image above. Within this space, there is a region where snare hits exist.
    They are somewhat grouped together because of their similar timbral and transient
    characteristics. This is shown by the blue blob and each blue dot is representative
    of a single snare sample that we trained our model on. The red dots represent
    the fully noised versions of the snares the model was trained on and correspond
    to their un-noised blue dot counterparts.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 假设所有可能的声音，从吉他弹奏到狗吠声，再到白噪声，都可以绘制在一个二维平面上，平面由上图中的黑色矩形表示。在这个空间中，有一个区域是小军鼓击打声所在的位置。由于它们在音色和瞬态特性上的相似性，它们被稍微聚集在一起。这由蓝色的斑点显示，每一个蓝色的点代表我们用来训练模型的一个小军鼓样本。红色的点代表模型训练时使用的已经加入噪声的小军鼓版本，并与它们未加入噪声的蓝色点样本相对应。
- en: In essence, our model learned to take dots from the “not snare” region and bring
    them into the “snare” region. So if we take a new green dot in the “not snare”
    region (e.g. random noise) that does not correspond to any blue dot, and ask our
    model to bring it into the “snare” region, it will bring it to a new location
    within that “snare” region. This is the model generating a “new” snare sample
    that contains some similarities to all other snares it was trained on in the snare
    region, but also some new unknown characteristics.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们的模型学会了将“非小军鼓”区域的点带入“军鼓”区域。所以，如果我们从“非小军鼓”区域（例如随机噪声）选取一个新的绿色点，它与任何蓝色点都不对应，并要求我们的模型将其带入“军鼓”区域，模型将把它带到“军鼓”区域内的一个新位置。这就是模型生成一个“新的”军鼓样本，虽然它与模型训练时的所有军鼓样本有相似之处，但也包含一些新的、未知的特征。
- en: This concept can be applied to any type of sound, including full songs. This
    is an incredible innovation that can lead to numerous new ways to create. It is
    important to understand that these models will not create something outside of
    the bounds of how they are trained, however. As shown in the previous illustration,
    while our conceptual model can take in any type of sound, it can only produce
    snare samples similar to those it was trained on. This holds true for any of these
    audio diffusion models. Because of this, it is critical to train models on extensive
    datasets so the known regions (like the snare region) are sufficiently diverse
    and large enough to not simply copy the training data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这一概念可以应用于任何类型的声音，包括完整的歌曲。这是一个令人惊叹的创新，能够引领创作方式的多种变化。然而，重要的是要理解，这些模型不会生成超出它们训练范围的内容。如前图所示，尽管我们的概念模型可以处理任何类型的声音，但它只能生成类似于训练样本的小军鼓样本。所有这些音频扩散模型都遵循这一原则。因此，训练模型时使用广泛的数据集至关重要，以确保已知区域（如小军鼓区域）足够多样化且规模足够大，从而避免仅仅复制训练数据。
- en: '**All of this means that no model can replicate human creativity, just simulate
    variations of it.**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**这一切意味着没有任何模型能够复制人类的创造力，只能模拟它的变体。**'
- en: Applications of Diffusion Models
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩散模型的应用
- en: 'These models will not magically generate new genres or explore unknown sonic
    landscapes as humans do. With this understanding, these generative models should
    not be viewed as a replacement for human creativity, but rather as tools that
    can enhance creativity. Below are just a few ways that this technology can be
    leveraged for creative means:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型并不会像人类那样神奇地生成新的音乐风格或探索未知的声音景观。理解这一点后，我们应该将这些生成模型视为增强创意的工具，而不是替代人类创意的替代品。以下是这项技术在创作中应用的几种方式：
- en: '**Creativity Through Curation**: Searching through sample packs to find a desired
    sound is a common practice in production. These models can effectively be used
    as a version of an “unlimited sample pack”, enhancing an artist’s creativity through
    the curation of sounds.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过策展激发创造力**：在采样包中搜索以找到所需的声音是制作过程中常见的做法。这些模型可以有效地作为“无限采样包”的一种形式，通过声音的策展来增强艺术家的创造力。'
- en: '**Voice Transfer**: Just like how diffusion models can take random noise and
    change it into recognizable audio, they can also be fed other sounds and “transfer”
    them to another type of sound. If we take our previous snare model, for example,
    and feed it a kick drum sample instead of white noise, it will take the kick sample
    and begin to morph it into a snare sound. This allows for very unique creations,
    being able to combine the characteristics of multiple different sounds.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**声音转移：** 就像扩散模型可以将随机噪声转化为可识别的音频一样，它们也可以接收其他声音并将其“转移”到另一种类型的声音上。例如，如果我们使用之前的军鼓模型，并输入一个踢鼓样本而不是白噪声，它会将踢鼓样本转变成军鼓声音。这使得非常独特的创作成为可能，能够结合多种不同声音的特征。'
- en: '**Sound Variability (Humanization)**: When humans play a live instrument, such
    as a hi-hat on a drum set, there is always inherent variability in each hit. Various
    virtual instruments have attempted to simulate this via a number of different
    methods, but can still sound artificial and lack character. Audio diffusion allows
    for the unlimited variation of a single sound, which can add a human element to
    an audio sample. For example, if you program a drum kit, audio diffusion can be
    leveraged so that each hit is slightly different in timbre, velocity, attack,
    etc. to humanize what might sound like a stale performance.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**声音变异性（人性化）：** 当人类演奏现场乐器时，例如鼓组中的高帽，每一次击打都会有固有的变异性。各种虚拟乐器尝试通过不同的方法模拟这一现象，但仍然可能听起来不自然，缺乏个性。音频扩散可以实现单一声音的无限变化，从而为音频样本添加人性化元素。例如，如果你编程一个鼓组，音频扩散可以用来让每次击打在音色、力度、起音等方面略有不同，从而使原本可能显得呆板的演奏更具人性化。'
- en: '**Sound Design Adjustments**: Similar to the human variability potential, this
    concept can also be applied to sound design to create slight changes to a sound.
    Perhaps you mostly like the sound of a door slam sample, but you wish that it
    had more body or crunch. A diffusion model can take this sample and slightly change
    it to maintain most of its characteristics while taking on a few new ones. This
    can add, remove, or change the spectral content of a sound at a more fundamental
    level than applying an EQ or filter.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**声音设计调整：** 类似于人类的变异性潜力，这一概念也可以应用于声音设计，创造对声音的轻微变化。也许你大多喜欢门砰的一声样本，但希望它有更多的质感或脆响。扩散模型可以利用这个样本并对其进行微调，保持大部分特征，同时加入一些新的特征。这可以在比使用均衡器或滤波器更基础的层面上，添加、去除或改变声音的频谱内容。'
- en: '**Melody Generation:** Similar to surfing through sample packs, audio diffusion
    models can generate melodies that can spark ideas to build on.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**旋律生成：** 类似于浏览样本包，音频扩散模型可以生成旋律，激发出可供进一步创作的灵感。'
- en: '**Stereo Effect**: There are several different mixing tricks to add stereo
    width to a single-channel (mono) sound. However, they can often add undesired
    coloration, delay, or phase shifts. Audio diffusion can be leveraged to generate
    a sound nearly identical to the mono sound, but different enough in its content
    to expand the stereo width while avoiding many of the unwanted phenomena.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**立体声效果：** 有多种不同的混音技巧可以为单声道（单声道）声音添加立体声宽度。然而，这些方法往往会带来不必要的色彩、延迟或相位偏移。音频扩散可以用来生成几乎与单声道声音相同的声音，但其内容足够不同，以扩展立体声宽度，同时避免许多不希望出现的现象。'
- en: '**Super Resolution:** Audio diffusion models can enhance the resolution and
    quality of audio recordings, making them clearer and more detailed. This can be
    particularly useful in audio restoration or when working with low-quality recordings.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超分辨率：** 音频扩散模型可以增强音频录音的分辨率和质量，使其更加清晰和详细。这在音频修复或处理低质量录音时尤为有用。'
- en: '**Inpainting:** Diffusion models can be leveraged to fill in missing or corrupted
    parts of audio signals, restoring them to their original or improved state. This
    is valuable for repairing damaged audio recordings, completing sections of audio
    that may be missing, or adding transitions between audio clips.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像修复：** 扩散模型可以用来填补音频信号中缺失或损坏的部分，将其恢复到原始或改进后的状态。这对于修复损坏的音频录音、完成可能缺失的音频片段或在音频剪辑之间添加过渡非常有价值。'
- en: Conclusion
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: There is no doubt that these new generative AI models are incredible technological
    advancements, independent of whether they are viewed in a positive or negative
    light. There are many more aspects to diffusion models that can optimize their
    performance regarding speed, diversity, and quality, but we have discussed the
    base principles that govern the functionality of these models. This knowledge
    provides a deeper context into what it really means when these models are generating
    “new sounds”.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，这些新的生成型人工智能模型是令人惊叹的技术进步，不论它们被视为积极还是消极的。关于扩散模型的优化空间非常广泛，可以在速度、多样性和质量等方面提升它们的性能，但我们已经讨论了这些模型功能的基本原理。这些知识为我们提供了更深刻的背景，让我们理解当这些模型生成“新声音”时，真正意味着什么。
- en: On a broader level, it is not only the music, itself, that people care about
    — it is the human element in the creation of that music. Ask yourself, if you
    were to hear a recording of a virtuosic lightning-fast guitar solo, would you
    be impressed? It all depends. If it was artificially generated by a virtual MIDI
    instrument programmed by a producer, you will likely be unphased and may not even
    like how it sounds. However, if you know an actual guitarist played the solo on
    a real guitar, or even saw him or her do it, you will be completely enamored by
    their expertise and precision. We are drawn to the deftness in a performance,
    the thoughts and emotions behind lyrics, and the considerations that go into each
    decision when crafting a song.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从更广泛的层面来看，人们关心的并不仅仅是音乐本身——更重要的是音乐创作中的人类元素。问问自己，如果你听到一段高超且迅速的吉他独奏录音，你会感到印象深刻吗？这取决于情况。如果这段独奏是由一个制作人编程的虚拟MIDI乐器人工生成的，你可能会毫不动容，甚至不喜欢它的声音。然而，如果你知道这段独奏是由一位吉他手用真实吉他演奏的，或者甚至亲眼看到他或她演奏，你将完全被他们的专业技巧和精准度所吸引。我们被演奏中的灵巧、歌词背后的思想和情感，以及创作歌曲时每个决定背后的考虑所吸引。
- en: While these incredible advancements have led to some existential dread for artists
    and producers, AI can never take that human element away from the sounds and music
    that we create. So we should approach these new advancements with the intent that
    they are tools for enhancing artists’ creativity rather than replacing it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些令人难以置信的进步让艺术家和制作人感到一些生存焦虑，但人工智能永远无法剥夺我们创作的声音和音乐中那份人类元素。因此，我们应该以一种工具的心态来看待这些新的进展，认为它们是为了增强艺术家的创造力，而不是取而代之。
- en: '*All images, unless otherwise noted, are by the author.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者提供。*'
- en: I am an audio machine learning engineer and researcher as well as a lifelong
    musician. If you are interested in more audio AI applications, see my previously
    published articles on [Tiny Audio Diffusion](https://medium.com/towards-data-science/tiny-audio-diffusion-ddc19e90af9b)
    and [Music Demixing](https://medium.com/rock-nheavy/the-music-demixing-ai-revolution-9d1528c6ef7c).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我是一名音频机器学习工程师和研究员，同时也是一名终身音乐人。如果你对更多音频人工智能应用感兴趣，可以阅读我之前发布的文章：[Tiny Audio Diffusion](https://medium.com/towards-data-science/tiny-audio-diffusion-ddc19e90af9b)
    和 [音乐分离](https://medium.com/rock-nheavy/the-music-demixing-ai-revolution-9d1528c6ef7c)。
- en: 'Find me on [LinkedIn](https://www.linkedin.com/in/christopher-landschoot/)
    & [GitHub](https://github.com/crlandsc) and keep up to date with my current work
    and research here: [www.chrislandschoot.com](https://www.chrislandschoot.com/)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [LinkedIn](https://www.linkedin.com/in/christopher-landschoot/) 和 [GitHub](https://github.com/crlandsc)
    上找到我，了解我当前的工作和研究进展，访问我的网站：[www.chrislandschoot.com](https://www.chrislandschoot.com/)。
- en: Find my music on [Spotify](https://open.spotify.com/artist/2i6noWJnJQPXPsudoiJuMS?si=fvLOxUPqTAWKB894WXMz5Q),
    [Apple Music](https://music.apple.com/us/artist/after-august/259370281), [YouTube](https://www.youtube.com/AfterAugust),
    [SoundCloud](https://soundcloud.com/after-august), and other streaming platforms
    as [After August](https://www.instagram.com/the_after_august/).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [Spotify](https://open.spotify.com/artist/2i6noWJnJQPXPsudoiJuMS?si=fvLOxUPqTAWKB894WXMz5Q),
    [Apple Music](https://music.apple.com/us/artist/after-august/259370281), [YouTube](https://www.youtube.com/AfterAugust),
    [SoundCloud](https://soundcloud.com/after-august) 和其他流媒体平台上找到我的音乐，艺名为 [After August](https://www.instagram.com/the_after_august/)。
